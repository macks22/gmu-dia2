{"80448":{"abstract":"","title":"Efficient Lossy Data Compression Via Statistical Model Selection","awardID":"0296038","effectiveDate":"2001-07-01","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":[209513],"PO":["223414"]},"62914":{"abstract":"The LinBox group of twelve researchers in three countries (USA, France, Canada) proposes research in the design of efficient algorithms for linear algebra, in their implementation in a software library, and in how to interface the library to widely-used scientific computing software. Algorithms will be implemented, and new algorithms designed, for the black box representation of matrices---hence the name LinBox---over entry domains that are either symbolic, that is, exact, or floating point, that is, inexact. The library is generically programmed as C++ template classes with abstract underlying arithmetics; they can be compiled with a variety of fast libraries for the basic field, floating point, and polynomial operations. A server\/client interface seamlessly attaches the library to the common general purpose symbolic systems Maple and Mathematica and to the numeric system MatLab. Parallel execution of the implemented algorithms is facilitated. <br\/><br\/>Black box matrices are stored as functions (as linear operators in effect): the matrix is a procedure that takes an arbitrary vector as input and efficiently computes the matrix-times-vector product. Black box linear algebra generalizes sparsity. The LinBox library will contain algorithms for solving singular and non-singular systems of linear equations whose coefficient matrix is given in black box representation. Furthermore, it is proposed to develop fast methods for the rank and the minimal and characteristic polynomial of a black box matrix. Finally, LinBox will contain methods for linear Diophantine problems with black box matrices, such as computing an integral solution to a linear system with integer entries and computing the Smith normal form of an integer matrix.","title":"ITR\/ASC: Collaborative Research - Linbox: A Generic Library for Seminumeric Black Box Linear Algebra","awardID":"0112807","effectiveDate":"2001-07-15","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"9199","name":"UNDISTRIBUTED PANEL\/IPA FUNDS"}}],"PIcoPI":["565201",161531,"451563"],"PO":["381214"]},"80416":{"abstract":"","title":"RUI: Testing and Enhancing a Prototype Program Fusion Engine","awardID":"0296006","effectiveDate":"2001-07-01","expirationDate":"2002-05-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}}],"PIcoPI":["331030"],"PO":["397849"]},"80606":{"abstract":"","title":"POWRE: Parallel and Distributed Protein Sequence-Structure Alignment","awardID":"0296197","effectiveDate":"2001-07-31","expirationDate":"2002-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1592","name":"PROF OPPOR FOR WOMEN IN RSCH"}}],"PIcoPI":[209897],"PO":["289456"]},"59281":{"abstract":"The goal of this research is to develop scalable procedures for the derivation of high-quality tests specifically designed for digital logic circuits that contain scan to enhance testability. Such procedures are needed since scan (either full or partial scan) is currently used in most electronic chips, and is expected to continue to be the prevalent design-for-testability technique for design paradigms such as core-based design. The investigators develop procedures for test generation, test compaction, identification of undetectable faults, built-in test generation and delay fault testing specifically targeting scan circuits.<br\/><br\/>In the test application scheme used in this research, a sequence of one or more primary input vectors is applied between every two scan operations. In all the procedures developed, the goal is to use sequences of primary input vectors that are as long as possible. The reasons are that long sequences of primary input vectors contribute to at-speed testing of the circuit, which is important for detecting delay defects, and they allow the number of tests to be kept low, which reduces the test application time. In addition, the circuit operates in its normal mode of operation, potentially resulting in average power consumption which is typical of normal operation. Several commercial tools use the test application scheme adopted in this research, justifying its consideration. However, only a small number of studies have been reported in the literature of effective solutions to the various testing problems under this scheme. This research develops tools that may be used together to provide a comprehensive and scalable solution to the special problems associated with testing<br\/>of scan circuits under this test application scheme.","title":"Collaborative Research: High Quality Tests for Scan Circuits","awardID":"0098091","effectiveDate":"2001-07-01","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}}],"PIcoPI":["550779"],"PO":["562984"]},"59360":{"abstract":"The International Technology Roadmap for Semiconductors identifies a need for innovative testing and diagnostic methods for digital and mixed-signal devices. Traditional testing strategies are becoming less effective for several reasons. As device operational frequencies increase, the cost of functional test equipment, capable of testing devices at their native speed (at-speed), is becoming cost prohibitive. Testing methods which use slower, cheaper test equipment need to be able to detect defects that cause at-speed delay failures. The increasing complexity and diversity of these devices also make it difficult to access internal nodes and achieve good fault coverage and parametric device information. Technology trends, such as increases in <br\/>device leakage currents, have reduced the effectiveness of alternative tests such as IDDQ. This research is designed to address these short-coming by investigating device transient and novel quiescent <br\/>signal techniques. <br\/><br\/>In previous work, process-tolerant VDDT and IDDQ methods have been demonstrated to (1) detect resistive shorting and open defects, (2) predict performance in defect-free devices, and (3) predict defect <br\/>location in defective devices. These methods are based on the cross-correlation of multiple static and transient power supply signals. Cross-correlation is used to calibrate for process and technology-related <br\/>variations, such as shifts in transistor betas and increases in leakage current. This research focuses on the implementation of these techniques in a production test environment, and on identifying their capabilities <br\/>and limitations. The scalability of the methods to large commercial devices is investigated with industrial partners. Production test environment issues, such as measurement noise and instrumentation <br\/>sampling requirements, test generation strategies and specialized hardware are also investigated.","title":"Production-Oriented VDDT and IDDQ Device Testing Methods Based on Multiple Power Supply Pad Measurements","awardID":"0098300","effectiveDate":"2001-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}}],"PIcoPI":["486576"],"PO":["562984"]},"59140":{"abstract":"This is the first year funding of a three-year continuing award. Computer-integrated engineering design consists of a variety of complex and challenging processes, ranging from conceptual design, geometric modeling, evaluation, prototyping, manufacturing, assembly, to production. To ameliorate CAD\/CAM processes, as well as to revolutionize human-computer interaction technology, the PIs will develop an interactive and tangible virtual environment to advance the current state-of-the-art through the novel integration of dynamic modeling and real-time haptic sculpting. To these ends, they will focus on critical fundamental issues relating to the rapid and accurate synchronization of multiple heterogeneous representations of geometric primitives and physical properties of virtual material in a software environment, including optimal algorithms and their time\/space analysis, and numerical characteristics such as stability, robustness, and error bounds. They will investigate haptic interaction techniques towards the next-generation design technology in a systematic way through the development of novel haptic sculpting toolkits and the evaluation of both toolkit utilities and human factors. Finally, they will investigate the effective integration of haptic principles with mature geometric design techniques and develop an experimental virtual environment with haptic interface and real-time haptic sculpting capabilities. The PIs will disseminate the novel haptic technology and its software to the U.S. design industry and computer enterprises. To further broaden the accessibility of the new haptic technology in engineering, sciences, and medicine, the PIs will make extra efforts (through extensive collaborations) towards generalizing their prototype software system to other haptics-relevant applications such as surgical simulation and training, and haptic visualization of large scientific data sets, as time allows.","title":"A Haptic-based Interface and Sculpting System for Virtual Environments","awardID":"0097646","effectiveDate":"2001-07-01","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6845","name":"HUMAN COMPUTER INTER PROGRAM"}}],"PIcoPI":["416071","541908"],"PO":["565227"]},"59285":{"abstract":"This project concerns links between programming language theory and semantics and formal methods in security analysis, on two levels of modeling detail. 1) Multiset rewriting framework (MSR), in which protocol execution is carried out symbolically, detects common protocol errors. The project studies decision problems for reachability in the MSR formalism extended with disequality testing. MSR formal analysis is also extended to properties of fair exchange protocols. The relationship between MSR and the secure pi-calculus is investigated. 2)A different approach develops an analysis of cryptographic protocols based on the methods of programming language theory, but incorporating the probabilistic techniques of cryptography. This approach allows the analysis of probabilistic protocols and probabilistic encryption. The proposed work in this direction develops principles and proof rules for reasoning about probabilistic observational equivalence. Foundational questions about probabilistic process calculus are addressed, such as a precise operational semantics of the calculus and the establishment of polynomial upper bounds on process execution in an extended version of the calculus that allows polynomial iteration. An information-theoretic variant of the calculus and the relationship to MSR are investigated.","title":"Advances in Language-Based Security Analysis","awardID":"0098096","effectiveDate":"2001-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2876","name":"DISTRIBUTED SYSTEMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}}],"PIcoPI":["381704"],"PO":["564388"]},"59286":{"abstract":"Switching activity estimation is an important aspect of power estimation at circuit level. Although simulations can provide accurate estimates of switching activity, since simulations are time consuming, recently, probabilistic models are being considered that can capture temporal, spatial and sequential correlations in a circuit. In this research, we propose to explore a novel switching probability estimation strategy using Bayesian networks.<br\/><br\/>Bayesian Networks (BN) can be used to effectively model complex conditional dependencies over a set of random variables. The BN inference schemes serve as powerful computational mechanisms that transform the circuit into a junction tree of cliques to allow for probability propagation by local message passing. The proposed approach is accurate and fast.<br\/><br\/>We intend to investigate (i) power estimation of very large combinational circuits (by developing segmentation schemes) with input modeling to handle complexly correlated input signals, (ii) Bayesian network formalism to handle real delay models with glitches, and (iii) switching estimation in sequential circuits using the Bayesian networks model.","title":"Switching Activity Estimation Using Bayesian Networks","awardID":"0098103","effectiveDate":"2001-07-01","expirationDate":"2004-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}}],"PIcoPI":["297760"],"PO":["562984"]},"59297":{"abstract":"Innovations in computer and communications technologies are making it easier<br\/>and faster to do things but also bring with them new security risks. This<br\/>proposal is concerned with cryptography, which is an important component in the<br\/>provision of security in the electronic world. A cryptographer designs<br\/>schemes, or protocols, for tasks such as data authentication, data encryption,<br\/>identification, and key distribution which are implemented and incorporated<br\/>into the computer systems and are responsible for imbuing data and transactions<br\/>with attributes like privacy and integrity. Providing high-quality,<br\/>cost-effective cryptography is a challenge because cryptographic protocols are<br\/>easy to specify but hard to analyze, and notorious for containing bugs that<br\/>take a long time to be discovered. The goal of this project is to identify and<br\/>tackle practical problems in cryptographic protocol design and analysis based<br\/>on theoretical techniques, delivering real-world usable protocols backed by<br\/>theoretically sound security analyses, and thereby impact implementation, usage<br\/>and standardization of cryptography in the computer and communications<br\/>industry.<br\/><br\/>The specific research proposed here is on three broad topics: anonymity,<br\/>authenticated encryption and asymmetric identification protocols. The term<br\/>``anonymity'' is broadly used to refer to issues regarding keeping private the<br\/>identities of parties engaging in electronic transactions. The researchers<br\/>will investigate anonymity for mix-nets, anonymous encryption, and blind<br\/>signatures. In each case they will look at both foundational issues such as<br\/>definitions, and then the possibility of practical, proven secure<br\/>constructions. In the authenticated encryption domain they will investigate the<br\/>security of the popular encrypt-with-redundancy paradigm. Finally they will<br\/>consider the design of identification protocols secure against attackers having<br\/>the powerful capability of resetting the internal state of the client machine<br\/>and also seek improved transformations of identification protocols to digital<br\/>signature schemes.","title":"Design and Analysis of Cryptographic Protocols for Secure Communication","awardID":"0098123","effectiveDate":"2001-07-01","expirationDate":"2004-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["521666"],"PO":["543507"]},"59320":{"abstract":"Proposal #0098175<br\/>Hitz, Markus A.<br\/>North Georgia College<br\/><br\/>Data collected in experiments, or constants in physics and other sciences are of limited precision, introducing some degree of uncertainty in computations. Traditional exact methods fail to reveal the entire set of solutions to algebraic problems that have uncertain parameters. Often, these problems are inherently ill-conditioned, such that numerical methods become sensitive to small perturbations of the input parameters.<br\/><br\/>Hybrid symbolic-numeric algorithms have proven to be successful for finding \"nearest\" solutions of problems that cannot be solved exactly. Efficient (polynomial time) algorithms have been developed for common problems, such as computing approximate GCDs of univariate polynomials. For other problems, e.g., finding the nearest singular Hankel or Toeplitz matrix, it is currently unknown whether there exist efficient algorithms. This project will continue research in this area, in particular on the problem of approximate bivariate and multivariate factorization.<br\/><br\/>For both areas, symbolic and numeric computing, vast software libraries and programming environments are readily available. For hybrid computing those libraries either have features that are computationally expensive (symbolic), or lack symbolic support at all (numeric). A new class of systems adds Limited Symbolic Capabilities (LSC) to existing libraries that have optimized implementations of rational and floating point arithmetic. We will investigate, and contribute to, LSC systems.<br\/><br\/>This is an RUI project. Undergraduate students will engage in research that is appropriate for their level of expertise. In the process they will also gain valuable skills in configuring local area networks, and in operating clusters of computers which become more and more important in server applications.","title":"Hybrid Symbolic-Numeric Computing in Distributed LSC Environments","awardID":"0098175","effectiveDate":"2001-07-15","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":[152364],"PO":["321058"]},"59001":{"abstract":"IIS-0097278<br\/>Miroslaw Truszczynski, Raphael A. Finkel and Victor Marek<br\/>University of Kentucky<br\/>$146,167 - 12 mos<br\/><br\/><br\/><br\/>Nonmonotonic Reasoning and Computational Knowledge Representation<br\/><br\/><br\/><br\/><br\/>This is the first year funding of a three year continuing award. The PI will study and implement computational knowledge-representation systems based on the paradigm of answer-set programming (ASP) with nonmonotonic logic that he recently identified, in order to demonstrate the practicality and effectiveness of the approach. Logic is most commonly used in knowledge representation as follows. To solve a problem we represent its constraints and the relevant background knowledge as a theory in the language of first-order logic (or its fragment). We formulate the goal (the statement of the problem) as a formula of the logic. We then use proof techniques to decide whether this formula follows from the theory. A proof of the formula, variable substitutions or both determine a solution. Taking a different approach, the PI will study and develop computational knowledge representation tools based on nonmonotonic logics rather than on the first-order logic. In addition, he departs from the single-intended model approach dominant in logic programming. Under the ASP paradigm, a theory in a nonmonotonic formalism is regarded as a specification of a family of sets - a collection of its intended models. Each model is viewed as a representation of a different single solution. The PI will investigate syntactic and semantic issues of ASP formalisms based on nonmonotonic logics, study methods for fast computing with these formalisms, develop practical implementations, and demonstrate effectiveness of answer-set programming engines and their applicability in knowledge representation. If successful, the work will establish answer-set programming as a viable approach to declarative programming, which in turn will provide AI researchers and practitioners with a new generation of computational tools for knowledge representation.","title":"Nonmonotonic Reasoning and Computational Knowledge Representation","awardID":"0097278","effectiveDate":"2001-07-15","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0406","name":"Office of INTL SCIENCE & ENGINEERING","abbr":"OISE"},"pgm":{"id":"5979","name":"CENTRAL & EASTERN EUROPE PROGR"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6856","name":"ARTIFICIAL INTELL & COGNIT SCI"}}],"PIcoPI":["501232","407833",151512],"PO":["491702"]},"57571":{"abstract":"The objective of this project is to develop and implement new and<br\/>efficient optimization methods for robust and discrete optimization<br\/>problems. The applications of interest to us are in the fields of<br\/>financial engineering and network design. The robust optimization<br\/>framework is an attempt to correct for the modeling uncertainties<br\/>that are inevitable in engineering. Optimization problems are<br\/>especially susceptible to modeling errors since, in trying to exploit<br\/>the constraints, the optimal solutions typically amplify the errors<br\/>several fold. In the robust framework, the perturbations are modeled<br\/>as unknown, but bounded, and optimization problems are solved<br\/>assuming worst case behavior of these perturbations. Robustness to<br\/>modeling and estimation errors is an issue of critical importance for<br\/>financial optimization problems because of the serious consequences<br\/>of making wrong bets! Surprisingly, however, robust optimization has<br\/>not been widely explored in financial engineering. The research<br\/>proposed here formulates robust dynamical models for financial<br\/>problems and develops semidefinite rogramming based methods for<br\/>solving them. These models systematically account for parameter<br\/>uncertainty and robustly update error-bounds as more information<br\/>becomes available over time. In addition, the project extends the<br\/>semidefinite relaxation methodology to probabilistically robust<br\/>optimization problems that naturally emerge in the financial context.<br\/>The other research focus of this proposal is on developing<br\/>semidefinite models for graph theoretic problems such as the<br\/>traveling salesman problem and network design. These models employ<br\/>linear matrix inequalities (LM ) to represent geometric<br\/>constraints, such as graph connectivity, specified number of<br\/>edge\/vertex disjoint paths, etc. The optimization problems resulting<br\/>from these LM models are, typically, mixed integer semidefinite<br\/>programs, i.e. semidefinite programs where some of the decision<br\/>variables are constrained to be integers. Currently, mixed<br\/>semidefinite programs are appproximately solved by relaxing the<br\/>integrality constraints. However, as computational ower increases and<br\/>the interior point methods for solving semidefinite programs become<br\/>more efficient, the PI expects that there would be a push for<br\/>developing systematic methods of tightening the relaxations - as in<br\/>the case of linear programming relaxations of mixed integer programs.<br\/>As a first step in this direction, the PI proposes to develop several<br\/>cutting lane strategies for mixed semidefinite programs. Although the<br\/>problems of interest to the PI belong to disparate application areas,<br\/>they are linked in that linear matrix inequalities and semidefinite<br\/>programming provide the necessary tools to efficiently model and<br\/>solve them. The education component of this proposal includes<br\/>developing a sequence of graduate courses on engineering applications<br\/>of optimization. These courses would fill an important gap in the<br\/>curriculum of the Deppartment by providing students with a firm<br\/>theoretical and practical grounding in optimization. The World Wide<br\/>Web will be extensively used in these courses. All the teaching<br\/>material will be available on the web. The PI will develop Java-based<br\/>applets for all the examples used in the courses which would allow<br\/>students to experiment with these examples in real time. Also, the<br\/>optimization resources on the web will be integrated into the<br\/>curriculum. This should be particularly useful to students from<br\/>industry who would take the courses over the Video Network. To expose<br\/>undergraduate students to research, the PI plans to organize an<br\/>interdisciplinary research program in optimization and its<br\/>applications. To facilitate industry outreach, the PI plans to<br\/>implement the results of the proposed research into a software<br\/>package and publish expository articles on the applications of the<br\/>new techniques. The PI expects that the availability of a user<br\/>friendly software will spur further applications and encourage<br\/>industrial collaboration.","title":"CAREER: Semidefinite Methods for Robust and Discrete Optimization and Their Applications","awardID":"0092972","effectiveDate":"2001-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["524834"],"PO":["381214"]},"57484":{"abstract":"EIA-0092614<br\/>SUNY-Albany<br\/>Sharon S. Dawes<br\/><br\/>Digital Government: A Multinational Investigation of New Models of Collaboration for Delivering Government Services<br\/><br\/>The intention of this research is to enhance understanding of multi-organizational partnerships in the innovative delivery of government services. Studies will include not only cross-sector partnerships, but also partnerships between government agencies. Collaborating in the project's leadership is Canada's Centre Francophone d'Informatisation des Organisations; several other countries around the world are also participating in the study.<br\/><br\/>Questions to be addressed by the project include:<br\/><br\/>1. Which political, socio-economic and cultural factors promote successful interorganizational collaboration?<br\/>2. What are the characteristics of organizations that engage in such partnerships?<br\/>3. What are critical success factors?<br\/>4. Which technologies offer the most promise?<br\/>5. What are the advantages and disadvantages of different forms of collaboration?","title":"Digital Government: A Multinational Investigation of New Models of Collaboration for Delivering Government Services","awardID":"0092614","effectiveDate":"2001-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1706","name":"DIGITAL GOVERNMENT"}}],"PIcoPI":["433032"],"PO":["371077"]},"59310":{"abstract":"Proposal Number: CCR-0098154<br\/> PI: R. Sekar<br\/> Department of Computer Science<br\/> SUNY @ Stony Brook<br\/> NY 11794<br\/><br\/>Networked software systems are playing increasingly important roles in critical services such as commerce, banking and telecommunication. Existing techniques for protecting such systems against intruder attacks are reactive in nature, offering little protection against unknown attacks. Solutions, such as applying security patches, last only until newer attacks emerge. System administrators are thus in a constant struggle to stay ahead of a vast army of resourceful hackers. This project develops a proactive approach to protect software systems against known and unknown attacks. It is based on high-level models of security-relevant system behaviors. Actual behaviors are compared against these models to detect deviations, which are deemed to indicate attacks. In order for the approach to work with COTS software, behaviors are modeled in terms of events observable external to the software system, e.g., invocation of system calls and reception\/transmission of network packets. In contrast with previous work, which was mainly concerned with post-attack detection, the proposed approach can prevent and\/or contain damage due to attacks. Moreover, it addresses a wide range of threats within a single framework, including software errors in trusted programs, untrusted mobile code and malicious software.","title":"A Model-Based Approach for Securing Software Systems","awardID":"0098154","effectiveDate":"2001-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2876","name":"DISTRIBUTED SYSTEMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}}],"PIcoPI":["550275"],"PO":["564388"]},"59321":{"abstract":"To isolate a designer from the complexity of a distributed system, there is a need for appropriate middleware abstractions. It has been recognized that these abstractions must be flexible and customizable to accommodate the wide ranging application requirements. Although many customizable frameworks have been proposed, a general framework that can be used to implement different synchronization-related requirements of a distributed application is lacking. The goal of the proposed project is to design a coordination service that provides a basic infrastructure to implement synchronization. A coordination service will be designed that accepts a synchronization specification from an application and interact with the application entities to allow only those execution sequences permitted by the specification. The used of event-notification technology will be explored to implement the coordination service. New synchronization elements to be added to convert an event-notification service into a coordination service will be identified. The research will focus on developing a scalable, fault-tolerant and customizable coordination service. A critical challenge will be to accommodate synchronization without sacrificing the scalability of the event-notification mechanism. The effectiveness of the coordination service in providing services to other middleware services will be evaluated.","title":"Coordination Services for Distributed Applications","awardID":"0098179","effectiveDate":"2001-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2876","name":"DISTRIBUTED SYSTEMS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["528451"],"PO":["309350"]},"59266":{"abstract":"Emerging web applications are increasingly likely to use dynamic web<br\/>data. Traditionally, requests for dynamic data---both time-varying and<br\/>dynamically-generated---have been handled directly by servers and<br\/>intermediate proxies haven't been allowed to process requests for such<br\/>objects (web proxies have been primarily employed to disseminate data<br\/>that is mostly static). A pure server-based approach for managing<br\/>dynamic data is likely to limit the scalability of emerging web<br\/>applications and increase their vulnerability to server failures. To<br\/>overcome this drawback, novel proxy-based techniques will be developed<br\/>to manage and disseminate dynamic web data. The proposed<br\/>research will address two key issues: (i) data dissemination, which<br\/>addresses the issue of disseminating time-varying web data using<br\/>proxy-based push and pull techniques, and (ii) computation<br\/>dissemination, which addresses the issue of moving computations from<br\/>servers to proxies so as to dynamically generate web objects at a<br\/>proxy. Both techniques have the potential of radically changing the<br\/>way web proxies are designed and used. These techniques will need to<br\/>satisfy three key requirements: user-cognizance (i.e., awareness<br\/>of user and application requirements), intelligence (i.e., the ability<br\/>to dynamically choose the most efficient set of mechanisms to service<br\/>each application), and adaptivity (i.e., the ability to react to<br\/>changing load characteristics). The proposed techniques will be<br\/>evaluated using an eclectic mix of simulation, analysis and prototype<br\/>implementation.","title":"Proxy-based Dissemination of Dynamic Web Data","awardID":"0098060","effectiveDate":"2001-07-01","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2876","name":"DISTRIBUTED SYSTEMS"}}],"PIcoPI":[152242,"558563"],"PO":["309350"]},"63050":{"abstract":"Techniques such as program generation, partial evaluation, just-in-time compilation, and run-time code generation respond to the need for general purpose programs which do not pay unnecessary<br\/>run-time overheads. The thesis of this project is that a uniform, principled, high-level, and practical view of these diverse techniques is possible through multi-stage programming, a novel paradigm for the development of maintainable, high-performance software. The key idea in multi-stage programming is the use of simple, high-level annotations to allow the programmer to break down the cost of a computation into distinct stages.<br\/><br\/>The goal of this proposal is to demonstrate that the theoretical machinery that has been developed for multi-stage programming can be put to work. This project will involve the development of compilers<br\/>of multi-stage programming languages, addressing both practical and theoretical problems that arise in the development of such systems, and using these compilers in interesting applications ranging from dynamic programming algorithms and rewriting systems to implementations of domain specific programming languages.","title":"ITR\/SY(CISE): Putting Multi Stage Annotations to Work","awardID":"0113569","effectiveDate":"2001-07-01","expirationDate":"2003-05-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["541946","495365"],"PO":["564388"]},"59355":{"abstract":"The explosive growth of the World Wide Web, the variety of information hosted by a given site, the rapid fluctuations in user demand, and the complexity of the preferred architectural trend are among the reasons that make increasingly challenging for system administrators to effectively mange today's complex web server systems. This proposal addresses this problem by seeking to develop smart algorithms that help performing difficult tasks such as web server cluster configuration, capacity planning, fault management and load balancing, by continuously monitoring and adapting to changes in the workload and system. Workload mointoring will provide input to analytic models whose solution will in turn be used by both off-line and on-line algorithms that can propose improved system configurations. To this end, the proposed research objectives will provide: (a) methods to derive detailed statistical workload characterizations of web servers, (b) new and efficient solutions for analytic models of systems that serve tasks drawn from heavy tail probability distributions, (c) a software tool particularly targeted to load balancing in clustered web servers, to be used by non expert modelers, including system administrators, and (d) workload-aware and system-aware algorithms that significantly improve performance and ease of web system management.","title":"Effective Techniques and Tools for Resource Management in Clustered Web Servers","awardID":"0098278","effectiveDate":"2001-07-15","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2876","name":"DISTRIBUTED SYSTEMS"}}],"PIcoPI":["451554","518455"],"PO":["289456"]},"64371":{"abstract":"IIS-0119389<br\/>Marilyn A. Walker<br\/>Association for Computational Linguistics<br\/>$7,000 - 6 mos<br\/> <br\/><br\/><br\/>WORKSHOP: ACL'2001 Student Research Workshop<br\/><br\/><br\/><br\/>This is funding to subsidize expenses of student participants in the Student Research Workshop organized in conjunction with the Association for Computational Linguistics Conference (ACL'2001), which will be held July 6-11, 2001, in Toulouse, France. The Association for Computational Linguistics (ACL) is the primary international organization in the field of natural language processing and language engineering, with two regional chapters, Europe (EACL) and North America (NAACL), of approximately equal size. The Association's annual conference, which is the major international meeting in the field, has traditionally rotated between North America and Europe but was held last year for the first time in a Pacific Rim location (Hong Kong). The ACL Student Workshop is an inexpensive yet highly effective means of encouraging young and upcoming computational linguists. The intimate workshop format, in addition to affording student participants sufficient time to present their research (20 minutes) and receive feedback from a panel of established researchers in the field (15 minutes), encourages them to begin building a rapport with established researchers. Through feedback from the panel and other student participants, this nurturing effort will pay dividends by providing the students with invaluable exposure to outside perspectives and guidance on their work at a critical time in their research in this rapidly changing research field.","title":"ACL 2001 Student Research Workshop, in Toulouse, France","awardID":"0119389","effectiveDate":"2001-07-01","expirationDate":"2002-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6845","name":"HUMAN COMPUTER INTER PROGRAM"}}],"PIcoPI":["542063"],"PO":["564456"]},"59312":{"abstract":"Proposal #0098156<br\/>Purdue Research Foundation<br\/>Doerschuk, Peter C<br\/><br\/>A key challenge in computational structural biology is the determination of the 3-D structure of a virus, and<br\/>especially dynamical changes in 3-D structure which are central to understanding the function of the virus. This information is central to rational design of drugs to combat viral infections and to the use of viruses for other purposes, e.g., as vehicles for the targeted delivery of drugs to specific organs. Solving these problems involves the development, analysis, implementation and use of new algorithms for two numerical computation problems, global optimization and multidimensional quadrature.<br\/><br\/>The investigators compute a 3-D structure by locating the global minimum of a cost which is a function of experimental data and of a predictor, and which quantitates the difference between the data and the prediction of the data. The predictor, whose evaluation requires multidimensional quadrature, is a function of parameters describing the 3-D structure of the virus and any unknown aspects of the data collection process and the minimization is with respect to these parameters. Performance of the approach is limited by<br\/>the global optimization and quadrature tools and therefore these tools are the foci of this research. Key global<br\/>optimization issues are exploiting the multi-scale structure of the data and parameters in the cost due to the presence of Fourier transforms and the tradeoff between accuracy and computational expense in the evaluation of the cost due to embedded quadratures. Key multidimensional quadrature issues are the unusual integrands and regions of integration, e.g., to integrate a function of three variables with icosahedral symmetry over the three Euler angles that define a 3-D rotation.","title":"Computation for Structural Biology: Tools to Enable Dynamic 3-D Reconstruction of Time-varying Viral Structures","awardID":"0098156","effectiveDate":"2001-07-15","expirationDate":"2005-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["226023","518003"],"PO":["532791"]},"63030":{"abstract":"The LinBox group of twelve researchers in three countries (USA, France, Canada) proposes research in the design of efficient algorithms for linear algebra, in their implementation in a software library, and in how to interface the library to widely-used scientific computing software. Algorithms will be implemented, and new algorithms designed, for the black box representation of matrices---hence the name LinBox---over entry domains that are either symbolic, that is, exact, or floating point, that is, inexact. The library is generically programmed as C++ template classes with abstract underlying arithmetics; they can be compiled with a variety of fast libraries for the basic field, floating point, and polynomial operations. A server\/client interface seamlessly attaches the library to the common general purpose symbolic systems Maple and Mathematica and to the numeric system MatLab. Parallel execution of the implemented algorithms is facilitated. <br\/><br\/>Black box matrices are stored as functions (as linear operators in effect): the matrix is a procedure that takes an arbitrary vector as input and efficiently computes the matrix-times-vector product. Black box linear algebra generalizes sparsity. The LinBox library will contain algorithms for solving singular and non-singular systems of linear equations whose coefficient matrix is given in black box representation. Furthermore, it is proposed to develop fast methods for the rank and the minimal and characteristic polynomial of a black box matrix. Finally, LinBox will contain methods for linear Diophantine problems with black box matrices, such as computing an integral solution to a linear system with integer entries and computing the Smith normal form of an integer matrix.","title":"ITR\/ACS: Collaborative Research - Linbox: A Generic Library for Seminumeric Black Box Linear Algebra","awardID":"0113463","effectiveDate":"2001-07-15","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["346013"],"PO":["321058"]},"59961":{"abstract":"Lenstra<br\/>0100485<br\/><br\/>The proposed research belongs to the interface between number theory and<br\/>algebra. It is inspired by problems that come up in an algorithmic<br\/>context and in arithmetic algebraic geometry. Altogether, the proposal<br\/>contains 19 problem sets: five from Algorithmic Number Theory, three<br\/>from Algebraic Number Theory, five from Commutative and Homological<br\/>Algebra, three from the Geometry of Numbers, and three from Group Theory.<br\/>The collection has been composed with a view towards assisting the<br\/>investigator's many current and future graduate students in choosing<br\/>suitable thesis subjects. The problems have the appealing features of<br\/>appearing to be feasible without being trivial, and of being specific<br\/>without being narrow. They belong to mainstream areas that will also<br\/>serve the students after obtaining their degrees. Of the nineteen<br\/>problem sets, the following two are both easy to formulate and<br\/>attractive. The first is the development of an algorithmic theory of<br\/>quadratic forms over rings and fields of arithmetic interest. A typical<br\/>question is how quickly one can find a representation of a positive<br\/>integer as a sum of four squares. Or: it is known that any odd unimodular<br\/>indefinite inner product space over the ring of integers is<br\/>diagonalizable; given the symmetric matrix that defines the inner<br\/>product, how quickly can one find the change of basis that diagonalizes<br\/>the form? A first investigation shows that one may expect a wide spectrum<br\/>of answers to the algorithmic questions in this area, displaying all the<br\/>riches of number-theoretic algorithms. The second is giving class number<br\/>estimates for orders in number fields. What is a good upper bound for the<br\/>number of equivalence classes of fractional ideals of a giving order,<br\/>expressed as a function of the degree and the discriminant of the order?<br\/>And can one find better estimates for orders that have nice properties,<br\/>such as being Gorenstein? This type of question is of importance in the<br\/>theory of abelian varieties, and one will need to apply techniques coming<br\/>from commutative algebra, abelian group theory, combinatorics, and<br\/>elementary analytic number theory.<br\/><br\/>In order to place the project in perspective one may consider the recent<br\/>development of number theory. Present day number theory differs in two<br\/>important respects from number theory twenty five years ago, namely in<br\/>the roles played by algorithms and computers, and by algebraic geometry.<br\/>It has been found that algorithmic number theory has important<br\/>applications, notably in cryptography, and in addition number theorists<br\/>have learned how to use computers for their research. Inventing good<br\/>computational methods for number-theoretic problems has thus become of<br\/>central importance. One of the principal investigator's strengths is in<br\/>the interaction between theory and practice, on the one hand using recent<br\/>theoretical advances for algorithmic purposes and on the other hand<br\/>deriving purely mathematical inspiration from the problems suggested by<br\/>the applications. At the other end of the spectrum, knowledge of<br\/>algebraic geometry has become a standard requirement for aspiring number<br\/>theorists. Virtually every breakthrough in number theory over the past<br\/>few decades, including Andrew Wiles's work on Fermat's Last Theorem, has<br\/>involved arithmetic algebraic geometry. Algebraic geometry depends on a<br\/>broad spectrum of techniques from algebra and algebraic number theory,<br\/>and gives rise to an unending array of tantalizing questions in those<br\/>areas, of which the project studies a sample. What is maybe the most<br\/>exciting of all, is the way in which arithmetic algebraic geometry and<br\/>algorithmic number theory are presently being tied together, both in the<br\/>application of geometric objects to cryptography and in the application<br\/>of algorithmic techniques to investigate geometric objects in number<br\/>theory. The project will be carried out by the investigator's graduate<br\/>students, many of whom will, as experience shows, acquire combined<br\/>expertise in these two areas, which is a very precious but fairly rare<br\/>commodity.","title":"Number Theory with Emphasis on Algorithms and Algebraic Number Theory","awardID":"0100485","effectiveDate":"2001-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1264","name":"ALGEBRA,NUMBER THEORY,AND COM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":[154031],"PO":["565280"]},"57552":{"abstract":"The proposal addresses research issues of distributed vision. In particular, new techniques will be developed to vertically integrate vision algorithms that enable loosely configured large networks of cameras to be treated as a single logical sensor. The proposed work includes: 1) calibration of large heterogeneous sensor networks; 2) wide-area object tracking and recognition; 3) a cooperative processing framework; and 4) development of new distributed multimedia courses and a community outreach program","title":"CAREER: Wide Area Computer Vision - A Theoretical and Practical Framework","awardID":"0092874","effectiveDate":"2001-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["359059"],"PO":["564318"]},"59401":{"abstract":"This project addresses basic theoretical and computational aspects of integer programming and combinatorial optimization, using tools of linear algebra and graph theory. In the nineties the principal investigators have developed a computationally successful approach to mixed 0-1 programming known as lift-and-project. A central theme of the research is to develop this approach in new directions that seem computationally even more promising. One of these directions uses a one to one correspondence recently established by the investigators' team between basic solutions to the higher dimensional linear program used to generate lift-and-project cuts, and certain basic solutions of the LP relaxation of the mixed integer program itself. This correspondence can be used to generate \"deepest cuts\" in the lift-and-project sense without explicitly generating the higher dimensional linear program. Another important topic is the creation of bridges from integer programming to the branch of computer science known as constraint programming. A recently discovered linear characterization of cardinality rules and similar logical constructs, along with the linear time separability of the inequalities involved, makes it possible to develop symbolic constraints usable in an integer programming context that may significantly enhance the power of algorithms dealing with problems involving logical conditions. A third line of research pursued under this project investigates properties of a 0-1 matrix that make the set packing problem or the set covering problem (or both) defined by it have only integer basic solutions. Structural properties of balanced matrices were obtained under previous NSF grants.; ideal and perfect matrices are currently under investigation.<br\/><br\/>Decision makers often face problems that have a combinatorial aspect: choose one among a very large number of possible decisions. A standard approach is to formulate such problems as integer programs and to use a \"solver\" to find the best solution. Although integer programming solvers have improved significantly over the last decade, they are still unable to solve many large scale problems to optimality. This project lays the theoretical and analytical foundation for a new generation of solvers for integer programs. The lift-and-project approach developed by the principal investigators has proved well suited to solve hard integer programming problems. Speed remains an issue however. This research project addresses the speed issue by investigating new, faster ways of computing the cuts, lift-and-project as well as other. The potential benefits of the project are significant since cut generators are already being implemented in commercial integer programming solvers and, obviously, the performance of these solvers would be improved by better cut generators. Based on the recent increase in the use of solvers by managers in most fields of business, solvers with improved performance have the potential to increase productivity in the industries where these solvers are used (manufacturing, airline, financial and other industries).","title":"Integer and Combinatorial Optimization: Polyhedral and Graph Theoretic Methods","awardID":"0098427","effectiveDate":"2001-07-01","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"5514","name":"OPERATIONS RESEARCH"}}],"PIcoPI":["539128","539129"],"PO":["529360"]},"64252":{"abstract":"0118576<br\/>Hemley<br\/><br\/>This twelve-month award will provide support for15 US students and junior researchers to participate in the first multidisciplinary summer school on \"High Pressure Phenomena\" held at the Enrico Fermi International School of Physics in Varenna, Italy, from July 3-13, 2001. The course spans fundamental physics and chemistry, earth and planetary science, materials science and technology, and biology. The PI will co-direct this school with Guido Chiarotti of the International School for Advanced Studies in Trieste, Italy. The course is largely financed by the Enrico Fermi school, the Italian Society of Physics (SIF), Italy's National Research Council (CNR), and UNESCO. At least 80 participants - students, observers, and lecturers - are expected to attend. The course offers opportunities for graduate students and post-doctoral fellows to learn many fundamental concepts applications of high-pressure research. It provides as well the opportunity to develop further international research experiences or collaborations in these fields.","title":"U.S.-Italy Summer School: \"High-Pressure Phenomena\" - A Summer School at the Enrico Fermi International School of Physics, Varenna, Italy","awardID":"0118576","effectiveDate":"2001-07-15","expirationDate":"2002-06-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0109","name":"Office of INTL SCIENCE & ENGINEERING","abbr":"OISE"},"pgm":{"id":"5980","name":"WESTERN EUROPE PROGRAM"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"1710","name":"CONDENSED MATTER PHYSICS"}},{"dir":{"id":"06","name":"Directorate for DIRECTORATE FOR GEOSCIENCES             ","abbr":"GEO"},"div":{"id":"0603","name":"Division of EARTH SCIENCES","abbr":"EAR"},"pgm":{"id":"1580","name":"INSTRUMENTATION & FACILITIES"}}],"PIcoPI":["487381"],"PO":["214302"]},"69543":{"abstract":"","title":"ITW: Women in Information Technology Workplaces: A Study of Women Computer Science Degree Recipients in the Software Industry","awardID":"0196431","effectiveDate":"2001-07-01","expirationDate":"2004-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1713","name":"WORKFORCE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2885","name":"CISE RESEARCH INFRASTRUCTURE"}}],"PIcoPI":["424075","513727","464191"],"PO":["289456"]},"57224":{"abstract":"EIA-0091505<br\/>Teresa Harrison<br\/>Rensselaer Polytech Institute<br\/><br\/>Digital Government: Connected Kids: Designing Database Software for Web Based Info Dissemination to Multiple Audiences<br\/><br\/>In this project, computer and social scientist will collaborate with the city of Troy, NY. The focus will be on youth service educational and non-profit organizations to develop community information systems relevant to the topic area. Many sectors (youth, education, government, non-profit) will be involved in the project.<br\/><br\/>A software package named CIRCLE will be developed to enable design, authoring and maintenance of multimedia applications by multiple and non-technical content developers and user groups. Various focus groups will be used to test CIRCLE's abilities and potential. In addition to computer science, an important research element will be to study the social processes associated with the construction and use of information systems for community audiences.","title":"Digital Government: Connected Kids: Designing Database Software for Web Based Info Dissemination to Multiple Audiences","awardID":"0091505","effectiveDate":"2001-07-15","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1706","name":"DIGITAL GOVERNMENT"}}],"PIcoPI":["212922","168426","168426","430757"],"PO":["371077"]},"57477":{"abstract":"The goal of this research project is to develop agent-based systems--for electronic commerce, automated manufacturing, or supply-chains, for example--where the agents adapt their behaviors so as to maximize their individual rewards. A general predictive theory is developed, which can determine expected emergent behaviors from initial agent capabilities and interaction protocols. A methodology for the engineering of utility-based adaptive multi-agent systems is derived from these studies. Finally, the theories are applied to the development of an incentive-compatible information exchange protocol and its implementation in a prototype information exchange multi-agent system. Information serves as a currency in the protocol, and its value is determined via the agents' adaptive behavior. The aggregate of all the users' knowledge, opinions, and biases are immediately accessible to all users. Such research is critical to begin to predict the emergent behavior of systems and to move in the direction of integrating multi-agent systems theory and practice. New graduate curricula with an emphasis on decentralized systems and an undergraduate course on agent-based software engineering will be developed.","title":"CAREER: An Analysis of the Dynamics of Adaptive Multiagent Systems, with Application to Global Information Exchange Systems","awardID":"0092593","effectiveDate":"2001-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6850","name":"DIGITAL SOCIETY&TECHNOLOGIES"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["525589"],"PO":["564456"]},"65441":{"abstract":"This recommendation memo provides support and justification for funding the Principal Investigator to participate in a DELOS Digital Libraries Workshop to be held in San Cassiano, Italy in June, 2001. This meeting, entitled \"Digital Libraries: Future Research Directions for a European Research Programme,\" is expected to be a seminal event for producing the core intellectual agenda and operational framework for the next phase of European activities in digital libraries research and applications. The US and EU agendas are naturally linked by common goals to achieve interoperability at the networking, systems, content, usage and other stages in the information lifecycle. A long-term overall goal is the development of shared resources for a global information cyber-infrastructure. The meeting is being organized by the DELOS Network of Excellence on Digital Libraries, in cooperation with the Cultural Heritage Applications Unit of the 5th FP IST Programme of the European Commission.","title":"Participation in DELOS Digital Libraries Workshop in San Cassiano, Italy","awardID":"0124707","effectiveDate":"2001-07-15","expirationDate":"2001-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6857","name":"DIGITAL LIBRARIES AND ARCHIVES"}}],"PIcoPI":["464231"],"PO":["433760"]},"57631":{"abstract":"\"CAREER: Point-Based and Image-Based Volumetric Rendering and Detail Modeling For Volume Graphics\"<br\/><br\/>This project will advance and promote the emerging field of volume graphics for science, engineering, medicine, education, and entertainment. It will develop new data representations, rendering techniques, and modeling schemes that are all geared toward making the processing of volumetric datasets more feasible and realistic in a wide range of application domains. Volumetric data - essentially, stored values for every point in a volume - has been most popular in computational science and medicine. For example, a mathematical simulation in computational fluid dynamics produces pressure and speed of the fluid at every point. Similarly, medical scanners like MRI, PET, SPECT, and CT produce a value at every point in the body that they scan. Recently, the size of these datasets has grown at an alarming pace. Medical datasets of over one billion volume elements (voxels) have become frequent, and simulations will soon produce datasets that require terabytes to petabytes of storage. The need to develop efficient visualization methods for these has been identified as an immediate national interest.<br\/><br\/>Technically, this project addresses two critical issues in the larger field of volume visualization. The first is the persistent lack of interactive volume rendering, which is hampered by the immense computational requirements of even moderate-sized datasets. The other is the inherently discrete nature of volumetric datasets. This is important because it establishes a boundary between reality and model, and pushing this boundary therefore provides increased realism in applications. More concretely, the work includes:<br\/>Point-based representations of data, Point-based volumetric objects, Image-based rendering to assist in volume rendering and, Subdivision volumes with detail-on-demand.<br\/><br\/>This research agenda is accompanied by a solid education plan that provides students with comprehensive knowledge in fields pertinent for visualization, volume rendering, and volume graphics. These fields are rather diverse, ranging from image processing to user interfaces to computer vision. Teamwork among the participating students is promoted by integrating all the research into a common application.","title":"Point-Based and Image-Based Volumetric Rendering and Detail Modeling For Volume Graphics","awardID":"0093157","effectiveDate":"2001-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}}],"PIcoPI":["486115"],"PO":["565272"]},"57532":{"abstract":"In the past fifteen years, researchers have developed new algorithms<br\/>for machine learning (computer programs that learn from experience)<br\/>that have excellent theoretical guarantees on their error. These<br\/>so-called multiplicative weight update algorithms receive inputs (like<br\/>an image taken by a robot), and make a prediction as to whether or not<br\/>that image came from a particular location. The theoretical error<br\/>bounds imply that the number of mistakes made by these algorithms is<br\/>guaranteed to be very small. However, many applications of these<br\/>algorithms require an enormous amount of time to learn and predict.<br\/>Thus special techniques must be employed to make them efficient. The<br\/>investigators study new, general, theoretical techniques to make these<br\/>algorithms faster. This research also involves empirically evaluating<br\/>such algorithms in new areas, including computational biology, which is<br\/>studied extensively at the investigators' university. Applying<br\/>theoretical techniques to real problems creates a better understanding<br\/>of the real-world problems and helps direct future theoretical work,<br\/>guiding the transfer of results from theory to practice.<br\/><br\/>Specifically, the investigators study multiplicative weight-update<br\/>algorithms such as Weighted Majority (WM) and Winnow, which have<br\/>on-line mistake bounds with a logarithmic dependence on N, the total<br\/>number of features. This attribute efficiency allows them to be<br\/>applied to problems where N is exponential in the input size, yielding<br\/>great flexibility in their application areas. Such areas include<br\/>pruning decision trees, pruning ensembles of classifiers, learning<br\/>finite geometric concepts, learning DNF formulas, and using<br\/>pseudo-Bayesian predictors over finite hypothesis spaces. However, a<br\/>large N requires techniques to efficiently compute the weighted sums of<br\/>these algorithms. This research explores methods to overcome this<br\/>difficulty, including exploiting commonalities among the features, and<br\/>the more general approach of using Markov chain Monte Carlo (MCMC)<br\/>methods to estimate the total weight contribution without the need for<br\/>special structure in the problem. The investigators also are applying<br\/>their algorithms to various problems in computational biology,<br\/>including drug activity prediction, analyzing microbial population<br\/>dynamics, and identifying special types of human genes.","title":"CAREER: Making Exponential-Time Learning Algorithms Efficient","awardID":"0092761","effectiveDate":"2001-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["424849"],"PO":["499399"]},"59326":{"abstract":"Search and optimization problems are central to all areas of computer science<br\/>and engineering. Finding the optimal layout for a VLSI circuit or the lowest <br\/>energy configuration of a crystal are both examples of optimization problems.<br\/>While such problems are believed to be intractable, requiring exponential time <br\/>to solve the worst-case instances, many heuristic methods have been observed <br\/>to be relatively successful on instances that arise in different applications.<br\/>This project addresses questions concerning the quantitative measures of the <br\/>intractability of search and optimization problems, as opposed to qualitative notions<br\/>such as NP-completeness. The following are some of the questions addressed in this project:<br\/><br\/>1. Which instances of optimization problems are the most intractable ones?<br\/><br\/>2. Exactly how difficult are these problems? <br\/><br\/>3. What are good heuristic methods for solving optimization problems ? <br\/>When and how well do they work? <br\/><br\/>4. Are specific non-complete problems such as factoring also intractable? <br\/><br\/>5. How much does randomness help in solving problems? <br\/><br\/>6. Are hard problems suitable for cryptographic applications?<br\/>If so, what levels of security do they provide these applications? <br\/><br\/>Unconditional answers to these questions first require solving the P=NP problem. <br\/>However, this project will use two approaches to find the most likely answers to <br\/>these questions. The first approach is to provide proofs resolving these issues under<br\/>plausible complexity assumptions. The second approach is to examine restricted but <br\/>powerful classes of algorithms that include the most successful heuristics for the problems<br\/>under study. This approach will include attempts to both explain the success of <br\/>such heuristics and to show limitations that can be used as a guide for the likely inherent<br\/>complexity of the problems.","title":"Quantifying Intractability and the Complexity of Heuristics","awardID":"0098197","effectiveDate":"2001-07-01","expirationDate":"2004-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["515841","515842"],"PO":["543507"]},"59216":{"abstract":"The goal of this research is to develop scalable procedures for the derivation of high-quality tests specifically designed for digital logic circuits that contain scan to enhance testability. Such procedures are needed since scan (either full or partial scan) is currently used in most electronic chips, and is expected to continue to be the prevalent design-for-testability technique for design paradigms such as core-based design. The investigators develop procedures for test generation, test compaction, identification of undetectable faults, built-in test generation and delay fault testing specifically targeting scan circuits.<br\/><br\/>In the test application scheme used in this research, a sequence of one or more primary input vectors is applied between every two scan operations. In all the procedures developed, the goal is to use sequences of primary input vectors that are as long as possible. The reasons are that long sequences of primary input vectors contribute to at-speed testing of the circuit, which is important for detecting delay defects, and they allow the number of tests to be kept low, which reduces the test application time. In addition, the circuit operates in its normal mode of operation, potentially resulting in average power consumption which is typical of normal operation. Several commercial tools use the test application scheme adopted in this research, justifying its consideration. However, only a small number of studies have been reported in the literature of effective solutions to the various testing problems under this scheme. This research develops tools that may be used together to provide a comprehensive and scalable solution to the special problems associated with testing<br\/>of scan circuits under this test application scheme.","title":"Collaborative Research: High Quality Tests for Scan Circuits","awardID":"0097905","effectiveDate":"2001-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}}],"PIcoPI":[152126],"PO":["562984"]},"76101":{"abstract":"EIA-0078854<br\/>Moldovan, Dan I<br\/>Southern Methodist University<br\/><br\/>CADRE: A Tool for Transforming WordNet into a Core Knowledge Base<br\/><br\/> This project extends a popular database of English words to make it more useful in such tasks as question answering, information retrieval, and summarization. Wordnet is a lexical database for English that has been widely adopted in artificial intelligence and computational linguistics for a variety of practical applications. The basic elements of WordNet are sets of words that are linked according to semantic relations: synonomy, antonymy, superordination, and so forth. WordNet is publicly available, widely used, and is currently being into a multilingual database.<br\/><br\/>This project will develop a set of tools that can be applied to current and future versions of WordNet to extend it for knowledge processing applications. The extensions are enhancements of the glosses that currently contain definitions, comments, and examples of sets of words that are linked in WordNet. Enhanced glosses will be syntactically parsed, will have each word tagged with its part of speech, and will themselves be linked with other glosses that describe related concepts.","title":"CADRE: A Tool for Transforming WordNet into a Core Knowledge Base","awardID":"0226861","effectiveDate":"2001-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4725","name":"EXPERIMENTAL SYSTEMS\/CADRE"}}],"PIcoPI":["198121"],"PO":["297837"]},"65002":{"abstract":"Proposal 122293<br\/>U of Ill Urbana-Champaign<br\/>PI: Bresler, Yoram<br\/><br\/>In spite of the focus in recent years on lossy compression of audio, images, and video, lossless data compression remains crucial in applications such as text files, facsimiles, software executables, and<br\/>medical imaging. Universal source coding algorithms, which deal with sources whose statistics are unknown, are of particular importance. Universal coding methods are designed for universal performance over a broad class of possible sources. In these methods the source parameters are estimated, either implicitly or explicitly, and the sequence itself is encoded accordingly. Therefore the coding length for universal methods is g eater than the entropy; the extra coding length, called the redundancy satisfies a fundamental lower bound by Rissanen. <br\/><br\/>The focus of research in universal data compression has been on reducing redundancies. In this sense, context tree weighting (CTW) has achieved the ultimate goal for the important class of tree sources, because it<br\/>has essentially achieved Rissanen 's bound. However, in addition to low redundancies, a universal coding method must be computationally fast, and consume little memory. Neither of the two leading methods, CTW or<br\/>PPM, a compression method that has been fine-tuned by various heuristics for practical use, are particularly strong performers in these respects. Therefore, the main goal of the proposed research is to develop algorithms featuring fast computation and low memory use, while providing compression near Rissanen 's bound. <br\/><br\/>Like some of the most efficient high-performance universal compression algorithms to-date, the proposed approach is based on the Burrows Wheeler transform (BWT). The BWT is an invertible transform whose output contains segments in which symbols are approximately independent identically distributed. Owing to this similarity to piecewise i.i.d. (PIID), compressing the BWT output using PIID methods yields good<br\/>compression results. However, such methods cannot achieve universal coding redundancies close to Rissanen 's bound because they require (whether implicitly or Explicitly) extra bits to encode the positions of<br\/>transitions between segments in the BWT output. Recognizing this hidden overhead, this project proposes to take a fresh look at BWT based-methods and the relationship to the fundamental redundancy bounds.<br\/>The project will explore ways to close the gap between traditional BWT-based methods and Rissanen 's bound while retaining the computational efficiency of the BWT. A particular challenge will be to apply this approach to lossless image compression. <br\/><br\/>The resulting algorithms will have linear complexity, and be better than any current algorithm with comparable asymptotic compression performance, in terms of computation and\/or memory use. Some versions of these algorithms will also have simple structure, admitting fast hardware implementations. Furthermore, this research will reveal the role of context modeling in universal lossless image compression. Since near-Rissanen redundancies with linear complexity are hard to beat, we expect a shift in the universal coding literature from compression improvement to implementation and practicality.","title":"Efficient Algorithms for Lossless Data and Image Compression","awardID":"0122293","effectiveDate":"2001-07-01","expirationDate":"2003-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["551068"],"PO":["564898"]},"65299":{"abstract":"IIS-0123712<br\/>Allen L. Ambler and Jennifer L. Leopold, University of Kansas<br\/>$26,250 - 12 mos<br\/><br\/><br\/><br\/>Children's Programming Odyssey<br\/><br\/><br\/><br\/>This is a standard award. The 2001 IEEE Symposia on Human-Centric Computing Languages and Environments (HCC'01), to be held in Stresa, Italy on September 5-7, will include a special event - the Children's Programming Odyssey - featuring some of the world's premier researchers in children's programming languages, including Alan Kay (Squeak), Ken Kahn (Toontalk), Allan Cypher (Stagecast), and Alex Repenning (AgentSheets). This is funding to support participation in the Odyssey of up to ten graduate students from the United States, who will each give brief presentations of their thesis work followed by a positive critique from the panel of experts. Student participants will be invited based on materials submitted to a selection committee, to consist of the PI (who is also organizer of the Odyssey), Margaret Burnett from Oregon State University, and another member yet to be named. The primary criterion to be used by the selection committee is relevance of the student's dissertation research to children's programming, computer-based construction environments for learning, or end-user programming. In addition, the student must have done sufficient work so that s\/he can talk from some experience. The selected students will be expected to submit a 5-page written paper describing their current work, to attend HCC'01, and to make a 10-minute presentation in the Odyssey. The students will benefit immensely from this unique opportunity to expose their ideas to and to interact with the leading researchers in this area; this nurturing effort is an inexpensive yet effective means of encouraging young and upcoming scientists.","title":"Children's Programming Odyssey","awardID":"0123712","effectiveDate":"2001-07-01","expirationDate":"2002-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6845","name":"HUMAN COMPUTER INTER PROGRAM"}}],"PIcoPI":["323733",168988],"PO":["564456"]},"59547":{"abstract":"This is the first year funding of a three year continuing award. A variety of on-line planning methods are used in artificial intelligence including, for example, real-time search methods such as LRTA*, reinforcement-learning methods such as Q-learning, and robot-navigation methods such as D*. The PIs intend to improve the performance of these and other on-line planning methods substantially so that, for example, future robot-navigation methods will be able to map unknown terrain significantly faster than is now possible, yet have the same advantageous properties as existing on-line planning methods. Many on-line planning methods, either always or most of the time, execute actions that move the agent in the perceived direction of the goal, that is, move the agent so that it reduces the estimates of the goal distances the most. However, the PIs preliminary theoretical results show that executing actions that move the agent in the perceived direction of the goal is usually not a good idea. For example, D* does not reach a goal location in unknown terrain with a minimal travel distance in the worst case. The key to improving the performance of these on-line planning methods then is to exploit the distance estimates that they maintain (or can maintain) in a way that is more directly related to the planning or learning objective. The PIs will study the properties of on-line planning methods both theoretically and experimentally, and will develop improved on-line planning methods that have the same interface as the existing methods, which allows users of these methods to easily substitute the new methods for the ones they are currently using. Side benefits of the proposed research include developing a test-bed for the experimental evaluation of robot navigation methods in unknown terrain, and creating a solid theoretical foundation for understanding robot-navigation methods in unknown terrain, including D*.","title":"Understanding and Improving On-Line Planning Methods","awardID":"0098807","effectiveDate":"2001-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6856","name":"ARTIFICIAL INTELL & COGNIT SCI"}}],"PIcoPI":["555889","555889","460643"],"PO":["491702"]},"60680":{"abstract":"This proposal was received in response to NSE, NSF-0019. Sensitivity at the level of individual quanta is required for such diverse applications of nanosensors as characterization of macromolecules and biological objects, monitoring of molecular binding, and control of dephasing processes in quantum dots. In nanoscale structures the phonon exchange is too fast, and strong thermal (phonon) coupling between the sensor and its surroundings puts strict limitations on the sensitivity of ordinary bolometric sensors. In the hot-electron sensor, the incoming quanta overheat only electron states, which relax to equilibrium due to electron-phonon coupling. The sensitivity of hot-electron sensors can be improved by weakening the effective coupling between electrons and phonons. In nanoconductors, the electron-phonon interaction is substantially modified in comparison with the interaction in bulk materials. Due to the interference between electron-phonon and electron-boundary scattering, the electron relaxation\/dephasing rate depends drastically on vibrations of boundaries. It may vary over a wide range, spanning several orders of magnitude, and may be controlled by selection of a substrate material. The proposed research includes complex investigations of the interference between electron scattering mechanisms in superconducting nanostructures and experimental demonstration of the electron energy relaxation controlled by elastic electron scattering from boundaries and defects as well as the design of a new hot-electron sensor with a record value of the noise equivalent power, NEP=10-20W\/Hz1\/2, and the energy resolution of 5 10-24J, which will be able to count individual low-energy quanta (photons or phonons).","title":"NER: Quantum Nanosensors Based on Controllable Electron-Phonon Coupling","awardID":"0103072","effectiveDate":"2001-07-01","expirationDate":"2003-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"1517","name":"ELECT, PHOTONICS, & MAG DEVICE"}}],"PIcoPI":["525365","445659"],"PO":["564900"]},"63056":{"abstract":"Verification tools based on decision procedures including OBDD based tools and model-checkers have been effectively used in many application areas including hardware verification, protocol analysis and verification, static analysis and type-checking of code, byte-code verification, analysis of mobile code and proof-carrying code. These tools are however unable to deal with computations modeled using large state space (including infinite state space), partly because they do not support inductive reasoning. Induction based theorem provers, while quite powerful, lack automation and require tremendous user guidance. A novel and radical approach is proposed to combine decision procedures, rewriting and induction schemes in a restricted way so as not to lose automation. Using this approach, recursive definitions are given as terminating rewrite rules on top of decidable theories, such as Presburger arithmetic. Induction schemes are generated from these terminating definitions. By imposing structure on recursive definitions, it becomes possible to automatically decide a large class of conjectures requiring inductive reasoning. It is proposed to extend and generalize this approach to consider a large class of recursively defined functions, their interactions with each other, as well as a large class of conjectures about these functions, that can be automatically decided (without any need for user guidance).","title":"ITR: Integrating Induction Schemes into Decision Procedures","awardID":"0113611","effectiveDate":"2001-07-15","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["531861"],"PO":["279077"]},"69623":{"abstract":"","title":"Learning and the Design of the Internet","awardID":"0196514","effectiveDate":"2001-07-01","expirationDate":"2003-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4095","name":"SPECIAL PROJECTS IN NET RESEAR"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"1320","name":"ECONOMICS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"1321","name":"DECISION RISK & MANAGEMENT SCI"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"6850","name":"DIGITAL SOCIETY&TECHNOLOGIES"}}],"PIcoPI":["560562","516972"],"PO":["565090"]},"59306":{"abstract":"Proposal #0098145<br\/>New York University<br\/>Michael Overton<br\/><br\/>Non-Lipschitz Optimization Problems involving Eigenvalues<br\/><br\/>Optimization problems involving eigenvalues arise in many applications. In recent years, attention has focused on semidefinite programs, which are linear optimization problems in the space of real symmetric matrices, with positive semidefinite constraints. This project focuses on optimization problems in the larger space of square matrices, not necessarily symmetric. <br\/><br\/>In the problems being studied, eigenvalues may appear in the optimization objective, in the constraints, or both. The dependence of eigenvalues as functions of a matrix is non-Lipschitz at points where the eigenvalue<br\/>multiplicity is greater than one. Hence, such optimization problems are non-Lipschitz. They arise in areas ranging from control theory (e.g., stability constraints) to Markov chains (e.g., optimizing convergence rates).<br\/><br\/>The goal is fourfold: analyze theoretical questions including necessary and sufficient conditions for optimality; build on these theoretical foundations to develop numerical algorithms that are able to find minimizers and verify that they satisfy optimality conditions; implement the algorithms in software that can be used by the general scientific community; and apply the results to the solution of important interesting problems that arise in practice.","title":"Non-Lipschitz Optimization Problems Involving Eigenvalues","awardID":"0098145","effectiveDate":"2001-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["549376"],"PO":["321058"]},"59317":{"abstract":"Proposal #0098170<br\/>U of Ill Urbana-Champaign<br\/>Michael Garland<br\/><br\/>Numerous graphics applications in areas ranging from CAD\/CAM to realistic immersive simulators rely on increasingly complex datasets to achieve convincing levels of visual realism. However, the enormity of the raw geometric data frequently makes it impossible to efficiently process such datasets given limited hardware capacity. Surface models containing millions of triangles are now commonplace, and advances in acquisition technology are making models containing several billion triangles available. Consequently, there has been considerable interest over the last decade in techniques for the automatic simplification of highly detailed polygonal models. However, current methods are, almost without exception, completely incapable of<br\/>processing input models of this enormous magnitude. This is a very serious shortcoming, as these are exactly the class of models for which effective simplification methods are most pressingly needed. The goal of this project is to develop new techniques for representing and processing very large scale polygonal surface models, enabling the efficient use of extremely complex models far beyond the capability of current systems.<br\/><br\/>Algorithmic scalability is essential in this domain. This research is focused on developing simplification methods which combine simple out-of-core data operations with more complex output-sensitive (i.e., dependent only on the output, rather than the input, size) processing phases. The general approach<br\/>of this project is to adopt recursive partitioning strategies directed by quadric error metrics. An approximation can be produced from any partition of the vertex set by merging all vertices within each cell of the partition. The use of quadric error metrics means that the aggressive simplification methods<br\/>designed for this project can be seamlessly coupled with other quadric-based simplification algorithms in a multi-phase process.","title":"Efficient Representation of Massive Geometric Models","awardID":"0098170","effectiveDate":"2001-07-01","expirationDate":"2004-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["289744"],"PO":["532791"]},"60043":{"abstract":"Recent technological advances in wireless networks and portable information appliances have engendered the new paradigm of mobile computing, enabling users carrying portable devices to access and update data regardless of their physical location or movement behavior. However, the use of this technology will remain limited unless we develop the necessary concepts, theory and infrastructure that can seamlessly integrate mobility and disconnection into everyday networked computing. Extensive research is needed in order for mobile computing to become pervasive. <br\/>Current research efforts do not support sufficient automation and management of mobile data access and update. Assumptions are usually made regarding the application, the source of mobile data, or the particularities of the mobile device. Ubiquitous data access is lacking due to the tight coupling between the mobile device and the mobile data. Users are therefore not allowed to switch mobile devices without spending a lot of effort on copying and re-hoarding. Additionally, current solutions do not support mobile access and hoarding from heterogeneous sources of data such as file systems, database servers, web servers, etc. Current research is also limited to basic synchronization schemes that do not take into consideration the variable and individual needs of consistency and up-to-dateness of mobile data. Finally, current mobile transaction models are not efficient in terms of successful commit rate, especially under prolonged periods of disconnection. <br\/>This proposal is based on ongoing research on mobile computing, operating systems and data warehousing. The overall goal is to make mobile computing available to a broader range of users and applications by automating the hoarding of a wide variety of data from multiple heterogeneous sources into mobile devices, and to facilitate sophisticated synchronization between the mobile devices and the fixed networks where the sources reside. The specific goals are:<br\/>Develop and evaluate a three-tiered architecture based on the Coda file system that provides independence between the mobile data and the mobile devices via a data warehouse for storing the user's working set. <br\/>Develop algorithms for automatically and incrementally hoarding data on mobile devices and for synchronizing the contents of the mobile device with the working set in the warehouse and the original sources.<br\/>Develop new synchronization techniques for maintaining the user's working set, based on programmable and conditional consistency specifications of mobile data items. Different data items may have different consistency requirements, and must therefore be synchronized differently.<br\/>Develop a new model of mobile transactions that can guarantee that transactional updates performed during disconnection are highly likely to be committed upon reconnection.<br\/>Our goal is to build upon existing results to develop the architecture and algorithms to make smart hoarding and synchronization in mobile environments a reality. We see the realization of such a framework as an important and necessary step towards making mobile computing a viable practice for a broad audience; a step that some day may lead us to the realization of ubiquitous computing. In our vision, users should be allowed to switch to any mobile device to connect to the fixed network and carry the necessary data with them, without having to worry about hoarding, synchronization, and other low-level burdens. Given our prior experience and research results in the areas of mobile computing, distributed systems, and data warehousing, we believe that we can successfully apply data warehousing technology to manage important user data as well as support the incremental maintenance of this data in light of frequent updates.","title":"Adaptive Synchronization Framework Supporting Device-independent Mobile Computing","awardID":"0100770","effectiveDate":"2001-07-01","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2876","name":"DISTRIBUTED SYSTEMS"}}],"PIcoPI":["561046","337768"],"PO":["309350"]},"61297":{"abstract":"The research focuses on techniques, algorithms, and methodologies for the analysis and transformation of monolithic programs, which use operations on entire arrays. High-level monolithic analysis drives the mechanical optimization and efficient scalarization of such programs. Whereas optimization of monolithic code has previously focused primarily on expressions, this project investigates optimization over larger units of program granularity.<br\/><br\/>The optimum elimination of unnecessary array partial results, with a particular focus on partial results assigned to a program variable may studied. A given array value assigned to a program variable may contain a permutation of the elements in some other array variable, and hence a compiler may be able to avoid materializing the given value. In contrast to minimizing materializations, there are situations where compiler introduced materializations, such as data rearrangement, or partial materializations, can significantly improve the efficiency of memory access at various levels of the memory hierarchy. Optimization techniques are studied both for avoiding materializations and for utilizing compiler introduced materializations.<br\/><br\/>An intrinsic aspect of compiling monolithic code is scalarization. The use of monolithic analysis to obtain information that guides or drives scalarization, hopefully directly yielding optimized scalarized code.","title":"On the Analysis, Optimization, and Efficient Scalarization of Monolithic-Level Array Programs","awardID":"0105536","effectiveDate":"2001-07-01","expirationDate":"2003-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2876","name":"DISTRIBUTED SYSTEMS"}}],"PIcoPI":[157621,"399214",157623],"PO":["551992"]},"57700":{"abstract":"A number of new applications have recently emerged that require the storage,<br\/>maintenance, and analysis of massive amounts of data. Examples such as web<br\/>search engines, data warehouses, and large scientific data repositories <br\/>often involve multiple terabytes of data that are simultaneously searched and <br\/>explored by many users. This research project investigates fundamental <br\/>algorithmic problems arising in the context of such large data sets, studies <br\/>the complexity of these problems, develops new techniques for their efficient <br\/>solution, and experimentally validates proposed techniques in the appropriate <br\/>system and application context. The main focus is on problems arising in <br\/>databases and in searching and analyzing the World-Wide Web.<br\/><br\/>More precisely, the research focuses on problems concerning the storage, <br\/>maintenance, partitioning, indexing, and approximate representation of very <br\/>large data sets, and the efficient exploration, analysis and precise and <br\/>approximate querying of such data. The types of problems that are studied can<br\/>be grouped into two categories, one consisting of problems motivated mainly <br\/>by applications in the database area, and one motivated by applications in <br\/>web search and analysis. In the first category, the project studies <br\/>multi-dimensional data partitioning problems arising in selectivity estimation<br\/>and indexing, feedback-based approaches to selectivity estimation, association<br\/>rule mining problems, and the approximate and precise evaluation of complex<br\/>queries on large data sets. The problems studied in the second category are<br\/>concerned with online search on the web, the study of random graph models for<br\/>the web, and efficient computing with large web graphs and hypertext <br\/>collections.","title":"CAREER: Algorithmic Techniques for Massive Data Sets","awardID":"0093400","effectiveDate":"2001-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}}],"PIcoPI":["521989"],"PO":["499399"]},"51199":{"abstract":"Abstract:<br\/>The main objective of the proposed research is to develop novel<br\/>Software-Implemented Fault Tolerance (SIFT) techniques for increasing<br\/>the dependability of the distributed and networked environments based on<br\/>COTS.<br\/>The issues of dependability will be addressed by the design and<br\/>implementation of (SIFT), dynamic reconfiguration in distributed systems<br\/>and high-speed computer networks. Novel fast dynamic reconfiguration<br\/>techniques for various irregular network topologies will be developed and<br\/>analyzed. An analytical method is proposed for evaluating the reliability<br\/>improvement by using SIFT for any size of distributed systems .<br\/>The goal of this research is to identify and develop new key building<br\/>blocks<br\/>for reliable distributed systems built with inexpensive off-the shelf<br\/>components. The platform consists of computing nodes connected via<br\/>multiple interfaces to networks configured in fault-tolerant topologies.<br\/>Thus, the research community and a large number of users will have the<br\/>opportunity to build reliable distributed systems based on COTS in a user<br\/>transparent way.The goal will be to achieve a high reconfiguration rate<br\/>and a small recomputation overhead in the presence of faults in the<br\/>distributed<br\/>systems and high-speed computer networks in a way transparent to the user,<br\/>which will provide a high dependability.<br\/>This research project will have a positive impact on a large community<br\/>implementing applications in distributed and network environments such as<br\/>: e-commerce, telepresence, telemedicine, Internet data centers,<br\/>distributed applications running concurrently across","title":"Distributed Systems with Sift, Based on COTS","awardID":"0004515","effectiveDate":"2001-07-15","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2876","name":"DISTRIBUTED SYSTEMS"}}],"PIcoPI":[130670],"PO":["309350"]},"57326":{"abstract":"Game theory provides the basis for rational behavior in situations involving multiple agents. It provides the means for modeling the objectives of agents and the information available to them, and specifies notions of optimal strategic behavior. However, the practical use of game theory has hitherto been limited by the lack of efficient algorithms for computing optimal strategies, and of software tools for game analysis. In this project, several approaches are explored to develop effective game-theoretic solution algorithms. The approaches are unified in their use of structured representations. Similarities between game states are exploited by abstraction-based approximation methods; regularities in the creation and revelation of information are exploited by information-based decomposition methods; and the decomposition of a complex situation using a set of weakly interacting variables is exploited by factored representations. <br\/><br\/>In addition to the new algorithms, software tools are developed for the representation, solution and analysis of games. An innovative course on computational game theory is also developed. The impact of these methods, tools and educational materials will be to transform game theory from a theoretic tool for analyzing prototypical situations to a practical tool for designing strategic agents for real-world problems.","title":"CAREER: Efficient Algorithms for Imperfect Information Games","awardID":"0091815","effectiveDate":"2001-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6850","name":"DIGITAL SOCIETY&TECHNOLOGIES"}}],"PIcoPI":["269188"],"PO":["564456"]},"61496":{"abstract":"DMS-0106804<br\/>Gunnar Carlsson<br\/><br\/>This award provides partial support for participants at a <br\/>conference on Algebraic Topological Methods in Computer Science <br\/>to be held at Stanford University, July 30 through August 3, 2001. There has been increasing interest in the potential applications of algebraic topology in recent years. Topology has now become a <br\/>valuable tool in computational geometry, as well as a method for studying various problems in algorithms and combinatorics. This conference will cover several different themes, including topology <br\/>in computational geometry, topology and algorithms, topology in combinatorics, and topology as an aid to visualization. The speakers include many of the leaders in this area, including both mathematicians with an interesting in computational questions as well as computer scientists who are applying algebraic topology in various ways. We expect substantial participation from researchers in Silicon Valley <br\/>who ae working in this area. Further information, including invited speakers, schedule of talks, and housing information, is available at http:\/\/ math.stanford.edu <br\/>and <br\/>http:\/\/www.math.uwo.ca\/~jardine\/at-cs.html","title":"Algebraic Topological Methods in Computer Science","awardID":"0106804","effectiveDate":"2001-07-01","expirationDate":"2002-06-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1267","name":"TOPOLOGY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["521231"],"PO":["218970"]},"61298":{"abstract":"This research takes a significant step towards enabling and evaluating the application of compiler<br\/>optimizations for uniprocessor performance within explicitly parallel programs, with a flexible view<br\/>of memory consistency. Two important unanswered questions are targeted: <br\/>(1) What kind of performance can be gained with the Location Consistency (LC) memory model in comparison<br\/>to the sequential consistency (SC)-derived models for shared memory parallel programs,<br\/>amidst the new developments of compiler analysis and optimization for SC-derived models?<br\/>(2) As a compromise between the two divergent approaches, can both the SC-derived models and the LC model be supported within the same program, by developing a programmer-controlled memory consistency strategy<br\/>supported by compiler technology?<br\/>The results<br\/>will include:<br\/>(1) specification and implemenation of a programming model that assumes an end-to-end view of the memory system based on the LC model, and a study of its programmability,<br\/>(2) compilation analyses to uphold programmer-controlled memory consistency so programmers can choose between the memory models in different parts of the same application,<br\/>(3) development and refinement of cache consistency protocols based on the LC model in a software caching context,<br\/>(4) experimental evaluation of compiler optimization and program performance under<br\/>different memory consistency models and cache protocols.","title":"Increasing Parallel Program Performance with the LC Memory Consistency Model","awardID":"0105540","effectiveDate":"2001-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2876","name":"DISTRIBUTED SYSTEMS"}}],"PIcoPI":["527984","475422"],"PO":["565272"]},"65225":{"abstract":"A workshop will be conducted by Florida International University (FIU) to discuss the current and future needs and activities of researchers needing a high performance network for their research projects associated with activities and projects in South and Central America, Mexico and the Caribbean. These projects will focus on the interests and benefits to US science. The participants in the works hop will be scientist from the United States and scientists from the region countries. The expected output from the workshop will be :<br\/><br\/> A report of the factual situation on the present and foreseeable future needs of scientific research projects in the region<br\/>A statement of recommendation for action concerning these needs.","title":"FIU AMPATH Workshop to Identify Areas of Scientific Collaboration between the US and the AMPATH Service Area; August 2001","awardID":"0123388","effectiveDate":"2001-07-01","expirationDate":"2001-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4090","name":"ADVANCED NET INFRA & RSCH"}}],"PIcoPI":["564024","564025"],"PO":[168746]},"57624":{"abstract":"This research focuses on enhancing the state of scientific knowledge<br\/>regarding cryptography and security in electronic commerce. The<br\/>investigators study new models and abstractions, and find novel<br\/>e-commerce applications for a wide range of cryptographic techniques.<br\/>The research is focused on four main areas: (1) Secure multi-party<br\/>protocols -- How can mutually mistrustful agents cooperate? (2)<br\/>Anonymity and privacy mechanisms -- How can the protection of<br\/>sensitive information be guaranteed by design? (3) Fraud detection and<br\/>prevention -- How can illicit profit be systematically reduced? (4)<br\/>Secure content distribution -- How can virtual classrooms, concerts<br\/>and bookstores be streamlined? Deeper connections are sought by<br\/>combining ideas from these areas.<br\/><br\/>Within the area of secure multi-party protocols, the researchers are<br\/>exploring efficient special-purpose protocols for e-commerce functions<br\/>of practical importance, to improve on the powerful but inefficient<br\/>completeness theorems of Goldreich et al. (1987), Ben-Or et<br\/>al. (1988), and others. Anonymity and privacy mechanisms under<br\/>consideration include new methods for deniable payment mechanisms.<br\/>For fraud detection and prevention, the investigators examine<br\/>adversary models such as malicious-but-rational and<br\/>malicious-but-uncoordinated faults, and new uses of lightweight cost<br\/>functions. The research in secure content distribution includes new<br\/>traitor tracing schemes, and new applications of zero knowledge<br\/>proofs.","title":"New Directions in Cryptography for Electronic Commerce","awardID":"0093140","effectiveDate":"2001-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["382002"],"PO":["348215"]},"59989":{"abstract":"This project focuses on some of the unexpectedly fruitful connections between proof theory and important open questions in computational complexity. In mathematical logic, Buss investigates proof theory and proof complexity, especially weak proof systems with close connections to open problems in computational complexity. He investigates aspects of theoretical computer science related to open questions such as the \"P versus NP\" problem and related problems in complexity including open problems in the mathematical foundations of cryptography. Buss studies the proof complexity of propositional systems such as Frege systems, cutting planes systems, Nullstellensatz proof systems, counting axioms, the polynomial calculus, and intuitionistic proof systems. He plans to extend previous work on bounded arithmetic and its relationships with proof complexity, computational complexity and cryptographic conjectures. The goals of this research are firstly to give bounds on proof size and on proof search algorithms, and to determine what kinds of computational content can be extracted from formal proofs; and secondly to investigate open problems in computational complexity from the viewpoint of mathematical logic.<br\/><br\/>The work of this project is motivated by the desire to obtain a better understanding of open problems in computational complexity. These open problems include the \"P versus NP\" problem regarding the difficulty of solving a large range of combinatorial problems including scheduling and optimization; they also include establishing the possibility of mathematically secure cryptographic systems. It is commonly believed that many of the computational problems in NP and in cryptography are intractible, and it important for many applications that they be intractible. However, mathematical proofs of intractibility have not been obtained yet, in spite of extensive efforts. Buss works on aspect of these problems in the setting of mathematical logic and proof theory. His work addresses the logical and computational complexity of formal, symbolic proofs; this includes the analysis of proofs in a variety of proof systems corresponding to feasible computation, and the possibility of extracting computational information from proofs. <br\/><br\/>This project will be supported by the Foundations program of the Division of Mathematical Sciences and the Theory of Computing program of the Division of Computer and Communications Research.","title":"Proof Theory and Computational Complexity","awardID":"0100589","effectiveDate":"2001-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1268","name":"FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["515840"],"PO":["565063"]},"57569":{"abstract":"Animation is a tremendously powerful medium of expression.<br\/>Ideas that would be complex or even impossible to express<br\/>with still images or words can easily be conveyed using<br\/>animated sequences. Unfortunately, even when the concept to<br\/>be conveyed via animation is simple, the process of<br\/>producing the animation is inordinately difficult. Indeed,<br\/>this is one of the reasons computer animation has become so<br\/>dominant - it holds the promise of automating, and thus<br\/>significantly simplifying, the production process. Sadly,<br\/>the process of transferring ideas into computer animations<br\/>is still far from simple. For example, the animated feature<br\/>film Toy Story 2 required extremely hard work by as many as<br\/>200 highly skilled computer and animation experts for more<br\/>than three years.<br\/><br\/>My career goal is to develop the concepts and algorithms<br\/>that will enable effortless, rapid development of realistic<br\/>animations that effectively convey the creator's purpose.<br\/>This research hopes to create the theoretical and<br\/>experimental foundation for building much more powerful<br\/>animation tools than exist today. Such tools will sharply<br\/>reduce the production time needed by skilled animators and,<br\/>more importantly, enable average computer users to express<br\/>their ideas through animation. Such tools will also help<br\/>educators teach more effectively and allow each one of us to<br\/>become creators and directors of personal and fictional<br\/>stories. Only when animation production is easier will<br\/>animation be as commonplace on the web as images and text<br\/>are today. Moreover, intuitive methods for creating<br\/>realistic human motion will further enable the creation of<br\/>human avatars in tele-presence applications, new<br\/>anthropomorphic human-computer interfaces, and realistic<br\/>digital actors in feature films and video games. Computer<br\/>animation will not achieve its full potential until 1)<br\/>animation tools require little or no skill, 2) an animator's<br\/>creativity is not stifled by the limitations of the<br\/>animation tools, and 3) the animation systems are available<br\/>to every person with a computer and a story to tell. I hope<br\/>to significantly contribute towards these goals.<br\/><br\/>At the heart of this research is the observation that the<br\/>animator's creativity will not be fully realized until it<br\/>becomes easier to produce realistic motion sequences. This<br\/>problem is tremendously challenging because the underlying<br\/>physical models of motion are difficult to create and<br\/>control. I believe that the solution lies in the creation of<br\/>highly-flexible realistic motion libraries, together with<br\/>tools to modify them intuitively and extensively. The power<br\/>of libraries stems from the ability to reuse already<br\/>existing high-quality motion, reducing the need for<br\/>animation skills. In this paradigm, the process of animation<br\/>turns into selecting the specific motion library, and<br\/>modifying a set of motion properties that transform the<br\/>original motion into a final animation.<br\/><br\/>In my research, I propose to develop a methodology for<br\/>creating and using realistic motion libraries flexible<br\/>enough to be used in a wide range of applications. In 1995,<br\/>I proposed the use of motion transformation as new way to<br\/>create animations - a method fundamentally different from<br\/>the traditional way of creating animations from scratch.<br\/>This transformation approach is particularly useful for<br\/>modifying realistic motion data captured from real-world<br\/>actors. Unfortunately, during the transformation process,<br\/>much of the realism tends to be lost. Recently, I published<br\/>a novel transformation approach which demonstrates that<br\/>animations can be intuitively transformed into a wide range<br\/>of new sequences without violating the fundamental dynamic<br\/>properties of motion. The realism is preserved by<br\/>maintaining a model of the dynamic and biomechanic<br\/>properties of the animated character.<br\/><br\/>In the future, I will further develop mathematical models<br\/>that can be combined with real-world data to create reusable<br\/>motion libraries. Aside from preserving the realism of<br\/>motion, the most important requirement for effective motion<br\/>libraries is the flexibility with which the libraries can be<br\/>adjusted to meet the needs of an animator. I plan to achieve<br\/>this flexibility by decomposing a character's motion into a<br\/>fundamental component and a style component in a way<br\/>that allows us to independently transfer these components to<br\/>new characters. For example, the animator can produce a<br\/>child's cheerful run sequence by starting with a running<br\/>motion library extracted from the captured human run. This<br\/>motion can be modified by applying a happy, exuberant motion<br\/>style, which is then transferred to an animated character of<br\/>a small child. Such decoup","title":"CAREER: Reusable, Realistic Motion Libraries for Computer Animation","awardID":"0092970","effectiveDate":"2001-07-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["549589"],"PO":["532791"]},"57217":{"abstract":"EIA-0091489<br\/>Nancy Wiegand<br\/>University of Wisconsin<br\/><br\/>Digital Government: Integrating Metadata Development XML, and DBMS Search and Query Techniques in a State of Wisconsin Land Information System<br\/><br\/>This project will involve several state, local and national agencies in developing a query able database of heterogeneous spatial and textual data. The data will continue to be held in various local and other databases, but will appear to be integrated though the system to be developed. Automated development of Metada and semantic conversions will be required. Three technical thrusts are i) to locate and search data sources, and to support user manipulation of the data. XML will be an important technical tool in the project in expressing Metadata.","title":"Digital Government: Integrating Metadata Development, XML, and DBMS Search and Query Techniques in a State of Wisconsin Land Information System","awardID":"0091489","effectiveDate":"2001-07-15","expirationDate":"2004-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1706","name":"DIGITAL GOVERNMENT"}}],"PIcoPI":[147205,"554456","433344"],"PO":["371077"]},"62971":{"abstract":"The LinBox group of twelve researchers in three countries (USA, France, Canada) proposes research in the design of efficient algorithms for linear algebra, in their implementation in a software library, and in how to interface the library to widely-used scientific computing software. Algorithms will be implemented, and new algorithms designed, for the black box representation of matrices---hence the name LinBox---over entry domains that are either symbolic, that is, exact, or floating point, that is, inexact. The library is generically programmed as C++ template classes with abstract underlying arithmetics; they can be compiled with a variety of fast libraries for the basic field, floating point, and polynomial operations. A server\/client interface seamlessly attaches the library to the common general purpose symbolic systems Maple and Mathematica and to the numeric system MatLab. Parallel execution of the implemented algorithms is facilitated. <br\/><br\/>Black box matrices are stored as functions (as linear operators in effect): the matrix is a procedure that takes an arbitrary vector as input and efficiently computes the matrix-times-vector product. Black box linear algebra generalizes sparsity. The LinBox library will contain algorithms for solving singular and non-singular systems of linear equations whose coefficient matrix is given in black box representation. Furthermore, it is proposed to develop fast methods for the rank and the minimal and characteristic polynomial of a black box matrix. Finally, LinBox will contain methods for linear Diophantine problems with black box matrices, such as computing an integral solution to a linear system with integer entries and computing the Smith normal form of an integer matrix.","title":"ITR\/ACS: Collaborative Research LinBox: A Generic Library for Seminumeric Black Box Linear Algebra","awardID":"0113121","effectiveDate":"2001-07-15","expirationDate":"2004-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["221311","485433"],"PO":["562362"]},"61112":{"abstract":"Proposal # 0104800<br\/>PI: Eve Riskin<br\/>U of Washington<br\/><br\/>In the not-too-distant future, downloading images and video to a handheld device will be as commonplace as viewing an image on a Web page is today. To enable this, high quality, flexible, and robust image and video compression algorithms will be required. Recently, Group Testing for Wavelets (GTW), a new type of wavelet coder based on group testing was developed. It offers competitive performance to the best compression coders available today and with further development, GTW could outperform them significantly.<br\/><br\/>This research involves developing new algorithms for robust and flexible coding of images and video. Whereas<br\/>GTW is used as a motivating example, the research is applicable to many compression algorithms and in many scenarios. First, a set of problems related to GTW will be explored. This includes applying GTW to wavelet packet decompositions and reduced complexity transforms; extending it to video and the new area of progressive geometry compression; and developing a theory for its strong performance.<br\/>Next, methods for applying forward error correction to compressed data in a progressive manner will be investigated. Finally, unequal frame expansions will be used to recover from packet loss. The research will have practical use in many arenas including the Internet and wireless communications and will be included<br\/>in a new undergraduate course on data compression, to be taught jointly by the two principal investigators.","title":"Flexible and Robust Coding of Images and Video","awardID":"0104800","effectiveDate":"2001-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["549399","485574"],"PO":["564898"]},"62355":{"abstract":"0110496<br\/>Sergio Antoy<br\/>Portland State University<br\/><br\/>ITR\/SY: Non-Deterministic Computations for Functional Logic Programs<br\/><br\/>Abstract:<br\/><br\/> Narrowing allows the seamless integration of functional and logic<br\/> computations. A narrowing strategy selects from an expression the<br\/> subexpression(s) to evaluate and instantiates variables if<br\/> necessary. Different selection strategies extend from functional<br\/> to functional logic programming computational behaviors such as<br\/> call-by-value and call-by-need. Sound, complete, and optimal (to<br\/> varying degrees) strategies are known for both Haskell-like<br\/> programs and programs that allow some forms of parallelism.<br\/><br\/> Unfortunately, these classes of programs do not support<br\/> non-deterministic computations. The lack of non-determinism is a<br\/> severe limitation in functional logic languages. It prevents the<br\/> use of familiar logic programming idioms and, in some cases, leads<br\/> to programs that violate the inherent laziness of a problem. The<br\/> research proposes a new computational framework, a class of<br\/> programs, and a strategy for narrowing computations in this class<br\/> that supports non-determinism without loss of soundness,<br\/> completeness or efficiency. Within this framework, programs become<br\/> textually shorter, conceptually simpler, more modular, easier to<br\/> understand and maintain, and arguably more efficient.<br\/><br\/> The proposed strategy has the potential to encompass strategies<br\/> for other interesting classes of functional logic programs and it<br\/> is expected to unify various concurrent disjoint efforts aiming at<br\/> integrating different narrowing strategies within a single<br\/> language.","title":"ITR\/SY: Non-Deterministic Computations for Functional Logic Programs","awardID":"0110496","effectiveDate":"2001-07-15","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["549403"],"PO":["564388"]},"61288":{"abstract":"Proposal #0105499<br\/>U of Rhode Island<br\/>PI: Kumaresan, Ramdas<br\/><br\/>Many natural and man-made sounds have a time-varying, broad-band spectrum. Although a number of computer processing methods have been developed to analyze such signals, the auditory systems of humans and animals seem to process such signals and make inferences on them much more successfully. What kind of signal analysis underlies this phenomenon? In this research auditory scientists and signal processing engineers collaborate to attempt to answer this question, and, in the process advance the state of the art in computer-based signal analysis. The potential applications of this research include improved feature extraction for sound\/speech recognition\/classification, separation of overlapping sounds\/speech, sound source localization and better understanding of the auditory system. <br\/><br\/>The auditory system, in addition to performing some form of spectral analysis, also extracts temporal modulation information from the acoustic signals. This information is encoded in the timing patterns of the spike trains that originate in the inner ear. In this research, with the help of auditory scientists, the above observations are translated into mathematically tractable signal processing problems. Specifically, the following basic question is explored: how can band-pass signals be represented by timing information only, as opposed to traditional Nyquist-rate amplitude sampling (as in Shannon's sampling theorem)? Recent results by the investigators indicate that the information about the phase and envelope modulations of arbitrary band-pass signals can be represented by certain zero-crossings alone, if appropriate adaptive preprocessing(demodulation) is performed on the signal. Based on these ideas an analysis-synthesis procedures is being developed in which time-varying signals such as speech can be effectively represented by a small number of modulated components using only timing information. The robustness of such procedures will be examined and the analysis-synthesis procedures will be applied to a number of applications mentioned in the above paragraph.","title":"New directions in signal processing inspired by the auditory system","awardID":"0105499","effectiveDate":"2001-07-15","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["173322"],"PO":["564898"]},"80450":{"abstract":"","title":"Computer Science Approaches to Finance Problems: Computational Complexity and Efficient Algorithms","awardID":"0296040","effectiveDate":"2001-07-01","expirationDate":"2003-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["320878","517969"],"PO":["543507"]},"61300":{"abstract":"Abstract for GRAPH-THEORETIC APPROXIMATION ALGORITHMS (NSF #0105548)<br\/>PI: Ramamoorthi Ravi, Carnegie Mellon University.<br\/><br\/>The increasing size of communication and information networks has motivated<br\/>several new problems in the design of networks with low cost and high<br\/>resilience; Many of these problems are known to be prohibitively expensive<br\/>in terms of computing power to solve to full accuracy. Yet these problem<br\/>abstractions capture models from a variety of application areas such as<br\/>communication network routing, multicasting messages in large<br\/>networks, VLSI layout, and transportation networks with economies of scale.<br\/>The research in this proposal aims to advance our fundamental knowledge of<br\/>the structure of good solutions to such inherently intractable computational<br\/>problems involving networks. The investigators will develop approximation<br\/>algorithms for these problems -- these are heuristic methods that trade off<br\/>some accuracy in the solution in return for lowered computational resources,<br\/>in a quantifiable way. The research will involve the application of and new<br\/>discovery of results in the theory of graphs to guide the design of these<br\/>heuristic solutions.<br\/><br\/>In particular, the investigators will design polynomial-time approximation<br\/>algorithms with improved performance ratios for many basic graph-theoretic<br\/>problems including the problem of augmenting a tree to make it<br\/>two-connected, buy-at-bulk network design problems and the minimum $k$-cut<br\/>problem. The investigators will continue their ongoing study of bicriteria<br\/>network design problems (problems that involve two objective functions to be<br\/>optimized simultaneously) to design improved bicriteria approximation<br\/>algorithms for many spanning tree problems arising in practice; These<br\/>problems involve a combination of commonly studied objectives such as the<br\/>maximum node degree, diameter and total cost of the tree. The research<br\/>effort will focus on the theme of studying natural mathematical programming<br\/>formulations for these NP-hard problems and attempt to derive improved<br\/>approximation guarantees via rounding algorithms that will also establish<br\/>the integrality gap of these basic formulations.","title":"Graph-theoretic Approximation Algorithms","awardID":"0105548","effectiveDate":"2001-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["561985"],"PO":["279077"]},"59518":{"abstract":"This research involves the study algorithmic problems that arise in<br\/>the design of next generation networking technology.<br\/>Rapid growth of the internet user base, coupled with rapid parallel<br\/>growth in mobile subscriber numbers, is creating powerful pent-up demand<br\/>for wireless access to the internet\/intranets and other data networks.<br\/>Communication in wireless networks differs in some fundamental <br\/>ways from communication in wired networks. In particular, the basic<br\/>form of communication in wireless networks is broadcasting,<br\/>as opposed to point-to-point communications in most wired<br\/>networks. This research investigates the effect of switching to <br\/>broadcast communication on server strategies<br\/>in the client-server computing model, one of the most common <br\/>computing paradigms.<br\/><br\/><br\/><br\/>A wide range of real-time applications, such as multimedia conferencing, <br\/>computer supported cooperated workspaces, remote medical diagnosis etc., <br\/>has become very popular in recent years. These applications have <br\/>dynamically changing bandwidth requirements. <br\/>It is clear that controlling and allocating<br\/>bandwidth for individual users and web browsers is becoming very <br\/>important for bursty traffic flows utilizing highly constrained<br\/>communications channels.<br\/>From the network management's perspective, it is advantageous to <br\/>allow the network to dynamically <br\/>change the tariff parameters of the charging scheme in response to demand. <br\/>This research investigates dynamic renegotiation and<br\/>pricing problems that arise in the design of networks<br\/>that give quality of service guarantees.","title":"Collaborative Research: Algorithmic Problems in Next Generation Networks","awardID":"0098752","effectiveDate":"2001-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["534213"],"PO":["279077"]},"61323":{"abstract":"Error correcting codes were originally invented to increase the<br\/>precision of retrieval, storage and communication of data.<br\/>The investigator's goal is to study and develop codes<br\/>with new features added on top of the error correcting property,<br\/>such as checkability, identifiable parent property (IPP), or<br\/>quantum-error correcting property. In spite of the<br\/>importance of designing codes with added features,<br\/>there has not ben a comprehensive study of them up to date, and each<br\/>design is treated as an isolated case. The investigator<br\/>wants to put the designs under one umbrella (whenever<br\/>it is possible) so that their properties<br\/>can be compared and further improved upon.<br\/>Among the benefits such improvement can bring about<br\/>are better non-approximability results in complexity theory,<br\/>better means to trace pirates of electronic images,<br\/>and new algorithms for quantum computers.<br\/><br\/>The investigator was the first to<br\/>observe that constructs in the theory of probabilistically checkable<br\/>proofs can be viewed as error correcting codes with added features.<br\/>The best parameters for these codes and their kins are<br\/>still not known, and they are the primary<br\/>targets of the proposed investigation in the first year.<br\/>The investigator also plans for a study that<br\/>would lead to a general checkability theorem for product codes,<br\/>with a surprise application in circuit complexity,<br\/>and recommends a simplified look at Raz's parallel repetition<br\/>theorem. A different thread of the the proposed research<br\/>is motivated by questions about schemes<br\/>that protect a multi-media publisher against piracy of<br\/>electronic images. The publisher can use words of a code<br\/>to mark electronic copies of an image. Then, if the code is<br\/>appropriately designed, any image forged by combining pieces of two legally<br\/>traded images, contains sufficient amount of information to trace<br\/>the identity of at least one of the source (parent) images.<br\/>Here the problem is to build efficient codes that can identify one of<br\/>three or more source images. Other research targets include<br\/>efficiently decodable quantum codes, and<br\/>complexity lower bounds for dynamic problems via unusual codes.<br\/>There is a general framework in which<br\/>the investigator plans to do the research. This entails:<br\/>1. The study and exploration of the relation in<br\/>between different unusual codes;<br\/>2. Their classification and axiomatization;<br\/>3. Search for novel applications of unusual codes<br\/>in the theories of lower bounds and pseudo-randomness,<br\/>and in other walks of computer science;<br\/>4. Building a 'family tree' of code properties, and finding a match<br\/>in between properties and applications.<br\/>The methodology should embrace and extend classical coding theory.","title":"Unusual Error Correcting Codes","awardID":"0105692","effectiveDate":"2001-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["531145"],"PO":["499399"]},"56901":{"abstract":"Large scale simulations in computational engineering and science often spend a great deal of their time in a few computational methods kernels, such as dense or sparse matrix-vector products, relaxation on a structured or unstructured mesh, or the computation of forces between pairs of attracting or repelling particles. There has been a great deal of work in generating high performance libraries for these applications, including dense and sparse linear algebra, multigrid methods, and n-body techniques.<br\/><br\/>One idea established in these application-level libraries is to organize the computations around a set of numerical kernels, with the assumption that these kernels will be highly optimized on each of the hardware platforms of interest. The best known example of this approach is the BLAS (the Basic Linear Algebra routines), which are used in building LAPACK, ScaLAPACK, and other libraries; the BLAS are implemented by hardware vendors and are highly tuned to the memory hierarchy of each machine.<br\/><br\/>However, this approach is limited by the growing number of kernels, the large number of machines, the increasing depth of memory hierarchies and complexity of processors, and by the difficulty of performance tuning each kernel on each machine. The great majority of these kernels are susceptible to large speedups when machine-specific tuning is performed. However, the hand tuning takes weeks or months of a skilled engineer's time, and this work must be repeated for each micro-architecture, or operating system change. <br\/><br\/>This research will work to automate the process of architecture-dependent tuning of numerical kernels, replacing the current hand-tuning process with a semi-automated search procedure. Prototypes of this approach exist for dense matrix-multiplication (Atlas and PHiPAC), FFTs (FFTW), and sparse matrix-vector multiplication (Sparsity). These results show that we can frequently do as well as or even better than hand-tuned vendor code on the kernels attempted. These systems use a hand-written \"search directed code generator (SDCG)\" to produce many different implementations of a single kernel, which are all run on each architecture, with the fastest one being selected. This approach will be extended to a much wider range of numerical kernels by combing compiler technology with algorithm-specific transformation rules to automate the production of these SDCGs.<br\/><br\/>Ultimately, the technology is expected to be useful in conventional compilers, provided that appropriate abstract data types or annotations are used to side-step very difficult or \"impossible\" dependency-analysis needed to justify the desired code transformations. This work should also stimulate research into new high level numerical methods and architectures, both of which are limited by the lack of highly tuned kernels.","title":"Automatic Performance Tuning of Numerical Kernels","awardID":"0090127","effectiveDate":"2001-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}}],"PIcoPI":["558475","436421"],"PO":["565272"]},"61257":{"abstract":"Title: \"Massive Data Streams: Algorithms and Complexity\"<br\/><br\/>Investigators: Joan Feigenbaum and Sampath Kannan<br\/><br\/>Abstract:<br\/> Massive data sets are increasingly important in many applications,<br\/>including observational sciences, product marketing, and monitoring and<br\/>operations of large systems. In network operations, raw data typically arrive<br\/>in streams, and decisions must be made by algorithms that make one pass<br\/>over each stream, throw much of the raw data away, and produce ``synopses''<br\/>or ``sketches'' for further processing. Moreover, network-generated massive<br\/>data sets are often distributed: Several different, physically separated<br\/>network elements may receive or generate data streams that, together, comprise<br\/>one logical data set. The enormous scale, distributed nature, and one-pass <br\/>processing requirement on the data sets of interest must be addressed with <br\/>new algorithmic techniques.<br\/> Two programming paradigms for massive data sets are \"sampling\" and<br\/>\"streaming.\" Rather than take time even to read a massive data<br\/>set, a sampling algorithm extracts a small random sample and computes<br\/>on it. By contrast, a streaming algorithm takes time to read all the input, <br\/>but little more time and little total space. Input to a streaming algorithm <br\/>is a sequence of items; the streaming algorithm is given the items in order, <br\/>lacks space to record more than a small amount of the input, and is required<br\/>to perform its per-item processing quickly in order to keep up with<br\/>the unbuffered input. The investigators continue the study of <br\/>fundamental algorithms for massive data streams. Specific problems of<br\/>interest include but are not limited to the complexity of proving properties <br\/>of data streams, the construction of one-pass testers of properties of <br\/>massive graphs, and the streaming space complexity of clustering.","title":"Massive Data Streams: Algorithms and Complexity","awardID":"0105337","effectiveDate":"2001-07-15","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["450982","382346"],"PO":["543507"]},"58958":{"abstract":"The basic purpose of this research is to automate and facilitate the use of rigorous logic. Rigorous reasoning plays (or should play) an important role in a wide variety of intellectual endeavors, and automated reasoning tools have many important potential applications. Procedures for proving theorems will be crucial components of automated reasoning tools, since these procedures can be used as inference mechanisms. The focus of this research is on proving theorems of a formulation of higher-order logic known as type theory (more specifically, the typed lambda-calculus). This formal language includes first-order logic, but in a practical sense it has greater expressive power, and it is particularly well suited to the formalization of mathematics and other disciplines and to specifying and verifying hardware and software.<br\/><br\/>Part of this research involves continued development of an existing computerized theorem proving system called TPS, which can be used to construct and check formal proofs (in natural deduction style) interactively, semi-automatically, and automatically. In automatic mode, TPS first searches for an expansion proof, which expresses in a non-redundant way the fundamental logical structure of proofs of the theorem in a variety of styles, and then transforms this into a proof in natural deduction style. The interactive commands for applying rules of inference are available in a related program called ETPS (Educational Theorem Proving System), which is used interactively by students in logic courses to construct natural deduction proofs. The possibility of using TPS in a mixture of automatic and interactive modes makes it an attractive tool for working on complex logical problems in a variety of disciplines. More information about TPS can be found at http:\/\/gtps.math.cmu.edu\/tps.html. <br\/><br\/>The research involves methods of searching for expansion proofs, including methods of finding appropriate substitutions for set variables, methods of searching for matings of subformulas, and the interactions between these; representations, manipulations, presentations, and translations of proofs; enhancement of TPS as a useful logical tool; and related problems and questions.","title":"Automated Theorem Proving in Type Theory","awardID":"0097179","effectiveDate":"2001-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["167072"],"PO":["321058"]},"57638":{"abstract":"CCR-0093174<br\/>CAREER: Improving Scalability of Finite State Verifiers<br\/><br\/>Software quality assurance is a critically important and expensive task. Finite State verification (FSV) is a family of automated approaches for proving that a given software system does not have errors of a specified kind of exposing such errors if they exist. Unfortunately, computing resources needed by FSV techniques are often prohibitive for systems of realistic size, impeding acceptance of FSV in software practice.<br\/><br\/>This research is aimed at improving scalability of FSV techniques. It seeks to generalize the existing optimizations beyond the scope of the FSV techniques for which these optimizations were originally proposed. In addition, new optimization techniques are being developed. The research evaluates, both analytically and experimentally, the impact of each optimization on the scalability of the FSV techniques to which it can be applied.<br\/><br\/>Successful completion of this work will produce scalable and effective SFV techniques. In turn, use of these techniques in software practice can result in improvements in software quality and reduction in software development costs. In addition, case studies with large software systems performed in the course of this work provide valuable experience in applying FSV techniques to real systems in form of guidelines and heuristics.","title":"CAREER: Improving Scalability of Finite State Verifiers","awardID":"0093174","effectiveDate":"2001-07-01","expirationDate":"2008-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}}],"PIcoPI":["280544"],"PO":["564388"]},"59959":{"abstract":"The planned research involves several interrelated areas in spectral graph theory, extremal graph theory and random graphs. A main goal is to deduce the fundamental properties and structures of a graph from its graph spectrum (or from a short list of easily computable invariants). Various combinatorial, geometric and probabilistic techniques are being developed for examining the relations and behaviors of various graph invariants and properties. Although the primary objective is to advance our understanding of the intrinsic characteristics and underlying principles that govern discrete structures, such principles are quite effective and essential in dealing with problems involving massive graphs that arise in Internet computing and massive data sets.<br\/><br\/>A number of combinatorial problems on Internet infrastructures are examined, including the modeling and scaling of massive graphs using probabilistic analysis. Of particular interest is the study of graphs with power law distributions that offer good approximations for realistic networks. The evolution process of large dynamic graphs with only partial information is being analyzed and this has led to challenging problems and new research directions.","title":"Spectral, Extremal & Probabilistic Methods in Graph Theory with Applications to Information Technology","awardID":"0100472","effectiveDate":"2001-07-01","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1264","name":"ALGEBRA,NUMBER THEORY,AND COM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["452845"],"PO":["218970"]},"60675":{"abstract":"The goal of this project is to carry out a detailed multi-disciplinary study of single-electron<br\/>latching switches and of possible use of 2D arrays of such switches for hardware<br\/>implementation of self-organizing (plastic) neuromorphic networks. Preliminary estimates<br\/>show that such networks may provide unparalleled possibilities for complex information<br\/>processing. By these estimates, the networks may also have remarkable scaling properties:<br\/>if implemented using a 10-nm technology, they may have density about 10 8 neurons per<br\/>cm 2 at manageable power dissipation below 100 W\/cm 2 , and feature full learning cycle time<br\/>of the order of a few seconds. This scaling gives every hope that the networks will be able,<br\/>after initial (largely unsupervised) learning, not only provide complex information processing<br\/>including complex image recognition, but possibly reproduce biological evolution of the<br\/>cerebral cortex at a time scale some 6 orders of magnitude shorter.<br\/>The objective of the proposed project is to carry out a preliminary study of this<br\/>remarkable opportunity, addressing all its basic aspects at several structural levels. In<br\/>particular, research will include the following components:<br\/>A. Single-electron switch node design (D. Averin, K. Likharev, J. Wells).<br\/>Detailed theoretical analysis and modeling (on two basic levels of single-electron transport<br\/>theory) of statics, dynamics, and statistics of the proposed single-electron latching switches.<br\/>B. Low temperature prototyping (J. Lukens). Fabrication and experimental<br\/>study of Al\/AlOx\/Al prototypes of single-electron latching switches, with the goal to scale<br\/>single-electron islands down to 100 nm and tunnel junctions to 10 nm, respectively, which<br\/>would bring the reliable operation temperature up to about 10 K.<br\/>C. Molecular single-electron device development (B. Brunschwig, J. Lukens,<br\/>A. Mayr). Exploration of the opportunity to implement the basic component of the switches,<br\/>the single-electron transistor, by chemical self-assembly of molecular components. The<br\/>molecular components will be deposited in solution on the prefabricated metallic wire<br\/>structures, and then characterized using a set of electrical, electrochemical, and time-resolved<br\/>laser-spectrometry methods.<br\/>D. Top level modeling and analysis (J. Barhen, M. Bender, K. Likharev).<br\/>Large-scale computer simulation and a partial analytical study of the growth, dynamics, and<br\/>self-adaptation of neuromorphic networks based on these switches.<br\/>Hopefully, the project will achieve enough progress to justify a large-scale R&D effort<br\/>in this exciting direction. In particular, a reliable evidence of self-organization of adaptive<br\/>neuromorphic networks during largely unsupervised learning would certainly be followed by<br\/>the first hardware implementations of sizable networks (possibly, after an initial stage of<br\/>purely-CMOS-based prototyping using commercially available FPGA technology).<br\/>The project will have a substantial educational component. Specifically (besides<br\/>participating in general educational Stony Brook initiatives), at least 4 FTE graduate<br\/>students will be involved in the project each year, and some 20 undergraduate and<br\/>graduate students will take part in the project during its full 4-year period. At least one<br\/>student will work in BNL and one in ORNL most of the time. Working in a multi-disciplinary<br\/>team will allow these students to overcome inter-departmental barriers in their education.<br\/>As another specific educational initiative, we plan to organize a Web-based undergraduate<br\/>course on massively parallel supercomputing and neural networks, using the IBM SP3<br\/>computer at Oak Ridge.<br\/>Work on the inter-related aspects of this multi-disciplinary project will be constantly<br\/>coordinated by its P.I. (K. Likharev). In particular, regular meetings of all Stony Brook and<br\/>Brookhaven participants of the team working on the project (including postdoctoral<br\/>associates and students), and annual meetings with Oak Ridge collaborators, are planned.","title":"Nanoscale Single-electron Switching Arrays for Self-evolving Neuromorphic Networks","awardID":"0103059","effectiveDate":"2001-07-01","expirationDate":"2003-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0705","name":"Division of ENGINEERING EDUCATION AND CENT","abbr":"EEC"},"pgm":{"id":"1415","name":"PARTICULATE &MULTIPHASE PROCES"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0705","name":"Division of ENGINEERING EDUCATION AND CENT","abbr":"EEC"},"pgm":{"id":"1480","name":"ENGINEERING RESEARCH CENTERS"}}],"PIcoPI":["533341","268760","380954"],"PO":["187585"]},"61302":{"abstract":"PROPOSAL #0105558<br\/>WILLIAM MARSH RICE UNIVERSITY<br\/>PI: JOHNSON, DON H.<br\/><br\/>The closely allied fields of signal processing and information theory have never found common ground. Signal processing focuses on how signals can represent information and how systems manipulate and change signal structure. Information theory revolves around the structure of information that signals represent, but ignores what information is meaningful to the receiver by concentrating on efficient compression and communication. The research develops a new theory of information processing that weds these two disciplines and has the dual goals of understanding how effectively signals, no matter what their nature, can represent information and of quantifying how well systems process information. Because of the theory's generality, we analyze both communication systems, to probe how effectively they convey information and meaning, and neural processing systems, to understand how neural groups process and represent information.<br\/><br\/>We quantify how well signals represent information by computing an information-theoretic distance (it obeys the Data Processing Theorem) between signals associated with two instances of the encoded <br\/>information. We assume that the signals are stochastic, and the distance measures how different are the probability distributions associated with the signals. We use the Kullback-Leibler distance because it is related both to optimal classifier performance via Stein's Lemma and to optimal least-squares estimator performance through the Cramer-Rao bound. A larger distance thus corresponds to a more effective representation of the information. The information processing ability of a system is measured by the information transfer ratio, defined to be the ratio of distances computed at the system's input and output. With this ratio, we quantify how well an information processing system behaves as an information filter.","title":"Information Processing Theory and Applications","awardID":"0105558","effectiveDate":"2001-07-01","expirationDate":"2004-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["142498"],"PO":["564898"]},"60004":{"abstract":"In heterogeneous distributed computing, a network of dissimilar machines is used to<br\/>execute a given application in parallel. With the advent of advanced network technologies,<br\/>the scale at which heterogeneous computing can be practicaly applied is growing to a<br\/>global scale, thus making it possible to collect high performance computers across the<br\/>globe into a single computational resource. Multiple users will be able to simulataneously<br\/>use this computational resource to execute a variety of large, parallel applications.<br\/>However, there are a number of problems to solve before this type of computing becomes a<br\/>reality.<br\/>Therefore this project proposes to develop effective matching and scheduling methods to<br\/>allow multiple users to execute applications in a large, decentralized, failure-prone,<br\/>heterogeneous computing environment. Due to the dynamic and uncertain nature of this<br\/>environment, the algorithms that will be developed will use statistical techniques to<br\/>allocate resources fairly in a highly dynamic system. The algorithms will be evaluated by<br\/>simulations and also on a small testbed. There are three different aspects of this<br\/>proposed work:<br\/>1. Methods will be developed to statistically obtain an estimate of the response time of a<br\/>task on an arbitrary target machine. This will be accomplished by developing separate<br\/>techniques to estimate the execution time, machine loads, and network loads. A method for<br\/>estimating the execution time using data gathered from past executions of the task has<br\/>already been developed. This method will be refined and similar stochastic methods will be<br\/>developed for machine and network load estimation.<br\/>2. A dynamic matching and scheduling method for heterogeneous machines will be developed.<br\/>This method will allow each application to make scheduling decisions without direct<br\/>knowledge of the other applications executing in the environment. Each application will<br\/>have to compete for the computational resources of the network. The scheduling heuristics<br\/>will take into account the uncertainty in the stochastic estimates of the execution time<br\/>and machine and network loads.<br\/>3. Finally, reliable scheduling algorithms based on either heuristic cost functions or<br\/>evaluation of the reliability of applications will be developed. The heuristic cost<br\/>functions developed will define the impact of a scheduling decision on the reliability of<br\/>an application in time units. These algorithms will consider both the execution time and<br\/>failure probability of applications while making scheduling decisions, and thus, will be<br\/>capable of producing task assignments that improve both the performance and reliability of<br\/>applications. In addition, algorithms which can trade performance for reliability of<br\/>applications will be designed.","title":"Stochastic and Reliable Matching and Scheduling Algorithms for a Heterogeneous Computing Environment","awardID":"0100633","effectiveDate":"2001-07-01","expirationDate":"2003-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2876","name":"DISTRIBUTED SYSTEMS"}}],"PIcoPI":["420958"],"PO":["309350"]},"61214":{"abstract":"CCR-0105166<br\/>Luqi<br\/>Naval Postgraduate School<br\/><br\/>\"Monterey Workshop 2001- Engineering Automation for Software Intensive System Integration\"<br\/><br\/>A 3-day workshop, to be held in Monterey, CA on June 19-21, is organized for the dissemination and integration of recent research results related to the production of reliable cost-effective software in heterogeneous environments. A major goal of the workshop is to help the software engineering community focus on issues that are vital to improving the state of software engineering practice, including all topics related to supporting engineering automation of reliable, cost-effective, integrated, distributed software development processes. The workshop activities aim to assess current research efforts in this area, identify results and directions that can increase the degree of automation, aid tool integration by building a common understanding, and increase the practical use of formal methods.","title":"Monterey Workshop 2001-- Engineering Automation for Software Intensive System Integration","awardID":"0105166","effectiveDate":"2001-07-01","expirationDate":"2002-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}}],"PIcoPI":[157430],"PO":["397849"]},"61269":{"abstract":"Abstract <br\/>Experimental Analysis and Modeling of Digital Video Quality<br\/><br\/>The cost of the production and transmission of digital video imagery is large and roughly proportional to the information in the images. Consequently, information is routinely deleted from video imagery to reduce cost. This deletion reduces the quality of the imagery as perceived by human viewers. The ultimate objective of this project is to quantify and minimize this loss of quality. As a step toward this goal, an understanding of how humans evaluate the quality of video images needs to be developed. To gain this understanding a series of experiments are being performed in which human observers describe and evaluate defects that they detect in video images of the kind that are typical in digital video applications. These images are processed so that they incorporate defects similar to those that occur in digital video.<br\/><br\/>The results of these experiments are then used to develop an understanding of how humans process and evaluate video imagery. Image processing methods are being used to create sets of video defects that are quite similar to typical video defects, but are under much greater control. These defects occur at random times and places within normal video clips. They are treated as visual signals presented in a context of normal video imagery. Established methods of visual pattern detection and visual pattern appearance analysis are applied to the study of these defects. These include the following tasks: detecting the presence of a defect, rating the annoyance caused by a defect, and analyzing the perceived features of a defect. The results of these experiments will be used to develop an increasingly detailed model of video quality evaluation by human observers.","title":"Experimental Analysis and Modeling of Digital Video Quality","awardID":"0105404","effectiveDate":"2001-07-01","expirationDate":"2004-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["516549",157558],"PO":["564898"]},"57716":{"abstract":"Configuration management (CM) is a software engineering discipline <br\/>that centers on managing the evolution of the artifacts that are <br\/>continuously produced and changed over the course of a software <br\/>development project. The combined move towards developing software <br\/>out of sets of components and managing software after it has been <br\/>delivered to its customers radically changes the nature of CM, <br\/>breaking several of its fundamental assumptions.<br\/><br\/>Addressing these issues, the objective of this research is to <br\/>develop and empirically evaluate the models, policies, and tools <br\/>that will lay the foundation for the creation of the next <br\/>generation of configuration management systems---those capable of <br\/>managing, in a seamless and continuous fashion, component-based <br\/>software throughout its life time. The research hypothesizes that <br\/>architecture description languages are the key to providing this <br\/>functionality: they create a single abstraction through which all <br\/>CM functionality can be provided. Specifically, we propose to <br\/>build architecture-based CM models, associated new development <br\/>and deployment policies, and the supporting tools necessary for <br\/>demonstration and evaluation.<br\/><br\/>Parts of the research will be used to seed the educational aspects <br\/>of the proposal: to establish the undergraduate research factory in <br\/>which teams of undergraduate students will be educated in carrying <br\/>out research projects.","title":"CAREER: Continuous Change Management of Component-Based Software","awardID":"0093489","effectiveDate":"2001-07-01","expirationDate":"2007-04-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}}],"PIcoPI":["551130"],"PO":["564388"]},"56319":{"abstract":"EIA0088064<br\/>Krogh, Bruce H.<br\/>Carnegie Mellon University<br\/><br\/>CRCD: Instructional Modules for Embedded Control System Design<br\/><br\/>This project develops and disseminates a set of instructional modules and laboratory exercises for advanced undergraduate and first-year courses in real-time embedded control systems. Students are introduced to concepts and tools that are emerging from recent research in hybrid dynamic systems, rapid prototyping and haptic interfaces using new commercial tools for computer-aided engineering. The innovative web-based instructional modules provide students with the necessary working knowledge of fundamental and advanced concepts needed to work on state-of-the-research projects in the laboratory.","title":"CRCD: Instructional Modules for Embedded Control System Design","awardID":"0088064","effectiveDate":"2001-07-15","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1709","name":"CISE EDUCAT RES & CURRIC DEVEL"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0705","name":"Division of ENGINEERING EDUCATION AND CENT","abbr":"EEC"},"pgm":{"id":"1340","name":"ENGINEERING EDUCATION"}}],"PIcoPI":["460548"],"PO":["551712"]},"60665":{"abstract":"0103022<br\/>Kuo<br\/><br\/>This proposal was received in response to NSE, NSF-0019. The goal of this proposal is to explore a novel method for etching copper nano-lines at room temperature, which is crucial for the fabrication of future nano-devices and circuits. The PI plans to study the key step in the process, i.e., a unique anisotropic plasma-copper reaction, in the nano-scale region.<br\/><br\/>This is an experimental research project. The PI is going to investigate a number of issues in the nano-line definition process, such as the critical dimension control, profile, sidewall surface, and residue formation, through fundamental studies of plasma-copper surface reaction, solid-state reactant transport and copper-halide reactions. This research involves the microstructure of the copper material, plasma chemistry, and directional solid-state reaction mechanisms. In addition to plasma process characterization, he will make extensive use of Transmission Electron Microscopy (TFM) as well as other thin-film analytical methods to probe the plasma-copper reaction phenomenon at the nano-scale.<br\/><br\/>The plasma reaction will he carried out in the principal investigator's (P1's) Thin Film Microelectronic Research Laboratory. All experiments will employ a simple parallel-plate reactive ion etching reactor with a 13.56 MHz RF generator. The nano-line pattern, e.g., sub-100 nm, will be prepared in the NSF National Nanofabrication Users Network (NNUN) facility located in the Pennsylvania State University using c-beam lithography. The industry sponsor AMD agrees to support this project by supplying thin film characterization service, technical consulting, and 6-inch wafers.","title":"NER: Exploring A Novel Plasma-Based Copper Nano-Line Etch Method for Nano-Devices And Circuits","awardID":"0103022","effectiveDate":"2001-07-01","expirationDate":"2002-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"1517","name":"ELECT, PHOTONICS, & MAG DEVICE"}}],"PIcoPI":["441000"],"PO":["564900"]},"61303":{"abstract":"This project addresses the challenges of successfully deploying large-scale parallel I\/O servers to meet the demands of modern data-intensive applications multimedia retrieval, We and database servers, visualization and graphics, and spatial and temporal databases. The goals are to develop scheduling and resource management algorithms for parallel I\/O systems, and increase our understanding of the complex underlying resource tradeoffs. These include basic scheduling issues dealing with parallel I\/O, including those related to prefetching and caching, on -line scheduling, fair servicing of multiple users and deadline-constrained real-time parallel I\/O. Secondly the algorithms designed in this research will be directly applied to areas like multimedia systems, including variable-bit rate (VBR) video retrieval, and Web, database and application servers dealing with large numbers of concurrent, interacting I\/Os.","title":"High Performance Parallel I\/O","awardID":"0105565","effectiveDate":"2001-07-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}}],"PIcoPI":["410022","517951"],"PO":["562984"]},"61314":{"abstract":"C-CR 0105639<br\/>Dana Randall<br\/>\"Markov Chain Algorithms for Computational Problems from Physics and Biology\"<br\/><br\/>This research in Markov chain Monte Carlo methods has three <br\/>primary goals: (i) developing new, general techniques for <br\/>analyzing convergence rates of Markov chains; (ii) designing rigorous, <br\/>efficient algorithms for specific computational applications, focusing <br\/>on problems from statistical physics and biology with relevance <br\/>to computer science; and (iii) exploring the connections between <br\/>the phase structure of physical models and the inherent limitations <br\/>of various sampling methods.<br\/><br\/>The research is concentrated in these areas. 1) Coupling has been <br\/>a very popular method for bounding the convergence rates<br\/>of Markov chains based on local updates, but only works in <br\/>restrictive settings. Heat bath algorithms, which allow possibly <br\/>nonlocal updates, appear to circumvent potentially bad situations <br\/>arising from simpler chains, but tend to be prohibitively complex <br\/>for analysis. Decomposition theorems provide a new tool which <br\/>allow a Markov chain to be broken into pieces whereby a hybrid <br\/>approach can be used to analyze each piece. The investigator <br\/>studies how these methods can be used together to approach some <br\/>new sampling problems. 2) Computational biologists have developed <br\/>a Turing-universal model of computation based on Wang tiles using <br\/>double-stranded DNA. New efficient sampling algorithms for some<br\/>of these simple models are explored with the goal of providing ways<br\/>to test the model predict outcomes of experiments.<br\/>3) The research additionally explores the connection between <br\/>rapid mixing of locally defined Markov chains and the uniqueness <br\/>of the Gibbs state of the underlying physical system, also <br\/>characterized by the lack of a phase transition. Knowledge of <br\/>this phase structure is used to develop algorithms which will allow <br\/>sampling below the critical point, where local Markov chains <br\/>are inefficient.","title":"Markov Chain Algorithms for Computational Problems from Physics and Biology","awardID":"0105639","effectiveDate":"2001-07-01","expirationDate":"2005-07-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1263","name":"PROBABILITY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["518569"],"PO":["279077"]},"61325":{"abstract":"The objective of this research is to establish methods for theoretical <br\/>study in an important area of theoretical computer science, which is<br\/>the analysis of randomized algorithms. The main focus is on problems<br\/>that either can be modeled by Markov chains or can be analyzed using<br\/>the Markov chain approach.<br\/><br\/>This project concentrates on three main topics: (1) general analysis<br\/>of discrete-time Markov chains and the Monte Carlo Markov Chain<br\/>method, (2) analysis of Markov chains for generating random<br\/>permutations, and (3) resource allocation problems in distributed<br\/>systems.<br\/><br\/>In the area of general analysis of Markov chains the goal of the<br\/>research is to develop new tools for the analysis of mixing times of<br\/>Markov chains. An important part of this study is to investigate<br\/>relationships between various known methods of the analysis of mixing<br\/>times of Markov chains.<br\/><br\/>In the investigations of Markov chains for generating random<br\/>permutations the main focus is on so called random switching<br\/>networks. These networks model behavior of many Markov chains that<br\/>are sought in applications in cryptography.<br\/><br\/>In the area of resource allocation problems in distributed systems,<br\/>the research focuses mostly on resource allocation problems in<br\/>networks, in which the cost of the allocation depend on the routing<br\/>properties of the input network as well as on the contention<br\/>resolution protocols applied to the system.","title":"Analysis of Randomized Algorithms: Markov Chain Approach","awardID":"0105701","effectiveDate":"2001-07-01","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["218164"],"PO":["543507"]},"52900":{"abstract":"The investigator and his colleagues study mathematical, statistical, and computational questions motivated by the problem in computer vision of defining and computing \"structural scene description,\" descriptions of the objects in a scene and their relations to each other. The first goal is to obtain a theoretical understanding of the problem of inferring objects and their compositional structure, both in standard optical images and in laser range imagery and motion images. The second goal is to test this understanding by developing effective algorithmsfor the statistical inference of this structure, algorithms that work with real data at reasonable speed on current computers. The general approach is to formulate mathematical representations of patterns, typically compositional with a hierarchy of structures, to encode the variability of these structures in stochastic models, and to use the models to infer information about the image. Standard stochastic models are rarely adequate, prompting the development of new classes of probability measures or completely new directions for traditional models. Additionally, in the context of this application the team aims to develop mathematical, statistical, and computational ideas that are of broader use. For example, compositional issues in vision are similar to ones in the grammars of language. One aspect of the project studies this connection.<br\/><br\/>The investigator and his five colleagues continue their mathematical, statistical, and computational investigations of a range of problems motivated by image processing and computer vision. The underlying question is simple enough: Here's an image, what is it an image of? Despite its simplicity, this is a hard question to answer. The approach taken here is to decompose the image into different components in some hierarchical structure and to use statistical analysis to infer information about the image from the relationships of the components within the structure. There are similarities with the grammatical structure of language, which the project explores. Recognition of objects in an image is a fundamental problem for both computer systems and biological systems. Advances are important for engineers developing computer vision applications, computer scientists seeking efficient algorithms in problems related to intelligent behavior of machines, and cognitive scientists studying human vision and language skills. Results of the project are published on CD-ROM, allowing demonstration of the dynamic behavior of algorithms in ways impossible through traditional publication modes and offering new ways to exploit multi-media communications for both education and research purposes.","title":"Mathematical Analysis of the Compositional Structure of Images","awardID":"0074276","effectiveDate":"2001-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1265","name":"GEOMETRIC ANALYSIS"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1269","name":"STATISTICS"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}}],"PIcoPI":["333515",134851,"446347",134853,"254233"],"PO":["565027"]},"60677":{"abstract":"This proposal was recieved in response to NSE, NSF-0019. Focused electron beam decomposition of molecules adsorbed on surfaces will be investigated as a means for fabricating electrical contacts to individual or small arrays of nanostructures. This direct write technique offers a convenient, flexible and practical method for bridging the gap between mesoscopic lithography and the nanoscale. Of primary importance is the purity and resistivity of the deposited film. Resistivity will be correlated with process parameters by depositing between predefined metallic contact pads. Contamination of deposited metallic features will be avoided by using inorganic precursor molecules such as TaF5, TiCl4 and WF6 . Film purity will be determined in situ ,using standard surface science techniques. Preliminary estimates of write speeds achievable in environmental electron microscopes indicate that 1:1 aspect ratio,nm-scale wires can be written at rates of 0.1 um \/sec. Additionally, use of environmental electron microscopy will allow simultaneous identification and contacting features of interest. The flexibility of this technique will allow tailoring the deposited structures for different applications. Examples are nanowires for electrical contacts, metal nanodot arrays for attachment of functionalized organic molecules or specially shaped metal gates deposited on semiconductor surfaces to allow charge confinement and manipulation in nanoscale regions. Thus,success of this technique will enable rapid prototyping of diverse concepts cutting across several nanoscience and technology subfields. Longer term, the capability for e-beam writing of entire nanodevices is envisioned. Electron-beam decomposition of inorganic species leading to growth of semiconductors and insulators will also be investigated. Novel precursor chemistries will be developed for e-beam growth of insulating and semiconducting phases compatible with Si-based nanoelectronics. Essentially,this is an athermal method for depositing nanoscale features at temperatures below that for which the features 'melt' via surface diffusion of deposited atoms.","title":"NER: Direct Electron Beam Writing for Fabrication of Functional Nano-Scale Architectures","awardID":"0103061","effectiveDate":"2001-07-01","expirationDate":"2003-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"1517","name":"ELECT, PHOTONICS, & MAG DEVICE"}}],"PIcoPI":["445522","273428","545414"],"PO":["219912"]},"61326":{"abstract":"A thorough investigation of componene-oriented programming languages<br\/>is undertaken that entails (a) designing a component-oriented language,<br\/>(b) implementing it efficiently on modern computer architectures, and<br\/>(c) ensuring its applicability through case studies.<br\/>Design of the new language entails evaluating the suitability of<br\/>existing programming-language concepts for component-oriented<br\/>programming, specifying and evaluating new programming-language<br\/>features, and integrating the new and the old into a coherent new<br\/>language. Implementation involves assessing existing compiler<br\/>technology, proposing, building, and evaluating new compiler<br\/>mechanisms, and merging them into an efficient compiler and runtime<br\/>system. Finally, to ensure the language's applicability, a variety of<br\/>case studies are designed and implemented and the resulting systems<br\/>compared to more traditional solutions.","title":"Design and Implementation of Component-Oriented Programming Languages","awardID":"0105710","effectiveDate":"2001-07-01","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2876","name":"DISTRIBUTED SYSTEMS"}}],"PIcoPI":["486136"],"PO":["551992"]},"61128":{"abstract":"Jian Li<br\/>Spectral Analysis of Gapped or Missing Data<br\/><br\/>This research focuses on the spectral analysis of gapped or incomplete data sequences. The gapped or missing data problem usually arises when contiguous data measurements for a long time are hard to obtain or the measurements during some intervals are not useful due to strong interference or jamming and must be discarded. Spectral analysis of gapped or incomplete data sequences holds promise to advance many fields including astronomy, communications, medical imaging, radar, and underwater acoustics.<br\/><br\/><br\/>This research involves the development and application of efficient and robust spectral analysis algorithms for incomplete data sequences. The goals of this research are to devise and evaluate statistically sound and mathematically solid spectral analysis methods for various applications that involve the spectral analysis of gapped or incomplete data sequences, to gain both practical and theoretical insights into the algorithm properties including accuracy, resolution, convergence, and computational complexity, and to understand the impacts of large gaps or large numbers of missing data samples on the algorithm design. It is anticipated that the results of this study will significantly impact the advances of many practical applications.","title":"Spectral Analysis of Gapped or Missing Data","awardID":"0104887","effectiveDate":"2001-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["554666"],"PO":["564898"]},"60622":{"abstract":"The overall goal of this work is to develop the computational tools to better study \"shaken baby syndrome\". This project will develop highly accurate parallel adaptive Lagrangian discontinuous Galerkin techniques for the simulation of fluid mechanics and solid mechanics. These techniques will be central to the development of high fidelity simulation tools for investigation of pediatric brain injury mechanisms and preventative strategies. Even the most powerful computers today using the best available codes would need weeks or months to do these calculations. Moreover, even those heroic calculations would not provide reasonable simulations due to the poor numerical accuracy of the lower order (O(h)) finite element \/ finite difference schemes and explicit time integration schemes (O(Dt)) used.<br\/><br\/>The key element of the new strategy is the development and use of parallel adaptive hp Lagrangian discontinuous Galerkin schemes that provide the accuracy and efficiency necessary for dealing with the complex geometric and material structure of the brain tissue, associated membranes, and blood vessels. These schemes will enable the use of higher order approximations to obtain accuracies that are O(hp) and O(Dtk), for p, k > 1. While the pediatric brain injury application will drive the research into this new class of parallel computational techniques, the codes and methodology should be useful for a much wider class of problems.","title":"Parallel Adaptive Lagrangian Discontinuous Galerkin Simulation of Pediatric Brain Injury","awardID":"0102805","effectiveDate":"2001-07-01","expirationDate":"2005-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}}],"PIcoPI":["361346"],"PO":["565272"]},"61668":{"abstract":"EIA-0107409<br\/> Aspray, William<br\/>Computing Research Association<br\/><br\/>Title: Special Projects: The Recruitment and Retention of Faculty and Graduates Students in Computer Science and Engineering.<br\/><br\/>This award to the Computing Research Association (CRA) provides funds to support a significant study of the current problems of recruitment and retention of graduate students and faculty in computing research fields. The project is collecting and analyzing data and information on the subject, will develop suggested actions for interested agencies and other organizations to take, and will establish means to collect the data and information on an ongoing basis so that it will be on hand as problems arise in the future. The study group consists of twelve to fifteen qualified individuals selected from various areas of the computing research community.","title":"Special Projects: The Recruitment and Retention of Faculty and Graduate Students in Computer Science and Engineering","awardID":"0107409","effectiveDate":"2001-07-01","expirationDate":"2003-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1713","name":"WORKFORCE"}}],"PIcoPI":["208531"],"PO":["289456"]},"64979":{"abstract":"This proposal is to cover expenses associated with hosting the Digital Libraries Initiative All Projects Meeting in June 2001. The All Projects Meetings have been held semi-annually since the program's inception in 1994 and have been essential to building collaborations between funded projects, establishing and maintaining program identity and direction, and stimulating valuable personal interactions between researchers, graduate students, and information professionals from all sectors. The meeting agenda is structured to allow projects to report and demonstrate research products, and discuss and assess the relative merits and promise of new capabilities.","title":"2001 Workshop for Digital Libraries Initiative Principal Investigators in Roanoke, Virginia","awardID":"0122201","effectiveDate":"2001-07-01","expirationDate":"2003-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6857","name":"DIGITAL LIBRARIES AND ARCHIVES"}}],"PIcoPI":["550462"],"PO":["433760"]},"60744":{"abstract":"Through evolution, biology has produced a remarkably diverse and efficient collection of proteins. These proteins assemble into a seemingly infinite variety of nanobiostructures, and they promote an extensive repertoire of chemical processes; hence, they comprise an interesting collection of biomolecular nanomachines. The proteins that are responsible for the maintenance and manipulation of DNA are one important subset of this collection. Their capacity to assemble with, and to alter the structure of, DNA is essential for all biological function. These proteins function in the packaging of DNA, the high-fidelity copying of genetic information, the reading of the genetic code and its conversion into RNA, the generation of genetic diversity, and the preservation of genetic and structural integrity of the genome. This project describes a new experimental approach to study the assembly and function of several of the nanomachines that function in these biological processes. The method involves the direct visualization by fluorescence microscopy, in real-time, of the assembly, disassembly, and movement of these nanobiostructures on single, optically-trapped DNA molecules using a novel, multi-port, laminar-flow, micro flow cell. This instrument will allows one to readily introduce an individual, optically-trapped DNA molecule sequentially into a series of reaction conditions, and to visualize the changes in structure\/assembly of the molecules in real-time using multi-wavelength fluorescence microscopy. The successful development of this instrument will reveal important information about the structure and function of DNA-protein interactions that cannot be obtained using large ensembles of DNA molecules (where such information is often lost by averaging, or obscured by competing intermolecular interactions). This research will provide revolutionary new information about how proteins function to alter the structure of DNA. Furthermore, the experimental techniques can be applied to the study of many other nanoscale biostructures.","title":"NER: Biological Nanomachines: Assembly and Function of Protein-DNA Nanostructures at the Single-Molecule Level","awardID":"0103556","effectiveDate":"2001-07-01","expirationDate":"2002-06-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0301","name":"Division of PHYSICS","abbr":"PHY"},"pgm":{"id":"1248","name":"PHYSICS-OTHER"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1708","name":"QuBIC"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0705","name":"Division of ENGINEERING EDUCATION AND CENT","abbr":"EEC"},"pgm":{"id":"1415","name":"PARTICULATE &MULTIPHASE PROCES"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0807","name":"Division of MOLECULAR AND CELLULAR BIOSCIE","abbr":"MCB"},"pgm":{"id":"1676","name":"NANOSCALE:  EXPLORATORY RSRCH"}}],"PIcoPI":[156286,156287],"PO":["293593"]},"60525":{"abstract":"This proposal was received in response to NSE, NSF-0019. A novel approach for the fabrication of atomistic electronic devices is proposed which, if realized, will have important industrial applications. The theoretical methods involve simulations using quantum tight-binding molecular dynamics scheme that can be used to accurately treat interactions in carbon systems at the nanoscale level. Large scale simulations will be performed using novel parallel computer algorithms using a synergistic interdisciplinary collaboration. Simulation results can be used as a guide in the experimental investigations.<br\/><br\/>Although the present electronic technology is dominated by silicon, it is becoming clear that Si based electronic devices cannot be relied on to sustain the current pace of miniaturization. It is<br\/>becoming clear that a new class of molecularly perfect materials are needed to make these new devices. Single-wall carbon nanotubes are one such material that are expected to execute a ``quantum'' leap i the area of nanoscale electronics, computers, and materials. A focussed effort to lay the foundation for fullerene and nanotube based molecular electronics which will revolutionize the electronics and computer industries is proposed. The emphasis is on the modeling and simulations which will be used to guide experimental efforts to realize these devices. The theoretical method for the treatment of these systems contains many state-of-the-art features, making it ideally suited for studying these systems.","title":"NER: Fabrication of Fullerene Based Novel Molecular Electronic Devices Using Quantum Mechanical Simulations","awardID":"0102345","effectiveDate":"2001-07-01","expirationDate":"2003-06-30","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0406","name":"Office of INTL SCIENCE & ENGINEERING","abbr":"OISE"},"pgm":{"id":"5979","name":"CENTRAL & EASTERN EUROPE PROGR"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"1517","name":"ELECT, PHOTONICS, & MAG DEVICE"}}],"PIcoPI":["490281","550407"],"PO":["564900"]},"61328":{"abstract":"An undesired consequence of the growing parallelism of modern processors is that it is dramatically more difficult to separate the events that limit execution speed from the >events whose latencies are tolerated. A method for focusing design effort is critical-path analysis. This research proposes to apply critical-path <br\/>analysis at the micro-architectural level, with the goal of detecting and eliminating execution bottlenecks.<br\/>This research will explore the potential of the critical path in four interrelated efforts: (1) Modeling the micro-architectural critical path. The main task is to define a model of the critical path, i.e., the set of events and dependences in a micro-execution that will be exposed in the dependence graph on which the critical path<br\/>will be computed. (2) Efficient tracing of the critical path. This effort will develop an on-line algorithm that will use a last-arrival edge at each node to calculate the critical path in one pass in a simulator. (3) Hardware critical-path predictors. This research will explore the use of approximation methods in avoiding the analysis of the entire dependence graph. (4) Criticality-aware processor policies. This research will<br\/>use hardware critical-path predictors to focus hardware policies on events likely to be on the critical path.","title":"Exploiting the Critical Path in the Design and Performance Analysis of Modern Processors","awardID":"0105721","effectiveDate":"2001-07-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}}],"PIcoPI":["541915","556819"],"PO":["325495"]},"60602":{"abstract":"EIA- 0102710<br\/>Donald, Bruce<br\/>Dartmouth College<br\/><br\/>CISE Postdoctoral Associates in Experimental Computer Science: Physical Geometric Algorithms and Systems for High-Throughput NMR<br\/><br\/>While automation is revolutionizing many aspects of biology, the determination of three-dimensional protein structure remains an expensive task. Traditional automated and semiautomated approaches to protein structure determination through nuclear magnetic resonance (NMR) spectroscopy require dozens of experiments and months of spectrometer time, making them unsuitable for high-throughput automation. The research proposed is to develop algorithms and systems for determining protein structure from only a few key NMR spectra. The system will use algorithms similar to and adapted from physical geometric algorithms, pattern recognition and machine vision, signal processing, and robotics, in order to analyze spectra, assign spectral peaks to atom interactions, compute secondary structure, and estimate the global fold. Previously developed software, JIGSAW, represents NMR data with graphs encoding potential interactions between amino acid residues. JIGSAW applies graph algorithms to find subgraphs encoding the secondary protein structure. The postdoctoral research associate will build on the insights of JIGSAW by assisting to 1) integrate automated analysis of geometry and correlations in three-dimensional input spectra, 2) prove correctness, completeness, and complexity results within a random graph formalism, 3) extend JIGSAW to larger proteins, and 4) utilize long-range interactions in order to estimate three-dimensional protein structure.","title":"Postdoctoral: Physical Geometric Algorithms and Systems for Structural Biology Using Mass Spectrometry","awardID":"0102712","effectiveDate":"2001-07-15","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1713","name":"WORKFORCE"}}],"PIcoPI":["213293"],"PO":["289456"]},"60976":{"abstract":"The advent of the exploding Internet and the terra-bytes of<br\/>heterogeneous data in it have made more acute the topic of searching<br\/>such large digital databases. The challenges involved go beyond<br\/>questions in information retrieval. One may envision seeking images in<br\/>films, seeking words in voice data, and seeking phrases in compressed<br\/>files and in files of various types.<br\/>These challenges have boosted the appearance of myriad start-up<br\/>companies and ad-hoc methods for the various tasks.<br\/>The PI's approach has been a basic bottom-up long-term study of the<br\/>theory of searching. They have a large center of pattern matching<br\/>research that has pursued and continues to pursue understanding of the<br\/>theoretical underpinnings of generalized searching, coupled with<br\/>applications of their various ideas.<br\/><br\/>the current research will continue the investigation of issues in<br\/>generalized searching. In particular:<br\/> 1. Approximate indexing with a small number of errors.<br\/> 2. In-place compressed search.<br\/> 3. ``Reusable'' dynamic programming code.<br\/> 4. Parameterized matching with ``don't care''s.<br\/>Research on the theory of image processing will also be<br\/>continued. The particular areas of concentration are: <br\/> 1. The effect of digitization.<br\/> 2. Real multi-dimensional scaling.<br\/> 3. Efficient search of rotated images.<br\/>The investigators' research group has started a program of selective<br\/>implementation of advanced pattern matching ideas, some in conjunction<br\/>with research groups from other application areas. There are plans to<br\/>implement text fingerprinting ideas and test their applicability in<br\/>IR. In addition a project is planned that incorporates many of the<br\/>ideas on searching compressed and heterogeneous files by constructing<br\/>an automatic scientific home-page generator and maintainer (guaranteed<br\/>to be an instant hit with all professors of Computer Science!).","title":"Collaborative Research: Pattern Matching-Theory and Practice","awardID":"0104307","effectiveDate":"2001-07-01","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["401676"],"PO":["543507"]},"63727":{"abstract":"EIA-0116289<br\/>Tayfun E. Tezduyar<br\/>Rice University <br\/><br\/>MRI: Acquisition of a Parallel Computing Facility for Computational Engineering<br\/><br\/>This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a range of computational engineering activities including computational mechanics, computational heat transfer, computational materials science, computational bioengineering, computational environmental engineering, computational mathematics, and computer science, with an emphasis on interdisciplinary projects. For example, work on shared memory in a network of processors has led to techniques which can be tested in a production environment in applications in large-scale turbulence control analyses and in molecular simulations of nanostructures.","title":"MRI: Acquisition of a Parallel Computing Facility for Computational Engineering","awardID":"0116289","effectiveDate":"2001-07-01","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}}],"PIcoPI":["551029","401471","191820",164101,"217964"],"PO":["241298"]},"60702":{"abstract":"This proposal was received in response to NSE, NSF-0019. Carbon nanotubes are of interest as electronic conductors because of their unique properties, notably their behavior as 1-D conductors and their ballistic nature over several microns at room temperature. A general picture of electronic conduction in nanotubes is being developed by researchers in the field. Although the transient response of current flow is expected to be quite rapid (picosecond range), present experimental data for conduction properties of nanotubes is primarily obtained at low frequencies or in static measurements. In order to shed light on the fundamental current transport processes in nanotubes, an experimental and theoretical effort is proposed to develop characterization techniques which would allow the measurement of the transient current responses through nanotube conductors. This work will address two issues: i) the integration of nanotube conductors with appropriate high speed photoconductor materials and ii) the development of characterization techniques for nanoscale conductors based on pump\/probe measurements using high speed pulsed laser systems. It is expected that the proposed work will shed light on the conduction processes of these interesting electronic materials and provide important capabilities for integration of these materials with other electronic materials, including semiconductor device structures.","title":"NER: Investigation of Electronic Transport in Carbon Nanotubes Using an Ultrafast Photoconduction Technique","awardID":"0103227","effectiveDate":"2001-07-01","expirationDate":"2002-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"1517","name":"ELECT, PHOTONICS, & MAG DEVICE"}}],"PIcoPI":["275165","384946","509541","547042"],"PO":["564900"]},"60724":{"abstract":"This proposal was received in response to NSE, NSF-0019.<br\/><br\/>The project involves blending of molecular biology, polymer chemistry, and materials physics, in an attempt to create a new class of spin-tronic functional nanostructures. A new architecture is proposed to create <br\/>spin-tronic magnetic devices, using a biologically modified, metal coated, polymer nanotube process. The advantages of spin-tronics include higher speed, greater storage density, low power dissipation, and non-volatility. The proposed architecture uses a form of molecular self-assembly based on biological models. The magnetic nano-wires are formed from a tubular polymer backbone, that can be coated with various metallic layers. By functionalizing the ends of the tubes with special molecules, the nano-tubes become what we call \"Smart Wires\". Smart-Wires are at the center of our proposed new architecture for nanoscale device fabrication, since the instructions for \"wiring\" the circuits are built into the molecular nanostructures <br\/>themselves, rather than having to be imposed afterwards using a patterning process. The \"Smart-Wires\" use molecular recognition, modeled after the biological antibody-antigen bonding reaction, to attach the wire ends to the appropriate substrate structures. Under this project metal coated nano-tube will be fabricated and the work will be extended to include the \"Smart Wire\" functionalization and bonding, and also metallic <br\/>multilayer coatings. A full range of sensitive magnetic and electronic probes will be used to measure the nano-tube properties, using methods perfected in a previous NSF project to study epitaxial spin-valve structures. The research team plans to develop a prototype of a magnetic nano-tube assembly with a switchable magnetic state.","title":"NER: Magnetic \"Smart-wires\" for Spintronic Nanostructured Devices","awardID":"0103430","effectiveDate":"2001-07-01","expirationDate":"2003-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"1517","name":"ELECT, PHOTONICS, & MAG DEVICE"}}],"PIcoPI":[156218,"377652"],"PO":["564900"]},"80447":{"abstract":"","title":"Average Complexity","awardID":"0296037","effectiveDate":"2001-07-01","expirationDate":"2003-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["554394"],"PO":["543507"]}}