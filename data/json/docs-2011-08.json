{"181082":{"abstract":"An integrated eye-tracking head-mounted display (ET-HMD) able to display stereoscopic virtual images while also tracking the direction of the user's gaze would benefit both fundamental scientific research and a host of emerging applications. Yet despite significant advances in and commercial availability of stand-alone HMD and eye-tracking technologies, a portable, lightweight, accurate and robust system that conforms to the form factor of a pair of sunglasses remains elusive. The PI's goal in this project is to develop a fundamentally new optical technology that will make it possible to realize this dream by overcoming the limitations of the conventional methods that have been applied to ET-HMD designs to date. In most previous work on designing ET-HMD systems, the optical systems for the HMD and eye-tracking paths were treated separately, and rotationally symmetric optical surfaces were used. In contrast, the PI's approach is to utilize freeform optical technology in combination with an innovative optical scheme that uniquely combines the display optics with the eye imaging optics. To these ends, she will investigate the challenges of designing the freeform ET-HMD system and develop appropriate design methods along with an optimization strategy for the required high performance optical system. She will implement a fully integrated, portable prototype system, and develop calibration and assessment methods for evaluating both the display and eye-tracking performance. As a testbed application for the new technology, the PI will evaluate the feasibility of adopting ET-HMD displays to augment communication by patients suffering from ALS (amyotrophic lateral sclerosis) or similar neurological disease that causes loss of speech. Project outcomes will include the first lightweight and portable ET-HMD display prototype that has a form factor close to that of sunglasses.<br\/><br\/>Broader Impacts: The new technology resulting from this project will have critical impacts in many fields. It will create a revolutionary and intriguing platform for mobile communication, wearable computing, and portable entertainment. It will provide a new tool for research related to vision and human factors, where eye movements provide a sensible metric for understanding human perception in 3D space and effectiveness at specific tasks. It will afford augmentative and alternative human-computer interfaces for people with proprioceptive disabilities or with situational impairments due to having their hands and feet occupied. And it will undoubtedly spur development of a host of new and thrilling applications. The research will also provide a vehicle for training the next generation of interdisciplinary scientists and engineers, at both the undergraduate and graduate levels.","title":"HCC: Small: Development of an Eyeglass-Style Compact Eye-Tracked Near-Eye Display Using Freeform Optical Technology","awardID":"1115489","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[485281],"PO":["565227"]},"186098":{"abstract":"Record and document retention (document disposition) has become a serious problem for both organizations and individuals since most documents now created are digital. Digital documents offer both problems and advantages. Digital documents are easily versioned, copied and disseminated. Thus, there can be several similar copies or versions of important or relevant documents in many locations. Document or record disposition can be applied to or is needed by individuals, organizations and domains (such as law, science, policy, etc.) for effective information management over long periods of time. This problem is of epic proportions and is becoming a major problem in organizations and for individuals throughout the world where effective record disposition is either required by law or by the organization or by practical limitations in systems.<br\/><br\/>This exploratory project investigates possible automatic document disposition methods based on algorithms for text inspection, mining, and search. The challenges lie in finding scalable, adaptable algorithms that can be used in several if not all application domains. In addition, variability in users presents many problems. A disposition method or procedure may vary depending on the user, organization and domain (e.g., law, health records, etc.). The approach explored in this project applies and extends machine learning methods to these problems since these methods adapt to variability in data, areas and domains. Using such approaches, automated disposition methods can be readily applied to these different areas such as science, email and legal records. This research lays the groundwork for adaptive methods for a variety of domains in terms of applicability, performance and scalability. this proof-of-concept project initially focuses on the Enron email data set that is publicly available and is be used to demonstrate the feasibility of the approach since email can be considered a special case of document disposition. If successful, other disposition domains such as science and government data will be explored. This work will show the viability of developing and applying machine learning methods to an important and diverse problem domain. <br\/><br\/>The results from this exploratory project together with insights gathered from methods used in large scale document search are expected to yield understanding as to how we can better manage our digital past and the rapidly expanding digital future. The results are expected to introduce this important problem to other researchers and document disposition professionals and lead to collaborations with industry. Data and research results will be made available through a publicly available website (http:\/\/clgiles.ist.psu.edu\/disposeseer\/) and research papers will be published and presented in appropriate venues. The project provides research experience for graduate and undergraduate students.","title":"EAGER: Automatic Document and Record Disposition and Retention","awardID":"1143921","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["562561"],"PO":["563751"]},"181390":{"abstract":"Increasing productivity in the area of analog integrated circuit design requires the development of a new generation of circuit synthesis tools. A major challenge is that continued scaling of transistor dimensions following Moore's Law makes it difficult to describe the physical behavior of transistors in the form suitable for circuit optimization. The work under this proposal will develop a new approach for optimization over approximate descriptions of transistor behavior, which is the only realistic way to capture analog circuit behavior in a manner appropriate for automated synthesis. The approach is based on explicitly modeling the divergence between the exact model and the approximate model. The research will specifically develop: (1) a new analog synthesis framework for model-based optimization over approximate functions that is able to explicitly take into account the distribution of errors between the approximate and exact models to drive optimization to a solution guaranteed to be true with respect to the exact model; (2) a new model-fitting algorithm, tailored for highly-constrained optimization over approximate functions, that will further enhance the ability of the synthesis tool to produce a good solution.<br\/><br\/>The outcomes of the work under this proposal will lead to increased automation of analog and mixed-signal design, and result in higher design productivity, as well as more power-efficient and cheaper integrated circuits. Thus, this work will help sustain the evolution and growth of semiconductor technology that has had enormous social implications over the last fifty years. The concepts to be developed will also benefit other scientific domains in which optimization using approximate functions is used. The educational component of this proposal aims to combine the active research program and research experience in this field with the creation of an instructional and teaching infrastructure. Specifically, the graduate courses offered by the PIs will incorporate the aspects of design methodologies developed in this research.","title":"SHF: Small: Overcoming Nanoscale Modeling Challenges in Analog Synthesis: A Data-Driven Paradigm for Optimization of Approximate Functions","awardID":"1116955","effectiveDate":"2011-08-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["535118","541981"],"PO":["562984"]},"182380":{"abstract":"Economic competitiveness relies upon innovation and organizations are increasingly pursuing open innovation through crowdsourcing. Though several models have arisen for how to tap into the crowd for ideas, little research has been done to understand and assess this new phenomenon. Previous work has investigated relatively simple crowd-sourcing techniques such as Amazon Mechanical Turk, but additional web-enabled platforms for organizing large numbers of individuals to contribute ideas and solutions to a client's innovation challenges have been developed. Firms providing such platforms for crowdsourcing are now competing with traditional innovation consultancies and together serve as intermediaries for open innovation. This project investigates and compares the socio-technical systems that these open innovation intermediaries have created.<br\/><br\/>Successful innovation requires translating and integrating ideas in science, engineering, design, and business originating from diverse organizations, industries, and countries. The current literature on this process focuses on collocated teams with little attention paid to organizational aspects the emergent technology-brokering phenomenon. This research extends the theory of technology brokering to this new form of virtual organization, building a more general framework of how technology brokering occurs and the role of individual backgrounds, organizational practices, and social networks in spanning multiple boundaries. It will explore the tradeoffs involved in increased reliance on codified methods to support technology brokering and investigate the role of institutional contexts in shaping the relative positions and organizational practices of technology brokers. Through in-depth, multi-site field studies this project will develop a theory of how technology brokering is accomplished in the new era of crowdsourcing and document this new phenomenon.<br\/><br\/>As web use has grown, new web-enabled methods have been developed for collecting ideas from the large globally distributed population of inventors. Crowdsourcing can be an effective innovation technique and these new platforms are being used to find solutions for such diverse problems as cleaning up oil spills, improving diaper absorbency, finding biomarkers for diseases, and designing new rail services. Understanding the advantages and disadvantages of different forms of crowdsourcing for various types of problems can help firms optimize their innovation processes and enhance economic competitiveness. Successful crowdsourcing also provides intellectual and economic opportunities for members of underrepresented groups and for scientists and engineers in impoverished regions. Research results will be disseminated broadly through curricular materials appropriate for undergraduate and MBA and through a cross-disciplinary research workshop focused on the crowdsourcing of innovation.","title":"VOSS Collaborative Research: Open Innovation Intermediaries: Brokering Technology and the Wisdom of the Crowds","awardID":"1122381","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7642","name":"VIRTUAL ORGANIZATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7953","name":"SOCIAL-COMPUTATIONAL SYSTEMS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"5376","name":"INNOVATION & ORG SCIENCES(IOS)"}}],"PIcoPI":[488606],"PO":["565342"]},"181181":{"abstract":"Over 80 million households in the United States have a home computer and an Internet connection. The vast majority of these are overseen by people who have little computer security knowledge or training, and many users try to avoid making security decisions because they feel they don't have the knowledge and skills to maintain proper security. Nevertheless, home computer users still make security-related decisions on a regular basis --- for example, whether or not to click on a link in an email message --- without being aware that is what they are doing. Their decisions are guided by how they think about computer security,their mental models. Interestingly, these models do not have to be technically correct to lead to desirable security behaviors. In other words, sometimes even \"wrong\" mental models produce good security decisions. This project will explore the implications of that insight. By eliminating the constraint that non-technical users must become more like computer security experts to properly protect themselves, this project will identify and create more effective ways of helping home computer users make good security decisions.<br\/><br\/>This project will help advance our understanding of how mental models of security are formed and how ideas are incorporated into mental models and transmitted from person to person. What kinds of information are incorporated into home computer users' mental models? Work will initially be focused on experimentally testing two hypotheses: a) stories about experiences have a larger influence on behavior than behavioral advice, and b) information from friends and colleagues has a stronger influence on mental models, and therefore behavior, than information from security experts. Additionally, the prevalence of particular mental models will be measured and correlated with actual user security behaviors. Through these investigations, this project will characterize the reasons that many home computer users choose not to act securely --- a question which is one of the biggest challenges of home computer security. Finally, this project will explore ways of encouraging behaviors that support secure system use by developing a prototype socio-technical system that is capable of influencing their mental models and moving people toward models that lead to greater security.<br\/><br\/>Home computer security and personal information security are large problems today. Current education campaigns have failed to effect widespread changes in the security behaviors of non-technical users. New technologies are being developed, but will do nothing if users intentionally choose to ignore the technology or to work around it. This project will find better ways of informing people about security issues, altering their understanding of security threats and thereby their security behaviors, which will ultimately create more secure home computers. It will produce research tools, including survey instruments and security behavior measurement software that can be used by other security researchers. It will train a number of students, both graduate and undergraduate, in working on multi-disciplinary, distributed teams. The results from this study will be disseminated broadly to multiple academic communities.","title":"TC: Small: Collaborative Research: Influencing Mental Models of Security","awardID":"1115926","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["517650"],"PO":["565136"]},"181083":{"abstract":"Understanding and analyzing the way our world is connected is a critical but new challenge in today's world, thanks to the technological advances of personal computers, mobile devices, as well as local and global Internet connections. Most current methods in the area of social media analysis, inference and understanding are based on textual data. However, the image data makes an increasingly large proportion of data in social media. Hence, there is an urgent need for tools that can effectively use image data to extract important information to infer patterns and activities of people, communities and society at large. <br\/><br\/>This project combines advances in computer vision, machine learning, and social networks in novel ways for understanding and analyzing large-scale social media data. The proposal brings together computer vision and machine learning research in novel ways to develop new methods for analyzing large-scale social media data. It pursues 4 inter-related aims: (i) Establishing a large-scale visual concept ontology and structures for the web-image world via crowdsourcing, taxonomy induction, and nonparametric learning methods; (ii) Understanding activity in social networks by analyzing image contents in the context of social media in large-scale and with connectivity; (iii) Inferring the structure of social networks and communities from image contents and activity of individuals in social networks; (iv) Discovering and analyzing dynamic social media trends. <br\/><br\/>Anticipated products of this research include new tools for analysis and modeling of socially generated content, with special emphasis on image data. The resulting methods provide potentially useful insights that characterize users, communities and societies, in a broad range of applications. The project offers enhanced research-based advanced training opportunities for graduate as well as undergraduate students and involves development of new courses on related topics at both Stanford University and Carnegie Mellon University.","title":"III: Small: Collaborative Research: Using Large-Scale Image Data for Online Social Media Analysis","awardID":"1115493","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[485283],"PO":["565136"]},"186110":{"abstract":"This project explores the design and use of multi-lifespan information systems; that is, information systems that not only meet the needs of today's users but can also maintain their usefulness beyond a single human lifespan. Specifically, the project will (a) test the feasibility of a multi-lifespan information system design approach; (b) generate preliminary design knowledge and methods for multi-lifespan information system design; and (c) contribute meaningful information system designs in our domain of study - international justice. To achieve these goals, the project will leverage the Voices from the Rwanda Tribunal testbed in two types of research activities: multi-lifespan information system envisioning; and digital information tagging and meaning making from a multi-lifespan perspective. <br\/><br\/>Intellectual Merit: The project will explore the feasibility of the potentially transformative multi-lifespan approach, including: (a) envisioning futures responsive to the information needs, concerns, and values of different generations; (b) through conceptual, technical and policy-oriented mechanisms, anticipating and supporting shifts in societal, political and technological conditions; and (c) conducting preliminary investigations into how to involve the public in interacting with deployed multi-lifespan systems. The project work is likely to catalyze new design methods and technical mechanisms, ones that can account for divergent futures, evolving socio-political situations, and robust information system adaptability. New techniques for managing privacy, security, and freedom of expression are anticipated.<br\/><br\/>Broader Impact: This research will promote important societal values including human rights, security, freedom of expression, and peace-building; and inform developments in the field of international justice. The results will also be highly relevant for other digital collections that are intended to outlast the creators' lifespans.","title":"EAGER: Speaking Across Generations: An Early Investigation into Multi-Lifespan Information System Design","awardID":"1143966","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["542095"],"PO":["564456"]},"186011":{"abstract":"Recent technological advances have enabled digital applications to move away from purely desktop and office settings to gain greater relevance in our everyday lives and spaces: homes, classrooms, public and cultural places, scientific laboratories and beyond. This has given rise to an increasing number of creative practices and research areas that seek to overcome the long-standing separation between the physical and digital worlds. The ACM Conference on Tangible, Embedded and Embodied Interaction (TEI) serves as a gathering place for an interdisciplinary community of computer scientists, engineers, artists and designers working in the emerging field of new interfaces that bridge the physical and virtual worlds. <br\/><br\/>This award is in support of the third annual TEI 2012 Graduate Student Consortium, to be held at the ACM TEI 2012 at Queen's University in Ontario, Canada. The ACM TEI 2012 Graduate Student Consortium will promote national and international exchange of research, methods and ideas at the intersection of this diverse field of engineering and creativity. The consortium is an opportunity to develop the research and design skills of a new generation of scientists, engineers, and designers who will shape the technological and socio-cultural landscape of the future of computing as it integrates physical objects and virtual spaces seamlessly. The consortium program aims to increase participation in the tangible, embedded and embodied interaction academic community by providing mentorship for young scientists, researchers, and designers in this field, and by giving them the opportunity to meet and engage with more senior TEI researchers at the conference.<br\/><br\/>Students from a range of fields, including industrial design, digital media arts, human centered computing and computer science will convene and share research projects that will help to broaden their perspectives on transformative practices and methods in the field. Consortium participants will be invited to present their work and receive constructive critique from a panel of faculty mentors. And, they will present their work in a poster session during the conference, and their accompanying papers will be published in the ACM Digital Library.","title":"Workshop: Graduate Student Consortium at the 2012 Tangible, Embedded and Embodied Interaction (TEI'12) Conference","awardID":"1143513","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["534174"],"PO":["565227"]},"177211":{"abstract":"Scalable kinship inference in wild populations across years and generations<br\/><br\/>A cornerstone of research in molecular ecology is the reconstruction of family groups (kinship analysis).<br\/>Understanding how individuals in free-living populations are related to each other provides the best<br\/>opportunity to study many important biological processes, ranging from sexual selection to patterns<br\/>of dispersal and recruitment. Recent advances in molecular DNA technologies and computational<br\/>methods have made these studies possible. However, many conceptual and computational challenges<br\/>remain and need to be addressed in order to advance these studies. To date, existing research work<br\/>on kinship analysis has primarily focused on computational methods that address a single relationship, such as parentage assignment or reconstruction of full sib groups. Inclusion of multiple objectives, such as half-sib reconstruction with minimum parentage assignment, or hierarchy over multiple generations, makes formulation of the underlying computational problem extremely challenging, and simple extensions of previous methods do not address in a practical, scalable, and robust manner the problem of kinship reconstruction for data sets that include multiple generations of species or involve multiple optimization functions.<br\/><br\/>The goal of the proposed research is to design robust, parsimonious, and versatile computational<br\/>approaches for inferring multi-generation kinship relationships in wild populations from multiallelic<br\/>markers. Parsimony assumption is fundamental to these approaches as it requires no prior knowledge,<br\/>assumptions about sampling methodology, or existence of models, which is the case for most free-living<br\/>populations. The diverse tasks of this project include formulating computational kinship inference<br\/>problems based on existing biological studies, analyzing computational complexity of and providing<br\/>solutions to the resulting combinatorial optimization problems, and designing robust, scalable and<br\/>efficient high performance implementations. The resulting computational methods will be evaluated<br\/>on datasets collected from existing biological studies and will be deployed to the biological community<br\/>through the Kinalyzer web-based service, currently actively used for sibship inference only.<br\/><br\/>The research proposed in this project will greatly impact diverse application areas including funda-<br\/>mental research in combinatorial optimization and data mining, and within biology, areas as diverse as<br\/>behavioral ecology, evolutionary genetics, conservation, forensics, and epidemiology. The multidisci-<br\/>plinary nature of the project and the research team will enhance curriculum design of related areas and<br\/>introduce new cross-disciplinary courses. This cohesive, multidisciplinary project will provide training<br\/>opportunities in biology, operation research, algorithms analysis, bioinformatics and high performance<br\/>computing, within a single application framework. The project will leverage the diverse scientific ex-<br\/>pertise and extensive mentoring experience of the team to foster a true interdisciplinary collaboration<br\/>and to provide a thriving environment for a new generation of interdisciplinary scientists.","title":"III: Medium: Collaborative Research: Scalable Kinship Inference in Wild Populations Across Years and Generations","awardID":"1064681","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0808","name":"Division of BIOLOGICAL INFRASTRUCTURE","abbr":"DBI"},"pgm":{"id":"1165","name":"ADVANCES IN BIO INFORMATICS"}}],"PIcoPI":["532978","533275",475088,"516991"],"PO":["565136"]},"177332":{"abstract":"This project brings together four investigators who identified an opportunity to bridge their disparate fields (gait analysis, body sensor networks, low power RF circuits, and gerontology) in order to address an extremely important health care problem of fall prevention. Their combined expertise addresses the following scientific objectives: (1) explore on-node signal processing and low power RF transceivers to balance energy consumption and mobility analysis fidelity in wireless on-body inertial sensing, (2) identify the minimal set of on-body sensor locations to provide high-quality mobility analysis, (3) determine the appropriate algorithms to identify fall prone individuals in controlled in- and out-of-lab data collections to assess the external influences of context and activities, and (4) validate the TEMPO-based data collection and processing algorithms in a naturalistic HFC living environment.<br\/><br\/>This project enables a new paradigm in elderly patient care to target fall prevention interventions to high fall risk individuals before fall incidents occur, preserving patient's health and quality of life while reducing health care costs. In addition, the above described fundamental scientific objectives apply in the broader application of energy efficient BSNs in a variety of application domains. The education and outreach plans focus on crossing the disciplinary divide between technology and health care, using hands-on public demonstrations, undergraduate laboratory modules, and multi-disciplinary videoconferencing graduate seminar courses to attract and develop leaders who can address the tremendous challenges that face the health care industry in the coming decades.","title":"SHB: Medium: Collaborative Research: Non-Intrusive Multi-Patient Fall-Risk Monitoring in Health Care Facilities","awardID":"1065442","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8018","name":"Smart Health & Wellbeing"}}],"PIcoPI":[475415,475416,"560070"],"PO":["564768"]},"177167":{"abstract":"Many applications in science and engineering encounter the problem of<br\/>identifying and processing topologically interesting features in the<br\/>digital representation of a geometry or data. Such features often need<br\/>to be optimal with respect to some metric (measurement). It is<br\/>recognized that homology groups from algebraic topology play an<br\/>essential role in these computations. Although the study of structural<br\/>properties of the homology groups has a rich history in mathematics,<br\/>their computations in combination with geometry are not that well<br\/>studied. The principal investigators (PIs) propose to study these<br\/>fundamental questions thoroughly, along with their connections to<br\/>practical problems from science and engineering.<br\/>Intellectual merit: Efficient solutions of the optimality questions in<br\/>homology computations require both mathematical and algorithmic<br\/>developments. The PIs bring aboard these required expertise. Apart<br\/>from the synergistic effect of the proposed study on mathematics and<br\/>theoretical computer science, the close ties with various applications<br\/>in science and engineering will play a synergistic role between<br\/>computational fields such as computer graphics, computer vision,<br\/>sensor networks, computer aided design, and scientific fields such as<br\/>biology, physics, chemistry, and others.<br\/><br\/>Optimization of aspects of homology groups provides<br\/>important insights in many scientific and engineering applications<br\/>ranging from tunnels in protein molecules to voids in large<br\/>machines. Solutions of such problems can aid in the manufacturing of<br\/>better machines, designing of new drugs, and rapid modeling of<br\/>customized objects. The educational impact of this project is in a<br\/>large synergy between mathematics and computer science motivated by<br\/>real applications. Course notes, internet distributions, and software<br\/>systems developed through the project will enable the scientific<br\/>community to study challenging problems in geometry, topology, and<br\/>algorithms. Graduate students supported by the project will develop<br\/>skills in mathematics and theoretical computer science and also in<br\/>writing robust, efficient, and user-friendly software.","title":"AF: Medium: Collaborative Research: Optimality in Homology - Algorithms and Applications","awardID":"1064429","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[474979],"PO":["565157"]},"176199":{"abstract":"This project is enhancing the public Emulab network testbed to better support research and educational activities that deal with computer virtualization. Virtualization is transforming computing. Cloud computing, virtual desktops, and similar techniques are changing the ways people think about, provision, and purchase resources. Virtualization is a cornerstone of future computing architectures, so it is critical to promote research in this area.<br\/><br\/>Although there is an abundance of virtual infrastructure today, there is a lack of public physical infrastructure on which to conduct sizable experiments in virtualization. Investigations into virtual-machine and cloud-management software require access to \"bare metal\" computer and network resources because the goal of virtualization is to abstract underlying physical resources. Currently, researchers and educators lack access to the modern, networked, physical facilities needed to develop the next generation of virtualization-based systems software.<br\/><br\/>This project is addressing the need for these physical facilities. It is extending Emulab, an existing and Internet-accessible network testbed, to support widespread experimentation and education in virtualization technologies. Activities include acquiring modern hardware on which to conduct virtualization experiments, developing software to support this new hardware, operating the enhanced testbed, and performing outreach through open-source software releases and support to sites that run Emulab software.<br\/><br\/>The enhanced testbed will support research and education toward new services that utilize virtualization: e.g., tomorrow's clouds, virtual machines, and Internet-based applications. It will enable thousands of researchers and students to run realistic virtualization-oriented experiments that would otherwise be impossible.","title":"CI-ADDO-EN: Enhancing Emulab for Virtualization and Clouds","awardID":"1059440","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["557575",472196,"559299"],"PO":["563661"]},"177178":{"abstract":"This project aims to improve the practice of shared-memory concurrent programming by exploring a fundamentally new way to specify, verify, test, and monitor how threads communicate via memory. Shared-memory concurrency has become an increasingly important style of programming because it is a common way to utilize multicore processors, i.e., machines where there is more than one processing core, and desktops, laptops, servers, and even mobile devices are increasingly multicore. Shared-memory concurrency is widely recognized as difficult and error-prone, and much prior work has aimed to detect bugs related to this style automatically. This project complements prior work by focusing on application-specific specifications in terms of how different parts of the code-base use concurrency to communicate, rather than focusing on how individual pieces of data are used. This approach aims to improve the quality of software used throughout society, to improve the productivity of software developers and testers, and to influence how students are taught concurrent programming.<br\/><br\/>At the heart of the approach is a communication graph in which the nodes are program points and the edges indicate communication via shared memory. That is, for each edge, the code that the source node represents performs a write in one thread that is subsequently read in another thread by the code that the target node represents. Such graphs can form the foundation for conceptual and intellectual tools useful throughout the development and maintenance of software, including specifications (declarations of what communication is allowed), static checking (program analysis to infer possible communication), dynamic checking (efficient run-time communication monitoring), testing (design\/evaluation of a test-suite in terms of communication coverage), and automatic anomaly detection and bug isolation (in terms of unexpected communication) for deployed software. This project is developing and evaluating tools inspired by this foundation, leveraging synergies across the execution stack, including work on computer architecture, run-time systems, compilers, programming languages, automatic testing, and static analysis.","title":"SHF: Medium: A Code-Centric Approach to Specifying, Checking, and Discovering Shared-Memory Communication","awardID":"1064497","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["517262","555936"],"PO":["564388"]},"178289":{"abstract":"Election mechanisms are broadly used in computational settings, including a rapidly expanding range of applications in multiagent systems. Twenty years ago, responding to results showing that all reasonable elections systems can be manipulated, Bartholdi, Orlin, Tovey, and Trick proposed protecting election systems from manipulation by making the attacker's task computationally prohibitive, e.g., NP-hard. Their work started a rich line of research, yielding many such computational protection results.<br\/><br\/>However, a number of weaknesses in this approach have emerged: (1) Much of the work assumes that voters have complete, transitive preferences and that the manipulator has perfect knowledge of the preferences of each voter. (2) Many election systems have polynomial-time algorithms for perfect manipulation and so cannot be computationally protected. (3) Even when there are NP-hardness results, these assume all ensembles of voters are possible, and it has recently been shown that when one looks at, for example, voters obeying the common behavior model known as single-peakedness, these NP-hardness results often evaporate. (4) Even when there are NP-hardness results, they are worst-case results, and so it is possible that often-correct heuristic attacks exist.<br\/><br\/>This project will respond to these weaknesses, rebuilding the computational approach to protecting elections and more rigorously delineating its limitations. More natural and realistic models will have strong consequences in terms of the weaknesses discussed above. Complexity theory and algorithmics will both be developed in a broad investigation of the above weaknesses and techniques to go beyond them to retain the value of using complexity theory to protect election systems against manipulation.","title":"ICES: Small: Collaborative Research: New Approaches to Computationally Protecting Elections from Manipulation","awardID":"1101452","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8052","name":"Inter Com Sci Econ Soc S (ICE)"}}],"PIcoPI":[478001],"PO":["565251"]},"181380":{"abstract":"This research project is exploring a new way of writing computer<br\/>programs, based on a new programming language Plaid. Plaid is the first<br\/>programming language to incorporate \"permissions\" natively into the<br\/>language. These permissions allow the programming language to more<br\/>accurately model things that change--for example, a connection to a web<br\/>site that might be interrupted and then later reconnected. Such a<br\/>permission-based language helps engineers coordinate on a software<br\/>project, making engineers more productive, and reducing bugs that<br\/>end-users see.<br\/><br\/>On a technical level, the project integrates permissions such as<br\/>\"unique,\" \"immutable,\" and \"shared\" into the language via a new kind of<br\/>type system, based on linear logic. These permissions express aliasing<br\/>information, and this aliasing information can be leveraged to do many<br\/>things, including safely changing the representation of objects at run<br\/>time and automatically parallelizing code. The project is investigating<br\/>the design of a single permission system that can address multiple such<br\/>concerns; investigating how models of inheritance, composition,<br\/>sub typing, casts, and polymorphism must be adapted to handle<br\/>permissions; and what design choices can best make such a language<br\/>practical for writing programs of significant scale and design complexity.","title":"SHF :Small: Foundations of Permission-Based Object-Oriented Languages","awardID":"1116907","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["498106"],"PO":["564588"]},"181270":{"abstract":"Visual search skills for hazard perception are critical in many domains. They are used by pilots to maintain situation awareness, by doctors reviewing screen images to diagnose health disorders, and by security screeners inspecting for hazardous materials. They are also critical to a novice driver's ability to detect roadway hazards. For novice drivers, poor visual search skills can increase the risk for traffic fatalities, which are the leading cause of death for teenagers nationwide. <br\/><br\/>Virtual reality can be used to provide visual search training during driver education. However, the success of these training programs depends on the validity of eye-scanning patterns in the virtual environment and on the simulation parameters employed to elicit those patterns. The overall objective of the research is to assess the validity of search patterns in the virtual world as they apply to the physical realm, and then to identify which simulation parameters are critical for successful simulator-based hazard perception training. <br\/><br\/>Intellectual merit: The project will integrate theory and data to advance our knowledge of which parameters of virtual reality-based hazard perception training promote the greatest transfer of training to real world driving. Specifically, the project will (a) examine the relationship between visual search in driving simulators vs. that in the real world, using specially equipped vehicles; (b) test which aspects of the driving simulator lead to the best training and transfer of learning; and (c) examine how age and experience influence hazard perception skills. <br\/><br\/>Broader impacts: The findings from the project will advance scientific understanding of how virtual reality training simulations can be used to improve driver education, thereby leading to better drivers and fewer traffic accidents. The results will also be applicable to other areas in which virtual reality training can be valuable, such as medical diagnosis and security screening. In addition, the project will enable a number of disadvantaged students to take part in engineering and technology research and includes a high school outreach component to engage students in research at an early age.","title":"HCC: Small: Modeling the Validity and Transfer of Eye-Scanning Patterns for Hazard Perception from Virtual Reality Training Environments to Reality","awardID":"1116378","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[485725,485726,485727],"PO":["564456"]},"181072":{"abstract":"Machine learning currently offers one of the most cost-effective approaches to building predictive models (e.g., classifiers for categorizing the millions of messages, news articles, and blogs that are generated every day). However, the effective use of machine learning methods in such settings is limited by the availability of a training corpus (i.e., a representative set of instances that have been labeled with the correponding categories). In domains where labeled data are scarce or expensive to acquire, there is an urgent need for cost-effective approaches to selectively acquiring labels for data samples used to train predictive models using machine learning. <br\/><br\/>This project explores novel techniques that take advantage of the low cost of micro-outsourcing using systems such as Amazon's mechanical Turk, to engage a large number of workers from around the world for acquiring the labels of instances to be used to construct the training corpus. There is currently little understanding of how to utilize the multiple noisy labels obtained using micro-outsourcing. There is a need for advanced techniques for taking advantage of the low cost of micro-outsourcing in order to improve data quality and the quality of models built from the available data. It explores novel approaches for utilizing multiple labels given to an instance by different labelers. It also extends active learning techniques for active selection of samples to be labeled to take into account the multi-sets of labels that have been already obtained from a pool of labelers. <br\/><br\/>Advances in techniques for active selection of data instances to be labeled in a micro-outsourcing setting can significantly improve the quality of data used to build predictive models in a broad range of applications, including gene annotation, image annotation, text classification, sentiment analysis, and recommender systems, where unlabeled data are plentiful yet labeled data are sparse. The project will provide research opportunities for students at University of Central Arkansas, a primarily undergraduate institution and help expand the STEM pipeline. Additional information about the project can be found at: http:\/\/sun0.cs.uca.edu\/~ssheng\/.","title":"III: Small: RUI: Improving Data Quality and Data Mining Using Noisy Micro-Outsourcing","awardID":"1115417","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[485258],"PO":["565136"]},"185242":{"abstract":"ACM Special Interest Group of Multimedia, ACM Multimedia has contributed significantly to the advance of all aspects of multimedia research, technologies and applications since 1993. Through the continuous efforts of the community, ACM Multimedia has become a dynamic and comprehensive program for publication, education, and interaction, including presentation and discussion of research papers, participation in tutorials, demos, doctoral symposium, industrial grand challenge competitions, and art exhibitions. All these activities provide a unique opportunity for students to share their knowledge, experience, and their current research with internationally recognized researchers from both academia and industry. ACM Multimedia 2011 (ACM MM 2011) is to be held in Scottsdale, Arizona, November 28 - December 1, 2011 and is expected to attract many US and international attendees from academia and industry. <br\/><br\/>This award provides support for ACM MM 2011 education related events, namely the Doctoral Symposium, Face-to-face Meeting with Leading Researchers, the Open Source Competition and partial travel expenses and registration for about 20 US-based students; in particular, female and minority students and students presenting in the doctoral symposium and participating open source software competition. A Female Student Mentoring Workshop will be supported by a separate funding from the ACM SIGMM.<br\/><br\/>The intellectual merit includes the opportunity for students to learn about the cutting edge research and interact with experts in the top multimedia conference. The broad impact is to train and develop the future generation of leaders and workforce in this critical field, as well as enhancing the participation of women and minority students in multimedia research.<br\/><br\/>The ACM Multimedia proceedings are published by ACM. The student award application procedure and results will be announced at the ACM MM 2011 conference website (http:\/\/www.acmmm11.org).","title":"Support for the Educational Activities at ACM Multimedia 2011","awardID":"1138910","effectiveDate":"2011-08-01","expirationDate":"2013-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["196924",497035],"PO":["563751"]},"186133":{"abstract":"The work of the proposed Message Passing Interface Forum addresses the continuation of activity focused on enhancing the Message Passing Interface (MPI) for passing data between processors in distributed computing systems built from heterogeneous multi-cores, for parallel remote file operations, and for dynamic process control of such parallel applications. The current standard provides a core of routines used for point-to-point and one-sided data exchange, blocking collective communications routines, remote parallel file access and dynamic process control. In addition, these are augmented with support for features such as derived data types, communications contexts, process groups, and virtual process topologies. <br\/>The work of this forum will produce a communications standard suitable for addressing the needs of parallel applications on the next generation computing facilities at the petascale level and beyond.","title":"Proposed Meeting Series: The Message Passing Interface Forum","awardID":"1144042","effectiveDate":"2011-08-01","expirationDate":"2012-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["558550","558548"],"PO":["565272"]},"177322":{"abstract":"This research project develops hardware software interfaces to enable standard<br\/>software programs to benefit from specialized hardware accelerators.<br\/>Standard CPUs are general purpose meaning that they are designed to execute<br\/>all types of applications -- from graphics processing in video games to<br\/>spreadsheets. Because of the diversity of workloads they are expected to<br\/>execute, general purpose processors can never be overly specialized for any<br\/>individual one. By contrast, processors which are tailored to a single<br\/>workload or workload domain are narrower in their applicability but more<br\/>efficient at executing applications in their target domains.<br\/><br\/>This research project examines specialized processors that target abstract<br\/>datatypes: moderate to large scale data structures and their associated<br\/>operations. Modern software engineering practice encourages the use of such<br\/>abstract types to improve programmer efficiency and software reliability.<br\/>Abstract datatype processors thus align with these practices making the<br\/>mapping of software to hardware intuitive and streamlined. By delivering<br\/>energy-efficient, application-specific hardware in an easy-to-program fashion,<br\/>this research empowers programmers to write faster software that consumes less<br\/>energy.<br\/><br\/>The research activities span three fields of computer science: Hardware system<br\/>and architecture research is carried out in software simulation. This portion<br\/>of the research explores multiple aspects of the hardware system including<br\/>efficient implementations of software-style polymorphism and mechanisms to<br\/>enforce data encapsulation. The project is grounded in a specific,<br\/>performance-critical, real-world problem of database query processing. This<br\/>component of the research identifies target types for hardware acceleration<br\/>that are used in common, complex database operations such as range<br\/>partitioning. Performance results will be obtained both by direct measurement<br\/>and by simulation. Finally, the compiler segment of the project develops<br\/>compiler techniques to link high-level languages to the accelerators available<br\/>on the target hardware system. The compiler adapts software at runtime to<br\/>best utilize the available accelerators and to partition code among<br\/>general-purpose and specialized processing cores. <br\/><br\/>As power and energy become the most critical resource in computing systems,<br\/>mechanisms to conserve these resources have come under significant scientific<br\/>scrutiny. The broader impact of this research project is to harness software<br\/>abstractions in specialized, energy efficient hardware. By prioritizing<br\/>programmability and efficiency equally, this research strives for usable,<br\/>energy-efficient, next-generation computing platforms. As a first area of<br\/>application, this project targets databases, the software engines that store<br\/>and index and compute on society's information.","title":"SHF: Medium: Type-Specific Instruction Processing","awardID":"1065338","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["531167","530445","534424"],"PO":["565272"]},"186155":{"abstract":"The goal of this project is to provide a unique opportunity for students to present their research results, learn the cutting edge research, and interact with internationally recognized researchers from both academia and industry at the ACM SIGMOD 2012 (International Conference on Data Management). As one of the most prestigious conferences in data management research, ACM SIGMOD has contributed significantly to the advance of all aspects of data management technologies and applications since 1975. Today ACM SIGMOD is a dynamic and comprehensive program for publication, education, and interaction; and it is a leading international forum for database researchers, practitioners, developers, and users to explore cutting-edge ideas and results and to exchange techniques, tools, and experiences. This project provides partial support for students matriculated at U.S. institutions, especially female and minority students, to attend and present their research work at ACM SIGMOD 2012 in Scottsdale, Arizona. Besides meeting with researchers from academia and industry during regular program sessions, in SIGMOD 2012 students are able to participate in the following interactive activities as partially supported by this project: a new researcher symposium, an undergraduate research poster competition, vis-a-vis meetings with leading researchers, and a female student mentoring workshop. These opportunities will have a long lasting impact on the future career of the participants. The broader impact is to train the future generation of leaders and workforce in the critical field of data management. The project details are available via the website http:\/\/www.sigmod.org\/2012\/.","title":"Student Research and Educational Activities at ACM SIGMOD 2012","awardID":"1144103","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["558560","233230"],"PO":["543481"]},"176167":{"abstract":"The goal of this project is to create a linguistically annotated, publicly available, and easily searchable corpus of video from American Sign Language (ASL). This will constitute an important piece of infrastructure, enabling new kinds of research in both linguistics and vision-based recognition of ASL. In addition, a key goal is to make this corpus easily accessible to the broader ASL community, including users and learners of ASL. As a result of our long-term efforts, we have an extensive collection of linguistically annotated video data from native signers of ASL. However, the potential value of these corpora has been largely untapped, notwithstanding their extensive and productive use by our team and others. Existing limitations in our hardware and software infrastructure make it cumbersome to search and identify data of interest, and to share data among our institutions and with other researchers. In this project, we propose hardware and software innovations that will constitute a major qualitative upgrade in the organization, searchability, and public availability of the existing (and expanding) corpus.<br\/><br\/>The enhancement and improved Web-accessibility of these corpora will be invaluable for linguistic research, enabling new kinds of discoveries and the testing of hypotheses that would otherwise have be difficult to investigate. On the computer vision side, the proposed new annotations will provide an extensive public dataset for training and benchmarking a variety of computer vision algorithms. This will facilitate research and expedite progress in gesture recognition, hand pose estimation, human tracking, and large vocabulary, and continuous ASL recognition. Furthermore, this dataset will be useful as training and benchmarking data for algorithms in the broader areas of computer vision, machine learning, and similarity-based indexing. <br\/><br\/>The advances in linguistic knowledge about ASL and in computer-based ASL recognition that will be accelerated by the availability of resources of the kind proposed here will contribute to development of technologies for education and universal access. For example, tools for searching collections of ASL video for occurrences of specific signs, or converting ASL signing to English, are still far from attaining the level of functionality and usability to which users are accustomed for spoken\/written languages. Our corpora will enable research that aims to bring such vision-based ASL recognition applications closer to reality. Moreover, these resources will afford important opportunities to individuals who would not otherwise be in a position to conduct such research (e.g., for lack of access to native ASL signers or high-quality synchronized video equipment, or lack of resources\/expertise to carry out extensive linguistic annotations). Making our corpora available online will also allow the broader community of ASL users to access our data directly. Students of ASL will be able to retrieve video showing examples of a specific sign used in actual sentences, or examples of a grammatical construction. ASL instructors and teachers of the Deaf will also have easy access to video examples of lexical items and grammatical constructions as used by a variety of native signers for use in language instruction and evaluation. Thus, the proposed web interface to our data collection will be a useful educational resource for users, teachers, and learners of ASL.","title":"Collaborative Research: CI-ADDO-EN: Development of Publicly Available, Easily Searchable, Linguistically Analyzed, Video Corpora for Sign Language and Gesture Research","awardID":"1059281","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["522465"],"PO":["565227"]},"181480":{"abstract":"This project anticipates significantly speeding up prediction of dust storms. The PIs have successful preliminary work in the area in collaboration with relevant domain experts at federal agencies such as NOAA and FEMA. <br\/><br\/>The major challenges in this project are geospatial interoperability and format heterogeneity across data sources. The PIs speed up collection of high-volume continuous sensor data, run dust-storm prediction models, and disseminate detailed prediction data to users such as emergency managers. The approach to speeding-up the first task is based on reduction of data via data differencing. Challenges in choice of data differencing schemes include client-server load-balancing, diversity of geospatial data types (e.g. raster, vector), and possibility of errors\/missing data in some snapshots, which may be magnified via difference techniques leading to errors in dust forecast. Speed up for prediction models is done by customizing caching and computation job scheduling to dust prediction models. For example, a location?s potential dust storm is predicted from properties of nearby places based on wind direction etc. To speed-up result dissemination, the project studies geospatial data access patterns and develops custom pre-fetching techniques to be able to serve a large number of concurrent users. <br\/><br\/>The results of this project will benefit society by improving forecasting of dust storms. Custom techniques for data difference, caching, job-scheduling and pre-fetching may also benefit other societal applications such as weather prediction. It will also lead to curriculum development as well as training of graduate and undergraduate students.","title":"CSR: Small: System Research to Advance Real-time Dust Storm Forecasting","awardID":"1117300","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["562674",486233],"PO":["551712"]},"180391":{"abstract":"This research involves collaboration among investigators at three institutions. The PIs anticipate a future in which humans and intelligent robots will collaborate on shared tasks. To achieve this vision, a robot must have sufficiently rich knowledge of the task domain and that knowledge must be usable in ways that support effective communication between a human and the robot. Navigational space is one of the few task domains where the structure of the knowledge is sufficiently well understood for a physically-embodied robot agent to be a useful collaborator, meeting genuine human needs. In this project, the PIs will develop and evaluate an intelligent robot capable of being genuinely useful to a human, and capable of natural dialog with a human about their shared task.<br\/><br\/>The Hybrid Spatial Semantic Hierarchy (HSSH) is a human-inspired multi-ontology representation for knowledge of navigational space. The spatial representations in the HSSH provide for efficient incremental learning, graceful degradation under resource limitations, and natural interfaces for different kinds of human-robot interactions. Speech is a natural though demanding way to use natural language to communicate with a robot. To maintain real-time performance, natural language understanding must be organized to minimize the amount of backtracking from early conclusions in light of later information. This project will answer three scientific questions.<br\/><br\/>(1) Can the HSSH framework, extended with real-time computer vision, express the kinds of knowledge of natural human environments that are relevant to navigation tasks? <br\/>(2) Can the HSSH representation support effective natural language communication in the spatial navigation domain? <br\/>3) Can we develop effective human-robot interaction that meets the needs of a person and improves the performance of the system?<br\/><br\/>To these ends, the PIs will perform this research with two different kinds of navigational robots, each learning from its travel experiences and building an increasingly sophisticated cognitive map: an intelligent robotic wheelchair which carries its human driver to desired destinations, and a telepresence robot that transmits its perceptions to a remote human driver as it navigates within an environment so the driver can achieve virtual presence and communicate with others remotely. To inform the design process, the PIs will conduct focus groups with potential users. They will also evaluate their implemented systems throughout the process, creating an iterative design-test cycle.<br\/><br\/>Broader Impacts: To be successful, an intelligent robot must not only be able to perceive the world, represent what it learns, make useful inferences and plans, and act effectively. It must also be able to communicate effectively with other agents, and particularly with people. This confluence among grounded knowledge representation, situated natural language understanding, and human-robot interaction is intellectually fundamental, and is the focus of this research. Since the domain of spatial knowledge is foundational for virtually all aspects of human knowledge, project outcomes will have broad applicability. This work will create technologies for mobility assistance for people with disabilities in perception (blindness or low vision), cognition (developmental delay or dementia), or general frailty (old age). It will also support telepresence applications such as telecommuting, telemedicine and search and rescue. The project includes outreach to K-12 and community college students, K-12 teachers, and the public in a number of venues.","title":"HCC: Large: Collaborative Research: Human-Robot Dialog for Collaborative Navigation Tasks","awardID":"1111125","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["526078"],"PO":["565227"]},"181370":{"abstract":"This project explores the design of green cellular network architectures and algorithms to reduce energy consumption. A major consumer of energy in cellular networks is the operation of the base stations. Most base stations are deployed and operated continuously based on peak traffic estimates. Intuitively, it saves energy to judiciously \"scale back\" un-utilized and under-utilized base stations during off-peak times. The remaining base stations must adapt accordingly so that the required coverage is maintained.<br\/><br\/>The PIs propose a dynamically adaptive cellular network where the cell size and capacity of the base stations are dynamically reconfigured based on the current user locations and Quality of Service (QoS) demands of the applications. Modeling of the network power consumption based on the network topography, followed by optimization of the topography is the key to this problem. The project will involve student participation at all stages through suitably designed projects. The results of the research will be disseminated through professional journals and conference proceedings and also through industry forums and workshops. Furthermore, the project addresses one of the most pressing issues of today, namely, reducing the carbon footprint of the cellular network. Radical new designs for the network have the potential to spur economic development, create jobs and forge stronger communities.","title":"NeTS Small: Collaborative Project: Towards a Green Cellular Network through User and Application Aware Dynamic Cell Reconfiguration","awardID":"1116874","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["547701","547702"],"PO":["565303"]},"181260":{"abstract":"In this age of identity theft, Facebook, and TSA screening, protecting confidential information from improper disclosure has emerged as a fundamental issue for trustworthy computing, involving both technical and social dimensions. While it is sometimes possible to stop undesirable information flows completely, it is perhaps more typical that some undesirable flows are unavoidable. For instance an ATM machine that rejects an incorrect PIN thereby reveals that the secret PIN is not the one that was entered. Similarly, revealing the tally of votes in an election reveals some information about the secret ballots that were cast. More subtly, the amount of time taken by a cryptographic operation may be observable by an adversary, and may inadvertently reveal information about the secret key. As a result, there is growing interest in quantitative theories of information flow, which allow us to talk about \"how much\" information is leaked and (perhaps) allow us to tolerate \"small\" leaks. But while it is tempting to base such theories on classic information-theoretic concepts like Shannon entropy and mutual information, these turn out not to provide very satisfactory confidentiality guarantees. As a result, several researchers have developed an alternative theory, based instead on Renyi's min-entropy, which gives strong, direct operational security guarantees.<br\/><br\/>This project aims to deepen our understanding of the theory and applications of min-entropy leakage by pursuing several themes concurrently. In the theory of min-entropy leakage, uncertainty is measured in terms of a random variable's vulnerability to being guessed in one try by an adversary; note that this is the complement of the Bayes Risk. The mathematical properties of this theory will be explored for both deterministic and probabilistic systems, with the goal of better understanding the relationship to other theories, such as mutual information leakage and differential privacy. A variant called smooth min-entropy leakage will be developed, giving a measure that is less sensitive to extremely unlikely events. To support the compositional analysis of complex systems, leakage bounds will be established for channels in cascade, where the output of one channel becomes the input to another; moreover, algorithms will be developed for approximately factoring a channel into a cascade, giving a technique for automatic sanitization. Techniques will be developed for computing the min-entropy leakage of systems, giving a way to verify whether they conform to a given quantitative flow policy; both model-checking and statistical-sampling based techniques will be considered. Finally, applications of min-entropy leakage will be explored, for example to accountability systems that use logging and auditing to identify misbehaving entities. These efforts will develop new theory, enforcement techniques, and applications of min-entropy leakage, contributing broadly to a rigorous science of quantitative information flow.","title":"TC: Small: Theory and Applications of Min-Entropy Leakage","awardID":"1116318","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["528350"],"PO":["564388"]},"181150":{"abstract":"Existing compilation techniques are unlikely to release the full potentials of the new many-simple-core architectures, where a processor consists of tens, hundreds or more cores that do not have fine grain instruction-level-parallelism. The fundamental reason is that traditional compilation technology is largely designed to maximize single-thread performance while the same strategy will likely hinder the realization of the best overall performance when programs on many-simple-core processors routinely run hundreds or even more threads. A many-simple-core processor usually delivers the highest performance when its resource is fully utilized. However, a thread compiled in the traditional way has all resources at its disposal to maximize its own performance. When applied to hundreds of threads, the resource requirement adds up and will create a resource bottleneck that is much more likely than the performance of single thread to be the main performance issue. More generally speaking, the question of what is compiler optimization needs to be redefined in the new context of many-simple-core processors.<br\/><br\/>This research improves the tradeoff between single-thread performance and overall resource utilization. The project includes a systematical study of techniques and strategies that are needed to adapt existing compiler and code optimization techniques to the new many-simple-core processors, and the development of new optimization techniques and strategies that are oriented specifically to improve resource utilization on the new architecture. Specifically, the investigators study the modeling of shared resource utilization in compiler, compiler optimizations and code transformations that redefine the profitability analysis of existing techniques, and optimization strategy that orchestrates the selection of compiler transformations and their parameter values to \"de-optimize\" or \"optimize\" single thread to improve overall performance.","title":"SHF: Small: De-optimizing Compilation for Many-Simple-Core Processors","awardID":"1115771","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["525208"],"PO":["565272"]},"191072":{"abstract":"Visual surveillance is widely used in military and commercial applications, and public transportation scenarios. There are an estimated 30 million surveillance cameras in the U.S. capturing 4 billion hours of footage a week. The large amounts of video data generated by these cameras necessitate automatic detection of semantically high-level events, since the attention of a human operator watching multiple video feeds decreases over time. In addition, requiring cameras to have access to electrical outlets and to have wired links hinders system flexibility in terms of the number and placement of cameras, incurs significant costs, and limits possible applications and mobility. Embedded smart cameras allow us to deploy many spatially-distributed cameras interconnected by wireless links. A smart camera combines sensing, processing, and communication on a single embedded platform. Yet, it has very limited energy and processing power. The objective in this project is to build a battery-powered, self-adapting, wireless embedded smart camera system for the detection of semantically high-level events, which can span multiple overlapping or non-overlapping camera views, in a scalable and energy-efficient manner, and remove the dependence on wired links. Resource-aware and distributed object tracking and event detection algorithms, and self-adapting decision methodologies will be developed to decrease the energy consumption, and increase the battery-life of cameras. <br\/><br\/>The outcomes are expected to have important positive impact, because they will fundamentally transform video surveillance and event detection solutions by addressing the privacy issues simultaneously. This technology can be used across disciplines and in wide-ranging areas, including next-generation cyber-physical systems, military and commercial applications, traffic analysis, health care and wildlife monitoring. Outreach to younger children is planned through the deployment of this project at the local Children?s Zoo and Museum.","title":"CAREER: Smart Cameras Getting Smarter: Detecting High-level Events Across Battery-powered Wireless Embedded Smart Cameras","awardID":"1206291","effectiveDate":"2011-08-06","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["542021"],"PO":["565255"]},"181293":{"abstract":"The project studies three important domains which highlight physical Human-Robot Interaction involved in assisting humans in the home and workplace: (1) human guidance through cluttered environments using physical contact, (2) cooperative carrying of large objects through complex and dynamic environments, and (3) robot assisted sitting and getting up. This is accomplished through the evolution of an experimental single-wheel mobility platform into an autonomous robot that is instructed and guided by people in a natural way.<br\/><br\/>Such systems are needed in many public health domains, including care for the elderly, rehabilitation and assistive programs. Project results are incorporated into coursework offered by two different departments at CMU, and exposes students to unique robot planning, control and Human-Robot Interaction issues. High school students are introduced to this area as part of the Andrew Leap program. Students from underrepresented groups participate through the ARTSI program. Results may be presented in cooperation with museums or entertainment companies.","title":"RI: Small: Physical Interaction with Dynamically Stable Mobile Robots","awardID":"1116533","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["524687","513331"],"PO":["564069"]},"185694":{"abstract":"Collaborative Projects: EAGER: A virtual eXchange to support networks of creativity and innovation amongst Science, Engineering, Arts and Design (XSEAD)<br\/><br\/>Intellectual Merit<br\/>One of the greatest challenges facing the United States in research and education is how to fundamentally encourage innovation across all sectors and spawn new solutions to address global challenges. Increasing research evidence and industrial innovations (i.e. mobile computing, social media) confirm that broad interdisciplinary collaborations that include both science and art fields have great potential for spawning creativity and innovation in computer science, engineering and the sciences. An emerging hybrid community of scientists, engineers, artists and designers is producing innovative and entrepreneurial research that advances new knowledge and proposes holistic solutions to societal challenges including health, education and environmental change. Yet, this burgeoning interdisciplinary community continues to face problems in its efforts to self-organize among constraints imposed by academic systems and historical biases; it continues to seek a dynamic and synergizing research and outreach exchange.<br\/><br\/>Building upon lessons-learned, a new Virtual eXchange to support networks of creativity and innovation amongst Science, Engineering, Art and Design (XSEAD) will be developed. The XSEAD project will address the following urgent needs of the interdisciplinary science-art community: establish a cohesive view of the field and provide a mechanism to attract entrepreneurs and industry; create a venue for multimodal documentation of research outcomes; provide extensive databases of prior and current research; allow rapid dissemination of research outcomes; facilitate forming of collaborations and specialized sub-communities; document and help evolve science-art curricula efforts and evaluation approaches; provide context and support mechanisms for science-arts careers; establish evidence of the societal impact of interdisciplinary science-art integration. The software engineering development components of XSEAD will contribute further knowledge in three technical areas: Content organization (improve the effectiveness of algorithms for dynamic, usage based, organization of large multimedia databases); Recommendation algorithms (promote the use of multi-relational structures for providing effective recommendations); Community dynamics (develop novel algorithms to extract structures that encode meaningful interactions in online social networks).<br\/><br\/>Broader Impact<br\/>XSEAD will expose general non-expert audiences to the evolution and potential of collaborative research across science and arts. It will attract the interest of young people searching for careers that combine the rigor of science and engineering with the creativity and reflection of arts and design. It will serve teachers and informal learning communities seeking exemplars for curricular development, active practitioners looking for further institutional opportunities to present and support their ongoing work, academics developing related interdisciplinary efforts and commercial companies seeking cross-trained expertise. XSEAD will enable rapid research exchange and in-depth peer-reviewed scholarship between the worlds of science and art and provide a unique and deeply engaging inroad to a vast and creative repository. XSEAD will help promote new paradigms for developing human centric solutions to complex societal problems (i.e. cost effective health and wellness, globalization and conflict, adaptive K-12 learning, electronic communication and security). These paradigms will combine knowledge across broad and diverse areas of human knowledge.","title":"Collaborative Research: EAGER: A Virtual eXchange to Support Networks of Creativity and Innovation Amongst Science, Engineering, Arts and Design (XSEAD)","awardID":"1141414","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["528809"],"PO":["565342"]},"186431":{"abstract":"Because both information and connectivity are more available today than ever before thanks to digital technologies, questions can now be addressed by enlisting massive human demographics to supplement the limitations of computer computation. This is especially relevant in the case of visual analytics, where human intuition remains far superior to existing computer object recognition algorithms. While algorithms are limited by pre-labeling requirements, humans can perceive subtle variations and nuances to identify and classify unexpected objects. These tasks, however, are often too massive in scale for a single human to accomplish. Distributing this task over a massive network not only succeeds in categorizing data, but generates massive quantities of human quantifiers (training data) to potentially teach computer vision algorithms to mimic human perception in order to distinguish the normal from the abnormal.<br\/><br\/>This exploratory project will combine collective human visual perception with machine learning and object recognition, through a study of 1.25 million crowd-sourced inputs provided by over 6,000 volunteers labeling satellite imagery in a search for anomalies in northern Mongolia. These data, collected from June 2010 to the present via an online platform developed by the PI in collaboration with National Geographic Digital Media, afford an ideal \"case study\" environment to investigate the nature of crowd generated data and methods that distill the wide variability of human input into computational algorithms. The online participants, excited by the potential of discovering the tomb of Genghis Khan, examined massive amounts of ultra-high resolution multispectral satellite imagery to label loosely defined anomalies into various categories. Trends that emerged from the massive volume of labels represent a collective human perspective on what the images contain. A team led by the PI traveled to Mongolia to ground-truth areas of high user input convergence. The resulting ground-truthed anomalies provide a unique opportunity to both accurately measure the quality of human\/automated analysis and to investigate the effect of supplementing noisy crowd-sourced data sets with small pools of absolute data in machine learning. In the current project the PI will develop a framework for applying and evaluating the following three research phases designed to study the nature of large scale human generated data for integration into supervised learning algorithms:<br\/><br\/>1. Consensus Clustering - Tag evaluation mechanisms based upon the volume and consistency of neighboring tags and the ability of the individuals creating those tags. Unsupervised methods for \"merging\" labels will also be applied for extended anomalies such as roads and rivers.<br\/><br\/>2. Feature Vector Extraction - Both the type of features (e.g., color, luminance, edges and gradients, scale, orientation, etc.) and the extent of the neighborhoods (e.g., local, wide and global) required to detect anomalies are unknown a priori. Thus, the aim is to determine sufficiently diverse features to capture all relevant cues within the image.<br\/><br\/>3. Machine Learning - Dominant features representative of, and excluded from, pixel groups of given categories will be determined from the results of Phase 2 above.<br\/><br\/>Broader Impacts: In this exploratory study the PI will lay the foundation for extracting new machine\/human collaborative opportunities from the resource of the crowd. Understanding the bonds between human and computer intelligence will have a profound impact on many branches of science. Thus, concepts developed in this effort may ultimately prove transformative by affording migration of crowd-sourcing from a project-based tool for distributed analytics into a portal bridging collective human perception and machine learning.","title":"EAGER: Human Computation: Integrating the Crowd and the Machine","awardID":"1145291","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["539187","557605",500272],"PO":["565227"]},"186211":{"abstract":"Computer science and biology have inspired each other by drawing analogies leading to new classes of algorithms such as neural networks and genetic algorithms and new fields such as computational biology and biocomputing (computing using biomolecules). The ever increasing data streams in everyday life as well as biology are most often characterized by networks, such as the internet, telephone network, disease transmission networks, social networks to name just a few. Networks consist of nodes connected by edges and only the nodes vary with the application areas. The network structures are conserved: the edges allow communication and information flow. Due to the size, complexity and dynamic nature of such networks, their control is challenging. As environments change, the structures of these networks change and are subject to numerous perturbations and failures. Nature faces these same types of challenges and has evolved robust strategies for ensuring that information is transmitted and that the system appropriately responds to changes in the environment: for example, the oxygen transport protein in the blood, hemoglobin, changes its affinity to oxygen in response to small molecule ligands favoring oxygen release where needed. The strategy that Nature employs in hemoglobin is called allostery.<br\/><br\/>Opportunity: Allostery is a biochemical term that refers to the ability of biomolecules, in particular proteins, to achieve action at a distance in the atomic network through a small, localized perturbation. Proteins can be viewed as networks of atoms interacting in three-dimensional space. Allostery is a classical text-book example of an experimentally well studied and firmly established mechanism of control of this atomic network. Here, PIs propose the hypothesis that one can share the biochemical principle of allostery with other domains such as disease transmission, social networks, economics, surveillance applications and cloud computing. Understanding how Nature performs acquisition, transmission and processing of information at the molecular level may lead to future enabling technologies in other domains. <br\/><br\/>Intellectual Merit: While the proposed hypothesis is potentially transformative, in-depth pursuit requires obtaining a proof-of-concept outlining (1) what kinds of mechanisms might exist in proteins that could be transferred to other domains and (2) develop an understanding of what are the requirements for such a transfer. Although allostery in proteins is an established biochemical principle, little is known how it works and how to predict it. Some proteins are regulated through allostery and others are not. Unfortunately, there is no simple property that determines whether a given protein is (or can be) allosterically regulated. While experimental methods provide direct evidence for allostery, they generally do not reveal the detailed physical and biological mechanisms for it. PIs propose to reveal these mechanisms through the combination of computation and experiments: a predicted path of communication between two distant sites can be validated or refuted by disrupting this path. The deliverables of this work will be a list of fundamental principles of allostery in proteins, and an analysis of their suitability for future extension to non-protein related networks. <br\/><br\/>Broader Impact: This grant will support investigators and train graduate students from the areas of computer science, chemistry, biology & biomedicine. Convergence of technologies, here between protein allostery modeling and network control, is expected to speed up scientific progress in potentially many disciplines. Thus, one may in the future be able to push the field of biocomputing forward, predict disease outbreaks or identify action at a distance in economic networks.","title":"EAGER: Exploring the biochemical principle of allostery for algorithm development","awardID":"1144281","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[499671,499672],"PO":["565223"]},"186101":{"abstract":"This grant will enhance participation in the third Midwest Verification Day 2011 (MVD 11), an informal<br\/>workshop with the goal of developing a regional research network in the broad area of verification<br\/>and formal methods. The long-term intention is to make MVD a regular annual event, rotating between universities in the midwest that have representation in the relevant research areas. A major focus of MVD is to provide undergraduate and graduate students a means for becoming familiar with ongoing research in the midwest region in the application of formal methods in hardware and software analysis. The workshop also aims to afford students a relaxed, non-competitive forum for presenting their own research to industrial participants and to students and faculty from neighboring universities.<br\/><br\/>For the significant number of graduate and undergraduate students who originate in the Midwest, such an event can help build the connections and the confidence to go on to graduate school, or advance further in academia. This helps strengthen the domestic talent pool for formal methods. MVD 11 will also strengthen the regional connections between academia and industry, which, over the long term, can have a potent regional economic effect.","title":"Midwest Verification Day, 2011","awardID":"1143933","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[499410,"536974"],"PO":["565264"]},"185386":{"abstract":"This proposal aims to secure travel support for the seventh ACM\/IEEE Symposium on<br\/>Architectures for Networking and Communications Systems (ANCS), which will be held in Brooklyn, NY (Oct. 3-4, 2011). Specifically, the PI?s interest is in providing travel support to ANCS for students. The ANCS conference has been created to provide a top tier venue to bring together researchers from the areas of computer systems architecture and networking systems, in order to foster greater collaboration between these increasingly related research areas.<br\/>It is proposed to offer up to 15 NSF-sponsored student travel grants for a total budget of $15,250. Applications will be solicited via email mailing lists, personal contacts, and the conference web site. The selection process will consider the applicant?s level of participation in ANCS, followed by how closely the student?s research interests match areas covered by ANCS.","title":"Support for the Symposium on Architectures for Networking and Communications Systems","awardID":"1139882","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["550551"],"PO":["565272"]},"177323":{"abstract":"Information theory, communication theory, and statistical signal <br\/>processing have proven spectacularly successful as innovation engines <br\/>for communication, data compression, and information processing <br\/>technologies. Although in principle these theories are quite general, <br\/>in practice they are most useful for a particular family of models, <br\/>such as discrete Markov sources and channels with additive <br\/>Gaussian noise. Although this family is quite rich and encompasses <br\/>many practical technologies, there are several important scenarios that<br\/>fall outside of these models. In particular, the timing of discrete events is <br\/>the modality of interest in many applications, such as neuroscience<br\/>and certain problems in network security, and this modality does <br\/>not fit neatly into the standard classes. This research involves<br\/>extending information theory, communication theory, and statistical signal<br\/>processing to develop a general theory of information transfer via timing. <br\/><br\/>The first phase of this research involves solving several concrete<br\/>problems involving information transfer via timing, such as the<br\/>secrecy capacity of timing channels and how to minimize information<br\/>transfer via timing side channels in both wireless and wireline<br\/>networks. The second phase involves interconnecting these disparate<br\/>problems to form a general theory of information transfer via timing.<br\/>By elevating the role of timing in information transfer,<br\/>this research better positions the fields of information theory,<br\/>communication theory, and statistical signal processing to<br\/>impact allied fields such as networking, neuroscience, and <br\/>operations research, where timing and delay are of fundamental<br\/>importance. This impact is facilitated through special topics <br\/>courses and invited sessions at conferences, both of which are <br\/>designed to attract participants from a range of areas.","title":"CIF: Medium: Collaborative Research: Toward a General Theory of Information Transfer via Timing","awardID":"1065352","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":["518371"],"PO":["564924"]},"177213":{"abstract":"With tremendous amounts of data existing in scientific applications, database management becomes a critical issue, but database technology is not keeping pace. This problem is especially acute in the long tail of science: the large number of relatively small labs and individual researchers who collectively produce the majority of scientific results. These researchers lack the IT staff and specialized skills to deploy technology at scale, but have begun to routinely access hundreds of files and potentially terabytes of data to answer a scientific question. This project develops the architecture for a database-as-a-service platform for science. It explores techniques to automate the remaining barriers to use: ingesting data from native sources and automatically bootstrapping an initial set of queries and visualizations, in part by aggressively mining a shared corpus of data, queries, and user activity. It investigates methods to extract global knowledge and patterns while offering scientists access control over their data, and some formal privacy guarantees. The Intellectual Merit of this proposal consists of automating non-trivial cognitive tasks associated with data work: information extraction from unstructured data sources, data cleaning, logical schema design, privacy control, visualization, and application-building. As Broader Impacts, the project helps scientists reduce the proportion of time spent \"handling data\" rather than \"doing science.\" All software resulting from this project are open source, and all findings are disseminated broadly through publications and workshops. Sustainable support for science users of the software is coordinated through the University of Washington eScience Institute. The research is incorporated in both undergraduate and graduate computer science courses, and the software is also incorporated into domain science courses as well. The project's outreach activities include advising students through special programs geared toward under-represented groups such as the CRA-W DREU. More information about this project is found at http:\/\/escience.washington.edu\/dbaas.","title":"III: Medium: Collaborative Research: Database-As-A-Service for Long Tail Science","awardID":"1064685","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["483300"],"PO":["565136"]},"187025":{"abstract":"The specification of goals in the form of utility or reward functions is a cornerstone of most approaches to developing autonomous artificial agents, and to understanding and shaping the behavior of natural biological agents - in fields ranging from control, AI, economics, psychology, and ethology. But in practice there are actually two notions of goals: the (human or evolutionary) designer's goals and the (artificial or natural) agent's goals. Should these be the same? The conventional (implicit) answer is \"yes\", but new work by the PIs shows that the answer may be \"no\" for computationally bounded agents. We define a new problem in agent design, the optimal rewards problem, whose solution is a reward function to assign to the agent so that in attempting to maximize its reward the agent best achieves the designer's goals. This project explores optimal rewards, through computational experimentation and analysis, on two broad fronts. We will develop new principles and algorithms for bounded planning agents, demonstrating increased performance over using conventional rewards, even when taking into account the cost of finding the optimal rewards. We will make significant advances in two areas of behavioral economics and ethology: the understanding of subjective utility in humans, and the understanding of foraging behavior in animals, by using optimal reward theory to rigorously derive subjective reward functions that take into account the computational limits of the natural agents. The results of this work, disseminated through published theory and software, should lead to foundational changes in the way we understand and design incentive structures for humans, animals, and artificial agents, and thus to significant practical benefits for any methods in engineering, education, economics, and health that depend on finding good incentives for desired behavioral outcomes.","title":"EAGER: On the Optimal Rewards Problem","awardID":"1148668","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8052","name":"Inter Com Sci Econ Soc S (ICE)"}}],"PIcoPI":["503531","550371"],"PO":["562760"]},"186068":{"abstract":"Automated decision support systems help users make informed and intelligent choices over a set of alternatives, taking into account user preferences and trade-offs among multiple system attributes. In the software engineering domain decision support systems are used to help in evaluating alternative design, technical and managerial choices in terms of quantitative preferences and trade-offs. Preferences over alternatives are evaluated either by directly soliciting from the stakeholders a measure of the perceived utility\/value of each attribute, or by quantifying such utility\/value based on past experience and expertise. In most practical settings, however, preferences over attributes cannot all be quantified. On the other hand, considering preferences only qualitatively (specifying them as simple relative orderings between alternatives) is also not practical. To overcome these limitations, the proposed research focuses on developing a new paradigm for decision support systems, where preferences are specified both in qualitative and quantitative terms.<br\/><br\/>The main thrust of this work will be to: (a) develop robust formalisms for representing and reasoning with quantitative and qualitative preferences in an unified fashion, (b) investigate application-domain specific extensions to the formalisms, and (c) identify implementation strategies for practical application of the decision support system as a preference analyzer. The anticipated results will help realize application-specific robust decision support systems in multiple domains, including product-line engineering, safety-critical system development, and goal-oriented requirements engineering, by enabling improved automated reasoning about preferences. This work will contribute to research-based training of a postdoctoral scholar and a graduate student in techniques that cut across software engineering, formal methods and artificial intelligence. Research results will be disseminated through publications in journals and conferences.","title":"EAGER: Decision Support System for Reasoning with Preferences","awardID":"1143734","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[499317,"531356"],"PO":["565264"]},"176157":{"abstract":"The goal of this project is to create a linguistically annotated, publicly available, and easily searchable corpus of video from American Sign Language (ASL). This will constitute an important piece of infrastructure, enabling new kinds of research in both linguistics and vision-based recognition of ASL. In addition, a key goal is to make this corpus easily accessible to the broader ASL community, including users and learners of ASL. As a result of our long-term efforts, we have an extensive collection of linguistically annotated video data from native signers of ASL. However, the potential value of these corpora has been largely untapped, notwithstanding their extensive and productive use by our team and others. Existing limitations in our hardware and software infrastructure make it cumbersome to search and identify data of interest, and to share data among our institutions and with other researchers. In this project, we propose hardware and software innovations that will constitute a major qualitative upgrade in the organization, searchability, and public availability of the existing (and expanding) corpus.<br\/><br\/>The enhancement and improved Web-accessibility of these corpora will be invaluable for linguistic research, enabling new kinds of discoveries and the testing of hypotheses that would otherwise have be difficult to investigate. On the computer vision side, the proposed new annotations will provide an extensive public dataset for training and benchmarking a variety of computer vision algorithms. This will facilitate research and expedite progress in gesture recognition, hand pose estimation, human tracking, and large vocabulary, and continuous ASL recognition. Furthermore, this dataset will be useful as training and benchmarking data for algorithms in the broader areas of computer vision, machine learning, and similarity-based indexing. <br\/><br\/>The advances in linguistic knowledge about ASL and in computer-based ASL recognition that will be accelerated by the availability of resources of the kind proposed here will contribute to development of technologies for education and universal access. For example, tools for searching collections of ASL video for occurrences of specific signs, or converting ASL signing to English, are still far from attaining the level of functionality and usability to which users are accustomed for spoken\/written languages. Our corpora will enable research that aims to bring such vision-based ASL recognition applications closer to reality. Moreover, these resources will afford important opportunities to individuals who would not otherwise be in a position to conduct such research (e.g., for lack of access to native ASL signers or high-quality synchronized video equipment, or lack of resources\/expertise to carry out extensive linguistic annotations). Making our corpora available online will also allow the broader community of ASL users to access our data directly. Students of ASL will be able to retrieve video showing examples of a specific sign used in actual sentences, or examples of a grammatical construction. ASL instructors and teachers of the Deaf will also have easy access to video examples of lexical items and grammatical constructions as used by a variety of native signers for use in language instruction and evaluation. Thus, the proposed web interface to our data collection will be a useful educational resource for users, teachers, and learners of ASL.","title":"Collaborative Research: CI-ADDO-EN: Development of Publicly Available, Easily Searchable, Linguistically Analyzed, Video Corpora for Sign Language and Gesture Research","awardID":"1059235","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["557541"],"PO":["565227"]},"177257":{"abstract":"Information theory, communication theory, and statistical signal <br\/>processing have proven spectacularly successful as innovation engines <br\/>for communication, data compression, and information processing <br\/>technologies. Although in principle these theories are quite general, <br\/>in practice they are most useful for a particular family of models, <br\/>such as discrete Markov sources and channels with additive <br\/>Gaussian noise. Although this family is quite rich and encompasses <br\/>many practical technologies, there are several important scenarios that<br\/>fall outside of these models. In particular, the timing of discrete events is <br\/>the modality of interest in many applications, such as neuroscience<br\/>and certain problems in network security, and this modality does <br\/>not fit neatly into the standard classes. This research involves<br\/>extending information theory, communication theory, and statistical signal<br\/>processing to develop a general theory of information transfer via timing. <br\/><br\/>The first phase of this research involves solving several concrete<br\/>problems involving information transfer via timing, such as the<br\/>secrecy capacity of timing channels and how to minimize information<br\/>transfer via timing side channels in both wireless and wireline<br\/>networks. The second phase involves interconnecting these disparate<br\/>problems to form a general theory of information transfer via timing.<br\/>By elevating the role of timing in information transfer,<br\/>this research better positions the fields of information theory,<br\/>communication theory, and statistical signal processing to<br\/>impact allied fields such as networking, neuroscience, and <br\/>operations research, where timing and delay are of fundamental<br\/>importance. This impact is facilitated through special topics <br\/>courses and invited sessions at conferences, both of which are <br\/>designed to attract participants from a range of areas.","title":"CIF: Medium: Collaborative Research: Toward a General Theory of Information Transfer via Timing","awardID":"1065022","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":[475205,"475207",475207],"PO":["564924"]},"180480":{"abstract":"Computer networks are now, arguably, the United States' most critical infrastructure. They control all communication amongst our citizenry, our businesses, our government, and our military. Worryingly, however, today's networks are remarkably unreliable and insecure. A significant source of vulnerability is the fact that the underlying network equipment (e.g., routers and switches) run complicated programs written in obtuse, low-level programming languages, which makes managing networks a difficult and error-prone task. Simple mistakes can have disastrous consequences including making the network vulnerable to denial-of-service attacks, hijackings, and wide-scale outages.<br\/><br\/>The goal of this research is to transform the way that networks are managed by introducing a new class of network programming languages with the following essential features: (i) network-wide, correct-by-construction abstractions; (ii) support for fault-tolerance and scalability; (iii) coordination with end-hosts and independently-administered networks, as well as mechanisms for establishing trust between them; (iv) formal verification tools based on rigorous semantic foundations; and (v) compilers capable of generating efficient and portable code that runs on heterogeneous equipment. To demonstrate how to build a language with these features, the researchers are designing a language for OpenFlow networks called Frenetic, and evaluating it on several novel security applications. This project will have broad impact by (i) discovering key techniques for increasing the reliability of our networks, (ii) opening up the interfaces used to program networks, thereby enabling grass-roots innovation where it was previously not possible, and (iii) educating a new community of researchers with advanced skills in both networking and programming languages.","title":"TC: Large: Collaborative Research: High-Level Language Support for Trustworthy Networks","awardID":"1111520","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":[483683,"531675"],"PO":["565327"]},"181470":{"abstract":"The continuing decline of conventional fossil fuel has resulted in increasing energy costs all around the world. Meanwhile, fossil fuel-induced greenhouse gas emissions have profound implications for our environment. Renewable energy is generated from natural resources that are naturally replenished. Photovoltaic (PV) generation is gaining increased popularity due to its advantages such as absence of fuel cost, low maintenance, and no noise and wear due to the absence of moving parts. Designing renewable energy driven computer systems poses various challenges in terms of intelligent control strategies for better energy utilization, optimizations for reducing overhead and improving reliability. The proposed research will develop novel enabling technologies for high-performance computer architectures (e.g. multi-core CPU\/GPU) that can achieve high efficiency and dependability in utilizing renewable energy. The research goal includes GPU power management schemes that can maximize a solar panel?s total energy output using load-matching and intelligently allocate the available solar power across multiple cores and threads so that maximum workload performance can be achieved. The proposed research project can greatly contribute to enabling high-performance computing systems to stay on track with its historic scaling and hence benefit numerous real-life applications. This project also contributes to society through engaging under-represented groups, research infrastructure dissemination for education and training, and outreach to renewable energy industries and research community.","title":"CSR: Small: Enabling Renewable Energy Powered Sustainable High Performance Computer Architectures and Systems","awardID":"1117261","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["550720"],"PO":["565255"]},"181250":{"abstract":"Recent advances in scanning technology and general progress in science and engineering have made it easy to sample a geometry presented explicitly or implicitly. In some cases, sampling allows the data to be equipped with values of a scalar function defined on the sampled domain. The goal is to process the resulting point cloud data (PCD) for approximating different geometric and topological structures of the original space and\/or functions defined on them. How can this be achieved is the main agenda of this award.<br\/><br\/>The extraction of geometric and topological structures from a PCD begins with the understanding of these structures on the original sampled space. In some cases one needs to adapt existing mathematical concepts to fit the discretization inherent in algorithmic development. In others new analysis techniques and new algorithmic tools need to be developed. PIs expect the more of the latter to tame the curse of dimensionality and scale. The two PIs bring their expertise in computational geometry and computational topology to the board to conduct the project. <br\/><br\/>A central problem in data analysis is to design computational methods for representing and retrieving information from massive and high-dimensional data. Various scientific, engineering, and social studies such as the ones in product development, medicine, economics, climate, disease control produce data that presumably sample a hidden parameter space sitting in a high dimension. They will benefit from the techniques developed in this project. By the very nature of the problems, this project will enhance a synergy between fields such as mathematics, theoretical computer science, and data analysis.<br\/><br\/>Other than standard research forums, results from the project will be disseminated through course notes, tutorials, and web-pages to reach wider audience. Graduate students supported by the project will develop skills in mathematics and theoretical computer science, most notably in geometry and topology and also in writing robust, efficient and user-friendly software.","title":"AF: Small: Analyzing Spaces and Scalar Fields via Point Clouds","awardID":"1116258","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["549998","550392"],"PO":["565157"]},"181151":{"abstract":"The solution of symbolic computation problems can be accelerated in at least two ways: one is to integrate limited precision floating point arithmetic on the scalars, and the other is to use parallel processes. We propose to investigate those approaches on problems in real algebraic optimization and exact linear algebra.<br\/><br\/>Semidefinite numerical optimization produces sum-of-squares representations for polynomial inequalities, which express global optimality. But the representations are numeric and approximate, and exact rational certificates for the inequalities are derived via exact symbolic means, thus leading to truly hybrid symbolic-numeric algorithms. When combining floating point arithmetic and randomization, analysis of the expected condition numbers of the random intermediate problems can guarantee success. We propose to introduce fraction-free algorithms and analyze the arising determinantal condition numbers.<br\/><br\/>LinBox is a C++ library for exact linear algebra. We propose to participate in the parallelization of the LinBox library by investigating problems with memory contention and by creating interactive symbolic supercomputing environments. Several related problems in computational algebraic complexity that will receive attention are: quadratic-time certificates for linear algebra problems, determinantal representation of polynomials by symmetric linear matrix forms, and interpolation of supersparse (lacunary) rational functions.<br\/><br\/>The PI's research expands the infrastructure in symbolic computation and the understanding of the underlying complexity. Aside from certifying optima, exact sums-of-squares certificates have proved theorems in mathematics, physics and control theory. Some of the important applications of LinBox are sparse linear algebra over finite fields and integer Smith forms for computational topology data. The PI is making the developed software freely available.<br\/><br\/>Algorithmic thinking and computation have become a cornerstone of modern science (pure and applied) and modern life. Symbolic computation programs, such as Mathematica, Maple, and the SAGE platform, have many applications, such as modeling data sets by symbolic expression for Toyota and Shell Oil, generating programs for signal processing, and program and protocol verification.","title":"AF: Small: Efficient Exact\/Certified Symbolic Computation By Hybrid Symbolic-Numeric and Parallel Methods","awardID":"1115772","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[485433],"PO":["565251"]},"181041":{"abstract":"The advent of multicore technologies has fueled much recent work on methods for designing, deploying, and verifying multiprocessor implementations of real-time applications. Such methods must necessarily be rooted in resource allocation techniques that facilitate predictable system designs. In work on such techniques, scheduling algorithms, which allocate processor time, have received the greatest attention. Synchronization algorithms, which coordinate access to other resources, have received much less attention.<br\/><br\/>The current state-of-the-art regarding real-time multiprocessor synchronization is that simple locking protocols can be supported, but not much else. This is a serious impediment that is limiting the evolution of real-time applications to \"multicore-ready\" versions. Driven by this, a rich set of multiprocessor real-time synchronization mechanisms is being developed in this project. These mechanisms are motivated by real application needs, as have arisen in joint work involving the investigators and industry colleagues at AT&T and Northrop Grumman. Such mechanisms are being designed for use within several system models, also motivated by real needs. These include models wherein various complexities exist, such as having tasks of differing criticalities, multiple subsystems that must be \"isolated\" from one another, heterogeneous hardware components, dynamic task behaviors, etc. In all of this work, the design of optimal synchronization protocols is being emphasized. These protocols are being prototyped and evaluated within a open-source UNC-produced real-time Linux extension called LITMUS^RT. Broader impacts will include continued joint research with industry colleagues, and the development of publicly-available open-source software that can be used by other institutions for research and teaching purposes.","title":"CSR: Small: A Comprehensive Framework for Real-Time Multiprocessor Synchronization","awardID":"1115284","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["31436","518412"],"PO":["565255"]},"188962":{"abstract":"Detection, identification, and tracking of CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive) plumes can be accomplished by combining the modalities of sensor and cyber networks. The sensor network provides information about physical-space activities, e.g., locations and movements of the plume sources. The cyber network provides storage and computational resources to analyze and infer where the plume originated, the trajectory of its movement, and the prediction of its future movement. The challenges in realizing such a sensor cyber network include intelligent sensing (intelligent sensor selection and coverage) and the capability to deal with uncertainties (uncertainty in measurement as well as in modeling). This project proposes to leverage the convergence between the sensor and the cyber networks to achieve these goals. In particular, the plan is to carry out three synergistic research tasks: 1) network formation by sensor selection, placement, and coverage; 2) sensor tasking protocol with temporal\/spatial uncertainty management; 3) protocols for reliable sensor-cyber communication supporting the above two tasks. The PIs will prototype the research results and integrate the prototypes for different components to build the sensor cyber network for plume detection, identification and tracking. They will also evaluate the sensor cyber network using various test scenarios in collaboration with Oak Ridge National Lab. If successful, the project will provide technology for building detection and tracking sensor networks that can give great protection to people and the environment against harmful plumes.","title":"NeTS: Medium: Collaborative Research: Building an Intelligent, Uncertainty-Resilient Detection and Tracking Sensor Network","awardID":"1158701","effectiveDate":"2011-08-26","expirationDate":"2014-05-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["558109"],"PO":["565303"]},"186135":{"abstract":"Significant resources are devoted to R&D on robotics across industry and government agencies. This project aims to (1) understand how the efforts can be coordinated to optimize the impact of such efforts, (2) build educational resources where best practice is used across all institutions in the US to ensure access to the best human capital, and t(3) study best practice for transition of results for exploitation. Finally, resources are provided to disseminate information about the impact of robotics to a broad community.<br\/><br\/>A national robotics initiative is launched for the creation of basic technologies that grow the economy, secure healthcare for future generations and provide support to first responders. It is essential from a societal perspective that the use of such resources is optimized to maximize the impact in terms of economic growth, job creation and provide services to the citizen. The organization of a Robotics-VO provides the required infrastructure support and coordination to ensure effective use of resources.","title":"NSF- EAGER: Robotics Virtual Organization (Robotics-VO)","awardID":"1144045","effectiveDate":"2011-08-01","expirationDate":"2013-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["565099"],"PO":["543539"]},"185057":{"abstract":"The objective of this project is to help support student attendance at the ACM-SIAM Symposium on Discrete Algorithms (SODA) in Kyoto, Japan to be held on January 17-19, 2012. SODA is the premier conference that covers research in algorithmic aspects of combinatorics, discrete mathematics, and computer science and their applications in the sciences and business. Research on algorithms is one of the most important and active areas in computing. Algorithmic questions pervade business, technology, and science and their study continues to be a major research topic. SODA produces one of the most widely read proceedings in algorithms.<br\/><br\/>Many of the students who have papers accepted at the conference may not be able to obtain sufficient support from their home institutions. Attendance is particularly valuable for students who are just starting out and are more likely to profit from direct NSF support. By providing it, the NSF will be able to help nurture a new generation of researchers in this key area.","title":"Student Travel to SODA12","awardID":"1137840","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["563333"],"PO":["565251"]},"177335":{"abstract":"Operating System (OS) kernels form the bedrock of all system software---they can have the greatest impact on the resilience, security, and extensibility of today's computing hosts. A single kernel bug can easily wreck the entire system's integrity and protection. The PIs are applying new advances in certified software to the design and development of novel kernel structures that generalize and unify traditional OS abstractions in microkernels, recursive virtual machines, and hypervisors. By replacing the traditional \"red line\" (between the kernel and user code) with customized safety policies, the PIs show how to support different isolation and kernel extension mechanisms (e.g., type-safe languages, software-fault isolation, or address space protection) in a single framework. The PIs are also building a new framework for certified kernel programming and a set of domain-specific variants of C-like languages. They are applying them to certify different components at different abstraction layers (ranging from scheduler, virtual memory manager, file system, to information flow control), and then linking everything together to build end-to-end certified OS kernels. Certified kernels built under this project will offer safe and application-specific extensibility, provable security properties with information flow control, and accountability and recovery from hardware or application failures. They will help improve the reliability and security of many key components in the world's critical infrastructure, and advance human knowledge on what is possible in building trustworthy systems on top of a reliable core.","title":"TC: Medium: Making OS Kernels Crash-Proof by Design and Certification","awardID":"1065451","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["550517","502304"],"PO":["564223"]},"186168":{"abstract":"The wide adoption of GPS and sensor technologies has created many applications that collect and maintain very large repositories of data in the form of trajectories. To better analyze such data, a user can pose complex pattern queries using a high level region-based representation that abstracts trajectories by the temporally ordered sequence of spatial regions (or areas\/points of interest) they visited. For example: \"find moving objects that first passed by the train station, then by the town center and were always within a mile from the harbor\". Temporal and counter constraints, as well as region variables and region hierarchies can be added to create very powerful queries. Similarly, one can formulate join queries that identify pairs of trajectories with similar behavior, etc. While such pattern-based queries are critical in analyzing vast trajectory archives, traditional methods fail to scale due to the large computational effort and size of the data. Adding more resources (i.e., many processors) will not eliminate the bottleneck (each processor still uses multiple clock cycles per operation) and may also create a large communication overhead between the processors. Instead, this project takes a different, high risk-high payoff approach by using reconfigurable hardware, namely, Field Programmable Gate Arrays (FPGAs). FPGAs are code accelerators where a portion of the application is mapped as a circuit on the FPGA; thus they avoid the traditional load\/store operations in the datapath that traditional CPUs perform. Such processing has the potential to provide orders of magnitude performance improvement, leading to further discoveries from vast amounts of data. The intellectual merit of this project emanates from the novel solutions needed: efficient FPGA designs to support region variables, time and counter constraints, region hierarchies, as well trajectory joins. If successful, this project has the potential to revolutionize the way queries over large trajectory data archives are processed. There is a broad range of applications (scientific, educational, and economic activities) that will be impacted from the fast processing provided by the FPGA filtering approach. Providing orders of magnitude speed improvement will have a profound effect in these applications. The combination of two distinct technologies (Databases and FPGAs) is an ideal vehicle for training graduate\/undergraduate students and for transferring gained experience into relevant courses. For further information see the project web site at the URL: http:\/\/www.cs.ucr.edu\/~tsotras\/fpga\/index.html","title":"III: EAGER: Accelerated Filtering of Spatiotemporal Archives Using Reconfigurable Hardware","awardID":"1144158","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["518640","543515"],"PO":["543481"]},"177236":{"abstract":"Participation in political debate and deliberation is critical to democracy. Browsing political material is a direct way of acquiring knowledge about civic activities, the operations of government, and the issues of the day. This project examines a fast growing, but little understood new type of political participation: online information seeking, deliberation and decision making in the context of Web 2.0 technologies. The research includes three intertwined threads of study: (a) user-centered design of enhancements to a search\/browse tool and a cross-application, user-generated interlinkage browser; (b) laboratory studies of how potential voters browse and make decisions in social computing environments; and (c) longitudinal observation of users of novel, socially-enabled political search\/browse tools through at least three U.S. election cycles.<br\/><br\/>Intellectual Merit: The central research question is how exposure to and participation in social networking activities influences information seeking and decision making in politics. An important practical outcome will be search\/browse tools for civic participation and political discourse that are integrated with the social networking environments that many people now use every day. The research will contribute to theories of deliberation in the domain of digital politics and to the design of tools for exposing and navigating the structure of online political dialog.<br\/><br\/>Broader Impacts: Tools and theory developed from this project will provide contexts that broaden opportunities for participation in civic life in the 21st century, thereby helping to bridge the digital divide. The work will also benefit education through the training of graduate students and post-doctoral researchers.","title":"HCC: Medium: Social Search and Deliberation in Digital Political Information and Collaboration Domains","awardID":"1064852","effectiveDate":"2011-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["547222"],"PO":["564456"]},"187059":{"abstract":"This project explores computer vision techniques aimed at exploiting compromising reflections associated with data input in mobile electronic devices such as smart phones. The ubiquity of these personal communication devices and their growing roles in data manipulation tasks, make unintended visual emanations an exploitable liability to data security. Nevertheless, there is still a gap in understanding of both the limitations of these techniques as well as the availability of effective mitigation mechanisms. It is the goal of this work to contribute to filling this conceptual gap.<br\/><br\/>The study builds upon recent state of the art techniques for automatic reconstruction of typed input from compromising reflections, comprising of robust keystroke event detection and classification mechanisms coupled to natural language processing modules. Such paradigm is both effective and amenable to low cost implementation in commodity devices. Based on these new developments, threat scenarios are no longer restricted to controlled scenarios using specialized equipment, but rather consist of highly flexible and possibly impromptu attacks. The project develops advanced cross-platform data input transcription prototypes used within a threat validation framework. This framework provides a characterization of both threat scenario operational limitations (e.g., imaging resolution, scene illumination, computational requirements) as well as the performance characteristics (e.g., robustness, accuracy) of the different vulnerability exploitation mechanisms. Moreover, the results of the analysis of diverse threat scenarios are being used to identify and develop appropriate mitigation mechanisms when possible.","title":"EAGER: Automatic Reconstruction of Typed Input from Compromising Reflections","awardID":"1148895","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"K576","name":"National Security Agency"}}],"PIcoPI":["565326","562635"],"PO":["564316"]},"177269":{"abstract":"Given the recent upward trend in wireless traffic, capacity demand increases faster than spectral efficiency and availability. However, it has been well-known that in wireless communications, some of the spectra are significantly under-utilized. This fact has attracted researchers from academia and industry who are interested in the next generation of cognitive radio communication systems. Despite the tremendous ongoing efforts on this research topic, there is a huge gap between the research on cognitive radios and the network applications. This project exploits a novel wireless networking paradigm called Application-Aware Cognitive Multihop Wireless Networks (AC-MWN). This architecture includes a mega network layer for multihop wireless networks, which combines the major functionalities of medium access control, network and transport layers in the traditional layered model of wireless networks. The mega network layer has cognition to the spectrum availability and to the requirements of the network applications. Additionally, the new design can efficiently utilize the spectrum resources and effectively adjust the application resources, such as storage and computational capabilities. While AC-MWN accommodates the requirements of different network applications, it also can drive the capacity closer to the upper theoretical limit for MWNs. Therefore, this architecture satisfies the requirements of application-aware cognitive multihop wireless networks. The AC-MWN research effort will have a significant impact on the better understanding of the designs for future cognitive multihop wireless networks and provide improvements for many application domains, including public safety, disaster rescues, environmental monitoring, and medical applications.","title":"NeTS: Medium: AC-MWN: A Novel Architecture for Application-Aware Cognitive Multihop Wireless Networks","awardID":"1065069","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[475235,"544676",475237],"PO":["557315"]},"181471":{"abstract":"Due to the increasing demands for computing and storage, energy consumption, heat generation, and cooling requirements have become critical concerns both in terms of the growing costs as well as their environmental and societal impacts. Thus, thermal awareness, which is the knowledge of local unevenness in heat generation and extraction rates, and hence, heat imbalance at various points inside a datacenter, is essential to maximize energy and cooling efficiency as well as to minimize server failure rates.<br\/>The objectives of this research are to acquire knowledge about the heat imbalance at different regions inside a datacenter and to enable thermal-aware self-configuration and self-optimization of computing resources inside a datacenter. These objectives are aimed at increasing the energy and cooling efficiency and at decreasing equipment failure rates so to minimize both the impact on the environment and the Total Cost of Ownership (TCO) of datacenters. Specifically, the project focuses on designing autonomic adaptive sampling solutions for enabling self-organization of heterogeneous sensors - composed of thermal cameras, scalar temperature and humidity sensors, and airflow meters - into a multi-tier sensing infrastructure, and on studying proactive, Quality of Service (QoS)-aware, heat-imbalance-based solutions for Virtual Machine (VM) consolidation and cooling system optimization in a virtualized air-cooled datacenter.<br\/>This project will also result in the generation of computer-literate undergraduate and graduate researchers with a comprehensive knowledge of complex optimization problems in energy-efficient design and management of large datacenters. The PI will create new teaching modules on distributed sensing, provide opportunities for exchange programs, leverage existing minority student outreach networks at Rutgers, and incorporate student exchange programs as well as team-teaching approaches.","title":"CSR:Small:Sensor-driven Thermal-aware Autonomic Management of Instrumented Datacenters","awardID":"1117263","effectiveDate":"2011-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["550654"],"PO":["565255"]},"181592":{"abstract":"This project focuses on the problem of making it easier to program performance-asymmetric multicore processors (AMP). A multicore processor is called performance-asymmetric when its constituent cores may have different characteristics such as frequency, functional units, etc. High-performance Computing (HPC) community has demonstrated significant interest in the AMPs as they are shown to provide nice trade-off between the performance and power. On the other hand, asymmetry makes programming these platforms hard. The programmers targeting these hardware platforms must ensure that tasks of a software system are well matched with the characteristics of the processor intended to run it. To make matters even more complicated, a wide range of AMPs exist in practice with varying configurations. To efficiently utilize such platforms, the programmer must account for their asymmetry and optimize their software for each configuration. This manual, costly, tedious, and error prone process significantly complicates software engineering for AMP platforms and leads to version maintenance nightmare. To approach this problem, this project is developing a novel program analysis technique, phase-based tuning. Phase-based tuning adapts an application to effectively utilize performance asymmetric multicores. Main goals are to create a technique that can be deployed without changes in the compiler or operating system, does not require significant inputs from programmer, and is largely independent of the performance-asymmetry of the target processor. The broader impacts are to help realize the potential of emerging AMPs and other novel extreme-scale computing architectures, which in turn will enable researchers in the scientific disciplines to analyze, model, simulate, and predict complex phenomena important to society.","title":"SHF: Small: Phase-Based Tuning for Better Utilization of Performance-Asymmetric Multicores","awardID":"1117937","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["562662"],"PO":["366560"]},"181361":{"abstract":"This project focuses on practical deployment of human\/multi-robot teams in situations where robots can explore regions that are unsuitable for humans. For example, a team of \"rescue\" robots can sweep through a collapsed building searching for victims and transmit their positions to human first-responders outside. Managing a human\/multi-robot team in a dynamic environment is a challenging problem. Not only is the world mutable, but also the team can experience altered membership because a robot gets lost or a human operator needs rest---the world is changing, and so is the team that is exploring that world.<br\/><br\/>The goal of this research is to develop strategies for human\/multi-robot teams to learn to perform consistently and effectively. Three primary aims will be pursued: first, to mitigate changes in team composition via a practical framework for institutional memory that remembers and uses past experiences; second, to model and record expertise for later use by learning behaviors performed by a human operator; and third, to distribute tasks among team members efficiently by providing a balanced mechanism for social choice. The novel approach of this project is applicable to a broad spectrum of human\/multi-robot, and human\/multi-agent teams, by integrating institutional memory, learning from human teammates, and resolving conflict among differing perspectives. The strategies will be evaluated using a human\/multi-robot testbed comprised of one human operator plus a heterogeneous set of inexpensive, limited-function robots. Although each individual robot has restricted mobility and sensing capabilities, together the team members constitute a multi-function, human\/multi-robot facility.<br\/><br\/>This project addresses important challenges in robust intelligence, including behavior modeling, learning from experience, making coordinated decisions, and reasoning under uncertainty. Expected outcomes include strategies for human\/multi-robot teams that learn to collaborate effectively under a variety of conditions and can maintain their performance despite run-time changes in team membership, as well as knowledge about how people interact with robot teams. Broader impacts include providing access to a networked experimental testbed for remote collaborators; publishing proven curricular materials on multi-robot teams addressed to graduate, undergraduate and high school students; involving undergraduates in research activities; and working with existing contacts at local museums to demonstrate results to the general public.","title":"RI: Small: Collaborative Research: Learning to perform consistently in human\/multi-robot teams","awardID":"1116843","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":["505729","523503"],"PO":["565035"]},"181372":{"abstract":"Analog and mixed-signal circuits are very sensitive to the process variations as many matching and regularities of the layout are required. This situation becomes worse as technology continues to scale to sub-40nm owning to the increasing process-induced variability. Transistor level mismatch due to process variation is the primary barrier to reach a high-yield rate for analog designs in sub-90nm technologies. Analog circuit designers usually perform a Monte-Carlo (MC) analysis to analyze the statistical mismatch and predict the variational responses of their designs under variations. As MC analysis requires a large number of repeated circuit simulations, its computational cost is expensive. Efficient variational performance analysis of mixed-signal\/analog circuits such as worst-case, bounding case and statistical analysis will become imperative for nanometer analog\/mixed-signal designs. <br\/><br\/>This research seeks to develop novel and efficient non-Monte-Carlo techniques for worst-case and statistical analysis of analog\/mixed-signal circuits. The PIs propose to develop novel worst-case analysis methods for analog\/mixed-signal circuits based on graph-based symbolic analysis technique, affine-like interval arithmetic and a control-theoretic method. The new method will first build variational transfer functions from linearized analog circuit by determinant decision diagram (DDD) based symbolic analysis and affine-like interval arithmetic. Then the performance bounds will be computed by control-theoretic theory based on the variational transfer functions. More conservative affine-like interval arithmetic to reduce conservation will also be investigated. The performance bounds in the time domains given frequency domain bounds will be investigated as well. The PIs plan to develop fast non-Monte-Carlo stochastic analysis methods to calculate statistical responses such as mismatch due to process variations. The problem is to be modeled as solving nonlinear stochastic differential-algebra-equations. Nonlinear stochastic methods (Galerkin or collocation methods) and new nonlinear macromodeling method will be investigated to solve the resulting problems.<br\/><br\/>The outcome of this research will add significantly to the core knowledge of variational and statistical analysis techniques for analog\/mixed-signal circuits, which will enable more efficient statistical optimization and design of analog\/mixed-signal systems. By working with the industry partner, the PI expects that the developed techniques will bring immediate impacts on the design community to improve the design productivity for nanometer integrated analog\/mixed-signal systems. The interdisciplinary nature of proposed research and relevant training will allow students to gain critical skills in the highly competitive high-tech job market. This grant will enable the PI to hire more female and underrepresented minority students to further contribute to the diversity in America's science and technology workforce.","title":"SHF: Small: Variational and Bound Performance Analysis of Nanometer Mixed-Signal\/Analog Circuits","awardID":"1116882","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}}],"PIcoPI":["535204"],"PO":["562984"]},"181383":{"abstract":"In this project the goal is to develop a computer vision system to assist visually impaired people when navigating in indoor environments. A novel feature of the vision system is that it uses a motion sensing input device to gather both an image and depth map of the scene as input. By learning joint representations that combine both depth and intensity information, powerful features can be learned that give a dramatic improvement over existing scene understanding algorithms (which rely on intensity information alone). The availability of depth information also allows the recovery of the room geometry and permits the construction of new types of 3D priors on the locations of objects, not currently possible with existing approaches. The output of the vision system will be communicated to the visually impaired person via a number of possible methods: (i) a tactile hand-grip on a cane; (ii) a wearable pad embedded with actuators and (iii) the BrainPort sensor which has an array of tiny actuators that are placed on the tongue.<br\/><br\/>The expected results of the project are: (i) a large dataset of indoor scenes with depth maps and dense labels; (ii) new open-source algorithms for fusing depth and intensity information to aid scene understanding and (iii) a prototype vision-based assistive device for visually impaired people. The project aims to assist the approximately 2.5 million people in the US are blind or partially sighted.","title":"RI: Small: Indoor Visual Navigation and Recognition for the Blind Using a Motion Sensing Input Device","awardID":"1116923","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["502163"],"PO":["564316"]},"181394":{"abstract":"The next-generation wireless networks are targeted at supporting various applications such as voice, data, and multimedia. However, wireless channels are unreliable and the capacity of a wireless fading channel is time-varying, which may cause severe degradation to video\/audio quality. Therefore, it is particularly challenging to design quality of service (QoS) assurance mechanisms and multimedia compression techniques for multimedia communication over wireless networks. This project aims to develop new theories and techniques for QoS assured multimedia communication over non-stationary wireless channels. To accomplish this goal, the following research tasks are conducted: 1) developing a novel evolutionary spectrum approach, which is able to accurately model the dynamics of general non-stationary fading channels; 2) developing QoS-assured sliding-mode congestion control and scheduling mechanisms for multimedia communication over non-stationary fading channels; and 3) developing gamma rate theory for causal rate control in source coding, and developing cross layer design to achieve both network friendliness and viewer friendliness for multimedia coding and transmission. Techniques developed in this project will fill some important gaps in fundamental understanding of QoS-assured multimedia communication over non-stationary wireless channels, and provide the theoretical underpinning for system design and algorithm implementation.","title":"NeTS: Small: QoS Assured Multimedia Communication over Non-stationary Wireless Channels","awardID":"1116970","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["486052"],"PO":["557315"]},"185673":{"abstract":"This travel support enables U.S.-based students to attend the 19th ACM SIGSPATIAL GIS 2011 Conference, held in Chicago, Illinois, USA, November 1-4, 2011 (http:\/\/acmgis2011.cs.umn.edu\/).<br\/><br\/>The ACM SIGSPATIAL GIS conference has established itself as the world's premier conference to foster research in the areas of Spatial Data and Analysis and Geographic Information Systems (GIS). The conference provides a forum for original research contributions covering all conceptual, design, and implementation aspects of GIS ranging from applications, user interfaces, and visualization to storage management and indexing issues. It brings together researchers, developers, users, and practitioners carrying out research and development in novel systems based on geospatial data and knowledge, and fostering interdisciplinary discussions and research in all aspects of GIS. It is the premier annual event of the ACM Special Interest Group on Spatial Information (ACM SIGSPATIAL). The conference seeks to continuously advance the state of-the-art in spatial data management and spatial data analysis and broaden its impact.<br\/><br\/>This grant provides partial travel support and conference registration for 20-25 qualified U.S. based graduate and promising undergraduate student participants. The students will greatly benefit from attending this conference, as they will be able to partake in the current state-of-the-art in the area of geospatial systems and applications, present their work, and potentially make connections for research collaborations and research mentoring. The total number of ACM SIGSPATIAL GIS participants in the past has been in excess of 250 participants, with a majority of the participants from the U.S., followed by Europe and Asia. The conference participation has shown a steady increase in the past few years due to the growing importance of geographic information. A strong representation of U.S.-based graduate students at ACM SIGSPATIAL GIS is useful in maintaining U.S. competitiveness in the important research areas crucial for U.S. infrastructures and applications that critically depend on geo-referenced information. Those who receive the award will be featured in the NSF Student Supported Research special section of the combined Poster, Demo, and PhD Showcase Session, a major event in the conference that lasts for over three hours and is attended by all conference participants.","title":"US-Based Students Support to Attend the ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems 2011 (ACM SIGSPATIAL GIS 2011)","awardID":"1141235","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["554456"],"PO":["563751"]},"181196":{"abstract":"This project develops distributed control algorithms for robot sensor networks using information dynamics to unify network mobility and communication. The new framework moves beyond location-based mobility control algorithms and packet-based communication protocols to a single information-theoretic approach. The framework is developed through: (1) formulation of a new quality of service of information metric and characterization of feasible quality of service demands, (2) adaptive receding horizon control that adjusts planning horizon and sample time based on information dynamics, (3) distributed calculation and optimization of team quality of service, and (4) experimental validation using a heterogeneous indoor robot sensor network and outdoor unmanned aircraft systems.<br\/><br\/>This research enables the collection of in situ data over large spatiotemporal scales. When deployed in remote or hazardous locations the data collected with these networks leads to a direct improvement to model or understand complex environments, which can help save lives. Students will benefit from the multi-disciplinary activities involved, including embedded computing, miniature sensor devices, wireless networking, and automatic control.","title":"RI: Small: Providing Quality of Information in Robot Sensor Networks","awardID":"1116010","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["507819"],"PO":["564069"]},"185212":{"abstract":"This award supports two workshops: the Workshop on Economics of Information Security (WEIS) and the Workshop on Cybersecurity Incentives (WOCI), to be held consecutively at George Mason University. These workshops will facilitate better understanding of how economic factors affect the deployment and use of technical cybersecurity measures and how incentives of various kinds might be influence improvements in the security of cyberinfrastructure.","title":"Cyber Security Economics and Incentives Workshop","awardID":"1138736","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"K598","name":"U.S. Department of Homeland Se"}}],"PIcoPI":[496951,496952],"PO":["565327"]},"177325":{"abstract":"Intellectual Merit:<br\/>The focus of the proposal is on finding semantically equivalent files in an efficient and scalable manner. If two files are identical, they are candidates for optimizations to reduce storage costs, increase performance, and generally improve the system. Traditionally, two files are only considered equivalent if they are byte-by-byte identical - i.e., byte equivalence. However, this team's preliminary research shows that there are many other files that are essentially equivalent, even though the bytes they contain are not the same. This proposal will investigate how to find such cases and perform optimizations that make use of semantic equivalence, rather than byte equivalence.<br\/><br\/>This project will design and implement a framework, Facets, which explores new capabilities by applying optimizations to files that are essentially transformed versions of each other. Many optimizations and improvements can be applied to semantically equivalent files, including:<br\/><br\/> -Ensuring that the security of semantically equivalent files is preserved<br\/> -Easing backup and maintenance of semantically equivalent files in various formats, fidelities, and versions<br\/> -Using semantically equivalent files to improve performance, reliability, and availability<br\/> -Regenerating semantically equivalent files to speed up recovery and network transfer<br\/> -Selecting which semantically equivalent files to access according to performance or energy constraints<br\/><br\/>This team's preliminary research shows that 5% of files on a typical user's machine are original content. The rest are copies of files from elsewhere or various derivatives of original content. While leveraging this observation to achieve advantages is not trivial, many significant improvements are possible if one can find these relationships and make proper use of them. These improvements include enhanced security, more efficient backup and restoration, better file caching, more intelligent tradeoffs in performance versus power use, and a host of other possibilities. <br\/><br\/>Broader Impacts: <br\/>The code and techniques developed will be released in open source form. The team will take steps (such as applying for supplemental REU grants) to involve undergraduates in the research. They will give talks and recruit at Hispanic-serving institutions. Materials and concepts from the research will be incorporated into classes taught by the principal investigators at their institutions.","title":"CSR: Medium: Collaborative Research: Facets: Exploring Semantic Equivalence of Files to Improve Storage Systems","awardID":"1065373","effectiveDate":"2011-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["537229"],"PO":["551712"]},"186037":{"abstract":"In recent years, power and thermal control has become one of the most serious concerns for large-scale data centers that are rapidly expanding the number of hosted servers. In addition to reducing operating costs, precisely controlling power consumption and heat dissipation is an essential way to avoid system failures caused by power capacity overload or overheating due to increasingly high server density (e.g., blade servers). Power and thermal control becomes even more challenging as many data centers start to adopt the virtualization technology for resource sharing, leading to increased utilization and power consumption. <br\/><br\/>This CAREER project addresses the following research topics. 1) We plan to design and evaluate advanced power and thermal control algorithms, based on feedback control theory, to achieve analytic assurance of control accuracy and system stability. First, we propose novel control algorithms at multiple layers to control power and application performance for virtualized server environments. Second, we propose highly scalable hierarchical algorithms to control the power consumption of an entire large-scale data center. Third, we will design cascaded control algorithms to control heat dissipation and handle thermal emergencies by coordinating with power control loops. 2) We propose power and thermal control middleware. Our middleware will find the optimal coordination strategy for multiple control loops to work together at different layers, and then configure them to achieve the desired control functions. In addition, our middleware can automate the procedure of controller design and analysis for a universal control solution. 3) We will also investigate other components such as hard drives and network switches, as well as the controllability and feasibility problems, in order to provide a complete power and thermal control framework for today's large-scale data centers.","title":"CAREER: Coordinated Power and Thermal Management for Virtualized Data Centers: Algorithms, Framework, and Middleware","awardID":"1143607","effectiveDate":"2011-08-15","expirationDate":"2015-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4090","name":"ADVANCED NET INFRA & RSCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["554423"],"PO":["565255"]},"186158":{"abstract":"This project is built on the interdisciplinary collaboration between multimedia and natural language processing (NLP) researchers. The goal of this exploratory research is to provide effective methods for organizing, searching, mining and reasoning with web-scale multimedia. The approach is based on formulation of a structured multimedia database, called Multimedia Information Networks (MINets) which enables new searching and mining paradigms such as keyword query of unlabeled image\/video databases, query expansion in content-based image retrieval (CBIR), and measuring similarity between different modalities such as an image and a piece of text). In addition, MINets framework is expected to provide the ability to perform robust inference (e.g., recognizing objects and activities in images or videos) in the presence of noise and uncertainty. In its simplest form, a MINet is a graph where nodes are either concepts (text) or data (such as images), and links are ontological\/semantic relationships between concepts, attachment of images to concepts, and visual similarity measures between images. Construction of an experimental MINets framework involves crawling the Web for a particular domain, gathering images with associated text and exploiting natural language processing, computer vision and mining techniques in establishing the concepts, associated images, interconnecting links and an ontology that supports inference. Validation against current web search engines and CBIR techniques is expected to provide a proof-of-concept for the novel MINets framework.<br\/><br\/>This interdisciplinary exploratory project is expected to yield general theoretical and algorithmic MINets framework that will provide new searching, mining and reasoning capabilities for multimedia data. It will help to define new research areas in effective utilization of multimedia information sources for cross-media and cross-conceptual knowledge discovery and analysis, large-scale annotation, information fusion and inference. Project results, including open source software, annotated corpora, scoring metrics will be disseminated via project Website (https:\/\/netfiles.uiuc.edu\/qi4\/www\/MINets.htm). This project will provide research opportunities for graduate and undergraduate students.","title":"EAGER: Exploring Multimedia Information Networks","awardID":"1144111","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["550184",499546],"PO":["563751"]},"177215":{"abstract":"Organizations, such as hospitals, financial institutions, and universities, that collect and use personal information are required to comply with privacy regulations, such as the Health Insurance Portability and Accountability Act (HIPAA), the Gramm-Leach-Bliley Act (GLBA), and the Family Educational Rights and Privacy Act (FERPA). Similarly, to ensure customer trust, web services companies, such as Google, Facebook, Yahoo!, and Amazon, publish privacy policies stating what they will do with the information they keep about customers' individual behaviors. These policies impose constraints on disclosure (or transmission) of personal information, articulate obligations (e.g., notifying customers about privacy breaches), and identify purposes for which personal information may or may not be used. Prior work has focused on formalisms for disclosure and obligations, but no such foundation has been developed for information use for specified purposes.<br\/><br\/>Intellectual Merit. This project addresses the central problem of developing a formal semantics that explains what it means to use information for a set of purposes, a logic for specifying such policies, and algorithmic methods for their enforcement. It advances the state of knowledge in the field of privacy by providing a foundation for a concept that is commonly used in practice, but has not been the subject of careful scientific study. The project also investigates the interaction of this concept with the previously studied concepts of disclosure and obligation, thereby enabling a more comprehensive understanding of privacy. The formal semantics the project develops is novel and draws on insights from prior work on philosophical theories of causation and intentions, and from the computer science literature on formal methods, information flow, and planning. The model is validated through user studies and its application through case studies in the healthcare domain. <br\/><br\/>Broader Impacts. The project addresses a problem of significant and growing importance to society. It initiates a new direction in providing foundations for privacy by studying the concept of information use for a purpose. This concept appears in privacy policies published by organizations in sectors as diverse as finance, web services, healthcare, insurance, education, and government - the cornerstones of modern society. The semantic foundation serves as the basis for developing practical tools to support the enforcement of such policies in such organizations. The project provides opportunities for engaging graduate and undergraduate students. The PIs plan to integrate the research results into their existing security and privacy courses, and, for wider dissemination, leverage outreach programs in Carnegie Mellon's Computer Science Department and CyLab aimed at K-12, women, persons with disabilities, and underrepresented minorities.","title":"TC: Medium: Semantics and Enforcement of Privacy Policies: Information Use and Purpose","awardID":"1064688","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["475098",475098,475099],"PO":["562974"]},"177347":{"abstract":"Mobile devices such as smartphones and tablets are intended for usage under demanding physical conditions. As a result, their displays face two unique challenges. First, contextual factors, including the viewing angle, ambient lighting, and unwanted shaking of the device, tend to distort the perceived display content. Moreover, the display is known to be among the largest power consumers on a state-of-the-art mobile device. The goal of this project is to provide the algorithmic foundation as well as software and hardware solutions required for addressing these two challenges. As a close collaboration between Rice University and the University of Southern California, the project targets the following scientific contributions. (1) Sensor-based inference of viewing context that efficiently obtains the viewing context from sensors available in modern mobile devices. (2) Compensation for contextual factors that transforms the display content to produce an improved user perception in terms of fidelity and\/or usability. (3) Supply voltage scaling and color\/image transformations that modify the display data to significantly reduce the power consumption of mobile OLED displays under human perceptual constraints. <br\/>By improving the performance and energy efficiency of mobile displays under challenging viewing conditions, this project will extend the reach of mobile computing beyond its current levels, therefore, having a far-reaching socio-economic impact. Through the NSF\/FDA Scholar-in-Residence (SIR) program, graduate students working on this project will visit the US FDA and collaborate with FDA researchers to apply research results to medical imaging problems with mobile displays. The multidisciplinary nature of the proposed research will invite new research problems in system architecture and circuit design, human-computer interaction, image processing, and photometry. It will also provide a multidisciplinary platform for undergraduate and graduate education and research.","title":"SHF: Medium: Collaborative Research: System Solutions for Context-aware and Energy-Efficient Mobile Displays","awardID":"1065506","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["548312"],"PO":["565255"]},"177358":{"abstract":"Mobile devices such as smartphones and tablets are intended for usage under demanding physical conditions. As a result, their displays face two unique challenges. First, contextual factors, including the viewing angle, ambient lighting, and unwanted shaking of the device, tend to distort the perceived display content. Moreover, the display is known to be among the largest power consumers on a state-of-the-art mobile device. The goal of this project is to provide the algorithmic foundation as well as software and hardware solutions required for addressing these two challenges. As a close collaboration between Rice University and the University of Southern California, the project targets the following scientific contributions. (1) Sensor-based inference of viewing context that efficiently obtains the viewing context from sensors available in modern mobile devices. (2) Compensation for contextual factors that transforms the display content to produce an improved user perception in terms of fidelity and\/or usability. (3) Supply voltage scaling and color\/image transformations that modify the display data to significantly reduce the power consumption of mobile OLED displays under human perceptual constraints. <br\/>By improving the performance and energy efficiency of mobile displays under challenging viewing conditions, this project will extend the reach of mobile computing beyond its current levels, therefore, having a far-reaching socio-economic impact. Through the NSF\/FDA Scholar-in-Residence (SIR) program, graduate students working on this project will visit the US FDA and collaborate with FDA researchers to apply research results to medical imaging problems with mobile displays. The multidisciplinary nature of the proposed research will invite new research problems in system architecture and circuit design, human-computer interaction, image processing, and photometry. It will also provide a multidisciplinary platform for undergraduate and graduate education and research.","title":"SHF: Medium: Collaborative Research: System Solutions for High-Quality and Energy-Efficient Mobile Displays","awardID":"1065575","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["518663"],"PO":["565255"]},"177369":{"abstract":"Statistical analysis is key to many challenging applications such as text classification, speech recognition, and DNA analysis. However, often the amount of data available is comparable or even smaller than the set of symbols (alphabet) constituting the data. Unfortunately, not much is known about optimal inference in this so-called large-alphabet domain. Recently, several promising approaches have been developed by different scientific communities, including Bayesian nonparametrics in statistics and machine learning, universal compression in information theory, and the theory of graph limits in mathematics and computer science.<br\/><br\/>The investigators study the problem drawing from these multiple perspectives, but with a particular focus on developing the information theoretic approach. The research studies analytical properties of the \"pattern maximum likelihood'' estimator, which performs well in practice but is not understood theoretically, and also explores computational speedups. Moreover, it attempts to delineate which problem classes are better handled by Bayesian nonparametric techniques and which by the pattern approach, and explores links between these approaches. The investigators use the resulting theory for automatic document classification, allowing for more automation in storing, retrieving, and analyzing data. Furthermore, the investigators use the theory to study genetic variations, whose link with disease diagnosis is a crucial step in the systematic quantification of biology that is playing an increasingly important role in medical advancement. The research also brings new courses to the classroom, with a special outreach effort to involve women and under-represented minorities, including through the Native Hawaiian Science and Engineering Mentorship Program.","title":"CIF: Medium: Collaborative Research: Information Theory and Statistical Inference from Large-Alphabet Data","awardID":"1065622","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["486456"],"PO":["564924"]},"181560":{"abstract":"Globalization and horizontal stratification of the semiconductor industry has exacerbated the threat of a compromise in the integrated circuit manufacturing supply chain. Specifically, intellectual property theft, unauthorized production, and hardware Trojan horse insertion are of significant concern. Further, the proliferation of advanced semiconductor fabrication equipment and expertise has enabled novel attacks against integrated circuits in the field. Recently, side-channel attacks that bypass the theoretical strength of cryptographic algorithms by exploiting inadvertent information leakage have shown to be particularly effective while requiring only a modest budget and expertise. Most extant countermeasures retrofit security features onto existing design flows, rather than build-in security from the ground up. <br\/><br\/>This project develops a novel application-specific integrated circuit (ASIC) architecture designed from the ground up to resist hardware Trojan horse insertion and have low side-channel information leakage, while still achieving high performance, low power, compact area, and future scalability. The architecture consists of an array of logic gates implemented as read-only memory (ROM) look-up tables (LUTs) interconnected by a reconfigurable network. The ROM LUTs achieve efficient operation with low side-channel emissions and are highly scalable due to their regular structure. The reconfigurable interconnect enables design obfuscation, metering of production, and complete internal node observability and controllability in order to detect hardware Trojans. The very nature of the architecture deters Trojan insertion and enables non-destructive post-manufacturing Trojan detection. The architecture has wide applicability across a range of ICs, from microprocessors to ASICs, that require efficient designs capable of secure operation","title":"TC: Small: A Secure Trojan-Resistant Application-Specific Integrated Circuit Architecture","awardID":"1117755","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["548336"],"PO":["565264"]},"181681":{"abstract":"This grant provides travel support to students so that they can attend the International Conference on Software Maintenance (ICSM) which this year is being held at William & Mary in Williamsburg, Virginia in September 2011. ICSM is a high quality conference addressing the research needs of the software development lifecycle. Attending enables students to hear the latest results in the field, meet with other researchers, and build community with the next generation of researchers, including the global community.","title":"Student Travel Support for the 27th IEEE International Conference on Software Maintenance (ICSM 2011)","awardID":"1118336","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["534438"],"PO":["564388"]},"180482":{"abstract":"Data provenance refers to the history of the contents of an object and its successive transformations. Knowledge of data provenance is beneficial to many ends, such as enhancing data trustworthiness, facilitating accountability, verifying compliance, aiding forensics, and enabling more effective access and usage controls. Provenance data minimally needs integrity assurance to realize these benefits. Additionally, provenance data may need assurances of confidentiality (e.g., protect the identity of a reviewer in a blinded paper review process from the authors but not from the editor) or of privacy (e.g., do not disclose identity of a source without the source's consent). In the past decade there has been significant progress regarding the structure and representation of provenance data as a directed acyclic graph. However, currently there is no overarching, systematic framework for the security and privacy of provenance data and their tradeoffs with respect to the utility of provenance data. The development of such a framework is recognized as one of a handful of promising thrusts in recent reports on Federal game-changing R&D for cyber security, particularly aligned with the theme of Tailored Trustworthy Spaces.<br\/><br\/>This project is to develop a comprehensive technical and scientific framework to address the security and privacy challenges of provenance data, and the attendant tradeoffs, so that our society can gain maximum benefit from applications of provenance data. Detailed foundational research is to be performed on security enhanced data models, access control and usage models, privacy including annonymization and sanitization, integrity, accountability and risk management techniques for provenance data. This foundational research is complemented by data provenance case studies in scientific and cyber security information sharing, and construction of prototype data provenance systems at the operating systems and data layers. Moreover, reference architectures and definitions of corresponding provenance management services are to be defined, identifying how these services can be effectively deployed in enterprises, and developing a risk-management framework to guide application architects, designers and users to effectively embed data provenance in their specific context. The project results will beneficially impact society at large by increasing trustworthiness of data acquired, transmitted and processed by computer systems. From the educational side, both theory and practice of data provenance are to be integrated in the undergraduate and graduate training of students, including underrepresented minority and female students, in all the collaborative institutions of this project.","title":"TC: Large: Collaborative Research: Privacy-Enhanced Secure Data Provenance","awardID":"1111529","effectiveDate":"2011-08-01","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["562519","542685","558641"],"PO":["565264"]},"181582":{"abstract":"Uncertainty is inherent in scientific data. Quantification and interpretation of the uncertainty are essential to a holistic understanding of the data. Uncertainty representation is often overlooked in high-dimensional and multivariate data due to a lack of effective methods and a lack of guidance on appropriate existing methods. This project systematically investigates uncertainty visualization in high-dimensional data, within the application domain of meteorology. The interdisciplinary research team spans different areas such as visualization, atmospheric science, and statistics. The research team is investigating (1) quantifying uncertainty by using bootstrap mean confidence intervals, which relax assumptions about the underlying distribution; (2) designing a number of uncertainty visualization methods and implement these methods in an integrated user interface; and (3) employing both qualitative and quantitative user studies to evaluate the uncertainty visualization methods.<br\/><br\/>The resulting tools and methods are deployed to the Environmental Modeling Center. Although the primary application area of this project is meteorology, the principles learned have the potential to be applied to a variety of scientific fields, including hydrology, geospatial science, medical imaging, and biology. The research work of this project is integrated into Visualization and Meteorology classes at Mississippi State University.","title":"CGV: Small: Quantification and Visualization of Ensemble Uncertainty","awardID":"1117871","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["551043",486506,486507,"541999",486509],"PO":["564316"]},"181230":{"abstract":"Wireless multimedia broadcast has been the central of human entertainments for a century. While the broadcasting signaling is being transformed to digital, its infrastructure is still incapable of supporting the full potential of wireless digital broadcast. This project develops a comprehensive framework to enable scalable and adaptive wireless multimedia broadcasting for an envisioned cellular-type infrastructure, as well as for infrastructures in transitional stages. The project consists of three components: 1) an adaptive channel coding scheme, 2) a stochastic power control and antenna beamforming algorithm, and 3) a feedback channel medium access control protocol. In the first step, the research develops an adaptive coding scheme to enable efficient and scalable broadcast communication over dynamic wireless channels with minimum channel information at the transmitters. In the second step, the research develops a stochastic power control and antenna beamforming framework to optimize energy efficiency of the broadcast system under communication throughput and delay constraints. In the third step, the research develops a medium access control protocol to feedback necessary channel information without sacrificing system scalability using a minimum amount of feedback channel bandwidth. This project also develops a demonstration system in wireless computer networks.","title":"NeTS: Small: Foundations of Energy Efficient Wireless Multimedia Broadcasting","awardID":"1116134","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[485630],"PO":["565303"]},"181593":{"abstract":"Central to the imaging process is the interaction of light with the objects in the scene. Remarkable progress has been made over the past several hundred years on solving inference problems (such as detection, classification, or estimation) for a large class of objects constructed from simple materials with Lambertian reflectance. Such objects scatter light such that the apparent brightness is invariant to the observer's view angle. Unfortunately, real world objects are made of considerably more complex materials that cannot be characterized in terms of such an isotropic reflectance. While humans are able to effortlessly reason about complex materials, today's image analysis and processing algorithms fail miserably. The reason is that complex, non-Lambertian materials can be characterized only by higher-dimensional functions that are relatively poorly understood and even more poorly modeled. <br\/><br\/>This research is developing new ways to model, capture, and process the rich reflectance patterns of complex materials. The key tool is the object's plenoptic transport function, which describes the transformation of the incident to the irradiated light due to the properties of the material. In full generality, the plenoptic transport function is 14-dimensional; hence, a fundamental complication for sensing, analysis, and processing systems for complex materials is the dimensionality gap between the high dimensional plenoptic function and the ability of most conventional sensors (cameras) to acquire at best 2D or 3D image projections. The tools and techniques under development include sparse and manifold models (to bridge the dimensionality gap), geometric features (to mitigate the presence of environmental illumination and other nuisance parameters), and new sensor designs (to most efficiently acquire plenoptic information from natural scenes).","title":"CIF: Small: Computational Tools for Visual Inference of Complex Materials","awardID":"1117939","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7936","name":"SIGNAL PROCESSING"}}],"PIcoPI":["497441","557517",486535],"PO":["564898"]},"181362":{"abstract":"Systems today must cope with failures induced by many factors outside the control of the organization producing the software: faults in infrastructure and components developed by third-parties, unpredictable loads, and variable resources. Modern systems must therefore take increasing responsibility for problem detection and repair at runtime. Effective fault detection and repair could be greatly enhanced by run-time fault diagnosis and localization -- the ability to identify the source of problem so that appropriate actions can be taken either by a human operator or automated mechanisms to repair the system. <br\/><br\/>In this research we are developing new foundations for run-time fault diagnosis and localization. To do this we are extending and synthesizing recent advances in two areas. The first is the use of architecture models for monitoring a system at run-time. The second is the use of spectrum-based reasoning for fault localization (SFL). SFL is a lightweight technique that takes as its input a form of trace abstraction and produces a list of likely fault candidates, ordered by probability of being the true fault explanation. It has been used with impressive results during design time but thus far has not been exploited at runtime in the context of architecture-based monitoring and diagnosis. <br\/><br\/>This research will improve the trustworthiness and robustness of modern software systems by providing new techniques for diagnosing faults while a system is running, thereby providing an improved basis for fault detection and resolution.","title":"CSR: Small: Architecture-based Run-time Fault Diagnosis","awardID":"1116848","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[485947],"PO":["565255"]},"181483":{"abstract":"Statistical spoken dialogue systems (SDSs) use reinforcement learning to learn a dialogue policy that decides what to do based on the dialogue context (also called dialogue state). Previous work on this problem has mainly addressed slot-filling dialogue, in which the user presents a complex request (e.g. an appointment booking), and the system tries to fill a set of slots (e.g. date and time) to satisfy the user's request. This project significantly extends and generalizes prior work by allowing automated dialogue policy creation for other genres of dialogue including question-answering and negotiation. The following open research issues are investigated: (1) the extent to which the three very different genres of dialogue (slot-filling, question-answering, and negotiation) can be represented using the same kind of dialogue policy representation; (2) whether state-of-the-art learning techniques, that work well for small state spaces and simple interactions, can scale to the needs of more complex dialogues and larger state spaces; (3) methods for compactly representing the dialogue state and for combining learned and hand-crafted policies; (4) development of automated metrics for measuring the quality of simulated users and learned policies; (5) validation of those metrics with respect to how well they correlate with human evaluations.<br\/><br\/>Statistical SDSs facilitate easier creation of dialogue systems (less hand-crafting by dialogue system experts) that are more tuned in to user behavior (learning policies from data and simulation). This project broadens the types of systems that can be developed with this kind of approach (not just slot-filling, but also simple question-answering and more complex negotiation). The advances made in the project are encoded in a toolkit (to be publicly distributed) specifically designed for statistical dialogue management. This toolkit allows broader access to this technology, by providing the potential to attract more researchers from academia and industry to the field of SDS, and make the use of statistical techniques available to non-experts; it can also be an excellent resource for teaching statistical dialogue management to students.","title":"RI: Small: Reinforcement Learning for Realistic Statistical Spoken Dialogue Systems - Beyond Slot-Filling Applications","awardID":"1117313","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["560564",486241],"PO":["565215"]},"180394":{"abstract":"Despite the prevalence of social network platforms and apps in nowadays daily life, existing research on social media takes the terms \"social\" and \"media\" separately, and fails to address important needs for intelligently managing and utilizing social media, such as finding the information that users want, situating information in a social context that gives it meaning, and providing order and structure to an intricate and intertwined network of relationships. This interdisciplinary project will provide a holistic view of social media by combining socially intelligent language processing with linguistically motivated social network analysis. Specifically, the project will: (a) discover sociolinguistic communities and identify the demographic and sociological factors that underlie community membership; (b) discover cross-community linguistic variation at various levels and develop new computational tools for dialectometric and sociolinguistic analysis and for prediction of user interests and trends; and (c) recommend content and social connections across community boundaries, which will help people to broaden their perspectives with new information, opinions, and social relationships.<br\/><br\/>Intellectual merit: The project will lead to (a) new modeling formalisms that jointly incorporate linguistic information with social network metadata; (b) a new computational methodology for sociolinguistic investigation from raw text; and (c) flexible models of linguistic variation that model temporal dynamics and move beyond simplistic bag-of-words approaches to higher-order phenomena such as multi-word expressions, syntax, and joint orthographic variation. <br\/><br\/>Broader impacts: The project will lead to advancements in basic research in statistical machine learning, social sciences, and language technology. It will also bring innovations and practical applications in all these areas, such as software that reasons intelligently about community structures and linguistic patterns and conventions in social media. The findings will benefit a wide range of needs, such as personalized information service and intelligence and security operations, which require precise and timely understanding of social-cultural events and trends. The project will also provide undergraduate research opportunities and outreach to high school students through summer programs.","title":"Collaborative Research: Discovering and Exploiting Latent Communities in Social Media","awardID":"1111142","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7953","name":"SOCIAL-COMPUTATIONAL SYSTEMS"}}],"PIcoPI":["562759","518220"],"PO":["564456"]},"181252":{"abstract":"While energy costs of data centers continue to double every five years, what is most disappointing is that most of this power is wasted. Servers are only busy 10-30% of the time on average, but they are often left on, while idle, utilizing 60% or more of peak power. This project, being conducted at Carnegie Mellon University, investigates simple load-oblivious, distributed auto-scaling of the data center capacity to reduce power consumption. Our policies scale the number of servers that are on, as a function of changing load, shutting down servers, or putting them into sleep states, or changing the frequency at which they are run, without knowing the load in advance. Our work combines full-scale implementation in a 23-server multi-tier data center together with queueing-theoretic analysis to reduce the search space of solutions and guide the algorithmic development. We expect to develop algorithms which significantly reduce power consumption, while still meeting response time service level agreements. Broader impacts of this work include direct collaboration with Intel Pittsburgh labs, publication of a textbook on queueing-theoretic modeling and its applications, and weekly coaching and management of the Western PA American Regions Mathematics League, to train the smartest young minds of tomorrow.","title":"CSR: Small: Simple Dynamic Traffic-Oblivious Power Management for Multi-Tier Web Clusters","awardID":"1116282","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["555524"],"PO":["565255"]},"181274":{"abstract":"Today, multimedia data are produced and consumed in massive quantities in a broad range of applications with significant economic and societal benefit, including e-commerce, surveillance, education, web services, and social media. Hence, there is an urgent need for systems to provide highly scalable processing and efficient analysis of large media data collections. The RanKloud prototype system, developed in this research project, focuses on the needs and requirements of applications that deal with large quantities of multimedia data in a cloud-based scalable environment. <br\/><br\/>Most multimedia applications share a few core operations, including integration\/fusion, classification, clustering, graph analysis, near-neighbor search, and similarity search. When performed naively, however, these core operations are often very costly, because the number of objects and object features that need to be considered can be prohibitive. Avoiding this cost requires that redundant work is avoided. This research focuses on the next generation cloud-based massive media processing and analysis systems where the fundamental principles that govern their design include an awareness of the utilities of data and features to a particular analysis task. Incorporating data and feature utilities for performing a particular utility task is expected to significantly reduce the overall cost of the analysis task. The RanKloud project research plan includes: (1) data model and query language to specify multimedia data processing workflows; (2) adaptable, rank-aware parallel multimedia data processing primitives; (3) run-time data sampling strategies to support adaptation to data and resource; and (4) waste- and unbalance-avoidance strategies for utility-aware data partitioning, resource allocation, and for incremental batched processing.<br\/><br\/>RanKloud bridges an important gap in our understanding of cloud-based computing in general, and efficient processing of multimedia data in particular. The results are expected to enable new tools and systems supporting scalability in a large class of problems in content-aware multimedia and social media analysis with impact in web intelligence, business intelligence, and scientific and sensor applications all of which need to handle imprecise multimedia data for more effective decision making. This project provides research experience opportunities for graduate and undergraduate students and includes research results and challenges in courses, including Capstone projects. Arizona State University (ASU) recruits top-quality undergraduates through a nationally recognized residential Honors College and the Minority Access to Research Careers program. The national and international dissemination of the project results includes premier conference and journal publications, as well as open source software licenses at the RanKloud Web site (http:\/\/aria.asu.edu\/rankloud).","title":"III: Small: RanKloud: Data Partitioning and Resource Allocation Strategies for Scalable Multimedia and Social Media Analysis","awardID":"1116394","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["558560","551811","558561"],"PO":["563751"]},"181296":{"abstract":"The project will conduct a series of user studies to investigate the effectiveness of error reporting with novice programming students. The studies will focus on how students understand error messages, the effectiveness of source-code highlighting, and the impact of language syntax on how students process errors. The PIs will instrument a programming environment to collect fine-grained data on how students respond to errors. Such scientific user studies, particularly at a fine-grained level, are rare in this context; the methodologies and tools developed for this project will be applicable to future studies. Study results will be used to improve programming environments for novice programmers with regards to interfaces for reporting errors and linguistic choices regarding syntax and type systems. Findings will identify principles for others who design new programming tools for novices. Results will be disseminated through the DrRacket environment, which is in wide use in introductory computing courses in universities, high schools, and after-school programs for middle school students.","title":"SHF: Small: User Studies to Improve Novice Programming","awardID":"1116539","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["485803","519555","533845"],"PO":["564388"]},"181197":{"abstract":"Recently, reconstructing full and detailed 3D models for large-scale urban environments has become the crucial technology for many applications and offers a natural platform for different services. Although conventional structure from motion (SFM) techniques have been engineered to their maturity, they normally do not utilize rich global structures of urban scenes, including regularity, symmetry, and self-symmetry; and the very repetitive shapes and textures ubiquitous in urban scenes make detecting and matching features extremely challenging for the conventional techniques. <br\/><br\/>This project develops a novel approach for inferring highly accurate 3D geometry from an individual or multiple 2D images of an urban scene. It takes full advantage of the rich global symmetry and regularity in urban scenes by leveraging powerful computational tools from modern high-dimensional convex optimization. The developed method can accurately recover the regular 3D geometry and 2D texture of the scene directly from the raw image pixels\/regions without relying on extracting any intermediate local features. The research includes developing a set of useful tools and a full system that can significantly improve the efficiency and scalability of 3D modeling of large urban scenes and give significantly more compact representation of the 3D geometry and 2D appearance, enabling online real-time rendering and visualization.<br\/><br\/>Research results from this project can be easily integrated into and significantly improve the current computer vision course on 3D reconstruction. The associated technologies can be useful for a very wide range of commercial applications such as online or mobile visual search, visual guidance, navigation, or surveillance, virtual tourism, and augmented reality etc.","title":"RI: Small: A Region-Based Approach to Reconstructing Urban Scenes","awardID":"1116012","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["518402","311520"],"PO":["564316"]},"185345":{"abstract":"Proposal #: CNS 11-39707<br\/>PI(s): Tsugawa, Mauricio; Figuereido, Renato J.; Fortes, Jose A.<br\/>Institution: University of Florida<br\/>Title: IT Virtualization for Disaster Mitigation and Recovery<br\/>Project Proposed:<br\/>This RAPID project, aiding the process of recovering Information Technology (IT) infrastructure damaged by catastrophic events, conducts research on the use of virtualization technologies to provide such aid. The work includes IT infrastructure needed to recover damages to non-IT infrastructures and human beings. Machine virtualization offers key mechanisms to move applications from one location (e.g., a data center) potentially affected by a disaster to another safe location. The project responds to many challenges such as: The<br\/>- Inability to migrate Virtual Machines (VMs) from a disaster site to an unaffected site maintaining live services;<br\/>- Severe limitation of power of network failures that limit the ability of performing live-migrations;<br\/>- Need for coordination with recovery efforts to effectively prioritize critical services.<br\/>Machine virtualization offers the ability to checkpoint VMs, thus enabling the creation of back-ups not only of data but also of partial application executions. VM checkpoints can be used to recover an IT infrastructure in a different location with minimal loss of data. The challenge lies in how to efficiently manage the massive amount of data and network traffic generated by the VM check-point process.<br\/>With the main goals of keeping alive IT services as long as possible, and restoring recovery-critical IT services as quickly as possible during and after a disaster, the project focuses on<br\/>- Analyzing data and events associated with damaged IT services due to the Great-East Japan Earthquake,<br\/>- Studying scalability of wide-area VM live-migration and Back\/checkpoint, and<br\/>- Developing a resilient architecture to partial physical infrastructure failure in order to deploy IT infrastructures in virtualized and distributed datacenters. <br\/>The investigators collaborate with Dr. Satoshi Sekiguchi, Director of the Information Technology Research Institute (ITIR) within the National Institute of Advanced Industrial Science and Technology (AOST), an Institution under the Ministry of Economy, Trade, and Industry (MET), Japan. This group are experts in the area of virtualization and has had some interactions with the Florida group.<br\/>Broader Impacts: <br\/>The work develops an understanding of how well virtualized IT systems can cope with partial physical damages, of what changes in hardware, software, and general practice are needed, and how to determine the best way to adopt them. In the long term the project should enable informing the adoption of a virtualized datacenter to host essential IT services. Hence, the project is likely to enable informed decisions and should also contribute in graduate student education.","title":"IT Virtualization for Disaster Mitigation and Recovery","awardID":"1139707","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["540130","558495","540132"],"PO":["557609"]},"177304":{"abstract":"This project explores (semi-)automatic ways to create \"semantically discriminative\" mid-level cues for visual object categorization, by introducing external knowledge of object properties into the statistical learning procedures that learn to distinguish them. In particular, the PIs investigate four key ideas: (1) exploiting taxonomies over object categories to inform feature selection algorithms such that they home in on the most abstract description for a given granularity of label predictions; (2) leveraging inter-object relationships conveyed by the same taxonomies to guide context learning, so that it captures more than simple data-driven co-occurrences; (3) exploring the utility of visual attributes drawn from natural language, both as auxiliary learning problems to bias models for object categorization, as well as ordinal properties that must be teased out using non-traditional human supervision strategies; (4) mining attributes that are both distinctive and human-nameable, moving beyond manually constructed semantics.<br\/><br\/>The project entails original contributions in both computer vision and machine learning, and is an integral step towards semantically-grounded object categorization. Whereas mainstream approaches reduce human knowledge to mere category labels on exemplars, this work leverages semantically rich knowledge more deeply and earlier in the learning pipeline. The approach results in vision systems that are less prone to overfit incidental visual patterns, and representations that are readily extendible to novel visual learning tasks. Beyond the research community, the work has broader impact through inter-disciplinary training of graduate and undergraduate students, and outreach to pre-college educators and students through workshops and summer camps encouraging young students to pursue science and engineering.","title":"RI: Medium: Collaborative Research: Semantically Discriminative: Guiding Mid-Level Representations for Visual Object Recognition with External Knowledge","awardID":"1065243","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["497149"],"PO":["564316"]},"178305":{"abstract":"This project focuses on network formation by self-interested agents. Specifically, it studies contribution games in social networks, where players\/nodes not only form connections\/relationships with others, but also determine the intensity of their connections. This project will analyze the quality of equilibrium solutions, convergence properties of local dynamics, and mechanisms to improve these properties. This work will result in efficient algorithms for computing various reasonable outcomes of contribution games, and algorithms for \"nudging\" the players to form better outcomes by giving them small incentives, using approximation algorithms for otherwise intractable problems. More concretely, this research will consider the following topics. (1) Analyzing a model of Network Contribution Games that captures the essence of players optimizing their local relationships. Concepts such as Nash equilibrium are of limited usefulness for such models, so the project instead focuses on pairwise equilibrium, strong equilibrium, and convergence dynamics. (2) General Contribution Games are an important extension of network contribution games: instead of pairwise relationships, players contribute to many-person projects. (3) Centrality, Friendship, and Altruism. Other than the desire to maximize total success of all projects\/relationships players are involved in, they may also desire to form connections that make them well-positioned in the social network, and so attempt to maximize some notion of \"centrality\" or they may care about the quality of the network as a whole, as in games with public goods. This project will analyze games where the utility of a player is a combination of such considerations, and derive general results on the structure and quality of equilibrium solutions depending on the types of player utility functions.<br\/><br\/>Social networks pervade most aspects of human life, and affect everything from the spread of epidemics to the ability to find a job. Now that data on social networks has become easier to collect, understanding the structure, properties, and incentives of social network formation has become a crucial research topic. By explicitly taking into account the strength of bonds in a social network, and not just their existence, this work will add a new dimension to game-theoretic models of social networks, and obtain new insight into participant behavior, and into the structure of social networks. This work may also suggest efficient methods for \"nudging\" the participants of a social network to form better outcomes by giving them small incentives. In addition, this project will contribute new techniques for analyzing systems with limited collusion. The study of contribution games where participants care about centrality, friendship, and altruism in addition to their local success, will be the first major attempt to understand the structure of social networks under such a combination of player incentives. This project will include collaborations with both theoretical and applied researchers working on social networks, including researchers from complex systems, epidemiology, sociology, and the school of management. Because of wide-spread interest in social networks, this project may inspire new approaches in many of the fields mentioned above. Finally, this research will be strongly complimented by the PI's education plan, which includes teaching several courses with research components, forming a workshop on this topic, and recruiting several graduate and undergraduate students to work on this project.","title":"ICES: Small: Contribution Games in Social Networks","awardID":"1101495","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8052","name":"Inter Com Sci Econ Soc S (ICE)"}}],"PIcoPI":["518263"],"PO":["565251"]},"177337":{"abstract":"Energy-proportional and low power computing, together with the increasing demands for high resiliency to random faults could benefit enormously from a \"Unified Memory\" - a memory cell that operate either as a dynamic cell or as a non-volatile cell, and switch seamlessly between the two states. This effort focuses on the development of such a cell based on a dual floating gate concept. The gate stack is optimized to low voltage operation. Because the dynamic storage mode relies on direct tunneling, the electric fields required are modest, and ALD methods are used to construct robust oxides, this device has potential for high endurance when operating in the dynamic storage mode. The device has a relatively low leakage compared to a DRAM cell and thus needs less frequent refreshing. The read is fast - almost SRAM speeds - and non-destructive, reducing power needs. Conversion between dynamic and non-volatiles modes is very fast - an entire row at a time, and consumes a lot less power than interfacing to a solid-state drive. The device can simultaneously store a non-volatile bit with a different volatile bit enabling fast in-situ check-pointing and rollback or state-saving to improve error resiliency.","title":"CSR: Medium: Computing via Monolithic Three Dimensional Assembly of Novel Floating Gate Transistors and Thin Film Semiconductors","awardID":"1065458","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[475429,"521979","556339","518384"],"PO":["565255"]},"188469":{"abstract":"Software maintenance and evolution is a vital and resource consuming phase of the software lifecycle. Introducing software changes is a particularly complex phenomenon in case of long-lived, large-scale, and globally distributed systems. Years of research efforts have recognized three core tasks to support developers during software maintenance: feature location (a starting point of a change in source code), impact analysis (other software entities that are also change prone), and expert developer recommendations (appropriate developers to implement changes). The research will develop a novel one-stop solution for these tasks by integrating and mining the latent information cluttered in structured and unstructured software artifacts produced and constantly changed during evolution of software systems, which are largely untapped in current solutions.<br\/><br\/>This research program has three main goals: 1) Define a new integrated framework SE2 for a comprehensive analysis of software evolution, based on conceptual and evolutionary information, under a single umbrella, 2) Define new methodologies for software maintenance tasks based on SE2, and 3) Perform empirical studies to evaluate SE2 and supported methodologies. Central to our solution are the state of the art data mining, information retrieval, and program analysis methods. The research will formulate both theoretical foundations and deliver novel practical solutions to uniformly represent, analyze, and use them within the SE2 framework. Among the broader impacts the project includes production of software tools under open source licenses and collaboration with industry to transfer technology.","title":"III: Small: Collaborative Research: An Inductive Framework to Support Software Maintenance","awardID":"1156401","effectiveDate":"2011-08-07","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":[505347],"PO":["564388"]},"180670":{"abstract":"From the viewpoint of researchers, developers, and even consumers, reliability analysis is recognized as an indispensable step before sensor network systems can be widely deployed for mission-critical applications. However, there is a big gap between traditional network reliability analysis theories and realistic sensor network system environments. The primary objective of this research is to close such gap through new metrics, analytical models, and approaches for the reliability analysis of large-scale sensor networks. Specifically, the research has three major components: 1) concepts, metrics, and models for describing the reliability behavior of sensor networks, 2) new algorithms and methodologies for efficient reliability evaluation of sensor networks, and 3) applications\/case studies for demonstrating the practical significance of the proposed theoretical approaches. The new reliability theories and methodologies developed through this project are fundamental contributions to the body of knowledge on the network system reliability as well as to the design of reliable sensor systems. Additionally, a software tool and a prototype sensor system for railway monitoring will be built to validate the proposed research, and render the theoretical research operational and applicable. This project has broader impacts through graduate and undergraduate education, and through an integrated outreach and group mentoring program that will promote recruitment and retention of female students in engineering disciplines.","title":"CSR: Small: Collaborative Research: Bridging Reliability Analysis and Reality in Sensor Systems: Theories and Applications","awardID":"1112935","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["491296"],"PO":["565255"]},"181440":{"abstract":"Much of today's information infrastructure makes use of virtual<br\/>machines (VMs). For example, consumer devices typically contain VMs<br\/>for Java, JavaScript, and Flash. VMs are often well suited to roll out<br\/>new capabilities to a broad range of target devices, which is why new<br\/>VMs are being created all the time. Unfortunately, most VMs today are<br\/>created from scratch, in spite of the fact that they are broadly<br\/>similar. This is a considerable waste of programmer time and money.<br\/>The goal of this project is to make creating VMs simpler, by creating<br\/>a \"toolbox\" of components from which new VMs can be constructed<br\/>easily.<br\/><br\/>The technical challenge is to create building blocks that can be put<br\/>together to work as well as the custom-crafted VMs of today. In many<br\/>cases, VM performance is critical. For example, on mobile devices, a<br\/>slower VM might force us to run the processor at a higher speed,<br\/>leading to faster battery depletion. This research promises that <br\/>one may one day be able to create a competitively<br\/>performing VM for Java, JavaScript and Flash in which some parts are<br\/>shared, so that the total size is much smaller than three separate<br\/>VMs. This is important for resource-constrained devices such as<br\/>mobile phones. Re-using components rather than reinventing the wheel<br\/>each time would reduce the entry cost for new languages. Sharing<br\/>components would also lead to pooling of debugging resources, reducing<br\/>errors and thereby leading to a safer and more stable computing<br\/>infrastructure.","title":"SHF: CSR: Small: Fine-Grained Modularity and Reuse of VM Components","awardID":"1117162","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[486136],"PO":["564588"]},"181561":{"abstract":"Computer users are increasingly faced with decisions that impact their personal privacy and the security of the systems they manage. The range of users confronting these challenges has broadened from the early days of computing to include everyone from home users to administrators of large enterprise networks. Privacy policies are frequently obscure, and security settings are typically complex. Missing from the options presented to a user is a decision support mechanism that can assist her in making informed choices. Being presented with the consequences of decisions she is asked to make, among other information, is a necessary component that is currently lacking.<br\/><br\/>This work introduces formal argumentation as a framework for helping users make informed decisions about the security of their computer systems and the privacy of their electronically stored information. <br\/>Argumentation, a mature theoretical discipline, provides a mechanism for reaching substantiated conclusions when faced with incomplete and inconsistent information. It provides the basis for presenting arguments to a user for or against a position, along with well-founded methods for assessing the outcome of interactions among the arguments. An elegant theory of argumentation has been developed based on meta rules characterizing relationships between arguments. Rules for argument construction and evaluation have been devised for specific domains such as medical diagnosis. This project investigates argumentation as the basis for helping users make informed security- and privacy-related decisions about their computer systems. Three specific aims are addressed:<br\/>1) Implementation of an inference engine that reasons using argumentation,<br\/>2) Facilitate security management through an argumentation inference engine, a rule base specialized for security management, and sensors providing security alerts all enhanced with an interactive front-end.<br\/>3) Reason about the consistency and completeness of domain knowledge, as it evolves.<br\/>To understand the kinds of domain-specific inference rules required, diverse security applications are studied, such as determining if an attack imperils a particular system, finding the root cause of an attack, deciding on appropriate actions to take in the presence of an uncertain diagnosis of an attack, and deciding on privacy settings. <br\/>Emerging from this project will be a prototype towards the practice of usable security. The team is working with organizations responsible for the security administration of large enterprise networks and will make the prototype tools available to these organizations. The team is working with everyday users from a cross-section of community members. <br\/>Curricular modules that cover the intersection of argumentation and security are being developed and shared.","title":"TC: Small: Collaborative Research: An Argumentation-based Framework for Security Management","awardID":"1117761","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["505729","523503"],"PO":["564388"]},"181462":{"abstract":"As data centers have increased in size, the energy consumed in powering and cooling server clusters has become a significant fraction of the total cost of ownership of operating a data center. As a result, there is an increasing trend towards reducing the energy and carbon footprint of data centers as well as in using green sources of energy such as solar and wind power to run data centers. In this project, we will conduct fundamental research on system support for server clusters that are designed to run using variable power, such as that from renewable sources. We will design a new supply-side technique to manage the energy and power footprint of a data center server cluster and to adapt this footprint to supply-side fluctuations. Our enables compute and data nodes in a server cluster to be duty-cycled so that their power and energy footprint can be gracefully adapted to a varying supply curve. We will design study how distributed storage and server virtualization can be adapted to exploit such duty-cycling of servers to reduce energy use. The broader impacts of this work include a new graduate seminar course on Sustainable Computing, summer REU projects to include undergraduate students, especially from underrepresented groups, in this research, and outreach to K-12 teachers and community college faculty via tutorials in sustainability.","title":"CSR: Small: Designing Server Clusters to Handle Variable Solar and Wind Power","awardID":"1117221","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["558563"],"PO":["565255"]},"181473":{"abstract":"The emergence of rich interactive collaborative applications on desktop and mobile platforms demands a robust network infrastructure. The network must recover quickly from link and node failures and must do so while having only a small amount of information. This project seeks to develop scalable proactive failure recovery techniques that require only a constant factor increase in the information maintained at every node. The techniques developed in this project are applicable to circuit-switched and packet-switched networks. In order to improve the robustness of the network further, link and node performances may be monitored periodically by observing the behavior of end-to-end paths, referred to as network tomography. This project seeks to develop fundamental theories and practical algorithms for the identifiability of link and node metrics from end-to-end path observations. The expected results from this project include: (1) algorithms and protocols that may be employed in the data link and network layers of the protocols stack; (2) tools for evaluating different recovery and monitoring techniques; and (3) novel games based on the research problems, targeted at mobile platforms such as iPhone\/iPod touch and Android-based devices. The results of the project will be integrated into the undergraduate and graduate networking and graph theory courses.<br\/><br\/>The intellectual merit of the project is in identifying the fundamental graph theoretical problems underlying the various techniques developed for recovering from multiple link failures and network tomography. The result will lead to new insights into network design, operation, and monitoring with guaranteed performance.<br\/><br\/>The results of this project will have significant broader impact. The algorithms developed for network tomography is applicable to similar problems in the fields of nanoelectronics, compressive sensing, and power systems management. The impact of games derived from the research concepts will reach beyond the intended audience, as the games will be made available through the iTunes App Store for world-wide dissemination and students will share the games their siblings, relatives, and friends.","title":"NeTS: Small: Efficient Techniques for Failure Recovery and Tomography in Networks","awardID":"1117274","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[486214],"PO":["564993"]},"181242":{"abstract":"The rapid adoption of multicore and manycore processors in every class of computing systems---from mobile handheld devices to high-end data center servers---is increasing our dependence on concurrent software. However, defects in concurrent programs are notoriously difficult for developers to detect and diagnose, because concurrent behavior is fundamentally harder to reason about than sequential behavior. Major software vendors report the tangible negative impacts on software development from concurrent faults, such as reduced development productivity or inadequate concurrency training and tools, along with the significant impact on end-users from defective software. The goal of this research project is to help developers of concurrent software find and understand defects quickly.<br\/><br\/>The research will employ novel approaches that combine testing and dynamic analysis with automated post-mortem statistical analysis of thread interactions. Dynamic analysis reduces false positive fault reports, and statistical data analysis exploits the inherent non-determinism of concurrent execution to produce informative prioritized rankings of candidate faults. The resulting techniques will greatly improve debugging productivity by helping the developer to hone in on the most likely causes of program the resulting techniques and tools will be transformative, and will automate fault-localization for concurrent software. The results of the research will have broader impacts on the state-of-the-art in concurrent software practice, research, and education. The research will prepare globally competitive students in the emerging area of concurrent software and parallel thinking, along with providing intellectual bridges between students in software engineering and in computational science and engineering, and advance diversity goals. Additionally, successes in improving fault-localization techniques for concurrent software will lead to improvements in software quality and, thus, benefit all segments of society that depend on software.","title":"SHF: Small: Locating and Explaining Faults in Concurrent Software","awardID":"1116210","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["530729","558498","558498"],"PO":["564388"]},"180395":{"abstract":"Many compelling applications involve computations that require sensitive data from two or more individuals. For example, as the cost of personal genome sequencing rapidly plummets many genetics applications will soon be within reach of individuals such as comparing one?s genome with the genomes of different groups of participants in a study to determine which treatment is likely to be most effective. Such comparisons could have tremendous value, but are currently infeasible because of the privacy concerns both for the individual and study participants. What is needed is a way to produce the result of the comparison without exposing either party's private inputs. The ultimate aim of this project is to make privacy-preserving computation practical and accessible enough to be used routinely in applications such as personalized genetics, medical research, and privacy-preserving biometrics.<br\/><br\/>Theoretical solutions to this problem, known as secure multi-party computation, have been known for several decades, including a general solution developed by Andrew Yao based on garbled circuits. Because of its extensive memory use and computational cost, however, the garbled circuits approach has traditionally been considered more of a theoretical curiosity than a practical mechanism for building privacy-preserving applications. Recent developments in cryptographic techniques and new implementation approaches are beginning to change this, however, and admit the possibility of scalable, practical secure computation. This project is designing methods for avoiding the memory bottleneck associated with garbled circuit evaluation by aggressively pipelining circuit generation and evaluation, and exploring a variety of techniques for reducing the size of garbled circuits. Another issue the limits the use of secure computation in practice is the need for standard protocols to assume an honest-but-curious adversary who always follows the specified protocol. This project is developing new techniques for dealing with malicious adversaries, improving the standard cut-and-choose and commit-and-prove approaches by using new cryptographic tools and exploring an alternate model in which a verifiable trusted party generates the circuit but is not trusted with any private data. The project is also developing techniques to audit the information that can be inferred from the result of a secure computation. Another goal is to make secure computation more accessible to developers by developing programming tools for defining secure computations at a high level, based on information-flow analysis and program partitioning.","title":"TC: Large: Collaborative Research: Practical Secure Two-Party Computation: Techniques, Tools, and Applications","awardID":"1111149","effectiveDate":"2011-08-15","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[483444],"PO":["564388"]},"185752":{"abstract":"Portable wireless devices such as cell phones are becoming powerhouses of computational ability, with multiple processors, large amounts of memory, touch screens, cameras, and motion sensors all connected to WiFi, Bluetooth, 3G and 4G transceivers. Many of these transceivers can transmit concurrently, and each transmission exposes the user to some level of electromagnetic radiation. The total exposure is potentially cumulative. At the same time, regulatory agencies such as the Federal Communication Commission (FCC) are constantly being challenged to re-evaluate their thresholds for exposure to electromagnetic radiation as new science becomes available and as the public becomes more reliant on portable wireless devices. This research examines using multiple transmitters, commonly used to improve wireless performance, as a tool to minimize exposure.<br\/><br\/>This research: (i) evaluates the exposure effects of a portable device as a function of its transmitter characteristics and the timing and phasing between transmitter elements; (ii) designs communication and constrained coding techniques that minimize exposure while not sacrificing wireless performance; (iii) evaluates the success of these techniques using accepted testing procedures for exposure such as the SAR (specific absorption rate) limit of 1.6 W\/kg. The research characterizes the tradeoff between minimizing near-field energy, which determines exposure, and maximizing far-field energy, which determines wireless communication system performance.","title":"EAGER: Multiple Transmitter Chains to Minimize Exposure to Electromagnetic Radiation in Portable Devices","awardID":"1141868","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["540627","512755"],"PO":["564898"]},"181396":{"abstract":"This project focuses on the development of ALERT: An Architecture for the Emergency Re-tasking of Wireless Sensor Networks. The novelty of this work lies in the theoretical foundation of re-tasking independently-deployed sensor networks, leading to a fundamental understanding of the design principles of capability reallocation and sharing to best satisfy the needs of emergency applications. Both re-tasking and integration of sensor capabilities will be transparent to the emergency applications. The resource-constrained nature of sensors, the wireless communication medium, and the failure-prone networking environment, combined with the dynamic QoS requirements of the emergency applications pose formidable challenges for the design of ALERT. <br\/>The understanding acquired from developing ALERT will promote a wider adoption of sensor network systems in support of guarding our national infrastructure and public safety. This project will result in significant scientific and technological advances that will provide invaluable help with disaster management and search-and-rescue operations. ALERT will have a broad societal impact as sensor networks are being integrated into the fabric of the society. The project will integrate research and education and will lead to the development of new graduate and undergraduate courses in sensor networks and embedded and distributed systems. In turn, these courses and their focus on information integration will introduce novel research topics to undergraduate and graduate students in computer science and engineering that fit within the overall missions of Old Dominion University and Clemson University. Focused efforts will be undertaken to stimulate interest and to facilitate the academic progress of women and underrepresented minorities.","title":"CSR: Small: Collaborative Research: An Architecture for the Emergency Retasking of Wireless Sensors Networks (ALERT)","awardID":"1116976","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["490677"],"PO":["565255"]},"181286":{"abstract":"This project addresses computing clouds, large-scale shared infrastructures that offer practically unlimited hardware to most users and applications. In order to achieve scalable performance, all components of the system, from hardware to operating system, middleware, various servers, and the application itself, need to cooperate. Bottlenecks in components can slow down the entire system. In traditional computer systems (e.g., as modeled by queuing theory), a typical assumption is that their workloads consist of independent jobs. This assumption, which is valid for old-style batch-oriented processing and interactive users, guarantees the appearance of single bottlenecks for an entire system. Single bottlenecks can be relatively easily detected, since they appear as resources reaching saturation (e.g., 100% utilization).<br\/><br\/>The \"independent jobs\" model does not hold for the important class of web-facing applications (e.g., e-commerce) that rely on the popular n-tier architecture. N-tier systems divide the system into a pipeline of processing components, e.g., consisting of web servers, application servers, and database servers. While the n-tier architecture supports good performance scalability at the web server and application server tiers, it also introduces several (sometimes unexpected) strong dependencies among other tiers and components. These dependencies produce an interesting phenomenon called multi-bottleneck. Multi-bottlenecks are characterized by system throughput limited by a ceiling regardless of additional hardware, and no single resource shows average utilization anywhere near saturation. (Anecdotally, this is an increasingly common situation in practice.) Multi-bottlenecks are difficult to find, diagnose, and remove when using traditional performance evaluation methods. They are also important in clouds since they will be the only bottlenecks left after the removal of easily spotted single bottlenecks.<br\/><br\/>This project develops, evaluates, and refines a systematic search method, called Telescoping, to find multi-bottlenecks by running large scale experiments on production clouds. A simulator generates well-defined multi-bottlenecks to help refine the Telescoping search method and tune its parameters. Then, n-tier benchmarks such as RUBiS and RUBBoS (e-commerce applications) on production clouds such as Open Cirrus, Amazon EC2, and Emulab, gather experimental evidence on multi-bottlenecks. These experiments shed light on a little-known phenomenon in a rich, but unexplored area (performance limits of jobs with dependencies). Success can lead to significant new developments in the theoretical understanding of jobs with dependencies and improve practical uses of clouds by n-tier systems.","title":"CSR:Small: Multi-Bottlenecks: What They Are and How to Find Them","awardID":"1116451","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["532971"],"PO":["551712"]},"182386":{"abstract":"Economic competitiveness relies upon innovation and organizations are increasingly pursuing open innovation through crowdsourcing. Though several models have arisen for how to tap into the crowd for ideas, little research has been done to understand and assess this new phenomenon. Previous work has investigated relatively simple crowd-sourcing techniques such as Amazon Mechanical Turk, but additional web-enabled platforms for organizing large numbers of individuals to contribute ideas and solutions to a client's innovation challenges have been developed. Firms providing such platforms for crowdsourcing are now competing with traditional innovation consultancies and together serve as intermediaries for open innovation. This project investigates and compares the socio-technical systems that these open innovation intermediaries have created.<br\/><br\/>Successful innovation requires translating and integrating ideas in science, engineering, design, and business originating from diverse organizations, industries, and countries. The current literature on this process focuses on collocated teams with little attention paid to organizational aspects the emergent technology-brokering phenomenon. This research extends the theory of technology brokering to this new form of virtual organization, building a more general framework of how technology brokering occurs and the role of individual backgrounds, organizational practices, and social networks in spanning multiple boundaries. It will explore the tradeoffs involved in increased reliance on codified methods to support technology brokering and investigate the role of institutional contexts in shaping the relative positions and organizational practices of technology brokers. Through in-depth, multi-site field studies this project will develop a theory of how technology brokering is accomplished in the new era of crowdsourcing and document this new phenomenon.<br\/><br\/>As web use has grown, new web-enabled methods have been developed for collecting ideas from the large globally distributed population of inventors. Crowdsourcing can be an effective innovation technique and these new platforms are being used to find solutions for such diverse problems as cleaning up oil spills, improving diaper absorbency, finding biomarkers for diseases, and designing new rail services. Understanding the advantages and disadvantages of different forms of crowdsourcing for various types of problems can help firms optimize their innovation processes and enhance economic competitiveness. Successful crowdsourcing also provides intellectual and economic opportunities for members of underrepresented groups and for scientists and engineers in impoverished regions. Research results will be disseminated broadly through curricular materials appropriate for undergraduate and MBA and through a cross-disciplinary research workshop focused on the crowdsourcing of innovation.","title":"VOSS Collaborative Research: Open Innovation Intermediaries: Brokering Technology and Wisdoms of the Crowds","awardID":"1122413","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7642","name":"VIRTUAL ORGANIZATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7953","name":"SOCIAL-COMPUTATIONAL SYSTEMS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"5376","name":"INNOVATION & ORG SCIENCES(IOS)"}}],"PIcoPI":[488622],"PO":["565342"]},"181297":{"abstract":"Today, the primary tool used for evaluating future computer system designs is simulation, where each machine instruction and its impact on processor and memory state are simulated in great details. While powerful and versatile, processor simulation is exceedingly slow: roughly four to five orders of magnitude slower than native execution. In the future, solely relying on processor simulation will be too constraining, due to simultaneous increase in the complexity of hardware and software systems, and variety of workloads. Analytical modeling is an attempt to capture fundamental relationships between various parameters of a design and to capture how they affect performance or power\/energy consumption. An analytical model is much faster to run and can be reasonably accurate. Unfortunately, widespread uses of analytical models have been hampered by the lack of widely available toolset that researchers can use and build upon, and the high barrier of entry into using and developing them. The objective of this proposal is to integrate various disparate analytical models into a single toolset that can be used by researchers for computer architecture design and evaluation. The expected outcome of this project is a tool that aids processor simulation through three important roles: providing ability to reason about what parameters determine the outcome of a performance phenomenon, discovering insights into how architecture parameters fundamentally relate to one another, and providing first-order approximation that allows narrowing down the search space for simulation studies. <br\/><br\/>The project enables computer design and evaluation to be improved significantly in terms of quality (deeper insights can be obtained), as well as resource efficiency (less simulation time is required to arrive at the same observations). A deliverable of the project includes a toolset that integrates components of the project into a software package that is easy-to-use and has intelligent interface, and classroom materials suitable for undergraduate and graduate courses. These materials will be made available for public download, community enhancement, and community extension.","title":"SHF: Small: Towards a Versatile Analytical Modeling Toolset for Evaluating Memory Hierarchy Design","awardID":"1116540","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[485790],"PO":["366560"]},"181198":{"abstract":"Communication technologies have taken a central role in daily life, business, government, industry, and social interactions over recent decades. Technology is used not only to allow people to communicate with each other from across the world but to allow machines to interact automatically---processing internet search queries, managing warehouse inventory, forecasting severe weather, updating computer software, and controlling complex systems such as a network of electric power stations. The need for secure communication has increased accordingly. Currently, technology is designed around the concept that a secure or private communication signal must be invisible to the outside world unless the security is compromised. This matches the Hollywood image of a top-secret classified conversation taking place over a \"secure line.\" However, in many emerging applications where machines transmit important information signals to each other, a less expensive type of privacy is adequate---one that simply distorts the view of the signal for any unintended eavesdroppers to the point where they cannot use the information to disrupt the system. This research involves a mathematical analysis of the secrecy of signals. The investigators derive fundamental minimum requirements to protect the communication of signals.<br\/><br\/>This research uses information theory to provide a theoretical foundation for secrecy of signals, using secret keys and physical layer security. However, this work differs from previous theoretical work in a number of ways. One key difference is that metrics for secrecy based on abstract or intangible mathematical quantities are omitted. Instead, secrecy is evaluated based on what an adversary would be able to do with information gleaned from the system. This research is theoretical and broad, yet the novelty of the results bring new insights to applied cryptography.","title":"CIF: Small: Causal Secrecy-A Theoretical Basis for Secrecy of Signals","awardID":"1116013","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["562903"],"PO":["564924"]},"175303":{"abstract":"Malicious software (malware) has become a major threat to computer security and will continue to be a central theme for computer security research for decades. This project takes a binary and virtualization centric approach to effectively and efficiently defeat malware using both online and offline analysis. Offline malware analysis aims to extract knowledge about the inner-workings for a newly discovered malware instance or software exploit, for the purpose of building up proper defense against similar attacks. Online malware defense aims to build efficient security mechanisms to effectively confine malicious behavior and collect enough evidence for subsequent security investigation.<br\/><br\/>For offline malware analysis, a novel virtualization-based malware analysis platform is used, on top of which new type inference techniques are applied to malware decomposition and vulnerability diagnosis. For online malware defense, new techniques for module-level sandbox and execution replay using virtualization are cooperatively used to defeat malware.<br\/><br\/>The results from this research will be disseminated through both peer-reviewed publications and software release. Based on this research, new course materials, modular hands-on projects, and professional training tutorials will be developed, to help future computer engineers and security researchers gain in-depth knowledge about malware defense.","title":"CAREER: Binary and Virtualization Centric Malware Defense","awardID":"1054605","effectiveDate":"2011-08-01","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[470093],"PO":["565327"]},"185258":{"abstract":"This award funds the conference registration and travel-related costs of 10 graduate students from the U.S. to attend The 20th IEEE International Conference on Computer Communications and Networks (ICCCN 2011). ICCCN will be held in Maui, Hawaii in August, 2011 (http:\/\/icccn.org\/icccn11\/).","title":"NSF Travel Grant Support for IEEE International Conference on Computer Communications and Networks 2011 Conference","awardID":"1138988","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["560198"],"PO":["551712"]},"186039":{"abstract":"This grant supports the Doctoral Consortium held in conjunction with the IEEE International Joint Conference on Biometrics to be held on October 11-13, 2011. Attendees of the Doctoral Consortium include graduate students nearing completion of their dissertations who have been accepted to the consortium. Each recent Ph.D. graduate or soon to be graduate chosen to participate in the consortium is paired with a senior researcher who will serve as their mentor during the conference. The goal of this consortium is to provide talented young researchers the opportunity to discuss their work formally and informally with both their peers and senior researchers. These interactions with others coming from different research labs with different viewpoints typically strengthen young researchers ability to communicate the value of their own work and to set their work more clearly in the broader context of the research field as a whole.","title":"IJCB 2011 Conference Dissertation Consortium","awardID":"1143615","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":[499247],"PO":["564316"]},"181540":{"abstract":"Information technology is an established component of the infrastructure of modern societies, and cryptography is a cornerstone of information security. Provable security quantifies the resilience of cryptographic constructions to attacks. This quantification is often relative to the hardness of a handful of reference \"intractable\" problems like integer factorization. Because of the intrinsic strength of heterogeneity, diversifying the set of intractable problems enhances the robustness of the overall structure. This project responds to the challenge by building a foundation for cryptography from problems in combinatorial group theory.<br\/><br\/>The project capitalizes on earlier efforts on the algorithmic unsolvability of standard computational problems in combinatorial group theory like the \"word problem\", but looks at them through the lens of the probabilistic modeling, characteristic of modern cryptography. The research objectives can be grouped into three main categories: 1) identifying efficiently sampleable distributions on which (variants of) the standard computational group-theoretic problems remain _difficult on average_; 2) developing new _hard group-theoretic learning problems_; and 3) exploring the implications of this foundational work to applications with enhanced cryptographic functionality, with the long-term goal of deriving group-theoretic instantiations of public-key cryptography with homomorphic properties.<br\/><br\/>Results of this project will enrich the theory and practice of cryptography with new tools and alternatives, stimulating for researchers throughout the cryptography and group-theory communities, thus fostering collaborations between the two fields. The undertaking of the research entails significant student participation in research at institutions that serve demographic groups that have been historically underrepresented in computer science and mathematics.","title":"TC: Small: Collaborative Research: Provable Security from Group Theory and Applications","awardID":"1117679","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":[486393],"PO":["565239"]},"181430":{"abstract":"It is an irony of our time that despite living in the 'information age' we are often data-limited. After decades of research, scientists still debate the causes and effects of climate change, and recent work has shown that a significant fraction of the most influential medical studies over the past 13 years have been subsequently found to be inaccurate, largely due to insufficient data. One reason for this apparent paradox is that modeling complex, real-world information sources requires rich probabilistic models that cannot be accurately learned even from very large data sets. On a deeper level, research inherently resides at the edge of the possible, and seeks to address questions that available data can only partially answer. It is therefore reasonable to expect that we will always be data-limited.<br\/><br\/>This research involves developing new algorithms and performance bounds for data-limited inference. Prior work of the PIs has shown that, by taking an information-theoretic approach, one can develop new algorithms that are tailored specifically to the data-limited regime and perform better than was previously known, and in some cases are provably optimal. This project advances the goal of developing a general theory for data-limited inference by considering a suite of problems spanning multiple application areas, such as classification; determining whether two data sets were generated by the same distribution or by different distributions; distribution estimation from event timings; entropy estimation; and communication over complex and unknown channels. Whereas these problems have all been studied before in isolation, prior work of the PIs has shown it is fruitful to view them as instances of the same underlying problem: data-limited inference.","title":"CIF: Small: Collaborative Research:Algorithms and Information-Theoretic Limits for Data-Limited Inference","awardID":"1117128","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["518371"],"PO":["564924"]},"181441":{"abstract":"The continuing and accelerated advancement of nanotechnology promises a future in which computing will be truly ubiquitous. The ability to develop minuscule Integrated Chips that implement exceedingly complex systems will impact all aspects of human development ranging from medicine to space exploration. A majority of these applications will be safety critical applications in which the cost of failure will result in loss of human life and exceedingly high economic costs. Scalable verification technology at the nanoscale for developing reliable and bug-free designs is therefore a requirement for the growth and impact of digital systems in the nano-era. One of the key optimizations used extensively in digital design is hardware pipelining, which is similar in operation to the pipelined automotive assembly line. Pipeline implementations at the nanoscale are very complex and prone to errors. <br\/><br\/>This research seeks to develop solutions to enable efficient and scalable verification of pipelined circuits and systems at the nanoscale. The verification solutions are based on refinement, a notion of equivalence used to compare systems defined at very disparate levels of abstraction. The overall approach is to use high-level refinement-based transformations to reduce the nano-pipelined circuit\/system to be verified, in incremental steps, to a functionally equivalent non-pipelined synchronous machine. The non-pipelined machine can then be easily compared with high-level specifications using existing verification techniques.","title":"SHF: Small:Methodologies and Tools For Verification of Nano-Pipelined Circuits and Systems","awardID":"1117164","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[486138],"PO":["565264"]},"181573":{"abstract":"Coarse-grained reconfigurable fabrics have great promise for achieving energy-efficient flexible designs for an application domain. In order to use these fabrics for practical applications, however, there is a great need to develop smart mapping algorithms to implement applications of interest onto these fabrics. This research is focused on discovering new mapping strategies for customized coarse-grained devices through crowd sourcing. <br\/><br\/>The main thrust of the proposed research is to develop a science game to discover better mapping algorithms by making use of human intuition and ability to recognize patterns and opportunities even in complex problems. Players are presented with successively more difficult mapping problems in a game environment, and the vast dataset of players. Moves are analyzed to recognize common patterns used by successful game players. The insights gained from strategic moves humans make while solving problems based on their visual intuition and experience can be used to discover new mapping approaches that are beyond what can be conceived with traditional algorithms. If time permits, architectural innovations will also be investigated by giving players opportunities to make architectural changes in the game environment. New energy-efficient architectures and mapping algorithms that are developed in this research will spur development of a broad range of portable\/wearable computing applications critical to health, safety and security, personal multimedia, and aerospace.","title":"SHF: Small: Discovering New Mapping Strategies and Architectures for Coarse-Grained Reconfigurable Devices Through Crowdsourcing and Data-Driven Techniques","awardID":"1117800","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["518393"],"PO":["565272"]},"180363":{"abstract":"The emergence of massive human-rated and commented visual data has opened avenues for exploring fundamental questions in artificial intelligence beyond the horizon. This project tackles the challenge of automatically inferring visual aesthetics and emotions and inventing new systems that assist creative and decision-making activities of the general public. An interdisciplinary team, with expertise in visual modeling, data mining, psychology, and computational sciences will build tools to distill information from a combination of visual, textual, and numerical data. Visual features, selected based on published literature and consultation with domain experts, will be extracted for discriminating types of emotions. The resulting systems can select and rank visual information based on aesthetics and emotions.<br\/><br\/>Intellectual Merits: This project will allow computer scientists to gain understanding of next-generation computerized visual aesthetics and emotion assessment systems. The complex inter-relationship among content, context, and subjectivity in aesthetics and emotion assessment makes the corresponding learning problems especially challenging, which is likely to trigger innovation in machine learning and statistical modeling. Such capabilities will fundamentally change the way visual information is analyzed, processed, and managed. The project will advance our understanding of the computability of emotions, and lead to new applications that can be used in a variety of settings.<br\/><br\/>Broader Impacts: The research will have a transformative impact in the fields of information retrieval, human-computer interaction, information processing, consumer electronics, and design. The technology can also be used to refine multimedia content that serves as education resources. The project will disseminate research findings, generate new software implementations and collected datasets, and provide online services that can be used by researchers, educators, and industry. Education efforts include developing an interdisciplinary curriculum, training cross-disciplinary scientists, and involving underrepresented groups in research.","title":"SoCS: Studying the Computability of Emotions by Harnessing Massive Online Social Data","awardID":"1110970","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7953","name":"SOCIAL-COMPUTATIONAL SYSTEMS"}}],"PIcoPI":["554666","554666","483351",483351,483352,"503706"],"PO":["565342"]},"180484":{"abstract":"Applications of wireless sensor nodes are evolving at a previously unimaginable rate. But current technology is limited because devices are bulky - measuring one cubic centimeter or more - and hampered by short lifetimes. This project is producing a one cubic millimeter sensor node. This ultra-miniaturized device is a complete sensing platform that includes transducers (for imaging, temperature sensing and other signal detection), wireless communication, a high accuracy timer, processor, memory, a battery and energy harvesting that provides the node with an extended lifetime.<br\/><br\/>The central challenge in reducing the form factor for sensor nodes is to reduce power consumption and <br\/>densely package discrete components (crystals, inductors, etc.). To this end, this team's innovations involve research and development of:<br\/><br\/>1. A novel processor that operates at a supply voltage near the threshold voltage of the transistors for optimal energy consumption.<br\/>2. A new ultra-low-leakage memory system.<br\/>3. An Ultra Wide-Band (UWB) transmitter and receiver that can communicate with other nodes over a distance of three meters with an integrated antenna. <br\/>4. A 100pW timer that is temperature compensated and designed for reduced jitter to allow accurate synchronization between sensor nodes and enable short, low energy radio communication windows.<br\/>5. A new CMOS imaging approach capable of ultra-low power motion detection and image-acquisition, and, reconfigurable to act as a solar energy harvesting unit. <br\/>6. An energy-aware software development environment to control the node<br\/><br\/>These PIs implemented early versions of several of these technologies in silicon, demonstrating the potential to package them as sensor nodes. The team's track record of producing ultra-low power circuits, and other sensing components, position them to deliver the needed 1000\u00d7 form factor reduction. This research team will assemble and package 100 first- and second-generation of these sensor node platforms and disseminate them to the broader community for trials in a wide range of uses. <br\/><br\/>The development of cubic-millimeter sensor nodes will enable applications that have long been envisioned but were unachievable. For example, sensory skins could cover surfaces with a dense deployment of nodes that monitor the properties of the manifold itself or its surroundings. Implantable intelligence can enable deeply embedded physical and biological processes, e.g., malignant tumor growth monitoring or intra-ocular pressure sensing to determine the risk for retinal detachment. Applications such as these, and a myriad of other \"Thinking and Linking\" applications, can give everyday objects sensing, computing, communication, and tracking ability, allowing, for example, research ranging from the social network patterns of small insects to asset tracking in dynamic environments like hospitals. By shrinking sensor node size to one cubic millimeter, with potentially perpetual lifetime, the concept of \"smart dust\" can be taken from fiction to reality.<br\/><br\/>By disseminating the first generation of these sensors to members of the sensor network community, this project will dramatically accelerate the adoption of cubic-millimeter-class computing devices. This will have immediate impact on a wide array of research programs for intelligently sensing, tracking, measuring and optimizing physical processes. This research in turn will have a fundamental and long term impact on a diverse set of applications with critical societal import, ranging from energy conservation, environmental quality management, and health care.","title":"CSR: Large: Collaborative Research: Integrating Circuits, Sensing, and Software to Realize the Cubic-mm Computing Class","awardID":"1111533","effectiveDate":"2011-08-15","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[483695],"PO":["551712"]},"181342":{"abstract":"Title: Modeling and Analysis of Multidimensional Shapes<br\/><br\/><br\/>Abstract<br\/><br\/>The media world can be classified into one-dimensional media, like text and sound, and multidimensional media. Among these latter, digital shapes are characterized by a visual appearance and by a geometric nature. Examples of shapes are pictures, images, 3D models of solid objects, videos, animations, etc. The general goal of the project is defining and developing innovative tools for modeling and analyzing shapes that evolve over time, such as animations of 3D objects, or descriptions of time-varying phenomena arising from scientific simulations. Application areas benefitting from this research include medicine, biology, earth sciences, physics, and chemistry.<br\/><br\/>A large amount of research has been done on modeling and analyzing 2D shapes in image processing and vision, 2.5D and 3D shapes in computer graphics and geographic data processing, and 3.5D shapes (representing the graph of volume data sets) in scientific visualization. Most of this research has been focused on a geometric representation of a shape. More recently structural representations based on topological tools have been developed. This project is developing geometric and structural representations and algorithms for modeling and analyzing 4D shapes, which describe the evolution of a shape over time, and 4.5D shapes, which represent the graph of 4D scalar fields. Examples of the latter are time-varying dynamic three-dimensional scalar fields, examples of the former are animation sequences or isosurfaces of 3D scalar fields varying over time. Specifically, the project is developing effective multi-resolution geometric representations for 4D scalar fields and 4D shapes, new approaches to the segmentation of such shapes based on Morse theory and a generalized notion of curvature, and multi-scale structural modeling of 4D and 4.5D shapes based on segmentation.","title":"CGV: Small: Modeling and Analysis of Multidimensional Shapes","awardID":"1116747","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":[485902],"PO":["565227"]},"181474":{"abstract":"Terrain, in this project, is defined as the elevation of the earth's surface above some reference geoid. Over the last few decades, ever larger quantities of terrain data with higher accuracy in (x, y) and z have become available. Improved bathymetry data of the sea floor has also been collected, and elevation data for other planets and their satellites is now available (for a generalized definition of \"terrain\"). The PI's goal in this project is to develop and validate a new mathematical representation of terrain, which will be closer to the physics of how terrain is formed and be designed to represent legal realistic terrain more easily than unrealistic terrain. Aside from constituting an interesting application of deeper mathematics in its own right, such a foundation for terrain representation that is geologically sound will enable the design of operators such as compression and siting from first principles. This work will generalize and extend the PI's previous successful terrain representation and algorithms work, such as ODETLAP. The new terrain representation will be a sequence of parameterized transformations of various classes inspired by the physics of how terrain is formed. Modeling the real world, the transformations will be nonlinear (e.g., real river valleys cannot be superimposed and added). Nonlinearity is powerful, but difficult to study. The first class of transformations, called scooping, will model how river valleys form, and will guarantee to produce only hydrologically valid terrain. Erosion, deposition and hill creation transformations will also be studied. Each class of transformation has many design options; for example, should fewer and more powerful, rather than many but less powerful, transformations be used? The PI's goal is to encode the terrain in as few bits as possible while satisfying, in addition to RMS error, richer, application-dependent, metrics such as multi-observer siting to maximize viewshed, and then path planning to avoid those observers. Hydrological accuracy and visual recognizability are other metrics. This project continues the PI's collaboration with Professor Marcus Andrade at the Federal University of Vicosa in Brazil. Project outcomes will be validated by means of extensive tests on real terrain databases.<br\/><br\/>Broader Impacts: The simplest implication of this work will be more compact terrain compression algorithms. Thus, this research will allow larger terrain datasets to be accessed and processed by consumers in portable products such as GPS navigators. Easier access to large terrain databases will facilitate a probability distribution over possible realistic terrain, which in turn will allow optimizing operations such as multi-observer siting and path planning (the former has applications ranging from cell phone tower siting to surveillance, while the latter is important for energy conservation during transportation). Hydrological applications of better large terrain data include floodplain planning (flood damage in the US amounted to $50,000,000,000 during the 1990s). Through involvement of graduate students in the PI's research and through his graduate courses, this project will also help to increase the educated workforce in a foundational discipline that is important to American productivity and future economic prosperity.","title":"CGV: Small: Towards a Mathematics of Terrain","awardID":"1117277","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["506756"],"PO":["565227"]},"181595":{"abstract":"Modern smartphones from Apple, Google, and others have remarkably complex security needs. Applications, installed from a variety of third-party vendors, must be separated from one other, since some might be buggy or malicious, yet they must also communicate and share in a variety of ways, including displaying multimedia, sharing authentication credentials, and acting as local proxies for remote Internet sites to support payment services, advertisements, and so forth.<br\/><br\/>We design, implement, and evaluate novel smartphone mechanisms, leveraging Google's open-source Android project. For example, we carefully control how privileges are managed within the phone as applications collaborate. We must defeat \"confused deputy\" attacks, where privileged-but-buggy applications inadvertently allow their callers to exercise sensitive privileges, yet our infrastructure must also enable \"intentional deputies\" who are trusted to leverage dangerous privileges while offering safe interfaces.<br\/><br\/>Our work also considers the user's view of security features. Many applications require users to frequently retype passwords, annoying users and also making them vulnerable to spoofing attacks, because an attacker can fake a pixel-perfect dialog. We are studying a variety of approaches to improve security and usability, including better ways for applications to share credentials with one another (avoiding dialog boxes), and better ways for multiple applications to share screen real-estate (avoiding the need for singleton applications to be granted unnecessary privileges).<br\/><br\/>All of our research output will be available under suitable open-source licenses, helping our work to influence phone vendors and ultimately to have impact on the huge installed base of smartphone users.","title":"TC: Small: Security Architectures for Smartphones","awardID":"1117943","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["548177"],"PO":["565239"]},"181243":{"abstract":"The research in this project focuses on diagnostic test generation methods for non-classical faults, such as transition delay and bridging faults, which represent the fault behaviors of modern VLSI devices. Although, the existing tools for VLSI circuit testing incorporate years of research they only deal with classical stuck-at faults. A stuck-at fault is detectable by a single input pattern. Detection of a transition fault is more complex because it requires a sequence of two patterns. Also, the existing tools find tests that detect faults and may not diagnose them, i.e., identify the exact cause of a failure. The traditional metric used in testing is fault coverage. This research investigates the use of a new metric termed diagnostic coverage for the effectiveness of tests in their role of fault diagnosis. For example, to distinguish between two faults one must use a test that detects one fault but not the other; such a test is called an exclusive test. This research provides new algorithms to generate tests for diagnosis of non-classical faults while allowing the use of the available testing tools.<br\/><br\/>Moore's law prediction of the number of devices on a VLSI chip doubling every eighteen months simply follows the trend of minimum cost per transistor. The enabling technology driver is the shrinking geometry of features allowing higher transistor density and speed. Nanometer geometries have, however, led to greater process variations. Two characteristics separate the testing of modern VLSI technologies. First, the complex fault mechanisms are no longer represented by the classical stuck-at faults. Second, the impact of increased process variation on yield requires testing to be diagnostic-oriented; faults must be identified so that their causes can be eliminated. The research in this project addresses both needs of the advancing VLSI technology.","title":"SHF: Small: Methods for Diagnosis of Non-Classical Faults in Digital Circuits","awardID":"1116213","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[485659],"PO":["562984"]},"181364":{"abstract":"This project seeks to improve the energy efficiency of cost-sensitive embedded computer systems by splitting them into multiple separate voltage domains and running each as efficiently as possible by adjusting voltage and frequency dynamically. Improving energy efficiency allows a designer to extend a system?s operational life, reduce battery size and weight, and\/or improve functionality. Switch-mode power supplies are commonly used to convert power efficiently from one voltage level to another. However, existing switching power supplies are often quite expensive compared with the microcontrollers and peripherals which they power. The major challenge in exploiting multiple voltage domains to save energy is to make power supplies that are efficient yet inexpensive enough to be practical for low-cost devices. The innovation here is to move control of the power supply into software and a low-cost generic microcontroller, which can be shared by the embedded application. Real-time systems scheduling techniques ensure that the entire system works reliably and that the power supply does not emit electromagnetic noise during periods that the system needs to perform noise-sensitive operations. Advanced Gallium Nitride (GaN) transistors are used to achieve high switching frequencies, enabling the use of smaller and less expensive components. New, highly efficient methods are developed to store and use energy from energy-scavenging devices (photovoltaic, thermoelectric, ambient radio).<br\/><br\/>Improvements in power supply technology mean lower energy consumption and longer battery life for electronic devices, and lower production costs. Beyond developing such technology, the project advances education in it.","title":"CSR:Small: Enabling Aggressive Voltage Scaling for Real-Time and Embedded Systems with Inexpensive yet Efficient Power Conversion","awardID":"1116850","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[485952,485953],"PO":["564778"]},"181485":{"abstract":"For a robot to operate in a complex environment over a period of hours or days, it must be able to plan actions involving large numbers of objects and long time horizons. Furthermore it must be able to plan and carry out actions in the presence of uncertainty, both in the outcome of its actions and in the actual state of the world. Thus, key challenges are hedging against bad outcomes, dealing with exogenous dynamics, performing efficient re-planning, and determining conditions for correctness and completeness. <br\/><br\/>This project will develop an approach to robot planning that addresses these challenges by integrating several key ideas: (1) Planning in belief space, that is, the space of probability distributions over the underlying state space, to enable a principled approach to planning in the presence of state uncertainty; (2) Planning with simplified models and re-planning as necessary to enable planning efficiently with outcome uncertainty while still enabling action choices based on looking ahead into likely outcomes; (3) Combining logical and geometric reasoning to enable detailed planning in large state spaces involving many objects; and (4) Hierarchical planning with interleaved execution to enable plans with very long time horizons by breaking up the planning problem into a sequence of smaller problems.<br\/><br\/>The methods developed will be tested in a system that combines planning, perception and execution for real physical robots navigating and manipulating objects in real, complex environments. The software developed in this project will be freely available as a collection of ROS (Robot Operating System) modules for easy porting to a wide variety of robots. The research in this project will contribute materials for two courses that the PIs are developing: (1) a lab-based introduction to electrical engineering and computer science based on mobile robots (currently taken by around 500 MIT students per year) and (2) a new project-based senior-level subject on robot planning and perception. All of the materials for these subjects will be available freely through MIT's OpenCourseWare site.","title":"RI: Small: Hierarchical Planning for Robots in Complex Uncertain Domains","awardID":"1117325","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[486246,486247],"PO":["565035"]},"181012":{"abstract":"WDM (wave-length multiplexing) Optical networks form the critical backbone of all modern communication networks and systems. Therefore, there has been a great deal of interest in the fault tolerant design and operation of these networks. In the past decade or so several fundamental advances relating to the fault tolerance, protection and restoration issues have appeared in the literature. Most of these advances have been in the context of failures in one layer. But, modern communication systems consist of multiple physical implementations communicating via layered protocols. As such, a single failure at one layer may lead to cascading failures, i.e., failures at the physical layer lead to failures at the logical layer. Research in this area of cross-layer survivability is still in its infancy. In this basic research project the principal investigators will carry out a study of network survivability across layers to deal with cascading failures in layered networks. The research will be in the context of IP-over-WDM Optical networks. The focus will be on multiple failures in the physical (optical) layer and their consequences at the higher layer, namely the IP layer. Specifically, the broad scope of the project will cover i) survivable logical topology mapping under multiple failures, ii) Logical topology mapping for guaranteed survivability, iii) Logical topology mapping under multiple constraints, and iv) A generalized theory of flows across layers, capacity of survivable logical topologies and related algorithmic challenges.<br\/><br\/>The project seeks to develop unifying theories and methodologies that will make significant advances to our understanding of cross-layer survivability issues, and providing the theoretical foundation for future advances in the general area of cross-layer design and optimization. These theories will be based on modern advances in graph theory, mathematical programming (e.g., network interdiction) and algorithm design. The principal investigators will develop innovative algorithmic techniques based on advanced data structures and computer algorithms such as approximation techniques as well as a generalized theory of cross layer flows that will go well beyond the widely used classical theory of single layer flows. <br\/><br\/>Broader Impact: Although IP-Over-WDM networks will provide the context for the research, the theory of cross-layer flows and the algorithmic (in particular approximation) techniques that will be developed will have multidisciplinary value spanning computer science, electrical engineering, graph theory and mathematical programming. The research will also have significant educational value in training highly skilled researchers for research and development in cutting edge technologies in different areas of information technology.","title":"NeTS: Small: Collaborative Research: Cross Layer Survivability to Cascading Failures in Layered Networks","awardID":"1115184","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[485106],"PO":["564993"]},"181133":{"abstract":"The Gram--Schmidt orthogonal factorization algorithm and the Golub--Kahan--Lanczos (GKL) bidiagonal reduction algorithm produce two fundamental factorizations for numerical linear algebra. Enhancements to these algorithm provide the foundation for the ``New and Improved Algorithms for Minimization and Subspace Tracking'' that are the subject of this proposal. These are both orthogonal factorization algorithms designed to produce bases for subspaces of interest and, for both algorithms, stability analyses are necessary to consider what is necessary to keep these bases orthogonal. The algorithms are also adapted to be based upon matrix--matrix operations, thereby making them implementable using the level-3 BLAS and sparse BLAS routine necessary to make them efficient on modern architectures.<br\/><br\/>Based upon the new Gram--Schmidt and GKL procedures, regularized least squares and algorithms for tracking the leading principal subspace of a matrix are developed. Using discrete cosine and fast Fourier transform<br\/>based preconditioners developed by the PI and collaborators in a previous NSF project, a regularized least squares algorithm is extended into a Newton--based regularized total least squares algorithm that is well suited to image deblurring problems.<br\/><br\/>The research on the block Gram--Schmidt and GKL bidiagonalization algorithms advance the scientific community's knowledge of how to develop efficient software in modern computing environments for two fundamental algorithms in numerical linear algebra, a core field that straddles computational science and applied mathematics. The Gram--Schmidt algorithms are important in the development of iterative methods for the solution of large systems of linear equations arising in a long list of scientific and engineering disciplines. The GKL bidiagonal reduction algorithm and subspace tracking work is useful in the numerous applications of dimension reduction within statistics, most notably web search algorithms. Bidiagonal reduction is also a key component in the solution of the Netflix problem--the problem of identifying which films a customer would enjoy watching among a very large sample based upon a smaller sample of films which he\/she has already rated. The image deblurring algorithms are important in the reconstruction of high-resolution images. These images, similar to those produced by high-resolution television, are expensive to transmit. The total least squares research develops an algorithm that would be useful in recovering a high-resolution image from cheaper-to-transmit lower resolution images.","title":"AF: Small: New and Improved Algorithms for Minimization and Subspace Tracking","awardID":"1115704","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[485393],"PO":["565251"]},"181265":{"abstract":"From an algorithmic point of view, systems based on wireless communication pose unique challenges that are not present in standard networks. Wireless devices may move around and communication between these devices can be disrupted for several reasons including obstacles, background noise, and interference problems due to transmissions from the own wireless network, from a malicious jammer trying to disrupt communication, as well as coexisting networks using the same frequency band. Finding suitable models that on one hand allow the rigorous design and analysis of protocols and on the other hand are useful in practice is a major challenge and deserves significant research efforts.<br\/><br\/>We will investigate models for wireless communication that cover a wide range of physical layer phenomena and that are yet simple enough so that they are useful in theory and practice. In contrast to prior algorithmic approaches, our approach will be to model communication problems due to physical layer issues (such as ackground noise, obstacles, jammers, etc.) with the help of an adversary, and to develop medium access (MAC) protocols that are provably robust against these adversaries. Such an approach has many interesting applications. First, it allows for more general scenarios for the background noise than previous approaches as it covers bursty situations that might be due to some temporary obstacle or operation of a machine that creates interference. Second, the adversarial model would also allow us to determine how robust a protocol is against wireless jamming attacks, which are a real threat to standard protocols such as the 802.11 family or networks of simple sensing wireless devices (where traditional physical layer techniques cannot be successfully applied). Finally, the adversarial model may allow us to abstract from interference problems due to transmissions of far away devices in the wireless network. In addition, we will also focus on important applications such as leader election and broadcasting. <br\/><br\/>Since wireless networks are a component of many widespread and\/or critical systems, the proposed research will have an impact in several respects, including immediate applications to emergency services, the military, and local area networks in hazardous areas. Moreover, the proposed research will also have an impact in solidifying the international collaboration with the U. of Paderborn, Germany, and in advancing education and enhancing diversity.","title":"AF: Small: Adversarial Models for Wireless Communication","awardID":"1116368","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["563358"],"PO":["565251"]},"185500":{"abstract":"This is funding to support travel for a diverse group of US PhD students and distinguished faculty mentors to participate in an international doctoral consortium on research on information science that will be co-located with the 2012 iConference in Toronto, Canada. The iConference is a leading forum that brings together faculty, students, research staff, and industry researchers who share an interest in supporting and augmenting human engagement with information and technology. The main goal of this Doctoral Colloquium is to help train the next generation of information science researchers.<br\/><br\/>The 2012 iConference Doctoral Consortium will provide a group of approximately 20 PhD students studying all aspects of information science (IS) with an environment in which they can share and discuss their goals, methods and results at an early stage of their research. It will take place on February 10, 2012, the final day of the iConference. By participating in the doctoral consortium, students will gain feedback on their work from other students and six prominent faculty members, allowing them to enhance their own research proposal. Students will also develop a better understanding of the different research communities engaged in the study of information science, and learn how to position their own work within the IS community. In addition, the consortium will provide students with opportunities to make new professional connections beyond their own disciplines. <br\/><br\/>Students will be recruited for the doctoral consortium through advertisement on the conference website, postings to relevant mailing lists and direct solicitation to faculty working in the area of information science and related fields. Particular attention will be placed on identifying participants from under-represented groups. To apply for the consortium, students will submit a 1000 word paper outlining their research goals and work to date, a list of key questions they would like to discuss with other doctoral consortium attendees, and the names of people they consider prominent in their areas of research. Applications will be screened by the consortium chairs for fit to the IS topic area, the state of development of the student's research, and the quality of the research project. Priority will be given to students who have proposed their dissertation topic but not yet attended any doctoral consortia.<br\/><br\/>Broader impacts: The iConference doctoral consortia traditionally bring together the best of the next generation of researchers in information science and related areas, allowing them to create a social network both among themselves and with senior researchers at a critical stage in their professional development. Participation is encouraged from a broad range of relevant disciplines and approaches, thereby broadening attendees' perspectives on their topics of study and promoting advancement of the field. The organizers will try explicitly to identify and include the broadest possible group of highly qualified participants. As a consequence of these steps, the student and faculty participants will constitute a diverse group across a variety of dimensions, which will help broaden the students' horizons to the future benefit of the field.","title":"Doctoral Colloquium at iConference 2012","awardID":"1140343","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[497802,"548113"],"PO":["565227"]},"181397":{"abstract":"Over time, software systems architecture eventually deviates from the original designers intent and degrades through unplanned introduction of changes that invalidate original design decisions. Architectural degradation increases the cost of making new modifications and decreases systems reliability, until engineers are no longer able to effectively evolve the system. At that point, the systems actual architecture may have to be recovered from the implementation artifacts, but this is a time-consuming and error-prone process, and leaves critical issues unresolved: the problems caused by architectural degradation will likely be obfuscated by the systems many elements and their interrelationships, thus risking further degradation.<br\/><br\/>This collaborative project aims at pinpointing locations in software systems architecture that reflect architectural degradation. The proposed research comprises four integrated research tasks: (1) Develop a catalog of commonly occurring symptoms of degradation. (2) Develop an architecture recovery technique that automatically extracts both systems major building blocks and the concerns that influence, drive, and interact with these building blocks. (3) Devise a technique for formally capturing the recovered architectural design decisions, their involving concerns, and the identified causes of degradation. (4) Devise a suite of techniques that leverage the catalog to automatically identify system-specific instances of degradation. As a result, this project will have a potential for broad impact by providing a rigorous, scientific basis for software engineers to streamline the currently prohibitively expensive and error-prone system maintenance and evolution tasks.","title":"SHF: Small: Collaborative Research: Automating the Detection of Architectural Degradation in Software Systems","awardID":"1116980","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["498101"],"PO":["564388"]},"181166":{"abstract":"This project develops new and promising techniques in the area of side-channel attacks and their corresponding countermeasures. In a side-channel attack, an attacker captures the implementation effects of cryptography, such as power consumption and execution time. A distinctive feature of a side-channel analysis (SCA) attack is that it can reveal a small part of the secret-key. Hence, side-channel attacks avoid the brute-force complexity of cryptanalysis. Using novel side-channel estimation techniques based on Bayesian statistics, the project develops more powerful side-channel attacks. The development of novel side-channel analysis techniques is crucial in order to obtain the best possible countermeasures. The project also develops novel software-oriented countermeasures that more flexible and general than traditional hardware-oriented side-channel countermeasures. The efficiency of side-channel attacks and side-channel countermeasures are evaluated using hardware and software prototyping. The project combines advanced statistical techniques with advanced computer engineering, building synergy between Statistics and Computer Engineering. In the field of Statistics, the Bayesian matching technique can be used for variable selection, a technique that is applicable to related problems in biostatistics, machine learning, data mining, genomics, and other areas with high dimensional data. Project results will be disseminated by distributing open-source prototype implementations, measurement data, and in open publications. A formal training program within the Laboratory for Interdisciplinary Statistical Analysis (LISA) at Virginia Tech is developed to distribute the results of this project to students.","title":"TC: Small: New Directions in Side Channel Attacks and Countermeasures","awardID":"1115839","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["548205",485466],"PO":["564223"]},"181287":{"abstract":"This is a collaborative project leveraging expertise of Ashok Veeraraghavan, William Marsh Rice University (IIS-1116718) and Ramesh Raskar, Massachusetts Institute of Technology (IIS-1116452). Imaging and display devices are all around us and are used in a variety of applications. The spatial resolution, depth range, depth resolution, temporal resolution, frame-rate and bandwidth of these devices are usually fixed a priori. When the resolution and other properties of the content being imaged or displayed does not exactly mimic those that were assumed a priori, this leads to inefficiencies (in utilizing available resources) and undesirable artifacts (aliasing, blurring and noise). Since both imaging and display devices are fast becoming multi-purpose, there is a need to develop imaging and display architectures (and algorithms) that are capable of adapting their resolution and bandwidth characteristics to match those of the content.<br\/><br\/>The goal of this project is to develop imaging and display devices that adapt to scene, motion, geometry, viewer, or illumination conditions. Such adaptive devices lead to performance improvements and novel capabilities hitherto unexplored. This research agenda is organized into four intellectual thrusts: (1) the establishment a theoretical framework for Adaptive Coded Imaging and Displays (AdaCID) that enables efficient exploration of the space of designs (2) the design of adaptive coded imaging systems that adapt to scene geometry, motion, and illumination (3) the design of adaptive and interactive coded 2D\/3D displays that adapt in real-time to content, viewer position, and the human visual system enhancing visual appearance and allowing intuitive 3D interaction and (4) the demonstration of coded feedback projector-camera systems enabling rapid acquisition of range and material characteristics.<br\/><br\/>It is expected that AdaCID will have far-reaching impact to diverse applications spanning consumer imaging and displays, machine vision and automation, scientific\/medical imaging and displays and surveillance. Since AdaCID and the broader field of computational imaging and displays is increasingly important, they will be integrated into various courses offered at Rice University and MIT. Broad dissemination of the educational material will be achieved through participation in the free, open-licensed Connexions program and OpenCourseWare and in public-domain museum initiatives (at the MIT Museum). This project also offers collaborative research opportunities for students at the two institutions. Project Website (http:\/\/cameraculture.media.mit.edu\/AdaCID\/) provides additional information.","title":"CGV: Small: Collaborative Research: AdaCID: Adaptive Coded Imaging and Displays","awardID":"1116452","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["532050"],"PO":["563751"]},"181056":{"abstract":"This project focuses on the principled design, enhancement, analysis and application of model-directed hybrids, which combine model-directed optimization and traditional optimization techniques. The key idea is to use models of the problem landscape constructed by model-directed optimization techniques to facilitate decisions about the nature and likely effectiveness of particular local search procedures and appropriate neighborhood structures for those procedures. The importance of the developed techniques will be demonstrated in a broad spectrum of applications, including problems in computational physics, biology, chemistry and operations research.<br\/><br\/>The project will have transformative effects on computational optimization mainly because it will automate the design of advanced neighborhood structures and problem-specific operators applicable to broad classes of optimization problems, including problems with noise, dynamic landscape, complex structure and multiple conflicting objectives. Besides advancing computational optimization, the project will have a strong impact on a variety of other disciplines in science, engineering and commerce because it will provide practitioners in these disciplines with tools that allow practical solutions of problems intractable with current techniques. Furthermore, the project will provide methodology for development of advanced techniques for landscape analysis as well as theoretical study of advanced hybrids of metaheuristics and traditional optimization techniques.<br\/><br\/>To complement the research goals, the project puts a strong emphasis on broader impacts with the focus on advising student researchers, facilitating graduate mentoring of undergraduates, participating in multidisciplinary and collaborative projects, ensuring effective dissemination of research outcomes, organizing student competitions, boosting the local research infrastructure, and supporting groups underrepresented in science and engineering.","title":"RI: Small: Model-Directed Hybridization: Principled Design of Hybrids of Model Building, Metaheuristics and More Traditional Optimization Techniques","awardID":"1115352","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[485221,485222],"PO":["562760"]},"181298":{"abstract":"This proposal aims to create purely image-based reasoning methods for solving visual analogy problems, particularly so-called Raven's Progressive Matrices (RPM) problems. The project draws on recent results from the study of human cognition as well computer science and mathematics. Raven's Progressive Matrices consist wholly of visual analogy problems in which a matrix of geometric figures is presented with one entry missing, and the correct missing entry must be selected from a set of answer choices. Recent analysis of RPM data suggests that although in general the performance of individuals with autism on most intelligence tests is significantly inferior to that of typically developing individuals, on the Raven's test the performance of the two groups is comparable. This data is consistent with the \"Thinking in Pictures\" hypothesis that has been proposed as a potential, partial cognitive explanation of autism. In both artificial intelligence and psychology, current theories of solving RPM problems first convert the visual inputs into verbal representations and then process the verbal representations. In contrast, this project explores the hypothesis that many RPM problems can be solved using only visual representations, without extracting any verbal representations from the input images. This project will develop and analyze computational techniques for addressing RPM problems with only visual representations. <br\/><br\/>In particular, this project will develop a novel algorithm based on affine transformations for addressing RPM problems as well as a second algorithm that makes use of fractal encodings. With both approaches -- affine and fractal -- the project seeks to achieve human-level performance on RPM in terms of percentages of problems solved correctly. The two algorithms will also be tested on the \"odd-man-out\" corpus that contains thousands of visual analogy problems. The project will formally characterize the set of visual analogy problems for which the affine and fractal algorithms are applicable, analyze the computational properties of the algorithms, construct proofs of their correctness for specific classes of problems, and compare the errors made by the two algorithms with those made by two groups of humans -- typically developing individuals and individuals with autism. The project will parameterize the visual algorithms to detect the settings under which the patterns of errors made by an algorithm on RPM problems most closely match the error patterns of the two human groupings. <br\/><br\/>Autism is an important problem of growing social concern. While the thinking-in-pictures hypothesis has long been a significant insight into cognition in autism, and empirical evidence -- both behavioral and neuroimaging -- in its favor is increasing, there have been no computational models for it. The proposed research would help provide a computational form to this hypothesis and may help establish a disposition towards visual thinking with autism. RPM is considered one of the core tests of intelligence, and although there have been several suggestions about the visuospatial nature of RPM problems, all current computational models addressing such visual analogy problems use sequential processing on propositional representations of the input images. The algorithms from this project that rely on visual representations for RPM could provide new insights into intelligence testing. Lastly, while fractal encodings have been used in computer graphics for generating images and in computer vision for texture analysis in image processing, this project's use of fractal encodings for visual analogies on intelligence tests will contribute to knowledge of fractal computing.","title":"RI: Small: Addressing Visual Analogy Problems on the Raven's Intelligence Test","awardID":"1116541","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["539354"],"PO":["565035"]},"185105":{"abstract":"This award supports the development and publication of a policy report and a special edition of a peer-reviewed journal to disseminate the findings of an OECD-NSF workshop on Building a Smarter Health and Wellness Future. The report will analyze how technological developments such as high-speed and mobile applications, sensor-based devices, and electronic health records can support new models of care to address 21st century health and wellness needs, reflecting a vision of a patient-centered, rapidly learning healthcare system. It will consider the challenges encountered in redesigning processes of car, the socio-technical and usability factors, access and privacy issues, the sustainability of the new models and their impacts. The special edition will include renowned experts from the United States and Europe. The special edition journal, which will be published online, has an international readership and is recognized for its high impact and extensive citation. The publications provide a way to engage a broad international audience and go beyond describing current trends to focus on governance issues and policies, looking to promote the most efficient and effective roles for both the public and private sectors in sustaining health care innovation.","title":"OECD-NSF Policy Report: \"Building a Smarter Health and Wellness Future\"","awardID":"1138151","effectiveDate":"2011-08-15","expirationDate":"2012-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["499471",496645,496646],"PO":["565136"]},"186117":{"abstract":"This project will explore the future of molecular programming area via a Special Session at the 17th <br\/>International Conference on DNA Computing & Molecular Programming (DNA17). The conference will take<br\/>at California Institute of Technology, Pasadena, California, USA on September 19, 2011. The conference website is http:\/\/dna17.caltech.edu. The goals of the event are to elucidate future directions and specific challenges for the field of DNA computation and molecular programming, and to facilitate discussion on how these visions are to be met. The DNA17 conference is the flagship conference in this emerging area. <br\/><br\/>A report will be provided to NSF after the event, consisting of a short statement from each invited panelist and a summary of each panel discussion by the organizers. The panel discussions will provide a timely assessment of the field and the challenges and opportunities for the future, which will be useful to conference attendees and other researchers working in this area as they formulate their next research directions. <br\/><br\/>Requested funds will primarily go to support registration and travel expenses for the invited panelists, <br\/>as well as incidental costs associated with the impromptu parallel sessions.","title":"Future directions for molecular programming: DNA17 special session","awardID":"1143993","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["549585","558957","518688","518689"],"PO":["565223"]},"186249":{"abstract":"The goal of this research program is to develop a new paradigm for databases, called aspect-oriented data (AOD). AOD enables cross-cutting data concerns to be added to a database using aspect-oriented programming. A cross-cutting data concern is a data need that is universal (potentially applicable to the entire database) and widespread (can be used to enhance many, different databases). Data has a wide variety of cross-cutting data concerns, including provenance, time, lineage, and security. So this research has the potential to impact every database. The project achieves its goal by conducting the following tasks: 1) Develop a model for aspect-oriented data in which advice (metadata) can \"tag\" or annotate data, 2) Build a data aspect weaver to weave the advice into queries, constraints, and data modification, 3) Develop advice-specific modules for common cross-cutting concerns that plug into the data aspect weaver and add advice-specific behaviors, 4) Demonstrate that aspects themselves can be aspected to model meta-metadata, and 5) Develop test cases for AOD. The data aspect weaver is being developed for Pig Latin, which is a cloud-computing platform for data analysis. The research impacts database management systems, scientific databases, and digital government. The project supports a Ph.D. student to pursue research in databases. Publications, technical reports, and software from this research are disseminated on the project's web site (http:\/\/www.cs.usu.edu\/~cdyreson\/aspectOrientedData).","title":"III: EAGER: Aspect-Oriented Data Weaving","awardID":"1144404","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[499773],"PO":["563727"]},"177317":{"abstract":"In his visionary 1991 article \"The Computer for the 21st Century\", Mark Weiser described a world in which people interact with smart information appliances, and where one would routinely have over a hundred such devices per room. He also described some of the key technologies needed to realize this vision, including devices where \"a small battery will provide several days of continuous use.\" Since then, many of Weiser's predictions about the advent of ubiquitous computing have come to pass or have been exceeded. The capabilities of modern computing systems have far exceeded the requirements for Weiser's vision in terms of capacity and performance.<br\/>However, truly ubiquitous computing still eludes us because the combination of increasing energy requirements and (relatively) slow progress in battery technology limits the lifetime of battery-powered devices. The goal of this project is to create hardware and software solutions to enable long-lifetime mobile information appliances.<br\/><br\/>The emergence of non-volatile memory technologies (e.g. magnetic random access memory) and bi-stable displays (e.g. e-Ink) significantly alter the energy signature of mobile devices by enabling near-zero quiescent power for the main memory and display. This project alters the energy signature for the microprocessor by tailoring its performance and energy to the requirements of typical user interaction patterns in the information appliance context. These patterns include viewing\/reading the display (no computation), interacting with the display (minimal computation), and requesting a task that requires significant computation such as rendering a new screen. The processor architecture operates in high-performance mode with an energy signature resembling a modern embedded processor, low performance mode with an energy signature resembling a microcontroller, and ultra low power idle mode with extremely fast switching times between the different modes so as to provide the illusion of a single high-performance processor. This project creates the software interfaces necessary to exploit the low power properties of the new microprocessor thus simplifying application development for low power information appliances. The final goal of the project is to demonstrate the new concepts in a prototype e-book reader that is also a hardware testbed for the evaluation of the energy requirements of mobile information appliances.<br\/><br\/>This project will create enabling technologies for the development of mobile computing devices whose battery life can be measured in months, rather than days. The results of this project will be disseminated via research publications, by making a prototype mobile platform available to the research community, and by training undergraduate and graduate students in the design and implementation of energy-efficient mobile appliances.<br\/><br\/>Given the billions of mobile devices currently in use worldwide, a significant reduction in the energy requirements for mobile platforms could lead to a reduction in the energy consumed by the world's information technology infrastructure. Mobile devices that do not need to be re-charged on a daily basis would help us truly realize Mark Weiser's vision of ubiquitous computing.","title":"HCC: Medium: Hardware and Software Architectures for Next-Generation Mobile Platforms","awardID":"1065307","effectiveDate":"2011-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["550887",475375,475376],"PO":["366560"]},"181552":{"abstract":"The recovery of visual colors from captured images and video signals is among the most prevalent and fundamental problems in digital imaging. This problem affects billions of consumers, and it impacts the quality of the visual signal that these consumers capture, communicate, and display. Millions of image and video cameras, mobile and smart phones, and many other types of visual devices and applications are impacted in a significant way. For example, virtually all consumer cameras are based on an architecture that utilizes a Color Filter Array (CFA), which captures single-color-per-pixel images to reduce cost, size, and power consumption. Hence, one needs to recover the original three color images (Red-Green-Blue) from the captured single-color-per-pixel CFA image. Despite numerous contributions and noticeable progress that has been made in this area, this problem is still largely unsolved.<br\/><br\/>This project addresses the general problem of the recovery of multiple color channels (RMCC) from limited color information. The project develops a joint rank-minimization sparsity-maximization (RMSM) framework for the recovery of multiple color channels. Rank minimization of matrices, which is a more general framework than compressed sensing (CS) of vectors, provides many powerful tools. The project targets both approaches jointly in novel ways. An important question is how to strike an optimal balance between rank-minimization and sparsity-maximization under a joint framework. Furthermore, this effort designs optimization frameworks for a sparsifying \"color\"-dictionary paradigm. The notion of utilizing overcomplete, sparsifying \"color\" dictionaries represents a major departure from prior work. The project is also extending the applications of this research to video demosaicing and visual coding systems.","title":"CIF: Small: Recovery of Multi-Channel Visual Signals from Limited Color Information Using Rank Minimization and Sparsity Maximization","awardID":"1117709","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["554511"],"PO":["564898"]},"185930":{"abstract":"The Network to support Science, Engineering, Arts and Design (NSEAD) will support transformative research and pedagogy that are only possible through the combined expertise of diverse knowledge domains and disciplines. For example, as physicists and engineers developed new imaging techniques, visual artists experimented with the new expressive potentials they enabled, often influencing development of the technologies. Visual artists and musicians have created and continue to create computer languages and algorithms while pushing technologies for composing and recording in fields of software engineering, artificial intelligence, graphics and visualization. Students who are involved in the arts have higher math, verbal, and composite SAT scores than students who are not involved in the arts. (Vaughn and Winner, 2000). There is a growing movement by higher education academic institutions in the United States to integrate the Arts and STEM disciplines to educate the whole student while leveraging creative cognitive skills for solving complex problems in science and technology disciplines. And finally, diverse ecosystem of academic programs in pre-K to gray formal and informal STEM learning; scientific research conferences; exhibitions, and cultural institution programs continues to emerge as new information technologies, creativity support and social networking tools become pervasive in our society. This project envisions a network that addresses fundamental challenges including the need to align academic pedagogies with 21st century thinking skills; to promote diversity of perspectives, approaches, and people in the creative information technology economy; and to benchmark best practices that create critical thinkers and leaders for the ever-changing technology-driven job market. The development of such a network will provide a platform to disseminate and generate public dialogue about the intellectual, cultural, and economic potential of intersections of science, technology and creativity. <br\/><br\/>NSEAD will be a platform to support the burgeoning research community of Computer Scientists, Engineers, Artists and Designers engaged in integrative research and pedagogy across these disciplines. NSEAD will provide a bridge for academic institutions, non-profit organizations, industry liaisons, and resource providers to collaborate, share best practices in research and pedagogy, and build stronger affinities. It will serve as a junction for elements such as: 1) research community development; 2) collaboration and project matchmaking opportunities; 3) skills expertise referrals; 4) inter-institutional collaborations; 5) forums to share best practices in pre-K to gray STEM learning and creative enrichment; and 6) strategies for network leadership and resource sustainability.","title":"Collaborative Research: EAGER: Network for Science, Engineering, Arts and Design (NSEAD)","awardID":"1142663","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[499002],"PO":["565342"]},"181332":{"abstract":"A central objective for computer science is to develop methods for building<br\/>reliable and maintainable software. The most important technique for ensuring<br\/>these properties is abstraction, the decomposition of a system into separable<br\/>and reusable components. The theory of abstraction in programming is called<br\/>type theory. A type is a specification of the behavior of a software component;<br\/>type checking ensures that programs obey these specifications. This ensures<br\/>that components can be modified or replaced without fear of disrupting the<br\/>behavior of other components. By supporting the expression and enforcement of<br\/>component behaviors, type theory integrates programming with verification, the<br\/>process of ensuring compliance with specifications. All modern programming<br\/>languages and development methodologies are based on, or draw inspiration from,<br\/>type theory. The broad goal of this project is to extend the capabilities of<br\/>type theory to a wider range of properties, and to use type theory to facilitate<br\/>the development of reliable software.<br\/><br\/>Specifically, the research will develop the theory of higher-dimensional type<br\/>theory, and explore its application to generic programming, a technique<br\/>for generating programs from their specifications. Higher-dimensional<br\/>type theory draws on recent advances in category theory and algebraic<br\/>topology that emphasize the algebraic structure of relations between<br\/>programs, and relations between such relations, in direct analogy with<br\/>the higher-dimensional structure of topological spaces. In this setting<br\/>dependent families of types must respect the algebraic structure of such<br\/>relations, and in doing so, they implicitly provide transformations that<br\/>correspond to generic programs whose behavior is determined by their<br\/>type. More broadly, the project will apply ideas from category theory<br\/>and topology to improve software development, and apply ideas from type<br\/>theory to facilitate computer-verified proofs of mathematical theorems.","title":"SHF: Small: Foundations and Applications of Higher-Dimensional Directed Type Theory","awardID":"1116703","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[485877],"PO":["564588"]},"181695":{"abstract":"All of computing today relies on an abstraction where software expects the hardware to behave flawlessly for all inputs under all conditions. However, for emerging circuits\/devices, the cost of maintaining the abstraction of flawless hardware will be prohibitive due to variations and we may need to rethink the correctness contract between hardware and software. <br\/><br\/> The primary focus of the project is application robustification ? fundamental algorithmic methodologies to transform arbitrary applications such that they can continue to make forward progress in spite of errors produced by the hardware. In this project, our preliminary research effort is focused on a) techniques to convert different classes of application kernels into robust, efficiently solvable stochastic optimization problems that can tolerate hardware errors, b) techniques based on Krylov subspace methods, gradient projection, quasi-Newton approaches, stochastic approximation theory-based approaches, preconditioning techniques, and intelligent step sizing to reduce the cost of robustness for different forms of hardware variations, and c) low overhead checksum-based techniques robustifying sparse linear algebra libraries and graph algorithms. Broader impact of this project includes development of a potentially promising approach to ride Moore's Law and training students in both the hardware and software aspects of computing in face of errors. Broader education will also be achieved through research artifacts (e.g., library of error tolerant kernels) that will be made available for research and education.","title":"Application Robustification","awardID":"1118391","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["535191"],"PO":["565255"]},"181343":{"abstract":"A smart grid is made possible by applying sensing, measurement and control devices with two-way communications to electricity production, transmission, distribution, and consumption parts of the power grid that communicate information about grid condition to system users, operators and automated devices, making it possible to dynamically respond to changes in grid condition. A smart grid is eventually a wireless hybrid network, and its capacity and achievability are studied. An information theoretic approach for multiple access channels is applied to increase the throughput capacity, and three tasks are studied: 1) smart grid versus smart home, 2) square network versus smart grid network, and 3) base stations with antenna array. The achievability of the capacity is studied via analyzing the rate of smart grid which consists of multimodal sensors\/meters. Different sensor modalities may be independent or dependent, and some preliminary fundamental results are derived and are subsequently applied to smart grid wireless network.<br\/><br\/>One study conducted by the US Department of Energy concludes that internal modernization of US grids with smart grid capabilities would save between 46 and 117 billion dollars over the next 20 years. With the segments set to benefit the most will be smart metering hardware sellers and makers of software used to transmit and organize the massive amount of data collected by meters. The capacity study performed in this project provides the upper bound for the smart grid to transmit the massive amount of data, and the achievability study provides an efficient way to organize the massive amount of data collected by meters.","title":"NeTS: Small: Smart Grid Wireless Networks: Capacity and Achievability","awardID":"1116749","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["531713"],"PO":["557315"]},"181233":{"abstract":"This project investigates the use of image curve fragments to augment the use of isolated features in multi-view calibration and reconstruction tasks. The research team develops infrastructure based on differential geometry that utilizes curves and surfaces beyond lines and planes to correlate structure in multiple images of a scene: a pair of corresponding curve fragments in two views initiates a candidate 3D curve fragment whose presence can be validated in additional views. This results in a 3D curve sketch. A significant advantage of the 3D curve sketch over an unorganized cloud of point reconstruction is that it can correlate with image curve structure in novel views without referring back to the original views. This allows both incremental reconstruction (incorporating one additional view at a time) and simultaneous reconstruction from numerous views. Calibration methods are also being explored by using curve fragments under a RANSAC regime by using differential geometry in three views or more, and differential geometry together with appearance in two views. In a similar vein the correlation of surface patches by matching observable intensity local form in images is being investigated as a method for reconstruction of local surface patches. The project provides a core technology for many applications in the computer vision field. The developed technology can be also applied to other fields such as archaeology and art.","title":"RI: CGV: Small: Multiview Reconstruction and Calibration Using Differential Geometry of Curve Fragments and Surface Patches","awardID":"1116140","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["550631"],"PO":["564316"]},"181354":{"abstract":"The last decades have seen an enormous growth in the performance capabilities of computing platforms. One important driver in the computing revolution was productivity enabled by higher-level languages and software engineering methods that allow building and maintaining complex software systems. However, unable to fully utilize modern computer systems, software built that way can under-perform by several orders of magnitude on top-end workstations. With CPU frequency scaling coming to a halt and the switch to multicores, high-end systems become increasingly energy-constrained. In addition, as more computing is becoming mobile, inefficiency directly translates into wasted energy and shorter battery life, and ultimately, less usefulness.<br\/><br\/>Today, high efficiency is mostly achieved through heroic human programming by a small number of experts. This research studies how to go beyond this unsustainable situation and how to make a high level of efficiency accessible to average programmers across a wide range of platforms and algorithms. The Hot Bench system developed in his research provides interactive optimization capabilities for the generation of highly efficient programs, for both average and expert users. It includes a comprehensive library of optimization strategies for targeting various kinds of algorithms, data structures, and target platforms. Expert users can add optimization strategies, target new instruction sets, or add new output languages. Less experienced programmers can learn how optimization works and how complicated architectures are programmed. The system is designed to be extensible to virtually any program type and target hardware, and to be able to encode all the necessary optimization methods and tricks. Hot Bench is in spirit similar to computer algebra\/mathematics systems like Matlab, Maple, or Mathematical, and interactive theorem proves, applying their concept to hotspot tuning.","title":"SHF: Small: HotBench: An Optimization Workbench for Hotspots","awardID":"1116802","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["533346"],"PO":["565272"]},"181596":{"abstract":"On both practical and theoretical levels, linear programming is one of the most important optimization problems studied in Computer Science and Operations Research. But, on both levels, our current understanding of linear programming has substantial room for improvement. <br\/><br\/>Theoretically, the worst-case time bounds that are known for current algorithms are far higher than what is observed in practice. Although the recent development of smoothed analysis gives polynomial-bounded running time on certain kinds of instances, the bounds are still very high-degree polynomials, much larger than what we see in practice. Our current theoretical models do not accurately model how linear-programming algorithms behave in practice. Practically, current algorithms often take time quadratic in the size of the input (even to find approximate, as opposed to optimal, solutions). An ideal algorithm should take linear, or nearly linear, time. State-of-the-art implementations (such as CPLEX) are generally commercially developed, taking research on them out of the public\/academic domain. Effective implementation of Simplex and Interior-Point algorithms is difficult, as evidenced by the performance gap between free and commercial solvers and the cost of the best commercial solvers. To solve very large problems, even the most effective implementations sometimes require manual testing and tuning, and even then can be unpredictably slow. <br\/><br\/>Existing algorithms leave room for improvement on several fronts, including ease of implementation, ease of use, numerical stability, public accessibility, running time, and good theoretical analyses. Given the widespread practical and commercial importance of linear programming, the development and publication of algorithms with provably good worst-case running times, provable numerical stability, and relatively simple, open-source implementations, have the potential for broad practical and commercial impact. The goal of the project is to make progress in this direction by designing provably fast (nearly linear time) and numerically robust approximation algorithms for very large linear-programming problems. The project focuses on algorithms for so-called mixed packing and covering linear programs --- an important special case in which all coefficients in the constraint matrix are non-negative. In practice, for problem instances having more than thousands of rows and columns, the goal of the project is algorithms that find solutions within 1% of optimal, and do so orders of magnitude more quickly than algorithms now used in practice (Simplex, Interior Point, and Ellipsoid). The algorithms should be numerically stable --- requiring no special treatment of instances with ill-conditioned constraint matrices. The algorithms will be made publically available via implementations and open-source publications.","title":"AF: Small: Nearly Linear-Time Algorithms for Mixed Packing and Covering Linear Programs","awardID":"1117954","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[486542],"PO":["565251"]},"181486":{"abstract":"Today's massive generation of digital data is greatly outpacing the development of computational methods and tools and presents critical challenges for achieving the full transformative potential of these data. For example, recent advances in acquiring multi-modal brain imaging and genome-wide array data provide exciting new opportunities to study the influence of genetic variation on brain structure and function. Major computational challenges are, however, bottlenecks for comprehensive joint analysis of these data due to their unprecedented scale and complexity. This project will employ the new capabilities of large-scale data mining techniques in multi-view learning, multi-task learning, and robust classification to address critical challenges in systematically analyzing massive multi-modal genetic, imaging, and other biomarker data. Specifically, this project will: (1) develop new multi-view learning methods to detect task-relevant phenotypic biomarkers from large scale heterogeneous imaging and other biomarker data, (2) implement new sparse multi-task regression models to reveal the genetic basis of phenotypic biomarkers at multiple levels (e.g., SNP, haplotype, gene and\/or pathway), (3) design novel robust classification methods via structural sparsity for outcome prediction using integrated genotypic and phenotypic data, and (4) package these new methods into a data mining toolkit and release it to the public. <br\/><br\/>The intellectual merits of this project derive not only from the development of novel data mining methods, but also from their application to imaging genetic studies. These methods are designed to take into account interrelated structures among multiple data modalities and offer systematic strategies to reveal structural imaging genetic associations. The proposed methods and tools are expected to impact neurological and psychological research and enable investigators to effectively test imaging genetics hypothesis and advance biomedical science and technology. In addition, the proposed data mining framework addresses generic critical needs of large-scale data analysis and integration and, therefore, will impact a large number of research areas where high-value knowledge and complex patterns can potentially be discovered from massive high-dimensional and heterogeneous data sets. This project will facilitate the development of novel educational tools to enhance several current courses at UT Arlington and IUPUI. Both universities are minority-serving institutions, and the PIs will engage the minority students and under-served populations in research activities to give them a better exposure to cutting-edge scientific research.","title":"III: Small: Collaborative Research: A Large-Scale Data Mining Framework for Genome-Wide Mapping of Multi-Modal Phenotypic Biomarkers and Outcome Prediction","awardID":"1117335","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["13807",486250],"PO":["565136"]},"181255":{"abstract":"This project develops the Propagator Model, a concurrent decentralized<br\/>framework designed to support computing in large, open, dynamic<br\/>environments. It provides powerful tools for organizing computations<br\/>to operate effectively in a world of rapidly changing and globally<br\/>inconsistent data by adopting a fundamental shift in viewpoint: the<br\/>things manipulated by basic computing elements are not fixed values.<br\/>Rather, they are information about values, and this information is<br\/>continually refined as new information becomes available.<br\/><br\/>This project creates the architectural and linguistic foundations for<br\/>systems that can operate effectively in environments where there is no<br\/>central management, and where one cannot rely on resources being<br\/>consistently available or consistently operating, and where the data is<br\/>rapidly changing and globally inconsistent. Using three mechanisms<br\/>implicit in the computational substrate: (1) constraint propagation,<br\/>(2) partial information structures, and (3) dependencies, systems<br\/>built on the propagator model automatically have the ability to<br\/>support their conclusions with arguments and report on the provenance<br\/>of the ingredients. They can automatically discover and use<br\/>consistent subsystems of inconsistent data.<br\/><br\/>This project erects a naturally concurrent and distributed model and<br\/>infrastructure for computation that makes it easier to build systems<br\/>that are reliable in the face of natural failures and deliberate<br\/>attacks. It provides support for auditable and accountable systems<br\/>that are robust and adaptable to novel applications.","title":"CSR: SHF: Small: Propagator-Based Computing, A Programming Foundation for Decentralized Systems","awardID":"1116294","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["551967",485691],"PO":["565255"]},"181497":{"abstract":"Holistic design approaches have gained in prominence in microprocessor design research, as they show tremendous potential in improving multiple design objectives such as energy efficiency and reliability. However, holistic techniques encompassing multiple design layers, which are traditionally decoupled, are facing severe challenges from the economic feasibility standpoint. On the other hand, to reduce the design costs, several industry leaders are advocating techniques to effectively involve design automation in multiple layers of computer system design.<br\/><br\/>This research combines two promising directions for future computer designs: holistic perspective and embracing design automation at multiple design layers. The research builds a foundation for realizing an end-to-end holistic design, where design choices from multiple layers are seamlessly integrated. Joint<br\/>design space exploration is performed in logic and physical design, while considering system level architectural design choices and their implication on workload execution. Two central themes are explored under this paradigm: (a) improving system wide energy efficiency, and (b) improving system reliability focusing on intermittent timing fault vulnerability. Innovations in computer design technology that surpass boundaries between multiple design layers can be modeled and investigated using this framework, stimulating research on far-reaching problems.<br\/><br\/><br\/>Research in the area of cross-layer design will enable circuit designers and system architects to increase collaborative design to develop affordable, energy-efficient and reliable computer systems. Major research insights will be integrated in courses taught by the PIs to train computer engineers. Skill sets disseminated through teaching will create new specialties and lead to more efficient future computer systems.","title":"CSR:Small:Employing Design Automation to Build Foundations for Holistic Multicore Design","awardID":"1117425","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["550105","550106"],"PO":["565255"]},"181156":{"abstract":"Encouraging broader adoption of renewable energy sources is key to minimizing our dependence on the electric grid. Though installation of solar panels at homes is increasingly common, the process of managing the energy they generate is both manual and ad hoc. In this work, the PIs are building an infrastructure for data collection and analysis of energy generation, energy consumption, and user behavior in green homes; and an an integrated approach to green home energy management validated using a novel recommendation-based evaluation platform. Using a custom-built measurement infrastructure, the project conducts a broad study of homes powered by a variety of renewable sources. The study examines both energy generation by renewable sources as well as how and why energy is consumed by a variety of devices. The results inform the design of a holistic control system that matches predicted supply with demand of a distributed set of devices in the home. Moreover, the system is being deployed in a few homes and evaluated using a recommendation system implemented as a mobile application that suggests when users should run devices.<br\/><br\/>This project supports research critical to encouraging adoption of more environmentally responsible practices in the home and enables a collaboration between PI Banerjee, who teaches at an EPSCoR institution, and PI Rollins, who teaches at an undergraduate institution. Further, the project is developing a CS1 course that will increase awareness of green energy concepts by introducing computing through collection and analysis of data on energy consumption practices.","title":"CSR: Small: Collaborative Research: System Support for Green Homes","awardID":"1115798","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["563481"],"PO":["565255"]},"181277":{"abstract":"Learning visual representations remains a challenge for systems which interact with the real world and\/or analyze visual information available on the web. Significant progress has been made with simple visual features based on gradient histograms: these models work extremely well on objects that have highly textured and nearly planar patterns or parts. However, these systems suffer when faced with certain classes of real world objects that do not have discriminative locally-planar opaque texture patches, especially objects with complex photometric models. This project develops layered visual models for visual recognition, which can model these classes of phenomena. The research team grounds the methods in a probabilistic foundation, primarily exploiting a sparse Bayesian approach to factoring observed image features into a set of component layers corresponding to an additive image formation process. Considering both local descriptor and local feature detector variants of the model, the research team offers a new concept for interest point detection in the case of transparent objects: extrema detection in a latent-factor scale space. This model has the potential to find invariant local detections despite transparency, and could be useful in a range of vision applications beyond pure recognition for which sparse local feature detectors have proven valuable (e.g., registration, mosaicing, SLAM). Robotic vision systems can use this representation for enhanced recognition of everyday objects, supporting domestic and industrial applications. These representations also facilitate intelligent media processing and indexing.","title":"RI: Small: Hierarchical Probabilistic Layers for Visual Recognition of Complex Objects","awardID":"1116411","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["515704"],"PO":["564316"]},"186854":{"abstract":"This Early Grant for Exploratory Research seeks to investigate the viability of a knowledge-rich, joint-learning approach to coreference resolution, with the ultimate goal of advancing the state of the art in coreference resolution. Given recent advances in research on lexical semantics and discourse, and the development of large-scale lexical databases, the first objective of this grant is to investigate whether existing language technologies are mature enough to accurately extract semantic, discourse, and world knowledge from structured and unstructured data so that learning-based coreference systems can be significantly improved when such knowledge is employed.<br\/><br\/>An assumption underlying the first objective is the use of a pipeline system architecture, where sophisticated linguistic information from various sources is computed prior to coreference resolution. While a pipeline architecture is popularly-used in coreference research, the errors made by the upstream components may propagate to the coreference component and adversely affect its performance. To address this problem, the second objective of this grant is to explore an approach in which multiple tasks in the pipeline are learned in a joint fashion. While most research on joint learning for language processing focuses on two tasks, this work seeks to take the challenge involved in joint learning to the next level by simultaneously learning a large number of tasks in semantics, discourse, and information extraction, which can all benefit from their interactions with each other and with coreference in the learning process.","title":"EAGER: Joint Learning for Knowledge-Rich Coreference Resolution","awardID":"1147644","effectiveDate":"2011-08-15","expirationDate":"2014-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["518623"],"PO":["565215"]},"181299":{"abstract":"Over 80 million households in the United States have a home computer and an Internet connection. The vast majority of these are overseen by people who have little computer security knowledge or training, and many users try to avoid making security decisions because they feel they don't have the knowledge and skills to maintain proper security. Nevertheless, home computer users still make security-related decisions on a regular basis --- for example, whether or not to click on a link in an email message --- without being aware that is what they are doing. Their decisions are guided by how they think about computer security,their mental models. Interestingly, these models do not have to be technically correct to lead to desirable security behaviors. In other words, sometimes even \"wrong\" mental models produce good security decisions. This project will explore the implications of that insight. By eliminating the constraint that non-technical users must become more like computer security experts to properly protect themselves, this project will identify and create more effective ways of helping home computer users make good security decisions.<br\/><br\/>This project will help advance our understanding of how mental models of security are formed and how ideas are incorporated into mental models and transmitted from person to person. What kinds of information are incorporated into home computer users' mental models? Work will initially be focused on experimentally testing two hypotheses: a) stories about experiences have a larger influence on behavior than behavioral advice, and b) information from friends and colleagues has a stronger influence on mental models, and therefore behavior, than information from security experts. Additionally, the prevalence of particular mental models will be measured and correlated with actual user security behaviors. Through these investigations, this project will characterize the reasons that many home computer users choose not to act securely --- a question which is one of the biggest challenges of home computer security. Finally, this project will explore ways of encouraging behaviors that support secure system use by developing a prototype socio-technical system that is capable of influencing their mental models and moving people toward models that lead to greater security.<br\/><br\/>Home computer security and personal information security are large problems today. Current education campaigns have failed to effect widespread changes in the security behaviors of non-technical users. New technologies are being developed, but will do nothing if users intentionally choose to ignore the technology or to work around it. This project will find better ways of informing people about security issues, altering their understanding of security threats and thereby their security behaviors, which will ultimately create more secure home computers. It will produce research tools, including survey instruments and security behavior measurement software that can be used by other security researchers. It will train a number of students, both graduate and undergraduate, in working on multi-disciplinary, distributed teams. The results from this study will be disseminated broadly to multiple academic communities.","title":"TC: Small: Collaborative Research: Influencing Mental Models of Security","awardID":"1116544","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["562836"],"PO":["565136"]},"184203":{"abstract":"Realizing the power of CyberLearning to transform education will require vision, strategy, and an engaged, talented community. Activities are needed to energize the community, refine and sharpen the path forward, and provide a more active and ongoing forum for clarifying the big ideas and challenging questions. In response to this need, SRI International, together with the Lawrence Hall of Science and with key support from the National Geographic Society, will organize a set of activities to advance a shared vision of the future of learning, encompassing the systems, people, and technology dimensions mutually necessary for any scalable and lasting advances in education. <br\/><br\/>The innovative format for these activities is inspired by the TED talks, Wikipedia, and social networking. As in TED, a small set of leading researchers will be selected to give very short, very high quality, stimulating talks. These CyberLearning Talks will be featured at a 1-day summit meeting in Washington, DC, streamed so that local cyberlearning research communities may participate at a distance, and posted on a website. As in Wikipedia, CyberLearning Pages will be created, each page featuring a synopsis of a big idea in CyberLearning and the relevant research challenges. The 1-day conference will be followed by a small 1-day workshop focusing on how to evaluate cyberlearning efforts, identify progress, and identify important new directions. Finally, to disseminate and stimulate conversation about both the video talks and Wikipedia entries, a presence for the community will be created on social networking sites.<br\/><br\/>The target outcomes of the effort will be (i) a cyberlearning research community with participants from across the many current constituent communities, and fostered awareness and appreciation of the broad range of expertise and interests across that wider community; (ii) foundations for sustained discussion of big ideas, insights, and challenges to help this new community define a more engaged, crisper vision of its own future, (iii) a community resource that can become a site for interconnecting stakeholders in the CyberLearning community and supporting investigators in improving field-generated proposals, and (iv) an emerging sense of direction for CyberLearning among a wider audience of leaders. Such community building and awareness is expected to foster collaborations that will lead to innovative and research-grounded ways of using technology to transform education -- formal and informal and across a lifetime.","title":"Imagining the Future of Learning: Systems, People, and Technology","awardID":"1132393","effectiveDate":"2011-08-15","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8020","name":"Cyberlearning: Transforming Ed"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7261","name":"PROGRAM EVALUATION"}}],"PIcoPI":["524324",493972],"PO":["562669"]},"187965":{"abstract":"The acquisition of high resolution multi-dimensional image data is becoming increasingly important in several bio-medical applications (e.g., spectroscopic imaging and cardiac imaging). Very often, imaging devices are pushed to their limits in the quest for high spatio-temporal\/spatial-spectral resolution, resulting in several artifacts and SNR loss. Recently, the recovery of the image data from sub-Nyquist sampled measurements using constrained image models has emerged as a promising alternative. A challenge in using pre-determined models is the misfit between the representation and dataset; many coefficients are often required to represent the signal at hand. The main focus of this work is to develop a novel theoretical framework and efficient algorithms to adapt image representations to under-sampled measurements. <br\/><br\/>We specifically focus on blind or adaptive representations, which are a significant departure from classical approaches based on pre-determined dictionaries. By adapting the signal model to measurements, we expect to obtain unbiased reconstructions from far smaller numbers of measurements. We formulate the joint estimation of the representation and the signal from the entire under-sampled data as a single optimization problem, where the criterion is only dependent on the recovered signal. This enables the development of efficient optimization algorithms, performance optimization using tailored cost functions, and determination of the conditions for perfect recovery. This approach is expected to considerably improve the resolution in several multi-dimensional imaging schemes, which will facilitate several basic science and clinical applications of very high significance; the proposed research is truly transformative. The impact of this work is strengthened by the sharing of software and data-sharing, integration of research and teaching, and well-designed out-reach program.","title":"CIF: Small: Adaptive signal representation for accelerated multidimensional imaging","awardID":"1153512","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":[503850],"PO":["564898"]},"186316":{"abstract":"Anonymous communication has received increasing attention in the past decade because of concerns on privacy issues of communication through the Internet. Although these existing anonymous communication networks can greatly help protect communication privacy, these networks, especially the low-latency anonymous communication networks, are vulnerable to a number of traffic analysis attacks.<br\/><br\/>The main objective of this project is to study new architectures for anonymous communications in order to defeat traffic analysis attacks, both passive and active traffic analysis attacks. The project focuses on hiding traffic patterns by dissipating statistical structure information of network traffic through network coding, which has never been tried before. Compared to the existing approaches, the new approaches will be able to hide traffic patterns while achieving higher bandwidth efficiency and suffering less degradation in the quality of service. A scientific foundation for anonymity networks is to be established through an understanding and analysis of the complex relationship among the levels of anonymity, usability of the networks, and amount of network resources needed. The project, if successful, will help stimulate new researches on anonymization approaches.<br\/><br\/>Given increasingly pervasive nature of computing and communication setting and increasing popularity of anonymity networks, protecting communication privacy and privacy of various Internet applications is of great societal importance. The project also serves as a means to introduce minority students to academic research activity and to bring additional undergraduate and graduate to research in the network security and privacy area.","title":"EAGER: Towards Next Generation Anonymous Communication Networks","awardID":"1144644","effectiveDate":"2011-08-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["469752"],"PO":[499958]},"185139":{"abstract":"As computing moves into the PetaFlop era, and beyond to ExaFlop levels, issues of equipment cost, power consumption, reliability, programmability, etc., are becoming increasingly important. Over the past few years more and more systems are depending upon accelerator hardware technologies (AT). The Asian Technology Information Program (ATIP) plans to hold a workshop that will explore the ways Asian scientists are incorporating AT technologies into their research. Speakers will present on their level of success and the impediments they have encountered, etc. Their progress will be compared with that in the US, and ATIP will generate a short informative report summarizing the findings for distribution throughout the global HPC community.<br\/>ATs can be very cost effective. The most popular ATs are based on technology used in the game community, and hence are familiar to younger researchers, and even to some students in high school. Therefore, they have the potential to help migrate high-performance computing broadly, into smaller research settings, including small universities and beyond. Absolute cost as well as cost performance is crucial in researchers decisions about the practical directions of their research. This is especially true in most parts of Asia, where essential hardware must be purchased from abroad (US), and Asians have been quick to consider ATs as a result. This workshop brings together top scientists and graduate students from both the US and Asia hoping that they will form connections that may lead to future research collaborations or cooperative commercialization efforts for practical HPC applications.","title":"Accelerator Technologies (AT) for High Performance Computing (HPC) (Does Asia Lead the Way?)","awardID":"1138301","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[496762],"PO":["565272"]},"186129":{"abstract":"This project explores the ways in which massively-multiparticipant online interactions provide experiences of meaningful accomplishment and how this engagement may portend the social use of digital technology for mass persuasion and motivation. Use of distributed network gaming technology has expanded rapidly in all age and ethnic demographics over the past decade, and current predictions conclude that growth will continue. As many of these online engagements and computationally produced virtual worlds explore idealistic themes or enable experiences of fulfillment, they could powerfully reshape the mores of modern life , especially given that it is the experiences made possible which account for much of their appeal. The principal investigator and four undergraduate students will examine a variety of such virtual worlds for the significant themes within them, such as the conflict between good and evil, a system of principles and adherents, development of personal meaning or character development, and individual identity expressed in heroic terms. All researchers will conduct fieldwork, interviewing users within the virtual worlds, and surveys will be conducted online for the residents of one or more virtual worlds. While shedding light upon the appropriation and modification of such experiences in virtual worlds, the project will also help clarify the ways in which cultural beliefs interact with technological development and, more importantly, the influence of technology on the public. <br\/><br\/>The results of this research will be disseminated through several publication efforts. The principal investigator will mentor the students through the authorship of academic papers, which they will submit for presentation or publication. The students will thereby learn to carry out long-term research projects and develop their insights for an academic audience. An additional paper co-authored with the students will assess all of the findings and draw broad-based conclusions from the comparisons. The various publications will help the project reshape public and academic understandings of how digital technologies operate in public life, especially in their reconfiguration of ideas and practices.","title":"EAGER -- Virtually Meaningful: The Power and Presence of Meaning in Virtual Worlds","awardID":"1144028","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[499479],"PO":["564456"]},"177329":{"abstract":"This project explores (semi-)automatic ways to create \"semantically discriminative\" mid-level cues for visual object categorization, by introducing external knowledge of object properties into the statistical learning procedures that learn to distinguish them. In particular, the PIs investigate four key ideas: (1) exploiting taxonomies over object categories to inform feature selection algorithms such that they home in on the most abstract description for a given granularity of label predictions; (2) leveraging inter-object relationships conveyed by the same taxonomies to guide context learning, so that it captures more than simple data-driven co-occurrences; (3) exploring the utility of visual attributes drawn from natural language, both as auxiliary learning problems to bias models for object categorization, as well as ordinal properties that must be teased out using non-traditional human supervision strategies; (4) mining attributes that are both distinctive and human-nameable, moving beyond manually constructed semantics.<br\/><br\/>The project entails original contributions in both computer vision and machine learning, and is an integral step towards semantically-grounded object categorization. Whereas mainstream approaches reduce human knowledge to mere category labels on exemplars, this work leverages semantically rich knowledge more deeply and earlier in the learning pipeline. The approach results in vision systems that are less prone to overfit incidental visual patterns, and representations that are readily extendible to novel visual learning tasks. Beyond the research community, the work has broader impact through inter-disciplinary training of graduate and undergraduate students, and outreach to pre-college educators and students through workshops and summer camps encouraging young students to pursue science and engineering.","title":"RI: Medium: Collaborative Research: Semantically Discriminative : Guiding Mid-Level Representations for Visual Object Recognition with External Knowledge","awardID":"1065390","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["543538"],"PO":["564316"]},"181630":{"abstract":"New application scenarios (most notably arising from cloud computing) have given rise to a host of new security concerns. Unsurprisingly, classical notions of encryption leave much to be desired for dealing with these new application scenarios and concerns. Motivated by these concerns, we develop novel encryption systems that provide vastly greater functionality than classical notions of encryption. In particular, we focus on two broad research directions: Functional encryption and Efficient Secure Encodings of Computation. In functional encryption, instead of encrypting a message to an individual user, one can encrypt a general function f. If a receiving user possesses certain credentials X, then it obtains a secret key SK-X corresponding to those credentials. When this user obtains a ciphertext that is an encryption of f, by applying the decryption algorithm it obtains f(X), and no more. Efficient Secure Encodings of Computation is a totally new notion that has fundamental applications to novel expressive encryption schemes including functional encryption with public keys (recently introduced by the PI under the terminology ?Worry-Free Encryption?), and most notably controlled Homomorphic Encryption -- where expressive homomorphisms are possible on ciphertexts, but the initial encryptor can control what homomorphisms are allowed.","title":"TC: Small: New Directions in Encryption","awardID":"1118096","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["521734"],"PO":["565239"]},"181531":{"abstract":"This proposal investigates new computational and combinatorial tools for learning from data and performing inference about data. As such, it serves as a bridge between the machine learning community and the combinatorics community. The latter field develops efficient algorithms or shows when an efficient solution to a particular problem exists. The project will support graduate students in the intersection of these two fields to combine recent theoretical results with practical data problems. The team will produce a website of tutorials, data sets, downloadable code, interactive visual examples and course materials to allow better integration across the two fields. The combinatorics community has identified a family of inputs called perfect graphs where otherwise hard problems are efficiently solvable. This proposal will investigate how to bridge this powerful theoretical result to the area of machine learning and formally characterize which learning and inference problems are efficiently solvable.<br\/><br\/>More specifically, the team will investigate data learning problems (such as clustering a data set into subgroups that are self-similar or finding anomalies in a database) and statistical inference problems (computing the most likely or most typical outcome from observed measurements). These problems will be compiled or represented as graphs and networks. Then, these graphs and networks can be more formally diagnosed using perfect graph theory to determine if the instance of the problem is easy (or hard) and to provide efficient solutions via exact algorithms on perfect graphs. These solvers will be implemented using message passing or convex programming.","title":"RI: Small: Learning and Inference with Perfect Graphs","awardID":"1117631","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["541923","540411"],"PO":["562760"]},"181300":{"abstract":"This research focuses on facilitating the design of Self-Stabilizing (SS) network protocols, where a SS protocol eventually recovers from any troubled configuration to a legitimate configuration (i.e., convergence) and stays in legitimate configurations as long as there are no perturbations (i.e., closure). This is an important problem as today's complex distributed systems (e.g., the Internet) frequently reach illegitimate configurations due to the occurrence of the transient faults that perturb the protocols without causing permanent damage (e.g., bit flips in memory, bad initialization). Most existing methods are based on the manual creation of an initial design and after-the-fact verification of the manual design. This research presents a paradigm shift based on a philosophy of 'synthesize in small scale and generalize'. Specifically, the investigators study (i) the automatic generation of small instances of SS protocol, and (ii) the generalization of the synthesized instances to larger protocols.<br\/><br\/>Instead of designing and verifying problem-specific SS protocols, this research provides a reusable repository of synthesis methods for the addition of convergence to nonstabilizing protocols. Moreover, this research presents three techniques for the generalization of the small instances of the synthesized protocols, namely synthesized convergence stairs, cutoff theorems and structural induction. For evaluation, the investigators study the automated design of convergence for real world protocols and classic distributed computing problems (e.g., mutual exclusion and leader election) under different topologies and distinct fairness assumptions. The investigators also identify complexity criteria and compile them in a public benchmark.","title":"AF: Small: A Framework For Algorithmic Design of Self-Stabilizing Network Protocols","awardID":"1116546","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[485796],"PO":["565251"]},"180453":{"abstract":"Most computers today are 'multicore' parallel computers that are capable of executing several independent threads of computation simultaneously. Unfortunately, most existing programs are not parallel and cannot take advantage of this hardware capability. Furthermore, writing parallel programs using current notations like OpenMP is more difficult than writing sequential programs and, as a result, increases development costs and the likelihood of program defects.<br\/><br\/>The Kali project is building a software system that will permit most application programmers to write sequential programs and still obtain good performance on multicore processors. Parallelism will be hidden within object-oriented class libraries written by expert parallel programmers and it will be managed by a sophisticated runtime system that uses a range of parallel execution strategies customized to the needs of the application. Applications programmers can take advantage of the benefits of sequential programming such as familiarity, readability, maintainability, and debuggability. They will also be able to tune program performance and power without having to drop down to a lower abstraction level. In addition, the Kali project is studying the use of innovative hardware to facilitate the development of efficient programs. Finally, the project is producing a suite of application benchmarks that will be useful for performance evaluation of similar systems.","title":"CSR: Large: Collaborative Research: Kali: A System for Sequential Programming of Multicore Processors","awardID":"1111407","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":["533380","550505"],"PO":["551712"]},"181432":{"abstract":"Every aspect of parallel software development is more complicated than for<br\/>serial programs. This research focuses on one of the primary sources of<br\/>complexity: intra-application communication. Currently it is a programmer's<br\/>responsibility to find an efficient mapping of their application's<br\/>communication patterns onto the communication infrastructure of the target<br\/>system. This research flips that responsibility by developing a flexible<br\/>communication architecture and associated tools and algorithms that allow the<br\/>target platform to be specialized for a particular application, rather than<br\/>vice versa. In addition to reducing the programmer's burden, specialization<br\/>has the potential to improve communication efficiency while the automated<br\/>techniques can increase portability. <br\/><br\/>This research poses questions whose answers have consequences at several<br\/>levels of the traditional system stack: Can programmers be freed from<br\/>hardware-specific optimization of communication without degrading performance?<br\/>What abstractions are needed to allow hardware to adapt to the programmer,<br\/>rather than the other way around? Can communication efficiency be improved<br\/>when running on an application-specific communication platform? The project<br\/>answers these questions by exploring abstractions and algorithms to profile a<br\/>parallel program's communication, synthesize a custom network design, and<br\/>implement it in a configurable network architecture substrate. The research<br\/>methods center around the X10 language, and include compiler instrumentation<br\/>passes, offline communication profile analyses, development of a portable<br\/>network intermediate representation, and network place and route software<br\/>algorithms.","title":"CSR: Small: Fluid Communication for Parallel Programs","awardID":"1117135","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["534424"],"PO":["565255"]},"181564":{"abstract":"It is an irony of our time that despite living in the 'information age' we are often data-limited. After decades of research, scientists still debate the causes and effects of climate change, and recent work has shown that a significant fraction of the most influential medical studies over the past 13 years have been subsequently found to be inaccurate, largely due to insufficient data. One reason for this apparent paradox is that modeling complex, real-world information sources requires rich probabilistic models that cannot be accurately learned even from very large data sets. On a deeper level, research inherently resides at the edge of the possible, and seeks to address questions that available data can only partially answer. It is therefore reasonable to expect that we will always be data-limited.<br\/><br\/>This research involves developing new algorithms and performance bounds for data-limited inference. Prior work of the PIs has shown that, by taking an information-theoretic approach, one can develop new algorithms that are tailored specifically to the data-limited regime and perform better than was previously known, and in some cases are provably optimal. This project advances the goal of developing a general theory for data-limited inference by considering a suite of problems spanning multiple application areas, such as classification; determining whether two data sets were generated by the same distribution or by different distributions; distribution estimation from event timings; entropy estimation; and communication over complex and unknown channels. Whereas these problems have all been studied before in isolation, prior work of the PIs has shown it is fruitful to view them as instances of the same underlying problem: data-limited inference.","title":"CIF: Small: Collaborative Research: Algorithms and Information-Theoretic Limits for Data-Limited Inference","awardID":"1117765","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":[486456],"PO":["564924"]},"181454":{"abstract":"The advent of cheap, ubiquitous, portable, multi-radio devices means that mobile wireless networking will be the predominant future method for network access and edge network formation. Many of these devices will have mobility that is dictated by the movement of the humans who carry them. Hence, capturing real-world human mobility patterns is essential for improving our understanding of the new networks and communication opportunities that will arise. However, even small-scale measurements carry a significant organizational overhead in preparing, distributing and collecting contact loggers, not to mention non-technical issues related to legality and participant consent. This research program addresses two key issues: the recovery of mobility information from contact traces, and the transformation of traces collected under one set of circumstances to realistic alternative scenarios. The basis for the recovery work is a dynamic, force-based heuristic to recover plausible mobility from a contact trace. The basis for the transformation techniques is the combination of the original trace and the plausible mobility, plus models for real-world alternative settings. In both cases, the work addresses fundamental questions about efficient computational methods and the limits to computational capability for recovery and transformation. Answers to these two questions will allow researchers to collect and transform traces so as to extract maximum value from expensive measurement methods, enabling trace-driven simulation of mobile wireless networks to play a central role beside analysis and direct experimentation. In addition, the trace collection experiments used in validation will provide a valuable resource to researchers in mobile networking.","title":"NeTS: Small: Recovery and Transformation of Human Mobility Traces for Mobile Wireless Experimentation","awardID":"1117190","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["550428","550429"],"PO":["557315"]},"181355":{"abstract":"The electrical power used by all classes of computers is becoming of increasing importance. A major component of the cost of running large server operations is power, and, on the other end, the utility of laptops, palmtops, pads, and other portable devices is highly constrained by their batteries? capacity to power operations. <br\/>One important element of the power used by computers is that burned by the operating system, the software that controls the computer?s operation. Little is known about the impact different operating system options have on a machine?s power budget, due to difficulties with existing technology in precisely measuring power use and correctly ascribing power expended to its true source. Thus, determining if scheduling algorithm alternatives, different forms of memory management, or different security options offered by the operating system have good, bad, or neutral power impacts has been difficult. The Kalipers project will address this issue by using new technology to obtain detailed and reliable information about how important operating system components affect machine power use.<br\/>Kalipers will achieve this goal through a program of experimentation using a unique hardware\/software platform called LEAP. The LEAP platform, which we have already built and tested, allows fine-grained assignments of power use to particular piece of code. It also measures the differing power consumption by important system components, such as the CPU, the memory, and the hard drive. We will use the LEAP?s capabilities to determine how operating system actions and decisions affect the power use of platforms that have strong needs to minimize their power budget.<br\/>Our experiments will be concentrated in three areas: file systems, memory management, and security services. These are areas we have experience in and where we have reason to believe power savings can be found. We will investigate alternative technologies (such as the power costs of different file systems, or software vs. hardware full disk encryption). The LEAP technology will allow us to dive more deeply into the power costs of different system components, allowing us to shed light on issues like the degree to which higher power costs for ext3 are due to its implementation details and the degree to which they are inherent in the journaling nature of that file system. We will also reimplement key components of the operating system to demonstrate how the knowledge we have gained can be used to improve a system?s power consumption.","title":"CSR:Small:Kalipers-Deep Energy Inspection of Operating System Components","awardID":"1116809","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["539241",485932],"PO":["565255"]},"181476":{"abstract":"Frequent doubling of computer system performance has facilitated innovations in science, education, government, and commerce. The foundations of these improvements, Moore's Law improving density and Dennard scaling reducing transistor power, have enabled chips with exponentially more transistors at roughly fixed power and cost. However, upcoming physical limits threaten to force a choice between either chip power and cost escalation or stagnant chip performance where lowest cost wins.<br\/><br\/>This project seeks a new middle approach, called dark silicon, that keeps the number of powered-on transistors roughly constant even as the number of transistors per chip grows. Rather than add general-purpose cores, future mainstream chips will deploy many accelerators to improve performance or power by 10x-100x. Such chips will turn on one or more accelerators when needed to help power and\/or performance and leave most others off. Accelerators for system-on-chip (SoC) have already been designed, for such uses as encryption, (de)compression, network protocols, XML, and graphics. This research seeks to invent and refine architecture and system support to make existing and future accelerators possible in mainstream processors. Specific focus is given to low-overhead solutions facilitating fine-grain use that allow accelerators to be used and shared while protecting security and privacy. The project relies on co-design of hardware and software interfaces to accelerators in order to enable direct, low-latency access from user-mode code via coherent shared-memory communication.<br\/><br\/>More efficient access to accelerators enables Moore's law to bring continued performance increases without corresponding increases in power. This can reduce the overall power consumption of computing and reduce greenhouse gas production, as well as provide greater computation power at any scale, whether in a mobile device or a supercomputer. The PIs continue to impact broadly the state-of-the-art of computer systems through students, courses, talks, industrial affiliates, commercial influence, and sharing of infrastructure.","title":"CSR: Small: Codesign of Accelerator Interface Software and Hardware","awardID":"1117280","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["541915","541916","559530"],"PO":["551712"]},"181597":{"abstract":"Many problems in artificial intelligence, engineering, and management can be advantageously modeled and solved as Constraint Satisfaction Problems, but scalability remains in practice a major obstacle to the successful deployment of Constraint Solvers. The goal of this project is to design algorithms and strategies that enable computers to overcome, in practice, the scalability barrier. To this end, the proposed research targets the two fundamental mechanisms in Constraint Processing that are the most promising for breaking the complexity barrier, namely, enforcing consistency and detecting and breaking symmetry. This project aims to design new algorithms for those two mechanisms and develop strategies for intertwining them. <br\/><br\/>The tractability of a problem is guaranteed by a relationship between a structural parameter of the problem and its level of consistency. This project aims to design algorithms that enforce the needed level of consistency without adversely modifying the structure of the problem. Symmetries can be detected and exploited to dramatically reduce the cost of problem solving. This project aims to (1) design algorithms for detecting symmetries, and (2) develop approximation strategies for exploiting them without sacrificing the soundness and completeness of problem solving. A key observation is that algorithms for enforcing consistency and those for locally detecting symmetry are based on the same atomic operations. Furthermore, they seem to be effective under complementary operating conditions. This project further aims to design strategies for intertwining the operation of the two types of algorithms so that the application of the one type enables and facilitates that of the other type, in order to yield new opportunities to control the combinatorial explosion. The approach will be validated on applications of practical importance and extended to address similar combinatorial problems in other areas of Computer Science such as Databases and Software Engineering.<br\/><br\/>The proposed activities contribute to the progress of the research on two fundamental aspects of Constraint Processing. From a practical perspective, this research directly benefits many combinatorial problems of practical importance. From a scientific standpoint, this project will identify connections and build new bridges with other areas of Computer Science, such as Databases and Software Engineering. The insight gained from these investigations will be used to improve the scope and content of introductory and advanced courses on Constraint Processing. The opportunities and research avenues will be heavily exploited to involve undergraduate students in research and to give them experience in using the project's insights on problems that they find engaging (e.g., Sudoku) and for understanding the operation of algorithms in Computer Science courses; the goal is to motivate students to conduct research in Constraint Processing and to transfer results from this field to other students and researchers in Computer Science, Mathematics, Engineering, to entice high-school students to study Computer Science, and in addition, to explain to the general public some of the fundamental mechanisms at the heart of complex problem solving.","title":"RI: Small: Towards Practical Tractability in Constraint Processing","awardID":"1117956","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[486544],"PO":["565035"]},"181003":{"abstract":"The two most important commodities of the 21st century are time and energy; traffic congestion wastes both. Several disciplines have studied the traffic congestion problem using mathematical models, simulation studies and field surveys. Recently, due to the sensor instrumentation of road networks and the availability of commodity sensors from which traffic information can be derived (e.g., CCTV cameras, GPS devices), a large volume of real-time traffic data at high spatiotemporal resolutions has become available. These data lay the ground for better understanding of the transportation network and for new applications.<br\/><br\/>The first objective of this project is to study the efficient acquisition and storage of traffic data. In particular, at the acquisition time, compact traffic patterns and outliers are constructed from raw data in real-time, supporting fast queries with approximate results and error bounds. The second objective is to utilize the traffic patterns to enable two critical query types that are building blocks of several important applications in time-dependent road networks: 1) shortest travel-time computation, and 2) k-nearest-neighbor search.<br\/><br\/>The broader impact of this project is to bring large amount of historical and real-time traffic data at the fidelities unseen before to the fingertip of transportation engineers, policy makers and end-users. This in turn enables them to alleviate one of the major problems of megacities, the main societal fabrics of the 21st century. Research results are incoporated into courses on Database Systems and Geospatial Information Management taught at University of Southern California and are also disseminated through papers published in conferences and journals. Details about the project including publications and open-source code are available at the project website: http:\/\/infolab.usc.edu\/projects\/TransDec\/.","title":"III: Small: Real-World Traffic Data Management for Time-Dependent Spatial Queries","awardID":"1115153","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["550740"],"PO":["563727"]},"180398":{"abstract":"The goals of this research are (1) to understand ways of recruiting and socializing volunteers to online production communities like Wikipedia, (2) to design processes and tools that assist the newcomers' information-seeking as part of their socialization, and (3) to build processes and tools to support the interpersonal processes of socialization, including peer mentorship and mentorship with more senior community members. Online production communities are becoming increasingly important, because they are creating the software that drives the Internet, generating valuable scientific data and building history's largest encyclopedia. In the face of inevitable turnover, every online community must incorporate successive generations of newcomers to survive. Newcomers are a source of content, labor, new ideas, and audience. However, attracting and incorporating newcomers into existing communities can be difficult. Socialization is the process of teaching newcomers the behaviors and attitudes essential to playing their roles in the group. Communities have available a variety of socialization tactics. Research from offline organizations shows that organizations' use of institutionalized socialization tactics and newcomers' active information seeking are effective in increasing newcomers' commitment to the organization, their satisfaction and their productivity. However, those tactics are not commonly used in online communities and seem to have different effects when they are used.<br\/><br\/>This project pursues theory-guided design. The findings from the research will extend existing theories on socialization in groups and organizations by supplementing findings primarily based on self-report measures with ones based on behavioral measures, and by providing evidence on socialization in online communities, where constraints on newcomers are radically different than they are offline. The research will develop processes and tools for solving important problems of newcomers' socialization, which will be evaluated in the context of socializing newcomers in Wikipedia and especially in the Wikipedia initiative of the Association for Psychological Science, which seeks to improve the scientific quality of articles in psychology. These tools will be made freely available to other scientific associations and other online production communities more generally.<br\/><br\/>This project supports NSF's mission to inform the public about science by improving Wikipedia as a vehicle for disseminating scientific knowledge about psychology in particular, and by developing a model for how other scientific societies could partner with Wikipedia or similar efforts to better generate and assess scientifically up-to-date and accurate information meant for the public. It will directly involve many college students, who will be assigned to write or improve psychology articles; they will get feedback from the broader community on their performance. As such it directly supports teaching and learning psychological science and will help increase students' involvement with their scientific societies.","title":"Collaborative Research: Supporting Newcomer Socialization in Online Production Communities","awardID":"1111166","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7953","name":"SOCIAL-COMPUTATIONAL SYSTEMS"}}],"PIcoPI":[483452,"560995"],"PO":["564456"]},"181014":{"abstract":"The goal of probabilistic databases is to manage large databases where the data is uncertain. Applications include Web-scale information extraction, RFID systems, scientific data management, biomedical data integration, business intelligence, data cleaning, approximate schema mappings, and data deduplication. Despite the huge demand and the intense recent research on probabilistic databases, no robust probabilistic database systems exist to date. The reason is that the probabilistic inference problem is, in general, intractable. Fortunately, in databases there are two distinct inputs to the probabilistic inference problem: the query and the database instance. This has led recently to the discovery of safe queries, which are queries that can be evaluated efficiently on any input database, and to new probabilistic inference algorithms that exploit the structure of the query. However, unsafe queries remain a major challenge in probabilistic databases.<br\/><br\/>This project studies novel algorithms for evaluating unsafe queries on probabilistic database, with guaranteed performance. It uses a novel approach, query compilation, which translates the query into one of four targets: OBDDs, FBDDs, d-DNNFs, and circuits using inclusion\/exclusion nodes. The project pursues two thrusts: (1) It develops instance-dependent compilation techniques that significantly extend the reach of instance-independent techniques used in safe queries. (2) It develops approximate query compilation techniques , which always run efficiently, even on intractable query, instance pairs, by sacrificing accuracy. These algorithms are conservative, in the sense that they return correct probabilities in all cases when the input query, instance pair is tractable.<br\/><br\/>The Intellectual Merit of this project consists of new techniques for compiling queries into one of four compilation targets, OBDD, FBDD, d-DNNF, and inclusion\/exclusion-based inference, using both exact inference (without performance guarantees), and approximate inference (with performance guarantees). It expands our understanding of probabilistic inference, and leads to practical approaches for probabilistic database engines. As Broader Impact, the project benefits a large class of applications that require general purpose management of uncertain data, ranging from large-scale information extraction systems, to scientific data management, to business intelligence. The project gradually incorporates topics from probabilistic data into into a curriculum for graduate level education; query compilation is already discussed in the PI's book on Probabilistic Databases ( http:\/\/dx.doi.org\/10.2200\/S00362ED1V01Y201105DTM016), a graduate-level textbook.<br\/><br\/>For further information see the project web site at the URL: http:\/\/www.cs.washington.edu\/homes\/suciu\/project-querycompilation.html","title":"III: Small: Query Compilation on Probabilistic Databases","awardID":"1115188","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["531543"],"PO":["563727"]},"181256":{"abstract":"From Virtual to Real<br\/>Wojciech Matusik, MIT, and Hanspeter Pfister, Harvard University<br\/><br\/>Novel and innovative digital output devices, such as stereoscopic TVs, passive (e-Ink) displays, and 3D printers, are entering the mass market. They are rapidly improving in quality and decreasing in price. This trend empowers users to consume and produce digital media like never before. However, while there has been tremendous progress in the hardware development of these output devices, the provided digital content creation software, algorithms, and tools are largely underdeveloped. For example, creating a 3D hardcopy of an animated computer graphics character is well beyond the reach of consumers, and to approximate the character's appearance and deformation behavior using multi-material 3D printers is difficult or perhaps even impossible for professionals. The main issues are a lack of accurate previews of how the output will look like, a lack of standardization between devices with similar capabilities, and a lack of accurate conversion tools and algorithms to go from the virtual (i.e., the computer model) to the real (i.e., the physical output).<br\/><br\/>This research involves the development of a complete process and software framework that allows moving from abstract computer models to their physical counterparts efficiently and accurately. Designing this process is posing the following fundamental computational challenges: (1) accurate and efficient simulation methods that can predict the properties and behavior of an output without physically generating it; (2) efficient methods to compute an output gamut that describe physically-realizable outputs for a given device; (3) general gamut mapping algorithms that convert abstract computer models to realizable points in the device gamut; and (4) accurate perceptual metrics that allow comparing different output elements during the gamut mapping algorithm. This research is focusing on two emerging classes of important output devices: multi-view auto-stereoscopic displays and multi-material 3D printers. The research is creating a complete and general software architecture that will support both existing and future output devices.","title":"CGV: Small: Collaborative Research: From Virtual to Real","awardID":"1116296","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["497067"],"PO":["565227"]},"181025":{"abstract":"Spatial positioning has emerged as a fundamental principle governing nuclear processes. Research on chromosome territories has indicated that the 3-D arrangement of these territories within the architecture of the cell nucleus may be closely linked to genomic function, regulation and cell differentiation. Despite this progress, the degree of non-random arrangement of chromosome territories remains unclear and no overall model at the global level has been proposed. In addition, little is known about the gene positioning inside the chromosome territories and its relationship to gene expression. This project is for developing algorithmic tools to facilitate the study of three important spatial positioning problems, (1) topology of chromosome territory, (2) chromosomes associations and spatial positioning, and (3) topological structures of associated chromosomes territories. The focus of this project is on designing efficient algorithms for several challenging computational problems which are essential for the spatial positioning problems, such as chromatic cone clustering, realization, maximum median graph, and optimal surface extraction.<br\/><br\/><br\/>The project will use computational geometry techniques and optimization methods to develop a set of efficient algorithmic tools for the proposed problems. The designed and tested algorithms and techniques will be used as automatic (or semi-automatic) tools to accurately and reliably determine the spatial positioning information of genes and chromosome territories, and further elucidate the coordination of genomic expression. Algorithms from this project are also likely to be used in many other areas as information integration tools and positively impact them. This project could lead to several long term impacts. It could yield a much needed global model for studying the spatial positioning of chromosome territories and genes. The obtained algorithmic and biological results will be used to study and compare the difference of various cancer and normal cells in topological structures and associations of chromosome territories and genes. This could potentially lead to significant biological discoveries and help better understanding the mechanism of diseases (such as cancers) and their relationship to chromosome structures and associations.","title":"III: Small: Algorithmic Tools for Spatial Positioning Studies in the Cell Nucleus","awardID":"1115220","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[485149,485150],"PO":["565136"]},"181388":{"abstract":"Software defects, or bugs, are a significant issue in software development and society. The cost of dealing with the faults caused by these defects is significant. Once a software fault has been identified, either by a failing test case or due to a failure in use, it can be difficult and time consuming to identify the defect responsible for the fault and how to repair it. This research will combine information from analysis of the program and the failing run to help understand the context of the fault, and provide a developer with visualizations and other information to help them understand the fault and repair the software.<br\/><br\/>The technical focus of this work consists of novel software analyses, software models, and software-development interfaces and visualizations to support software development, comprehension, and fault repair. These techniques capture and utilize static and dynamic information about the program and its execution to model relations among software elements and their functionality. These hybrid static\/dynamic models can be used as the basis for the exploration of a program when attempting to understand the nature and context of software faults so that they may be effectively and efficiently repaired. These software models utilize lightweight and commonplace dynamic information, thus making them practical and ready for near-term adoption. The interfaces and visualizations present the program to software developers in a way to reveal interactions within software, thus allowing them to comprehend larger and more complex systems than is currently possible. As such, the research will enable software-development industry to better understand and repair software faults, thus producing higher quality software, which will benefit society at large.","title":"SHF: Small: Promoting Efficient Debugging and High-Quality Software through Contextual Understanding of Faults","awardID":"1116943","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["562935"],"PO":["564588"]},"181278":{"abstract":"As scientists begin to get access to data sets that are accompanied by automatically generated provenance records, they are faced with the challenge of integrating and analyzing this metadata. Independent sources are likely to have captured provenance at distinct levels of abstraction, have different levels of completeness, used separate sets of identifiers to refer to the same artifacts, processes, and agents, and introduced dissimilar semantics in the annotations.<br\/><br\/>This research studies the problem of semi-automatically integrating and analyzing the provenance of scientific data that originates from diverse sources, with independent annotation schema, semantics that may overlap only partially, representations at different granularity, and incomplete characterizations of the activity being recorded. In particular, (i) it develops a formal framework for combining provenance, (ii) provides an extensible software system for provenance ingestion, integration, and analysis, and (iii) creates canonical provenance data sets of various sizes, granularity, and domains, that can be utilized for comparison of provenance integration and analysis algorithms.<br\/><br\/>Maintaining a record of all the transformations the data undergoes becomes increasingly critical as the length of the analysis grows and the age and diversity of sources of the data grow. Such provenance metadata can address a range of queries. For example, in situations where only derivative data is preserved, a provenance record can help validate claims about the procedures used to obtain the final results. Concerns about whether privacy-sensitive data (such as information from patient records) has been used in contravention to legal or security policies can be alleviated by checking for violations in the provenance records.<br\/><br\/>More information about the project can be found at: http:\/\/spade.csl.sri.com","title":"III: Small: Scalable Integration and Analysis of the Provenance of Diverse Scientific Data","awardID":"1116414","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[485746],"PO":["563727"]},"184798":{"abstract":"The International Exascale Software Project (IESP) brings together representatives from the major segments of the global high performance computing (HPC) community -- software research and development, leading edge scientific applications, leadership-class facilities, major system vendors and relevant government funding agencies -- in a proactive effort to plan and create a new software infrastructure for the extreme scale systems that represent the future of computational science. Through a series of workshops, begun in the spring of 2009 and rotating between the US, Europe, and Asia, the IESP and its international partners have endeavored to provide leadership in formulatung a long-term roadmap to an exascale-capable software infrastructure, and in initializing the corresponding research and development process necessary to produce it. As expressed in the IESP Roadmap version 1.0 (referenced below as \"Roadmap\" and published on May 30, 2010), the project's mission commits to a vision of the future of scientific computing that involves to two key objectives: <br\/><br\/>The guiding purpose of the IESP is to empower ultrahigh resolution and data intensive science and engineering research through the year 2020 by developing a plan for 1) a common, high quality computational environment for peta\/exascale systems and for 2) catalyzing, coordinating, and sustaining the effort of the international open source software community to create that environment as quickly as possible. (Roadmap, p. 3)<br\/><br\/>In a series of workshops, rotating between the United States, Europe and Asia, IESP participants from all segments of the HPC community have begun to forge a working consensus on a plan to address the momentous challenge of creating a software stack for extreme-scale Computational Science, which we call an \"X-stack.\" Building on this success, our proposed IESP workshop for the spring of 2012 in Chicago will focus on further developing and expanding upon the strategies identified in the plan in order to make more rapid progress toward the project's objectives.","title":"Extending the Work of the International Exascale Software Project","awardID":"1136509","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"8004","name":"Software Institutes"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["558550",495745],"PO":["564022"]},"177308":{"abstract":"This project brings together four investigators who identified an opportunity to bridge their disparate fields (gait analysis, body sensor networks, low power RF circuits, and gerontology) in order to address an extremely important health care problem of fall prevention. Their combined expertise addresses the following scientific objectives: (1) explore on-node signal processing and low power RF transceivers to balance energy consumption and mobility analysis fidelity in wireless on-body inertial sensing, (2) identify the minimal set of on-body sensor locations to provide high-quality mobility analysis, (3) determine the appropriate algorithms to identify fall prone individuals in controlled in- and out-of-lab data collections to assess the external influences of context and activities, and (4) validate the TEMPO-based data collection and processing algorithms in a naturalistic HFC living environment.<br\/><br\/>This project enables a new paradigm in elderly patient care to target fall prevention interventions to high fall risk individuals before fall incidents occur, preserving patient's health and quality of life while reducing health care costs. In addition, the above described fundamental scientific objectives apply in the broader application of energy efficient BSNs in a variety of application domains. The education and outreach plans focus on crossing the disciplinary divide between technology and health care, using hands-on public demonstrations, undergraduate laboratory modules, and multi-disciplinary videoconferencing graduate seminar courses to attract and develop leaders who can address the tremendous challenges that face the health care industry in the coming decades.","title":"SHB: Medium: Collaborative Research: Non-Intrusive Multi-Patient Fall-Risk Monitoring in Health Care Facilities","awardID":"1065262","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["527785"],"PO":["564768"]},"181620":{"abstract":"Software design sketching refers to the practice of software designers to, when faced with a design problem, not immediately turn to existing modeling tools, instead choosing to use pen-and-paper or the whiteboard to work through the problem and come up with a solution. This research explores modern hardware such as electronic whiteboards, tablets, and their latest incarnations, iPads and Slates, in creating a novel, computer-supported software design sketching environment that significantly enhances the software design sketching experience.<br\/><br\/>The work consists of two interwoven strands. One strand involves field studies of professional software designers at three different companies working on real design problems in their native environments. Findings from these studies anchor the work in the second strand, which involves the design, development, and evaluation of Calico, a design sketching environment centering on five distinct features: (1) fluid manipulation of sketched content through carefully designed gestures, (2) refinement from rough sketches to precise models through scraps and softmodes, (3) approximate analysis of incomplete and imprecise models via annotations and imprecise languages, (4) abstraction from code to sketch through contextualized sketching, and (5) access to and reflection upon design history through intentional interfaces and design history views.<br\/><br\/>The techniques and tools developed by this project are intended to transform how software designers work and improve the quality of the resulting software-intensive systems, with impact in both in industry and in education.","title":"SHF: HCC: Small: Software Design Sketching","awardID":"1118052","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["551130"],"PO":["564388"]},"180531":{"abstract":"Most computers today are 'multicore' parallel computers that are capable of executing several independent threads of computation simultaneously. Unfortunately, most existing programs are not parallel and cannot take advantage of this hardware capability. Furthermore, writing parallel programs using current notations like OpenMP is more difficult than writing sequential programs and, as a result, increases development costs and the likelihood of program defects.<br\/><br\/>The Kali project is building a software system that will permit most application programmers to write sequential programs and still obtain good performance on multicore processors. Parallelism will be hidden within object-oriented class libraries written by expert parallel programmers and it will be managed by a sophisticated runtime system that uses a range of parallel execution strategies customized to the needs of the application. Applications programmers can take advantage of the benefits of sequential programming such as familiarity, readability, maintainability, and debuggability. They will also be able to tune program performance and power without having to drop down to a lower abstraction level. In addition, the Kali project is studying the use of innovative hardware to facilitate the development of efficient programs. Finally, the project is producing a suite of application benchmarks that will be useful for performance evaluation of similar systems.","title":"CSR: Large: Collaborative Research: Kali: A System for Sequential Programming of Multicore Processors","awardID":"1111766","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":["556730","535119"],"PO":["551712"]},"181510":{"abstract":"Designing analog integrated circuits is more of an art than a science. Their continuous nature makes them difficult to both verify to be correct before fabrication as well as difficult to test to be free of faults after fabrication. This fact leads to design errors forcing costly re-spins (repetitions of the design and fabrication process) and even worse faulty parts being shipped to customers. In an attempt to address this problem, engineers are exploring the use of digitally-intensive analog circuits. In these circuits, designers use the simpler 0-1 binary digital assumption for the majority of the implementation, and they only use analog components when absolutely essentially. While this has some advantages, it creates new challenges as traditional verification and test methodologies for digital and analog design are extremely different. The project is attempting to address these challenges. In particular, the project is exploring the integration of both design-time verification as well as new built-in self-test and tuning techniques. This integration will allow not only joint enhancement of design correctness and robustness, hence a holistic guarantee of design quality, but also verification of self healing analog systems with built-in digitally-assisted test and tuning.<br\/><br\/>The broader impact of this work is that it will enable the design of nanoscale robust computing systems vital to a wide range of applications. Also, interdisciplinary explorations will provide new opportunities for solving research problems of practical significance and offer educational opportunities to make students well grounded in both theory and application. The PIs will promote the research participation from undergraduate students and students from underrepresented groups. The research outcomes of this work will be integrated into undergraduate and graduate curriculum and widely disseminated in the research community. The developed software computer-aided design tools will be released in the public domain.","title":"SHF: Small: Collaborative Research: Integrated Verification, Built-in Self-test, and Tuning for Digitally-Intensive Analog Systems","awardID":"1117515","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["448038"],"PO":["562984"]},"181400":{"abstract":"High-confidence computer systems are those that require a high level of assurance of correct operation.<br\/>Many of these systems are interactive - they interact with a human being - and the human operator's role is central to the operation of the system. Examples of such systems include fly-by-wire aircraft control systems (interacting with a pilot), drive-by-wire automobile systems (interacting with a driver), medical devices (interacting with a doctor), and electronic voting machines (interacting with a voter). The costs of incorrect operation in all such systems can be very severe. It is essential to develop techniques to ensure correct operation of such systems. However, this is very challenging due in part to the difficulty of formally specifying all parts of an interactive system.<br\/><br\/>This project is developing an approach for the principled design of high-confidence interactive systems where correctness is certified through a combination of formal verification and testing by humans. The approach has three components. First, systems are designed according to a set of guiding principles that ease verification and testing, including determinism, independence, and unambiguity. Second, new algorithmic techniques are being developed to perform formal verification of the above determinism, independence, and unambiguity properties on system designs. Finally, the above design principles and formal verification are leveraged to test the human-computer interface with a tractable number of tests. The approach is applicable to a range of high-confidence interactive systems, including avionics, medical devices, and electronic voting systems.<br\/><br\/>Human\/operator error is one of the major sources of failures in high-confidence systems, and this project seeks to reduce the occurrence of such failures through a tight integration of principled design, formal verification, and systematic testing. The approach is being integrated into the curriculum and projects in undergraduate and graduate courses at UC Berkeley.","title":"SHF: CSR: Small: Integrated Design and Verification of High-Confidence Interactive Systems","awardID":"1116993","effectiveDate":"2011-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["497138"],"PO":["565264"]},"181521":{"abstract":"This project applies tools from homomorphic encryption to improve security in cloud services. Cloud computing saves money for companies by allowing them to pay for services only if they need them and just at the time that such services are needed, rather than purchasing and maintaining hardware\/software that may see little use. The drive to gain these savings may transform both business and personal computing. This new level of resource sharing and complexity naturally generates new problems related to security and privacy, problems that are both novel and challenging. <br\/>In the broadest sense, homomorphic encryption allows one to borrow computational power from an untrusted source. If made practical, this has especial implications for cloud computing. The software owner may employ homomorphic encryption in this setting to reduce the threat of reverse engineering and to administer software licenses. When made available over a untrusted cloud, the software is run homomorphically, data is submitted and returned in encrypted form, and the execution of instances is highly robust against malicious users. This project removes the uncertainties in the implementation efficiency of homomorphic encryption schemes and improves it to the point where it becomes practical for use in cloud computing. Specifically, the project centers on three modules: instruction set development for homomorphic computing, processor-specific optimizations for homomorphic schemes, and the investigation of new homomorphic schemes.","title":"TC: Small: Homomorphic Encryption for Cloud Privacy","awardID":"1117590","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["564461","550273"],"PO":["565239"]},"180432":{"abstract":"This research involves collaboration among investigators at three institutions. The PIs anticipate a future in which humans and intelligent robots will collaborate on shared tasks. To achieve this vision, a robot must have sufficiently rich knowledge of the task domain and that knowledge must be usable in ways that support effective communication between a human and the robot. Navigational space is one of the few task domains where the structure of the knowledge is sufficiently well understood for a physically-embodied robot agent to be a useful collaborator, meeting genuine human needs. In this project, the PIs will develop and evaluate an intelligent robot capable of being genuinely useful to a human, and capable of natural dialog with a human about their shared task.<br\/><br\/>The Hybrid Spatial Semantic Hierarchy (HSSH) is a human-inspired multi-ontology representation for knowledge of navigational space. The spatial representations in the HSSH provide for efficient incremental learning, graceful degradation under resource limitations, and natural interfaces for different kinds of human-robot interactions. Speech is a natural though demanding way to use natural language to communicate with a robot. To maintain real-time performance, natural language understanding must be organized to minimize the amount of backtracking from early conclusions in light of later information. This project will answer three scientific questions.<br\/><br\/>(1) Can the HSSH framework, extended with real-time computer vision, express the kinds of knowledge of natural human environments that are relevant to navigation tasks? <br\/>(2) Can the HSSH representation support effective natural language communication in the spatial navigation domain? <br\/>3) Can we develop effective human-robot interaction that meets the needs of a person and improves the performance of the system?<br\/><br\/>To these ends, the PIs will perform this research with two different kinds of navigational robots, each learning from its travel experiences and building an increasingly sophisticated cognitive map: an intelligent robotic wheelchair which carries its human driver to desired destinations, and a telepresence robot that transmits its perceptions to a remote human driver as it navigates within an environment so the driver can achieve virtual presence and communicate with others remotely. To inform the design process, the PIs will conduct focus groups with potential users. They will also evaluate their implemented systems throughout the process, creating an iterative design-test cycle.<br\/><br\/>Broader Impacts: To be successful, an intelligent robot must not only be able to perceive the world, represent what it learns, make useful inferences and plans, and act effectively. It must also be able to communicate effectively with other agents, and particularly with people. This confluence among grounded knowledge representation, situated natural language understanding, and human-robot interaction is intellectually fundamental, and is the focus of this research. Since the domain of spatial knowledge is foundational for virtually all aspects of human knowledge, project outcomes will have broad applicability. This work will create technologies for mobility assistance for people with disabilities in perception (blindness or low vision), cognition (developmental delay or dementia), or general frailty (old age). It will also support telepresence applications such as telecommuting, telemedicine and search and rescue. The project includes outreach to K-12 and community college students, K-12 teachers, and the public in a number of venues.","title":"HCC: Large: Collaborative Research: Human-Robot Dialog for Collaborative Navigation Tasks","awardID":"1111323","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["549200"],"PO":["565227"]},"181532":{"abstract":"The long-term goals of this project are to bring the technology of<br\/>software contracts to widely-used programming languages and, through<br\/>the use of manifest contracts, to provide software developers with a<br\/>migration path from simply typed code to fully functional correctness.<br\/>Since computational effects are notoriously hard to reason about and<br\/>pervade even the simplest realistic programs, the proposed research<br\/>should have significant impact on programmers' ability to develop<br\/>software that is more reliable and more secure. <br\/><br\/>Contracts in software establish clear interfaces between program<br\/>components. Like contracts in the legal realm, they delineate each<br\/>party's expectations and obligations. Such contracts are becoming<br\/>increasingly important for the regulation of modern software systems,<br\/>providing an expressive framework for verification and error tracking.<br\/>To be effective in a software environment, contracts must have formal<br\/>semantics and must be supported by a monitoring system that precisely<br\/>tracks the flow of values as they cross interfaces. To date, however,<br\/>the formal study of contracts has mostly been limited to small<br\/>idealized languages without computational effects, such as reading<br\/>data from or writing data to a display or file, managing resources<br\/>such as memory, and performing probabilistic or speculative<br\/>computation.<br\/><br\/>This research aims to extend the semantic framework of software<br\/>contracts to languages with various computational effects: the<br\/>extension is qualitative in nature and will enable the use of<br\/>contracts in new application domains. Specifically, the PIs propose<br\/>to add support for computational effects to the two flavors of<br\/>contracts studied to date: latent contracts, which are runtime checks<br\/>not reflected in the type system, and manifest contracts, where a<br\/>system of precise types records the most recent runtime check applied<br\/>to each value. The extension of latent contracts will be done in the<br\/>context of a monadic met language. The extension of manifest<br\/>contracts will make use of a variant of Hoare Type Theory to precisely<br\/>record computational effects. The PIs will also implement prototype<br\/>systems and use them to present novel applications of software<br\/>contracts.","title":"SHF: Small: Effectful Software Contracts","awardID":"1117635","effectiveDate":"2011-08-01","expirationDate":"2011-11-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["517796","509997"],"PO":["523800"]},"180443":{"abstract":"This project will create and demonstrate experimentally key features of a new optical network architecture, 'HyperFlow' which is a hybrid future Internet architecture that include key designs for both hardware and algorithms. The network is designed to be dynamic (both agile and adaptive), and significantly more cost effective and power efficient for future growth in data volumes and number of users. The architecture relies on a novel optical network infrastructure comprising new transport mechanisms and a new comprehensive control mechanism including the physical hardware, algorithms and applications. <br\/><br\/>Between locations that exchange large volumes of data - say, Los Angeles and New York City ? the flow switching mechanism of HyperFlow would establish a dedicated path across the network. The allotment of bandwidth would change constantly. As traffic between New York and Los Angeles increased, new, dedicated wavelengths would be recruited to handle it; as the traffic tailed off, the wavelengths would be relinquished. The goal is to develop network management protocols that can perform new session allocations in a matter of sub-seconds. HyperFlow can easily increase the data rates of optical networks 100-fold with the new network management scheme to be addressed in this program, with corresponding decrease in cost per bit and power consumption as well.<br\/><br\/>Intellectual Merit: HyperFlow creates a complete hybrid flow\/IP network architecture spanning local to wide areas, sharing the same unified control plane. The network provides both guaranteed end-to-end multi-Gbps flow service and conventional Internet Packet services throughout the network down to individual users. To achieve that goal, HyperFlow includes novel access and long haul core network technologies designed to support end-to-end flows as well as conventional IP network services. This approach achieves a breakthrough with respect to previous approaches by providing both services end-to-end with a unified control plane. HyperFlow includes major innovations based on highly efficient, agile and control of pools of shared tunable lasers and passive optical devices, a fast MAC, partitioned routing strategy and a new transport protocol for flows. <br\/><br\/>Broader impact: If successful the proposed HyperFlow technology will have a profound influence on the way the future Internet is designed and perceived by the society, as the new on-demand cost-effective gigabit service will enable applications that are hard or costly to support today with existing broadband access, such as instantly-available 3D distributed virtual-reality systems, cloud computing and instant file transfer services including telemedicine, education, 3D movies, concerts, sporting events, and government services. The concept of the new Hyperflow development will be brought into industry forums such as IETF\/IRTF during the project, so that the best minds of the industry can be leveraged for conceptual validation.","title":"NeTS: Large: Collaborative Reseaarch: HyperFlow:-A Hybrid IP\/Optical Flow Network Architecture","awardID":"1111374","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[483588],"PO":["564993"]},"181301":{"abstract":"Today, practically all processors employ instruction set architectures which are functionally<br\/>equivalent to each other. Compiler\/micro-architecture cooperation using these traditional<br\/>representations has already reached the point of diminishing returns. This project therefore<br\/>investigates the domain of single assignment program representations and direct support of this<br\/>domain through micro-architecture implementation as a key concept that can break the barriers<br\/>between the compilers and architectures. If successful, this new approach can have a significant<br\/>impact on the design of future processors, design of compiler internal representations as well as<br\/>the back-end of the compilers. It can also affect how parallelism is exploited at various<br\/>granularities and how various optimizations are carried out. The investigated framework can help<br\/>revitalize computer architecture and compiler optimization research by opening up unexplored paths<br\/>for research in high-performance systems. Consequently, it can affect every field of science and<br\/>commerce which rely on high-performance computation. <br\/><br\/>Compiler\/hardware integration around the concept of single-assignment form has many benefits<br\/>spanning three fields. First, in the area of process synchronization it provides the opportunity to<br\/>eliminate the need for explicit synchronization. Second, in the field of micro-architecture renaming<br\/>of instruction streams becomes substantially simpler, micro-architectures can become loop-aware,<br\/>renamed instruction streams can be re-renamed and compiler techniques such as partial redundancy<br\/>elimination or constant propagation can dynamically be performed by the micro-architecture.<br\/>Finally, the compilers can focus on what they do best by sharing a common representation with the<br\/>micro-architecture. As a result, many key optimizations can be efficiently performed. Within<br\/>this paradigm, it becomes possible to develop new optimization algorithms which will rely on the<br\/>micro-architecture to perform the optimization using analysis performed by the compilers.","title":"SHF: Small: Single Assignment Architecture \/ Single Assignment Compiler","awardID":"1116551","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[485798],"PO":["565272"]},"181422":{"abstract":"This project represents an ongoing collaboration between teams at two institutions. As people live longer, blindness caused by degenerative diseases of the retina such as macular degeneration or retinitis pigmentosa is today a major disability among the aging in the developed world. These types of \"neural\" blindness cannot currently be medically treated in any satisfactory manner. There is now compelling experimental evidence in humans that even when such diseases cause a loss of photoreceptors (i.e., rod and cone cells in the retina), electrical stimulation of the remaining retinal neurons that survive this loss can be used to bypass the damaged tissue and deliver visual information to the brain. This is essentially the same concept that supported the development of the cochlear prosthesis, which has been a fabulous success, restoring hearing to many tens of thousands of deaf patients. The PIs and their respective teams have been working for over 20 years toward the goal of developing a retinal prosthesis to restore truly useful vision to patients in an analogous manner. With prior funding from a number of agencies including NSF, they have created enabling technology for a miniaturized high-density implantable wirelessly-driven neuro-prosthesis package with over 200 individually-addressable channels, which is over three times the inputs and outputs in any current commercially available neurostimulator. The field's ability to create complex integrated circuitry for neurostimulation and\/or recording has outpaced the development of long-term implantable packaging, microelectrode array, and assembly technology. If optimized, those technologies would make possible new devices that interface with hundreds of neural tissue sites simultaneously. This is the PI's aim in the current project. The funding will complement existing grants to the PIs and their collaborators, and will allow them to complete development of a new 200+ channel co-fired ceramic signal feed-through disc, to optimize the micro-fabrication process for high-density microelectrode arrays that interface with neural tissue, and to improve the bonding and interconnection processes required to assemble the implant package. <br\/><br\/>Broader Impacts: The 200+ channel wirelessly-driven implant that will constitute the primary project outcome will have over three times the number of individually-addressable stimulating electrodes now available from any group. This funding will further allow the PI to ready devices for later pre-clinical testing (with anticipated follow-on support from the VA). Project results will be widely disseminated in publications, and by distributing sample devices within the rehabilitation R&D community. The device which is the focus of this project will also be useful in a myriad other future chronically implantable prosthetics, palliative devices, and human-computer interface devices.","title":"HCC: Small: Collaborative Research: Packaging Optimization for Next-Generation Implantable Human-Computer Interface Devices","awardID":"1117093","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[486094],"PO":["565227"]},"181444":{"abstract":"Digital signal processing has been a cornerstone of the modern communications and electronics revolution, transforming application areas such as wired and wireless communications, storage, and biomedical signal processing. This project will study digital signal processing in an entirely new domain: molecular systems. In contrast to electronic systems, where signals are represented by time-varying voltage values, in molecular systems signals are represented by time-varying concentrations of different molecular types, such as proteins, RNA and DNA. <br\/><br\/>This project will develop, implement, and evaluate molecular-level designs for a variety of digital signal processing operations such as filtering, equalization, and noise cancellation. These operations will be synchronized by clock signals, created through sustained chemical oscillations. Memory will be created by transferring signals between different molecular types in alternating phases of the clock. The key idea underpinning this research is that the computation should be essentially rate-independent: it should only depend on coarse categories for the rates of the chemical reactions (e.g., ?fast? vs. ?slow?). It should not matter how fast any ?fast? reaction is ? only that ?fast? reactions are fast relative to ?slow? reactions. Designs with this property can be mapped to different chemical substrates. They compute accurately in spite of variations in environmental conditions such as temperature. <br\/><br\/>The impetus for this work is not computation per se; chemical systems will never be useful for number crunching. Rather the field of molecular computing aims for the design of custom, embedded biological ?sensors? and ?controllers? ? viruses and bacteria that are engineered to perform useful tasks in situ, such as cancer detection and drug therapy. As an experimental chassis, this project will map designs for digital signal processing operations to chemical reactions involving DNA strands. These designs will be evaluated with computer simulations of the chemical kinetics.<br\/><br\/>Techniques for analyzing the dynamics of biological systems are well established. However, synthesizing computation with such mechanisms requires new techniques ? and an entirely new mindset. The digital circuit design community has unique expertise that can be brought to bear on the challenging design problems encountered in synthetic biology. Applications in biology, in turn, offer a wealth of interesting problems in algorithmic development. With its cross-disciplinary emphasis, this project will bring new perspectives to both fields.<br\/><br\/>If successful, the proposed research will transform disciplines such as genetic engineering of drug-delivery systems. Currently, a costly, ineffective ad-hoc approach prevails. With robust and rate-independent techniques for implementing operations such as digital signal processing, much more effective systems will be developed. An important goal of the project is to communicate the impetus for interdisciplinary research to a wide audience. A new course will be developed, titled \"Circuits, Computation, and Biology\" offered jointly through the Electrical Engineering Department and the Biomedical Informatics and Computational Biology Program at the University of Minnesota. Building upon current recruitment efforts that have brought in female students, students from underrepresented groups will be recruited into the project.","title":"SHF: Small :Digital Signal Processing with Biomolecular Reactions","awardID":"1117168","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["550262","528637"],"PO":["565223"]},"181213":{"abstract":"Encoding of emotions in speech is achieved by vocal modulations that require an intricate control of human voicing and vocal tract articulation. The aim of this research is to identify and model articulatory processes for emotional speech production based on advanced speech production data acquisition technologies including Electromagnetic articulography (EMA) and a real-time Magnetic resonance imaging (rt-MRI). The research focuses on directly measuring and modeling articulatory kinematics and their interplay with prosodic modulation of pitch, loudness and segmental durations in speech emotion expression in order to understand the emotional speech production strategies across emotion types as well as across speakers. The validity of the emotional speech production models is verified by using a software articulatory synthesizer in an analysis-by-synthesis fashion. Theoretical implications of the findings are interpreted in relation to the Hyper and Hypo theory and the Converter\/Distributor (C\/D) model of speech production.<br\/><br\/>Detailed knowledge on the effects of emotion on the human speech articulatory and prosodic patterning has transformative potential in developing improved speech processing technologies for emotional speech recognition and synthesis that are critical for the development of natural and robust man-machine interfaces. This goal also critically includes informing quantitative assessment of expressive speech to characterize atypical or distressed vocal behavior in diverse populations, for instance, children with Autism Spectrum Disorder (ASD). Finally, a natural by-product of this research effort is the unique articulatory database that will be shared freely with the community for further expansion of the knowledge of human speech production.","title":"RI: Small: Emotional Speech Production: Analysis, Modeling and Synthesis","awardID":"1116076","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["515830",485589],"PO":["565215"]},"181334":{"abstract":"This is a collaborative project leveraging expertise of Ashok Veeraraghavan, William Marsh Rice University (IIS-1116718) and Ramesh Raskar, Massachusetts Institute of Technology (IIS-1116452). Imaging and display devices are all around us and are used in a variety of applications. The spatial resolution, depth range, depth resolution, temporal resolution, frame-rate and bandwidth of these devices are usually fixed a priori. When the resolution and other properties of the content being imaged or displayed does not exactly mimic those that were assumed a priori, this leads to inefficiencies (in utilizing available resources) and undesirable artifacts (aliasing, blurring and noise). Since both imaging and display devices are fast becoming multi-purpose, there is a need to develop imaging and display architectures (and algorithms) that are capable of adapting their resolution and bandwidth characteristics to match those of the content.<br\/><br\/>The goal of this project is to develop imaging and display devices that adapt to scene, motion, geometry, viewer, or illumination conditions. Such adaptive devices lead to performance improvements and novel capabilities hitherto unexplored. This research agenda is organized into four intellectual thrusts: (1) the establishment a theoretical framework for Adaptive Coded Imaging and Displays (AdaCID) that enables efficient exploration of the space of designs (2) the design of adaptive coded imaging systems that adapt to scene geometry, motion, and illumination (3) the design of adaptive and interactive coded 2D\/3D displays that adapt in real-time to content, viewer position, and the human visual system enhancing visual appearance and allowing intuitive 3D interaction and (4) the demonstration of coded feedback projector-camera systems enabling rapid acquisition of range and material characteristics.<br\/><br\/>It is expected that AdaCID will have far-reaching impact to diverse applications spanning consumer imaging and displays, machine vision and automation, scientific\/medical imaging and displays and surveillance. Since AdaCID and the broader field of computational imaging and displays is increasingly important, they will be integrated into various courses offered at Rice University and MIT. Broad dissemination of the educational material will be achieved through participation in the free, open-licensed Connexions program and OpenCourseWare and in public-domain museum initiatives (at the MIT Museum). This project also offers collaborative research opportunities for students at the two institutions. Project Website (http:\/\/cameraculture.media.mit.edu\/AdaCID\/) provides additional information.","title":"CGV: Small: Collaborative Research: AdaCID: Adaptive Coded Imaging and Displays","awardID":"1116718","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["557517"],"PO":["563751"]},"180487":{"abstract":"Applications of wireless sensor nodes are evolving at a previously unimaginable rate. But current technology is limited because devices are bulky - measuring one cubic centimeter or more - and hampered by short lifetimes. This project is producing a one cubic millimeter sensor node. This ultra-miniaturized device is a complete sensing platform that includes transducers (for imaging, temperature sensing and other signal detection), wireless communication, a high accuracy timer, processor, memory, a battery and energy harvesting that provides the node with an extended lifetime.<br\/><br\/>The central challenge in reducing the form factor for sensor nodes is to reduce power consumption and <br\/>densely package discrete components (crystals, inductors, etc.). To this end, this team's innovations involve research and development of:<br\/><br\/>1. A novel processor that operates at a supply voltage near the threshold voltage of the transistors for optimal energy consumption.<br\/>2. A new ultra-low-leakage memory system.<br\/>3. An Ultra Wide-Band (UWB) transmitter and receiver that can communicate with other nodes over a distance of three meters with an integrated antenna. <br\/>4. A 100pW timer that is temperature compensated and designed for reduced jitter to allow accurate synchronization between sensor nodes and enable short, low energy radio communication windows.<br\/>5. A new CMOS imaging approach capable of ultra-low power motion detection and image-acquisition, and, reconfigurable to act as a solar energy harvesting unit. <br\/>6. An energy-aware software development environment to control the node<br\/><br\/>These PIs implemented early versions of several of these technologies in silicon, demonstrating the potential to package them as sensor nodes. The team's track record of producing ultra-low power circuits, and other sensing components, position them to deliver the needed 1000\u00d7 form factor reduction. This research team will assemble and package 100 first- and second-generation of these sensor node platforms and disseminate them to the broader community for trials in a wide range of uses. <br\/><br\/>The development of cubic-millimeter sensor nodes will enable applications that have long been envisioned but were unachievable. For example, sensory skins could cover surfaces with a dense deployment of nodes that monitor the properties of the manifold itself or its surroundings. Implantable intelligence can enable deeply embedded physical and biological processes, e.g., malignant tumor growth monitoring or intra-ocular pressure sensing to determine the risk for retinal detachment. Applications such as these, and a myriad of other \"Thinking and Linking\" applications, can give everyday objects sensing, computing, communication, and tracking ability, allowing, for example, research ranging from the social network patterns of small insects to asset tracking in dynamic environments like hospitals. By shrinking sensor node size to one cubic millimeter, with potentially perpetual lifetime, the concept of \"smart dust\" can be taken from fiction to reality.<br\/><br\/>By disseminating the first generation of these sensors to members of the sensor network community, this project will dramatically accelerate the adoption of cubic-millimeter-class computing devices. This will have immediate impact on a wide array of research programs for intelligently sensing, tracking, measuring and optimizing physical processes. This research in turn will have a fundamental and long term impact on a diverse set of applications with critical societal import, ranging from energy conservation, environmental quality management, and health care.","title":"CSR: Large: Collaborative Research: Integrating Circuits, Sensing, and Software to Realize the Cubic-mm Computing Class","awardID":"1111541","effectiveDate":"2011-08-15","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0809","name":"Division of INTEGRATIVE ORGANISMAL SYS","abbr":"IOS"},"pgm":{"id":"7658","name":"PROCESSES STRUCS & INTEGRITY"}}],"PIcoPI":["534190","517834","517835","562959"],"PO":["551712"]},"185943":{"abstract":"This exploratory project (EMERG) is aimed at developing the capability to predict emerging topics in science from a highly detailed global model of the scientific literature. Identification of emergent opportunities in science is a central issue in academia and practice. Applications range from a simple understanding of the broader context in which individual research is conducted to the direction of research funds toward emerging topics. Previous studies of emergence have had the following shortcomings: they are retrospective (the area of emergence is identified after the fact), narrowly defined (lacking the context of related scientific topics) and\/or highly aggregated (field level rather than topic level).<br\/><br\/>The approach is based on a highly detailed global model of science, consisting of hundreds of thousands of micro-communities over a period of nine years (2000-2008). The average size of these micro-communities is 15 papers per year. Micro-communities are linked from year to year using co-citation methods. Some micro-communities are part of long thread-like structures while others may be isolated. At the micro-structure level, science appears to have a high level of discontinuity. The mixture of continuity and discontinuity makes it possible to see emergence at the topical level. A variety of indicators, some structural, some based on the micro-community contents (articles, authors, ages, etc.), and some based on full text analysis, are calculated for each micro-community. The hypothesis is that several, if not all, of the proposed indicators will correlate with emergence. <br\/><br\/>To test this hypothesis, a data from research funding agencies and foundations that identified emergent micro-communities will be collected, together with identifying and tracking the key articles responsible for emergence in those areas. This history will be compared with the results of indicators from the model of science. If successful, the indicators can be applied to a current (rather than retrospective) model, suggesting the particular current micro-communities in science that are emerging or that are likely to emerge in the next year or two.<br\/><br\/>This project provides a completely new method for developing useful knowledge about the micro-structure and dynamics of science and technology from literature databases, whether of scientific literature, patent, grant, or web resources. This work has the potential to transform the way the structure and dynamics of science and technology are understood, and to impact conduct and management of research at the scientists, students, general public and policy maker levels. Project results will be disseminated via web site (http:\/\/www.mapofscience.com\/emerg.html) and publications.","title":"EAGER: Identifying Emergent Opportunities in Science","awardID":"1142795","effectiveDate":"2011-08-01","expirationDate":"2013-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[499034],"PO":["563751"]},"181224":{"abstract":"The past decade has seen an explosion of interest in so-called \"dynamic\" or \"scripting\" programming languages, which emphasize programmer productivity at the possible expense of run-time performance. Among other applications, scripting languages are central to much web-based and mobile computing. With increasing use, and with the proliferation of multicore processors, there will be inevitable pressure to improve the performance of these languages through parallel execution. Unfortunately, the state of the art in parallel scripting has not kept pace with parallel architecture developments. While many scripting languages support asynchronous handling of events from the external world, programmer-centric features, like dynamic typing, make it very difficult for event handlers and the main program<br\/>-- or multiple aspects of the main program -- to execute efficiently and simultaneously on separate cores of a modern processor.<br\/><br\/>The goal of this project is to build a detailed understanding of the tradeoffs between scripting language design and the performance of parallel execution. This goal is accomplished through two main tasks: (1) minimizing the overhead necessary to safeguard the language implementation in the face of parallel execution, and (2) quantifying the marginal cost of different models of data sharing and memory consistency. The bulk of the work takes place in the Ruby scripting language, widely used for Internet server development. This approach will leverage recent developments in transactional memory, read-mostly synchronization, and high-performance data structures.<br\/><br\/>This project will contribute directly to the training of students at both the graduate and undergraduate level, and to curricula for courses at both the advanced and introductory level. More broadly, effective support for parallelism in mainstream scripting languages can be expected to produce significant improvements in productivity across the full range of computer applications, in government, industry, science, the arts, and entertainment.","title":"CSR:Small:Parallelism and Concurrency in Scripting Languages","awardID":"1116109","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["564815"],"PO":["551712"]},"185712":{"abstract":"Collaborative Projects: EAGER: A virtual eXchange to support networks of creativity and innovation amongst Science, Engineering, Arts and Design (XSEAD)<br\/><br\/>Intellectual Merit<br\/>One of the greatest challenges facing the United States in research and education is how to fundamentally encourage innovation across all sectors and spawn new solutions to address global challenges. Increasing research evidence and industrial innovations (i.e. mobile computing, social media) confirm that broad interdisciplinary collaborations that include both science and art fields have great potential for spawning creativity and innovation in computer science, engineering and the sciences. An emerging hybrid community of scientists, engineers, artists and designers is producing innovative and entrepreneurial research that advances new knowledge and proposes holistic solutions to societal challenges including health, education and environmental change. Yet, this burgeoning interdisciplinary community continues to face problems in its efforts to self-organize among constraints imposed by academic systems and historical biases; it continues to seek a dynamic and synergizing research and outreach exchange.<br\/><br\/>Building upon lessons-learned, a new Virtual eXchange to support networks of creativity and innovation amongst Science, Engineering, Art and Design (XSEAD) will be developed. The XSEAD project will address the following urgent needs of the interdisciplinary science-art community: establish a cohesive view of the field and provide a mechanism to attract entrepreneurs and industry; create a venue for multimodal documentation of research outcomes; provide extensive databases of prior and current research; allow rapid dissemination of research outcomes; facilitate forming of collaborations and specialized sub-communities; document and help evolve science-art curricula efforts and evaluation approaches; provide context and support mechanisms for science-arts careers; establish evidence of the societal impact of interdisciplinary science-art integration. The software engineering development components of XSEAD will contribute further knowledge in three technical areas: Content organization (improve the effectiveness of algorithms for dynamic, usage based, organization of large multimedia databases); Recommendation algorithms (promote the use of multi-relational structures for providing effective recommendations); Community dynamics (develop novel algorithms to extract structures that encode meaningful interactions in online social networks).<br\/><br\/>Broader Impact<br\/>XSEAD will expose general non-expert audiences to the evolution and potential of collaborative research across science and arts. It will attract the interest of young people searching for careers that combine the rigor of science and engineering with the creativity and reflection of arts and design. It will serve teachers and informal learning communities seeking exemplars for curricular development, active practitioners looking for further institutional opportunities to present and support their ongoing work, academics developing related interdisciplinary efforts and commercial companies seeking cross-trained expertise. XSEAD will enable rapid research exchange and in-depth peer-reviewed scholarship between the worlds of science and art and provide a unique and deeply engaging inroad to a vast and creative repository. XSEAD will help promote new paradigms for developing human centric solutions to complex societal problems (i.e. cost effective health and wellness, globalization and conflict, adaptive K-12 learning, electronic communication and security). These paradigms will combine knowledge across broad and diverse areas of human knowledge.","title":"Collaborative Research: EAGER: A Virtual eXchange to Support Networks of Creativity and Innovation Amongst Science, Engineering, Arts and Design (XSEAD)","awardID":"1141631","effectiveDate":"2011-08-01","expirationDate":"2013-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["551811","563297"],"PO":["565342"]},"185954":{"abstract":"The challenges of Exascale computing suggest that a new model of computation is needed upon<br\/>which a new framework for mass storage may be built. Such a model currently under development is the<br\/>ParalleX model. ParalleX addresses Exascale parallelism by removing most (if not all) global<br\/>synchronization and providing a natural means for programs to manage local dependencies that maintain<br\/>consistency. In a similar manner, we propose to develop a model of parallel storage that achieves the<br\/>desired semantics without global synchronization. At the same time, the model will adopt a new<br\/>programmer interface that reflects this approach. This I\/O model is named PXFS and is intended to be<br\/>integrated with the ParalleX model of computation, both by providing an I\/O model for ParalleX and by<br\/>being defined in terms of ParalleX.<br\/>The goals of the proposed research are to derive a new model of persistent mass storage that<br\/>unifies it with active in-memory data and develop PXFS, a proof-of-concept file system to enable<br\/>effective and scalable Exascale computing. The objectives of the proposed project are to describe the<br\/>PXFS model in complete terms, develop an initial reference implementation integrated with the HPX<br\/>implementation of ParalleX, and evaluate the model via the reference implementation.","title":"EAGER: PXFS - A Persistent Storage Model for Extreme Scale","awardID":"1142905","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["521239"],"PO":["565272"]},"184865":{"abstract":"The Nation?s universities and industrial research labs are facing continuing budgetary pressures as a result of the economic downturn. The aim of this project is to forestall a permanent loss of research talent that is likely to occur if new computer science PhDs are forced to seek employment outside the field due to hiring reductions brought about by the current economic situation. The NSF Computing Innovation Fellows (CIFellows) project will enable approximately twenty new PhDs in the computing fields to develop the additional credentials and experience to make them more effective researchers and\/or teachers. This is the third year of the CIFellows program, which has already supported a total of 107 Fellows in one- to two-year positions at academic institutions and industrial labs across the country. The selection process will actively seek applications from women and people from underrepresented groups; it will also identify candidates for awards from a cross-section of institutions. As a result, a broad range of institutions will strengthen their research groups and labs, thereby, improving the national infrastructure and research workforce in computer science, allied fields and corresponding application domains.","title":"The Third Computing Innovation Fellows Project","awardID":"1136996","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[495943,"209556","557660","562011","531055","530092"],"PO":["565264"]},"181114":{"abstract":"Wireless communication networks continue to get more complex. Communication over unused TV bands (cognitive radios), self-organizing ad-hoc networks for military and emergency applications, or the heterogeneous networks envisioned in the next generation cellular (LTE advanced) systems all are clear evidence of this trend. As consumer demands, dependence on wireless services, and the complexity of networks continue to grow, there is a need to manage the resources efficiently without overburdening the network owner and end user. To address these challenges, this research involves significant and novel enhancements to the building blocks of modern communication systems. <br\/><br\/>The modeling, feedback, and cognitive techniques being studied in this project are essential for the development of robust and efficient wireless systems. The research project involves the following three parts: 1) Development of novel channel modeling methods that decompose the channel into a specular component and a diffuse component. A key consideration in this work is developing a rigorous framework for channel prediction and utilizing the insight to develop robust feedback based Multiple Input Multiple Output (MIMO) systems that degrade gracefully. 2) The development and analysis of feedback based multi-user MIMO-OFDM systems. This includes novel channel estimation and representations schemes, novel schemes for encoding sparsity, and performance analysis of reduced feedback MIMO-OFDM systems. 3) The development of advanced cognition at the physical layer. This includes waveform design in cognitive radios based on timing consideration and more comprehensive models for channels to provide awareness based on location, learning and memory.","title":"CIF: Small: Novel (Channel Modeling, Feedback, and Cognitive) Approaches in Wireless Communications","awardID":"1115645","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["499664"],"PO":["564924"]},"181246":{"abstract":"This work addresses the capture of concurrency bugs that can occur in parallel software, focusing on four types of concurrency bugs that are frequent, have received little attention, and are hard to fix.<br\/><br\/>The first type is atomicity violations -- bugs that occurs when the programmer fails to enclose in the same critical section all of the memory accesses that should occur atomically. As a result, during execution, such accesses get interleaved with accesses from another thread that make the program state inconsistent.<br\/><br\/>The second type is over-synchronization -- a defect that implies that there are redundant synchronization operations or that the synchronization operations are performed at a grain that is too coarse. As a result, the program's performance is poor.<br\/><br\/>The third type is ordering violations ? these occur when there is a correct order between memory accesses from different threads and, in an execution, such order is flipped, usually due to not using the correct form of synchronization.<br\/><br\/>The final type is asymmetric data races ? bugs that occur when a thread accesses shared variables inside a critical section with appropriate synchronization. However, a second thread concurrently accesses the same shared variables without synchronization, and makes the state inconsistent.<br\/><br\/>The approach taken by this project involves characterizing these bugs and proposing techniques to detect and fix them. A deliverable of this work is a unified framework for bug detection.<br\/><br\/>The realization of parallel programming is a Grand Research Challenge and is crucial to our computer industry's ability to continue to make progress. The University of Illinois is a leader in parallel computing and the outcomes of this proposal will be used to enhance courses on parallel computing. In addition, this project will involve close collaboration with Intel and provide students with the opportunity to engage in internships at Intel.","title":"CSR: Small: A Framework for Advanced Concurrency Debugging","awardID":"1116237","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[485665],"PO":["551712"]},"181367":{"abstract":"This project explores the design of green cellular network architectures and algorithms to reduce energy consumption. A major consumer of energy in cellular networks is the operation of the base stations. Most base stations are deployed and operated continuously based on peak traffic estimates. Intuitively, it saves energy to judiciously \"scale back\" un-utilized and under-utilized base stations during off-peak times. The remaining base stations must adapt accordingly so that the required coverage is maintained.<br\/><br\/>The PIs propose a dynamically adaptive cellular network where the cell size and capacity of the base stations are dynamically reconfigured based on the current user locations and Quality of Service (QoS) demands of the applications. Modeling of the network power consumption based on the network topography, followed by optimization of the topography is the key to this problem. The project will involve student participation at all stages through suitably designed projects. The results of the research will be disseminated through professional journals and conference proceedings and also through industry forums and workshops. Furthermore, the project addresses one of the most pressing issues of today, namely, reducing the carbon footprint of the cellular network. Radical new designs for the network have the potential to spur economic development, create jobs and forge stronger communities.","title":"NeTS: Small : Collaborative Research: Towards a Green Cellular Network through User and Application Aware Dynamic Cell Reconfiguration","awardID":"1116859","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["507608"],"PO":["565303"]},"181378":{"abstract":"Due to the industry-wide shift to multi-core processing, future<br\/>applications will only run faster if they are written as parallel<br\/>software (i.e. where the work for a single program is accomplished by<br\/>simultaneously performing different parts of that work on different<br\/>processors). Unfortunately history has taught us that writing<br\/>parallel software is a very error-prone task for programmers. To<br\/>address this challenge, this research project is creating a powerful<br\/>new mechanism for identifying and hopefully fixing bugs in parallel<br\/>software on large-scale production systems. Hopefully the resulting<br\/>framework will not only support the parallel execution of existing<br\/>monitoring tools, but it will also spawn the creation of new classes<br\/>of powerful tools that can recognize subtle bugs in parallel software.<br\/>Such tools can help programmers continue to reap the performance<br\/>benefits of future microprocessors, thereby continuing to enable all<br\/>of the benefits that increasingly-faster computation has had on the<br\/>economy, science and technology, and our everyday lives in an<br\/>Internet-connected world.<br\/><br\/>More specifically, the focus of this project is on sophisticated<br\/>online program monitoring tools that model various aspects of program<br\/>correctness at an instruction-by-instruction granularity. To strike a<br\/>practical balance between performance, precision, and convenience for<br\/>the tool developer, this research has developed a novel framework that<br\/>introduces explicit \"windows of uncertainty\" combined with a new<br\/>approach for avoiding a state space explosion in the analysis<br\/>(inspired by Tarjan's interval analysis approach to dataflow analysis).","title":"SHF: Small: Adapting Dataflow Analysis for Efficient and Precise Parallel Program Monitoring","awardID":"1116898","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[485986],"PO":["564588"]},"181389":{"abstract":"Blind source separation (BSS) has found wide use in many disciplines including signal processing as it starts from a simple generative model minimizing assumptions on the data generation mechanism and achieves useful decompositions of the observed data. In particular, independent component analysis (ICA) has been the most commonly used approach to achieve BSS since statistical independence of the underlying components is plausible in many applications. Besides independence, sample correlation is another inherent property of many signals of interest. Traditionally, these two properties are addressed separately when developing methods for source separation. Entropy rate, on the other hand, is a natural cost that allows one to account for independence and sample correlation jointly, and hence promises to result in a new class of powerful solutions with wide applicability. In addition, it enables one to easily incorporate model selection---another key problem complementing the power of BSS---into the problem through the use of information theoretic criteria.<br\/><br\/>The focus of this research is the development of a class of powerful methods for source separation and model selection using entropy rate so that one can take both the higher-order-statistical information and sample correlation into account to achieve significant performance gains in more challenging problems. The main application domain is one that can truly take advantage of this fully combined approach: the analysis of functional magnetic resonance (fMRI) data and the rejection of gradient and pulse artifacts in electroencephalography (EEG) in concurrent EEG-fMRI data. Both are applications that have proven challenging for the traditional model-based approach due to the unique nature of the noise and artifacts in these problems. Hence, they provide a unique testbed for the performance evaluation of the new class of methods developed under this study. Since independence and sample correlation are intrinsic properties of many other types of data, the new set of methods will be attractive solutions for many other problems as well.","title":"CIF: Small: Collaborative Research: Entropy Rate for Source Separation and Model Selection: Applications in fMRI and EEG Analysis","awardID":"1116944","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":[486018],"PO":["564898"]},"181048":{"abstract":"Understanding and analyzing the way our world is connected is a critical but new challenge in today's world, thanks to the technological advances of personal computers, mobile devices, as well as local and global Internet connections. Most current methods in the area of social media analysis, inference and understanding are based on textual data. However, the image data makes an increasingly large proportion of data in social media. Hence, there is an urgent need for tools that can effectively use image data to extract important information to infer patterns and activities of people, communities and society at large. <br\/><br\/>This project combines advances in computer vision, machine learning, and social networks in novel ways for understanding and analyzing large-scale social media data. The proposal brings together computer vision and machine learning research in novel ways to develop new methods for analyzing large-scale social media data. It pursues 4 inter-related aims: (i) Establishing a large-scale visual concept ontology and structures for the web-image world via crowdsourcing, taxonomy induction, and nonparametric learning methods; (ii) Understanding activity in social networks by analyzing image contents in the context of social media in large-scale and with connectivity; (iii) Inferring the structure of social networks and communities from image contents and activity of individuals in social networks; (iv) Discovering and analyzing dynamic social media trends. <br\/><br\/>Anticipated products of this research include new tools for analysis and modeling of socially generated content, with special emphasis on image data. The resulting methods provide potentially useful insights that characterize users, communities and societies, in a broad range of applications. The project offers enhanced research-based advanced training opportunities for graduate as well as undergraduate students and involves development of new courses on related topics at both Stanford University and Carnegie Mellon University.","title":"III: Small: Collaborative Research: Using Large-Scale Image Data for Online Social Media Analysis","awardID":"1115313","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["518220"],"PO":["565136"]},"186109":{"abstract":"Variegated and multifaceted shapes fill our world. Currently, there is no universally accepted vocabulary for shape analysis which inhibits the emergence and deployment of shape comparison technologies. This project draws its inspiration from an old idea usually attributed to Christiaan Huygens: when waves (usually represented as complex exponentials) emanate orthogonally from each point on the shape boundary, they travel in open space until they meet other waves emanating from different shape boundary locations. Each point in space is then \"owned\" by a wavefront thereby giving rise to the complex wave representation (CWR) of shape. When shape alignment is required (usually as part of a larger shape recognition goal), the CWRs of the two shapes can be more robustly aligned than the original shapes, since in the CWR, every point in space (in a bounded region) now carries an imprint of the shape. Shape analysis often raises the need for performing shape statistics: shape averages (usually referred to as shape atlases) and deviations from the mean are required. Shape statistics carried out in the CWR are arguably simpler since the vocabulary shifts to averages and standard deviations of frequencies (of the waves in the shape representation) which is rather straightforward. The significance of this work lies in the introduction of complex wave representations into the shape analysis lexicon. This research is expected to impact all areas of shape representation and analysis: the matching, registration, indexing and recognition of shapes with open source code dissemination facilitating re-use and vertical integration.","title":"RI: EAGER: Complex Wave Formulations for Shape Analysis","awardID":"1143963","effectiveDate":"2011-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[499431,499432],"PO":["564316"]},"181511":{"abstract":"The project exploits the sophisticated and robust machinery of bacteria for actuation, sensing, communication, and control of a new class of micron scale robotic systems called BacteriaBots. This is achieved through (1) computational modeling of bio-actuation and sensing, (2) use of quorum sensing-based behaviors, and (3) use of mobile networks of BacteriaBots.<br\/><br\/>This effort contributes to the critical understanding required to advance the science of bio-hybrid micro-robotics and distributed control at reduced length scales. This holds enormous promise for socio-economic benefit by significantly impacting the fields of bio-sensing, medicine, micro-manufacturing and assembly, microelectronics, and bio-materials. A multi-tier education and outreach plan integrates research elements and discoveries into multi-disciplinary educational research experiences for K-12, undergraduate and graduate students.","title":"RI: Small: Distributed Network of BacteriaBots","awardID":"1117519","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[486310,"534227"],"PO":["534411"]},"181632":{"abstract":"All-pairs similarity comparison is one of the core algorithms in many data-intensive mining and search applications such as near duplicate detection among web pages, spam detection, advertisement click analysis, similar news\/fresh content grouping, and recommendation for similar product purchases and search queries. Conducting similarity search on large datasets is time consuming and becomes more challenging when data are being updated continuously. It is important to develop high performance algorithms and software to meet the increasing speed demands in many consumer and business applications using similarity computation. <br\/><br\/>This project studies efficient and cost-effective parallel algorithms when data are being updated periodically or dynamically. Techniques for partitioning data and balancing computation on a cluster of machines are developed to optimize input\/output operations, communication, and computing resource usage. As data are often updated continuously, leveraging previously computed results to handle updated data can eliminate a large amount of unnecessary operations and speedup the entire computation process by an order of magnitude. The project develops efficient software on a cluster of machines. The project starts with incremental duplicate detection for web data analysis and search, and continues to work on similarity comparison in several other applications. Performance of developed software is evaluated in those applications.<br\/><br\/>This research has the potential to develop fully-optimized solutions with significantly reduced cost and increased speed for a variety of big data applications that perform similarity analysis. Developed software will be made available for application developers or data engineers to conduct large-scale computation without involving the complexity of managing parallelism. The project web site (http:\/\/www.cs.ucsb.edu\/projects\/psc\/) is used for dissemination of results. The educational plan contains research mentoring, undergraduate and graduate instruction improvement, and outreach activities such as working with high school students.","title":"III: Small: Parallel Similarity Comparison and Duplicate Detection with Incremental Computing","awardID":"1118106","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[486626],"PO":["563751"]},"181401":{"abstract":"Data networking via satellite relays remains an important means of linking globally-distributed network terminals for both commercial and governmental applications, especially in remote regions. Modern applications demand both spectrum and power efficiency in the network. The archetype network model in such networks is one with two terminals wishing to exchange data via a single satellite transponder. Relative to traditional time-sharing or frequency-sharing for the bidirectional communication paths, information-theory reveals that spectrum efficiency gains of up to 100% can be obtained for a given set of link power resources. These gains are possible when non-orthogonal transmission methods are adopted, and the decoders exploit side-knowledge on previously-transmitted information. <br\/><br\/>The project codifies various protocols appropriate to this two-terminal data exchange model, including amplify-forward, as well as protocols that involve satellite decoding\/re-encoding. The possible gains depend on link resources as well as the desired bidirectional rate targets. Existing research for this problem presumes perfect synchronization and side-information at both terminals, but practical issues of large round-trip delay, carrier phase\/frequency synchronization, and symbol synchronization are important obstacles to achieving the promise of information theory. So the investigators develop realistic synchronization protocol designs that approach the ideal information-theoretic limits. In addition, the project studies a new decode-and-forward relaying protocol based on nested LDPC coding on downlinks that is flexible in terms of rate-asymmetry.","title":"CIF: Small: Efficient Satellite Relaying","awardID":"1116997","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["508479","556971"],"PO":["564924"]},"181522":{"abstract":"Ensemble methods are general techniques in machine learning for combining several hypotheses to create a more accurate predictor. In the batch learning setting, techniques such as bagging, boosting, stacking, error-correction techniques, Bayesian averaging, or other averaging schemes are common instances of these methods. These methods often significantly improve performance in practice and often benefit from favorable learning guarantees, typically in terms of the margins of the training samples. <br\/><br\/>However, ensemble methods and their theory have been developed primarily for the common binary classification problem, or standard regression tasks where the target labels are real numbers and thus have no structure. These techniques do not readily apply to structured prediction problems such as pronunciation modeling, speech recognition, parsing, machine translation, or image processing. The objective of this proposal is to create the theoretical foundation, large-scale algorithms, and practical techniques for devising effective ensembles of structured prediction techniques. The benefits of these algorithms are likely to be at least as significant as those resulting from ensemble techniques in binary classification.<br\/><br\/>Our solutions will be crucial to a broad set of applications and will be made widely accessible through open-source software programs. These software and open-source programs will make the use of our learning algorithms accessible to a broad community of researchers and engineers. More broadly, our techniques will benefit the society through the discovery of significantly more accurate solutions to a variety of important problems including speech recognition, speech synthesis, and machine translation.","title":"RI: Small: Ensemble Methods for Structured Prediction","awardID":"1117591","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[486340],"PO":["562760"]},"181412":{"abstract":"Networked systems have always been designed to operate even in the presence of failures, especially in communication links and storage. Until recently other components of such systems had relatively low probabilities of failures and for most networked systems, desired levels of resilience could be achieved using minimal redundancy added in an ad hoc manner. Two opposing trends are likely to make the task of achieving resilience significantly more difficult in the coming years: (a) increasing hardware failure probabilities: with the move towards finer nano-scale fabrication, chips are increasingly vulnerable to soft errors caused by external noise and are increasingly likely to fail early due to fatigue; (b) higher resilience requirements: as critical services continue to migrate to clouds, service providers are compelled into more stringent service-level agreements (SLAs), including higher reliability, higher availability, and tighter guarantees on service times. The above combination can dramatically increase the overhead of existing approaches for achieving desired levels of resilience. <br\/><br\/>Intellectual merit: The first outcome of this project will be a holistic roadmap for resilience of networked systems. This resilience roadmap will take the roadmaps from the nano-scale CMOS (trends in chip cost, functionality, performance, power, and resilience that can be attained at chip level) and attempt to realistically project the future cost of currently-used networking and systems techniques for achieving desired level of resilience. The second outcome of this project is to develop resilience methods that scale gracefully in the face of increasing hardware failures. Such techniques will use novel partitioned redundancy strategies that achieve reliability at different levels across hardware and software layers. <br\/>Broader Impacts. The resilience roadmap will provide unprecedented understanding of the trends in resilience and a uniquely realistic assessment of challenges and opportunities. This will significantly influence the research in the hardware as well as networking communities. A systematic design of scalable resilience methods will lead to significantly higher levels of resilience, lower costs - capital (equipment) as well recurring (especially, energy), and\/or higher levels of performance. The utilitarian gains to society by the proposed project are likely to be substantial, since networked systems now constitute one of our most critical infrastructures and consume an increasingly large proportion of our resources.<br\/><br\/>This project will draw upon two different disciplines, hardware architecture and networked systems, and involve detailed case studies and development of completely new theory and techniques, and will therefore provide unique educational and training opportunities for students and working professionals in these fields.<br\/><br\/>Budget Impact Statement: The item numbers in this paragraph refer to those in Figure 9 and Section 3.2 (entitled 'Proposed Research Tasks and Plan') of our original proposal. We will undertake all tasks and sub-tasks proposed in item-1 (and all its sub-items). In item-2, we will undertake the development of a general framework to consider all basic redundancy schemes and alternative ways of deploying them (sub-item-2.1). We will also characterize the associated tradeoffs (sub-item-2.2) and the consequences of realistic constraints (sub-item-2.3). However, we will pursue the development of prototype tools (as outlined in sub-item-2.4), to the extent necessary to demonstrate the benefits of our approach and to conduct case studies (described in item-3). Finally, we will undertake the case studies as originally proposed in item-3.","title":"NeTS:Small:Understanding the Impact of Unreliable Hardware on the Resilience of Networked Systems","awardID":"1117049","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["553707","535238"],"PO":["565090"]},"181533":{"abstract":"The goal of this project is to gain a deeper understanding of mobile remote presence systems (MRPs) and to create guidelines for their effective design, development, and adaptation into organizational use. MRP systems enable embodied mediated communication in which individuals at a remote location connect to a local robot that is used to physically navigate in the local environment and to interact with local users via audio and video. MRP systems allow remote users to visit individuals in an organization and attend group meetings, seminars, and social gatherings in the local environment. MRPs enable new forms of interactions and offer remote users an improved sense of presence compared with stationary video-conferencing systems.<br\/><br\/>The project will study the use of MRPs in communication from an interdisciplinary approach drawing from and building on knowledge and methods from design, social and cognitive psychology, communication studies, and computer science. A series of field and laboratory studies will focus on four topics: (a) how remote users present themselves through MRPs; (b) how local users perceive remote users; (c) the role that social cues play in embodied mediated communication; and (d) the social and organizational outcomes of embodied mediated communication.<br\/><br\/>Intellectual merit: The project will advance understanding of the role of embodiment in mediated communication in collaborative work and inform the design of future mobile remote presence systems. In addition, the results will contribute to basic science in human-computer and human-robot interaction.<br\/><br\/>Broader impact: The results will enable more effective embodied mediated communication in organizations, thereby improving collaboration in distributed work groups. The results will also inform the development of tools that help individuals with mobility-related disabilities interact with their social and professional communities. In addition, the project will enhance the undergraduate and graduate curriculum at the University of Wisconsin-Madison, and there will be an outreach program to disperse interdisciplinary knowledge in and methods for designing robotic technology into K-12 education through an annual summer camp and biannual daylong workshops.","title":"HCC: Small: Embodied Mediated Communication in Collaborative Work","awardID":"1117652","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[486367,"520932"],"PO":["565227"]},"185911":{"abstract":"The Network to support Science, Engineering, Arts and Design (NSEAD) will support transformative research and pedagogy that are only possible through the combined expertise of diverse knowledge domains and disciplines. For example, as physicists and engineers developed new imaging techniques, visual artists experimented with the new expressive potentials they enabled, often influencing development of the technologies. Visual artists and musicians have created and continue to create computer languages and algorithms while pushing technologies for composing and recording in fields of software engineering, artificial intelligence, graphics and visualization. Students who are involved in the arts have higher math, verbal, and composite SAT scores than students who are not involved in the arts. (Vaughn and Winner, 2000). There is a growing movement by higher education academic institutions in the United States to integrate the Arts and STEM disciplines to educate the whole student while leveraging creative cognitive skills for solving complex problems in science and technology disciplines. And finally, diverse ecosystem of academic programs in pre-K to gray formal and informal STEM learning; scientific research conferences; exhibitions, and cultural institution programs continues to emerge as new information technologies, creativity support and social networking tools become pervasive in our society. This project envisions a network that addresses fundamental challenges including the need to align academic pedagogies with 21st century thinking skills; to promote diversity of perspectives, approaches, and people in the creative information technology economy; and to benchmark best practices that create critical thinkers and leaders for the ever-changing technology-driven job market. The development of such a network will provide a platform to disseminate and generate public dialogue about the intellectual, cultural, and economic potential of intersections of science, technology and creativity. <br\/><br\/>NSEAD will be a platform to support the burgeoning research community of Computer Scientists, Engineers, Artists and Designers engaged in integrative research and pedagogy across these disciplines. NSEAD will provide a bridge for academic institutions, non-profit organizations, industry liaisons, and resource providers to collaborate, share best practices in research and pedagogy, and build stronger affinities. It will serve as a junction for elements such as: 1) research community development; 2) collaboration and project matchmaking opportunities; 3) skills expertise referrals; 4) inter-institutional collaborations; 5) forums to share best practices in pre-K to gray STEM learning and creative enrichment; and 6) strategies for network leadership and resource sustainability.","title":"Collaborative Research: EAGER: Network for Science, Engineering, Arts and Design (NSEAD)","awardID":"1142505","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[498957],"PO":["565342"]},"181555":{"abstract":"Broadband cellular networks are emerging to be the most common means for mobile data access worldwide. Predictions from industry analysts indicate that the volume of data through cellular data networks will increase exponentially in near future. Understanding of the mobile data traffic via measurement and analysis is critical for the development of resource management techniques for these networks. While spectrum resources are of great concern, this project specifically focuses on the ?energy? required to operate the cellular network infrastructure, specifically base stations. The project undertakes a significant modeling exercise with two goals. One goal is ?intellectual,? driven towards understanding the spatio-temporal dynamics of mobile traffic and discovering possible structure or relationships. The project uses state-of-the-art machine learning tools to develop models using large-scale data collected directly from the operators? networks. Such modeling will bring new insights that in turn will help to deploy and manage future generation cellular data networks. The second goal is ?utilitarian.? Here, techniques are developed to predict base station loads for use in resource management, specifically energy. Algorithms are designed to exploit energy-optimization opportunities to turn off specific network resources based on the forecasted load.<br\/><br\/>The project has significant broader impact. It develops technologies to appreciably reduce energy consumption in cellular networks. Overall, this exercise will both reduce cost, and contribute to the environment. The project also contributes to several 'green? initiatives in both institutions and to the education and training of graduate students.","title":"NeTS: Small: Collaborative Research: Understanding Traffic Dynamics in Cellular Data Networks and Applications to Resource Management","awardID":"1117719","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["563568",486433,486434],"PO":["565303"]},"180345":{"abstract":"Despite the prevalence of social network platforms and apps in nowadays daily life, existing research on social media takes the terms \"social\" and \"media\" separately, and fails to address important needs for intelligently managing and utilizing social media, such as finding the information that users want, situating information in a social context that gives it meaning, and providing order and structure to an intricate and intertwined network of relationships. This interdisciplinary project will provide a holistic view of social media by combining socially intelligent language processing with linguistically motivated social network analysis. Specifically, the project will: (a) discover sociolinguistic communities and identify the demographic and sociological factors that underlie community membership; (b) discover cross-community linguistic variation at various levels and develop new computational tools for dialectometric and sociolinguistic analysis and for prediction of user interests and trends; and (c) recommend content and social connections across community boundaries, which will help people to broaden their perspectives with new information, opinions, and social relationships.<br\/><br\/>Intellectual merit: The project will lead to (a) new modeling formalisms that jointly incorporate linguistic information with social network metadata; (b) a new computational methodology for sociolinguistic investigation from raw text; and (c) flexible models of linguistic variation that model temporal dynamics and move beyond simplistic bag-of-words approaches to higher-order phenomena such as multi-word expressions, syntax, and joint orthographic variation. <br\/><br\/>Broader impacts: The project will lead to advancements in basic research in statistical machine learning, social sciences, and language technology. It will also bring innovations and practical applications in all these areas, such as software that reasons intelligently about community structures and linguistic patterns and conventions in social media. The findings will benefit a wide range of needs, such as personalized information service and intelligence and security operations, which require precise and timely understanding of social-cultural events and trends. The project will also provide undergraduate research opportunities and outreach to high school students through summer programs.","title":"Collaborative Research: Discovering and Exploiting Latent Communities in Social Media","awardID":"1110904","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7953","name":"SOCIAL-COMPUTATIONAL SYSTEMS"}}],"PIcoPI":["489495"],"PO":["564456"]},"181203":{"abstract":"This project addresses 3D communication which is becoming a viable medium for collaborative activities between geographically dispersed users. Supported by 3D realistic video reconstruction and real-time media transmission, fusion and rendering, this technology creates a shared visual cyberspace and delivers a strong sense of \"being-there\" for participants. Despite incremental progress, realization of this technology has not yet occured. One fundamental barrier is the need for investigating how to effectively connect multiple sites, aggregate and disseminate visual contents to the end user. <br\/><br\/>Research in this area is critical because of the following challenges:<br\/><br\/>-3D communication is very resource-intensive in data, transmission and rendering, especially when multiple sites are connected. <br\/><br\/>-3D communication represents a highly user-centric and interactive system. Traditional system-based resource management assuming limited set of user interests and quality-of-service measures becomes less effective. Instead, we need to investigate resource management to deliver quality-of-experience under the dynamics of user view. <br\/><br\/>-3D communication represents a content-rich system where each site generates a set of 3D video streams. The correlation among these streams is more complicated as their importance is determined dynamically by the user interest. Traditional systems assuming a fixed multi-stream prioritization will fail. <br\/><br\/>-Finally, the nature of physical collaboration requires the interconnection in a seamless way such that they operate like a single environment. However, the information needed to achieve desired global system behavior is scattered at each site which makes it a non-trivial problem to coordinate content dissemination among all sites. <br\/><br\/>The project will make contributions to enabling multi-party 3D communication via the investigation of a view-based multi-party\/multi-stream management framework with two components: content organization methods (COM) and content dissemination services (CDS). <br\/><br\/>The project will provide valuable insights into building multi-party 3D collaborative systems through the perspective of resource allocation, content organization and dissemination, and user experience. The work will be evaluated using a real testbed involving multiple institutes across the Internet, the success of which will enable the rapid development and deployment of 3D communication. <br\/><br\/>The project will also provide a natural platform to incorporate research findings into curriculum development on distributed multimedia systems and engage graduate\/undergraduate students into related research. The testbed will also serve high-school teachers and students via an outreach program. Florida International University is one of the largest minority institutes in the country but has a relatively small number of minority Ph.D. recipients. The PI is currently working with one Hispanic and one female Ph.D. students. It is the PI's plan to leverage the funding and his prior experience to enlarge the presence of students from minor groups in computer science research.","title":"CSR: Small: Resource Management for 3D Communication Systems","awardID":"1116023","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[485564],"PO":["551712"]},"180477":{"abstract":"Data provenance refers to the history of the contents of an object and its successive transformations. Knowledge of data provenance is beneficial to many ends, such as enhancing data trustworthiness, facilitating accountability, verifying compliance, aiding forensics, and enabling more effective access and usage controls. Provenance data minimally needs integrity assurance to realize these benefits. Additionally, provenance data may need assurances of confidentiality (e.g., protect the identity of a reviewer in a blinded paper review process from the authors but not from the editor) or of privacy (e.g., do not disclose identity of a source without the source's consent). In the past decade there has been significant progress regarding the structure and representation of provenance data as a directed acyclic graph. However, currently there is no overarching, systematic framework for the security and privacy of provenance data and their tradeoffs with respect to the utility of provenance data. The development of such a framework is recognized as one of a handful of promising thrusts in recent reports on Federal game-changing R&D for cyber security, particularly aligned with the theme of Tailored Trustworthy Spaces.<br\/><br\/>This project is to develop a comprehensive technical and scientific framework to address the security and privacy challenges of provenance data, and the attendant tradeoffs, so that our society can gain maximum benefit from applications of provenance data. Detailed foundational research is to be performed on security enhanced data models, access control and usage models, privacy including annonymization and sanitization, integrity, accountability and risk management techniques for provenance data. This foundational research is complemented by data provenance case studies in scientific and cyber security information sharing, and construction of prototype data provenance systems at the operating systems and data layers. Moreover, reference architectures and definitions of corresponding provenance management services are to be defined, identifying how these services can be effectively deployed in enterprises, and developing a risk-management framework to guide application architects, designers and users to effectively embed data provenance in their specific context. The project results will beneficially impact society at large by increasing trustworthiness of data acquired, transmitted and processed by computer systems. From the educational side, both theory and practice of data provenance are to be integrated in the undergraduate and graduate training of students, including underrepresented minority and female students, in all the collaborative institutions of this project.","title":"TC: Large: Collaborative Research: Privacy-Enhanced Secure Data Provenance","awardID":"1111512","effectiveDate":"2011-08-01","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["529959","486459",483676],"PO":["565264"]},"181577":{"abstract":"The largest web search engines now receive hundreds of millions of queries per day that need to be answered in fractions of a second on collections of tens of billions of web documents. In order to process all these queries, search engines consume increasing amounts of hardware and energy resources. This project focuses on developing new algorithms, index structures, and other software techniques for scaling query processing in search engines, that is, techniques that allow queries to be executed faster and on larger data sets using fewer hardware and energy resources. <br\/><br\/> Research activities in this project focus on three main approaches. First, the project studies how index size and access time can be reduced through improved index compression techniques. Second, work on new early termination techniques considers how the top results for a query can be computed without exhaustive traversal of the index structures for the query terms, for simple ranking functions such as BM25 or Cosine, and for the more complex functions with many features used by current web search engines. Finally, the project explores general techniques for query optimization in information retrieval (IR) systems, inspired by the significant body of work on query optimizers in database systems. <br\/><br\/> Web search engines are a multi-billion dollar industry and a crucial component of the internet. Techniques resulting from this project are expected to benefit this industry by reducing the hardware cost and energy consumption of large-scale search services. Results will be disseminated through publications in major conferences and journals, tutorials at conferences, distribution of software libraries, contributions to existing software tools such as Lucene. This project provides research and educational opportunities for graduate and undergraduate students and prepare them for later work at companies, research labs, or universities. Web site (http:\/\/cis.poly.edu\/westlab\/queryproc\/) provides more information about this project.","title":"III: Small: Efficient Query Processing in Large Search Engines","awardID":"1117829","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["521989"],"PO":["563751"]},"181467":{"abstract":"AF: Small: Collaborative Research: Active DNA Assembly of Aperiodic Structures<br\/>PIs: N.C. Seeman, N. Jonoska<br\/><br\/>The science of computing and information processing is rapidly proliferating within established disciplines that at first glance appear unrelated. Examples include biochemistry and biology through the areas of bioinformatics and algorithmic molecular assembly, or physics, through quantum computing. Current top-down photolithographic methods of electronic component construction are likely to encounter severe barriers as miniaturization requires features at very small sizes. An alternative approach to such constructions is programmable bottom-up assembly on the molecular scale. The most popular basis for bottom-up assembly is DNA self-assembly, because DNA contains information through which assembly can be controlled in a variety of ways. This approach has been explored by a variety of laboratories in recent years, but was pioneered by the PI Seeman's laboratory roughly thirty years ago. Robust DNA motifs can be self-assembled into objects, lattices and devices, using the information content inherent in the DNA molecules (or their analogs) themselves. Notable milestones include the self-assembly of DNA polyhedra, the self-assembly of 2D DNA periodic lattices, the self-assembly of high-resolution 3D DNA crystals, the development of clocked and autonomous cascade-like DNA walkers, the development of DNA origami, and the construction of 1D and 2D algorithmic assemblies. <br\/><br\/>This project aims to combine several of these developments to produce robust aperiodic structures that will enable nano-scale construction of computational machinery. The goal of the project is to introduce a new paradigm in the self-assembly of nano-scale devices by DNA self-assembly: active (rather than passive) recursion-based assembly of aperiodic structures. The project will incorporate elements of DNA walkers into DNA origami-based tiles as signals that guide hierarchical active assembly of aperiodic arrays. <br\/><br\/>The proposed hierarchical algorithmic assembly would be the first demonstration of recursion in molecular self-assembly that can be seen as a physical incarnation of programmed recursion. It is known that self-similar and fractal-like structures have advantages in material design, heat exchange and information processing, e.g., the very significant advances obtained by miniature fractal-like antennas. Consequently, the possibilities for self-similar arrangements at the nano-scale can be expected to advance further the design of materials and electronics, while decreasing the costs involved in their construction. <br\/><br\/>The project is highly interdisciplinary, ranging from computer science to molecular design, from biochemistry to crystallography, physics and engineering. It directly supports training of a postdoctoral associate (a female Hispanic) and graduate students. The training that the postdoc and the students receive in doing this work will prepare them as unique interdisciplinary research scientists, trained researchers able to pursue studies that require broader scientific knowledge. The postdoctoral fellow will be trained to lead and coordinate a project of a fundamentally interdisciplinary nature. Through their teaching, the PIs impact a wider student body, and through writing textbooks they impact the wider society. The PIs are prominent members of the DNA-based nanoengineering community. Their contributions are well recognized in the field of nanoscale engineering, DNA computing and DNA nanotechnology, and as such the PIs are involved in the wider scientic organizations and events as both, lecturers and organizers. PI Seeman is frequently involved in science-art activities, and has just written an article on this topic for the new NSF journal Mosaic.","title":"SHF: Small: Collaborative Research: Active DNA Assembly of Aperiodic Structures","awardID":"1117254","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7946","name":"BIO COMPUTING"}}],"PIcoPI":["506031"],"PO":["565223"]},"181588":{"abstract":"Data is at the heart of the digital revolution. Changing work habits, increasingly stringent electronic record-keeping mandates, and developments in the health care, energy, and retail sectors all suggest a growing demand for data storage and memories for the foreseeable future. Fast, low-power, ultra-high density, and non-volatile magnetic storage and memory systems are projected to accommodate the bulk of the global data storage needs for decades to come and spawn revolutionary data-intensive computing modalities along the way. Their timely development, however, critically depends on the availability of fast and accurate computational tools capable of simulating electromagnetic field and magnetization dynamics in complete magnetic data storage and memory systems. Unfortunately, current simulators are not up to this task. <br\/>This project focuses on the development of high-performance hybrid micromagnetic-electromagnetic simulators for modeling next-generation magnetic memory and data storage systems. The simulators leverage analytically preconditioned time-domain integral equation methods to solve the Maxwell and Landau-Lifshitz-Gilbert-Slonczewski equations, which govern coupled electromagnetic field and magnetization phenomena. To facilitate the analysis of such phenomena in complex systems involving billions of degrees of freedom, the simulators are implemented on massively parallel Central Processing Unit (CPU) and Graphics Processing Unit (GPU) computers. These simulators are used for the design of next generation magnetic random memory devices as well as bit patterned media and heat-assisted magnetic recording storage systems. Such advanced memory and storage systems are to be essential to future high-performance computing systems.","title":"SHF: Small: Collaborative Research: High-performance hybrid micromagnetic-electromagnetic simulators for memory and high-performance data storage devices","awardID":"1117911","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["547339"],"PO":["565272"]},"180378":{"abstract":"The ability to interact remotely over the Internet is redefining the nature of collaboration. Many collaborative activities require coordinating attention and action with another person moment-by-moment; without the benefit of being physically present with another person these sorts of collaborations are difficult to conduct efficiently. This project explores using human eye gaze to create partner models for mediating time-critical collaborative activities. A partner model is a dynamically learned description of what a partner is trying to do - for example, what someone may be looking for, or what they consider to be relevant within a task. <br\/><br\/>Intellectual Merit: Many tasks and events are implicit or poorly defined, requiring that partner models be learned from evidence unfolding as part of a person's ongoing behavior. Eye trackers will be used to determine the task-relevant objects that a person chooses to look at (and not look at); through analysis of these gaze patterns and the properties of the objects, human and computer partners will learn a model of what this person is attempting to do. Various tasks will be explored, such as searching for a new and\/or ambiguous moving target specified only by incomplete semantic descriptions, or monitoring a complex dynamic environment for unusual events, defined by atypical target movements and relationships between people and objects. The findings will advance the fields of human-computer interaction, psycholinguistics, artificial intelligence, object and event detection by humans and computers, and multimodal human communication.<br\/><br\/>Broader Impacts: The results of the project will facilitate the development of new tools that can help people with their tasks, by, for example, finding and highlighting objects in a scene that match the viewer's goals and helping the viewer track moving targets. The results will also lead to new tools for remote collaboration, with the goal being to make coordination at a distance as efficient as face-to-face interaction. The tools and techniques from the project are expected to benefit a variety of applications, including the development of assistive technologies for people with communication impairments and the creation of better security screening procedures. The project will provide training and research experiences for Stony Brook University's racially, ethnically, and economically diverse students, including women and others underrepresented in science and engineering.","title":"Using Gaze Cues to Build Partner Models for Collaborative Behavior","awardID":"1111047","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7953","name":"SOCIAL-COMPUTATIONAL SYSTEMS"}}],"PIcoPI":[483394,"508251","508252"],"PO":["565342"]},"180499":{"abstract":"Many compelling applications involve computations that require sensitive data from two or more individuals. For example, as the cost of personal genome sequencing rapidly plummets many genetics applications will soon be within reach of individuals such as comparing one?s genome with the genomes of different groups of participants in a study to determine which treatment is likely to be most effective. Such comparisons could have tremendous value, but are currently infeasible because of the privacy concerns both for the individual and study participants. What is needed is a way to produce the result of the comparison without exposing either party's private inputs. The ultimate aim of this project is to make privacy-preserving computation practical and accessible enough to be used routinely in applications such as personalized genetics, medical research, and privacy-preserving biometrics.<br\/><br\/>Theoretical solutions to this problem, known as secure multi-party computation, have been known for several decades, including a general solution developed by Andrew Yao based on garbled circuits. Because of its extensive memory use and computational cost, however, the garbled circuits approach has traditionally been considered more of a theoretical curiosity than a practical mechanism for building privacy-preserving applications. Recent developments in cryptographic techniques and new implementation approaches are beginning to change this, however, and admit the possibility of scalable, practical secure computation. This project is designing methods for avoiding the memory bottleneck associated with garbled circuit evaluation by aggressively pipelining circuit generation and evaluation, and exploring a variety of techniques for reducing the size of garbled circuits. Another issue the limits the use of secure computation in practice is the need for standard protocols to assume an honest-but-curious adversary who always follows the specified protocol. This project is developing new techniques for dealing with malicious adversaries, improving the standard cut-and-choose and commit-and-prove approaches by using new cryptographic tools and exploring an alternate model in which a verifiable trusted party generates the circuit but is not trusted with any private data. The project is also developing techniques to audit the information that can be inferred from the result of a secure computation. Another goal is to make secure computation more accessible to developers by developing programming tools for defining secure computations at a high level, based on information-flow analysis and program partitioning.","title":"TC: Large: Collaborative Research: Practical Secure Two-Party Computation: Techniques, Tools, and Applications","awardID":"1111599","effectiveDate":"2011-08-15","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["550282","519626"],"PO":["564388"]},"181599":{"abstract":"Successful software systems continue to change. Most programmers work on projects that they did not start, and most companies spend more on maintaining old systems than on building new ones. This goal of this research is to make programs easier to change by developing better software tools and by studying how programmers change software. The project is extending Photran, an open-source programming environment for FORTRAN, so that it better supports the way FORTRAN programmers change their software to make them run on next-generation supercomputers. The new version of Photran will have the potential to make it much less expensive to port high-performance software, and the ideas have the potential to reduce the cost of software development in general.<br\/><br\/>The new system will record each change that a programmer makes and will represent these changes at a high-level, i.e. not just as textual changes, but as more meaningful units of changes, such as refactoring or optimizations. It will let programmers modify these changes after the fact, making it possible to change the portable version of a program and then replay the hand-crafted optimizations. Programmers can port a program to a new architecture by starting with a portable version and then choosing optimizations that were useful for similar machines, or that were discovered by an auto-tuner, or that were invented as needed. They will be able to think of a program as a sequence of program transformations, and to generate a new program by reusing sub-sequences from other programs. Thus, a sequence of changes will be just as valid a representation of a program as a set of modules","title":"CCF: Small: A Programming Environment for High Performance Computing Based on Practical Program Transformation","awardID":"1117960","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["561355"],"PO":["564588"]},"181005":{"abstract":"Networks of sensors are increasingly important in a variety of applications including national security, environmental monitoring, and health care. These systems should serve their purposes with minimal communication between sensors and minimal computation overhead from coding. In particular, these efficiencies can dramatically improve battery life, and in systems such as those implanted in a body, replacing spent batteries is very difficult<br\/><br\/>This project will develop original approaches to the signal coding in sensor network systems. Though distributed source coding principles seem to be a natural fit for sensor networks, they are rarely used in these systems. Reasons include high complexity, high delay, and sensitivity to the accuracy of the assumed probabilistic models. Also, there may be no node with the memory and computing power to do Slepian-Wolf decoding. The failure of these methods in practice has left a glaring technological gap. Most sensor networks use simple uniform scalar quantization and compression that does not exploit inter-sensor correlation, or no compression at all. This project will use high-resolution quantization theory to develop a framework for providing and exploiting quantized side information among nearby nodes in a network, with the aim of supporting inference and computation tasks.<br\/><br\/>The central innovative idea is to allow limited (low-rate, short-range) communication among encoders to enable adaptation. A second key area of innovation is a focus on information acquisition systems that are designed to make a computation rather than enable reproduction of every measured value. The focus on acquisition and computation -- as opposed to communication -- is consistent with the actual motivation for deploying sensor networks, and it brings robustness to uncertainty in measurement distributions to the forefront.","title":"CIF: Small: Quantization for Acquisition and Computation Networks","awardID":"1115159","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["508043"],"PO":["564924"]},"181126":{"abstract":"CGV: Small: Inverse Light Transport under Femto-Photography and Transient Imaging<br\/>Raskar, Ramesh, Massachusetts Institute of Technology<br\/><br\/>How can you photograph objects beyond the line of sight? How can you recover bidirectional reflectance of materials from a single viewpoint? These seemingly impossible tasks are possible by considering the finite speed of light and using a new type of computational photography called, Femto-Photography. New advances in ultra-fast imaging provide tremendous new opportunities in modeling, representing and synthesizing light transport in computer graphics and computer vision. Research in computational photography and scene understanding will benefit by analyzing the transient response of the scene to extremely short duration active illumination. Traditional imaging uses steady-state response where the global illumination has reached an equilibrium state. The investigators are developing a new theoretical framework for transient light transport and are addressing inverse problems using time-resolved imaging. The investigators have recently developed the first physical demonstration of hidden geometry recovery.<br\/><br\/>The research aims to develop a new branch of computational imaging by developing a mathematical framework for studying higher dimensional light transport that exploits time-resolved imaging. This research brings ultra-fast imaging in the realm of computer graphics\/vision and computational photography. The finely sampled time-dimension provides a range of research directions for modeling and measuring geometry and photometry of scenes that were previously considered beyond the reach of traditional machine vision. The techniques for time-resolved imaging exploit multiplexing, sparsity-exploiting reconstructions, state-space formulation, system identification methods and parameterized reflectance models in novel ways. Overall, the research pushes the boundaries of light transport based methods by an extra (time) dimension and hopes to show that forward and inverse problems in 5D light transport can inspire the next generation of imaging hardware and algorithms.","title":"CGV: Small: Inverse Light Transport Under Femto-Photography and Transient Imaging","awardID":"1115680","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["532050"],"PO":["565227"]},"181247":{"abstract":"This project focuses on the development of ALERT: An Architecture for the Emergency Re-tasking of Wireless Sensor Networks. The novelty of this work lies in the theoretical foundation of re-tasking independently-deployed sensor networks, leading to a fundamental understanding of the design principles of capability reallocation and sharing to best satisfy the needs of emergency applications. Both re-tasking and integration of sensor capabilities will be transparent to the emergency applications. The resource-constrained nature of sensors, the wireless communication medium, and the failure-prone networking environment, combined with the dynamic QoS requirements of the emergency applications pose formidable challenges for the design of ALERT. <br\/>The understanding acquired from developing ALERT will promote a wider adoption of sensor network systems in support of guarding our national infrastructure and public safety. This project will result in significant scientific and technological advances that will provide invaluable help with disaster management and search-and-rescue operations. ALERT will have a broad societal impact as sensor networks are being integrated into the fabric of the society. The project will integrate research and education and will lead to the development of new graduate and undergraduate courses in sensor networks and embedded and distributed systems. In turn, these courses and their focus on information integration will introduce novel research topics to undergraduate and graduate students in computer science and engineering that fit within the overall missions of Old Dominion University and Clemson University. Focused efforts will be undertaken to stimulate interest and to facilitate the academic progress of women and underrepresented minorities.","title":"CSR:Small:Collaborative Research:An Architecture for the Emergency Re-tasking of Wireless Sensor Networks (ALERT)","awardID":"1116238","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[485667,485668],"PO":["565255"]},"181379":{"abstract":"This project seeks to quantify the frequency, duration, causes, and impact of faults across a variety of network classes. Unlike previous efforts that have relied on substantial special-purpose instrumentation and monitoring infrastructure, the PIs are conducting their analysis using only commonly available data sources, such as device logs and configuration records maintained by network operations staff. In this way, they hope not only to provide concrete data regarding the particular networks being evaluated, but to define a repeatable methodology that can be employed by other researchers and even commercial operators to assess the reliability of other networks. Through partnerships with an academic backbone network (CENIC), an enterprise network services company (Hewlett-Packard), and large-scale Web services provider (Microsoft), the PIs have obtained access to device logs, operational maintenance records, and configuration information for a significant number of real networks.<br\/><br\/>Concretely, this project is working to deliver a fault analysis methodology based on readily available data like device logs, configuration information, and operator records such as email lists and trouble tickets; comparative studies regarding the differing failure characteristics of wide-area, enterprise, and data center networks; and a generative model of network faults that can be used to evaluate the suitability and efficacy of different applications and protocols to various network designs.<br\/><br\/>Broader Impact: In addition to these concrete technical contributions, the broader impacts of this project include the potential to cause network operators to reconsider how current networks are designed to make them less likely to fail, or to fail in more straightforward and easily manageable ways. Moreover, the research effort is imparting onto the next generation of computer scientists the skills necessary to assess, analyze, and model the performance characteristics of operational networks. This project supports two graduate students who assist the PIs in conducting the described work, and receive significant exposure to commercial network environments through industrial internships.","title":"NeTS: Small: Understanding Network Failure","awardID":"1116904","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["525661","548363","525664"],"PO":["565090"]},"181269":{"abstract":"Operational and safety goals for the built environment demand robust, scalable and reliable large<br\/>scale monitoring for infrastructure systems. High performance real-time event detection and decision making<br\/>requires models and algorithms to process large amounts of data from dense sensor networks deployed<br\/>in these systems. Despite advances in the development of detection algorithms for such networks, there are<br\/>two widely recognized and conflicting obstacles: detection rules need to be sufficiently complex to adapt to<br\/>the spatiotemporal changes in the environment, requiring the sharing of data; but rules are constrained by<br\/>statistical performance guarantees and computation and communicational budgets imposed by the network.<br\/>This project addresses these challenges by developing a fundamentally new approach that jointly accounts<br\/>for statistical detection, communication constraints and distributed computation.<br\/>This research develops a framework that integrates the distributed computation and communication constraints<br\/>of the underlying network infrastructure with flexible stochastic modeling and learning algorithms<br\/>with spatiotemporal data. The modeling and algorithms enable simultaneous and sequential decision making<br\/>at many local sites, by borrowing information across the network in a statistically coherent and computationally<br\/>efficient manner. Combining the formalism of sequential change point detection, nonparametric and<br\/>probabilistic graphical models and spatiotemporal statistics, the project develops distributed and sequential<br\/>message-passing algorithms for detecting changes in the underlying distributions generating network data.<br\/>The models developed also offer new theoretical understanding of the trade-offs between statistical model<br\/>complexity, distributed computation efficiency, and structure of communication constraints within the network.<br\/><br\/>This interdisciplinary research brings together students and researchers from different areas, utilizing<br\/>and developing knowledge and cross-disciplinary skills in the fields of computer science, statistics, signal<br\/>processing and civil engineering.","title":"CIF: Small: Collaborative Research: Distributed Detection Algorithms and Stochastic Modeling for Large Monitoring Sensor Networks","awardID":"1116377","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["564235"],"PO":["564898"]},"185405":{"abstract":"The award is to support the series of workshops on security in emerging areas that are affiliated with the 2011 ACM Conference on Computer and Communications Security (CCS), and will be held on October 17-21st, 2011 in Chicago, IL. <br\/><br\/>The annual ACM Computer and Communications Security Conference is a leading international forum for information security researchers, practitioners, developers, and users to explore cutting-edge ideas and results, and to exchange techniques, tools and experiences. Since year 2001, CCS started accommodating series of workshops to explore security issues in a variety of emerging areas. CCS workshops quickly become active forums for researchers to form focus groups, discuss and collaborate on emerging and critical security problems, and disseminate fresh, revolutionary (and sometimes even controversial) ideas. These workshops also serve as natural venues to bring together researchers from multiple disciplines to address security issues in specific domains, such as smartphone technology, cloud computing and national critical infrastructures.","title":"A Series of Workshops on Security in Emerging Areas","awardID":"1139947","effectiveDate":"2011-08-15","expirationDate":"2012-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["532213"],"PO":[497499]},"186418":{"abstract":"This exploratory research is a \"latitudinal,\" multi-world study that compares the creative practices of women and their influence in information technology skills acquisition across three different virtual worlds: Second Life, There.com, and OpenSim. The aim of this experimental, potentially transformative research is to better understand the role of creative practice in development of IT skills among women, the motivations and process whereby women develop these skills, and the extent to which they are being parlayed into IT careers. The study will use qualitative methods, including participant observation and interviews, as well as analysis of player-created digital artifacts, to examine the relationship between creative practice and technical skills acquisition among women in virtual worlds. <br\/><br\/>This exploratory research innovates in three ways: First, it is a latitudinal that makes a significant contribution to the body of research on online cultures in virtual worlds by providing a comparative analysis of cultural practices across multiple \"metaverse\" type worlds. Second, it provides new potentially transformative knowledge about the practices of women in online worlds and their relationship to IT skills acquisition, an understudied area as compared to K-12 girls and STEM skills. Third, it will help us determine the extent to which skills learned in virtual worlds are impacting other aspects of women's lives, such as their general competency and self-efficacy with IT, or their real-world career choices. <br\/><br\/>Because the research seeks to understand the specific motivations of women in virtual worlds to learn IT skills through creative practices, it will provide useful guidelines for expanding female participation in computational fields. It may also form the foundation for a workforce development strategy among post college-age women who are developing IT skills through informal and peer-learning communities that could be parlayed into real-world careers, a phenomenon that is already occurring emergently within the target group.","title":"EAGER: Creative Activity and Development of IT Skills","awardID":"1145224","effectiveDate":"2011-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["561204"],"PO":["564456"]},"175308":{"abstract":"Viruses, worms, and other self-propagating malware remain significant ongoing security threats to almost all sectors of the nation's cyber-infrastructure, including government, business, and home consumers. The escalating rate of new malware appearances increasingly threatens to outpace the defense community's ability to maintain effective detection systems. This is in part because many malware detection algorithms identify malicious software based on syntactic features. Polymorphic malware continually evolves new syntaxes at it propagates, introducing hundreds or thousands of new syntaxes per day that implement the same malicious behavior. Discovering practical, scalable techniques for reliably detecting new polymorphic malware variants is therefore one of the most significant challenges currently facing the computer security industry.<br\/><br\/>This project develops hybrid static-dynamic technologies that detect malware based on semantic rather than purely syntactic code features. Thus, malware is identified based on the meaning of its malicious programming rather than the syntax with which it implements it. Malicious payloads are identified by applying traditionally static code analyses to decrypted memory pages intercepted dynamically at runtime. A major goal of the project is to develop technologies that are scalable and practical for standard computer hardware and operating systems. This will allow wide-scale deployment of results, and help to protect the nation from distributed attacks that compromise large numbers of low-priority targets to attack higher-priority targets. Results from the research will lead to powerful new strategies, concepts, and practical tools that give defenders a significant new advantage in the virus-antivirus arms race, and improving the national cyber-infrastructure's resilience against cyber-attacks.","title":"CAREER: Language-based Security for Polymorphic Malware Protection","awardID":"1054629","effectiveDate":"2011-08-01","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["522482"],"PO":["564223"]},"181600":{"abstract":"Today's massive generation of digital data is greatly outpacing the development of computational methods and tools and presents critical challenges for achieving the full transformative potential of these data. For example, recent advances in acquiring multi-modal brain imaging and genome-wide array data provide exciting new opportunities to study the influence of genetic variation on brain structure and function. Major computational challenges are, however, bottlenecks for comprehensive joint analysis of these data due to their unprecedented scale and complexity. This project will employ the new capabilities of large-scale data mining techniques in multi-view learning, multi-task learning, and robust classification to address critical challenges in systematically analyzing massive multi-modal genetic, imaging, and other biomarker data. Specifically, this project will: (1) develop new multi-view learning methods to detect task-relevant phenotypic biomarkers from large scale heterogeneous imaging and other biomarker data, (2) implement new sparse multi-task regression models to reveal the genetic basis of phenotypic biomarkers at multiple levels (e.g., SNP, haplotype, gene and\/or pathway), (3) design novel robust classification methods via structural sparsity for outcome prediction using integrated genotypic and phenotypic data, and (4) package these new methods into a data mining toolkit and release it to the public. <br\/><br\/>The intellectual merits of this project derive not only from the development of novel data mining methods, but also from their application to imaging genetic studies. These methods are designed to take into account interrelated structures among multiple data modalities and offer systematic strategies to reveal structural imaging genetic associations. The proposed methods and tools are expected to impact neurological and psychological research and enable investigators to effectively test imaging genetics hypothesis and advance biomedical science and technology. In addition, the proposed data mining framework addresses generic critical needs of large-scale data analysis and integration and, therefore, will impact a large number of research areas where high-value knowledge and complex patterns can potentially be discovered from massive high-dimensional and heterogeneous data sets. This project will facilitate the development of novel educational tools to enhance several current courses at UT Arlington and IUPUI. Both universities are minority-serving institutions, and the PIs will engage the minority students and under-served populations in research activities to give them a better exposure to cutting-edge scientific research.","title":"III: Small: Collaborative Research: A Large-Scale Data Mining Framework for Genome-Wide Mapping of Multi-Modal Phenotypic Biomarkers and Outcome Prediction","awardID":"1117965","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["560636","229551"],"PO":["565136"]},"181512":{"abstract":"The RICOLLA project manages inconsistencies in structured databases maintained collaboratively by an online community. RICOLLA remains fully functional in the presence of inconsistencies, enabling \"resolve-as-you-go\" consistency. RICOLLA allows users to collaboratively resolve certain conflicts while disagreeing on others. Building Ricolla involves the following technical contributions: a) a novel architecture that tolerates inconsistency, allows data query and update, while aiding inconsistency resolution by community members; b) a data model and interface for explaining the inconsistencies to the users; c) a set of resolution actions that allow each user to resolve individual data inconsistencies; d) a resolution policy language for summarizing a set of resolution actions based on high level criteria; and e) a set of algorithms for implementing the system on top of a relational database management system.<br\/><br\/><br\/>The resulting techniques and prototype contribute to the infrastructure for the next generation of online databases. This benefits a variety of online communities who need to collaboratively edit structured data, ranging from the scientific domain to digital government and social networks. RICOLLA's evaluation includes as use cases two scientific communities (biologists and geoscientists) and UCSD students taking database classes. Direct deployment in teaching serves to both improve students' online collaboration and collect their feedback for RICOLLA's evaluation and tuning purposes. Publications, technical reports, software and experimental data resulting from this research are available at the project web site http:\/\/db.ucsd.edu\/ricolla.","title":"III: Small: Personalized Inconsistency Resolution in Online Databases","awardID":"1117527","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["518657","525612"],"PO":["563727"]},"180544":{"abstract":"The most powerful computing systems in the world have historically been dedicated to solving scientific problems. Until recently, the computations performed by these systems have typically been simulations of various physical phenomena. However, a new paradigm for scientific discovery has been steadily rising in importance, namely, data-intensive science, which focuses sophisticated analysis techniques on the enormous (and ever increasing) amounts of data being produced in scientific, commercial, and social endeavors. Important research based on data-intensive science include areas as diverse as knowledge discovery, bioinformatics, proteomics and genomics, data mining and search, electronic design automation, computer vision, and Internet routing. Unfortunately, the computational approaches needed for data-intensive science differ markedly from those that have been so effective for simulation-based supercomputing. To enable and facilitate efficient execution of data-intensive scientific problems, this project will develop a comprehensive hardware and software supercomputing system for data-intensive science.<br\/><br\/>Graph algorithms and data structures are fundamental to data-intensive computations and, consequently, this project is focused on providing fundamental, new understandings of the basics of large-scale graph processing and how to build scalable systems to efficiently solve large-scale graph problems. In particular, this work will characterize processing overheads and the limits of graph processing scalability, develop performance models that properly capture graph algorithms, define the (co-design) process for developing graph-specific hardware, and experimentally verify our approach with a prototype execution environment. Key capabilities of our system include: a novel fine-grained parallel programming model, a scalable library of graph algorithms and data structures, a graph-optimized core architecture, and a scalable graph execution platform. The project will also address the programming challenges involved in constructing scalable and reliable software for data-intensive problems.","title":"SHF: Large: Collaborative Research: PXGL: Cyberinfrastructure for Scalable Graph Execution","awardID":"1111888","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["562709"],"PO":["565272"]},"181402":{"abstract":"This project focuses on practical deployment of human\/multi-robot teams in situations where robots can explore regions that are unsuitable for humans. For example, a team of \"rescue\" robots can sweep through a collapsed building searching for victims and transmit their positions to human first-responders outside. Managing a human\/multi-robot team in a dynamic environment is a challenging problem. Not only is the world mutable, but also the team can experience altered membership because a robot gets lost or a human operator needs rest---the world is changing, and so is the team that is exploring that world.<br\/><br\/>The goal of this research is to develop strategies for human\/multi-robot teams to learn to perform consistently and effectively. Three primary aims will be pursued: first, to mitigate changes in team composition via a practical framework for institutional memory that remembers and uses past experiences; second, to model and record expertise for later use by learning behaviors performed by a human operator; and third, to distribute tasks among team members efficiently by providing a balanced mechanism for social choice. The novel approach of this project is applicable to a broad spectrum of human\/multi-robot, and human\/multi-agent teams, by integrating institutional memory, learning from human teammates, and resolving conflict among differing perspectives. The strategies will be evaluated using a human\/multi-robot testbed comprised of one human operator plus a heterogeneous set of inexpensive, limited-function robots. Although each individual robot has restricted mobility and sensing capabilities, together the team members constitute a multi-function, human\/multi-robot facility.<br\/><br\/>This project addresses important challenges in robust intelligence, including behavior modeling, learning from experience, making coordinated decisions, and reasoning under uncertainty. Expected outcomes include strategies for human\/multi-robot teams that learn to collaborate effectively under a variety of conditions and can maintain their performance despite run-time changes in team membership, as well as knowledge about how people interact with robot teams. Broader impacts include providing access to a networked experimental testbed for remote collaborators; publishing proven curricular materials on multi-robot teams addressed to graduate, undergraduate and high school students; involving undergraduates in research activities; and working with existing contacts at local museums to demonstrate results to the general public.","title":"RI: Small: Collaborative Research: Learning to perform consistently in human\/multi-robot teams","awardID":"1117000","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["528943"],"PO":["565035"]},"181523":{"abstract":"Over time, a software system's architecture eventually deviates from the original designer's intent and degrades through unplanned introduction of changes that invalidate original design decisions. Architectural degradation increases the cost of making new modifications and decreases a system?s reliability, until engineers are no longer able to effectively evolve the system. At that point, the system's actual architecture may have to be recovered from the implementation artifacts, but this is a time-consuming and error-prone process, and leaves critical issues unresolved: the problems caused by architectural degradation will likely be obfuscated by the system?s many elements and their interrelationships, thus risking further degradation.<br\/><br\/>This collaborative project aims at pinpointing locations in a software system's architecture that reflect architectural degradation. The proposed research comprises four integrated research tasks: (1) Develop a catalog of commonly occurring symptoms of degradation. (2) Develop an architecture recovery technique that automatically extracts both a system's major building blocks and the concerns that influence, drive, and interact with these building blocks. (3) Devise a technique for formally capturing the recovered architectural design decisions, their involving concerns, and the identified causes of degradation. (4) Devise a suite of techniques that leverage the catalog to automatically identify system-specific instances of degradation. As a result, this project will have a potential for broad impact by providing a rigorous, scientific basis for software engineers to streamline the currently prohibitively expensive and error-prone system maintenance and evolution tasks.","title":"SHF: Small: Collaborative Research: Automating the Detection of Architectural Degradation in Software Systems","awardID":"1117593","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["551145"],"PO":["564388"]},"180434":{"abstract":"This project will create and demonstrate experimentally key features of a new optical network architecture, 'HyperFlow' which is a hybrid future Internet architecture that include key designs for both hardware and algorithms. The network is designed to be dynamic (both agile and adaptive), and significantly more cost effective and power efficient for future growth in data volumes and number of users. The architecture relies on a novel optical network infrastructure comprising new transport mechanisms and a new comprehensive control mechanism including the physical hardware, algorithms and applications. <br\/><br\/>Between locations that exchange large volumes of data - say, Los Angeles and New York City ? the flow switching mechanism of HyperFlow would establish a dedicated path across the network. The allotment of bandwidth would change constantly. As traffic between New York and Los Angeles increased, new, dedicated wavelengths would be recruited to handle it; as the traffic tailed off, the wavelengths would be relinquished. The goal is to develop network management protocols that can perform new session allocations in a matter of sub-seconds. HyperFlow can easily increase the data rates of optical networks 100-fold with the new network management scheme to be addressed in this program, with corresponding decrease in cost per bit and power consumption as well.<br\/><br\/>Intellectual Merit: HyperFlow creates a complete hybrid flow\/IP network architecture spanning local to wide areas, sharing the same unified control plane. The network provides both guaranteed end-to-end multi-Gbps flow service and conventional Internet Packet services throughout the network down to individual users. To achieve that goal, HyperFlow includes novel access and long haul core network technologies designed to support end-to-end flows as well as conventional IP network services. This approach achieves a breakthrough with respect to previous approaches by providing both services end-to-end with a unified control plane. HyperFlow includes major innovations based on highly efficient, agile and control of pools of shared tunable lasers and passive optical devices, a fast MAC, partitioned routing strategy and a new transport protocol for flows. <br\/><br\/>Broader impact: If successful the proposed HyperFlow technology will have a profound influence on the way the future Internet is designed and perceived by the society, as the new on-demand cost-effective gigabit service will enable applications that are hard or costly to support today with existing broadband access, such as instantly-available 3D distributed virtual-reality systems, cloud computing and instant file transfer services including telemedicine, education, 3D movies, concerts, sporting events, and government services. The concept of the new Hyperflow development will be brought into industry forums such as IETF\/IRTF during the project, so that the best minds of the industry can be leveraged for conceptual validation.","title":"NeTS: Large: Collaborative Research: HyperFlow - A Hybrid IP\/Optical Flow Network Architecture","awardID":"1111329","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["564819","564820","564821"],"PO":["564993"]},"181534":{"abstract":"Designing analog integrated circuits is more of an art than a science. Their continuous nature makes them difficult to both verify to be correct before fabrication as well as difficult to test to be free of faults after fabrication. This fact leads to design errors forcing costly re-spins (repetitions of the design and fabrication process) and even worse faulty parts being shipped to customers. In an attempt to address this problem, engineers are exploring the use of digitally-intensive analog circuits. In these circuits, designers use the simpler 0-1 binary digital assumption for the majority of the implementation, and they only use analog components when absolutely essentially. While this has some advantages, it creates new challenges as traditional verification and test methodologies for digital and analog design are extremely different. The project is attempting to address these challenges. In particular, the project is exploring the integration of both design-time verification as well as new built-in self-test and tuning techniques. This integration will allow not only joint enhancement of design correctness and robustness, hence a holistic guarantee of design quality, but also verification of self healing analog systems with built-in digitally-assisted test and tuning.<br\/><br\/>The broader impact of this work is that it will enable the design of nanoscale robust computing systems vital to a wide range of applications. Also, interdisciplinary explorations will provide new opportunities for solving research problems of practical significance and offer educational opportunities to make students well grounded in both theory and application. The PIs will promote the research participation from undergraduate students and students from underrepresented groups. The research outcomes of this work will be integrated into undergraduate and graduate curriculum and widely disseminated in the research community. The developed software computer-aided design tools will be released in the public domain.","title":"SHF: Small: Collaborative Research: Integrated Verification, Built-in Self-Test and Tuning for Digitally-Intensive Analog Systems","awardID":"1117660","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[486370],"PO":["562984"]},"190257":{"abstract":"The long-term goals of this project are to bring the technology of software contracts to widely-used programming languages and, through<br\/>the use of manifest contracts, to provide software developers with a migration path from simply-typed code to fully functional correctness. Since computational effects are notoriously hard to reason about and<br\/>pervade even the simplest realistic programs, the proposed research should have significant impact on programmers' ability to develop<br\/>software that is more reliable and more secure. <br\/><br\/>Contracts in software establish clear interfaces between program components. Like contracts in the legal realm, they delineate each<br\/>party's expectations and obligations. Such contracts are becoming increasingly important for the regulation of modern software systems,<br\/>providing an expressive framework for verification and error tracking. To be effective in a software environment, contracts must have formal semantics and must be supported by a monitoring system that precisely<br\/>tracks the flow of values as they cross interfaces. To date, however, the formal study of contracts has mostly been limited to small<br\/>idealized languages without computational effects, such as reading data from or writing data to a display or file, managing resources<br\/>such as memory, and performing probabilistic or speculative computation.<br\/><br\/>This research aims to extend the semantic framework of software contracts to languages with various computational effects: the<br\/>extension is qualitative in nature and will enable the use of contracts in new application domains. Specifically, the PIs propose<br\/>to add support for computational effects to the two flavors of contracts studied to date: latent contracts, which are runtime checks<br\/>not reflected in the type system, and manifest contracts, where a system of precise types records the most recent runtime check applied<br\/>to each value. The extension of latent contracts will be done in the context of a monadic meta language. The extension of manifest<br\/>contracts will make use of a variant of Hoare Type Theory to precisely record computational effects. The PIs will also implement prototype systems and use them to present novel applications of software<br\/>contracts.","title":"SHF: Small: Effectful Software Contracts","awardID":"1203008","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[509997],"PO":["564588"]},"185912":{"abstract":"The Network to support Science, Engineering, Arts and Design (NSEAD) will support transformative research and pedagogy that are only possible through the combined expertise of diverse knowledge domains and disciplines. For example, as physicists and engineers developed new imaging techniques, visual artists experimented with the new expressive potentials they enabled, often influencing development of the technologies. Visual artists and musicians have created and continue to create computer languages and algorithms while pushing technologies for composing and recording in fields of software engineering, artificial intelligence, graphics and visualization. Students who are involved in the arts have higher math, verbal, and composite SAT scores than students who are not involved in the arts. (Vaughn and Winner, 2000). There is a growing movement by higher education academic institutions in the United States to integrate the Arts and STEM disciplines to educate the whole student while leveraging creative cognitive skills for solving complex problems in science and technology disciplines. And finally, diverse ecosystem of academic programs in pre-K to gray formal and informal STEM learning; scientific research conferences; exhibitions, and cultural institution programs continues to emerge as new information technologies, creativity support and social networking tools become pervasive in our society. This project envisions a network that addresses fundamental challenges including the need to align academic pedagogies with 21st century thinking skills; to promote diversity of perspectives, approaches, and people in the creative information technology economy; and to benchmark best practices that create critical thinkers and leaders for the ever-changing technology-driven job market. The development of such a network will provide a platform to disseminate and generate public dialogue about the intellectual, cultural, and economic potential of intersections of science, technology and creativity. <br\/><br\/>NSEAD will be a platform to support the burgeoning research community of Computer Scientists, Engineers, Artists and Designers engaged in integrative research and pedagogy across these disciplines. NSEAD will provide a bridge for academic institutions, non-profit organizations, industry liaisons, and resource providers to collaborate, share best practices in research and pedagogy, and build stronger affinities. It will serve as a junction for elements such as: 1) research community development; 2) collaboration and project matchmaking opportunities; 3) skills expertise referrals; 4) inter-institutional collaborations; 5) forums to share best practices in pre-K to gray STEM learning and creative enrichment; and 6) strategies for network leadership and resource sustainability.","title":"Collaborative Research: EAGER: Network for Science, Engineering, Arts and Design (NSEAD)","awardID":"1142510","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[498959],"PO":["565342"]},"181446":{"abstract":"Wireless networking constitutes an important component of future information technology applications for both military and civilian purposes ranging from ad-hoc and sensor networks deployed in a battle-field to cellular networks with a very strong infrastructure. In order to design, implement and deploy highly reliable wireless networks, it is of utmost importance to address the presence of interference from other users in the system. In existing designs, interference is either avoided by a carefully chosen networking protocol (e.g. through time-division, space-division, frequency-division), or it is controlled to be near an acceptable level (e.g. through code-division). While simplifying the implementation, both of these commonly taken approaches are far from being optimal, and for reliable and efficient wireless networks of the future, a different look at the problem of handling the interference is needed.<br\/>This research aims to develop explicit and practical coding\/modulation solutions for multi-user communication channels, and to assess the possible gains with the enhanced physical-layer designs through end-to-end network simulation studies. In particular, a number of problems ranging from design of coding\/modulation schemes for wireless interference channels, and two-way relay channels, to practical signaling for finite-input finite-output multi-user links are addressed. Both long block length coding\/modulation solutions aiming at near capacity (or capacity bound) operation as well as short block length designs for delay sensitive applications are explored. Through these research activities, the project directly contributes to design and development of reliable wireless networks managing interference in an efficient manner, and to the field of general multi-user communications in a fundamental way. The project also integrates research and education generating many opportunities for undergraduate students to be involved in these activities.","title":"CIF: Small: Advancing State of the Art of Coding\/Modulation for Wireless Networks","awardID":"1117174","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":[486150],"PO":["564924"]},"181215":{"abstract":"Intellectual Merit: Data-intensive Scalable Computing (DISC) is increasingly important to peta-scale problems, from search engines and social networks to biological and scientific applications. Already datacenters built to support large-scale DISC computing operate at staggering scale, housing up to hundreds of thousands of compute nodes, exabytes of storage, and petabytes of memory. Current DISC systems have addressed these data sizes through scalability, however the resulting per-node performance has lagged behind per-server capacity by more than an order of magnitude. For example, in current systems as much as 94% of available disk I\/O and 33% of CPU remain idle. This results in unsustainable cost and energy requirements. Meeting future data processing challenges will only be possible if DISC systems can be deployed in a sustainable, efficient manner.<br\/><br\/>This project focuses on two specific, unaddressed challenges to building and deploying sustainable DISC systems: <br\/><br\/> -a lack of per-node efficiency and cross-resource balance as the system scales, and <br\/> -highly-efficient storage fault tolerance tailored to DISC workloads. <br\/><br\/>This project's approach is to automatically and dynamically ensure cross-resource balance between compute, memory, network, and underlying storage components statically during system design, as well as dynamically during runtime. The goal is to support general DISC processing in a balanced manner despite changing application behavior and heterogeneous computing and storage configurations. This work will result in a fully functional prototype DISC system supporting the Map\/Reduce programming model to support general-purpose application programs.<br\/><br\/>Broader impacts include:<br\/> -training diverse students, such as undergraduates and underrepresented groups - to understand DISC services as an interesting part of the overall curriculum and as a resource for interdisciplinary collaboration.<br\/> -a public release of the proposed balanced runtime system, including support for higher-level programming models;<br\/> -working with industrial partners as part of UCSD's Center for Networked Systems to address sustainability and efficiency issues in this critical portion of industrial and governmental data processing.","title":"CSR: Small: Highly Efficient, Pipeline-oriented Data-intensive Scalable Computing","awardID":"1116079","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[485593,"548364"],"PO":["551712"]},"181336":{"abstract":"Web pages and most other things created for computers are interactive, in that they respond to what people do with them. They have animations, buttons that cause information to change, and game characters that move around under a person's control. Creating these \"interactive behaviors\" has usually required writing programs, usually in a conventional programming language such as C++, JavaScript, or Adobe's Flash. However, this is a barrier to the vast majority of people who do not know how to program. In particular, there is a large class of people, often called \"interaction designers\", who are trained in how to make more aesthetic and usable interactive behaviors, but who are not professional programmers, and therefore cannot create these parts by themselves. Research shows that they do identify programming as the main barrier to creating and improving interactive behaviors. One reason that it is important that interaction designers be able to create the behaviors themselves is because by quickly creating, trying out and modifying the behaviors, they are able to creatively explore and develop new and better designs.<br\/><br\/>The ultimate goal of this research is to provide a new tool that enables interaction designers and other non-professional programmers to create systems with interactive behaviors in a more natural way. To achieve this, investigators will first study how designers and other people think about interactive behaviors. This will provide insight about how such behaviors can be expressed more \"naturally\", which means how a person can instruct a computer in a way that is close to the way the person is thinking about the desired result. Preliminary studies show that designers do not think about behaviors in the same way as professional programmers. Next, the investigators will use this knowledge about the natural expressions to create a new authoring tool which will make it much easier for designers to create their own interactive behaviors. The initial design for the tool uses techniques that are familiar to designers, such as the drawing model of programs such as Adobe Photoshop or Microsoft PowerPoint, the computation style of spreadsheets such as Microsoft Excel, and the event-based style (such as: \"when a bullet intersects a spaceship, then the spaceship should start the blowing-up animation\"), which has been found to be a natural way to express these behaviors. The result will be new knowledge and tools that will make programming more accessible to more people, and thus broaden the range of people who can program, while specifically enabling interaction designers to create their own behaviors.","title":"HCC: Small: Better Tools for Authoring Interactive Behaviors","awardID":"1116724","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["548127"],"PO":["565227"]},"181457":{"abstract":"AF: Small: Collaborative Research: Active DNA Assembly of Aperiodic Structures<br\/>PIs: N.C. Seeman, N. Jonoska<br\/><br\/>The science of computing and information processing is rapidly proliferating within established disciplines that at first glance appear unrelated. Examples include<br\/>biochemistry and biology through the areas of bioinformatics and algorithmic molecular assembly, or physics, through quantum computing. Current top-down photolithographic methods of electronic component construction are likely to encounter severe barriers as miniaturization requires features at very small sizes. An alternative approach to such construction is programmable bottom-up assembly on the molecular scale. The most popular basis for bottom-up assembly is DNA self-assembly, because DNA contains information through which assembly can be controlled in a variety of ways. This approach has been explored by a variety of laboratories in recent years, but was pioneered by the PI Seeman's laboratory roughly thirty years ago. Robust DNA motifs can be self-assembled into objects, lattices and devices, using the information content inherent in the DNA molecules (or their analogs) themselves. Notable milestones include the self-assembly of DNA polyhedra, the self-assembly of 2D DNA periodic lattices, the self-assembly of high-resolution 3D DNA crystals, the development of clocked and autonomous cascade-like DNA walkers, the development of DNA origami, and the construction of 1D and 2D algorithmic assemblies. <br\/><br\/>This project aims to combine several of these developments to produce robust aperiodic structures that will enable nano-scale construction of computational machinery. The goal of the project is to introduce a new paradigm in the self-assembly of nano-scale devices by DNA self-assembly: active (rather than passive) recursion-based assembly of aperiodic structures. The project will incorporate elements of DNA walkers into DNA origami-based tiles as signals that guide hierarchical active assembly of aperiodic arrays. <br\/><br\/>The proposed hierarchical algorithmic assembly would be the first demonstration of recursion in molecular self-assembly that can be seen as a physical incarnation of programmed recursion. It is known that self-similar and fractal-like structures have advantages in material design, heat exchange and information processing, e.g., the very significant advances obtained by miniature fractal-like antennas. Consequently, the possibilities for self-similar arrangements at the nano-scale can be expected to advance further the design of materials and electronics, while decreasing the costs involved in their construction.","title":"AF: Small: Collaborative Research: Active DNA Assembly of Aperiodic Structures","awardID":"1117210","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["554823"],"PO":["565223"]},"181578":{"abstract":"Network coding has demonstrated to be very effective in increasing wireless network capacity and reliability. However, the benefits of network coding are only applicable in networks consisting of reliable and trustworthy nodes. Non-negligible network coding (NC) noise and network pollution can cause substantial performance decrease. There is a cogent need for adaptive network coding schemes that can optimize network level throughput by exploiting NC noise and NC pollution. This research develops efficient adaptive model that can provide optimal network level throughput by designing of adaptive network coding schemes, pollution detection model, and joint treatment of coding and dynamic routing. The overall objective is threefold. First, it develops adaptive characteristic model and efficient estimation on the NC noise and NC pollution. Based on this, novel on-the-fly adaptive network coding schemes are developed. Second, this research develops theoretical characterization and effective pollution detection and immunization schemes in relay networks through source privacy protection. Third, this research establishes the mathematical structure for the wireless relay node, and designs cost function for optimal throughput computation. Based on this, provably optimal algorithms with polynomial time complexity are developed under the given pollution constraint and security requirement. In addition, this project also includes a significant education component aimed at integrating frontier research with undergraduate and graduate curricula.","title":"NeTS: Small: Adaptive Network Coding for Wireless Relay Networks","awardID":"1117831","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["523634","523780"],"PO":["557315"]},"181226":{"abstract":"We are confronted with richer, more detailed and more forms of data than ever before. Eigen-analysis based techniques constitute a powerful class of algorithms for discovering statistical signatures in mountains of data. Such techniques are used widely for the detection, estimation and classification of weak signals in a variety of applications that span the breadth of science and engineering such as gene microarray analysis, and climate science among others. This research will uncover the fundamental limits of these techniques and develop new techniques that can tease out weaker signals from large, noisy datasets. The results of this research will be disseminated broadly to advance relevant technology and clarify both advantages and limitations of eigen-analysis based dimensionality reduction.<br\/><br\/>Eigen-analysis based dimensionality reduction techniques are ubiquitous in statistical signal processing and machine learning applications. Their popularity is due to a sound theoretical justification, near-optimal computational complexity and strong performance guarantees in regimes where the techniques are known to work well. This research will provide a deeper understanding of when eigen-analysis based dimensionality reduction techniques fail so that these limitations can be potentially overcome. The project focuses on high-dimensional, noisy settings and uses random matrix theory as an enabling mathematical tool in this endeavor. The informational limits developed will provide a principled way of comparing eigen-analysis based algorithms with other techniques, and allow for an objective quantification of any performance losses due to fast implementations, measured in terms of how weak the signal or statistical signature is that can be identified. The research is interdisciplinary - powerful, new tools from random matrix theory are used to prove theorems and establish sharp, asymptotic performance bounds.","title":"CIF: Small: The Informational Limit of Eigen-Analysis Based Dimensionality Reduction, Learning and Classification","awardID":"1116115","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":[485620],"PO":["564898"]},"181468":{"abstract":"This project investigates computational techniques for modeling dissipative physical systems experiencing contact, impact, friction and plasticity. The attendant optimization problems are strongly nonlinear, nonsmooth, and nonconvex in nature, necessitating the development of novel numerical methods. The research grounds the development of these methods by building on discrete geometric mechanics and variational integrators, which restate the fundamental principles of physics in a discrete, hence immediately computable form. The research examines five core behaviors arising from the Principle (momentum\/energy\/symmetry preservation, breaking contact, and non-interpenetration) and thereby derives computer algorithms whose outputs capture these physical behaviors. The project success is measured by (a) the production of novel computer algorithms that are able to simulate dissipative physical phenomena more accurately and efficiently than ever before, (b) new theoretical results on what can be expected of computer algorithms that simulate these phenomena, and (c) the successful adoption of the novel techniques by industry partners. The project results are publicly disseminated via articles in journals, release of source code on the world wide web, and transfer of technological expertise and data to industrial partners. <br\/>Improving computational techniques for the simulation of contact, impact, and dissipative phenomena helps make engineering safety analyses, biomechanical models, computer visualizations, surgical training tools, and industrial manufacturing simulations that better predict reality. Algorithms that accurately capture the physics help to gain important new insights into many open questions that influence our understanding of large-scale geophysics of earthquakes and calving of icebergs, small-scale dissipation in high-frequency micro- and nano-electronic mechanical devices, prosaic domestic phenomena such as the chattering of chalk on a board and even the excitation of violin strings.","title":"CGV: Small: Discrete Variational Contact, Impact, and Dissipative Dynamics","awardID":"1117257","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["554341",486204],"PO":["564316"]},"185868":{"abstract":"This project seeks dramatically improved access to, and dissemination of, scientific information. Working with cooperating scientific users, it exploits synergies among three important innovations. These are: (1) adaptive and domain specific automatic derivation of topical representations. These topics describe both the documents in the collection, and the interests of the users, during particular searches. The topics support mechanisms for collaborative recommendation, and for exploring the precise contours of each user,s need. (2) Recognition that a combination or set of several items, together, is worth much more (or perhaps much less) than the sum of the values of the items individually. The arXiv experimental system (arXiv_XS) uses topics, and user feedback, to model the complexity of the user's need and interests. (3) Based on these innovations, the system can probe user's interest, selecting items where the user's feedback greatly improves the system's model of that user and his or her search. This \"exploration\" is designed to improve the systems performance, with minimal degradation of the current search. All these innovations are studied together with complex experimental design and statistical analysis; users may also volunteer to be interviewed, by the researchers, to provide richer information about their experiences with the system. Researchers from Rutgers, Cornell and Princeton lead the project.<br\/><br\/>This exploratory project focuses on the following tasks: (1) develop a richly instrumented voluntary alternative interface to the arXiv, with suitable IRB consent materials supporting active user feedback in the research process, as users search; (2) implement three specific innovative technologies (topics, sets, probes); (3) study their impact on system effectiveness, using experimental design and well-defined performance measures; (4) collect rich user assessments, by telephone and online interviews; (5) assess scalability with respect to the size of the collection, and the size of the \"communities of interest\" that define the topical user models; (6) seek relations at other domain-specific archives, for potential future studies. If successful, this research will refute a perception that improvement in access and dissemination of scientific literature requires massive techniques adapted from the commercial models for recommender systems and crowd-sourcing. This research will also add to on experimental design, user modeling, and the study of active learning and exploratory system designs.<br\/><br\/>This research will accelerate the production and sharing of scientific information, initially at the arXiv, and subsequently, wherever these innovations are implemented. The research aims to enable researchers who never meet each other to form an \"invisible college\" by enriching the arXiv systems understanding of all of its users. The project entails some risks, as users may be unwilling to share information about their research interests. While malevolent persons might seek to spam the system, falsely marking information as useful, it is anticipated that scientific communities will generate far less spam than does the world at large. Results of the research will be made available to other researchers, and incorporated in courses at all three universities. The Web site (http:\/\/arxiv_xs.rutgers.edu) is used to disseminate information and results from this project.","title":"EAGER: Adaptive Methods for Scalable Dissemination and Retrieval of Scientific Information","awardID":"1142251","effectiveDate":"2011-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["210763","531640","534602","531625","531624"],"PO":["563751"]},"181149":{"abstract":"Operational and safety goals for the built environment demand robust, scalable and reliable large<br\/>scale monitoring for infrastructure systems. High performance real-time event detection and decision making<br\/>requires models and algorithms to process large amounts of data from dense sensor networks deployed<br\/>in these systems. Despite advances in the development of detection algorithms for such networks, there are<br\/>two widely recognized and conflicting obstacles: detection rules need to be sufficiently complex to adapt to<br\/>the spatiotemporal changes in the environment, requiring the sharing of data; but rules are constrained by<br\/>statistical performance guarantees and computation and communications budgets imposed by the network.<br\/>This project addresses these challenges by developing a fundamentally new approach that jointly accounts<br\/>for statistical detection, communication constraints and distributed computation.<br\/>This research develops a framework that integrates the distributed computation and communication constraints<br\/>of the underlying network infrastructure with flexible stochastic modeling and learning algorithms<br\/>with spatiotemporal data. The modeling and algorithms enable simultaneous and sequential decision making<br\/>at many local sites, by borrowing information across the network in a statistically coherent and computationally<br\/>efficient manner. Combining the formalism of sequential change point detection, nonparametric and<br\/>probabilistic graphical models and spatiotemporal statistics, the project develops distributed and sequential<br\/>message-passing algorithms for detecting changes in the underlying distributions generating network data.<br\/>The models developed also offer new theoretical understanding of the trade-offs between statistical model<br\/>complexity, distributed computation efficiency, and structure of communication constraints within the network.<br\/><br\/>This interdisciplinary research brings together students and researchers from different areas, utilizing<br\/>and developing knowledge and cross-disciplinary skills in the fields of computer science, statistics, signal<br\/>processing and civil engineering.","title":"CIF: Collaborative Research:Small: Distributed Detection Algorithms and Stochastic Modeling for Large Monitoring Sensor Networks","awardID":"1115769","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["563031"],"PO":["564898"]},"186528":{"abstract":"This grant supports the 2nd CPS National Principal Investigators Meeting, held August 1-2, 2011, at the Gaylord National Hotel, National Harbor, Maryland. The purpose of this meeting is to review research progress, enhance research interaction among projects funded under the NSF Cyber-Physical Systems program, and to provide a forum for stakeholders in academia, industry and federal agencies to review new developments in CPS foundations and domains. It provides an opportunty for the CPS community to promote new, emerging applications, and to collectively identify technology gaps and barriers.","title":"CPS Principal Investigator Meeting 2011","awardID":"1145923","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["563656"],"PO":["561889"]},"190511":{"abstract":"CAREER: Adaptive Power Management for Multiprocessor System-on-Chip<br\/><br\/>Multiprocessor System-on-Chip (MPSoC) is becoming a major VLSI system design platform due to its advantages in low design cost and high performance. However, power consumption in MPSoC is a crucial factor that is limiting the growth of system performance and functionality. The complexity of the MPSoC hardware and software imposes new challenges and requirements for research in system-level power management.<br\/>An effective power manager must be aware of the status of the hardware, the application and the working environment and be able to adapt to the changes. It should be able to work robustly even if the perfect system information is not available. As the number of components that can be power controlled increases, it is increasingly difficult to perform power management in a centralized manner. A hierarchical and distributed power management method is more suitable for MPSoC platforms. Finally, resource management, power management, and thermal management are inter-correlated tasks and it is desirable for them to be optimized simultaneously.<br\/>This research project addresses the above mentioned challenges by investigating the theoretical foundation and the applied framework of adaptive power management for the next generation MPSoC. This project consists of four research components: (1) investigate online modeling techniques for runtime workload prediction and hardware performance\/power characterization; (2) research new optimization techniques for adaptive resource and power management in a partially observable system; (3) model the distributed power management problem as a multi-agent cooperative game and develop control policy using game theory; (4) develop a unified and standard platform for modeling, optimization and evaluation of power-managed MPSoC.<br\/>The educational components of this project will introduce the students to the implementation and optimization techniques of system-level power management and provide students unique hands-on experience with MPSoC design and optimization.","title":"CAREER: Adaptive Power Management for Multiprocessor System-on-Chip","awardID":"1203986","effectiveDate":"2011-08-23","expirationDate":"2014-05-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["556758"],"PO":["565255"]},"180996":{"abstract":"WDM (wave-length multiplexing) Optical networks form the critical backbone of all modern communication networks and systems. Therefore, there has been a great deal of interest in the fault tolerant design and operation of these networks. In the past decade or so several fundamental advances relating to the fault tolerance, protection and restoration issues have appeared in the literature. Most of these advances have been in the context of failures in one layer. But, modern communication systems consist of multiple physical implementations communicating via layered protocols. As such, a single failure at one layer may lead to cascading failures, i.e., failures at the physical layer lead to failures at the logical layer. Research in this area of cross-layer survivability is still in its infancy. In this basic research project the principal investigators will carry out a study of network survivability across layers to deal with cascading failures in layered networks. The research will be in the context of IP-over-WDM Optical networks. The focus will be on multiple failures in the physical (optical) layer and their consequences at the higher layer, namely the IP layer. Specifically, the broad scope of the project will cover i) survivable logical topology mapping under multiple failures, ii) Logical topology mapping for guaranteed survivability, iii) Logical topology mapping under multiple constraints, and iv) A generalized theory of flows across layers, capacity of survivable logical topologies and related algorithmic challenges.<br\/><br\/>The project seeks to develop unifying theories and methodologies that will make significant advances to our understanding of cross-layer survivability issues, and providing the theoretical foundation for future advances in the general area of cross-layer design and optimization. These theories will be based on modern advances in graph theory, mathematical programming (e.g., network interdiction) and algorithm design. The principal investigators will develop innovative algorithmic techniques based on advanced data structures and computer algorithms such as approximation techniques as well as a generalized theory of cross layer flows that will go well beyond the widely used classical theory of single layer flows. <br\/><br\/>Broader Impact: Although IP-Over-WDM networks will provide the context for the research, the theory of cross-layer flows and the algorithmic (in particular approximation) techniques that will be developed will have multidisciplinary value spanning computer science, electrical engineering, graph theory and mathematical programming. The research will also have significant educational value in training highly skilled researchers for research and development in cutting edge technologies in different areas of information technology.","title":"NeTS: Small: Collaborative Research:Cross Layer Survivability to Cascading Failures in Layered Networks","awardID":"1115129","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["517891"],"PO":["564993"]},"181502":{"abstract":"A reliable system for automated image understanding can have immense impacts on many applications including image search, video surveillance, autonomous vehicles, and robotics, to name a few. However, the progress of the technology has been slow compared to text understanding and speech recognition. The problem can be attributed to lacking ways of breaking an image into a set of meaningful components analogous to words in text and speech processing. This research will develop an algorithm to partition a digital image into such meaningful components efficiently and effectively. To reach the goal, the investigators focus on discontinuities in color and brightness often called \"edges\" and study algorithmic ways to group them and delineate objects found in the image. Undergraduate students will actively participate in interdisciplinary research involving computer science, mathematics, statistics, psychology and biology. The PI will also develop an interdisciplinary course that integrates cognitive, neuro, and computer sciences. The resulting source code, software tools, data, and visual materials will be made publicly available to promote STEM education.<br\/><br\/>More specifically, the study centers on two recent innovations developed by the investigators: semi-group smoothing with a matrix of linear filters, and successive partitioning of a graph with increasing complexity. The former is an affine commutative linear operator that is shown to be effective in extracting high-curvature points while robust against aliasing. It will be used to smoothen, partition, and characterize contour fragments. The latter generates a set of closed contours surrounding a focal point successively from a simple shape to more complex ones. Furthermore, the study investigates new context sensitive perceptual saliency metrics that quantify the importance of edges based on their surroundings.","title":"CIF: Small: RUI: Fixation-Driven Contour Integration of Natural Images for Early Visual Processing","awardID":"1117439","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":[486288],"PO":["564898"]},"181623":{"abstract":"Inexpensive multi-core processors and many-core GPUs present tremendous opportunities as well as serious challenges for software developers. Developing concurrent programs is intrinsically difficult because multi-threading introduces a whole new class of errors that do not exist in sequential programs. This problem is exacerbated when developing and debugging large-scale, data-intensive, and computation-intensive programs. Traditional testing and debugging techniques are not appropriate for multi-threaded programs which may behave differently from one run to another because threads are scheduled non-deterministically.<br\/><br\/>This project develops a toolkit to detect correctness and performance problems on shared memory systems with the following techniques. (1) Exploit the benefits of static and dynamic analyses while avoiding their shortcomings. Specifically, extend dynamic analysis by augmenting it with static analysis to systematically explore program code for error detection and prevention. (2) Investigate different optimization approaches to lower runtime overhead and improve the toolkit's scalability. (3) Design a unified framework that can predict potential errors and enforce the scheduler to avoid the errors by manipulating accessing orders.<br\/><br\/>The success of this project may enhance the dependability of parallel computing systems and help design more reliable multi-threaded programs. Research results will be integrated into the teaching of undergraduate and graduate courses such as operating systems, parallel programming, and compiler design.","title":"CSR:Small: Towards Reliable Concurrent Computing Using Hybrid Program Analysis","awardID":"1118059","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[486603],"PO":["551712"]},"180534":{"abstract":"Many compelling applications involve computations that require sensitive data from two or more individuals. For example, as the cost of personal genome sequencing rapidly plummets many genetics applications will soon be within reach of individuals such as comparing one?s genome with the genomes of different groups of participants in a study to determine which treatment is likely to be most effective. Such comparisons could have tremendous value, but are currently infeasible because of the privacy concerns both for the individual and study participants. What is needed is a way to produce the result of the comparison without exposing either party's private inputs. The ultimate aim of this project is to make privacy-preserving computation practical and accessible enough to be used routinely in applications such as personalized genetics, medical research, and privacy-preserving biometrics.<br\/><br\/>Theoretical solutions to this problem, known as secure multi-party computation, have been known for several decades, including a general solution developed by Andrew Yao based on garbled circuits. Because of its extensive memory use and computational cost, however, the garbled circuits approach has traditionally been considered more of a theoretical curiosity than a practical mechanism for building privacy-preserving applications. Recent developments in cryptographic techniques and new implementation approaches are beginning to change this, however, and admit the possibility of scalable, practical secure computation. This project is designing methods for avoiding the memory bottleneck associated with garbled circuit evaluation by aggressively pipelining circuit generation and evaluation, and exploring a variety of techniques for reducing the size of garbled circuits. Another issue the limits the use of secure computation in practice is the need for standard protocols to assume an honest-but-curious adversary who always follows the specified protocol. This project is developing new techniques for dealing with malicious adversaries, improving the standard cut-and-choose and commit-and-prove approaches by using new cryptographic tools and exploring an alternate model in which a verifiable trusted party generates the circuit but is not trusted with any private data. The project is also developing techniques to audit the information that can be inferred from the result of a secure computation. Another goal is to make secure computation more accessible to developers by developing programming tools for defining secure computations at a high level, based on information-flow analysis and program partitioning.","title":"TC: Large: Collaborative Research: Practical Secure Two-Party Computation: Techniques, Tools, and Applications","awardID":"1111781","effectiveDate":"2011-08-15","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[483825,"486395",483827],"PO":["564388"]},"181634":{"abstract":"Data cleaning technologies, traditionally designed to improve quality of data in back-end data warehouses, are fast emerging as a vital component of real-time information access. As the Web evolves towards supporting interactive analytics and basic search migrates from simple keyword retrieval to retrieval based on semantically richer concepts (e.g., entities) extracted from web pages, the need for \"on-the-fly\" cleaning techniques that can help alleviate data quality challenges is rapidly increasing. This project explores three new innovations that will help advance data cleaning towards becoming an embedded enabling technology for real-time information access. The first innovation is \"query-aware data cleaning\" which is based on the observation that the specificity of the real-time task such as a query can be exploited significantly to bring new optimizations to the data cleaning process. The second innovation is a data cleaning framework that migrates from the \"best-effort\" adhoc setup of today's systems into a principled approach that exposes and exploits a fundamental tradeoff between the cost of cleaning and quality of results achieved. Finally, since results of cleaning need to be fed to the end-user or analysis code, the proposal postulates and addresses approaches towards how results processed through data cleaning code can be presented to the end-recipient. The primary contribution is mechanisms to hide the uncertainty in the data and determinize the results while maximizing the end application goals. <br\/><br\/>The proposed research is intended to bring transformative improvements in interactive analytics and search on the web by facilitating real-time data cleaning and data quality enhancements. The project also aims to benefit the research community by incorporating mechanisms developed as part of this research into the Web People Search Technology (WEST), enabling WEST to become a real-time on-the-fly web people search tool. The goal is to support WEST as a plug-and-play system wherein other researchers could embed and test their data cleaning algorithms and tools. Finally, the planned research, system development, and educational activities are going to significantly enhance the educational experience of students, preparing them for a brighter future in the today's knowledge driven society.<br\/><br\/>For further information see the project web site at the URL: http:\/\/sherlock.ics.uci.edu","title":"III: Small: Query and Goal Driven Entity Resolution Framework","awardID":"1118114","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["515756",486631],"PO":["563727"]},"181524":{"abstract":"Broadband cellular networks are emerging to be the most common means for mobile data access worldwide. Predictions from industry analysts indicate that the volume of data through cellular data networks will increase exponentially in near future. Understanding of the mobile data traffic via measurement and analysis is critical for the development of resource management techniques for these networks. While spectrum resources are of great concern, this project specifically focuses on the energy required to operate the cellular network infrastructure, specifically base stations. The project undertakes a significant modeling exercise with two goals. One goal is intellectual, driven towards understanding the spatio-temporal dynamics of mobile traffic and discovering possible structure or relationships. The project uses state-of-the-art machine learning tools to develop models using large-scale data collected directly from the operator's networks. Such modeling will bring new insights that in turn will help to deploy and manage future generation cellular data networks. The second goal is utilitarian. Here, techniques are developed to predict base station loads for use in resource management, specifically energy. Algorithms are designed to exploit energy-optimization opportunities to turn off specific network resources based on the forecasted load.<br\/><br\/>The project has significant broader impact. It develops technologies to appreciably reduce energy consumption in cellular networks. Overall, this exercise will both reduce cost, and contribute to the environment. The project also contributes to several green initiatives in both institutions and to the education and training of graduate students.","title":"NeTS: Small: Collaborative Research: Understanding Traffic Dynamics in Cellular Data Networks and Applications to Resource Management","awardID":"1117597","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[486344],"PO":["565303"]},"180446":{"abstract":"This project will create and demonstrate experimentally key features of a new optical network architecture, 'HyperFlow' which is a hybrid future Internet architecture that include key designs for both hardware and algorithms. The network is designed to be dynamic (both agile and adaptive), and significantly more cost effective and power efficient for future growth in data volumes and number of users. The architecture relies on a novel optical network infrastructure comprising new transport mechanisms and a new comprehensive control mechanism including the physical hardware, algorithms and applications. <br\/><br\/>Between locations that exchange large volumes of data - say, Los Angeles and New York City ? the flow switching mechanism of HyperFlow would establish a dedicated path across the network. The allotment of bandwidth would change constantly. As traffic between New York and Los Angeles increased, new, dedicated wavelengths would be recruited to handle it; as the traffic tailed off, the wavelengths would be relinquished. The goal is to develop network management protocols that can perform new session allocations in a matter of sub-seconds. HyperFlow can easily increase the data rates of optical networks 100-fold with the new network management scheme to be addressed in this program, with corresponding decrease in cost per bit and power consumption as well.<br\/><br\/>Intellectual Merit: HyperFlow creates a complete hybrid flow\/IP network architecture spanning local to wide areas, sharing the same unified control plane. The network provides both guaranteed end-to-end multi-Gbps flow service and conventional Internet Packet services throughout the network down to individual users. To achieve that goal, HyperFlow includes novel access and long haul core network technologies designed to support end-to-end flows as well as conventional IP network services. This approach achieves a breakthrough with respect to previous approaches by providing both services end-to-end with a unified control plane. HyperFlow includes major innovations based on highly efficient, agile and control of pools of shared tunable lasers and passive optical devices, a fast MAC, partitioned routing strategy and a new transport protocol for flows. <br\/><br\/>Broader impact: If successful the proposed HyperFlow technology will have a profound influence on the way the future Internet is designed and perceived by the society, as the new on-demand cost-effective gigabit service will enable applications that are hard or costly to support today with existing broadband access, such as instantly-available 3D distributed virtual-reality systems, cloud computing and instant file transfer services including telemedicine, education, 3D movies, concerts, sporting events, and government services. The concept of the new Hyperflow development will be brought into industry forums such as IETF\/IRTF during the project, so that the best minds of the industry can be leveraged for conceptual validation.","title":"NeTS: Large: Collaborative Research: HyperFlow - A Hybrid IP\/Optical Flow Network Architecture","awardID":"1111383","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[483597],"PO":["564993"]},"190379":{"abstract":"This five-year CAREER proposal aims at designing, evaluating, and implementing a learning-by-collaborating system that provides haptic, visual, and auditory feedback to students with and without visual impairments to work together in hands-on science learning opportunities. The setting and sample of the proposed study includes sighted and visually impaired students (N=120) from the Arkansas School for the Blind, the Virginia School for the Deaf and Blind, the Kansas State School for the Blind, and the Fayetteville (Arkansas) High School. The hypothesis that a haptically enhanced learning-by-collaborating system may increase the science learning of both sighted and visually impaired students guides the study and its four research questions: (1) To what extent does additional haptic feedback during collaborative hands-on practice influence students' learning performance?; (2) To what extent does additional haptic feedback during collaborative hands-on practice influence students' attitudes towards science learning?; (3) How does additional haptic feedback during collaborative hands-on practice influence students' motivation to learn?; and (4) What are the relationships among learner motivation, learning attitudes, and learning performance? <br\/><br\/>The study employs a research and development design consisting of three stages. The first, Synthesis and Application, allows the PI to identify the set of points of collaboration that need to be supported haptically, audibly, or both, through two group sessions with three pairs of totally blind and partially blind students per session. The second, Development and Formative Evaluation, facilitates the development of two modules on the Nature of Light (Electromagnetic Waves, and Vibrating Charges), as well as the modification of the Molecular Properties, and Heat and Temperature modules, already designed for visually impaired students only. A Design for Co-Touch software framework is used for this purpose, and quality of the interaction techniques will be assessed using internationally established standards for usability, effectiveness, efficiency, and satisfaction. All materials that visually impaired students use are provided in Braille. <br\/><br\/>The PI will investigate the cognitive and affective impacts of shared haptic experiences on students' science learning through the third stage, Summative Evaluation of Shared Haptic Experiences. A total of 30 pairs of visually impaired and 30 pairs of sighted students participate during this stage. The study employs a two-level-between-subjects condition to manipulate sensory feedback: visual + auditory (students will receive visual and verbal instructions on science concepts) vs. visual + auditory + haptic (students will receive haptic feedback in addition to visual and verbal). This research stage utilizes Campbell & Stanley's (1966) pretest-posttest control group design in which participants are randomly assigned to one of the two groups. <br\/><br\/>Instruments to be used in the study will be developed or modified and pilot-tested to determine their validity and reliability. To measure learning performance, two tests will be developed: (a) a recall and recognition test using Bloom's taxonomy, and (b) a transfer test to assess learners' ability to integrate and apply knowledge. To measure learning attitudes, the Test of Science-Related Attitudes (Fraser, 1981) will be used. To measure learning motivation, Keller's (2007) Instructional Materials Motivation Survey will be utilized. Statistical analysis (e.g., ANCOVA) will be used to control any initial differences in pretest scores between the groups. Multiple regression will be performed with the learning performance score (the sum of recall and transfer tests) as the dependent variable. Independent variables are learners' attitudes and learners' motivation.","title":"CAREER: WE FEEL SCIENCE: We Engage with the Flexible, Experimental Environment for Learning in SCIENCE","awardID":"1203450","effectiveDate":"2011-08-15","expirationDate":"2016-02-29","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7645","name":"DISCOVERY RESEARCH K-12"}}],"PIcoPI":[510331],"PO":["549236"]},"181216":{"abstract":"A variety of applications in the nascent field of cloud computing and in other fields requires stable, low-delay network connectivity together with capabilities of co-scheduling network and server resources. As a result, several network providers have recently started to deploy scheduled dynamic circuit services (SDCS). Under such services, a network provider allows its customers to place scheduling requests for a fixed-rate circuit lasting for a fixed duration, for either earliest possible usage or scheduled usage at a desired point of time in the future.<br\/><br\/>While carrying significant potential impact, SDCS currently provide little support for dynamic routing and scheduling of circuits across different administrative domains. This lack of adequate support for inter-domain routing represents a major hurdle to the full-scale deployment of SDCS. The main objective of this project is to tackle this challenge by devising distributed solutions supporting scheduled dynamic circuit services across domains. The project consists of two inter-related thrusts: (i) theory, which focuses on the problem of devising distributed routing algorithms for SDCS that provably converge and provide performance guarantees on user-performance metrics; and (ii) protocol design and implementation, which focuses on the design, development, and real network prototyping of a new inter-domain routing protocol for SDCS, referred to as the scheduled circuit routing protocol (SCRP).<br\/><br\/>Broader Impact: Successful completion of this project promises to impact a wide array of high-throughput and real-time applications belonging to medical, scientific and commercial domains, and facilitate the creation of a new ecosystem for communication networks. The project is further striving for high impact through collaborations with international academic and industrial partners, and network operators. Students at all levels will be engaged in real network experiments and in the development of innovative applications enabled by SDCS, such as haptic-based communication.","title":"NeTS: Small: Collaborative Research: Boosting Inter-Domain Scheduled Dynamic Circuit Services (SDCS)","awardID":"1116081","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["564799"],"PO":["565090"]},"181337":{"abstract":"We face a future in which computational resources at the processor level are \"free\" with hundreds or thousands of cores, yet we have little idea how to utilize these resources except for well-structured and highly parallel applications. This research aims, in the long term, to provide programmers with the same computing power that Nature exploits to seemingly solve difficult problems efficiently. The dominant programming model in Nature is that of a massive number of \"cheap\" computing elements collaborating by message passing. One such strategy to solving complex problems is to speculatively pursue many possible solutions in parallel, discarding (partial) computations that lack promise or violate necessary constraints. Unfortunately concurrent speculative algorithms are challenging to develop because of the intermingling of execution paths with concurrency and communication. The goal of this project is to take significant steps towards a theory of speculation for concurrent algorithms and to develop an experimental framework for their development. This research will enable the study of novel applications to utilize the vast computational resources that future processors seem destined to provide.<br\/><br\/>This project is inspired and informed by recent work on reversible computing which is itself inspired by reversibility in the laws of Physics. Research on reversible concurrency, while illuminating key properties that a system must satisfy (e.g. causal unwinding of communication), has neither yielded models that can reasonably be implemented in a distributed environment nor provided necessary details for a practical language. This project will develop concurrent programming languages that support explicit speculation in concurrent systems using the ideas from reversible concurrent programming to factor out the mechanisms used to realize speculation from speculative algorithms. The project will also leverage ideas from backtracking monad and monad transformers to isolate the interactions between speculation and computation effects including communication, and ideas from process algebras to develop a model for understanding language constructs supporting speculative execution. The research includes experimental work to implement and test linguistic constructs and theoretical work to provide both formal models for these constructs, and algebraic tools to enable reasoning about programs that utilize them.","title":"SHF: Small: Reversible Concurrency","awardID":"1116725","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["565319","517796"],"PO":["565264"]},"181579":{"abstract":"The PI's goal in this research is to develop tools on a state-of-the-art platform (the Apple iPad) that will afford access to textual information for individuals who are blind or who suffer from severe visual impairments (IBSVI). The PI's approach is to use an embossed screen overlay to provide spatial and tactile correlates to text read aloud, and to engage the spatial cognition and memory resources of the target population for navigating through a document and annotating it if\/as desired. The PI argues that from the invention of print media forward, information has been formulated and optimized for consumption by beings (people) with a dominant visual capability. This visuo-spatial bias is not well-understood or studied in the context of information access by and delivery to the IBSVI community; most technological information aids funnel information to them as sequential aural streams, obviating the use of broader spatial cognitive resources. In this project the PI will develop a Spatial Touch Audio Annotator and Reader (STAAR) testbed to explore a multimodal alternative that enables the user to fuse spatial layout and informational content through touch location on a slate-type device and audio rendering of text to speech, respectively. STAAR will enable self-paced reading using a tactile overlay pattern on an iPad surface, which will be designed to provide tactile landmarks to help the user navigate the \"page.\" STAAR will render the text chunk touched audibly. The use of touch gestures to enable contextualized highlighting and note-taking will also be investigated. The PI will study how the target population may employ spatial strategies and exploration to re-find and re-access information both in the act of reading and for recall after some time interval. <br\/><br\/>Broader Impacts: A new generation of slate-type devices exemplified by the Apple iPad threatens to widen the accessibility gap between the IBSVI community and the majority of the population. By supporting the use of spatial cognitive and memory resource for both reading and contextualized annotation, the PI hopes to ameliorate this endemic barrier to participation. Project outcomes will contribute to our understanding of the role of space in information design and representation, and the spatial cognition and memory resources needed for uptake of such information. And they will contribute to the domain of mobile computing, for the IBSVI community in particular but ultimately for the population in general, through the STAAR system, which will be designed and implemented from the ground up as a portable device on a state-of-art interaction form-factor. The multimodal fusion of haptics and speech in STAAR will have implications for the designs of such things as navigational aids and service delivery systems for the non-sighted as well as the sighted. The project will in addition provide unique cross-disciplinary educational and learning opportunities for undergraduate and graduate students. All software developed in this project will be placed in open source.","title":"HCC: Small: STAAR: Spatial Touch Audio Annotator and Reader for Individuals with Blindness or Severe Visual Impairment","awardID":"1117854","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["505416",486496,486497],"PO":["565227"]},"185704":{"abstract":"Collaborative Projects: EAGER: A virtual eXchange to support networks of creativity and innovation amongst Science, Engineering, Arts and Design (XSEAD)<br\/><br\/>Intellectual Merit<br\/>One of the greatest challenges facing the United States in research and education is how to fundamentally encourage innovation across all sectors and spawn new solutions to address global challenges. Increasing research evidence and industrial innovations (i.e. mobile computing, social media) confirm that broad interdisciplinary collaborations that include both science and art fields have great potential for spawning creativity and innovation in computer science, engineering and the sciences. An emerging hybrid community of scientists, engineers, artists and designers is producing innovative and entrepreneurial research that advances new knowledge and proposes holistic solutions to societal challenges including health, education and environmental change. Yet, this burgeoning interdisciplinary community continues to face problems in its efforts to self-organize among constraints imposed by academic systems and historical biases; it continues to seek a dynamic and synergizing research and outreach exchange.<br\/><br\/>Building upon lessons-learned, a new Virtual eXchange to support networks of creativity and innovation amongst Science, Engineering, Art and Design (XSEAD) will be developed. The XSEAD project will address the following urgent needs of the interdisciplinary science-art community: establish a cohesive view of the field and provide a mechanism to attract entrepreneurs and industry; create a venue for multimodal documentation of research outcomes; provide extensive databases of prior and current research; allow rapid dissemination of research outcomes; facilitate forming of collaborations and specialized sub-communities; document and help evolve science-art curricula efforts and evaluation approaches; provide context and support mechanisms for science-arts careers; establish evidence of the societal impact of interdisciplinary science-art integration. The software engineering development components of XSEAD will contribute further knowledge in three technical areas: Content organization (improve the effectiveness of algorithms for dynamic, usage based, organization of large multimedia databases); Recommendation algorithms (promote the use of multi-relational structures for providing effective recommendations); Community dynamics (develop novel algorithms to extract structures that encode meaningful interactions in online social networks).<br\/><br\/>Broader Impact<br\/>XSEAD will expose general non-expert audiences to the evolution and potential of collaborative research across science and arts. It will attract the interest of young people searching for careers that combine the rigor of science and engineering with the creativity and reflection of arts and design. It will serve teachers and informal learning communities seeking exemplars for curricular development, active practitioners looking for further institutional opportunities to present and support their ongoing work, academics developing related interdisciplinary efforts and commercial companies seeking cross-trained expertise. XSEAD will enable rapid research exchange and in-depth peer-reviewed scholarship between the worlds of science and art and provide a unique and deeply engaging inroad to a vast and creative repository. XSEAD will help promote new paradigms for developing human centric solutions to complex societal problems (i.e. cost effective health and wellness, globalization and conflict, adaptive K-12 learning, electronic communication and security). These paradigms will combine knowledge across broad and diverse areas of human knowledge.","title":"Collaborative Research: EAGER: A Virtual eXchange to Support Networks of Creativity and Innovation Amongst Science, Engineering, Arts and Design (XSEAD)","awardID":"1141480","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["550695"],"PO":["565342"]},"185946":{"abstract":"The Acoustic-Articulator Modeling EArly Grant for Exploratory Research (EAGER) investigates the use of three-dimensional Electromagnetic Articulograph (EMA) technology to create improved pronunciation error analysis and corrective articulatory feedback. This improved EMA technology is a recent development, and there are no speech or language data sets currently available that include such high resolution three dimensional position\/orientation information. This EAGER investigates the potential of this type of data. A matched acoustic-kinematic corpus with five degree of freedom EMA data is being collected, across both native and nonnative (Mandarin Chinese) speakers of English. The project investigates methods for normalizing speakers' articulatory data into a common reference frame and disseminating the data to the broader research community to promote continued work in this area.<br\/><br\/>Current tools for pronunciation assessment are very limited in the specificity of the corrective pronunciation feedback that can be provided, due to limitations of our ability to accurately map acoustic inputs to corresponding articulator patterns and obtain detailed analysis of the manner and degree of pronunciation errors. The development of effective tools for Computer Aided Language Learning (CALL) is an essential component of enabling nonnative speakers of English to quickly integrate into America's 21st century global workforce. Better tools for pronunciation assessment and accent modification can have tremendous impact on workforce effectiveness, especially within key sectors related to medicine, science, technology, engineering, and mathematics, where it is important to continue to attract and support top talent from across the globe.","title":"EAGER: Acoustic-Articulator Modeling for Pronunciation Analysis","awardID":"1142826","effectiveDate":"2011-08-01","expirationDate":"2014-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":["551034","551035"],"PO":["565215"]},"181227":{"abstract":"This project addresses energy issues in sensor systems. Energy harvesting permits wireless sensor network (WSN) applications to extract and store energy from the environment. Certain evolving application classes, particularly those supporting critical infrastructures like long-term monitoring and surveillance systems, require higher levels of communication and computational performance. Often, systems in this application class must be able to provide robust and flexible responses to unforeseen or emergency situations. Many of these proposed applications will also be required to run autonomously for years or even decades, and therefore may require energy harvesting techniques. <br\/><br\/>Current methods for WSN-based energy harvesting typically do not provide a coordinated energy management policy for this class of applications. This project addresses this gap in three ways. First, the team is designing and evaluating a set of optimal energy harvesting policies that combine dynamic voltage scaling (changing the CPU speed) with dynamic modulation scaling (changing wireless transmission rates.) The algorithms efficiently support time-critical applications that have a wide range of workloads while maintaining necessary energy reserves in order to deal with emergency situations. Second, is the development of distributed protocols to implement these algorithms. Finally, is the development of prototypes of two energy harvesting hardware testbeds, a flow-based water energy generation system and a solar-based energy harvester used in a hybrid robotic sensor network application. The two prototype testbeds will not only yield critical technical insight but will introduce students of various backgrounds into the issues of networking, device, and robotic design.<br\/><br\/> The PIs have a multi-step plan to integrate their research and education mission of their proposal. This includes development of new courses, expanding and strengthening the schools effort on energy management, developing week-long training camps for high school instructors through the STEM program, and contributing to the advancement of the GMU robotics and sensor labs.","title":"CSR:Small: Energy Harvesting for Performance Sensitive Wireless Sensor Networks","awardID":"1116122","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[485622,"511512"],"PO":["551712"]},"181359":{"abstract":"Experiments have demonstrated the temporal and spatial correlations of spectrum availability, which are of key importance in the design and analysis of cognitive radio networks. Motivated by the observation, this research applies the theory of random fields, which describes the behavior of multiple correlated random variables, to model the spectrum availabilities in time and space domains. For global spectrum activity, a homogeneous random field like Ising model is used to model the spatial correlation and analyze the performance. For local spectrum activities, Bayesian networks are used to describe the causality in spectrum and statistically infer the future spectrum situations. Furthermore, the model of controlled random fields is employed to design the networking protocols in cognitive radio networks. A low-cost spectrum sensor is designed to collect the real spectrum measurement in multiple locations simultaneously. The research promotes the understanding of frequency spectrum activities and enhances the design and analysis of the next generation cognitive radio networks. The research involves aspects of wireless communications, networking, artificial intelligence and imaging processing; thus the inter-disciplinary essence of the research also lends itself to cross-disciplinary education. Novel courses will be devised, which involve the topics of cognitive radio networks, machine learning and image processing. This project also expects to attract traditionally underrepresented groups, as well as outreach high school students.","title":"NeTS: Small: Networking over Random Fields: A Statistical Model for Cognitive Radio Networks","awardID":"1116826","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["531702"],"PO":["557315"]},"181029":{"abstract":"Massive graphs arise in many social media applications, such as social networks, E-commerce recommendation systems, e-mail communication patterns, and other collaborative applications. Such data is often sensitive from a privacy point of view. Recently there are many privacy preserving schemes being proposed to protect the release of network data. The question is how effective these schemes are on preventing the re-identification of nodes, i.e., preserving the identity anonymization of the network nodes.<br\/><br\/>This project will raise the issue of the inadequacy of the current network anonymization schemes for massive and sparse graphs. It is important to understand the theoretical properties which make them susceptible to re-identification attacks. By a systematic study of the re-identification risks of the existing approaches, and development of new principles for anonymization of network data, we will deepen our understanding of the problems and be better able to protect the data privacy. By designing a new type of attack algorithms and raising the issue on the privacy exposure of the current network anonymization schemes, the work can lead to fundamentally different thinking on how to perform privacy preserving data publishing on network data. It provides new insights on how to devise anonymization schemes to protect the privacy of social network data.<br\/><br\/>One of the biggest obstacles on sharing information is the privacy concern. This project has the potential to make fundamental, disruptive advances in protecting the privacy of network data. It provides new insights on the inadequacy of the current anonymization schemes. Many researchers need access to sensitive data, e.g., social network data, e-mail and communication patterns, etc. By advancing the knowledge on privacy preserving data publishing, the barrier of sharing data will come down to facilitate scientific research activities.","title":"TC: Small: Robust Anonymization on Social Networks","awardID":"1115234","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["492421"],"PO":["562974"]},"185319":{"abstract":"This is funding to support a Doctoral Colloquium (workshop) of about 12 dissertation stage doctoral students, in a variety of visualization subfields, for a day of discussions and interactions with 6 distinguished research faculty, to be held in conjunction with this year's IEEE VisWeek meeting, which will take place during the week of October 23-28, 2011, in Providence, RI. Visualization, or the use of interactive graphics to support data analysis and understanding, has become an integral part and critical component of many application areas. IEEE VisWeek is the premier forum for visualization advances in science and engineering for academia, government, and industry, now bringing together about 900 researchers and practitioners from around the world with a shared interest in techniques, tools, and technology. VisWeek consists this year of five main events: IEEE Visualization (Vis), IEEE Information Visualization (InfoVis), the IEEE Visual Analytics Science and Technology Symposium (VAST), the IEEE Symposium on Large-Scale Data Analysis and Visualization, and the IEEE Symposium on Biological Data Visualization. The papers published in the special conference issue of IEEE Transactions of Visualization and Computer Graphics are rigorously refereed and widely cited. More information is available online at http:\/\/www.visweek.org.<br\/><br\/>The Doctoral Colloquium at IEEE VisWeek is a research-focused meeting which has taken place annually at the Visualization conference since 2006, and has helped launch the careers of a number of outstanding young researchers. In 2011 the workshop will convene on Sunday, October 23, with follow-up events during the VisWeek technical program. A primary goal of the Doctoral Colloquium is to allow students to discuss their research directions in a supportive atmosphere with a panel of distinguished leaders and with their peers, who will provide helpful feedback and fresh perspectives. The workshop supports community building, by connecting beginning and advanced researchers, one of the objectives being to build a cohort group of new researchers who will then have a network of colleagues across the world. Student research will be disseminated via posters during the VisWeek technical program, and via publication in the VisWeek Extended Abstracts. Feedback about the Doctoral Colloquium will be provided to future conference committees.<br\/><br\/>Broader Impacts: The VisWeek Doctoral Colloquium brings together the best of the next generation of visualization researchers and allows them to create a social network both among themselves and with senior researchers, which plays a major role in their enculturation into the profession. Since the students and faculty are a diverse group on several dimensions (nationality, scientific discipline, research specialization), the students' horizons are broadened at a critical stage in their professional development. The PI has affirmed that in managing this event he and his colleagues will try explicitly to identify and include the broadest possible group of highly qualified participants, that they will make an effort to encourage the participation of women, racial\/ethnic minorities, and persons with disabilities, and they will ensure that NSF funds are used chiefly to support participation by students enrolled in graduate programs in the United States.","title":"WORKSHOP: Doctoral Colloquium at IEEE VisWeek 2011","awardID":"1139350","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":[497247,497248,"530660","532924"],"PO":["565227"]},"180513":{"abstract":"Most computers today are 'multicore' parallel computers that are capable of executing several independent threads of computation simultaneously. Unfortunately, most existing programs are not parallel and cannot take advantage of this hardware capability. Furthermore, writing parallel programs using current notations like OpenMP is more difficult than writing sequential programs and, as a result, increases development costs and the likelihood of program defects.<br\/><br\/>The Kali project is building a software system that will permit most application programmers to write sequential programs and still obtain good performance on multicore processors. Parallelism will be hidden within object-oriented class libraries written by expert parallel programmers and it will be managed by a sophisticated runtime system that uses a range of parallel execution strategies customized to the needs of the application. Applications programmers can take advantage of the benefits of sequential programming such as familiarity, readability, maintainability, and debuggability. They will also be able to tune program performance and power without having to drop down to a lower abstraction level. In addition, the Kali project is studying the use of innovative hardware to facilitate the development of efficient programs. Finally, the project is producing a suite of application benchmarks that will be useful for performance evaluation of similar systems.","title":"CSR: Large: Collaborative Research: Kali: A System for Sequential Programming of Multicore Processors","awardID":"1111691","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":["501195"],"PO":["551712"]},"180997":{"abstract":"The objective of this research is to carry out a novel study of network survivability across layers to deal with cascading failures in layered networks. This research will be in the context of IP-over-WDM (wavelength division multiplexing) optical networks, with a focus on multiple failures in the physical (optical) layer and their consequences at the higher layer, namely the IP layer. Specifically, the broad scope of the project covers i) Survivable logical topology mapping under multiple failures, ii) Logical topology mapping for guaranteed survivability, iii) Logical topology mapping under multiple constraints, and iv) A generalized theory of flows across layers, capacity of survivable logical topologies and related algorithmic challenges.<br\/><br\/>The intellectual merits of the proposed research lie in developing unifying theories and methodologies that will make significant advances to the understanding of cross-layer survivability issues, and providing the theoretical foundation for future advances in the general area of cross-layer design and optimization. The research team will build these theories on modern advances in graph theory, mathematical programming and algorithm design. Innovative algorithmic techniques based on advanced data structures and computer algorithms such as approximation techniques will be developed. A generalized theory of cross layer flows that will go well beyond the widely used classical theory of single layer flows will be developed.<br\/><br\/>Broader Impact: WDM Optical networks form the critical backbone of all modern communication networks and systems. Modern communication systems consist of multiple physical implementations communicating via layered protocols. As such, a single failure at one layer may lead to cascading failures, i.e., failures at the physical layer lead to failures at the logical layer. Research in this area of cross-layer survivability is still in its infancy. Though IP-Over-WDM networks will provide the context for this research, the theory of cross layer flows and the algorithmic (in particular approximation) techniques that will be developed will have multidisciplinary value spanning several STEM related areas: computer science, electrical engineering, graph theory, mathematical programming, and the emerging area of network science. Besides extending the frontiers of knowledge in cross layer theory, the research will have significant educational value in training highly skilled researchers for research and development in cutting edge technologies in different areas of information technology. The training program will have a specific focus on training undergraduate students for future leadership in STEM areas.","title":"NeTS: Small: Collaborative Research:Cross Layer Survivability to Cascading Failures in Layered Networks","awardID":"1115130","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[485068],"PO":["564993"]},"181514":{"abstract":"Deployment of wireless local area networks (WLAN) has undergone dramatic growth in recent years. Multiple WLAN standards have been developed, including several versions of IEEE 802.11. The widespread deployment of IEEE 802.11 is based on the Distributed Coordination Function (DCF) component of the standard, which incorporates channel contention resolution using CSMA (Carrier Sense Multiple Access). However, an important shortcoming of past deployments of CSMA-based protocols has been their inability to achieve optimal performance. In recent years, researchers have developed new throughput-optimal CSMA algorithms. These results have opened up the possibility of designing CSMA-based protocols that can perform close to the optimal. However, the prior work on throughput-optimal CSMA makes several simplifying assumptions, including assumptions of perfect carrier sensing, symmetric interference relationship between links, and fixed rate transmissions. <br\/><br\/>The thrust of this project is on developing CSMA-based scheduling mechanisms that can work well in practical networks. Towards this goal, the project investigates systematic relaxation of unrealistic assumptions and constraints, and attempts to develop CSMA algorithms that can perform well under the relaxed conditions. The scope of the project includes implementation of selected protocols on an experimental testbed. The project explores CSMA in the context of wireless multi-hop mesh networks, as well as infrastructure-based wireless LANs. Expected results from the project include new theoretical algorithms as well as efficient CSMA-based protocols that can perform well in practice.","title":"NeTS: Small: Efficient CSMA in Wireless Networks: Theory, Protocol Design, and Implementation","awardID":"1117539","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["553537"],"PO":["557315"]},"181415":{"abstract":"Blind source separation (BSS) has found wide use in many disciplines including signal processing as it starts from a simple generative model minimizing assumptions on the data generation mechanism and achieves useful decompositions of the observed data. In particular, independent component analysis (ICA) has been the most commonly used approach to achieve BSS since statistical independence of the underlying components is plausible in many applications. Besides independence, sample correlation is another inherent property of many signals of interest. Traditionally, these two properties are addressed separately when developing methods for source separation. Entropy rate, on the other hand, is a natural cost that allows one to account for independence and sample correlation jointly, and hence promises to result in a new class of powerful solutions with wide applicability. In addition, it enables one to easily incorporate model selection---another key problem complementing the power of BSS---into the problem through the use of information theoretic criteria.<br\/><br\/>The focus of this research is the development of a class of powerful methods for source separation and model selection using entropy rate so that one can take both the higher-order-statistical information and sample correlation into account to achieve significant performance gains in more challenging problems. The main application domain is one that can truly take advantage of this fully combined approach: the analysis of functional magnetic resonance (fMRI) data and the rejection of gradient and pulse artifacts in electroencephalography (EEG) in concurrent EEG-fMRI data. Both are applications that have proven challenging for the traditional model-based approach due to the unique nature of the noise and artifacts in these problems. Hence, they provide a unique testbed for the performance evaluation of the new class of methods developed under this study. Since independence and sample correlation are intrinsic properties of many other types of data, the new set of methods will be attractive solutions for many other problems as well.","title":"CIF: Small: Collaborative Research: Entropy Rate for Source Separation and Model Selection: Applications in fMRI and EEG Analysis","awardID":"1117056","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":[486076],"PO":["564898"]},"181547":{"abstract":"Privacy in networked systems goes beyond the secrecy of exchanged data; it is equally critical to protect the identities of communicating parties and the paths of data flow. Availability of such networking information is not only a violation of user privacy, but also provides a convenient platform for a malicious adversary to launch more powerful attacks. Anonymous network systems are networks of special routers and proxy servers that provide protection using cryptographic tools and covert relaying. Commonly deployed systems are, however, vulnerable to timing analysis of packet transmissions and do not provide the necessary protection under resource limitations.<br\/><br\/>The principal investigators will investigate the provably optimal design of anonymous network systems under constraints on resources and desired network quality of service (QoS). They will develop a novel quantitative measure of anonymity that takes into account the complete information available to an adversary and the resource and topological constraints of a network. Using the quantitative model, they will explore the following three critical problems in network system design: optimal resource allocation, stochastic routing, and topological design. If successful, the analysis will give rise to novel networking algorithms to achieve anonymity in information exchange that cater to modern paradigms of network design such as mobility and application independence.<br\/><br\/>Broader Impact: The research will have a significant impact in delivering provable privacy for networked systems. The research promotes graduate and undergraduate education in network security and provides access to recent advances in the field to community college students and law enforcement personnel through distance learning programs. In addition, awareness of security vulnerabilities in social networks will be broadened for middle to high school students.","title":"Nets: Small: Optimal Design of Anonymous Network Systems under Resource Constraints","awardID":"1117701","effectiveDate":"2011-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["502115"],"PO":["565090"]},"181316":{"abstract":"From Virtual to Real<br\/>Wojciech Matusik, MIT, and Hanspeter Pfister, Harvard University<br\/><br\/>Novel and innovative digital output devices, such as stereoscopic TVs, passive (e-Ink) displays, and 3D printers, are entering the mass market. They are rapidly improving in quality and decreasing in price. This trend empowers users to consume and produce digital media like never before. However, while there has been tremendous progress in the hardware development of these output devices, the provided digital content creation software, algorithms, and tools are largely underdeveloped. For example, creating a 3D hardcopy of an animated computer graphics character is well beyond the reach of consumers, and to approximate the character's appearance and deformation behavior using multi-material 3D printers is difficult or perhaps even impossible for professionals. The main issues are a lack of accurate previews of how the output will look like, a lack of standardization between devices with similar capabilities, and a lack of accurate conversion tools and algorithms to go from the virtual (i.e., the computer model) to the real (i.e., the physical output).<br\/><br\/>This research involves the development of a complete process and software framework that allows moving from abstract computer models to their physical counterparts efficiently and accurately. Designing this process is posing the following fundamental computational challenges: (1) accurate and efficient simulation methods that can predict the properties and behavior of an output without physically generating it; (2) efficient methods to compute an output gamut that describe physically-realizable outputs for a given device; (3) general gamut mapping algorithms that convert abstract computer models to realizable points in the device gamut; and (4) accurate perceptual metrics that allow comparing different output elements during the gamut mapping algorithm. This research is focusing on two emerging classes of important output devices: multi-view auto-stereoscopic displays and multi-material 3D printers. The research is creating a complete and general software architecture that will support both existing and future output devices.","title":"CGV: Small: Collaborative Research: From Virtual to Real","awardID":"1116619","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["489920"],"PO":["565227"]},"180469":{"abstract":"This research involves collaboration among investigators at three institutions. The PIs anticipate a future in which humans and intelligent robots will collaborate on shared tasks. To achieve this vision, a robot must have sufficiently rich knowledge of the task domain and that knowledge must be usable in ways that support effective communication between a human and the robot. Navigational space is one of the few task domains where the structure of the knowledge is sufficiently well understood for a physically-embodied robot agent to be a useful collaborator, meeting genuine human needs. In this project, the PIs will develop and evaluate an intelligent robot capable of being genuinely useful to a human, and capable of natural dialog with a human about their shared task.<br\/><br\/>The Hybrid Spatial Semantic Hierarchy (HSSH) is a human-inspired multi-ontology representation for knowledge of navigational space. The spatial representations in the HSSH provide for efficient incremental learning, graceful degradation under resource limitations, and natural interfaces for different kinds of human-robot interactions. Speech is a natural though demanding way to use natural language to communicate with a robot. To maintain real-time performance, natural language understanding must be organized to minimize the amount of backtracking from early conclusions in light of later information. This project will answer three scientific questions.<br\/><br\/>(1) Can the HSSH framework, extended with real-time computer vision, express the kinds of knowledge of natural human environments that are relevant to navigation tasks? <br\/>(2) Can the HSSH representation support effective natural language communication in the spatial navigation domain? <br\/>3) Can we develop effective human-robot interaction that meets the needs of a person and improves the performance of the system?<br\/><br\/>To these ends, the PIs will perform this research with two different kinds of navigational robots, each learning from its travel experiences and building an increasingly sophisticated cognitive map: an intelligent robotic wheelchair which carries its human driver to desired destinations, and a telepresence robot that transmits its perceptions to a remote human driver as it navigates within an environment so the driver can achieve virtual presence and communicate with others remotely. To inform the design process, the PIs will conduct focus groups with potential users. They will also evaluate their implemented systems throughout the process, creating an iterative design-test cycle.<br\/><br\/>Broader Impacts: To be successful, an intelligent robot must not only be able to perceive the world, represent what it learns, make useful inferences and plans, and act effectively. It must also be able to communicate effectively with other agents, and particularly with people. This confluence among grounded knowledge representation, situated natural language understanding, and human-robot interaction is intellectually fundamental, and is the focus of this research. Since the domain of spatial knowledge is foundational for virtually all aspects of human knowledge, project outcomes will have broad applicability. This work will create technologies for mobility assistance for people with disabilities in perception (blindness or low vision), cognition (developmental delay or dementia), or general frailty (old age). It will also support telepresence applications such as telecommuting, telemedicine and search and rescue. The project includes outreach to K-12 and community college students, K-12 teachers, and the public in a number of venues.","title":"HCC: Large: Collaborative Research: Human-Robot Dialog for Collaborative Navigation Tasks","awardID":"1111494","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["534124"],"PO":["565227"]},"181206":{"abstract":"This proposal is developing theoretical foundations as well as algorithms and software for secrecy-preserving reasoning. The key novel idea, rather different from existing work, is to exploit the indistinguishability (as far as the querying agent is concerned) between secret information, on one hand, and incomplete information, on the other hand, under the open world semantics. The Principal Investigator utilizes his ideas of for computing small secrecy envelopes so that answering \"Unknown\" to queries inside the envelope while answering queries outside the envelope truthfully, constitutes a secrecy-preserving query-answering algorithm. The Principal Investigator plans to integrate this research into education and to enhance interdisciplinary collaborations within and beyond Iowa State University. In addition, he plans to advance collaborations with industrial partners (Boeing in the high-tech area and Mayo Clinic in the health care area) as well as applications in the areas of information assurance and national security.","title":"TC: Small: Secrecy-Preserving Reasoning: Foundations, Algorithms, and Software","awardID":"1116050","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":[485571],"PO":["562974"]},"181327":{"abstract":"As modern technology infrastructure spreads throughout the world, the quantity of electronic text, written in hundreds of different languages, continues to grow in size and diversity. Building effective information retrieval, extraction, and translation systems across this vast array of languages currently requires time-consuming and expensive linguistic annotations for each language. Generic, fully unsupervised, methods are unlikely to provide a language independent solution to this problem.<br\/><br\/>Focusing on part-of-speech prediction, this project undertakes a novel approach, combining elements of supervised and unsupervised learning without assuming any specific knowledge of the target language. Instead of treating individual languages as closed systems, language-independent \"universals\" are statistically estimated from dozens of languages for which annotated corpora exist, and these learned universals are used to predict the part-of-speech categories of unannotated languages. At the heart of the project is a data-driven exploration of language-independent corpus characteristics that relate cross-lingual linguistic categories to surface statistics of text. These learned patterns are incorporated into expressive structured prediction models using novel approximate learning and inference methods developed by the Principal Investigators of the project.<br\/><br\/>Of the world?s spoken languages, hundreds are at risk of immediate extinction and thousands more are likely to disappear over the coming decades. By facilitating the rapid creation of language-independent linguistic analysis tools, the technology developed under this project has the potential to revolutionize the documentation of endangered languages. In the long-term, this research direction will also help realize the full social benefits of the global technology infrastructure by creating intelligent text processing tools for hundreds of low-resource languages.","title":"RI: Small: Collaborative Research: Statistical Learning of Language Universals","awardID":"1116676","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["493589"],"PO":["565215"]},"181217":{"abstract":"Data is at the heart of the digital revolution. Changing work habits, increasingly stringent electronic record-keeping mandates, and developments in the health care, energy, and retail sectors all suggest a growing demand for data storage and memories for the foreseeable future. Fast, low-power, ultra-high density, and non-volatile magnetic storage and memory systems are projected to accommodate the bulk of the global data storage needs for decades to come and spawn revolutionary data-intensive computing modalities along the way. Their timely development, however, critically depends on the availability of fast and accurate computational tools capable of simulating electromagnetic field and magnetization dynamics in complete magnetic data storage and memory systems. Unfortunately, current simulators are not up to this task. <br\/>This study focuses on the development of high-performance hybrid micro magnetic-electromagnetic simulators for modeling next-generation magnetic memory and data storage systems. The simulators leverage analytically preconditioned time-domain integral equation methods to solve the Maxwell and Landau-Lifshitz-Gilbert-Slonczewski equations, which govern coupled electromagnetic field and magnetization phenomena. To facilitate the analysis of such phenomena in complex systems involving billions of degrees of freedom, the simulators are implemented on massively parallel Central Processing Unit (CPU) and Graphics Processing Unit (GPU) computers. These simulators are used for the design of next generation magnetic random memory devices as well as bit patterned media and heat-assisted magnetic recording storage systems. Such advanced memory and storage systems are to be essential to future high-performance computing systems.","title":"SHF: Small: Collaborative Research: High-performance hybrid micromagnetic-electromagnetic simulators for memory and high-performance data storage devices","awardID":"1116082","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[485598],"PO":["565272"]},"181338":{"abstract":"The project brings together an interdisciplinary team of researchers from Johns Hopkins University, Carnegie Mellon University, and the University of Chicago to develop methods, theory and algorithms for discovering hidden structure from complex scientific datasets, without making strong a priori assumptions. The outcomes include practical models and provably correct algorithms that can help scientists to conduct sophisticated data analysis. The application areas include genomics, cognitive neuroscience, climate science, astrophysics, and language processing.<br\/><br\/>The project has five aims: (i) Nonparametric structure learning in high dimensions: In a standard structure learning problem, observations of a random vector X are available and the goal is to estimate the structure of the distribution of X. When the dimension is large, nonparametric structure learning becomes challenging. The project develops new methods and establishes theoretical guarantees for this problem; (ii) Nonparametric conditional structure learning: In many applications, it is of interest to estimate the structure of a high-dimensional random vector X conditional on another random vector Z . Nonparametric methods for estimating the structure of X given Z are being developed, building on recent approaches to graph-valued and manifold-valued regression developed by the investigators; (iii) Regularization parameter selection: Most structure learning algorithms have at least one tuning parameter that controls the bias-variance tradeoff. Classical methods for selecting tuning parameters are not suitable for complex nonparametric structure learning problems. The project explores stability-based approaches for regularization selection; (iv) Parallel and online nonparametric learning: Handling large-scale data is a bottleneck of many nonparametric methods. The project develops parallel and online techniques to extend nonparametric learning algorithms to large scale problems; (v) Minimax theory for nonparametric structure learning problems: Minimax theory characterizes the performance limits for learning algorithms. Few theoretical results are known for complex, high-dimensional nonparametric structure learning. The project develops new minimax theory in this setting. The results of this project will be disseminated through publications in scientific journals and major conferences, and free dissemination of software that implements the nonparametric structure learning algorithms resulting from this research.<br\/><br\/>The broader impacts of the project include: Creation of powerful data analysis techniques and software to a wide range of scientists and engineers to analyze and understand more complex scientific data; Increased collaboration and interdisciplinary interactions between researchers at multiple institutions (Johns Hopkins University, Carnegie Mellon University, and the University of Chicago); and Broad dissemination of the results of this research in different scientific communities. Additional information about the project can be found at: http:\/\/www.cs.jhu.edu\/~hanliu\/nsf116730.html.","title":"III: Small: Nonparametric Structure Learning for Complex Scientific Datasets","awardID":"1116730","effectiveDate":"2011-08-01","expirationDate":"2013-04-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[485891,"496977","532936"],"PO":["560586"]},"181349":{"abstract":"Recent trends in computing systems, such as cloud computing, virtual execution environments, and grid computing, have been away from centralized designs towards highly extensible, open, and distributed systems comprising multiple independently-developed and operated subsystems. Such systems present a fundamental design challenge for applications that require real-time guarantees, due to the reliance upon all subsystem owners and developers to truthfully express their resource requirements. Currently, no real-time open system environment framework prevents a user from obtaining an ``unfair'' allocation of system resources by simply misrepresenting their subsystem's resource requirements. A solution is needed, that formally guarantees an efficient\/equitable resource allocation among subsystems. The approach taken here is to apply a novel combination of real-time scheduling and game theory (i.e., algorithmic mechanism design) techniques, to design truth-inducing allocation mechanisms for shared real-time processing platforms. These techniques are proven and evaluated by developing a middleware implementation and a grid simulation for a real-time setting.<br\/><br\/>A trustworthy scheme for allocating time-critical resources among independently developed subsystems is a key to reducing development and operational costs, and increasing the reliability of the complex computing systems upon which our society and economy increasingly depend. Beyond direct benefits of the technology, this project furthers educational goals by actively involving undergraduate and underrepresented students in research.","title":"CSR: Small: Designing Mechanisms for Resource Allocation in Competitive Real-Time Open Environments","awardID":"1116787","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[485918,"511432"],"PO":["564778"]},"181239":{"abstract":"In this project, protocols, mechanisms, and a Java implementation of <br\/>nested distributed transactional memory (TM) are developed. Distributed TM promises <br\/>to alleviate the programmability, scalability, and performance challenges of lock-based <br\/>distributed concurrency control. Emerging distributed TM implementations export <br\/>a simple programming interface that precludes locks and are yielding performance <br\/>comparable to, or better than lock-based distributed concurrency control. Nesting is <br\/>essential to distributed TM for composability, functionality, fault-management, and <br\/>performance. <br\/><br\/>The project is developing protocols and mechanisms to support closed and open <br\/>nesting in distributed TM. Closed nesting allows a nested transaction to be aborted <br\/>without aborting the parent transaction, but not vice versa. Open nesting allows a <br\/>parent transaction to be aborted without aborting the nested transaction, and vice <br\/>versa, permitting greater concurrency. To support these nesting models, the project <br\/>is developing distributed transactional conflict resolution protocols, distributed <br\/>cache coherence protocols, and mechanisms for transparently executing <br\/>compensating transactions to undo the effects of committed transactions. The <br\/>project is implementing these techniques in the open-source, HyFlow distributed <br\/>TM Java package (hyflow.org). <br\/><br\/>The project is also transitioning this technology (techniques and HyFlow <br\/>implementation) to US Navy's Aegis Combat System, which uses distributed <br\/>concurrency control. Additionally, the project's results are being incorporated into <br\/>advanced graduate courses at Virginia Tech that includes students at Blacksburg, <br\/>VA, scientists and engineers at US Naval Surface Warfare Center Dahlgren Division <br\/>(NSWCDD), VA (through Virginia Tech's graduate outreach program at NSWCDD), <br\/>and students in the Middle East and North Africa through Virginia Tech's VT-MENA <br\/>program at Egypt.","title":"CSR: Small: Nested Distributed Software Transactional Memory: Protocols, Mechanisms, and Java Package","awardID":"1116190","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["517750"],"PO":["565255"]},"181504":{"abstract":"Spectrum opportunities in white space hinge heavily on the traffic patterns of the licensed users (PUs), and vary across space, time, and frequency. Making a paradigm shift, this project advocates to leverage traffic shaping and mobility patterns of PUs for inducing predictable structures of spectrum holes in the spatio-temporal domain, which in turn enables more efficient spectrum access by cognitive radio users. With such a common thread, this project will 1) study joint traffic shaping and network coding for PUs, as a spectrum shaper, to induce predictive structures in spectrum holes; 2) investigate SUs? cognitive transmissions via adaptive file fragmentation and predetermined file fragmentation that can match the characteristics of spectrum opportunities discovered on the fly; and 3) explore cognitive routing via exploiting PU-mobility predictability.<br\/><br\/>Efficient spectrum usage will facilitate a wide variety of scientific and engineering applications and result in a significant impact on the society at large. This research will open a new direction for spectrum shaping that induces predictable structures of spectrum opportunities, which can then be exploited by SUs for effective cognitive communications. The findings will advance the state-of-the-art of cognitive radio networking and spur a new line of thinking. Another major task of this project is to integrate research with educational activities. In particular, the PIs will continue to involve under-represented and minority students in research.","title":"NeTS: Small: Inducing and Exploiting Spectrum Predictability via Traffic Shaping and Mobility for Cognitive Communication in White Space","awardID":"1117462","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["518327","551041"],"PO":["557315"]},"181625":{"abstract":"Lack of physical activity is a serious health concern for individuals with visual impairments. In particular, children with visual impairments have limited access to physical education, recreation, and athletic programs for a variety of reasons including fear of injury, safety concerns of parents and teachers, and lack of sighted guides with whom to exercise. The PI's hypothesis is that video games, which have been identified as a contributing factor to the increasingly sedentary behavior of children in general and associated higher levels of obesity, may provide a solution. A new genre of \"exergames\" use upper and\/or lower-body gestures (such as steps, punches, and kicks directed at virtual targets), to provide players with an immersive experience that engages them in moderate-to-vigorous physical activity that is high enough to yield health benefits. The PI's goal in this project is to investigate how proprioceptive displays can be designed and built to facilitate exercise games for blind adolescents. Proprioception, the ability to sense the position and orientation of one's body and its parts, is an interoceptive sensory modality distinct from exteroceptive sensory modalities such as sight, touch, and hearing, and has remained largely unexplored as a modality of feedback. In performing a gesture we generally combine visual and proprioceptive information, but once the location of a target has been memorized proprioceptive feedback alone is sufficient for relating the position of the user's limb to the target. Since the location of a target primarily is acquired visually, gestures are difficult to perform for users with visual impairments. This project will test the hypothesis that whole body gestures can be performed by blind users in exergames when target direction is acquired using a proprioceptive display.<br\/><br\/>In prior work, the PI has explored single proprioceptive displays for acquiring the direction to a target in a horizontal plane by having users scan a horizontal line with a hand-held orientation aware device; a vibrotactile cue was used to indicate when the device was pointed at the target, with target direction conveyed to the user using proprioception. Preliminary results have demonstrated that blind users can perform an upper body gesture aimed at a target in a plane where direction to the target is acquired using a proprioceptive display. This project will significantly advance our understanding of proprioceptive displays by: (1) quantifying spatial and temporal resolutions of discrete proprioceptive displays; (2) developing efficient scanning strategies for conveying points in a plane (directions to points in a space) using an analog proprioceptive display and determining the effectiveness of using multiple proprioceptive displays; (3) investigating the efficacy with which whole-body directed gestures can be performed using proprioceptive displays; and (4) establishing whether rhythmic sequences of whole-body directed gestures performed using a proprioceptive display can be memorized using audio cues. Through generation of empirical data with exergames that involve whole-body gestures, this project will develop a fundamental and transformative understanding of proprioceptive displays in order to articulate a set of principles for their design. <br\/><br\/>Broader Impacts: Children with visual impairments have significantly higher levels of obesity and often exhibit delays in motor development caused by a general lack of opportunities to be physically active. Through development of open-source exergames that can be played using commercial off-the-shelf controller technology without the aid of a sighted guide and with minimal risk of injury, this project will increase existing exercise and socialization opportunities for members of this vulnerable population. More generally, because proprioceptive displays can be implemented using low cost technology that is often already present in mobile devices, these displays may ultimately facilitate the development of robust ear- and eye-free forms of interaction for everyone (e.g., for motor rehabilitation or sports training). This research will directly involve both graduate and undergraduate students, and the PI will use this work as the basis for compelling design projects in his courses. The PI will also develop outreach efforts such as summer camps for K-12 students with visual impairments (and other minorities) in order to attract them to computer science and STEM.","title":"HCC: Small: Proprioceptive Displays to Engage Blind Users into Healthy Whole Body Interaction","awardID":"1118074","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[486609],"PO":["565227"]},"180536":{"abstract":"The most powerful computing systems in the world have historically been dedicated to solving scientific problems. Until recently, the computations performed by these systems have typically been simulations of various physical phenomena. However, a new paradigm for scientific discovery has been steadily rising in importance, namely, data-intensive science, which focuses sophisticated analysis techniques on the enormous (and ever increasing) amounts of data being produced in scientific, commercial, and social endeavors. Important research based on data-intensive science include areas as diverse as knowledge discovery, bioinformatics, proteomics and genomics, data mining and search, electronic design automation, computer vision, and Internet routing. Unfortunately, the computational approaches needed for data-intensive science differ markedly from those that have been so effective for simulation-based supercomputing. To enable and facilitate efficient execution of data-intensive scientific problems, this project will develop a comprehensive hardware and software supercomputing system for data-intensive science.<br\/>Graph algorithms and data structures are fundamental to data-intensive computations and, consequently, this project is focused on providing fundamental, new understandings of the basics of large-scale graph processing and how to build scalable systems to efficiently solve large-scale graph problems. In particular, this work will characterize processing overheads and the limits of graph processing scalability, develop performance models that properly capture graph algorithms, define the (co-design) process for developing graph-specific hardware, and experimentally verify our approach with a prototype execution environment. Key capabilities of our system include: a novel fine-grained parallel programming model, a scalable library of graph algorithms and data structures, graph-optimized core architecture, and a scalable graph execution platform. The project will also address the programming challenges involved in constructing scalable and reliable software for data-intensive problems.","title":"SHF: Large: Collaborative Research: PXGL: Cyberinfrastructure for Scalable Graph Execution","awardID":"1111798","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["557314"],"PO":["565272"]},"180426":{"abstract":"This project will explore how remote socialization enabled by information and communication technologies (ICTs) is transforming four occupations: graphic design, automotive engineering, banking, and Internet entrepreneurship. Following a comparative, field-based research design, this research will examine the effects of both organizational environments and socialization tactics on ICT use and consider issues of technology use, socialization, and the changing nature of work. In today's workplaces, it is increasingly common to encounter arrangements in which occupational members are geographically distributed from one another. This new reality calls into question existing theories of socialization and learning practices that highlight the importance of collocated interaction and in situ knowledge transfer. By focusing on how individuals use ICTs to learn what it means to be an occupational member, this research will contribute to a new breed of theory on socialization that indicates the processes, practices, and strategies individuals can use to become effective members of an occupation even though they work remotely from others. By drawing on recent theorizing, which suggests that ICTs may provide particular affordances for interaction that non-mediated (e.g. face-to-face) contexts do not, it will explore the possibility that remote socialization may, in fact, help occupations to transform themselves. One aim is to build theory about the mechanisms by which technology leads to occupational transformation. <br\/><br\/>Occupational skill is critical to economic success, social progress, and individual well-being. However, many occupations seem to be failing to adapt quickly to changes in science, technology, and policy. The failure of occupations to change and refashion themselves to meet new social and technological pressures portends job loss from reduced skill for American citizens, and potentially increased outsourcing to other countries. This study will provide insight into how technology-enabled remote socialization may be able to contribute to faster occupational transformation. Although, for many years, ICT-mediated communication has been seen to be impoverished when compared to face-to-face communication, but now that it has developed considerably, ICT-mediated communication may provide more opportunities for workers to break free from the inertia of established occupations and develop new work practices and strategies that move the occupation forward. Additionally, it is imperative that new occupational members learn how to effectively acquire the knowledge and skills they need to perform their jobs well when they work remotely from others. This study will provide insight into effective practices of remote socialization and occupational learning such that individuals who are attempting to learn new work practices and knowledge will be successful in their efforts.","title":"HCC: Large: Collaborative Research: Information Technology, Remote Socialization, and the Development of Occupational Identity","awardID":"1111288","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[483548],"PO":["564456"]},"181526":{"abstract":"This project aims to develop a comprehensive approach for characterization of RF environment in terms of spatial and temporal features including: i) the number of active transmitters, ii) their power, direction of arrival and location, and iii) modulation class. Spatio-temporal spectrum sensing requires novel multi-dimensional parameter estimation algorithms as opposed to conventional spectrum sensing that uses hypothesis testing based detection algorithms. In this work, the Bayesian estimation approach using angle-of-arrival measurements is applied to create the probabilistic map of the transmitter presence in a given region taking into account measurement noise and uncertainties. The tools from random matrix theory are applied to perform joint detection and parameter estimation, and analytically derive performance bounds. The methods for modulation classification are based on goodness-of-fit statistical tests with reduced sampling complexity, and provide the unified classification framework for a wide range of modulation classes. The results of this research will impact the design of novel medium access and routing protocols that manage the interference through awareness of the location and link quality of other transmitters in the region. The developed technologies could potentially apply for monitoring of RF transmissions within wireless infrastructure, and for the defense and national security applications.","title":"NeTS: Small:Spatio-Temporal Spectrum Sensing","awardID":"1117600","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["560230"],"PO":["557315"]},"181317":{"abstract":"The primary goal of this project is improve the stability of software<br\/>systems. Many modern programming languages use static type checking to<br\/>eliminate errors from programs. However, the type systems that<br\/>underlie these checkers are conservative and reject several classes of<br\/>useful, correct programs. In particular, the techniques of generic<br\/>programming, which optimize data structures, generalize interfaces and<br\/>eliminate boilerplate in program development, are difficult to type<br\/>check. Although dependent type systems provide flexibility in type<br\/>checking, particularly with respect to generic programming, they have<br\/>not yet been integrated into realistic programming languages. <br\/><br\/>The proposed work of this project will augment a mature, general<br\/>purpose, statically-typed programming language with support for<br\/>dependently-typed programming. Specifically, the project will extend<br\/>the source and intermediate languages of the Glasgow Haskell Compiler<br\/>(GHC). Although generic programming is the key application area, the<br\/>Haskell extensions proposed here will enhance the capabilities of<br\/>dependently-typed programming in Haskell in many domains. This project<br\/>is in collaboration with the designers and implementers of the GHC<br\/>compiler at Microsoft Research, Cambridge. The team includes world<br\/>leaders in the areas of dependent type systems, generic programming,<br\/>language design and implementation. As GHC is an open-source project,<br\/>the extensions to Haskell will be freely distributed. Furthermore, the<br\/>project will have educational benefits through the funding of doctoral students<br\/>and the integration of its results into advanced graduate and<br\/>undergraduate courses.","title":"SHF: SMALL: Dependently-typed Haskell","awardID":"1116620","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["550615"],"PO":["564588"]},"181438":{"abstract":"A variety of applications in the nascent field of cloud computing and in other fields requires stable, low-delay network connectivity together with capabilities of co-scheduling network and server resources. As a result, several network providers have recently started to deploy scheduled dynamic circuit services (SDCS). Under such services, a network provider allows its customers to place scheduling requests for a fixed-rate circuit lasting for a fixed duration, for either earliest possible usage or scheduled usage at a desired point of time in the future.<br\/><br\/>While carrying significant potential impact, SDCS currently provide little support for dynamic routing and scheduling of circuits across different administrative domains. This lack of adequate support for inter-domain routing represents a major hurdle to the full-scale deployment of SDCS. The main objective of this project is to tackle this challenge by devising distributed solutions supporting scheduled dynamic circuit services across domains. The project consists of two inter-related thrusts: (i) theory, which focuses on the problem of devising distributed routing algorithms for SDCS that provably converge and provide performance guarantees on user-performance metrics; and (ii) protocol design and implementation, which focuses on the design, development, and real network prototyping of a new inter-domain routing protocol for SDCS, referred to as the scheduled circuit routing protocol (SCRP).<br\/><br\/>Broader Impact: Successful completion of this project promises to impact a wide array of high-throughput and real-time applications belonging to medical, scientific and commercial domains, and facilitate the creation of a new ecosystem for communication networks. The project is further striving for high impact through collaborations with international academic and industrial partners, and network operators. Students at all levels will be engaged in real network experiments and in the development of innovative applications enabled by SDCS, such as haptic-based communication.","title":"NeTS: Small: Collaborative Research: Boosting Inter-Domain Scheduled Dynamic Circuit Services (SDCS)","awardID":"1117160","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[486132],"PO":["565090"]},"181559":{"abstract":"The project aims to develop effective approaches to anomaly detection from high-dimensional time series data, motivated by applications such as oil drilling, semiconductor fabrication, and railroad operation.<br\/>The proposed approach takes advantage of Granger Graphical models, which uncover the temporal dependencies between variables, to efficiently compute a robust correlation anomaly score for each variable and obtain insights regarding the causes of anomalies. The project develops effective approaches to addresses several specific challenges that arise in real-world applications of anomaly detection, including (1) nonlinear temporal dependencies; (2) hidden variables; and (3) massive amounts of data. The resulting algorithms will be evaluated on two real production systems: an oil-field mechanical system and a semi-conductor fabrication system.<br\/><br\/>The project is expected to advance the state of the art in anomaly detection for high-dimensional time series data that arise in many application domains. It offers research-based training opportunities at the intersection of machine learning, data mining, and intelligent production management, as well as operational research in general.Workshops and mini-courses will be organized to introduce advanced machine learning techniques to students, practitioners, and researchers in production management. The anomaly detection code and data sets will be freely disseminated to the broader research and educational community. Additional information about the project can be found at: http:\/\/www-bcf.usc.edu\/~liu32\/ggm.htm.","title":"III: Small: Uncovering the Myths of Unlikelihood: Granger Graphical Models for Anomaly Detection in Multivariate Time-Series Data","awardID":"1117740","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["563325"],"PO":["565136"]},"181207":{"abstract":"More than 160 million blind, low-vision, and deaf-blind people worldwide have not realized the full potential of the mobile revolution. People in these groups often use special-purpose portable devices to solve specific accessibility problems, such as obtaining product information from bar codes, finding location information via GPS, and accessing printed text using optical character recognition (OCR). Unfortunately, devices targeted at these groups are specialized for one or few functions, usually not networked, and expensive. Devices also target one disability, thereby preventing a deaf-blind person from, for instance, using a device designed for a low-vision person. Blind, low-vision, and deaf-blind people who can afford it must carry multiple devices with varying interfaces. This is despite the fact that many mainstream mobile devices already have the necessary sensors, such as a camera, microphone, GPS locator, accelerometer, and compass, to provide all of these functions on one device. MobileAccessibility is the PI's approach to providing useful mobile accessible functionality to blind, low-vision, and deaf-blind users. This approach leverages a smart phone's sensors, multi-modal output, and access to remote services to reduce the cost of existing accessibility solutions and enable completely new ones to be created. Some key user interaction problems for these groups of users that will be addressed in this project include: (i) how can a blind, low-vision, or deaf-blind person effectively use the camera on a smart phone to achieve an accessibility goal, (ii) how can enlarged presentations be effectively navigated by a low-vision person on the small screen of a smart phone, (iii) how can vibration be effectively used to convey information to a blind or deaf-blind person, (iv) how can valuable network services be best utilized by these communities, (v) how can the knowledge of one person about their environment be effectively captured, stored, and used among these communities. The user-centered design of these applications will involve blind, low-vision, and deaf-blind people throughout their development. Prototype applications to provide context to the research questions will be built for all three groups. Input will use speech recognition, the touch screen, and the keyboard. Output will be audio for blind users, enlargement for low-vision users, and vibration and tethering to Braille devices for deaf-blind and blind users. The resulting interfaces will be evaluated both in the lab and in the field. There will a focus on identifying common interaction techniques that can be employed by multiple applications.<br\/><br\/>Broader Impacts: This research represents a new paradigm in mobile assistive technologies where a single programmable device can serve a multitude of accessibility needs. Rather than using separate devices for different needs, accessibility solutions can be downloaded to a single device. The research challenge is to design, build, and evaluate novel accessibility solutions in this new paradigm. A mobile phone that can accomplish multiple accessibility tasks has the potential to provide the target communities with more independence than they have currently. Furthermore, the MobileAccessibility solution has the potential to be inexpensive and more sustainable than current accessibility solutions. Qualified students with disabilities will be recruited as researchers, giving them a chance to participate in work directly affecting them. New project-oriented curricula based on MobileAccessibility will be created.","title":"HCC: Small: MobileAccessibility: Bridge to the World for Blind, Low-Vision, and Deaf-Blind People","awardID":"1116051","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["557943",485574],"PO":["565227"]},"181328":{"abstract":"A use-after-free error is a software flaw that potentially allows an attacker to remotely inject malicious software or corrupt memory values. Such attacks can result in the theft of private data, propagation of worms and viruses, or the creation of botnet nodes that can be programmed to spew spam or disrupt Internet traffic. Recently, use-after-free vulnerabilities have been found in crucial software such as Microsoft's Internet Explorer, Adobe Acrobat Reader, and Firefox among others. The goal of the Watchdog project is to devise hardware and software mechanisms to prevent all such vulnerabilities.<br\/><br\/>To prevent use-after-free vulnerabilities, the researchers will develop hardware for enforcing safe manual memory management, without compromising system performance. They will study a formal model of their designs to establish the correctness of the techniques. The hardware designs will be prototyped using detailed micro-architectural simulations. The researchers will evaluate correctness and performance by using a suite of benchmark tests and off-the-shelf software. The tools and prototypes will be openly distributed for others to build upon, and the research findings will be integrated into the security and hardware courses taught by the researchers. If successful, the technology developed by this research will have significant societal impacts, improving the security of our computing ecosystem by eliminating an important class of vulnerabilities that is actively being exploited to compromise systems and spread malware.","title":"TC: Small: WATCHDOG: Hardware-Assisted Prevention of All Use-After-Free Security Vulnerabilities","awardID":"1116682","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["556654","556655"],"PO":["564388"]},"185706":{"abstract":"Collaborative Projects: EAGER: A virtual eXchange to support networks of creativity and innovation amongst Science, Engineering, Arts and Design (XSEAD)<br\/><br\/>Intellectual Merit<br\/>One of the greatest challenges facing the United States in research and education is how to fundamentally encourage innovation across all sectors and spawn new solutions to address global challenges. Increasing research evidence and industrial innovations (i.e. mobile computing, social media) confirm that broad interdisciplinary collaborations that include both science and art fields have great potential for spawning creativity and innovation in computer science, engineering and the sciences. An emerging hybrid community of scientists, engineers, artists and designers is producing innovative and entrepreneurial research that advances new knowledge and proposes holistic solutions to societal challenges including health, education and environmental change. Yet, this burgeoning interdisciplinary community continues to face problems in its efforts to self-organize among constraints imposed by academic systems and historical biases; it continues to seek a dynamic and synergizing research and outreach exchange.<br\/><br\/>Building upon lessons-learned, a new Virtual eXchange to support networks of creativity and innovation amongst Science, Engineering, Art and Design (XSEAD) will be developed. The XSEAD project will address the following urgent needs of the interdisciplinary science-art community: establish a cohesive view of the field and provide a mechanism to attract entrepreneurs and industry; create a venue for multimodal documentation of research outcomes; provide extensive databases of prior and current research; allow rapid dissemination of research outcomes; facilitate forming of collaborations and specialized sub-communities; document and help evolve science-art curricula efforts and evaluation approaches; provide context and support mechanisms for science-arts careers; establish evidence of the societal impact of interdisciplinary science-art integration. The software engineering development components of XSEAD will contribute further knowledge in three technical areas: Content organization (improve the effectiveness of algorithms for dynamic, usage based, organization of large multimedia databases); Recommendation algorithms (promote the use of multi-relational structures for providing effective recommendations); Community dynamics (develop novel algorithms to extract structures that encode meaningful interactions in online social networks).<br\/><br\/>Broader Impact<br\/>XSEAD will expose general non-expert audiences to the evolution and potential of collaborative research across science and arts. It will attract the interest of young people searching for careers that combine the rigor of science and engineering with the creativity and reflection of arts and design. It will serve teachers and informal learning communities seeking exemplars for curricular development, active practitioners looking for further institutional opportunities to present and support their ongoing work, academics developing related interdisciplinary efforts and commercial companies seeking cross-trained expertise. XSEAD will enable rapid research exchange and in-depth peer-reviewed scholarship between the worlds of science and art and provide a unique and deeply engaging inroad to a vast and creative repository. XSEAD will help promote new paradigms for developing human centric solutions to complex societal problems (i.e. cost effective health and wellness, globalization and conflict, adaptive K-12 learning, electronic communication and security). These paradigms will combine knowledge across broad and diverse areas of human knowledge.","title":"Collaborative Research: EAGER: A Virtual eXchange to Support Networks of Creativity and Innovation Amongst Science, Engineering, Arts and Design (XSEAD)","awardID":"1141503","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[498455],"PO":["565342"]},"183539":{"abstract":"Continuous technology scaling allows tens or hundreds of processing cores integrated on the same chip; this represents the multicore computing paradigm which makes it possible to run multiple heterogeneous applications concurrently on a single chip. In such multiprocessor systems, individual processing nodes can communicate and coordinate via networks-on-chip (NoCs). Therefore, a major challenge is to determine the mathematical techniques for designing and optimizing such on-chip networks in a rigorous manner.<br\/><br\/>Recent multicore platforms (such as Intel Single Chip Cloud Computer) benefit from the multiple voltage and frequency island (VFI) design style with support for dynamic voltage and frequency scaling (DVFS). In such systems, the voltage and frequency of each island can be set independently of all other islands and adapt at run-time in response to temporal variations in application characteristics. Spatio-temporal workload variations result in various on-chip power and thermal gradients, which also raise major concerns for lifetime reliability. On-chip power and thermal management has therefore become a critical component of every step in the multicore design flow, from physical design all the way up to micro-architecture and system-level design.<br\/><br\/>Starting from these overarching ideas, this project addresses the fundamental issue of designing effective and highly scalable control algorithms for power and thermal management in VFI-based multicore systems. Unlike much of the existing work in this area, the focus of this research is on truly scalable system-level design methodologies that can take advantage of the existing knobs at circuit-level (e.g., voltage, frequency) for systems comprised of hundreds or thousands of cores. At the same time, the proposed control techniques may be useful for regulating other on-chip shared resources such as network bandwidth or off-chip bandwidth.<br\/><br\/>This new design methodology enables the development of a wide variety of energy-efficient multicore applications ranging from gaming and entertainment platforms, to communication systems, data centers, and vehicular traffic management. More broadly, the results of this project impact significantly other research communities by improving the level of understanding of networking concepts needed to design and control complex systems.","title":"Distributed Control and Coordination for Massively Integrated Multicore Platforms","awardID":"1128624","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["554461","554462"],"PO":["551712"]},"185739":{"abstract":"The process of clustering or partitioning of nodes within a graph is a fundamental task with applications in many areas ranging from social network analysis to chip design and from biological network analysis to the analysis of intelligence networks. This project seeks to explore and develop a new class of algorithms for graph clustering based on the principle of stochastic flows. Such algorithms have been used effectively on small scale biological networks and have been shown to be robust to noise effects. However, widespread utilization has been limited due to the lack of scalability of the algorithm and its inability, in its current form, to accommodate domain-specific constraints on clustering.<br\/><br\/>This exploratory project seeks to address these two limitations: First, it seeks to develop a novel approach for supporting flexible clustering in the context of stochastic flow clustering, allowing users to control the skew of the resulting clustering arrangement (e.g., to ensure balanced clusters), and allowing the nodes of a graph to participate in multiple clusters (so as to allow clusters to overlap). Second, it seeks to develop solutions that can scale to very large graphs (e.g. social networks, web graphs) through the innovative applications of graph sparsification and novel parallel algorithms on high performance systems. Open source implementation of the resulting a proof-of-concept solution will be distributed to the broader scientific community. <br\/><br\/>The scientific impact of this exploratory research agenda include the following: First, if one is successful in scaling up stochastic flow algorithms to web-scale datasets while retaining its many advantages, this would open up a viable robust and improved alternative to the current state-of-the art. Second, one can also employ stochastic flow clustering algorithms in a manner analogous to spectral methods on more traditional data sources (non-graphical), enabling more wide-spread use of flow clustering algorithms. <br\/><br\/>The broader impacts of the project include increased research-based training opportunities for undergraduate and graduate students in data analytics. Additional information about the project can be found at: http:\/\/www.cse.ohio-state.edu\/~srini\/EAGER11\/","title":"EAGER: Towards New Scalable Stochastic Flow Algorithms","awardID":"1141828","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["527873"],"PO":["560586"]},"181615":{"abstract":"As the exponential growth of data volumes and complexity continues in all sciences (and indeed all other fields of the modern society, economy, commerce, security, etc.), there is a growing need for powerful new tools and methodologies which can help us extract knowledge and understanding from these massive data sets and data streams. The newly gained knowledge is often used to guide our actions, and in science that typically means follow-up studies and measurements, as the research cycle continues. As the data rates and volume increase, it becomes necessary to take humans out of the loop, and develop automated methods for time-critical knowledge extraction and optimized response to anomalous or interesting events found by the data processing pipelines. This proposal is to develop a system that will be an example of a new generation of scientific experiments and methods that involve real-time mining of massive data streams, and dynamical follow-up strategies. The system would be developed and validated in the context of real scientific situations from the emerging field of time-domain astronomy. A new generation of synoptic sky surveys covers the sky repeatedly, detecting variable or transient phenomena, over a broad range of astrophysics, from the Solar system and stellar evolution, to cosmology and extreme relativistic objects; from extrasolar planets to gamma-ray bursts and supernovae as probes of the dark energy. As we explore the observable parameter space, there is a real possibility of discovery of new types of objects and phenomena. <br\/><br\/>The system will enable exciting new astrophysics, and facilitate discovery. The key to this is a fully automated classification and prioritization of the transient events, and their follow-up observations. This poses some interesting challenges for applied computer science, especially in the area of Machine Learning, including an automated classification where only a sparse, incomplete, and heterogeneous data are available, and contextual information and domain expertise must be folded in the process. The process must be dynamic, incorporating new data as they become available, and revising the classifications accordingly. The system would then generate automatically decisions for an optimal follow-up of the most interesting events, given the available limited assets and resources. This project will aid the entire astronomical community in developing new scientific strategies and procedures in the era of large synoptic sky surveys, facilitate data sharing and re-use, and stimulate further development of Virtual Observatory capabilities. The methods and experiences gained here will be described in the open literature, so that they may find a broader use outside astronomy, wherever similar time-critical situations occur, thus fostering constructive new synergies between applied computer science and other domains. The proposers will train undergraduate and graduate students and postdocs, in the methods of scientific computing and computational thinking, and develop effective EPO materials, touching on both the new science and computation.<br\/><br\/>The challenges posed by the knowledge extraction in the era of data abundance become even sharper in the time-critical situations where we mine the information from massive data streams, especially when the phenomena under study are short-lived, and\/or a rapid follow-up reaction is needed. Potentially interesting phenomena and events must be identified, classified, and prioritized in real time, typically using some combination of the new measurements, and existing archival data and models. Then an optimal decision has to be made as to what is the best follow-up that will provide the essential new information in any given individual case; this can be critical if the follow-up assets are scarce or costly. If the time scales are short, and data rates large, the implication is that humans should be taken out of the loop, and that the classification, prioritization, and follow-up decision process must be fully automated. Machine learning (ML) and machine intelligence tools become a necessity. This proposal is to develop a novel, ML-based system for a real-time classification and prioritization of transient events, using the newly emerging field of time-domain astronomy and synoptic sky surveys as a scientific testbed. The classification problem here is different from the usual situations: the data are sparse and\/or incomplete, heterogeneous, and evolving as the new measurements come in; the decision process has to take into account the uncertainties of the classification process, and the available assets; and so on. While the sky surveys detect transient cosmic events, the scientific returns come from their directed follow-up. It is essential to be able to classify and prioritize interesting events, especially as we move from the present Terascale data streams and tens of candidate events per night, to the future Petascale da","title":"III: Small: Automated Event Classification and Decision Making in Massive Data Streams","awardID":"1118041","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0302","name":"Division of ASTRONOMICAL SCIENCES","abbr":"AST"},"pgm":{"id":"1798","name":"SPECIAL PROJECTS (AST)"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["547607"],"PO":["565136"]},"180416":{"abstract":"This project will explore how remote socialization enabled by information and communication technologies (ICTs) is transforming four occupations: graphic design, automotive engineering, banking, and Internet entrepreneurship. Following a comparative, field-based research design, this research will examine the effects of both organizational environments and socialization tactics on ICT use and consider issues of technology use, socialization, and the changing nature of work. In today's workplaces, it is increasingly common to encounter arrangements in which occupational members are geographically distributed from one another. This new reality calls into question existing theories of socialization and learning practices that highlight the importance of collocated interaction and in situ knowledge transfer. By focusing on how individuals use ICTs to learn what it means to be an occupational member, this research will contribute to a new breed of theory on socialization that indicates the processes, practices, and strategies individuals can use to become effective members of an occupation even though they work remotely from others. By drawing on recent theorizing, which suggests that ICTs may provide particular affordances for interaction that non-mediated (e.g. face-to-face) contexts do not, it will explore the possibility that remote socialization may, in fact, help occupations to transform themselves. One aim is to build theory about the mechanisms by which technology leads to occupational transformation. <br\/><br\/>Occupational skill is critical to economic success, social progress, and individual well-being. However, many occupations seem to be failing to adapt quickly to changes in science, technology, and policy. The failure of occupations to change and refashion themselves to meet new social and technological pressures portends job loss from reduced skill for American citizens, and potentially increased outsourcing to other countries. This study will provide insight into how technology-enabled remote socialization may be able to contribute to faster occupational transformation. Although, for many years, ICT-mediated communication has been seen to be impoverished when compared to face-to-face communication, but now that it has developed considerably, ICT-mediated communication may provide more opportunities for workers to break free from the inertia of established occupations and develop new work practices and strategies that move the occupation forward. Additionally, it is imperative that new occupational members learn how to effectively acquire the knowledge and skills they need to perform their jobs well when they work remotely from others. This study will provide insight into effective practices of remote socialization and occupational learning such that individuals who are attempting to learn new work practices and knowledge will be successful in their efforts.","title":"HCC: Large: Collaborative Research: Information Technology, Remote Socialization, and the Development of Occupational Identity","awardID":"1111237","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[483518],"PO":["564456"]},"181637":{"abstract":"During database query processing, data migrates across different system components each of which may offer different levels of security guarantees and may be susceptible to different attacks. If the underlying data (or part of it) is sensitive, data migration, especially from secure components to those that are relatively insecure could increase risk of data loss. This proposal takes a \"risk-based\" approach to security wherein instead of designing approaches to prevent attacks, the proposed research controls flow of data during query processing through various components in such a way to strike a balance between risk of exposure and system performance. Risk-aware query processing techniques is explored in two settings: (a) A stand-alone database server where data on disks is stored encrypted (and is hence secure) but is loaded in plaintext into memory during query processing. (b) Cloud computing environment where, during peak load queries (and corresponding data) are shipped from (relatively secure) private storage to be processed at (relatively insecure) public cloud infrastructure, a phenomena known as cloudbursting. In both these settings, techniques to co-optimize query execution to simultaneously minimize both disclosure risks as well as performance costs are explored. The research offers a complementary approach to traditional techniques based on preventing attacks to support practical security in the context of database systems.<br\/><br\/>The project seeks to help launch a new direction to database security research that explores techniques to limit risks (instead of only preventing attacks). The research, if successful, will make cloud-based data management solutions more secure increasing the rate of their adoption even for applications that have substantial data confidentiality concerns. Finally, the planned research, system development, and educational activities are expected to significantly enhance the educational experience of students.","title":"TC: Small: Risk Aware Query Processing in Mixed Security Database Environments","awardID":"1118127","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["515756"],"PO":["565327"]},"181406":{"abstract":"This project addresses network virtualization by developing a suite of bandwidth allocation and packet scheduling techniques as an integrated framework for switch virtualization. The goal is to achieve scalable, efficient, and isolated link sharing among virtual networks. <br\/><br\/>Network virtualization allows for the creation of multiple coexisting logical networks, each customized to a specific purpose, by reorganizing the resources of a common physical infrastructure. Network virtualization has many important applications in delivering reduced hardware cost, improved resource utilization, simplified maintenance, and incremental service deployment. Implementation of network virtualization relies on creating slices for various resources of the underlying physical infrastructure. However, effective solutions are needed to create slices for switches, the important interconnecting components of any network. This project develops Scalability and Efficiency Aware Link Sharing (SEALS) framework for switch virtualization. <br\/><br\/>This project considers two types of switch virtualization: intra-switch virtualization which creates multiple logical switches within a physical one, and inter-switch virtualization which creates a logical switch by combining multiple heterogeneous physical ones. The SEALS framework accurately emulates the ideal Generalized Processor Sharing (GPS) model to realize scalable, efficient, and isolated switch virtualization. <br\/><br\/>A combination of theoretical analysis, simulations, and experiments are used to evaluate the design. The SEALS framework is implemented in OpenFlow switches to obtain experimental data from a realistic environment. The outcome of this project will facilitate the creation of commercial and academic virtual networks to build cost-saving infrastructure or experiment new network prototypes. <br\/><br\/>The educational component of this project offers research opportunities in computing to underrepresented minority groups, through summer workshops, curriculum development, and undergraduate research programs. Florida International University is among the top grantors of computer science and engineering degrees to Hispanic students in the nation. Its history of involving underrepresented groups in research efforts will be leveraged during the course of this project. The project web site will be used for dissemination of all publications, course materials, and source code resulted from the project.","title":"CSR: Small: A Scalable and Efficient Framework for Switch Virtualization","awardID":"1117016","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[486056],"PO":["551712"]},"181527":{"abstract":"This project represents an ongoing collaboration between teams at two institutions. As people live longer, blindness caused by degenerative diseases of the retina such as macular degeneration or retinitis pigmentosa is today a major disability among the aging in the developed world. These types of \"neural\" blindness cannot currently be medically treated in any satisfactory manner. There is now compelling experimental evidence in humans that even when such diseases cause a loss of photoreceptors (i.e., rod and cone cells in the retina), electrical stimulation of the remaining retinal neurons that survive this loss can be used to bypass the damaged tissue and deliver visual information to the brain. This is essentially the same concept that supported the development of the cochlear prosthesis, which has been a fabulous success, restoring hearing to many tens of thousands of deaf patients. The PIs and their respective teams have been working for over 20 years toward the goal of developing a retinal prosthesis to restore truly useful vision to patients in an analogous manner. With prior funding from a number of agencies including NSF, they have created enabling technology for a miniaturized high-density implantable wirelessly-driven neuro-prosthesis package with over 200 individually-addressable channels, which is over three times the inputs and outputs in any current commercially available neurostimulator. The field's ability to create complex integrated circuitry for neurostimulation and\/or recording has outpaced the development of long-term implantable packaging, microelectrode array, and assembly technology. If optimized, those technologies would make possible new devices that interface with hundreds of neural tissue sites simultaneously. This is the PI's aim in the current project. The funding will complement existing grants to the PIs and their collaborators, and will allow them to complete development of a new 200+ channel co-fired ceramic signal feed-through disc, to optimize the micro-fabrication process for high-density microelectrode arrays that interface with neural tissue, and to improve the bonding and interconnection processes required to assemble the implant package. <br\/><br\/>Broader Impacts: The 200+ channel wirelessly-driven implant that will constitute the primary project outcome will have over three times the number of individually-addressable stimulating electrodes now available from any group. This funding will further allow the PI to ready devices for later pre-clinical testing (with anticipated follow-on support from the VA). Project results will be widely disseminated in publications, and by distributing sample devices within the rehabilitation R&D community. The device which is the focus of this project will also be useful in a myriad other future chronically implantable prosthetics, palliative devices, and human-computer interface devices.","title":"HCC: Small: Collaborative Research: Packaging Optimization for Next-Generation Implantable Human-Computer Interface Devices","awardID":"1117601","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[486350,486351,486352],"PO":["565227"]},"181417":{"abstract":"More than seven years after traditional processor designs hit the edge of their power envelope, the path of extreme scale Computational Science to a 100 petaflop (Pflop\/s) system, which researchers had hoped to be using by the middle of the coming decade, has never looked steeper. On current high performance computing (HPC) systems, the 'application-architecture performance gap,' i.e. the gap between theoretical peak performance and the performance realized by full applications is already substantial. But with clock frequencies now capped below 4 GHz and trending downward, latencies in key areas (e.g. memory access, bus, system interconnect) expected to remain relatively stagnant, and software parallelism required to increase by at least three orders of magnitude to make effective use of the tens of thousands of processors and millions of cores that 100 Pflop\/s systems are projected to contain, it is now reasonable to worry that a widening application-architecture performance gap will make such systems unproductive to use and therefore irrational to build. <br\/><br\/>The proposed research aims to provide the kind of coordinated math and computer science research effort needed to solve the interrelated cluster of software problems that threaten to cripple application performance on future large-scale systems. Under the PULSAR project, the PIs use a variety of both classic and novel dense linear algebra algorithms to explore the potential of well known, but now little used systolic array design principles to exploit all the power that future multi-core and heterogeneous systems, built to extreme scales. If a software platform that virtualizes classic systolic array architecture, allowing for suitable flexibility in the granularity of its application can be created, then libraries and applications that use this data-driven execution model to achieve outstanding performance and scalability on future massively parallel and data-starved HPC systems can be produced.","title":"SHF: Small: Parallel Unified Linear algebra with Systolic ARrays (PULSAR)","awardID":"1117062","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["558550",486083,486084],"PO":["565272"]},"184816":{"abstract":"University network facilities, both those dedicated to research and those that support core institutional activities, play a very important role in the Global Environment for Network Innovations (GENI) project. Campus networks pose interesting and unique challenges. GENI expects networked university research facilities to exploit existing university networks using linkage mechanisms that permit the use of experimental protocols down to the lowest layers in the stack and nodes that are programmable by experimenters, all without disrupting production use of the same networks. Deploying such infrastructure in a campus environment requires close coordination with university IT departments.<br\/><br\/>This award funds a workshop with researchers, campus network leaders, and GENI and NSF staff to address concrete campus implementation options. Meeting invitees will be among the top experts in higher-education networking and information technology, representing universities across the country. The purpose of the proposed meeting and other activities is to promote effective, efficient communication between the GENI staff and higher education CIO community. The report from this meeting will assist GENI in determining the most effective means for connecting the higher education research community to any future, experimental network infrastructure.<br\/><br\/>The meeting will have broader impacts of at least three types. First, it will lay the foundation for much more capable networking than the current Internet protocols can support. Second, it will enhance the ability of researchers to interact with network operators in ways that balance robustness and flexibility. Third, it will enable EDUCAUSE to disseminate information and conclusions from the meeting to a broader constituency, thus, helping them to contemplate future networking opportunities more efficiently.","title":"Advancing GENI Campus Implemention","awardID":"1136623","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[495795],"PO":[495796]},"181208":{"abstract":"With the proliferation of multicore processors has come resurgence<br\/>of interest in parallel programming languages and models,<br\/>particularly those intended to make it easier for non-expert<br\/>programmers to correctly implement important classes of parallel<br\/>applications. Unfortunately, most such languages and models are <br\/>informally -- and thus imprecisely -- defined. The aim of the <br\/>sponsored research is to develop more formal definitions, which will<br\/>be needed in order to truly understand and reason about programs,<br\/>guide language implementations, and verify implementation<br\/>correctness. Within computer science and allied fields, formal<br\/>definitions will facilitate the transition to ubiquitous parallel<br\/>computing. For society at large, this transition will be essential<br\/>to maintain the momentum of the IT revolution, across government,<br\/>industry, science, the arts, and entertainment.<br\/><br\/>The technical core of the sponsored research is the use of<br\/>history-based executions to capture both the behavior of individual<br\/>threads of control and the interactions among those threads. In a<br\/>departure from previous work, the interactions are always expressed<br\/>in terms of atomic blocks, which can capture arbitrary<br\/>language-level synchronization mechanisms. Specific topics being<br\/>addressed include transactional memory (including the concepts of<br\/>publication and privatization), explicit speculation, and <br\/>determinism. The notion of determinism, in particular, is central<br\/>to several emerging languages and models specifically intended for <br\/>non-expert programmers. A formal framework for the definition of<br\/>determinism will allow alternative definitions to be compared,<br\/>contrasted, and correctly implemented.","title":"SHF: Small: Ordering-Based Semantics for Emerging Models of Parallel Computing","awardID":"1116055","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["564815"],"PO":["564588"]},"190724":{"abstract":"Ensuring reliable computation at the nanoscale requires mechanisms to detect errors. The PIs propose fundamental research for developing efficient hardware techniques to support online error detection and manufacturing test by monitoring invariant relationships. These invariant relationships naturally occur across multiple levels of digital logic and across multiple time cycles. Violations of these relationships indicate that errors have occurred, either because of transient faults or manufacturing defects; thus monitoring them in hardware can significantly improve circuit reliability. While other techniques exist for error detection, this approach has several advantages, including significantly lower power dissipation, no high-level information requirements, fine-grained optimization capabilities, and providing a potentially powerful source of diagnostic information. A key challenge in this project is the efficient selection of an optimal set of implications to include in the hardware, such that desired reliability is obtained with low overhead. <br\/><br\/>Reliable operation of logic devices is key for the continued push for smaller and faster electronic circuits. Any benefit in performance and power brought forth by rapid scaling of transistors cannot be fully realized if high reliability cannot be guaranteed for systems composed of these devices. The proposed research is a collaborative effort between Brown and Bucknell Universities. The project involves undergraduate students, many of whom are women and under-represented minorities. The PIs will use this project to create new opportunities to expose undergraduates to research, and to develop outreach workshops to encourage women and under-represented minorities to pursue degrees in computing.","title":"SHF: Small: Collaborative Research: Using Identified Circuit Invariance for Online Error Detection","awardID":"1205176","effectiveDate":"2011-08-01","expirationDate":"2012-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7945","name":"DES AUTO FOR MICRO & NANO SYST"}}],"PIcoPI":[511316],"PO":["562984"]},"180703":{"abstract":"Optical networking forms the foundation of the global network infrastructure, hence the planning and design of optical networks is crucial to the operation and economics of the Internet and its ability to support critical and reliable communication services. This research project aims to increase greatly our ability to solve optimally a range of optical design problems. In particular, the project will develop compact formulations and solution approaches that can be applied efficiently to instances encountered in Internet-scale environments. The ultimate goal is to lower the barrier to entry in fully exploring the solution space and in implementing and deploying innovative designs. The solutions to be developed are ?future-proof? with respect to advances in dense wavelength division multiplexing (DWDM) transmission technology, as the size of the corresponding problem formulations is independent of the number of wavelengths.<br\/><br\/>Specific outcomes include: (a) computationally efficient formulations for the routing and wavelength assignment (RWA) problem, a subproblem of many design problems; (b) scalable optimal solutions to a suite of network design problems (including traffic grooming, survivability, and impairment-aware routing) that can be applied to topologies encountered in practice; (c) investigation and characterization of limiting solutions and technology tradeoffs in the ?large W? regime that is consistent with DWDM technology trends; (d) benchmarking of existing heuristics and development of new polynomial-time algorithms that leverage the structure of optimal solutions; and (e) a set of integrated community building, research collaboration, and technology transfer activities that engage a wide range of students, including underrepresented groups in computer science.<br\/><br\/>Broader Impact: The project will develop new capabilities for the design and operation of optical networks that form the backbone of the Internet infrastructure. The proposed framework with its emphasis on scaling optimal solutions to networks of realistic size will open new directions for network design by permitting extensive ?what-if? analysis to explore the sensitivity of design decisions to forecast traffic demands, capital and operational cost assumptions, service price structures, etc. The research agenda has the potential enable a wide range of 21st century science, education, and commercial applications through the design of networks that are better optimized for user and application requirements and are less expensive to build and operate.<br\/><br\/>Optical network design is an interdisciplinary area of research for students of networking that incorporates techniques from mathematical programming, discrete optimization, and operations research. Research results will be disseminated not only through journal and conference publications, but also through participation in community building and technology transfer activities and through graduate and undergraduate education, adding to the impact of the project. Graduate students engaged in this project will be provided with a collaborative setting that will expose them to international research environments. New course material and class projects will be developed to demonstrate the fundamental concepts and to present the results of advanced research to students taking these courses. The PI will also organize workshops on specific network design themes, and follow up with an edited book that will collect the research related to optimization techniques for optical networks.","title":"NeTS: Small: Computationally Scalable Optical Network Design","awardID":"1113191","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[484227],"PO":["564993"]},"180516":{"abstract":"Computer networks are now, arguably, the United States' most critical infrastructure. They control all communication amongst our citizenry, our businesses, our government, and our military. Worryingly, however, today's networks are remarkably unreliable and insecure. A significant source of vulnerability is the fact that the underlying network equipment (e.g., routers and switches) run complicated programs written in obtuse, low-level programming languages, which makes managing networks a difficult and error-prone task. Simple mistakes can have disastrous consequences including making the network vulnerable to denial-of-service attacks, hijackings, and wide-scale outages.<br\/><br\/>The goal of this research is to transform the way that networks are managed by introducing a new class of network programming languages with the following essential features: (i) network-wide, correct-by-construction abstractions; (ii) support for fault-tolerance and scalability; (iii) coordination with end-hosts and independently-administered networks, as well as mechanisms for establishing trust between them; (iv) formal verification tools based on rigorous semantic foundations; and (v) compilers capable of generating efficient and portable code that runs on heterogeneous equipment. To demonstrate how to build a language with these features, the researchers are designing a language for OpenFlow networks called Frenetic, and evaluating it on several novel security applications. This project will have broad impact by (i) discovering key techniques for increasing the reliability of our networks, (ii) opening up the interfaces used to program networks, thereby enabling grass-roots innovation where it was previously not possible, and (iii) educating a new community of researchers with advanced skills in both networking and programming languages.","title":"TC: Large: Collaborative Research: High-Level Language Support for Trustworthy Networks","awardID":"1111698","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":[483774,"556302"],"PO":["565327"]},"181627":{"abstract":"Computer users are increasingly faced with decisions that impact their personal privacy and the security of the systems they manage. The range of users confronting these challenges has broadened from the early days of computing to include everyone from home users to administrators of large enterprise networks. Privacy policies are frequently obscure, and security settings are typically complex. Missing from the options presented to a user is a decision support mechanism that can assist her in making informed choices. Being presented with the consequences of decisions she is asked to make, among other information, is a necessary component that is currently lacking.<br\/><br\/>This work introduces formal argumentation as a framework for helping users make informed decisions about the security of their computer systems and the privacy of their electronically stored information. <br\/>Argumentation, a mature theoretical discipline, provides a mechanism for reaching substantiated conclusions when faced with incomplete and inconsistent information. It provides the basis for presenting arguments to a user for or against a position, along with well-founded methods for assessing the outcome of interactions among the arguments. An elegant theory of argumentation has been developed based on meta rules characterizing relationships between arguments. Rules for argument construction and evaluation have been devised for specific domains such as medical diagnosis. This project investigates argumentation as the basis for helping users make informed security- and privacy-related decisions about their computer systems. Three specific aims are addressed:<br\/>1) Implementation of an inference engine that reasons using argumentation,<br\/>2) Facilitate security management through an argumentation inference engine, a rule base specialized for security management, and sensors providing security alerts all enhanced with an interactive front-end.<br\/>3) Reason about the consistency and completeness of domain knowledge, as it evolves.<br\/>To understand the kinds of domain-specific inference rules required, diverse security applications are studied, such as determining if an attack imperils a particular system, finding the root cause of an attack, deciding on appropriate actions to take in the presence of an uncertain diagnosis of an attack, and deciding on privacy settings. <br\/>Emerging from this project will be a prototype towards the practice of usable security. The team is working with organizations responsible for the security administration of large enterprise networks and will make the prototype tools available to these organizations. The team is working with everyday users from a cross-section of community members. <br\/>Curricular modules that cover the intersection of argumentation and security are being developed and shared.","title":"TC: Small: Collaborative Research: An Argumentation-based Framework for Security Management","awardID":"1118077","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1104","name":"Division of UNDERGRADUATE EDUCATION","abbr":"DUE"},"pgm":{"id":"1668","name":"FED CYBER SERV: SCHLAR FOR SER"}}],"PIcoPI":["521752","521753"],"PO":["564388"]},"181517":{"abstract":"Safety-critical systems refer to systems whose failure or malfunction may result in death or serious injury to people, loss or severe damage to equipment, or environmental harm. One key issue is whether it is safe to operate wireless devices for monitoring, control, and coordination in these systems, or whether current wireless devices can safely coexist with these critical devices? This project develops novel and holistic solutions to monitoring, policing and assessing the coexistence of wireless devices in and near safety-critical systems. Intellectually, the project advances the state-of-the-art in both algorithm design as well as experimental systems related to safety-critical applications through 1) the design of a distributed passive monitoring system that captures cross-layer information regarding radio frequency activities, 2) the development of device profiling and identification algorithms for characterizing and tracking active devices, 3) the development of wireless advisory to identify and make recommendations on the proper responses to potentially safety hazardous conditions, and 4) validation through real-world experiments. <br\/><br\/>Addressing the co-existence issues of wireless devices for safety-critical applications has far-reaching societal impacts. Ensuring the safe operation of wireless technologies in or near safety-critical applications can automate the process, reduce human errors, and avoid safety hazards. The interdisciplinary nature of this transformative research contributes to the development of the nation's future workforce by ensuring that undergraduate, pre-doctoral, and postdoctoral students receive the didactic and research experiences necessary to engage in integrative and team approaches to solve complex problems.","title":"NeTS: Small: Toward Wireless Co-existence For Safety-critical Applications","awardID":"1117560","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["486325",486325,"540080","533458","540080"],"PO":["557315"]},"180549":{"abstract":"Data provenance refers to the history of the contents of an object and its successive transformations. Knowledge of data provenance is beneficial to many ends, such as enhancing data trustworthiness, facilitating accountability, verifying compliance, aiding forensics, and enabling more effective access and usage controls. Provenance data minimally needs integrity assurance to realize these benefits. Additionally, provenance data may need assurances of confidentiality (e.g., protect the identity of a reviewer in a blinded paper review process from the authors but not from the editor) or of privacy (e.g., do not disclose identity of a source without the source's consent). In the past decade there has been significant progress regarding the structure and representation of provenance data as a directed acyclic graph. However, currently there is no overarching, systematic framework for the security and privacy of provenance data and their tradeoffs with respect to the utility of provenance data. The development of such a framework is recognized as one of a handful of promising thrusts in recent reports on Federal game-changing R&D for cyber security, particularly aligned with the theme of Tailored Trustworthy Spaces.<br\/><br\/>This project is to develop a comprehensive technical and scientific framework to address the security and privacy challenges of provenance data, and the attendant tradeoffs, so that our society can gain maximum benefit from applications of provenance data. Detailed foundational research is to be performed on security enhanced data models, access control and usage models, privacy including annonymization and sanitization, integrity, accountability and risk management techniques for provenance data. This foundational research is complemented by data provenance case studies in scientific and cyber security information sharing, and construction of prototype data provenance systems at the operating systems and data layers. Moreover, reference architectures and definitions of corresponding provenance management services are to be defined, identifying how these services can be effectively deployed in enterprises, and developing a risk-management framework to guide application architects, designers and users to effectively embed data provenance in their specific context. The project results will beneficially impact society at large by increasing trustworthiness of data acquired, transmitted and processed by computer systems. From the educational side, both theory and practice of data provenance are to be integrated in the undergraduate and graduate training of students, including underrepresented minority and female students, in all the collaborative institutions of this project.","title":"TC: Large: Collaborative Research: Privacy-Enhanced Secure Data Provenance","awardID":"1111925","effectiveDate":"2011-08-01","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["504045",483862,483863,483864],"PO":["565264"]},"181407":{"abstract":"Multicore processors are becoming ubiquitous among high-end servers, personal computers, mobile phones and embedded devices. While these processors can provide higher performance at lower power than single-core architectures, they pose significant challenges for system designers. This is particularly true in real-time embedded systems, such as in avionic and automotive control, manufacturing, and healthcare. In these systems, tasks often have critical timing requirements, with deadlines that must be met to avoid potentially catastrophic outcomes. Providing timing guarantees on multicore processors is made difficult by contention for shared on-chip caches and memory bus bandwidth, and other factors such as hardware interrupts, instruction pipelines and simultaneous multithreading that affect predictability.<br\/><br\/>This project focuses on the design of a real-time operating system that addresses micro-architectural resource contention and enforces predictable behavior on multicore processors. This system called ?Quest? is centered around time as a first class resource, guaranteeing real-time shares of computational resources among all tasks, including interrupts and application threads. Using hardware performance counters available on modern processors, this project implements a real-time performance monitoring subsystem to influence resource management.<br\/><br\/>An additional aspect of this work focuses on the use of hardware sandboxing techniques to guarantee the system is not compromised by ill-written software, such as drivers or services implemented by third party developers. This is especially relevant given the complexity of modern software systems that cannot easily be verified to behave correctly using only static methods. As part of the development of Quest, hardware virtualization capabilities are investigated to enforce heightened software reliability, in particular, techniques for isolating software components of a system, while ensuring predictability, <br\/><br\/>The outcomes of this work will impact the design of systems software on multicore processors. A greater understanding of hardware features to improve or ease the construction of software systems is gained, especially those with safety-critical requirements. Investigations into system predictability and safety will help prevent potentially disastrous outcomes for mission-critical tasks. This work could lead to more robust systems that avoid costs of failure in terms of loss of lives, equipment, or money. The Quest software will be shared among the research community and used in courses involving operating systems.","title":"CSR: Small: Quest: A Real-Time Operating System for Multicore Processors","awardID":"1117025","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["68374"],"PO":["551712"]},"181528":{"abstract":"Developers spend most of their time debugging software. This effort<br\/>results in perfective changes to their applications, but is otherwise<br\/>lost. No central repository exists that stores all bug descriptions<br\/>and fixes. The reason for this state of affairs is the belief that<br\/>debugging is an idiosyncratic, context-specific task that does not<br\/>generalize. In contrast, this project hypothesizes that applications<br\/>decompose into smaller, similar problems and that limitations of the<br\/>human mind imply that we are likely to make similar errors when<br\/>confronted by similar problems. In practice, most developers<br\/>agree. When fixing a bug, a developer often begins by searching for<br\/>similar bugs that have been reported and resolved in the past, because<br\/>a fix for a similar bug can help him understand his bug, or even<br\/>directly fix his bug. In short, the problem is that the knowledge<br\/>gained during debugging is wasted---either not stored or not<br\/>searchable.<br\/><br\/>This project seeks to revolutionize debugging by capturing and reusing<br\/>debugging knowledge. Developers can leverage the knowledge of the<br\/>community to speed up debugging. To this end, it proposes to create a<br\/>universal bug repository capable of storing all bug information,<br\/>indexed on bug traces. To speed debugging, this repository will<br\/>support efficiently finding similar bugs and their fixes. It will be<br\/>the basis of automatic debugging tools that match closed bugs to an<br\/>open bug, then test the applicability of their fixes to that open<br\/>bug. Programmers will consult the proposed bug repository as a matter<br\/>of course during debugging. Monitors that compare the current<br\/>execution against traces in the repository can also prevent bugs and<br\/>improve software reliability. This project promises to significantly<br\/>speed up debugging and reduce software production cost. The proposed<br\/>educational innovations and outreach efforts can also help train more<br\/>capable IT professionals for the workforce.","title":"SHF: Small: Reusing Debugging Knowledge","awardID":"1117603","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["531418","562712"],"PO":["564588"]},"181418":{"abstract":"The goal of this project is to provide protection against exploits through untrusted third-party software components and against malicious application manipulation. These problems constitute an important class of vulnerabilities in current software, and are tied to a common denominator -- the lack of ability to divide a program and the data manipulated by it in a fine-grained manner and to control the interactions between the resulting constituents. <br\/><br\/>This project proposes practical fine-grained ``least privilege'' enforcement through a computation model where the heap can be divided into a dynamic number of memory domains. Security contexts --- called secure memory views (SMVs) --- can be defined that map privileges of code executing within them onto memory domains. Threads are allowed to dynamically switch their set of privileges (i.e., switch which SMVs they are bound to) in a secure and controlled fashion, termed security context switching. These ideas are realized through three core contributions. (1) A programming model is devised to allow the application programmer to easily apply the proposed techniques, while also providing enough structure to reason about the resulting properties and to secure these. (2) The new concepts are reified within a modern mainstream programming language through development of a compiler supporting the language extensions, as well as modifications of the language runtime. (3) An efficient implementation of security context switching is proposed, involving both the language runtime and the operating system kernel itself. To validate the research, a popular web browser as well as a web server are enhanced to use SMVs. By using widely employed open-source software for implementing and validating the proposed support our concepts become available to a large community for use as well as further research and development.","title":"TC: Small: Least Privilege Enforcement through Secure Memory Views","awardID":"1117065","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":[486086],"PO":["564388"]},"181539":{"abstract":"Information technology is an established component of the infrastructure of modern societies, and cryptography is a cornerstone of information security. Provable security quantifies the resilience of cryptographic constructions to attacks. This quantification is often relative to the hardness of a handful of reference \"intractable\" problems like integer factorization. Because of the intrinsic strength of heterogeneity, diversifying the set of intractable problems enhances the robustness of the overall structure. This project responds to the challenge by building a foundation for cryptography from problems in combinatorial group theory.<br\/><br\/>The project capitalizes on earlier efforts on the algorithmic unsolvability of standard computational problems in combinatorial group theory like the \"word problem\", but looks at them through the lens of the probabilistic modeling, characteristic of modern cryptography. The research objectives can be grouped into three main categories: 1) identifying efficiently sampleable distributions on which (variants of) the standard computational group-theoretic problems remain _difficult on average_; 2) developing new _hard group-theoretic learning problems_; and 3) exploring the implications of this foundational work to applications with enhanced cryptographic functionality, with the long-term goal of deriving group-theoretic instantiations of public-key cryptography with homomorphic properties.<br\/><br\/>Results of this project will enrich the theory and practice of cryptography with new tools and alternatives, stimulating for researchers throughout the cryptography and group-theory communities, thus fostering collaborations between the two fields. The undertaking of the research entails significant student participation in research at institutions that serve demographic groups that have been historically underrepresented in computer science and mathematics.","title":"TC: Small: Collaborative Research: Provable Security from Group Theory and Applications","awardID":"1117675","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":[486388,"531699","534474",486391],"PO":["565239"]},"180506":{"abstract":"Today's Internet has some 1.7 billion users, fosters an estimated $1.5 trillion in annual global economic benefits, and is widely agreed to offer a staggering array of societal benefits. The network sees enormous demand---on the order of 40 Tbps of inter-domain traffic and an annual growth rate of 44.5%. Remarkably, in spite of the Internet's importance and rapid growth, the core protocols that support its basic functions (i.e., addressing, naming, routing) have seen little fundamental change over time.<br\/><br\/>However, the Internet is now in the midst of its first fundamentally disruptive modification: a phase transition imposing extensive changes throughout the network's vast set of components. On the 3rd of February, 2011 the Internet Assigned Numbers Authority (IANA) allocated the five remaining blocks of IPv4 address space to the five Regional Internet Registries (RIRs). This event was a watershed moment for the Internet, in that it represented the exhaustion of the Internet's pool of unallocated IPv4 addresses. While eliminating poor utilization of existing allocated IPv4 address space and application of techniques for IPv4 address re-use (i.e., Network to Address Translation (NAT) and Dynamic Host Configuration Protocol (DHCP)) will likely provide short term relief, the eventual scarcity of this critical Internet resource will be a strong catalyst for change.<br\/><br\/>This project seeks to validate the hypothesis that the scarcity of IPv4 addresses and accompanying solutions will have profound effects on the Internet and many of its vital properties, including heterogeneity, openness, security, scalability, reliability, availability, concurrency and transparency. This project's efforts to understand these impacts are divided into three broad areas: (i) studies of the dynamics by which IPv4 addresses are allocated and how these address resources are subsequently used, (ii) understanding the near-term coping mechanisms and transition technologies (e.g., carrier grade NAT, tunneling), and (iii) observing the adoption of longer-term solutions (i.e., the new version of the Internet Protocol, IPv6). To pursue the above goals this project has formed a cross-organizational team of researchers with a long history of scientific Internet measurement (University of Michigan, International Computer Science Institute). The project is bolstered by a substantial supporting team that includes ISPs (AT&T, Merit Networks, Inc.), Internet number resource registries (APNIC, ARIN, AFRINIC, RIPE NCC, and LACNIC), and infrastructure providers (Verisign, Packet Clearing House, Arbor Networks) who will provide access to massive, often unique data.<br\/><br\/>Intellectual Merit: Such measurement---at-scale and across numerous core Internet protocols---presents not only a technical challenge in itself, but offers a once-in-a-generation opportunity to view massive, distributed systems under scarcity and transition. The behaviors observed will inform not only the development of global distributed systems, but also the sciences of network architecture and design, network protocols, and mobile networks. Specific areas of study in these domains potentially include: address transfers, address reclamation, address pollution, address allocation policies and allocation policy abuse, new address space topologies, addresses and identity, dynamic addressing, addressing and routing, addressing and mobility, transition strategies, adoption incentives, adoption tipping points, balkanization in transition, adoption measurement, negative adoption incentives.<br\/><br\/>Broader Impact: For many the Internet has become a ubiquitous aspect of everyday life. A major impact of this work will be directly ensuring the availability and reliability of the Internet through instrumentation, data sharing, and problem diagnosis with operators and networks throughout the global network as it undergoes this singular phase in its functioning. Further, in the spirit of scientific exploration commensurate with this unique phenomenon and measurement, this project will make available the extensive data collected as a means to spur scientific research beyond the specific research of the project team. Finally, based on the observations and studies, this project will develop principles guiding the development of new, more resilient network architectures and protocols.","title":"NeTS: Large: Collaborative Research: Measuring and Modeling the Dynamics of IPv4 Address Exhaustion","awardID":"1111672","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["562327","563433","534318"],"PO":["565090"]},"180517":{"abstract":"Today's Internet has some 1.7 billion users, fosters an estimated $1.5 trillion in annual global economic benefits, and is widely agreed to offer a staggering array of societal benefits. The network sees enormous demand---on the order of 40 Tbps of inter-domain traffic and an annual growth rate of 44.5%. Remarkably, in spite of the Internet's importance and rapid growth, the core protocols that support its basic functions (i.e., addressing, naming, routing) have seen little fundamental change over time.<br\/><br\/>However, the Internet is now in the midst of its first fundamentally disruptive modification: a phase transition imposing extensive changes throughout the network's vast set of components. On the 3rd of February, 2011 the Internet Assigned Numbers Authority (IANA) allocated the five remaining blocks of IPv4 address space to the five Regional Internet Registries (RIRs). This event was a watershed moment for the Internet, in that it represented the exhaustion of the Internet's pool of unallocated IPv4 addresses. While eliminating poor utilization of existing allocated IPv4 address space and application of techniques for IPv4 address re-use (i.e., Network to Address Translation (NAT) and Dynamic Host Configuration Protocol (DHCP)) will likely provide short term relief, the eventual scarcity of this critical Internet resource will be a strong catalyst for change.<br\/><br\/>This project seeks to validate the hypothesis that the scarcity of IPv4 addresses and accompanying solutions will have profound effects on the Internet and many of its vital properties, including heterogeneity, openness, security, scalability, reliability, availability, concurrency and transparency. This project's efforts to understand these impacts are divided into three broad areas: (i) studies of the dynamics by which IPv4 addresses are allocated and how these address resources are subsequently used, (ii) understanding the near-term coping mechanisms and transition technologies (e.g., carrier grade NAT, tunneling), and (iii) observing the adoption of longer-term solutions (i.e., the new version of the Internet Protocol, IPv6). To pursue the above goals this project has formed a cross-organizational team of researchers with a long history of scientific Internet measurement (University of Michigan, International Computer Science Institute). The project is bolstered by a substantial supporting team that includes ISPs (AT&T, Merit Networks, Inc.), Internet number resource registries (APNIC, ARIN, AFRINIC, RIPE NCC, and LACNIC), and infrastructure providers (Verisign, Packet Clearing House, Arbor Networks) who will provide access to massive, often unique data.<br\/><br\/>Intellectual Merit: Such measurement---at-scale and across numerous core Internet protocols---presents not only a technical challenge in itself, but offers a once-in-a-generation opportunity to view massive, distributed systems under scarcity and transition. The behaviors observed will inform not only the development of global distributed systems, but also the sciences of network architecture and design, network protocols, and mobile networks. Specific areas of study in these domains potentially include: address transfers, address reclamation, address pollution, address allocation policies and allocation policy abuse, new address space topologies, addresses and identity, dynamic addressing, addressing and routing, addressing and mobility, transition strategies, adoption incentives, adoption tipping points, balkanization in transition, adoption measurement, negative adoption incentives.<br\/><br\/>Broader Impact: For many the Internet has become a ubiquitous aspect of everyday life. A major impact of this work will be directly ensuring the availability and reliability of the Internet through instrumentation, data sharing, and problem diagnosis with operators and networks throughout the global network as it undergoes this singular phase in its functioning. Further, in the spirit of scientific exploration commensurate with this unique phenomenon and measurement, this project will make available the extensive data collected as a means to spur scientific research beyond the specific research of the project team. Finally, based on the observations and studies, this project will develop principles guiding the development of new, more resilient network architectures and protocols.","title":"NeTS: Large: Collaborative Research: Measuring and Modeling the Dynamics of IPv4 Address Exhaustion","awardID":"1111699","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["549845","553725"],"PO":["565090"]},"180407":{"abstract":"The goals of this research are (1) to understand ways of recruiting and socializing volunteers to online production communities like Wikipedia, (2) to design processes and tools that assist the newcomers' information-seeking as part of their socialization, and (3) to build processes and tools to support the interpersonal processes of socialization, including peer mentorship and mentorship with more senior community members. Online production communities are becoming increasingly important, because they are creating the software that drives the Internet, generating valuable scientific data and building history's largest encyclopedia. In the face of inevitable turnover, every online community must incorporate successive generations of newcomers to survive. Newcomers are a source of content, labor, new ideas, and audience. However, attracting and incorporating newcomers into existing communities can be difficult. Socialization is the process of teaching newcomers the behaviors and attitudes essential to playing their roles in the group. Communities have available a variety of socialization tactics. Research from offline organizations shows that organizations' use of institutionalized socialization tactics and newcomers' active information seeking are effective in increasing newcomers' commitment to the organization, their satisfaction and their productivity. However, those tactics are not commonly used in online communities and seem to have different effects when they are used.<br\/><br\/>This project pursues theory-guided design. The findings from the research will extend existing theories on socialization in groups and organizations by supplementing findings primarily based on self-report measures with ones based on behavioral measures, and by providing evidence on socialization in online communities, where constraints on newcomers are radically different than they are offline. The research will develop processes and tools for solving important problems of newcomers' socialization, which will be evaluated in the context of socializing newcomers in Wikipedia and especially in the Wikipedia initiative of the Association for Psychological Science, which seeks to improve the scientific quality of articles in psychology. These tools will be made freely available to other scientific associations and other online production communities more generally.<br\/><br\/>This project supports NSF's mission to inform the public about science by improving Wikipedia as a vehicle for disseminating scientific knowledge about psychology in particular, and by developing a model for how other scientific societies could partner with Wikipedia or similar efforts to better generate and assess scientifically up-to-date and accurate information meant for the public. It will directly involve many college students, who will be assigned to write or improve psychology articles; they will get feedback from the broader community on their performance. As such it directly supports teaching and learning psychological science and will help increase students' involvement with their scientific societies.","title":"Collaborative Research: Supporting Newcomer Socialization in Online Production Communities","awardID":"1111201","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7953","name":"SOCIAL-COMPUTATIONAL SYSTEMS"}}],"PIcoPI":["518491",483488],"PO":["564456"]},"181507":{"abstract":"The goal of this project is to develop machine learning models that leverage underutilized data and account for individual patient differences to improve diabetes management. Diabetes is a chronic disease, which must be treated and managed over a lifetime. In type 1 diabetes, the pancreas does not produce insulin, an essential hormone needed to convert food into energy. The disease is treated with insulin and managed by monitoring and controlling blood glucose levels. Good blood glucose control is key to avoiding serious diabetic complications, but achieving and maintaining good blood glucose control is difficult. Patients are highly individual in their responses to treatment and to life events that affect blood glucose levels. Large volumes of blood glucose data are collected automatically, but automated analysis is lacking. Patients do not always know when problems are impending; problems occurring while patients are asleep are especially dangerous. Machine learning models that predict blood glucose levels would enable or facilitate new applications of direct benefit to patients, including: alerts to immediately notify patients of impending problems; decision support systems recommending actions to prevent problems; and educational simulations showing the effects of different treatment choices or lifestyle options on blood glucose levels.<br\/><br\/>The task of blood glucose prediction is approached as a time series forecasting problem. Blood glucose is predicted based on a patient's prior blood glucose levels, insulin data, meal data, exercise data, sleep patterns and work schedules. Batch and incremental time series regression models, including support vector machines and neural networks, are being investigated. To account for individual patient differences, separate models are trained for each patient. However, transfer learning may enable data from multiple patients to aid in building models for patients with limited historical data. Models are sought that are robust in the face of imperfect data, including missing life events, inaccurately recorded life events, and noisy glucose sensors. Disjoint sets of training and testing data extracted from non-overlapping time intervals are used to build models that are compared against baselines such as autoregressive integrated moving average models. Standard metrics for comparing models, such as root mean square error and coefficient of determination, will be supplemented with domain dependent measures of goodness, including the Clarke error grid.<br\/><br\/>This work aims to improve the overall health and wellbeing of the nearly two million Americans with type 1 diabetes. Predicting blood glucose problems in advance gives patients time to take steps to prevent the predicted problems from occurring. This improves blood glucose control, which is known to reduce the risk of serious diabetic complications, including blindness, amputations, kidney disease, strokes, heart attacks, and death from severe hypoglycemia. In addition, this research project forms the cornerstone of a new Smart Health and Wellbeing Laboratory at Ohio University. This new laboratory is designed to promote further interdisciplinary research among computer scientists and health care professionals as well as to attract more women to careers in computer science. Additional information about the project and the laboratory is available at http:\/\/oucsace.cs.ohiou.edu\/~marling\/shb.html.","title":"SHB: Small: Machine Learning Models for Blood Glucose Prediction in Diabetes Management","awardID":"1117489","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[486300,486301,486302],"PO":["560586"]},"180418":{"abstract":"This project will explore how remote socialization enabled by information and communication technologies (ICTs) is transforming four occupations: graphic design, automotive engineering, banking, and Internet entrepreneurship. Following a comparative, field-based research design, this research will examine the effects of both organizational environments and socialization tactics on ICT use and consider issues of technology use, socialization, and the changing nature of work. In today's workplaces, it is increasingly common to encounter arrangements in which occupational members are geographically distributed from one another. This new reality calls into question existing theories of socialization and learning practices that highlight the importance of collocated interaction and in situ knowledge transfer. By focusing on how individuals use ICTs to learn what it means to be an occupational member, this research will contribute to a new breed of theory on socialization that indicates the processes, practices, and strategies individuals can use to become effective members of an occupation even though they work remotely from others. By drawing on recent theorizing, which suggests that ICTs may provide particular affordances for interaction that non-mediated (e.g. face-to-face) contexts do not, it will explore the possibility that remote socialization may, in fact, help occupations to transform themselves. One aim is to build theory about the mechanisms by which technology leads to occupational transformation. <br\/><br\/>Occupational skill is critical to economic success, social progress, and individual well-being. However, many occupations seem to be failing to adapt quickly to changes in science, technology, and policy. The failure of occupations to change and refashion themselves to meet new social and technological pressures portends job loss from reduced skill for American citizens, and potentially increased outsourcing to other countries. This study will provide insight into how technology-enabled remote socialization may be able to contribute to faster occupational transformation. Although, for many years, ICT-mediated communication has been seen to be impoverished when compared to face-to-face communication, but now that it has developed considerably, ICT-mediated communication may provide more opportunities for workers to break free from the inertia of established occupations and develop new work practices and strategies that move the occupation forward. Additionally, it is imperative that new occupational members learn how to effectively acquire the knowledge and skills they need to perform their jobs well when they work remotely from others. This study will provide insight into effective practices of remote socialization and occupational learning such that individuals who are attempting to learn new work practices and knowledge will be successful in their efforts.","title":"HCC: Large: Collaborative Research: Information Technology, Remote Socialization, and the Development of Occupational Identity","awardID":"1111246","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["560755"],"PO":["564456"]},"184906":{"abstract":"The purpose of this project is to promote student participation in the long-running ER conference series, to be held in Belgium in Oct-Nov 2011. This will be the thirtieth ER, which is promised to continue the tradition of promoting collaboration among multiple areas. ER is a leading international forum for conceptual modeling, which is an important issue for information management and integration. ER provides a unique forum to promote multi- and interdisciplinary research and education in Conceptual Modeling. The proposed activities include recruiting a diverse group of US students to participate in ER, and will use the grant to help cover the cost of travel, subsistence, and registration to the ER 2011 conference for the students. Students participation of international scientific discussion forums is always an important part of their education and a great way to build a mature work force in scientific research. The project will also emphasize on recruiting under-represented groups of students in this effort.","title":"ER 2011: Fostering Interdisciplinary Research and Education in Software Engineering","awardID":"1137233","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["507731","557315"],"PO":["543481"]},"195917":{"abstract":"This project will support experimentation with the eXtensible Session Protocol (XSP) and the Phoebus network accelerator platform across various GENI substrates and platforms. XSP and Phoebus are innovative network initiatives that enable improved network performance and functionality. Phoebus will provide dramatically improved performance across networks with diverse technologies and characteristics, including emerging ultra-high speed optical networks as well as ubiquitous wireless networks. XSP provides the scaffolding to address the various well-known limitations in the Internet architecture including support for enhanced security, separation of location and identity in the routing infrastructure, and dynamic network service qualities driven by the user.<br\/><br\/>These experiments will deploy a novel session layer framework on a wireless testbed, as well as in a high-performance environment, like the Supercharged PlanetLab Platform. Security mechanisms and the performance effects of the Phoebus performance inlay will be evaluated. A novel session rendez-vous technique will be developed. Graduate students will gain critical research experience working on this project.","title":"EAGER: GENI Experiments in Optimizing Network Environments using XSP","awardID":"1235824","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["561820"],"PO":["564993"]},"181518":{"abstract":"This work builds on recent instrumentation approaches that provide low-overhead end-to-end tracing (e.g., Stardust and Dapper) that can capture the flow (i.e., path and timing) of individual requests within and across the components of a distributed system. <br\/><br\/>In addition to tools for profiling and examining system behavior, the PIs are creating tools that compare request flows between two executions to guide understanding of changes in performance, as contrasted with determining why a system has always been slow. Comparing request flows can help diagnose a large class of performance problems, such as degradations resulting from software changes\/upgrades or from usage over time (e.g., due to resource leakage or workload changes). <br\/><br\/>This research is evaluating the efficacy of this approach, addressing scalability challenges in instrumentation data collection and analysis, and exploring the effects of virtualization on variability.","title":"CSR: Small: Distributed System Diagnosis via Request Flow Comparison","awardID":"1117567","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[486330,486331],"PO":["565255"]},"181529":{"abstract":"As multi-core systems become the major computing platform, efficient cache management is even more crucial to system performance and power efficiency than before. An effective approach is to use software cache management (SCM) with hardware supports to manage shared last-level cache, because sophisticated SCM may adapt to the complex scenarios of cache usage on multi-core processors.<br\/><br\/>A critical and unsolved issue in SCM is the lack of rich and relevant information for software to reason about cache performance under different configurations. The project investigates the use of lightweight and Reconfigurable hardware Cache Emulators (RCEs) to extend the capability of SCM. With this new hardware support, sophisticated SCM algorithms that constantly monitor cache usage through RCEs are developed. Those SCM algorithms aim to improve cache power efficiency by turning off unused cache portion, optimize cache partitioning for multi-core processors, and improve software-controlled cache mapping to improve cache utilization.<br\/><br\/>The research will improve system performance, power efficiency, and performance predictability for laptop, desktop and server computers using multi-core processors of large shared caches. It may make an impact on industry processor design to include lightweight RCEs as well as enrich SCM algorithms. It will also introduce new educational materials for students to study multicore cache management through hand-on experiments.","title":"CSR: Small: Software Cache Memory Managements with Reconfigurable Hardware Emulators","awardID":"1117604","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[486357],"PO":["565255"]},"181519":{"abstract":"In engineering practice, models are<br\/>an essential part of understanding how to build complex systems. In<br\/>this project, high-level models and efficient implementations of<br\/>computer systems will be developed side-by-side under a single<br\/>framework that bridges the gap between them using a high degree of<br\/>automation. This is possible due to the use of a modern functional<br\/>language for both the model and implementation, and the deployment of<br\/>a new and powerful general-purpose and semi-automatic refinement technology.<br\/><br\/>The functional language Haskell has already enjoyed considerable<br\/>success as a platform for high-level modeling of complex systems with<br\/>its mathematical-style syntax, state-of-the-art type system, and<br\/>powerful abstraction mechanisms.<br\/>In this project, Haskell will be used to express a semi-formal<br\/>model and an efficient implementation, taking the form of two distinct<br\/>expressions of computation with the same mathematical foundation.<br\/>The project develops tools and methodologies that use transformations like<br\/>the worker\/wrapper transformation to construct links between these models<br\/>and implementations, lowering the cost of the development of<br\/>high-assurance software and hardware components in application<br\/>areas like security kernels and critical control systems.<br\/>Lowering the cost of linking semi-formal specifications and models to<br\/>real implementations will have considerable<br\/>impact. For example, Evaluation Assurance Level (EAL) 5 and 6 of the<br\/>Common Criteria call for semi-formal methods to construct such links,<br\/>and this project addresses keys part of this requirement.","title":"SHF: Small: Improving the Applicability of Haskell-Hosted Semi-Formal Models to High Assurance Development","awardID":"1117569","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["562945"],"PO":["564588"]},"181409":{"abstract":"Ad-hoc networks hold great promise, but there is a vast array of competing proposals for organizing such networks. Unfortunately it is unclear, in general, how to choose among the various proposed designs in any given deployment. In response, this project is asking a fundamental question: how should a collection of nodes decide what basic architecture to adopt in organizing into a network? Candidate basic architectures include those that organize via traditional routing protocols, employ epidemic protocols, or use delay-tolerant networking protocols. This project approaches the question as one of deciding what state should be maintained, and how long that state should be maintained, in the network. Relevant state includes both control state (properties of the network) and data state (the data in transit). The project is connecting dynamic properties of the network with the nature of control and data state to be maintained. A key project output is an identification of the network and traffic characteristics that can be used to determine an appropriate level of control and data state maintenance. The effect of these characteristics on the different classes of communication strategies is being rigorously evaluated. Using the characteristics that are identified, the project is developing signaling mechanisms to ensure that communication strategies optimally adapt to changing network conditions. <br\/><br\/>The result of the project will be more robust and broadly effective methods for creating ad hoc networks in new environments. Such environments can include battlefields as well as post-disaster deployments, where networks must be created quickly under unanticipated conditions.","title":"NeTS: Small: Understanding Communication Strategies for Ad hoc Networks","awardID":"1117039","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[486062],"PO":["557315"]},"181608":{"abstract":"This research explores the development of a wireless in-home location monitoring system for assessing and assisting independent living of the elderly, whose population demographics in the U.S. alone will exceed 80 million by 2040. Key areas where in-home location-tracking technologies exhibit particularly significant potential for helping assess and maintain independence include medication management, motor changes (e.g. walking speed changes), and fall detection. Research has revealed that high-resolution localization information is key to precisely characterizing these activities. Existing localization systems are inadequate for this purpose due to their lack of multi-person tracking capability, poor reliability, limited resolution in space and time, and large form factors that make them uncomfortable for seniors to wear.<br\/><br\/>The investigators focus on developing a robust, high-precision, 3-dimensional wireless localization system to simultaneously track multiple patients' position\/movement and activities. Specifically, they develop ultra-low power, tiny transmitters that are unobtrusive for seniors to wear, and address several unsolved technical challenges such as wireless synchronization of distributed receivers and localization error due to multipath overlap. Inference algorithms utilizing the location data acquired by the system to effectively characterize medication taking are created. To validate the accuracy of the inference models within independent living settings, the system is deployed in both a controlled smart home environment and in the homes of the ORCATECH Living Lab community-dwelling seniors. While the focus is on medication management due to project scope\/feasibility, the core technology created will enable characterization of various behaviors, including motor changes that are known to be predictive of patients' neurodegenerative diseases, such as Alzheimer's and Parkinson's diseases.","title":"SHB: Small: Enabling Technologies for Assessing and Assisting Independent Living","awardID":"1118017","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8018","name":"Smart Health & Wellbeing"}}],"PIcoPI":["560082","502807",486571],"PO":["564898"]},"181609":{"abstract":"Face-to-face communication is a highly dynamic process where participants mutually exchange and interpret linguistic and gestural signals. Even when only one person speaks at the time, other participants exchange information continuously amongst themselves and with the speaker through gesture, gaze, posture and facial expressions. To correctly interpret the high-level communicative signal, an observer needs to jointly integrate all spoken words, subtle prosodic changes and simultaneous gestures from all participants.<br\/><br\/>The proposed effort endeavors to create a new generation of computational models for modeling the interdependence between linguistic symbols and nonverbal signals during social interactions. This computational framework has wide applicability, including the recognition of human social behaviors, the synthesis of natural animations for robots and virtual humans, improved multimedia content analysis, and the diagnosis of social and behavioral disorders (e.g., autism spectrum disorder). This research effort is an important milestone, complementary to recent research efforts focusing on only two components (e.g., social signal processing, which focuses on nonverbal and social signals). The proposed unified approach to Social-Symbols-Signals will pave the way for new robust and efficient computational perception algorithms able to recognize high-level communicative behaviors (e.g., intent and sentiments) and will enable new computational tools for researchers in behavioral sciences.<br\/><br\/>The proposed research will advance this endeavor through the development of new probabilistic models for jointly capturing the interdependence between language, gestures and social signals, and novel computational representations, which integrates data-driven processing and logic rule-based approach (so that prior knowledge from social sciences can be easily included). Four fundamental research goals will be directly addressed: symbol-signal representation (joint representation of language and nonverbal), modeling social interdependence (joint modeling of communicative signals between multiple participants), variability in signal interpretations (variability with annotations of high-level communicative signals), and generalization and validation (generalization over different communicative signals and domains).<br\/><br\/>The proposed research will enable more natural interaction between users and embodied conversational dialogue systems, impacting the way in which computers are used, for example, in tutoring and in cultural and language training. The potential uses of such software and data go far beyond the scope of this project, making it possible, for example, to perform large scale corpus-based studies about social aspects of human face-to-face (multimodal) communication, or cognitive aspects of human multimodal processing. Following the investigators' past experience with sharing research software open-source, code and corpus annotations will be made available to the research community. These shared research results will be valuable for new researchers as well as important educational material for course development.","title":"HCC: Small: Modeling Human Communication Dynamics","awardID":"1118018","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["518675",486574],"PO":["565227"]},"177160":{"abstract":"Many applications in science and engineering encounter the problem of<br\/>identifying and processing topologically interesting features in the<br\/>digital representation of a geometry or data. Such features often need<br\/>to be optimal with respect to some metric (measurement). It is<br\/>recognized that homology groups from algebraic topology play an<br\/>essential role in these computations. Although the study of structural<br\/>properties of the homology groups has a rich history in mathematics,<br\/>their computations in combination with geometry are not that well<br\/>studied. The principal investigators (PIs) propose to study these<br\/>fundamental questions thoroughly, along with their connections to<br\/>practical problems from science and engineering.<br\/>Intellectual merit: Efficient solutions of the optimality questions in<br\/>homology computations require both mathematical and algorithmic<br\/>developments. The PIs bring aboard these required expertise. Apart<br\/>from the synergistic effect of the proposed study on mathematics and<br\/>theoretical computer science, the close ties with various applications<br\/>in science and engineering will play a synergistic role between<br\/>computational fields such as computer graphics, computer vision,<br\/>sensor networks, computer aided design, and scientific fields such as<br\/>biology, physics, chemistry, and others.<br\/>Broader impacts: Optimization of aspects of homology groups provides<br\/>important insights in many scientific and engineering applications<br\/>ranging from tunnels in protein molecules to voids in large<br\/>machines. Solutions of such problems can aid in the manufacturing of<br\/>better machines, designing of new drugs, and rapid modeling of<br\/>customized objects. The educational impact of this project is in a<br\/>large synergy between mathematics and computer science motivated by<br\/>real applications. Course notes, internet distributions, and software<br\/>systems developed through the project will enable the scientific<br\/>community to study challenging problems in geometry, topology, and<br\/>algorithms. Graduate students supported by the project will develop<br\/>skills in mathematics and theoretical computer science and also in<br\/>writing robust, efficient, and user-friendly software.","title":"AF: Medium: Collaborative Research: Optimality in Homology - Algorithms and Applications","awardID":"1064416","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["549998"],"PO":["565157"]},"177182":{"abstract":"With tremendous amounts of data existing in scientific applications, database management becomes a critical issue, but database technology is not keeping pace. This problem is especially acute in the long tail of science: the large number of relatively small labs and individual researchers who collectively produce the majority of scientific results. These researchers lack the IT staff and specialized skills to deploy technology at scale, but have begun to routinely access hundreds of files and potentially terabytes of data to answer a scientific question. This project develops the architecture for a database-as-a-service platform for science. It explores techniques to automate the remaining barriers to use: ingesting data from native sources and automatically bootstrapping an initial set of queries and visualizations, in part by aggressively mining a shared corpus of data, queries, and user activity. It investigates methods to extract global knowledge and patterns while offering scientists access control over their data, and some formal privacy guarantees. The Intellectual Merit of this proposal consists of automating non-trivial cognitive tasks associated with data work: information extraction from unstructured data sources, data cleaning, logical schema design, privacy control, visualization, and application-building. As Broader Impacts, the project helps scientists reduce the proportion of time spent \"handling data\" rather than \"doing science.\" All software resulting from this project are open source, and all findings are disseminated broadly through publications and workshops. Sustainable support for science users of the software is coordinated through the University of Washington eScience Institute. The research is incorporated in both undergraduate and graduate computer science courses, and the software is also incorporated into domain science courses as well. The project's outreach activities include advising students through special programs geared toward under-represented groups such as the CRA-W DREU. More information about this project is found at http:\/\/escience.washington.edu\/dbaas.","title":"III: Medium: Collaborative Research: Database-As-A-Service for Long Tail Science","awardID":"1064505","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["531543","531545"],"PO":["565136"]},"177370":{"abstract":"Statistical analysis is key to many challenging applications such as text classification, speech recognition, and DNA analysis. However, often the amount of data available is comparable or even smaller than the set of symbols (alphabet) constituting the data. Unfortunately, not much is known about optimal inference in this so-called large-alphabet domain. Recently, several promising approaches have been developed by different scientific communities, including Bayesian nonparametrics in statistics and machine learning, universal compression in information theory, and the theory of graph limits in mathematics and computer science.<br\/><br\/>The investigators study the problem drawing from these multiple perspectives, but with a particular focus on developing the information theoretic approach. The research studies analytical properties of the \"pattern maximum likelihood'' estimator, which performs well in practice but is not understood theoretically, and also explores computational speedups. Moreover, it attempts to delineate which problem classes are better handled by Bayesian nonparametric techniques and which by the pattern approach, and explores links between these approaches. The investigators use the resulting theory for automatic document classification, allowing for more automation in storing, retrieving, and analyzing data. Furthermore, the investigators use the theory to study genetic variations, whose link with disease diagnosis is a crucial step in the systematic quantification of biology that is playing an increasingly important role in medical advancement. The research also brings new courses to the classroom, with a special outreach effort to involve women and under-represented minorities, including through the Native Hawaiian Science and Engineering Mentorship Program.","title":"CIF: Medium: Collaborative Research: Information Theory and Statistical Inference from Large-Alphabet Data","awardID":"1065632","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":[475523],"PO":["564924"]},"186082":{"abstract":"The Software Design and Productivity (SDP) Cross Agency and National Needs Summit will be held at the NASA Ames Conference Center, September 20-23, 2011. The summit is organized and sponsored by SDP Coordinating Group within the National Coordination Office (NCO) for Networking and Information Technology Research and Development (NITRD). The purpose of this summit is to inform a crosscutting SDP research and technology agenda. It will provide a forum for bringing together policy makers, stakeholders, visionaries, and R&D leaders for software design and productivity research. The summit will focus on the key issues of the SDP mission: to advance software engineering concepts, methods, techniques and tools that result in more usable, dependable, cost-effective, and sustainable software-intensive systems. This grant will reimburse academic participants for travel and accommodations.","title":"Travel Support for Software Design and Productivity Summit","awardID":"1143825","effectiveDate":"2011-08-01","expirationDate":"2014-04-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["531356"],"PO":["564388"]},"186093":{"abstract":"Natural Computing is computing inspired by nature. Examples are evolutionary algorithms, neural networks, molecular computing, and quantum computing. Membrane computing is a branch of natural computing that was recently initiated by G. Paun in his seminal paper, \"Computing with Membranes\". Membrane computing identifies an unconventional computing model, namely a P system, from natural phenomena of cell evolutions and chemical reactions. A membrane system is a computing model, which abstracts from the way living cells process chemical compounds in their compartmental structure. Regions defined by a membrane structure (or cells related by communication channels) contain multi-sets of objects that evolve according to given rules. The objects can be described by symbols or by strings of symbols. By using the rules in a nondeterministic (or probabilistic) maximally parallel manner, transitions between the system configurations can be obtained. A sequence of transitions is a computation of how the system is evolving. Various ways of controlling the transfer of objects from a region to another while applying the rules (as well as possibilities to dissolve, divide or create membranes) are considered in this area. Due to the built-in nature of maximal parallelism inherent in the model, P systems have a great potential for implementing massively concurrent systems in an efficient way that would allow us to solve currently intractable problems, in much the same way as the promise of quantum and molecular computing.<br\/><br\/>More recently, spiking neural P systems (SN P systems) were introduced with the aim of incorporating into membrane computing specific ideas from spiking neurons. In short, an SN P system consists of a set of neurons placed in the nodes of a directed graph and sending signals (spikes) along the arcs of the graph which are called synapses. The objects evolve by means of spiking rules placed in the nodes and enabled when the number of spikes present in the nodes fulfill specified patterns. When a spiking rule is executed in a neuron, spikes are produced and sent in parallel to all neurons connected by an outgoing synapse from the neuron where the rule was applied. SN P systems are a good model of neural computing. The focus in the proposed project is on fundamental topics such as computational complexity and characterizations of several variants of SN P systems, universality\/non-universality of the different models, determinism versus non-determinism, various modes of parallelization, synchronization, synchronous versus asynchronous computations, relationships to well-known models of parallel computation, and verification problems.","title":"EAGER: Computing With Cells","awardID":"1143892","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[499391],"PO":["565223"]},"176150":{"abstract":"The goal of this project is to create a linguistically annotated, publicly available, and easily searchable corpus of video from American Sign Language (ASL). This will constitute an important piece of infrastructure, enabling new kinds of research in both linguistics and vision-based recognition of ASL. In addition, a key goal is to make this corpus easily accessible to the broader ASL community, including users and learners of ASL. As a result of our long-term efforts, we have an extensive collection of linguistically annotated video data from native signers of ASL. However, the potential value of these corpora has been largely untapped, notwithstanding their extensive and productive use by our team and others. Existing limitations in our hardware and software infrastructure make it cumbersome to search and identify data of interest, and to share data among our institutions and with other researchers. In this project, we propose hardware and software innovations that will constitute a major qualitative upgrade in the organization, searchability, and public availability of the existing (and expanding) corpus.<br\/><br\/>The enhancement and improved Web-accessibility of these corpora will be invaluable for linguistic research, enabling new kinds of discoveries and the testing of hypotheses that would otherwise have be difficult to investigate. On the computer vision side, the proposed new annotations will provide an extensive public dataset for training and benchmarking a variety of computer vision algorithms. This will facilitate research and expedite progress in gesture recognition, hand pose estimation, human tracking, and large vocabulary, and continuous ASL recognition. Furthermore, this dataset will be useful as training and benchmarking data for algorithms in the broader areas of computer vision, machine learning, and similarity-based indexing. <br\/><br\/>The advances in linguistic knowledge about ASL and in computer-based ASL recognition that will be accelerated by the availability of resources of the kind proposed here will contribute to development of technologies for education and universal access. For example, tools for searching collections of ASL video for occurrences of specific signs, or converting ASL signing to English, are still far from attaining the level of functionality and usability to which users are accustomed for spoken\/written languages. Our corpora will enable research that aims to bring such vision-based ASL recognition applications closer to reality. Moreover, these resources will afford important opportunities to individuals who would not otherwise be in a position to conduct such research (e.g., for lack of access to native ASL signers or high-quality synchronized video equipment, or lack of resources\/expertise to carry out extensive linguistic annotations). Making our corpora available online will also allow the broader community of ASL users to access our data directly. Students of ASL will be able to retrieve video showing examples of a specific sign used in actual sentences, or examples of a grammatical construction. ASL instructors and teachers of the Deaf will also have easy access to video examples of lexical items and grammatical constructions as used by a variety of native signers for use in language instruction and evaluation. Thus, the proposed web interface to our data collection will be a useful educational resource for users, teachers, and learners of ASL.","title":"Collaborative Research: CI-ADDO-EN: Development of Publicly Available, Easily Searchable, Linguistically Analyzed, Video Corpora for Sign Language and Gesture Research","awardID":"1059218","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["475203",472054],"PO":["565227"]},"186193":{"abstract":"This project explores new directions to solving the following problem. Given an image, determine whether and where specific objects, or objects from a specific category, appear in the image. Visual category is defined as earlier, namely, as a collection of objects which share characteristic features that are visually similar, and occur in similar configurations. The visual nature of objects sought is communicated through (training) data containing them, and estimated using machine learning. The approach consists of two main parts. First, it learns whether a given set of previously unseen images (including videos), say supplied by a user, contains any dominant themes, namely, subimages, that occur frequently and look similar. Second, given a set of categories automatically inferred during training and a new test image, the approach recognizes all occurrences in the image of the learned categories. It delineates each such object in the image, and labels it with its category name. Both learning and subsequent recognition do not require human supervision. The approach learns and recognizes categories as image hierarchies. The impact of the project includes accurate high-speed extraction of image regions, image representation by connected segmentation tree, robust image matching, unsupervised extraction of hierarchical category models, efficient recognition of a large number of categories, unsupervised estimation of perceptually salient, relevance weights of subcategory detections to category recognition, and generalization of the proposed approach to extraction of texture elements. More broadly, the proposed approach is useful for applications in search engines, surveillance, video analytics, monitoring and data mining.","title":"EAGER: Automated High Speed Object Category Modeling and Model Based Recognition, Segmentation, Clustering, and Classification","awardID":"1144227","effectiveDate":"2011-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"K565","name":"National Security Agency"}}],"PIcoPI":[499628],"PO":["564316"]},"188030":{"abstract":"It is well known that many students find science text challenging to comprehend. Students' reading ability is partly to blame. Reading problems become most apparent when the student is faced with a challenging text for which they have knowledge deficits. Science text, in particular, has many technical terms that are difficult to ground in everyday experience. Thus, there is a need for reading comprehension interventions to improve science comprehension. This project compares the effectiveness of two types of tutoring environments in improving high-school students' ability to understand challenging science text. Both environments contain the same pedagogical content, but present it differently: as a lesson or as a game. The first environment, developed and tested over the past 5 years, is an automated reading strategies tutor called iSTART (Interactive Strategy Training for Active Reading and Thinking) that uses animated pedagogical agents to deliver interactive instruction on self-explanation and reading strategies (comprehension monitoring, paraphrasing, generating inferences). Instruction occurs in three stages with each stage requiring increased interaction on the part of the learner. Results across a wide range of studies indicate that iSTART is highly effective in improving students' ability to understand challenging science text. While effective, iSTART can be somewhat unappealing to an average high-school student in extended practice situations. While students need extended practice to master the strategies, iSTART becomes monotonous over time. To increase students' engagement, an alternative version of iSTART practice will be developed that allows students to practice iSTART strategies in a game environment. iSTART-The Game (iTG) will present the same reading strategy practice to students, but will incorporate game-based principles to enhance engagement. This project examines whether a gaming environment for learning strategies for science text comprehension more effectively sustains students' attention and engagement during training, and thereby results in improved acquisition and mastery of these strategies. In the first year of funding, an automated reading strategy tutoring system that is framed in a gaming environment will be developed. In Year 2, pilot studies will be conducted to refine the system. In Year 3, practice using the strategies will be compared in three conditions: iTG, iSTART, and a control condition. Students will practice the strategies over a period of five additional sessions after the initial training. This experiment will provide information on the potential value of iTG over time and specifically, whether iTG engages students over repeated practice sessions. The potential interactive effects of individual differences such as prior science knowledge, reading skill, and motivational levels will also be examined.<br\/><br\/>This research will contribute to better understanding of engagement as a factor in learning gains. It is predicted by the investigators that many students who do not find the standard tutoring environment sufficiently engaging will significantly benefit from the alternative approach. Most importantly, this instructional intervention should be especially valuable for those students most at risk due to having lower ability and interest in science. This research will accomplish the goal of creating a test-bed learning environment intended to improve reader engagement and advance our understanding of the potential instructional gains from such environments. The project will contribute to our understanding of the relationship between game features and engagement. It will provide students with tools that help them more effectively meet the challenges of learning from difficult science texts. In addition, this research will contribute to our understanding of the roles that cognition and emotion play in fostering learning, the specific processes involved with learning from science texts, and the complex interplay of factors such as reading strategies, knowledge, reading skill, interest, and motivation.","title":"Learning Reading Strategies for Science Texts in a Gaming Environment: iSTART vs iTG","awardID":"1153822","effectiveDate":"2011-08-16","expirationDate":"2012-04-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7484","name":"IIS SPECIAL PROJECTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":["550495"],"PO":["565227"]},"186094":{"abstract":"A group of three principal investigators will undertake a holistic study of prospects of nano-photonic interconnects in the context of integration with silicon technologies for improved computational speed, latency and lower power dissipation. An unique aspect of the project is that the PIs come from different backgrounds, namely photonic devices and computer architecture, and an integrated design methodology that will exploit feedback from devices to architectures will be central to this high-risk project with potential for large payoff. Specifically, this project will examine thermal sensitivities of nano-photonic devices and study its influence on computing architectures making use of optical interconnects. Conversely, the requirements imposed by architectural considerations on the device technology will be studied as well.<br\/><br\/>The potential broader impact of the project is largely technological. If successful, t has the potential to revolutionize the interconnect technology, which is one of the major bottlenecks in further progress of large scale computer hardware design today. Nevertheless, the PIs also plan to disseminate the results of their findings through workshops and class room efforts and to include women and underrepresented groups in their activities, which should enhance the broader impact of the project.","title":"EAGER: Overcoming Thermal Sensitivity of CMOS-Compatible Nanophotonic Devices in Future Microprocessor Designs","awardID":"1143893","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["509533","556706",499395],"PO":["366560"]},"177195":{"abstract":"Many applications in science and engineering encounter the problem of<br\/>identifying and processing topologically interesting features in the<br\/>digital representation of a geometry or data. Such features often need<br\/>to be optimal with respect to some metric (measurement). It is<br\/>recognized that homology groups from algebraic topology play an<br\/>essential role in these computations. Although the study of structural<br\/>properties of the homology groups has a rich history in mathematics,<br\/>their computations in combination with geometry are not that well<br\/>studied. The principal investigators (PIs) propose to study these<br\/>fundamental questions thoroughly, along with their connections to<br\/>practical problems from science and engineering.<br\/><br\/>Intellectual merit: Efficient solutions of the optimality questions in<br\/>homology computations require both mathematical and algorithmic<br\/>developments. The PIs bring aboard these required expertise. Apart<br\/>from the synergistic effect of the proposed study on mathematics and<br\/>theoretical computer science, the close ties with various applications<br\/>in science and engineering will play a synergistic role between<br\/>computational fields such as computer graphics, computer vision,<br\/>sensor networks, computer aided design, and scientific fields such as<br\/>biology, physics, chemistry, and others.<br\/><br\/>Broader impacts: Optimization of aspects of homology groups provides<br\/>important insights in many scientific and engineering applications<br\/>ranging from tunnels in protein molecules to voids in large<br\/>machines. Solutions of such problems can aid in the manufacturing of<br\/>better machines, designing of new drugs, and rapid modeling of<br\/>customized objects. The educational impact of this project is in a<br\/>large synergy between mathematics and computer science motivated by<br\/>real applications. Course notes, internet distributions, and software<br\/>systems developed through the project will enable the scientific<br\/>community to study challenging problems in geometry, topology, and<br\/>algorithms. Graduate students supported by the project will develop<br\/>skills in mathematics and theoretical computer science and also in<br\/>writing robust, efficient, and user-friendly software.","title":"AF: Medium: Collaborative Research: Optimality in Homology - Algorithms and Applications","awardID":"1064600","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[475047],"PO":["565157"]},"181090":{"abstract":"Huge efforts and funds are today deployed, in both academia and industry, with the purpose to develop new tools, techniques, and methods to support the design of innovative and creative interactive digital products and services. However, while a large proportion of these attempts lead to research insights, they are not always successfully transformed into design methods that fit the reality that practitioners experience. This lack of understanding of practice has in many cases led to a substantial lack of trust from practitioners towards the value of research contributions, while at the same time leading to frustrated researchers not understanding the lack of enthusiasm from practitioners when it comes to adapting new design methods emanating from research efforts. The proposed research will develop a solid understanding of this unfortunate situation by carefully investigating existing practice from the perspective of practitioners. <br\/><br\/>The research proposal is made up of five major studies and activities, conducted in parallel over three years. These activities are a combination of analytical studies of design methods and interview studies with practitioners (designers who employ design methods to create products and systems) and researchers (design method developers). The research will lead to insights and principles suitable for practitioners on how to strategize and handle their choice and use of design tools, techniques, and methods, and to insights and principles suitable for organizations for their strategic choices of design methods. Most importantly, the research will lead to educational guidelines suitable for design education on how to professionally think about and handle design methods. With increased knowledge about what constitutes appropriate design methods, these results will increase the probability for more creative and innovative designs of interactive products and systems.","title":"HCC: Small: Design Methods: How They are Understood, Selected, and Used by Practitioners","awardID":"1115532","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["513299"],"PO":["565227"]},"177350":{"abstract":"Brain-computer interfaces (BCIs) are hardware and software systems that allow users to interact with computer applications by changing their mental activity, which causes variations in weak electrical voltages produced by the brain. BCIs measure these voltages in one of two ways: invasive methods use electrodes implanted in the brain, while noninvasive methods use electrodes resting on the scalp that are part of a cap worn by the user. A long-term goal of BCI research is a new mode of communication for subjects with diseases and injuries resulting in the loss of voluntary muscle control, such as amyotrophic lateral sclerosis (ALS), multiple sclerosis, high-level spinal cord injuries or severe cerebral palsy. If all voluntary muscle control is lost, a locked-in syndrome results in which a person is unable to communicate with the outside world. BCIs can provide a new way for users to communicate with their caregivers and to control devices such as televisions, wheelchairs, speech synthesizers and computers. While BCI technology holds great promise, most BCI systems remain in research labs. The goal of this project is to remove barriers to practical, noninvasive, BCI technology that exist in current approaches, and to field test the resulting BCI systems in the homes of users who suffer from motor impairments. Limitations of current BCI systems that will be addressed include the difficulty of applying an electrode cap, signal artifacts due to other assistive technology in the user's environment, and long computer and user training times required to calibrate current EEG classification algorithms.<br\/><br\/>A key barrier to practical BCI systems is the lack of methods for reliable, fast classification of EEG signals. In this project, this limitation will be addressed by conducting experiments in three areas. One set of experiments will investigate the quality of EEG signals recorded in subjects' homes and the performance of BCI applications in real-time in the homes. The second set of experiments will involve new algorithms for EEG artifact removal and signal classification that are tailored for EEG recorded in subjects' homes and for real-time use. For the second set of experiments, new user interfaces will be studied and compared to currently available interfaces. For the third set of experiments, several different user interface designs for BCI applications will be developed and studied. The effectiveness of visual and auditory feedback provided to the user in real-time will be investigated.<br\/><br\/> This interdisciplinary project involves a team of investigators and students from diverse backgrounds. Faculty and students in computer science will design and implement algorithms and the BCI user interface. Faculty and students in occupational therapy will guide the field testing of BCI systems and will guide the evaluation of these experiments. Progress will be evaluated in a number of ways, including experiments comparing EEG signal representations and classifiers by accuracy, reliability, and training time, and field tests of BCI systems. Ultimately, the project's success will be measured by new or improved means of individuals interacting with computers in their homes for purposes of communication with others and control of assistive devices like wheelchairs.<br\/><br\/>Broader Impacts: This project will develop a new technology for sensing and analyzing electroencephalogram signals (EEG) from human subjects. The resulting technology will help advance brain imaging and its application. The long term goal of this research is a new brain-computer interface based on EEG signals with which persons can use a computer to communicate with others in their vicinity or remotely over the net, to surf the net, and to control environmental entertainment, and assistive devices. The new technology will be simple enough for any person with minimal training to use. The project will also play a strong role in the education of future researchers and health professionals in this interdisciplinary field by involving graduate and undergraduate students from multiple departments as research assistants, by teaching a new course in BCI for students from a variety of backgrounds, and by providing fieldwork experiences.","title":"HCC: Medium: Removing Barriers to the Practical Use of Non-Invasive Brain-Computer Interfaces","awardID":"1065513","effectiveDate":"2011-08-01","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[475466,"549818",475468],"PO":["565227"]},"176151":{"abstract":"The goal of this project is to create a linguistically annotated, publicly available, and easily searchable corpus of video from American Sign Language (ASL). This will constitute an important piece of infrastructure, enabling new kinds of research in both linguistics and vision-based recognition of ASL. In addition, a key goal is to make this corpus easily accessible to the broader ASL community, including users and learners of ASL. As a result of our long-term efforts, we have an extensive collection of linguistically annotated video data from native signers of ASL. However, the potential value of these corpora has been largely untapped, notwithstanding their extensive and productive use by our team and others. Existing limitations in our hardware and software infrastructure make it cumbersome to search and identify data of interest, and to share data among our institutions and with other researchers. In this project, we propose hardware and software innovations that will constitute a major qualitative upgrade in the organization, searchability, and public availability of the existing (and expanding) corpus.<br\/><br\/>The enhancement and improved Web-accessibility of these corpora will be invaluable for linguistic research, enabling new kinds of discoveries and the testing of hypotheses that would otherwise have be difficult to investigate. On the computer vision side, the proposed new annotations will provide an extensive public dataset for training and benchmarking a variety of computer vision algorithms. This will facilitate research and expedite progress in gesture recognition, hand pose estimation, human tracking, and large vocabulary, and continuous ASL recognition. Furthermore, this dataset will be useful as training and benchmarking data for algorithms in the broader areas of computer vision, machine learning, and similarity-based indexing. <br\/><br\/>The advances in linguistic knowledge about ASL and in computer-based ASL recognition that will be accelerated by the availability of resources of the kind proposed here will contribute to development of technologies for education and universal access. For example, tools for searching collections of ASL video for occurrences of specific signs, or converting ASL signing to English, are still far from attaining the level of functionality and usability to which users are accustomed for spoken\/written languages. Our corpora will enable research that aims to bring such vision-based ASL recognition applications closer to reality. Moreover, these resources will afford important opportunities to individuals who would not otherwise be in a position to conduct such research (e.g., for lack of access to native ASL signers or high-quality synchronized video equipment, or lack of resources\/expertise to carry out extensive linguistic annotations). Making our corpora available online will also allow the broader community of ASL users to access our data directly. Students of ASL will be able to retrieve video showing examples of a specific sign used in actual sentences, or examples of a grammatical construction. ASL instructors and teachers of the Deaf will also have easy access to video examples of lexical items and grammatical constructions as used by a variety of native signers for use in language instruction and evaluation. Thus, the proposed web interface to our data collection will be a useful educational resource for users, teachers, and learners of ASL.","title":"Collaborative Research: CI-ADDO-EN: Development of Publicly Available, Easily Searchable, Linguistically Analyzed, Video Corpora for Sign Language and Gesture Research","awardID":"1059221","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[472056,472057],"PO":["565227"]},"177284":{"abstract":"Intellectual Merit:<br\/>The focus of the proposal is on finding semantically equivalent files in an efficient and scalable manner. If two files are identical, they are candidates for optimizations to reduce storage costs, increase performance, and generally improve the system. Traditionally, two files are only considered equivalent if they are byte-by-byte identical - i.e., byte equivalence. However, this team's preliminary research shows that there are many other files that are essentially equivalent, even though the bytes they contain are not the same. This proposal will investigate how to find such cases and perform optimizations that make use of semantic equivalence, rather than byte equivalence.<br\/><br\/>This project will design and implement a framework, Facets, which explores new capabilities by applying optimizations to files that are essentially transformed versions of each other. Many optimizations and improvements can be applied to semantically equivalent files, including:<br\/><br\/> -Ensuring that the security of semantically equivalent files is preserved<br\/> -Easing backup and maintenance of semantically equivalent files in various formats, fidelities, and versions<br\/> -Using semantically equivalent files to improve performance, reliability, and availability<br\/> -Regenerating semantically equivalent files to speed up recovery and network transfer<br\/> -Selecting which semantically equivalent files to access according to performance or energy constraints<br\/><br\/>This team's preliminary research shows that 5% of files on a typical user's machine are original content. The rest are copies of files from elsewhere or various derivatives of original content. While leveraging this observation to achieve advantages is not trivial, many significant improvements are possible if one can find these relationships and make proper use of them. These improvements include enhanced security, more efficient backup and restoration, better file caching, more intelligent tradeoffs in performance versus power use, and a host of other possibilities. <br\/><br\/>Broader Impacts: <br\/>The code and techniques developed will be released in open source form. The team will take steps (such as applying for supplemental REU grants) to involve undergraduates in the research. They will give talks and recruit at Hispanic-serving institutions. Materials and concepts from the research will be incorporated into classes taught by the principal investigators at their institutions.","title":"CSR: Medium: Collaborative Research: Facets: Exploring Semantic Equivalence of Files to Improve Storage Systems","awardID":"1065127","effectiveDate":"2011-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["485932"],"PO":["551712"]},"177196":{"abstract":"With tremendous amounts of data existing in scientific applications, database management becomes a critical issue, but database technology is not keeping pace. This problem is especially acute in the long tail of science: the large number of relatively small labs and individual researchers who collectively produce the majority of scientific results. These researchers lack the IT staff and specialized skills to deploy technology at scale, but have begun to routinely access hundreds of files and potentially terabytes of data to answer a scientific question. This project develops the architecture for a database-as-a-service platform for science. It explores techniques to automate the remaining barriers to use: ingesting data from native sources and automatically bootstrapping an initial set of queries and visualizations, in part by aggressively mining a shared corpus of data, queries, and user activity. It investigates methods to extract global knowledge and patterns while offering scientists access control over their data, and some formal privacy guarantees. The Intellectual Merit of this proposal consists of automating non-trivial cognitive tasks associated with data work: information extraction from unstructured data sources, data cleaning, logical schema design, privacy control, visualization, and application-building. As Broader Impacts, the project helps scientists reduce the proportion of time spent \"handling data\" rather than \"doing science.\" All software resulting from this project are open source, and all findings are disseminated broadly through publications and workshops. Sustainable support for science users of the software is coordinated through the University of Washington eScience Institute. The research is incorporated in both undergraduate and graduate computer science courses, and the software is also incorporated into domain science courses as well. The project's outreach activities include advising students through special programs geared toward under-represented groups such as the CRA-W DREU. More information about this project is found at http:\/\/escience.washington.edu\/dbaas.","title":"III: Medium: Collaborative Research: Database-As-A-Service for Long Tail Science","awardID":"1064606","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["558950"],"PO":["565136"]},"196150":{"abstract":"The rise of chip multiprocessors (CMP) featuring tens to hundreds of processing units on a single chip promises to significantly boost the performance of desktop systems, rivaling the performance of yesterday's supercomputers. Supercomputing applications typically exploit a high degree of parallelism present in the application's computational tasks, which allows multiple processing units to work on solving the problem simultaneously to obtain a solution fast. <br\/>However, common software applications are not written for specialized supercomputer architectures and lack sufficient explicit exposure of parallelism to gain speedups from CMPs automatically. Therefore, a successful exploitation of CMPs requires a rethinking of design, coding, and debugging by application developers. Programming languages and program annotations that natively support parallel concepts will be increasingly more successful, as well as programming languages in which sequential code can be more easily converted into parallel code. <br\/><br\/>This research investigates the combination and enhancements of several successful approaches to expose more parallelism in program code automatically. Firstly, the investigators will merge flow- sensitive loop-variant variable detection and optimization with the chains of recurrences (CR) algebra together with the NLVI (nonlinear variable interval) test that is based on interval theory. This aims to reduce the number of false positives prohibiting parallelization of loops with array dependences. Secondly, techniques for speculative parallelization of loops will be enhanced with a new run-time dependence analysis algorithm based on the CR algebra, NLVI test, and the theory of axiomatic semantics. Thirdly, a set of program annotations will be introduced to support speculative parallelization. This benefits source-to-source compilers and programmers who can leverage these annotations to extract more parallelism from loops by exercising application-specific knowledge.","title":"Collaborative Research: Flow-Sensitive Program Analysis for Speculative Parallelization","awardID":"1237502","effectiveDate":"2011-08-01","expirationDate":"2013-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":[525789],"PO":["565272"]},"186063":{"abstract":"This project performs comparative research in the United States and Japan on cultural models of social behavior and technology that influence how users perceive, make sense of, and interact with social robots. Social robots are designed to engage and communicate with people using socially appropriate behaviors, cues, norms, and roles. We (1) develop a comparative framework to study how users of social robots understand, apply, and react to cultural models of sociality and technology, as expressed in the material and discursive framing of robots, in the US and Japan, and (2) establish the foundation for long-term research collaboration on cross-cultural studies of robotics between Dr. Selma Sabanovic at Indiana University Bloomington (IUB), Dr. Takanori Shibata at the National Institute for Advanced Industrial Science and Technology (AIST) in Tsukuba, and Dr. Kazuyoshi Wada at Tokyo Metropolitan University (TMU) in Japan.<br\/><br\/>The project produces guidelines for best practices for using social robots in the US and Japan; this is particularly timely for Paro, which is commercially available in both countries. We also strengthen existing and build new collaborations with individuals and institutions in Japan. The project impacts education through the exchange of students and month-long internships and provides educational experiences for a broader audience, including other students, users, and relevant stakeholders through open houses and presentations by participating institutions.","title":"EAGER: Cultural models in social robotics - Comparative studies with users in the US and Japan","awardID":"1143712","effectiveDate":"2011-08-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7484","name":"IIS SPECIAL PROJECTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"7298","name":"COLLABORATIVE RESEARCH"}}],"PIcoPI":[499302],"PO":["564069"]},"181190":{"abstract":"A key aim in Natural Language Processing is to robustly map from natural language sentences to formal representations of their underlying meaning. Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations. The goal of this project is to develop models and learning algorithms for recovering lexical structure, in the context mapping sentences to logical form. This work is inspired by linguistic theories of the lexicon, but directly motivated by the limitations observed in current, state-of-the-art learning algorithms.<br\/><br\/>The central hypothesis is that a new probabilistic learning approach for lexical generalization can simultaneous achieve the goals of (1) language-independent learning, (2) robustness when analyzing natural, unedited text, and (3) requiring reduced data annotation effort, in a computationally efficient manner that will scale to large learning problems. The approach under development induces a Combinatory Categorial Grammar (CCG), that is modified to replace the traditional, explicit list of lexical items in the lexicon with a distribution over lexical items that allows for significant generalization in the construction of possible syntactic and semantic structures for given input words. Modifying the CCG lexicon in this manner greatly increases the potential to generalize from the available training data without sacrificing the scalability that comes from working within an established grammar formalism for which efficient learning and parsing algorithms have been developed. This work will have impact at the algorithmic level and through applications, including advanced natural language interfaces to databases for non-technical users.","title":"RI: Small: Scalable Algorithms for Learning to Recover Logical Form from Natural Language","awardID":"1115966","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["557091"],"PO":["565215"]},"181080":{"abstract":"In the past several decades, the access time of magnetic disks has improved about 8% per year while processor speeds have improved at an astounding 60% per year. The tremendous performance disparity between disks and processors means that many large-scale applications are limited by the performance of the underlying storage systems. Flash memory is an emerging storage technology that shows tremendous promise to compensate for the limitations of magnetic disk-based storage devices. A key advantage of flash-based storage is that its read-write performance is much better than disk. Flash memory-based storage like solid state drives provides fast random access to all areas of the device. On the other hand, writes to flash memory are much slower than reads. Furthermore, writes to flash must be preceded by an erase unless a clean block is used, and the number of erase cycles is limited. To overcome these drawbacks, a Flash Translation Layer (FTL) is designed to provide dynamic address mapping between logical addresses to physical addresses, wear-leveling, garbage collection, and buffer caching. A small memory buffer in FTL is used to perform these functions. <br\/><br\/>This project will focus on techniques to use the least amount of memory buffer to achieve high performance and low energy consumption. The research tasks include efficient heuristics for hot and cold data classification and how to reduce memory requirement for wear-leveling and garbage collection. This effort will improve the fundamental understanding of flash memory and will extend the capability of flash memory to support many critical applications.","title":"CSR:Small: Efficient FTL Buffer Management for High-Performance Solid State Drives","awardID":"1115471","effectiveDate":"2011-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["543507"],"PO":["551712"]},"183083":{"abstract":"The International Meshing Roundtable is a unique conference that aims to bring experts together to discuss mesh generation and related topics, which plays an important role in computational mechanics and related areas. This is a rare opportunity for researchers in the field of mesh generation to exchange ideas and also an excellent educational opportunity for students and postdoctoral fellows who are working on the meshing research and related applications. Potential topics for discussion in the conference include: CAD and computational geometry issues for meshing, surface and volume mesh generation, analysis and\/or demonstration, software design, and implications of application on finite element\/volume analysis, geometric modeling, computer graphics, scientific visualization, and other novel applications. The event will be held on October 23-26, 2011, in Paris, France. This is also the first time that this conference will be held outside the US in the past two decades. The conference includes four short courses, presentation of conference papers, presentation of research notes, a poster session, and a professional development session targeting students and postdocs.<br\/><br\/>The broader impact of the event consists of knowledge dissemination of a field of research essential to modern technologies. The symposium will include graduate students, post-docs and young researchers. The event will bring together international researchers and developers from academia, national labs, and industry in a stimulating, open environment to share technical information on mesh generation and related topics.","title":"Participant Support for the 20th International Meshing Roundtable; Paris, France; October 23-26, 2011","awardID":"1126378","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"1630","name":"MECHANICS OF MATERIALS"}}],"PIcoPI":["502143"],"PO":["503922"]},"185151":{"abstract":"Wearable computers are gaining significant attention due to their capability to enable a wide variety of new applications in domains such as wellness and health care. Despite their tremendous potential to impact our lives, wearable health monitoring systems face a number of hurdles before becoming a reality. The enabling processors and architectures demand a large amount of energy, requiring sizable batteries. This creates challenges for further miniaturization of the wearable units. This EAGER award is pursuing preliminary research in tiered, model based signal processing that can exploit pre-determined signal templates to enable extreme power optimization. In this approach, signal processing can be performed at several levels, where in each level, only the hardware for a specific template is active. If the template of interest is present, the next level of signal processing will be activated, otherwise hardware components corresponding to the next and the remaining levels will remain inactive. This approach, however, highly depends on the effectiveness of templates. In monitoring human movements, if templates do not accurately represent the physical activity of interest, the system will not exhibit acceptable accuracy. The goal is to develop effective techniques and methodologies to ensure templates adapt to remain valid throughout the operation of the system, accurately representing the corresponding physical movements.<br\/><br\/>The research focuses on speed-insensitive template matching architectures that can reduce the effects of movement variations on signal processing. Timing models for movements and user activity profiles are exploited to monitor the correctness of the signal processing, and tunable parameters decrease or increase the sensitivity of the signal processing. For example, if the user is expected to perform sit to stand at least once every two hours in the day time, and the tiered signal processing has not detected the movement in the past few hours, the sensitivity will be increased, or user interaction and template retraining can be initiated. When performing a movement that has been determined to be of interest, the user can initiate (re)training if the system does not recognize the movement. Effective template generation and on-line retraining are expected to open opportunities to individualize systems and signal processing and to reduce the complexity of storage and processing architectures. This research is expected to provide the groundwork for ongoing design and development of practical ultra low power signal processing architectures, reduce costs of computing platforms for medical sensing, and to enable future progress in areas such as gait and balance monitoring for fall prevention, and in-home movement monitoring for Parkinson?s disease.","title":"EAGER: Methodologies for Tight Integration of Physical and Cyber Models in Power Aware Wearable Computers","awardID":"1138396","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["558157"],"PO":["565255"]},"186482":{"abstract":"Through our hands, we communicate, care for ourselves and others, and use tools to affect our world. However, our understanding of how we control our hands to accomplish such feats is still in its infancy. Dexterous manipulation is especially challenging to study, as even \"simple\" manipulation tasks are complex in their details. The task of lifting a wrench into the hand, for example, can be decomposed into five separate actions, each of which may require a specialized control strategy. It is important to study human examples of such activities; through understanding human expertise we can reach practical outcomes such as physically intelligent animated characters for training and remote communication, robots capable of unprecedented dexterity, and prosthetic designs that far exceed the current state of the art in their elegance and functionality.<br\/><br\/>The key hurdle in making a significant advance in these areas is the difficulty of capturing human manipulation in a form that facilitates analysis and study. The rapid sequence of contact events, the hand's large number of degrees of freedom, and close contact between the hand and object all contribute to creating an impossible capture task using traditional methods. The investigators are developing an alternative: simulation motion capture, where a user interacts with and guides a running simulation. Through this innovative approach, details such as contact timing, contact area, and contact forces are made available for the first time for general manipulation tasks. Key innovations include a fast simulation system for a deformable human hand (or full body) and novel techniques to control such a high degree of freedom simulation with intent and precision. In parallel, the investigators create a database of manipulation tasks, study new languages for action segmentation and control law development, develop robust autonomous controllers for grasping and manipulation, and study novel classifiers for recognition of affordances.","title":"CGV: EAGER: Simulation-Based Manipulation Capture for Dexterous Character Animation","awardID":"1145640","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["560865"],"PO":["532791"]},"175262":{"abstract":"World food security depends on improving the productivity and profitability of small farmers across the developing world. Unfortunately, many of these farmers do not benefit from the latest information and technologies. With the right access, these farmers could solve their own problem. Unfortunately, due to their often limited education and technological access, the text-based, English-dominated Internet is not a useful resource for most small farmers across the world. On the other hand, low-cost mobile phones and wireless connectivity are revolutionizing communications. This research seeks to understand how voice-based social media, deployed on mobile phones, can be used by agrarian communities for collaborative problem-solving, information access and knowledge generation. In particular, the investigators are interested in the following research questions. 1) How can we make voice-based user interfaces more intuitive and accessible for users with limited formal education? 2) How can we make voice-based information more trust-worthy for users with no prior experience assessing online information resources? 3) What is the economic impact of providing access to new sources of information and advice?<br\/><br\/>Answering these questions can improve the design and adoption of voice-based social media applications worldwide. Advances could also make information access more intuitive for other inexperienced users, such as the elderly. Making information and services more accessible to marginalized groups holds tremendous potential for benefiting society. By making more voices heard, governments and service organizations can better understand and address their needs. This research will be conducted in collaboration with local non-profit organizations and community groups in India, who are actively developing and deploying real, working voice applications. By designing and testing proof-of-concept applications, we can influence wider scaling through empirical demonstrations of usability, adoption and impact. This research provides opportunities for students to develop new sustainable enterprises and working products. By conducting fieldwork abroad, U.S.-based students will develop a broader understanding of the opportunities and challenges of working there. Such opportunities can attract diverse new students to Computer Science - including women and minorities.","title":"CAREER: Improving the Accessibility and Trust of Voice-based Social Media for Small Farmers in Developing Regions","awardID":"1054332","effectiveDate":"2011-08-01","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["550609"],"PO":["565227"]},"185184":{"abstract":"The project advances the state-of-the-art of autonomous Simultaneous Localization and Mapping (SLAM) algorithms, and maximizes the ability to explore under extreme conditions with minimal time using real-time 3D methods in nuclear power plants. The integrated system consists of a hardware system with three LIDAR's, a CPU, a GPU, a battery and a wireless unit.<br\/><br\/>The state-of-the-art in nuclear power plant disaster mitigation is a direct consequence, as well as the development of more advanced and robust SLAM techniques that are applicable in all domains.","title":"RAPID: Robots Designed to Assist During Nuclear Catastrophes - Autonomously Creating 3-D Maps, Collecting Radiation\/Other Data at Japan's Fukushima Nuclear Plants","awardID":"1138625","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[496879,"538557"],"PO":["543539"]},"185195":{"abstract":"Proposal #: CNS 11-38666<br\/>PI(s): Pu, Calton<br\/>Institution: Georgia Institute of Technology<br\/>Title: RAPID: Automating Emergency Data and Metadata Management to Support Effective Short and Long Term Disaster Recovery Efforts<br\/>Project Proposed:<br\/>This RAPID project, collecting, processing, and disseminating appropriate sensor data, aims to contribute to an effective recovery. The work addresses the challenges of sensor data flood during an emergency, through integration, evaluation, and enhancement of current data management tools, particularly with respect to meta-data. Automation of data and meta-data collection, processing, and dissemination are expected to alleviate the time pressure on human operators. The fundamental tools support quality information dimensions such as provenance, timeliness, security, privacy, and confidentiality, enabling an appropriate interpretation of the sensor data in the long term. For the short term, the tools are expected to help relief the workers as data producers and consumers; for the long term, they will provide high quality information for disaster recovery decision support systems. Additionally, the cloud-based system architecture and implementation of the CERCS cluster of Open Cirrus provide high availability and ease of access for recovery efforts in Japan as well as for researchers worldwide. The integration of techniques from several information dimensions (e.g., data provenance, surety, and privacy) and the application of code generation techniques to automate the data and metadata management tools constitute the intellectual merit of the proposed research. New challenges will be encountered in the potential interferences among the quality of information dimensions. It is also a new challenge to apply code generation techniques in the adaptation of software tools to accommodate changes imposed by environmental damages and contextual as well as cultural differences among countries.<br\/>The investigator collaborates with Prof. Masaru Kitsuregawa from the University of Tokyo, Japan, a leading researcher in data management. He is the first database researcher from Asia to win the ACM SOGMOD Innovation Award (2009). In addition to a letter of support and biographical sketches of the Japanese collaborator, a support letter has been submitted by Intel to OISE, CISE and Engineering. Intel has offered access to the Intel Open Cirrus cluster to conduct the research.<br\/>Broader Impacts: <br\/>The proposed tools should contribute to improve both the quantity and quality of data being collected by a variety of sensors, thus improving the effectiveness of short and long term decision making. For example, measured radiation levels in agricultural products can serve as an indication of spreading radioactive contaminations that complement the direct readings of radiation in soil samples. The project enables informed decisions based on precise interpretation of real sensor data that may improve the quality of life at both human and social levels, while reducing costs. The project will also contribute in graduate student education.","title":"RAPID: Automating Emergency Data and Metadata Management to Support Effective Short Term and Long Term Disaster Recovery Efforts","awardID":"1138666","effectiveDate":"2011-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["532971"],"PO":["535244"]},"187021":{"abstract":"Innovation in computer hardware, ranging from the simplest consumer devices to the most powerful scientific computers, relies on availability of software and hardware design and evaluation tools. The development of the cyberinfrastructure is itself a significant undertaking due to the complexity of today's computers and applications. This complexity has lead to ad-hoc infrastructure that is difficult to quickly extend in new ways, minimally tested, and arduous to use for comparing innovations. This burden greatly impedes progress of computer hardware research and development. The community-driven creation and maintenance of robust cyberinfrastructure can overcome this challenge by leveraging and combining effort and investment made by multiple groups. This workshop sets a foundation to grow a community for developing more reusable, better tested and interoperable tools. The energy, performance, reliability and cost of information technology will be further improved at a faster pace as a result.<br\/><br\/>The cyberinfrastructure for computer architecture (hardware) includes software simulation, hardware emulation, analytic models and benchmarks for CPU\/cache, network-on-a-chip (NoC), memory and storage. These tools are used to analyze existing systems and their bottlenecks, develop new hardware capabilities, and study and evaluate design trade-offs. While the tools are deeply ingrained and fundamental to computer architecture, the architecture cyberinfrastructure suffers badly from fragmented, ad hoc and disparate efforts, arising from research expediency and development burden. To address the situation, this workshop brings together tool developers and users to assess the current state of the cyberinfrastructure and to establish strategies and build a community for more coordinated and joint development. The workshop develops 1) an assessment of the current state of the cyberinfrastructure; 2) roadmaps of needed tool capabilities and support for future technologies; 3) ways to leverage and combine disparate development effort by multiple groups; and 4) a methodology to establish and sustain a cohesive community that builds and supports a set of open-source, extensible, scalable, validated and interoperable tools. The workshop outcomes serve building a community framework for simulating computer architecture.","title":"Cyberinfrastructure for Computer Architecture Design and Evaluation","awardID":"1148646","effectiveDate":"2011-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["560516"],"PO":["565272"]},"186064":{"abstract":"This project will create a computational theory of visual common ground, allowing users to give directives to a robot (or other team members) and receive confirmation or constraints through visual communication over a shared visual display. The motivating example is an urban search and rescue (US&R) professional tapping, sketching, and annotating on an iPad in order to direct a small unmanned aerial system (sUAS) without training. Previous work in human-robot interaction with common ground has been limited to natural language, but recent work has shown that having all team members see the robot's eye view in unmanned ground robots significantly improved performance and situation awareness. The proposed work populate the computational theory using the Shared Roles Model to represent the inputs (directives, notations), outputs (display viewpoint, form, size, location, content, etc.), and transformations (visual communication engine). The computational theory will be prototyped, refined, and tested by US&R practitioners flying realistic sUAS missions at Texas A&M's Disaster City.<br\/><br\/>Intellectual merit: The project will create a computational theory of visual common ground that will enable two-way human-robot interaction using visual communication mechanisms such as tapping, sketching, and annotation on shared visual displays on mobile devices such as iPads, smartphones, and tablet PCs. The results will advance the fields of human-robot interaction, artificial intelligence, and cognitive science. <br\/><br\/>Broader impacts: The results could revolutionize how people use mobile devices to interact with robots (and with each other) using naturalistic visual mechanisms, bypassing extensive training. The project will actively recruit women, Hispanics, and persons with disabilities to participate through REU programs. An open source visual communication toolkit for HRI researchers will be produced. The results will improve robots for public safety, remote medicine, and telecommuting, and could also immediately help save lives through incorporation into Texas Task Force 1.","title":"EAGER: Shared Visual Common Ground in Human-Robot Interaction for Small Unmanned Aerial Systems","awardID":"1143713","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["565226",499305],"PO":["565227"]},"186185":{"abstract":"EAGER: Exploring the biochemical principle of allostery for algorithm development<br\/>PI: Judith Klein-Seetharaman, Department of Structural Biology, University of Pittsburgh<br\/>Co-PI: Christopher J. Langmead, Department of Computer Science, Carnegie Mellon University<br\/><br\/>Project Abstract<br\/>Motivation: Computer science and biology have inspired each other by drawing analogies leading to new classes of algorithms such as neural networks and genetic algorithms and new fields such as computational biology and biocomputing (computing using biomolecules). The ever increasing data streams in everyday life as well as biology are most often characterized by networks, such as the internet, telephone network, disease transmission networks, social networks to name just a few. Networks consist of nodes connected by edges and only the nodes vary with the application areas. The network structures are conserved: the edges allow communication and information flow. Due to the size, complexity and dynamic nature of such networks, their control is challenging. As environments change, the structures of these networks change and are subject to numerous perturbations and failures. Nature faces these same types of challenges and has evolved robust strategies for ensuring that information is transmitted and that the system appropriately responds to changes in the environment: for example, the oxygen transport protein in the blood, hemoglobin, changes its affinity to oxygen in response to small molecule ligands favoring oxygen release where needed. The strategy that Nature employs in hemoglobin is called allostery.<br\/>Opportunity: Allostery is a biochemical term that refers to the ability of biomolecules, in particular proteins, to achieve action at a distance in the atomic network through a small, localized perturbation. Proteins can be viewed as networks of atoms interacting in three-dimensional space. Allostery is a classical text-book example of an experimentally well studied and firmly established mechanism of control of this atomic network. Here, PIs propose the hypothesis that one can share the biochemical principle of allostery with other domains such as disease transmission, social networks, economics, surveillance applications and cloud computing. Understanding how Nature performs acquisition, transmission and processing of information at the molecular level may lead to future enabling technologies in other domains. <br\/>Intellectual Merit: While the proposed hypothesis is potentially transformative, in-depth pursuit requires obtaining a proof-of-concept outlining (1) what kinds of mechanisms might exist in proteins that could be transferred to other domains and (2) develop an understanding of what are the requirements for such a transfer. Although allostery in proteins is an established biochemical principle, little is known how it works and how to predict it. Some proteins are regulated through allostery and others are not. Unfortunately, there is no simple property that determines whether a given protein is (or can be) allosterically regulated. While experimental methods provide direct evidence for allostery, they generally do not reveal the detailed physical and biological mechanisms for it. PIs propose to reveal these mechanisms through the combination of computation and experiments: a predicted path of communication between two distant sites can be validated or refuted by disrupting this path. The deliverables of this work will be a list of fundamental principles of allostery in proteins, and an analysis of their suitability for future extension to non-protein related networks. <br\/>Broader Impact: This grant will support investigators and train graduate students from the areas of computer science, chemistry, biology & biomedicine. Convergence of technologies, here between protein allostery modeling and network control, is expected to speed up scientific progress in potentially many disciplines. Thus, one may in the future be able to push the field of biocomputing forward, predict disease outbreaks or identify action at a distance in economic networks.","title":"EAGER: Exploring the biochemical principle of allostery for algorithm development","awardID":"1144213","effectiveDate":"2011-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[499611],"PO":["565223"]},"177363":{"abstract":"The goal of this project is to better model the human perception of quality variations in audio, video, and audiovisual signals. A fundamental challenge in accomplishing this goal, however, is to develop a human subjective testing methodology that can assess such the perceived quality variations with sufficient accuracy in time. Rather than conducting human trials that simply ask subjects to rate the signal quality at given moments in time, in this research the electrical patterns of each subject's brain responses to multimedia signals of varying quality are captured using a high resolution electroencephalograph (EEG). By analyzing these EEG signals, it becomes possible to detect a change in the perceived quality of the signal before the human observer\/listener even becomes consciously aware that the quality has changed.<br\/><br\/>A major difficulty, however, is that the EEG waveforms captured during these trials contain large amounts of noise, and it is therefore necessary to sift through a large set of data to identify the components of the collected EEG waveforms that correspond to changes in perceived quality. To accomplish this task, both deterministic time-space-frequency analysis techniques will be applied as well as stochastic techniques based on information spectra. To create computer-based models of perceived quality, AR\/ARMA (autoregressive\/autoregressive moving average) modeling techniques will be considered and support vector machines along with related kernel-based classifiers will be designed to output class indexes corresponding to perceived quality. Beyond its potential for improving audiovisual transmission systems, the broader impacts of this research include the possibility that it will open up new and more efficient avenues for transferring information from computers to human beings.","title":"CIF: Medium: Assessment and Modeling of Temporal Variation in Perceived Audio and Video Quality Using Direct Brainwave Measurement","awardID":"1065603","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["551001",475506,475507],"PO":["564898"]},"189276":{"abstract":"The most powerful computing systems in the world have historically been dedicated to solving scientific problems. Until recently, the computations performed by these systems have typically been simulations of various physical phenomena. However, a new paradigm for scientific discovery has been steadily rising in importance, namely, data-intensive science, which focuses sophisticated analysis techniques on the enormous (and ever increasing) amounts of data being produced in scientific, commercial, and social endeavors. Important research based on data-intensive science include areas as diverse as knowledge discovery, bioinformatics, proteomics and genomics, data mining and search, electronic design automation, computer vision, and Internet routing. Unfortunately, the computational approaches needed for data-intensive science differ markedly from those that have been so effective for simulation-based supercomputing. To enable and facilitate efficient execution of data-intensive scientific problems, this project will develop a comprehensive hardware and software supercomputing system for data-intensive science.<br\/>Graph algorithms and data structures are fundamental to data-intensive computations and, consequently, this project is focused on providing fundamental, new understandings of the basics of large-scale graph processing and how to build scalable systems to efficiently solve large-scale graph problems. In particular, this work will characterize processing overheads and the limits of graph processing scalability, develop performance models that properly capture graph algorithms, define the (co-design) process for developing graph-specific hardware, and experimentally verify our approach with a prototype execution environment. Key capabilities of our system include: a novel fine-grained parallel programming model, a scalable library of graph algorithms and data structures, a graph-optimized core architecture, and a scalable graph execution platform. The project will also address the programming challenges involved in constructing scalable and reliable software for data-intensive problems.","title":"SHF: Large: Collaborative Research: PXGL: Cyberinfrastructure for Scalable Graph Execution","awardID":"1160602","effectiveDate":"2011-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["563330"],"PO":["565272"]},"178298":{"abstract":"Election mechanisms are broadly used in computational settings, including a rapidly expanding range of applications in multiagent systems. Twenty years ago, responding to results showing that all reasonable elections systems can be manipulated, Bartholdi, Orlin, Tovey, and Trick proposed protecting election systems from manipulation by making the attacker's task computationally prohibitive, e.g., NP-hard. Their work started a rich line of research, yielding many such computational protection results.<br\/><br\/>However, a number of weaknesses in this approach have emerged: (1) Much of the work assumes that voters have complete, transitive preferences and that the manipulator has perfect knowledge of the preferences of each voter. (2) Many election systems have polynomial-time algorithms for perfect manipulation and so cannot be computationally protected. (3) Even when there are NP-hardness results, these assume all ensembles of voters are possible, and it has recently been shown that when one looks at, for example, voters obeying the common behavior model known as single-peakedness, these NP-hardness results often evaporate. (4) Even when there are NP-hardness results, they are worst-case results, and so it is possible that often-correct heuristic attacks exist.<br\/><br\/>This project will respond to these weaknesses, rebuilding the computational approach to protecting elections and more rigorously delineating its limitations. More natural and realistic models will have strong consequences in terms of the weaknesses discussed above. Complexity theory and algorithmics will both be developed in a broad investigation of the above weaknesses and techniques to go beyond them to retain the value of using complexity theory to protect election systems against manipulation.","title":"ICES: Small: Collaborative Research: New Approaches to Computationally Protecting Elections from Manipulation","awardID":"1101479","effectiveDate":"2011-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8052","name":"Inter Com Sci Econ Soc S (ICE)"}}],"PIcoPI":[478022],"PO":["565251"]}}