{"107877":{"abstract":"Problem statement Quantum information theory, the quantum extension of Shannon's classi-<br\/>cal information theory, has experienced a surge of interest and rapid technical advance over the past<br\/>ten years, largely in response to the development of quantum-mechanically based cryptographic<br\/>protocols and algorithms for quantum computing. Very recent work by the Principal Investigator<br\/>and collaborators has led to a \\grand unification\" in the two-user setting, and much is now under-<br\/>stood about the interplay between the plethora of quantum information processing resources such<br\/>as quantum channels, entanglement and classical communication. As progress in the field has been<br\/>hitherto guided by experiences from the classical theory, the next natural step is to investigate the<br\/>quantum extensions of classical network (or multi-user) information theory. We propose a number<br\/>of problems in quantum network information theory, including quantum versions of distributed<br\/>compression, coding for multiple-access channels, broadcast channels and channels with state in-<br\/>formation, and channel simulation with side information. Preliminary investigations indicate that<br\/>many of the methods used in quantum two-user information theory apply to the multi-user setting,<br\/>while revealing interesting new behaviour. Moreover, solutions to certain two-user problems may<br\/>be interpreted in stunning new ways as solutions to long sought after multi-user ones.<br\/>Intellectual merit This research will contribute to both quantum information theory and clas-<br\/>sical network information theory. Multi-user scenarios shed new light on the properties of quantum<br\/>information, in particular i) the ways in which two independent sources of quantum information<br\/>may be used, despite their delicate nature, to decode each other; ii) the ways in which a distributed<br\/>quantum source may be localized. On the other hand, \\quantizing\" classical results has hitherto<br\/>lead to several new techniques and re-interpretations of the classical theory.<br\/>Broader impact This research will contribute to developing links between network information<br\/>theory and quantum mechanics. On a more practical level, due to the traditional connection<br\/>between quantumness and privacy, network quantum information theory bears relevance to multi-<br\/>user quantum cryptography. There is perhaps only a handful of experts in quantum information<br\/>theory currently in the US; the way to improve this it through education. The funding of this<br\/>proposal would enable the training of PhD students in this area, and facilitate the introduction<br\/>of this new material into the University of Southern California course curriculum. The Principal<br\/>Investigator is developing a course on quantum information theory to be ordered in Fall 2005,<br\/>covering topics not ordered at any other US university. He is also writing a book with co-authors<br\/>A. Winter and P. Hayden entitled \\Principles of Quantum Information Theory\".","title":"QnTM: EMT - Quantum Network Information Theory","awardID":"0524811","effectiveDate":"2005-08-15","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":["294913"],"PO":["565223"]},"105699":{"abstract":"The Earth System Curator collaboration will unify the treatment of models and datasets relating to climate change by developing a common language - a metadata formalism - with which to describe the two, and by prototyping a set of tools based on that formalism that allows researchers to manipulate models and datasets seamlessly and with ease. The goal, in the end, is to increase the productivity of climate researchers and understanding of the Earth system.<br\/><br\/>Collaborators include computer science and Earth science researchers at MIT, Princeton University, the Georgia Institute of Technology, and the National Center for Atmospheric Research. The work proposed builds on two ongoing community efforts, the Earth System Modeling Framework (ESMF) and the Earth System Grid II (ESG). The ESMF is a national initiative to develop common modeling infrastructure for the nation's climate and weather models, including coupling tools and standard modeling utilities. The primary objective of ESG is to make the output of high-resolution, long-duration simulations performed with climate models available to global change impacts researchers nationwide, through the use of Grid, data\/metadata, and portal technologies. The team will explore those aspects of ESMF and ESG that can be usefully aligned, and will prototype a new entity, the Earth System Curator, that spans the gap between the two.<br\/><br\/>The Curator begins with a crucial insight: that the descriptors used for comprehensively specifying a model configuration are needed for a scientifically useful description of the model output data as well. The development of a common metadata schema that describes both will be the basis for this unique and powerful community resource. The Curator will provide a community database from which researchers can archive and query a wide class of Earth system models, experiments, model components, and model output data and results. Researchers will subsequently be able either to analyze model output from pre-existing runs, or to access a model and modify and run it themselves, either on a local computer or on the virtualized resources of the computational Grid. In addition to the query function, the project will prototype a tool that will test if sets of model components or datasets can interact to form an application. Finally, as part of the Curator effort, tools for auto-generation of component wrappers and applications will also be prototyped.<br\/><br\/>The Curator is part of a community vision for the use of information technology in climate and related research. The Curator prototype will help to further suggest and define the form of next-generation modeling and data management tools, by offering a concrete representation to add credibility to innovative ideas. Further, the ESMF and ESG co-investigators, by virtue of their projects' emphases on production software and extensive customer bases, are in an excellent position to transition the Curator tools into a viable product following an NSF-funded prototype stage. Over the course of the Curator effort, many researchers associated with ESMF and ESG will be encouraged to try out and offer feedback on the Curator software. Advances achieved with the Curator project will influence other domains through conferences and publications, and through a web of relationships founded on a shared need for multi-component HPC modeling, ease of information archival and access, and similarities in simulation numerics. While the advances in climate prediction due directly to the Curator effort itself may be both difficult to track and modest, an integrated environment for Earth system research is critical to addressing world climate issues in the near future, and the Curator is a definitive step in that direction.<br\/><br\/>The Curator will also be incorporated into the project infrastructure for software engineering courses at Georgia Tech. Appropriate introductory material will be prepared and project opportunities defined. Many of the project opportunities will take the form of making climate data accessible to the general public. In this way the Curator not only provides software engineering students an opportunity to participate in an actual ongoing engineering development effort, but the resultant projects will make Earth science more available to the general public.","title":"Collaborative Research: Earth System Curator: Spanning the Gap Between Models and Datasets","awardID":"0513762","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7373","name":"ITR-INFORMATION INTEGRATION"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"7231","name":"CYBERINFRASTRUCTURE"}}],"PIcoPI":["487297"],"PO":["565136"]},"107888":{"abstract":"Proposal ID: 0524853<br\/>Title: Multi-Model Anomaly Detection for Web-based Applications<br\/>PI: Giovanni Vigna<br\/><br\/>Web-based systems are a composition of infrastructure components, such as web servers and databases, and of application-specific code, such as HTML-embedded scripts and server-side applications. While infrastructure components are usually developed by experienced programmers with solid security skills, application-specific code is often developed by programmers with little security training. As a result, vulnerable web-applications are deployed and made available to the whole Internet, creating easily-exploitable entry points for the compromise of entire networks. Unfortunately, existing signature-based intrusion detection solutions are not sufficient because Web-applications often implement custom, site-specific services for which there is no known signature or model.<br\/><br\/>The goal of this research is to develop intrusion detection tools that use novel anomaly detection techniques to autonomously learn the normal behavior of web-based systems. These tools will enable the detection of known and unknown attacks against both standard and custom-developed web-based applications without requiring expert knowledge. This effort is developing a multi-stream, multi-model anomaly detection approach to provide a more effective characterization of the behavior of web-based applications. The use of multiple anomaly models applied to different event streams (such as network packets, web requests, and system calls) allows for the creation of rich, multi-dimensional profiles that characterize different aspects of the behavior of web applications.<br\/><br\/>These intrusion detection tools have the potential of providing early warning against novel attacks and can be easily deployed on existing systems without requiring substantial security expertise. As a consequence, these tools will substantially improve the security of a wide range of critical applications.","title":"CT-ISG: Multi-Model Anomaly Detection for Web-Based Applications","awardID":"0524853","effectiveDate":"2005-08-15","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["535035",285015],"PO":["529429"]},"108526":{"abstract":"Digital scanning devices are capable of acquiring high-resolution 3D models and have recently become affordable and commercially available. Modeling detailed 3D shapes by scanning real physical models is becoming commonplace. Current scanners are able to produce very large amounts of raw, dense point sets; consequently, there has been a recent increase in the need for techniques for processing point sets. One of the principal challenges faced today is the development of surface reconstruction techniques that deal with the inherent noise of the acquired dataset. When the underlying surface contains sharp features, the requirement of being resilient to noise is especially challenging, since noise and sharp features are ambiguous, and most existing techniques tend to smooth important features or even to amplify noisy samples. The project helps to train young researchers to work at the intersection of graphics, geometry, and statistics, while enabling them to pursue theoretically sound work that has deep practical impact. The data, software, and models developed in this project will be disseminated for other researchers to use in benchmarking and testing.<br\/><br\/>This research involves producing efficient and theoretically sound techniques for robust, feature-preserving surface reconstruction. It builds on recent work on the construction of a manifold surface from a set of points by using a moving least-squares (MLS) technique. The project explores the use of robust statistical techniques arising from outlier identification in MLS-based surface reconstruction. The approach is related to recent developments in feature-preserving smoothing, but it defines a surface rather than filtering the geometry. The techniques not only point to more reliable MLS projection but also extend the representation power of the underlying MLS surface definition to enable the representation of objects with sharp features.","title":"MSPA-MCS: Collaborative Research: New Methods for Robust, Feature-Preserving Surface Reconstruction","awardID":"0528201","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7454","name":"MSPA-INTERDISCIPLINARY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":["521991"],"PO":["550859"]},"106029":{"abstract":"ABSTRACT<br\/>0515218<br\/>Alex Pothen<br\/>Old Dominion Research Foundation<br\/><br\/>Data Migration in Parallel Computing: Models and Algorithms<br\/><br\/>Scientific computing problems being solved on multiprocessors currently have large data sets that<br\/>require access to external memory. Redistributing the data among the processors requires an<br\/>inter-processor communication schedule that minimizes (or approximately minimizes) the communication costs. This problem also arises in several other contexts: parallel file systems, parallel I\/O, and in grid computing applications.<br\/>The objective of this proposal is to design, analyze and implement practical algorithms that enable data migration in a multiprocessor with low communication costs. Combinatorial models of the problems lead to<br\/>edge coloring and its generalizations on appropriate graph representations. Algorithms for these problems<br\/>will be implemented and evaluated on two application areas with different characteristics:<br\/>computational science and information science.<br\/><br\/>The broader impact this work is expected to have include the following: Applications from computational sciences that require parallel computing are becoming more data intensive, as large scale data sets become available in many scientific and engineering disciplines. Data access continues to be a significant bottleneck for large-scale multiprocessors, and the proposed work is expected to alleviate the data access costs for these applications. <br\/>A graduate student will be trained in the project and also on broader research problems in combinatorial scientific computing. A module based on this work will be included in a graduate course<br\/>on parallel computing. The PI is also involved in community building activities in combinatorial scientific computing.","title":"Problems in Combinatorial Scientific Computing (Data Migration in Parallel Computing: Models and Algorithms)","awardID":"0515218","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["518525"],"PO":["381214"]},"100870":{"abstract":"The exponential growth of the web, the recent technological progresses in molecular biology, the launch of massive-scale digital library projects, and the ability of exchanging information at our fingertips, have all contributed to the creation of an unprecedented quantity of textual data in digital form. Plain or semi-structured text is still the most versatile format in which to exchange information and there is so much of this data that is likely that the large majority of it will never be read by anyone, unless the way in which we access information drastically improves.<br\/><br\/>The major limiting factor in handling large textual datasets is typically related to space rather than time. When the amount of data is too large to be stored in main memory, computer scientists have to resort to algorithms capable of dealing with compressed representations of the data (called 'sketches' or 'indexes'). For textual data, the construction of the sketch typically involves keeping statistics on substrings or related associations or rules.<br\/><br\/>The first set of objectives of this project is centered around a new sketch based on a novel family of gapped patterns. We are applying the new index to three selected problems: databases; data compression; and computational biology. In the second set of objectives we are extending the pattern discovery problem to two-dimensional matrices. The discovery problem associated with two-dimensional patterns has a wide spectrum of applications including the analysis of gene expression data, recommender systems and collaborative filtering, identification of web communities, load balancing, and discovery of association rules.<br\/><br\/>The education goal of the proposal is to establish the algorithmic and the fundamental software development component of an interdisciplinary bioinformatics curriculum. Funds from this proposal are being used to enhance these activities through the development of new courses in computational genomics for in-depth training on individualized research topics. Since UCR is a minority-serving institution, this plan will also have an impact on the education of under-represented students.","title":"CAREER: Combinatorial Algorithms for Pattern Discovery with Applications to Data Mining and Computational Biology","awardID":"0447773","effectiveDate":"2005-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7294","name":"SCIENCE & ENGINEERING INFORMAT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["541840"],"PO":["565136"]},"104841":{"abstract":"Moore's law based doubling of transistor counts and decreasing feature sizes have resulted in a phenomenal increase in performance at the expense of an exponential increase in power consumption<br\/>and consequently, heat generation. Scaling according to Moore's law cannot continue unless system-level solutions for power and thermal management are developed. Existing techniques for system-level<br\/>power optimization including dynamic power management (DPM) and dynamic voltage frequency scaling (DVFS) operate in a piece-meal fashion and are sub-optimal in general. Thermal management schemes, to date, are also fairly simple and do not interact with DPM or DVFS policies that control the power consumption and therefore heat generation.<br\/><br\/>This research will provide a unified framework for energy and thermal management of multi-component computing platforms. The framework will be built around optimal analytical solutions to generic<br\/>problem formulations that will be applicable at multiple levels of computing system abstraction. The abstraction levels will include board level, multi-processor system-on-chip level and micro-architecture<br\/>pipeline level. The energy and thermal management strategies developed here will reduce the packaging and cooling costs and increase the lifetime of all computing systems -- from portable devices to high<br\/>performance desktops to servers. Furthermore, this research will train students in the diverse fields of energy management of microelectronic systems and thermal management of electronic and non-electronic components.","title":"CSR-EHS: Analytical Techniques for Global Energy Minimization of a System of Interacting Components","awardID":"0509540","effectiveDate":"2005-08-01","expirationDate":"2010-05-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["525996","518174","401065"],"PO":["561889"]},"105831":{"abstract":"Video-enabled cell phones have the potential to enable deaf Americans to speak in their community's native language, American Sign Language (ASL), and gain the freedom, flexibility, and comfort of the wireless phone revolution. This research involves the design, implementation, and evaluation of new standards-compliant data compression methods that will allow ASL video, and other structured video, to be transmitted over low bandwidth cell phone channels. To be more specific, the goal of this research is to develop a framework in which to implement low-complexity H.264 encoding to provide maximum quality compressed structured video. Because the developed algorithms will be H.264 standard-compliant, an off-the-shelf H.264 decoder can be used.<br\/><br\/>The research applies to any class of videos where there is some inherent structure that can be exploited. The five areas of research are: (1) Design of an appropriate objective distortion metric for ASL; (2) Design of algorithms to preprocess video for display on small devices; (3) Development of methods to exploit structure in video for efficient coding; (4) Implementation of a rate-distortion-complexity optimization of H.264 using the new metric; and (5) Applications of the framework to other structured video such as surveillance video. Studies with ASL users are used to develop the objective intelligibility metric and to evaluate the new compression techniques.","title":"Collaborative Research: A Framework for Encoding American Sign Language and Other Structured Video","awardID":"0514353","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6846","name":"UNIVERSAL ACCESS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["549399","485574"],"PO":["564898"]},"104753":{"abstract":"Many embedded systems rely upon communication systems to exchange<br\/>information and coordinate activities in distributed<br\/>applications. Developing communication systems which are fast,<br\/>reliable, energy-efficient and error-free is a challenge due to the<br\/>many trade-offs involved. Software implementations are often<br\/>inefficient due to poor coding practice.<br\/><br\/>This project is developing new ways to develop and implement<br\/>communication protocols for embedded networks using a building-block<br\/>approach. A tool is being developed which offers a collection of<br\/>commonly used protocol components which the developer tunes and<br\/>interconnects as needed for the application. The tool then generates<br\/>an efficient program from these components using software thread<br\/>integration. The tool also estimates protocol performance and<br\/>computational requirements, allowing system designers to have quick<br\/>feedback on the impact of design decisions.<br\/><br\/>A major goal of this project is making rapid prototyping of embedded<br\/>communication systems accessible to non-specialist researchers and<br\/>practitioners, as well as to address real-world problems. Two specific<br\/>applications are being used to promote these interdisciplinary<br\/>goals. The first application is ultrasonic marine biotelemetry, which<br\/>enables the analysis of movement, physiological function and behavior<br\/>of marine organisms. One team member is a marine biologist with<br\/>extensive experience in this area who is defining requirements and<br\/>guiding testing. The second application is structural health<br\/>monitoring (SHM) of bridges using wireless sensor networks. Another<br\/>team member is collaborating with civil engineers on an SHM project,<br\/>and leverages the sensor network protocol experience gained.","title":"CSR---EHS Rapid Efficient Implementation of Communication Protocols for Embedded Systems","awardID":"0509162","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["555636","485952","526944"],"PO":["493916"]},"105985":{"abstract":"Abstract<br\/>Ad hoc networks are expected to be an essential ingredient in tomorrow's communication sys- tems. Important applications include highly exible personal communication systems, rescue and disaster recovery operations. Wireless sensor networks that are envisioned to be indispensable in future infrastructure: from manufacturing to medical { can also be considered a special kind of ad hoc networks. This research studies the problem of overall performance (eciency, quality of service, and robustness) of large realistic ad hoc networks which are, in particular, characterized by variable node density and constant changes in the network due to node mobility, node failure, changing node membership, fading etc.<br\/>While some progress has been made in understanding the main factors a ecting the performance of ad hoc networks, theoretical exploration of the fundamental performance characteristics of realistic networks that possess essential space- and time-inhomogeneity has been largely absent.<br\/>This research attempts to close this gap and develop a theoretical framework for the analysis of fundamental limitations and performance of such complex large scale ad hoc networks. On a larger scale, this research contributes to the analytical methods of study of higher network layers by developing general analytical machinery for the analysis of the network layer in the presence of spatial and temporal inhomogeneity.","title":"Space-Time Inhomogeneity and Performance of Large Scale Ad Hoc Networks","awardID":"0514970","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["380971","319364"],"PO":["432103"]},"102113":{"abstract":"Abstract<br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: Infrastructure for Multi-Agent Decision-Making Research <br\/>Proposal: CNS 0453923<br\/>PI: Stuart M. Shieber<br\/>Institution: Harvard University <br\/><br\/><br\/>Project: This project will support the development and distribution of community resource software supporting research and education in multi-agent decision-making. The PI's will expand a research for use in research studies, modeling, and strategy development on multi-agent tasks. The testbed is based on a parameterized family of games called Colored Trails (CT) in which agents as individuals or teams make decisions about resource allocation. The community resource software will provide more realistic decision making settings than previously available systems, will facilitate analysis of decision making strategies of individuals, teams, and computer agents or combinations of these and enable development and testing of decision-making strategies. Broader impacts include facilitating research in multi-agents systems and machine learning, supporting new groups in research and education in these areas, development of research and education materials, support for REU students, and participation in diversity efforts.","title":"CRI: Infrastructure for Multi-Agent Decision-Making Research","awardID":"0453923","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["333902","379957",269188],"PO":["564456"]},"106854":{"abstract":"Proposal: 0519398 <br\/>Principal Investigator: Thomas Harrison <br\/>Title: A Reliable, Robust, Robotic One-Meter Telescope for Research and Graduate\/Undergraduate Student Training <br\/><br\/>ABSTRACT: <br\/><br\/>New Mexico State University owns and operates a one-meter telescope at Apache Point Observatory (APO) in southern New Mexico. This telescope is fully robotic and scientifically active, monitoring the light curves of cataclysmic variables, brown dwarfs, and supernovae discovered by the Sloan Digital Sky Survey. The telescope is in need of refurbishment. As a result, its operation will be made more reliable and new instrumentation and capabilities will be made available. <br\/><br\/>Work packages that will be undertaken will: (1) Improve the usage to greater than 90% of all clear nights, through increased manpower (including the use of a graduate student to share primary responsibility for operations), improvements to some critical under-engineered subsystems, and the purchase of basic spare parts. (2) Integrate a new, existing, 2048x2048 CCD (built in collaboration with Los Alamos National Labs) into the telescope to allow wider-field operation with a detector with sufficiently low dark current to allow for narrow-band imaging, and to replace our current tiny guide camera with a new CCD camera. (3) Improve performance via improvements in optical alignment and improved guiding software. (4) Improve optical performance and allow for simultaneously mounting a second instrument by constructing a new rotating tertiary with actuated control of rotation, translation, tip, and tilt. (5) Construct a high-speed photometer for the second Nasmyth port to allow high time-resolution photometry simultaneously in five colors. <br\/><br\/>The renovated telescope will be used for a number of research projects, including programs in narrow-band imaging, rapid-response follow-up to high energy transits (including those from SWIFT), monitoring of known extrasolar planet systems for transits, searches for periodic and quasi-periodic variability in interacting binaries, pulsating white dwarfs, and the optical counterparts of high-energy transients, etc. Up to 20% of the telescope time available to the community through queue scheduled observations, including the possibility of target-of-opportunity observations, through an internally review program. Additional time will be made available for external scientists who propose projects in collaboration with NMSU personnel. <br\/><br\/>The renovation efforts will provide graduate and undergraduate students with both engineering and research opportunities, including instrument design and deployment, as well as software development. Use of the telescope will be more reliably integrated into existing astronomy classes and the 1-m will be incorporated into an on-campus program aimed at under-represented middle and high school students in science. <br\/><br\/>This award is funded by the Division of Astronomical Sciences and the Office of Multidisciplinary Activities.","title":"A Reliable, Robust, Robotic One Meter Telescope for Research and Graduate\/Undergraduate Student Training","awardID":"0519398","effectiveDate":"2005-08-01","expirationDate":"2012-07-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0302","name":"Division of ASTRONOMICAL SCIENCES","abbr":"AST"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0302","name":"Division of ASTRONOMICAL SCIENCES","abbr":"AST"},"pgm":{"id":"7349","name":"PR FOR RES & EDU W\/SM TELESCOP"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"H188","name":"DEFENSE INTELLIGENCE AGENCY"}}],"PIcoPI":["513913","553824"],"PO":["496047"]},"102135":{"abstract":"Abstract<br\/><br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: Collaborative: Next Generation CiteSeer <br\/><br\/>Collaborative Proposal<br\/>Lead Proposal: CNS 0454052<br\/>PI: C. Lee Giles<br\/>Institution: Pennsylvania State University University Park<br\/><br\/>Proposal CNS 0454121<br\/>PI: Susan Gauch<br\/>Institution: University of Kansas Center for Research Inc <br\/><br\/><br\/>This community resource project builds on the CiteSeer project that provides access to over 700,000 academic articles in computer science with search methods that access authorship, citations, and other structural aspects of the articles. CiteSeer is currently freely available to the public for use, getting over one million hits per day. This project will support a Next Generation CiteSeer that will address performance and reliability in the existing system and support research and education in the national research community interested in search, citation studies, user studies, and related areas. Specific strategies for the project include increased server capacity, redesigning CiteSeer software, developing a Web Services architecture, expanding the collection, learning from user and search patterns, supporting collaborative usage, evaluation, and addressing sustainability. In broader impacts, this community resource will provide a realistic scale model for researchers and students across the nation to use in research and education. The project will also improve CiteSeer's core capabilities for use in bibliographic tasks in research.","title":"CRI: Collaborative: Next Generation CiteSeer","awardID":"0454052","effectiveDate":"2005-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"7373","name":"ITR-INFORMATION INTEGRATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["562561","549541","456075"],"PO":["563751"]},"107833":{"abstract":"Proposal Number: NSF-0524269<br\/>TITLE: DoS Prevention in Shared Channels<br\/>PI: Carl Gunter<br\/><br\/>Methods for analyzing and preventing Denial of Service (DoS) threats are of fundamental value for designing robust Internet protocols. Much work has been done to develop pragmatic solutions to protocol-specific DoS threats, but there is a lack of realistic theoretical models for studying DoS and of broad paradigms for designing DoS-resilient protocols. This project develops theoretical models based on a \"shared channel model\" which describes how adversaries and valid senders share the network bandwidth of attack targets. It exploits this model to design counter-measures based on a paradigm in which asymmetries in protocol workloads that are exploited by adversaries are systematically converted to the advantage of trusted parties. Specific project goals include developing (1) general techniques for obtaining DoS-resilience that can be used to adapt existing protocols or create new ones; (2) ways to automate DoS analysis of protocols to reduce the effort required to confirm practical availability properties theoretically and find unexpected attacks before protocols are deployed; and (3) a unified model of integrity, confidentiality, and availability based on both existing algebraic techniques and new probabilistic techniques.","title":"Collaborative Research: CT-T: DoS Prevention in Shared Channels","awardID":"0524516","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["550264","553855"],"PO":["529429"]},"106986":{"abstract":"Internet measurements drive improvements in Internet infrastructure and provide the foundation for the Internet performance research. Thus, the availability and quality of measurements is of fundamental importance to continued progress in these areas. There have been a number of research projects, such as IDMaps, NIMI, King, and Internet Weather Service, that collect general Internet measurements, and then answer specific measurement requests by estimating the requested values from the collected generic data. At the same time, a need often arises for a focused, on-demand measurement of a certain feature. As a typical example, a company selecting a content delivery network for accelerating its Web site may need to compare the performance of several content delivery networks with respect to a specific client population. While some platforms (most notably, Scriptroute service deployed on PlanetLab and, commercially, Keynote Systems) offer measuring hosts to serve this need, they can provide only a limited number of measuring hosts, and hence a limited perspective on the Internet performance. It is clearly not feasible for a single company to provide a representative sample of the entire Internet. Worse, having a limited number of well-known measuring hosts allows systems being measured to \"game the system\", by optimizing specifically for those hosts. Finally, the closed proprietary system for Internet measurements limits user choice of the types of measurements. For example, in HTTP, a measurement of a page download done with and without pipelining and persistent connections, and with different caching settings at the client, would yield very different results. One cannot rely on the prowess of a single company to timely detect all the trends in HTTP (or other Internet applications), and provide the entire menu of various modes of client operation. Given the scale of the Internet and the unpredictability of the needs for particular measurements, these limitations can only be overcome by harnessing the capacity of the Internet at large itself. <br\/><br\/>The principal investigators (PIs) propose an open Internet-wide system, named DipZoom (for \"Deep Internet Performance Zoom\"), which is based on peer-to-peer principles and which would enable focused measurements and numerous measuring choices. There are two key ideas behind DipZoom. First, rather than trying to build a global-scale measuring platform, the PIs propose a matchmaking service that merely provides \"plumbing\" to connect measurement providers and requesters. The measurements themselves are left to outside providers. Second, they will use a market approach as an effective regulator of system behavior without rigid built-in control mechanisms. Together, these two ideas would create a marketplace for Internet measurements: an open ecosystem where anyone can offer measurements from their computers and other computing devices, and anyone can request measurements. The hope is that the combination of an open system with market forces will encourage great and unpredictable innovation and diversity in the offered measurements and measuring devices. <br\/><br\/>Broader Impact: Such a system would significantly enhance our ability to gain insight into Internet characteristics, enable companies to optimize and monitor their Internet sites for focused client populations, and would make \"gaming the system\" infeasible due to the number of potential measuring hosts.<br\/><br\/>As the first step in realizing this vision, the PIs will concentrate on building the case for DipZoom during the current award period. This involves obtaining experimental data that would demonstrate DipZoom's benefits in gaining insight into the Internet performance, and detailed conceptual design for the system that would lay the groundwork for a future DipZoom implementation.","title":"NeTS-NBD: The Internet Measurements Marketplace","awardID":"0520105","effectiveDate":"2005-08-01","expirationDate":"2007-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[282137,"409444"],"PO":["565090"]},"102146":{"abstract":"Abstract<br\/><br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: An Open Linguistic Infrastructure for American English <br\/>Proposal: CNS 0454130<br\/>PI: Nancy M. Ide<br\/>Institution: Vassar College<br\/><br\/>This planning grant will support the PI in preparing a community resource proposal for further development of the American National Corpus (ANC) which is currently an 11 million word corpus of contemporary American English. The ultimate goal is to create a 100 million-word collection that will be an enabling data resource for research in linguistics, computational linguistics, natural language processing, information retrieval, machine translation and other areas. The PI will address the challenging problem of planning a resource that addresses the needs of these communities with different requirements for their research. The PI will convene an advisory board, organize a workshop to further define needs, and receive research community comments. Proof of concept developments will be conducted to demonstrate the use of annotation tools with different parsers and semantic taggers, and to augment existing analysis tools, and enhance the ANC web interface.","title":"CRI: An Open Linguistic Infrastructure for American English","awardID":"0454130","effectiveDate":"2005-08-15","expirationDate":"2006-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["501442",269296],"PO":["297837"]},"107723":{"abstract":"This proposal investigates quantum algorithms based on quantum physical processes.The goal<br\/>is to identify quantum processes that can be reformulated to serve as quantum algorithms,and to<br\/>identify the types of errors that can disrupt these processes.The main focus is on NP-intermediate<br\/>problems,which are problems in NP that are neither NP-complete nor in P.<br\/>Intellectual Merit .One main focus of this research is the study of graph isomorphism,an NP-<br\/>intermediate problem that is a central problem in complexity theory.A many-particle random<br\/>walk process that may be particularly well-suited to solve this problem will be investigated.Since<br\/>the quantum dynamics of many-particle systems can often be e .ciently simulated on quantum<br\/>computers but not on classical computers,understanding the performance of the many-particle<br\/>random walk may yield new insight into inherently quantum algorithms for this problem.Speci .c<br\/>graphs known as strongly regular graphs (already shown to be useful to test critically algorithms<br\/>proposed for solving graph isomorphism)will be investigated in detail numerically to test the<br\/>many-particle quantum random walk algorithm,and fully characterize its complexity.In addition,<br\/>an e .ort will be made to make progress by characterizing the algebraic graph invariants that are<br\/>accessible to quantum computers.<br\/>Other related problems and methods will also be investigated,including integer factorization,<br\/>the discrete logarithm,and .nding short vectors in lattices.It will also be investigated whether<br\/>classical algorithms that depend on the birthday paradox can be used to devise new quantum<br\/>algorithms.<br\/>A complementary research thrust is to analyze the sensitivity of the multi-particle dynamical<br\/>algorithms to errors,focusing speci .cally on the scaling of errors as the number of particles increases,<br\/>and to understand the connection of the physical processes utilized by the algorithms to the physical<br\/>processes that actually occur at the hardware level and use this to improve the e .ciency of quantum<br\/>computations.<br\/>The goal of this research is to extend the limits of computation to qualitatively new problems.<br\/>For example,at present one cannot reliably test for the isomorphism of graphs with more than a<br\/>few hundred vertices.A quantum computer could,however,accurately compute the dynamics of<br\/>quantum particles on graphs of this size.If it is possible to use the information so obtained to test<br\/>isomorphism,this would be a signi .cant step forward in conquering complex problems.<br\/>Broader Impacts .The proposed work will have broad impact because of the insight it will yield<br\/>into hard computational problems.Further broad impact will be through the training of graduate<br\/>and undergraduate students with substantial interdisciplinary experience with both computer sci-<br\/>ence and physics.In addition to web-based dissemination of research results,an interdisciplinary<br\/>workshop will be held that will help improve communication between computer scientists and<br\/>physicists working on problems of common interest.","title":"QnTM: Physically-inspired Quantum Algorithms for NP-intermediate Problems","awardID":"0523680","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":["505142","321051","366659","512529","565082"],"PO":["565157"]},"102168":{"abstract":"Abstract<br\/><br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: Collaborative Research: WORKIT: A Universal Wireless Open Research KIT <br\/><br\/>Lead Proposal: CNS 0454288<br\/>PI: Henning Schulzrinne<br\/>Institution: Columbia University <br\/><br\/>Proposal CNS 0453830<br\/>PI: Thomas LaPorta<br\/>Institution: Pennsylvania State Univ University Park <br\/><br\/>Proposal CNS 0454329<br\/>PI: Elizabeth M. Belding-Royer<br\/>Institution: University of California-Santa Barbara <br\/><br\/>Proposal CNS 0454174<br\/>PI: Scott C. MIller<br\/>Institution: Lucent Technologies, Bell Labs <br\/><br\/><br\/>This project addresses the need for wireless network tools and platforms as recommended in the 2003 NSF Wireless Network Workshop report. The project will build on the IOTA (Integration of Two Access Technologies) project at Bell Labs. The PI's will enhance and develop IOTA for a software and systems package in a distributable form called the Wireless Open Research Kit (WORKIT). WORKIT will include source code and documentation and also be embodied in low-cost off the shelf hardware. WORKIT will be an enabler for research in mobility management, interlayer awareness, software algorithms for optimal network selection, reconfiguration, security, accounting, authentication, policy download and enforcement, and hybrid wireless networking. Broader impacts of this project include use of WORKIT in education and enabling stronger university\/industry collaborations in this area of emerging importance.<br\/>at colleges and universities.","title":"CRI: Collaborative Research: WORKIT: A Universal Wireless Open Research KIT","awardID":"0454329","effectiveDate":"2005-08-15","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["560332"],"PO":["434241"]},"107889":{"abstract":"Proposal: 0524854<br\/><br\/>Title: CT-ISG Collaborative Research: DNS Security Revisited: Enabling Cryptographic Defenses in Large-Scale Distributed Systems<br\/><br\/>PIs: Lixia Zhang (UCLA), Songwu Lu (UCLA), and Dan Massey (Colorado <br\/>State)<br\/><br\/>The Domain Name System (DNS) is a core Internet protocol and virtually all Internet applications rely on some form of DNS data. This project is identifying and addressing fundamental technical challenges in deploying the DNS Security Extensions (DNSSEC) in the global Internet. DNSSEC aims at enhancing DNS with data origin authentication and data integrity checking by applying well defined cryptographic solutions, however a number of system issues have arisen in the process of moving the cryptographic solution to real deployment. This project is first conducting a systematic assessment of the gap between the DNSSEC specification and the deployment constraints. For each identified technical challenge, the project is proposing, implementing, and evaluating specific solutions and then integrating such solutions into a unified design improvement.<br\/><br\/>DNSSEC deployment is critical to enhanced security in cyberspace, and this effort will help move it forward by overcoming existing roadblocks, foreseeing new obstacles on the road, and developing enabling techniques to clear these obstacles. The project will also extrapolate a set of lessons and principles on major challenges in deploying cryptographic protection in large scale systems, which will hopefully provide input into other cryptographic deployment effort, such as the global routing system.","title":"CT-ISG: Collaborative Research: DNS Security Revisited: Enabling Cryptographic Defenses in Large-Scale Distributed Systems","awardID":"0524854","effectiveDate":"2005-08-15","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["543560","451118"],"PO":["521752"]},"107669":{"abstract":"NSF 0523243 <br\/><br\/>CT-ISG: Designing Next-Generation, Reliable Internet Servers <br\/><br\/>Eugene H. Spafford<br\/><br\/>Computing systems, and especially software on critical servers, are constantly becoming more complex. This addition of features has contributed to greater complexity and to the addition of significant, ongoing vulnerabilities. In part, this may be traced to design decisions that have depended upon adding software for new functions rather than adding additional hardware to support greater separation and enhanced security. The falling cost of IT hardware suggests that this latter approach may be more cost-effective for secure system development. <br\/><br\/>The focus of this research is to investigate the construction of a complex computer server system from simpler, separate computer systems. The effort will explore how to apply well-known (but seldom used) security engineering principles coupled with newer design features to produce highly-secured components. The research will also develop metrics that will allow objective comparison of the vulnerabilities and benefits of the resultant system against more conventional architectures. <br\/><br\/>The expected outcomes of this project will be: 1) validation or disproval of the value of a number of security engineering techniques; 2) a demonstration prototype of a server that should be more secure against attack than are more conventional systems; 3) an approach to designing objective metrics for measuring server vulnerabilities; 4) materials to enable use of the prototype in educational settings.","title":"CT-ISG: Designing Next-Generation, Reliable Internet Servers","awardID":"0523243","effectiveDate":"2005-08-01","expirationDate":"2009-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"S097","name":"NIST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["522969"],"PO":["529429"]},"108527":{"abstract":"Abstract<br\/>The difficulty of creating digital models is limiting productivity in computer graphics applications, and is impeding the use of computer graphics in promising new areas of analysis and education. This interdisciplinary research project explores the development of next-generation digital modeling tools based on geometric harmonic analysis, a promising new area in applied mathematics. Novel contributions of this work include the modeling of the interaction of material appearance and object shape; new methods to<br\/>organize object and material features; and powerful controls for defining well controlled complex shapes. The application of geometric harmonic analysis to data sets and problems in computer graphics provides opportunity to spur new developments in this growing area of applied mathematics. Improved methods for 3D digital content creation have the potential to improve productivity in industrial applications, such as consumer product design, training simulations, feature film animation and computer games, and to facilitate the application of visual simulation and analysis to emerging application areas, such as cultural heritage study and education.<br\/><br\/>Specifically, geometric harmonic analysis techniques are applied to object descriptions obtained by 3D scanning and digital photography, and by numerical simulation of complex phenomena. The goal is to identify relevant features in the object data that relate the geometry with the material properties. Different aspects of physical objects -- shape features, variation of surface properties with shape, small-scale structures and<br\/>reflectances -- are separated and organized in a natural way to repurpose existing data as starting point for new design. Initial experiments have identified promising data sets for examining material and geometry<br\/>relationships, and have demonstrated how geometric harmonic analysis tools enable creation of a rich set of shapes with little user input. Multiple types of data sets and analysis are examined and evaluated for their<br\/>usefulness in building a comprehensible system for digital content creation.","title":"MSPA-MCS: Geometric Harmonic Analysis for 3D Digital Content Creation","awardID":"0528204","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["545794","541919","548698"],"PO":["532791"]},"106019":{"abstract":"Over the past decade, iterative decoding methods have received a great deal of interest due to the astonishing error performances achieved first by turbo codes, and more recently by low-density parity check (LDPC)<br\/>codes. The importance of these methods can be best realized by the fast integration of turbo codes in several standards and last year, a binary LDPC code was first selected as a standard. Although this last decision<br\/>clearly indicates maturity, several issues remain problematic in the implementation of LDPC codes, especially for moderate lengths, which are required in many communications systems. In fact for these lengths, non binary LDPC codes outperform their binary counterparts, but iterative decoding of non binary LDPC codes does not scale well with the size of the finite field used. Furthermore, most standards still contain error control coding schemes based on Reed-Solomon (RS) codes. Although these codes are very powerful and have been used for decades, there still exists a large gap between the best achievable performance and that achieved in commercial products. This research activities address problems related to both classes of non binary codes and can therefore be divided into two major areas: (1) Near-optimum decoding of Reed-Solomon codes; and (2) Reduced complexity decoding of non binary LDPC codes.<br\/><br\/>Reliability based decoding of RS codes using their binary image has been shown to provide promising results at error rates that can be simulated. However several approaches (especially those based on iterative<br\/>techniques) RS codes since they are often designed for very low error rates. This research investigates the development of a new reliability based decoding technique which outperforms all previously proposed ones<br\/>for RS codes over GF(256). A tight performance analysis of this new approach for any SNR value is also possible. LDPC codes designed over GF(q) and decoded with the belief propagation (BP) algorithm have been<br\/>shown to perform better as q increases, but at the expense of an O(q log(q)) increase in complexity. Furthermore the BP algorithm is often too complex for fast VLSI implementations. For q=2, very efficient reduced complexity versions of the BP algorithm have been proposed with negligible performance degradation. However as q increases, the complexity of these approaches increases in O(q^2) and the performance gap with BP also increases with q. The research involves the development of new reduced<br\/>complexity versions of the BP algorithm over GF(q) which keep all advantages obtained for q=2, but with much lower complexity than existing algorithms.","title":"Near-Optimum Soft Decision Decoding of Non-Binary Linear Codes","awardID":"0515154","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["328464","492224"],"PO":["432103"]},"109539":{"abstract":"Robust intelligence rests on the ability to reason about missing, incomplete, ambiguous and corrupted data. This is particularly true in visual perception, where an intelligent system is faced with reasoning about the complexity of a changing three-dimensional world given only two-dimensional images. Bayesian inference has become popular for dealing with such problems because it provides a sound way of combining ambiguous sensor measurements with prior knowledge about the world. Priors represent the collected experience of a perceptual system and by integrating heterogeneous sources of information in a statistically sound way enable such a system to respond robustly to novel situations.<br\/><br\/>Markov random fields (MRFs) provide a powerful and popular formalism for representing visual priors. However, they have typically modeled only local, pairwise pixel interactions, which limit their modeling capabilities. This project aims at increasing the power and applicability of these models using larger pixel neighborhoods (cliques). The proposed Fields-of-Experts (FoE) model generalizes many previous MRF models, and all its parameters can be learned from real-world training data. Preliminary experiments have shown that, for example, image reconstruction applications benefit from such richer visual priors, but many other application domains have remained unexplored. The development of these statistical modeling tools will also have an impact on other domains outside of machine vision where the need for modeling complex, high-dimensional data arises. Finally, the dissemination of the collected experimental data, learned models, and software promises to stimulate research and make possible quantitative comparisons towards better statistical models of the visual world.","title":"Learning Rich Statistical Models of the Visual World for Robust Perception","awardID":"0535075","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7339","name":"COMPUTER VISION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["498150"],"PO":["564316"]},"102521":{"abstract":"This demonstration project will identify barriers and develop alternative incentive mechanisms for data producers to deposit archive-ready data sets in an archive. Archive-ready means data sets that meet an archive's deposit requirements and submission guidelines. Archives rely heavily on data producers to provide complete and accurate documentation when they deposit data and to comply with other requirements, such as file structures and formats, transfer media, and requirements for protection of privacy and confidentiality. The entire enterprise of digital archiving assumes some degree of cooperation between producers of digital information and archives. When data producers do not comply with submission guidelines, archives incur additional costs in preparing the data for preservation and dissemination, experience delays between ingest and release, and assume risks if data that do not meet quality assurance standards are released. The research literature and years of experience with data deposits at the Inter-university Consortium for Political and Social Research (ICPSR) indicate that, as a rule, current incentives are insufficient to overcome the obstacles that data producers report. This multi-disciplinary team of experts in digital archiving, social science research, and experimental economics and ICPSR will investigate ways to increase cooperation between producers and archives. With a government partner, the National Institute of Justice, the team will use multiple methods (surveys and experiments) to identify barriers to compliance, revise guidelines and<br\/>responsibilities, and develop and test alternative incentive mechanisms.<br\/><br\/>Intellectual Merit<br\/><br\/>This award will examine legal, social and economic impediments to archiving by analyzing problems that producers encounter when preparing data for deposit and by developing and field testing alternative incentive mechanisms. The project will enhance understanding of curatorial processes and work flow, especially when work is distributed between the data creator and the archive. It will develop metrics for the contributions expected of data producers. Contributions will also be made to our understanding of incentive mechanisms for public goods.<br\/><br\/>Broader Impacts<br\/><br\/>The results of research with social scientists will produce better models for sharing responsibility for archiving between data producers and archives. The models and incentive mechanisms will have broad applicability to other types of data producers who are mandated to deposit data by funding agencies or as a condition of publication. Ultimately, more cooperation between data producers and archives will reduce the costs of archiving, accelerate the release of data, and improve its quality. The research will also inform development of standards and guidelines for producer-archive relationships.","title":"Incentives for Data Producers to Create Archive-Ready Data Sets","awardID":"0456022","effectiveDate":"2005-08-15","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1706","name":"DIGITAL GOVERNMENT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["424319","477110","471467"],"PO":["371077"]},"105821":{"abstract":"Current state-of-the-art algorithms that process images for human use<br\/>treat images as signals; these techniques are successful because of both advanced signal processing techniques and signal-processing based human visual system (HVS) models. Many applications, however are better approached with a higher-level view of images, considering color, structure, or even object content. No unified model of either images or the HVS processing exists at this higher-level view; such a model can substantially advance the current state-of-the-art in image processing. This research involves developing such a model and incorporating it into practical algorithms such as low-rate compression, low-rate facial compression allowing recognition, and medical imaging.<br\/><br\/>The human visual system initially performs low-level signal analysis and ultimately ends in cognition. Many models of the signal analysis stage which predict responses to simple stimuli, but these models fail for natural images because humans' higher-order processing of structure produces cognitive effects which cannot be modeled using only responses to simple stimuli. Cognition is thought to gradually occur during input processing; no single portion of the HVS is singularly responsible for recognition or transformation of the internal representation to an abstract concept. The continuum of visual processing from signal analysis to cognition suggests that signal-processing-based modeling of the HVS can be extended to include some structural processing. This research develops such models through three psychophysical experiments. The first is a controlled masking study allowing development of a visual masking model incorporating structure. The second rates observers' willingness to accept distortions, relating acceptance with detection results from the first study. The third compares recognition times for images represented using structural and signal-based representations, quantifying the use of structure in cognitive tasks.","title":"A Signal Processing Approach to Modeling Visual Masking","awardID":"0514311","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["530523"],"PO":["564898"]},"104732":{"abstract":"As the scope of available information grows, it is increasingly<br\/>difficult to find what one needs when it is needed. The growth of<br\/>search as an industry is testament to this problem, but existing tools<br\/>are incomplete solutions. They index content, but not context. They<br\/>capture only static, syntactic relationships, not dynamic, semantic<br\/>ones. Perhaps most importantly, they have access only to information<br\/>stored either on a user's local machine or publicly-accessible<br\/>distributed stores---other sources are neither indexed nor searchable.<br\/>Information Pedigree solves this problem by capturing context and<br\/>semantic relationships between information items, collecting disparate<br\/>relationship meta-data to support searches, and providing for<br\/>availability of relevant items created on machines controlled by<br\/>individual users. Information Pedigree achieves this goal by<br\/>instrumenting the flow of information between a user's applications<br\/>and services, as well as the flow of information between different<br\/>users. By observing these transactions, we form a contextual and<br\/>semantic index over data that is used, simplifying its retrieval for<br\/>later use.<br\/><br\/>Information Pedigree will make it easier for people to find and share<br\/>data, without placing additional burdens on them or their IT support<br\/>staff. This greatly reduces time wasted in managing information,<br\/>allowing information workers to be more productive. The results from<br\/>this project will be widely disseminated in the research literature,<br\/>and a prototype of the system will be made available to other<br\/>researchers as well as users. This project will train several<br\/>graduate students, and influence the curricula of several graduate and<br\/>undergraduate courses.","title":"CSR---PDOS: Information Pedigree","awardID":"0509089","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["530391"],"PO":["493916"]},"104754":{"abstract":"Large scale parallel systems are critical to our computational infrastructure to take on the challenges imposed by applications whose scale and demands exceed the capabilities of machines available in the market today. Pushing the limits of hardware and software technologies to extract the maximum performance, in turn, exacerbates other problems. Notable amongst these problems is the susceptibility to failures, which arises as a consequence of growing hardware transient errors, hardware device failures, software complexity, and the complex hardware\/software inter-dependencies between the nodes of a parallel system. These failures can have substantial consequences on system performance, in addition to impacting the costs of maintenance\/operation, thereby putting at risk the very motivation behind deploying these large scale systems.<br\/><br\/>This research is expected to make three broad contributions towards developing a runtime infrastructure, called PROGNOSIS, for failure data collection and online analysis. The first set of contributions will be on collecting and analyzing system events and failure data from an actual BlueGene\/L system over an extended period of time. In addition to presenting the raw system events, the research will be developing filtering techniques to remove unimportant information and identifying stationary intervals, together with defining the attributes for logging and their frequency. The second set of contributions will be models for online analysis and prediction of evolving failure data by exploiting correlations between system events over time, across the nodes, and with respect to external factors such as imposed workload and operating temperature. The third set of contributions will be on demonstrating the uses of PROGNOSIS. Tools such as PROGNOSIS can help substantially in the development of self-healing systems, which has been noted to be an important goal in the emerging area of Autonomic Computing by several computer vendors.","title":"Collaborative Research: CSR---SMA+AES: PROGNOSIS to Enhance the Runtime Health of Large Scale Parallel Systems","awardID":"0509164","effectiveDate":"2005-08-01","expirationDate":"2006-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["564747"],"PO":["301532"]},"104776":{"abstract":"Clusters have emerged as the most cost-effective solution to<br\/>design high performance and dependable data centers,<br\/>which are increasingly being deployed <br\/>for a wide variety of Web-based services.<br\/>However, data centers contribute to a significant part of the<br\/>overall delay, which is likely to grow with the increasing use<br\/>of dynamic Web contents. Furthermore, (24x7) server availability<br\/>to host critical services appears to be a far-fetched objective. <br\/>Therefore, design of high performance and dependable data centers<br\/>has become a critical issue.<br\/><br\/>The objective of this research is to investigate the design of a three-tier<br\/>data center consisting of a front-end Web server, a middle-tier application<br\/>server, and a back-end database server using the industry standard, <br\/>InfiniBand Architecture (IBA) communication paradigm.<br\/>A prototype data center will be implemented on a 96-node<br\/>IBA-connected Linux cluster to investigate three research issues<br\/>using real applications and benchmarks. <br\/>The issues are quantifying the performance benefits of<br\/>using a user-level communication technique such as IBA instead of<br\/>the standard TCP\/IP stack, proposing new performance enhancement<br\/>techniques for data centers, and characterizing the impact of different<br\/>faults on server dependability.<br\/><br\/>The research will enable effectively utilizing the IBA communication mechanism<br\/>for designing multi-tier data centers, quantifying the end-to-end performance<br\/>benefits of using IBA, developing fault-tolerant mechanisms for highly available<br\/>data centers, and understanding the design tradeoffs between high performance<br\/>and dependability.<br\/>In addition to fostering new research directions in<br\/>several areas of data center design, the outcome of this research<br\/>is likely to have significant commercial interests.","title":"CSR--PDOS: Exploring Cluster-Based Data Center Design Space for High Performance and Dependability","awardID":"0509251","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["528208","550859"],"PO":["535244"]},"105645":{"abstract":"The Earth System Curator collaboration will unify the treatment of models and datasets relating to climate change by developing a common language - a metadata formalism - with which to describe the two, and by prototyping a set of tools based on that formalism that allows researchers to manipulate models and datasets seamlessly and with ease. The goal, in the end, is to increase the productivity of climate researchers and understanding of the Earth system.<br\/><br\/>Collaborators include computer science and Earth science researchers at MIT, Princeton University, the Georgia Institute of Technology, and the National Center for Atmospheric Research. The work proposed builds on two ongoing community efforts, the Earth System Modeling Framework (ESMF) and the Earth System Grid II (ESG). The ESMF is a national initiative to develop common modeling infrastructure for the nation's climate and weather models, including coupling tools and standard modeling utilities. The primary objective of ESG is to make the output of high-resolution, long-duration simulations performed with climate models available to global change impacts researchers nationwide, through the use of Grid, data\/metadata, and portal technologies. The team will explore those aspects of ESMF and ESG that can be usefully aligned, and will prototype a new entity, the Earth System Curator, that spans the gap between the two.<br\/><br\/>The Curator begins with a crucial insight: that the descriptors used for comprehensively specifying a model configuration are needed for a scientifically useful description of the model output data as well. The development of a common metadata schema that describes both will be the basis for this unique and powerful community resource. The Curator will provide a community database from which researchers can archive and query a wide class of Earth system models, experiments, model components, and model output data and results. Researchers will subsequently be able either to analyze model output from pre-existing runs, or to access a model and modify and run it themselves, either on a local computer or on the virtualized resources of the computational Grid. In addition to the query function, the project will prototype a tool that will test if sets of model components or datasets can interact to form an application. Finally, as part of the Curator effort, tools for auto-generation of component wrappers and applications will also be prototyped.<br\/><br\/>The Curator is part of a community vision for the use of information technology in climate and related research. The Curator prototype will help to further suggest and define the form of next-generation modeling and data management tools, by offering a concrete representation to add credibility to innovative ideas. Further, the ESMF and ESG co-investigators, by virtue of their projects' emphases on production software and extensive customer bases, are in an excellent position to transition the Curator tools into a viable product following an NSF-funded prototype stage. Over the course of the Curator effort, many researchers associated with ESMF and ESG will be encouraged to try out and offer feedback on the Curator software. Advances achieved with the Curator project will influence other domains through conferences and publications, and through a web of relationships founded on a shared need for multi-component HPC modeling, ease of information archival and access, and similarities in simulation numerics. While the advances in climate prediction due directly to the Curator effort itself may be both difficult to track and modest, an integrated environment for Earth system research is critical to addressing world climate issues in the near future, and the Curator is a definitive step in that direction.<br\/><br\/>The Curator will also be incorporated into the project infrastructure for software engineering courses at Georgia Tech. Appropriate introductory material will be prepared and project opportunities defined. Many of the project opportunities will take the form of making climate data accessible to the general public. In this way the Curator not only provides software engineering students an opportunity to participate in an actual ongoing engineering development effort, but the resultant projects will make Earth science more available to the general public.","title":"Collaborative Research: Earth System Curator: Spanning the Gap Between Models and Datasets","awardID":"0513457","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7373","name":"ITR-INFORMATION INTEGRATION"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"7231","name":"CYBERINFRASTRUCTURE"}}],"PIcoPI":["558026","521815"],"PO":["565136"]},"116777":{"abstract":"ABSTRACT<br\/><br\/>CCF- 0347683<br\/>Cameron, Kirk<br\/>University South Carolina Research Foundation<br\/><br\/><br\/> Intellectual Merit: Computer models of complex phenomena deepen our understanding of the universe and indirectly improve our quality of life. Large-scale parallel and distributed systems will be used to meet the computational demands of distributed simulations. If application performance efficiencies do not improve, the performance using thousands of power hungry components will lead to intolerable operating costs and failure rates. Fundamental technologies will be difficult. Years of intense research aimed at improving the performance of applications in parallel and distributed systems have led to average efficiencies of 5-10%. The continued exponential increase in complexity makes maintaining these efficiencies through tuning challenging. Improving efficiency dramatically will require innovation<br\/><br\/>Broader Impact: Completion of our research plan will create technologies that improve the performance of distributed simulations generally. This impacts a broad range of disciplines that perform simulation-based experimentation including computational physics, biology and chemistry. Reducing power consumption of large-scale applications will reduce operational costs for computational center, increase system reliability through decreased heat emissions, and impact the environment indirectly through energy conservation. Completion of our educational plan will elevate the profile of power-performance considerations in high performance computing. Recruitment and mentoring of students will produce graduates with marketable skill sets. The integration of developed technologies including research discoveries and tools into educational curriculum at the University of South Carolina (USC) will capture the interest of next generation computer scientist. Completion of this project will allow the PI to lay a solid foundation for continued contributions to the field of Computer Science. The objectives of this CAREER proposal are also in line with the institutional goals of USC, an EPSCORE designated State University, to promote research and education through <br\/>scholarship.","title":"CAREER: High-Performance, Power-Aware, Distributed Computing","awardID":"0614705","effectiveDate":"2005-08-16","expirationDate":"2009-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7329","name":"COMPILERS"}}],"PIcoPI":["563791"],"PO":["565272"]},"115589":{"abstract":"ABSTRACT<br\/>0438971<br\/>PI: David H. Lorenz<br\/>Co-PI's: Paul C. Attie, Dana H. Brooks<br\/>Northeastern University<br\/><br\/>SoD: Design Locality: A Concept for Controlling the Design Complexity of Large Software Systems<br\/><br\/>Large software systems are extremely difficult to design correctly. This difficulty stems largely from the exponential number of ways in which components can interact. This project's objective is to develop a design methodology for constraining interaction in large complex software systems. The central idea is design locality: synthesis, verification and analysis are applied to small subsystems; local properties are verified for subsystems in isolation and then combined to deduce global properties.<br\/><br\/>Our approach is ``design for verifiability'': design a large system so that verification is tractable. A mixed methodology will be developed: top-down for design, bottom-up for analysis and verification. A well-designed system can be decomposed top-down into small subsystems while its global properties follow bottom-up from subsystem properties.<br\/> <br\/>A goal is systems representation in \"pairwise normal form\": code that synchronizes a pair of components is cleanly separated from code that synchronizes other (even overlapping) pairs. Pairwise composition is a separation of concerns solution that facilitates modularity, modifiability, and maintenance, applicable to component assembly. Our test cases are aspect-oriented programming and the SCIRun software, which builds dataflow networks of components for integrated and interactive scientific computation and visualization.","title":"SoD: Design Locality: A Concept for Controlling the Design Complexity of Large Software Systems","awardID":"0609612","effectiveDate":"2005-08-31","expirationDate":"2008-11-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7372","name":"ITR-SCIENCE OF DESIGN"}}],"PIcoPI":["506512",306142,"540016"],"PO":["564388"]},"105678":{"abstract":"The Earth System Curator collaboration will unify the treatment of models and datasets relating to climate change by developing a common language - a metadata formalism - with which to describe the two, and by prototyping a set of tools based on that formalism that allows researchers to manipulate models and datasets seamlessly and with ease. The goal, in the end, is to increase the productivity of climate researchers and understanding of the Earth system.<br\/><br\/>Collaborators include computer science and Earth science researchers at MIT, Princeton University, the Georgia Institute of Technology, and the National Center for Atmospheric Research. The work proposed builds on two ongoing community efforts, the Earth System Modeling Framework (ESMF) and the Earth System Grid II (ESG). The ESMF is a national initiative to develop common modeling infrastructure for the nation's climate and weather models, including coupling tools and standard modeling utilities. The primary objective of ESG is to make the output of high-resolution, long-duration simulations performed with climate models available to global change impacts researchers nationwide, through the use of Grid, data\/metadata, and portal technologies. The team will explore those aspects of ESMF and ESG that can be usefully aligned, and will prototype a new entity, the Earth System Curator, that spans the gap between the two.<br\/><br\/>The Curator begins with a crucial insight: that the descriptors used for comprehensively specifying a model configuration are needed for a scientifically useful description of the model output data as well. The development of a common metadata schema that describes both will be the basis for this unique and powerful community resource. The Curator will provide a community database from which researchers can archive and query a wide class of Earth system models, experiments, model components, and model output data and results. Researchers will subsequently be able either to analyze model output from pre-existing runs, or to access a model and modify and run it themselves, either on a local computer or on the virtualized resources of the computational Grid. In addition to the query function, the project will prototype a tool that will test if sets of model components or datasets can interact to form an application. Finally, as part of the Curator effort, tools for auto-generation of component wrappers and applications will also be prototyped.<br\/><br\/>The Curator is part of a community vision for the use of information technology in climate and related research. The Curator prototype will help to further suggest and define the form of next-generation modeling and data management tools, by offering a concrete representation to add credibility to innovative ideas. Further, the ESMF and ESG co-investigators, by virtue of their projects' emphases on production software and extensive customer bases, are in an excellent position to transition the Curator tools into a viable product following an NSF-funded prototype stage. Over the course of the Curator effort, many researchers associated with ESMF and ESG will be encouraged to try out and offer feedback on the Curator software. Advances achieved with the Curator project will influence other domains through conferences and publications, and through a web of relationships founded on a shared need for multi-component HPC modeling, ease of information archival and access, and similarities in simulation numerics. While the advances in climate prediction due directly to the Curator effort itself may be both difficult to track and modest, an integrated environment for Earth system research is critical to addressing world climate issues in the near future, and the Curator is a definitive step in that direction.<br\/> <br\/>The Curator will also be incorporated into the project infrastructure for software engineering courses at Georgia Tech. Appropriate introductory material will be prepared and project opportunities defined. Many of the project opportunities will take the form of making climate data accessible to the general public. In this way the Curator not only provides software engineering students an opportunity to participate in an actual ongoing engineering development effort, but the resultant projects will make Earth science more available to the general public.","title":"Collaborative Research: Earth System Curator: Spanning the Gap Between Models and Datasets","awardID":"0513635","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7373","name":"ITR-INFORMATION INTEGRATION"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"7231","name":"CYBERINFRASTRUCTURE"}}],"PIcoPI":[278821,"397849"],"PO":["565136"]},"108715":{"abstract":"This proposal will support a collaboration between Boston University, CapWIN, and Harvard University to better understand how to deploy first responder applications that allow situational awareness and global<br\/>coordination in a dynamic wireless mobile environment. The specific objective of this project is to demonstrate an ability to deploy a wireless based sensor application within CAPWIN's infrastructure. A second objective is both to define the requirements and to propose a solution that would incorporate additional sensors including chemical or environmental detectors, into this wireless sensor application as well as to illustrate the exchange of this sensor data with other CAPWIN applications. Finally, the third objective will be to explore and develop a strength and weakness analysis of CAPWIN's current architecture and suggest a more scalable and robust architecture for the CAPWIN infrastructure.<br\/><br\/>Broader Impact<br\/><br\/>CAPWIN is one of the largest and most important deployments of an integrated wireless network in the public sector. Its operational success has demonstrated both the ability and value of providing transparent connectivity across a diverse set of first response organizations including police, fire and ambulance. CAPWIN also serves another vital purpose for the emergency response community - that of a platform for learning and organizational innovation. Based on the philosophy of open systems and a standards-based architecture, CAPWIN enables a mechanism or test bed to assess products and services that can lead to complex, process-based innovations. In short, it is an organizational platform that allows a timely assessment and, if<br\/>proven valuable, a rapid integration of an emerging product or service into operations. Most importantly, CAPWIN can provide this integrative capability without imposing narrow restrictions on any given user organization or product innovator.<br\/><br\/>Intellectual Merit<br\/><br\/>The academic research of the above project will: (1) explore the value of the CAPWIN infrastructure as a test-bed for evaluating the strengths and weaknesses of emerging technology (such as wireless sensors) when it is deployed in highly dynamic and distributed application environments (such as Emergency Services); (2) provide a specific evaluation of the use of a Mote based wireless sensor network as a means to track and monitor the conditions of individuals or the environment during an emergency response event; (3) assess lessons learned and emerging design principles of using the CAPWIN infrastructure in the context of: (a) how to enhance the innovation of services to the first responder community, (b) how to deploy, evolve, and then determine the likely benefits of a wireless sensor network for tracking and monitoring in a dynamic and distributed context; (4) investigate the advantages of a distributed, scalable, and robust architecture for CAPWIN infrastructure at both the sensor network layer and the application transport layer.","title":"SGER: The CapWIN project - Deployment of Dynamic Collaborative Sensor Networks","awardID":"0529798","effectiveDate":"2005-08-01","expirationDate":"2006-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1706","name":"DIGITAL GOVERNMENT"}}],"PIcoPI":[287458],"PO":["371077"]},"107879":{"abstract":"Quantum information processing (QIP) uses quantum resources-quantum systems, unitary evolutions, and measurement-to do information processing tasks. Quantum phenomena, such as superposition, interference, and entanglement, make possible protocols that are difficult or impossible using classical resources. A better understanding of measurement and entanglement, therefore, can be expected to yield a better understanding of existing QIP protocols, and hopefully lead to the development of new protocols as well. This proposal approaches this by studying sequences of weak measurements, and the behavior of quantum systems under them.<br\/><br\/>In quantum mechanics, systems evolve in time by two very different processes: by unitary evolution according to the Schrdinger equation, and by measurement. Unitary evolution is continuous, reversible, and deterministic; it is the evolution that quantum systems undergo when they are not observed. By contrast, measurement (in its usual form) is discontinuous, irreversible, and random. A measurement provides some information about the state of a quantum system; but at the same time, it disturbs the state of the system. There is a close relationship between acquiring information and disturbing the system; if a measurement yields a certain amount of information, it must disturb the state by at least a certain amount.<br\/><br\/>A more recent idea is that of a weak measurement: a measurement that disturbs the system only slightly, but provides only a very small amount of information. By repeatedly doing weak measurements, more and more information can be accumulated (and the disturbance grows progressively greater and greater). In fact, the PI has recently shown that any measurement can be decomposed into a sequence of weak measurements, in a way that has the structure of a random walk: the state of the system shifts randomly back and forth towards the possible outcomes of the measurement, and at long times is guaranteed to approach one or another of the outcomes with a given probability. In the limit, this is like a diffusion process, with the state diffusing continuously (but randomly) along a curve in the space of all possible states.<br\/><br\/>Using this technique, it is possible to make continuous processes that previously were discrete. This means that the techniques of differential calculus can be brought to bear on certain outstanding problems in quantum information processing. One very promising area is entanglement. Entanglement is a type of quantum correlation, which is stronger (in certain ways) than any classical correlation; it is a resource for a number of quantum protocols, such as teleportation and dense coding. For this reason, there has been a great deal of interest in finding good quantitative measures of entanglement. This problem is largely solved for one class of systems (bipartite pure states); but for others, little is known. An idea that has proven very fruitful is that of an entanglement monotone: a function of the state that always decreases on average under purely local operations. It has been difficult to investigate these quantities systematically; using weak measurement decompositions, one can find differential conditions for monotones, and open up a brand new avenue to the problem of entanglement.<br\/><br\/>In addition to the classical random walks that occur in these measurement procedures, there are purely quantum analogues of random walks, called quantum walks. Unlike the random walks, these are purely unitary evolutions, which are currently of great interest as possibly leading to new types of quantum algorithms. This project will also study quantum walks on graphs, with particular emphasis on the effects of decoherence (quantum noise) and other imperfections, to assess how well such new algorithms might be expected to perform under realistic conditions.","title":"QnTM: Weak Local Measurements, Entanglement Monotones, and Random Walks","awardID":"0524822","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":["381587"],"PO":["565223"]},"108528":{"abstract":"Digital scanning devices are capable of acquiring high-resolution 3D models and have recently become affordable and commercially available. Modeling detailed 3D shapes by scanning real physical models is becoming commonplace. Current scanners are able to produce very large amounts of raw, dense point sets; consequently, there has been a recent increase in the need for techniques for processing point sets. One of the principal challenges faced today is the development of surface reconstruction techniques that deal with the inherent noise of the acquired dataset. When the underlying surface contains sharp features, the requirement of being resilient to noise is especially challenging, since noise and sharp features are ambiguous, and most existing techniques tend to smooth important features or even to amplify noisy samples. The project helps to train young researchers to work at the intersection of graphics, geometry, and statistics, while enabling them to pursue theoretically sound work that has deep practical impact. The data, software, and models developed in this project will be disseminated for other researchers to use in benchmarking and testing.<br\/><br\/>This research involves producing efficient and theoretically sound techniques for robust, feature-preserving surface reconstruction. It builds on recent work on the construction of a manifold surface from a set of points by using a moving least-squares (MLS) technique. The project explores the use of robust statistical techniques arising from outlier identification in MLS-based surface reconstruction. The approach is related to recent developments in feature-preserving smoothing, but it defines a surface rather than filtering the geometry. The techniques not only point to more reliable MLS projection but also extend the representation power of the underlying MLS surface definition to enable the representation of objects with sharp features.","title":"MSPA-MCS: Collaborative Research: New Methods for Robust, Feature-Preserving Surface Reconstruction","awardID":"0528209","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7454","name":"MSPA-INTERDISCIPLINARY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":["471875"],"PO":["550859"]},"110574":{"abstract":"The vast majority of computer vision techniques continue to be predicated on restrictive assumptions about reflectance, and their ability to extract meaningful information from images of complex reflecting scenes remains limited. The proposed research activity works toward a framework for the analysis of complex reflecting scenes through the decomposition of reflectance. According to this approach, a reduced representation of an image is obtained on a point-by-point basis by its decomposition into simpler constituents; and this representation provides access to scene information (e.g., shape, illumination, material properties) that would be otherwise inaccessible. The distinguishing feature of this research is that instead of directly recovering complete estimates of reflection components, preliminary reduced representations will be sought, which isolate diffuse reflection effects in some usable form. This makes the problem tractable and enables the development of a robust and general framework for the analysis of complex reflecting scenes. The specific goals of the proposed research are: (1) Reduced representations that isolate the much simpler diffuse reflection effects in scenes with variable and complex illumination; and (2) Methods for dense, region-based tracking that apply the reduced representations.<br\/><br\/>Improvements in visual tracking will facilitate robust navigation systems and human-computer interfaces. Enhanced recognition systems will benefit systems for visual inspection, surveillance and homeland security (e.g., face recognition). Improvements in 3D reconstruction techniques will enhance a system's ability to learn appearance models of objects from their images, thereby enabling the system to predict the appearance of these objects in novel environments. Under this grant, the PI will also develop curricula for two undergraduate courses (at the freshmen and senior levels) at Harvard University. The freshman-level course will be designed to attract students from underrepresented groups into engineering and computer science by exposing them to intuitive and exciting computational aspects of visual understanding.","title":"SGER: Decomposing Reflectance for Vision-based Tracking","awardID":"0541173","effectiveDate":"2005-08-15","expirationDate":"2007-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7339","name":"COMPUTER VISION"}}],"PIcoPI":["515747"],"PO":["317663"]},"110695":{"abstract":"Rescue robots are equipped with a variety of sensors to enable them to search through a post-disaster environment to locate survivors. Video cameras and proximity sensors are used to find a path through the environment, while audio, heat, and CO2 sensors are used both to locate survivors and determine their condition. This means that robot operators need to integrate sensor data in order to navigate the space successfully and to determine the condition of survivors. This research investigates the use of a torso-mounted vibrotactile display to convey some of the sensor information to the operator. For example, a tactile cue can be given in the direction of an interesting temperature reading. As the operator turns the robot toward the source, the vibrotactile cue will move around the torso until it is centered in front of the operator, which can in turn provide a direction vector for the robot to follow in seeking the heat source. The goal of this project is to empirically determine the best ways to used vibrotactile feedback in the control of rescue robots. The intellectual merit of the work lies in its novelty and difficulty. Past applications of haptics have centered on force feedback for manipulators, or using haptic devices as controllers. In contrast, in this project the PI plans to use a vibrotactile display to improve the operator's situational awareness. Specifically, questions such as which sensor data is best suited to haptic display, how to map the data to tactors, and how effectively does the haptic display convey information to the operator, must be answered empirically.<br\/> <br\/>Broader Impacts: Answers to questions such as those enumerated above will have broad impacts that extend beyond the application to rescue robots, to encompass any use of wearable haptics to enhance situational awareness. For example, researchers have considered haptic displays as a means of providing wayfinding information to the blind, particularly to the blind and deaf, or to the blind with limited mobility such as being confined to a wheel chair.","title":"SGER: Vibrotactile Feedback for Enhanced Control of Mobile Robots","awardID":"0541718","effectiveDate":"2005-08-15","expirationDate":"2006-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6846","name":"UNIVERSAL ACCESS"}}],"PIcoPI":["398322"],"PO":["565227"]},"111795":{"abstract":"Although service-oriented environments and technologies are receiving a great deal of attention in both academic and corporate arenas, the ability to automate the discovery and composition of such services into higher-level capabilities remains elusive. This is funding to support an international web service discovery and composition competition, the WS-Challenge, whose objective is to identify, evaluate and baseline approaches to solving that problem. Building on the success of the first competition (The EEE05 Challenge, which was held this past March in Hong Kong), the PI will organize the planning and enactment of the second competition to be held in conjunction with the 2006 IEEE International Conference on e-Technology, e-Commerce, and e-Services (EEE 06) which will take place in San Francisco next July. The first competition was limited to syntactical matching using the Web Services Description Language (WSDL); participants were required to identify and compose WSDL-specified services based on their input and output messages as specified in a directory of WSDL documents. In 2006, the competition will focus on the semantic linking of web services using such technologies as the Resource Description Framework (RDF) and\/or the Web Ontology Language for Services (OWL-S). NSF funding will support participation in the competition of up to 8 teams from the United States, consisting on average of 3 students apiece. A small portion of the funds is earmarked for two undergraduate students who will support the competition throughout the year, as well as to cover the cost of travel by them and the PI to the competition.<br\/><br\/>The IEEE International Conferences on e-Technology, e-Commerce and e-Service bring together researchers and developers from diverse areas of computing. The conferences provide a venue for developers and practitioners to explore and address challenging research issues surrounding e-technology, in order to develop a common research agenda and vision for e-commerce and e-business. The focus is two-fold: to investigate enabling technologies to facilitate next generation e-transformation; and to disseminate application and deployment experience in e-themes such as e-business, e-learning, e-government, e-finance, etc. The EEE conferences are particularly timely and relevant because they focus on the application of electronic services as they cut across several \"e-domains\", whereas other conferences tend to either focus on a specific technology or on a single domain.<br\/><br\/>Broader Impacts: The workshop and resulting artifacts will serve as a centralized repository of algorithms, software, and techniques in a timely emerging area. The workshop will provide participants with an opportunity to gain exposure in the community for their innovative work, and to obtain feedback and guidance from senior members of the research community. It will further help foster a sense of community among these young researchers, by allowing them to create a social network both among themselves and with senior researchers at a critical stage in their professional development. The workshop experience will integrate well with the goals of a software engineering education, as participants are evaluated on their design in addition to the performance of their approaches. In an effort to engage a broader audience in next year's event, the PI will make a special effort to solicit teams from under-represented universities.","title":"WORKSHOP: A New Web Service Discovery and Composition Competition (EEE '06); July, 2006; San Francisco, CA","awardID":"0548514","effectiveDate":"2005-08-15","expirationDate":"2007-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6846","name":"UNIVERSAL ACCESS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6850","name":"DIGITAL SOCIETY&TECHNOLOGIES"}}],"PIcoPI":["444191"],"PO":["565227"]},"110387":{"abstract":"The Internet has become an indispensable infrastructure for our economy, society, and government. However, despite its critical importance, today's Internet is extremely fragile and suffers from frequent attacks. One of the main reasons for the security vulnerabilities of today's Internet is that the Internet protocols and architecture were designed for a trustworthy environment. This assumption is clearly no longer valid in today's Internet, connecting millions of people, computers, and corporations distributed throughout the world. Many researchers have studied how to secure the Internet, mostly by proposing patches to address current vulnerabilities. However, partial solutions and ad hoc mechanisms often do not address the root cause of the problems, and hence will not be able to eradicate the current problems and prevent them from manifesting in different forms in the future. Moreover, security patches and ad hoc security solutions increase network complexity, which in turn increases vulnerability. Thus, we need a radical new design for a next-generation Internet, which is designed ground-up from sound principles. <br\/><br\/>Intellectual Merit: The principal investigators (PIs) plan a series of efforts to engage the community in systematically exploring this important question of how to provide the next-generation Internet with a set of fundamental security design principles and mechanisms that will provide the next generation Internet with provable security guarantees. One part of the planning effort is one or more workshops. Starting from a clean-slate approach, participants in the workshops will investigate the fundamental issues in designing a secure next generation Internet, which not only removes many of the current security problems but also provides provable security guarantees against unforeseen future attacks. The deployment of a next-generation secure Internet will likely start out as a research test bed. Such an infrastructure will attract applications requiring high-assurance, resulting in a transition of hosts to the secure Internet. The success of such a test bed critically depends on the collaboration of researchers in networking, architecture, and security. A distributed effort with many small projects is unlikely to have the same impact as a coordinated collaborative research effort. This planning effort will provide coordination of research efforts and establishing community consensus for promising research directions.<br\/><br\/>Broader Impacts. This effort will involve the networking and security communities to establish a consensus on what security properties to provide in the network, and to establish promising research directions for designing a secure next-generation secure Internet. Included in this effort is a workshop with 30-40 networking and security experts from industry and academia to further explore and define a research direction for the next-generation secure Internet. The PIs will produce a report to communicate the results from this planning activity to the broader research community. Through a series of meetings, organized discussions, presentations, this effort will build a roadmap and help the community to reach consensus on what the important research directions are and how the different research efforts work together to achieve the desired goal of designing and implementing a testbed of an architecture for a next-generation secure Internet.","title":"Collaborative Research: Planning Grant: A Clean-Slate Design for the Next-Generation Secure Internet","awardID":"0540274","effectiveDate":"2005-08-01","expirationDate":"2006-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["402505"],"PO":["565090"]},"105833":{"abstract":"Video-enabled cell phones have the potential to enable deaf Americans to speak in their community's native language, American Sign Language (ASL), and gain the freedom, flexibility, and comfort of the wireless phone revolution. This research involves the design, implementation, and evaluation of new standards-compliant data compression methods that will allow ASL video, and other structured video, to be transmitted over low bandwidth cell phone channels. To be more specific, the goal of this research is to develop a framework in which to implement low-complexity H.264 encoding to provide maximum quality compressed structured video. Because the developed algorithms will be H.264 standard-compliant, an off-the-shelf H.264 decoder can be used.<br\/><br\/>The research applies to any class of videos where there is some inherent structure that can be exploited. The five areas of research are: (1) Design of an appropriate objective distortion metric for ASL; (2) Design of algorithms to preprocess video for display on small devices; (3) Development of methods to exploit structure in video for efficient coding; (4) Implementation of a rate-distortion-complexity optimization of H.264 using the new metric; and (5) Applications of the framework to other structured video such as surveillance video. Studies with ASL users are used to develop the objective intelligibility metric and to evaluate the new compression techniques.","title":"Collaborative Research: A Framework for Encoding American Sign Language and Other Structured Video","awardID":"0514357","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6846","name":"UNIVERSAL ACCESS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["530523"],"PO":["564898"]},"106966":{"abstract":"Current packet networks depend on the fundamental assumption that an end-to-end path exists between a source and a destination. However, in a number of real networks termed Delay Tolerant Networks (DTNs), for example, inter-planetary networks, wildlife tracking networks and military networks, this assumption does not hold. With this in mind, this project designs a family of efficient routing schemes that are suitable for such networks. The routing approaches are innovative in that they exploit scheduled, anticipated, and ad hoc connectivity. The work also introduces an analytical framework to analyze the performance of routing schemes, compare their behavior and refine their design. Performance metrics of interest include average message delivery delay, energy efficiency, network throughput, etc. Finally, the work includes results from extensive simulations of DTN routing algorithms under realistic scenarios, and implementations of the champion algorithms in simulation packages such as ns-2.<br\/><br\/>The outcome of this project will improve the communication capabilities in space explorations, under-sea experimentation, wildlife tracking and habitat monitoring networks, ad hoc vehicular networks for content distribution, etc. The results are expected to improve DTN's readiness for deployment in NASA's deep space research programs, and NSF's south polar programs. It will become possible to provide Internet services to under-provisioned remote areas, high latitude scientific outposts, nomadic communities, etc. Hence, the proposal will have broad social and economic impact.","title":"NeTS-NBD: Efficient Routing in Delay Tolerant Networks","awardID":"0520017","effectiveDate":"2005-08-15","expirationDate":"2009-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["309263","382322"],"PO":["434241"]},"105998":{"abstract":"Position information has long been a desired quantity in a host of applications. The advent of the Global Positioning System (GPS) provided reliable and accurate information world wide for outdoor environments. However, many applications are emerging where position information is required and the environment or complexity considerations prohibit the use of GPS (e.g., indoor applications). As a result, areas as diverse as robotics, sensor networks, and ubiquitous computing have seen an increase in research into indoor position location. However, the majority of current work relies on laser, ultrasound, or narrowband RF for positioning. Such techniques are severely limited in harsh environments with significant multipath and\/or interference. Unfortunately, many applications (e.g., emergency response, urban troop deployment, disaster relief) must deal with such harsh environments. In this research the investigators investigate an Ultra wideband-based (UWB) position location network approach which is ad hoc in nature and aims to overcome these limitations.<br\/><br\/>There are significant challenges associated with the development of position location networks for harsh environments. This work addresses four primary unsolved problems: (1) ranging with UWB signals in non-line-of-site environments, (2) signal acquisition of the primary path in dense multipath scenarios, (3) MAC design for UWB-based position location networks and (4) network position determination with a limited number of anchors. The research addressing these challenges uses a mixture of local signal processing methods and collaborative, network-level techniques.","title":"U-PoLo Net: UWB-Based Position Location Networks for Harsh Environments","awardID":"0515019","effectiveDate":"2005-08-01","expirationDate":"2008-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":["364912"],"PO":["564898"]},"104315":{"abstract":"NIRT: Metal-Dielectric Interfaces at the Nanoscale for Quantum Information and Microwave Devices <br\/>Abstract<br\/>A multidisciplinary program will be developed to understand the metal-dielectric interface and improve the quality of insulating materials for use in quantum information and microwave devices. The project will use in-situ epitaxial metal\/dielectric structures that will enable the growth of well-controlled, clean interfaces with a reduced defect density to allow for a fundamental understanding of the interface. Central to the approach are new tools based on superconductivity capable of analyzing even minute loss mechanisms in metal-dielectric structures and advanced transmission electron microscopy techniques. The combination of these methods will allow the establishment of a direct correlation between device performance and atomic structure, which will point to the most efficient method for the optimization of materials, interfaces and fabrication methods<br\/><br\/>This program will impact electronics in a number of ways, enabling the development of devices that are more complex, faster, and quieter. Understanding the basic science of materials at the nanoscale will open up for invention entirely new classes of devices, an example of which is the construction of a quantum computer that would be vastly more powerful than present day computers.","title":"NIRT: Metal-Dielectric Interfaces at the Nanoscale for Quantum Information and Microwave Devices","awardID":"0507227","effectiveDate":"2005-08-15","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1674","name":"NANOSCALE: INTRDISCPL RESRCH T"}}],"PIcoPI":["490825","304056","490826","503225"],"PO":["559883"]},"107714":{"abstract":"Progress in the biological sciences in the post-genomic era depends on our ability to make sense of genome-scale information. The genome and the proteome together establish an intricate network of interactions that exhibits many similarities to social and political networks. This interaction network forms a simple, conceptual representation of the molecular machinery in the living cell. Knowledge of individual interactions and patterns of interaction therefore significantly enhances our understanding of the mechanisms of biological function. <br\/><br\/>Unfortunately, experimental determination of molecular interactions at the scale of the entire genome is often error-prone: many interactions revealed by such high-throughput experiments are false, and conversely, many of the actual interactions are not revealed at all. It is important, therefore, to establish rigorous computational methods that utilize high-throughput data from a variety of sources to predict the existence (and lack thereof) of an interaction, and to assign confidence levels to each prediction in a systematic manner. Our project utilizes state-of-the-art predictive methods from the field of Artificial Intelligence-Bayesian support vector machines-to predict molecular interactions at the whole-genome level. The project is initiated by an exhaustive data collection effort involving a variety of data sources that supply putative predictors for the presence or absence of interactions among protein\/protein or gene\/protein pairs. Dominant predictors among these will be isolated, and the prediction system will be applied to the genomes of several organisms, including the budding yeast, worm, and fly. The accuracy of the method will be tested and refined by computational and biological means. Successful completion of this project will significantly enhance our ability to decipher genomic information and apply these findings to discover novel functional pathways of biological, agricultural, and medical importance. <br\/><br\/>All methods and results will be publicly disseminated, the former with stand-alone executable programs, and the latter via publications and web pages. The project will support interdisciplinary training of graduate and postdoctoral students, and should provide research opportunities for undergraduate students.","title":"Bayesian Support Vector Machines for the Prediction of Molecular-Genetic Network Motifs Across Organisms","awardID":"0523643","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["551355","560510","286131"],"PO":["565223"]},"107857":{"abstract":"ABSTRACT<br\/><br\/>Proposal Number: 0524695<br\/><br\/>Title: CT-CS: Trustworthy Cyber Infrastructure for the Power Grid<br\/><br\/>PI: William H. Sanders<br\/><br\/>Today's quality of life depends on the continuous functioning of the nation's electric power infrastructure, which depends in turn on the health of an underlying computing and communication network infrastructure that is at serious risk both from malicious cyber attacks and accidental failures. This Cyber Trust center-scale activity addresses the challenge of how to design, build, and validate a cyber infrastructure for the next generation power grid that can survive malicious cyber attacks while providing continuous power delivery. Since the constraints and vulnerabilities of the power system cyber infrastructure are similar to those faced by many other critical infrastructure systems, the solutions created are expected to be adaptable for use in those systems as well. The activity will thus have a significant impact on the way the cyber infrastructure of the future power grid and other critical infrastructures are built, making them more secure, reliable, and safe.<br\/><br\/>More specifically, this research creates infrastructure technology that will convey critical information to grid system operators despite partially successful cyber attacks and accidental failures. Security and trust validation techniques are developed that can quantify the trustworthiness of a proposed design with respect to critical properties. The activity's educational program provides an integrated undergraduate and graduate experience for students of Cyber Trust issues, including working on cross-disciplinary teams and understanding practical concerns of industry. An interactive simulator created by the project will allow users to experiment with new power grid cyber-infrastructure design approaches.","title":"CT-CS: Trustworthy Cyber Infrastructure for the Power Grid","awardID":"0524695","effectiveDate":"2005-08-15","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"H148","name":"US DEPT OF HOMELAND SECURITY"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"1518","name":"CONTROL, NETWORKS, & COMP INTE"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0701","name":"Division of ELECTRICAL, COMMUN & CYBER SYS","abbr":"ECCS"},"pgm":{"id":"7564","name":"COMMS, CIRCUITS & SENS SYS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"H232","name":"Department of Energy"}}],"PIcoPI":["542482","563534","457811","557160","564825"],"PO":["529429"]},"106934":{"abstract":"Traditional networks, both wired and wireless, have the property that end-to-end paths between nodes are relatively stable. However, not all environments that require communication will allow the creation of stable end-to-end paths. For example, the aftermath of a severe earthquake disaster will include collapsed buildings, persons trapped in debris, damaged utilities and roads, as well as fires and secondary explosions. Under this situation, the ability to communicate, even at low rates, is extremely valuable for sharing vital information, such as the number and location of survivors and the activities of rescue workers. To provide communication in these challenged environments, the network protocols must be explicitly designed to perform despite frequent disruptions in the availability and performance of network components (i.e., links and nodes). The resulting systems are termed Disruption Tolerant Networks (DTNs).<br\/><br\/>This work focuses on the construction of DTNs that go far beyond the task of finding unicast paths. Instead, these DTNs are robust under uncertainty and attack, and are highly efficient in their use of the node and link resources. Specifically, this project focuses on the following fundamental functions: group communication, single-hop transfers, and power management. The work is grounded by experimentation in testbeds at the collaborating universities. The project provides algorithms, protocols, and platforms to create robust and efficient DTNs enabling communication in the most critical of environments, as well as exposes cost-performance tradeoffs that might have broader applicability in less challenged networks.","title":"Collaborative Research: NeTS-NBD: Construction of Robust and Efficient Disruption Tolerant Networks","awardID":"0519881","effectiveDate":"2005-08-15","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["451830","309222"],"PO":["565090"]},"104756":{"abstract":"The project will develop an integrated aproach to improving communication performance in clusters. Cluster computing has become a common, cost-effective means of parallel computing. Although adding more CPUs increases the cluster's maximum processing power, real applications often can not efficiently use very large numbers of CPUs, due to lack of scalability. In regular codes the main impediment to achieving scalability is the communication overhead which increases as the number of CPUs increases. Most of these optimization methods proposed target specialized hardware or programming languages, and require specialized knowledge from the domain scientist, or are not enough to provide a comprehensive solution on their own, and do not adequately address the challenges of the layers of communication software between the sender processes and the receiver processes. Improved performance overall for these applications, it remains largely untapped due to (1) the need for the knowledge of the context of the communication operations to exploit the sophisticated network technology fully, and the (2) the low level nature of programming needed within the application program context to achieve that potential. In particular, performance can often be improved through increasing the use of lightweight asynchronous communication. Unfortunately, programming with asynchronous communication is difficult and error prone, even for the most experienced programmers.<br\/><br\/>The project will pursue a vertically integrated approach, where a set of optimizations in the compiler, network and operating system, can enable legacy parallel applications to scale to a much larger number of CPUs, even if written without any knowledge of our techniques. An experimental prototype and preliminary experiments with real scientific applications, show that significant performance improvements are possible with a vertically integrated approach where knowledge of the context of communication operations is joined with knowledge of the network and cluster details to provide a fine-grained strategy for overlapping communication and computation. Based on these initial promising results, the overall goal of this proposed research is to create a means for scalable cluster computing through enabling integrated knowledge and cooperation between the source optimizer, operating system, and network technology of the cluster, without relying on the programmer to learn about the low level details of the cluster communications system.","title":"CSR - AES: An Integrated Approach to Improving Communication Performance in Clusters","awardID":"0509170","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["527984","561820"],"PO":["493916"]},"104536":{"abstract":"Abstract<br\/><br\/>PROPOSAL NO: 0508245<br\/>INSTITUTION: Southern University<br\/>PRINCIPAL INVESTIGATOR: Guang-Lin Zhao<br\/>TITLE: NER: Integration of Ab-Initio Computation with Large Scale Molecular Dynamics Simulation for Nanomaterials Research<br\/><br\/>Ab-initio quantum mechanics calculation is a state-of-the-art method in materials research. Complex nanomaterials may involve thousands, even millions, of atoms per unit cell or super-cell. Computations for the complex nanomaterials are beyond the limits of traditional ab-initio quantum calculations. Classical molecular dynamics (MD) simulations, on the other hand, can probe the properties of these systems based on pre-developed interatomic potentials. However, the usefulness of the method is limited by the reliability of the interatomic potential, particularly for complex nanomaterials. The objective of this project is to develop a new computational method and related computer code (computer software) that integrates ab-initio quantum computations with MD simulations. The resulting software will have the capability of MD calculations with the reliability of ab-initio method. The proposed research will have a broad impact on the simulations of nanomaterials for understanding and in some cases for predicting the properties of nanomaterials. Such understanding, based on quantum mechanics at a microscopic level, will shed light on possible mechanism(s) to improve the desired properties of nanomaterials in such a way that it will reduce expensive and redundant experimentation.","title":"NER: Integration of Ab-Initio Computation with Large Scale Molecular Dynamics Simulation for Nanomaterials Research","awardID":"0508245","effectiveDate":"2005-08-15","expirationDate":"2007-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1676","name":"NANOSCALE:  EXPLORATORY RSRCH"}}],"PIcoPI":["557461","435157"],"PO":["562984"]},"105999":{"abstract":"Multi-input multi-output (MIMO) wireless communication systems, equipped with multiple transmit and receive antennas, offer dramatically improved error performance and enhanced capacity. However, realizing the potential of MIMO systems is countered by the effects of fading, mobility, interference, and low-power requirements. The problem of interference is exacerbated by the proliferation of ad-hoc networks <br\/>and the emergence of ultra-wideband communications.<br\/><br\/>Existing approaches to MIMO wireless transceiver design assume either perfect channel knowledge or no channel knowledge at the transmitter. The latter yields pessimistic designs, while the former is unrealistic and can result in substantial performance losses in cases of severe channel mismatch. Thus, the researchers are investigating the fundamental capacity and error performance limits of MIMO wireless <br\/>systems for varying degrees degrees of PARTIAL channel knowledge. Correspondingly, they are developing channel-adaptive MIMO transceivers based on partial channel knowledge that minimize transmit-power and maximize data rates at a prescribed error probability margin. Hybrid schemes are being researched that incorporate both space-time block coding and space-time beam forming to effect interference avoidance and robustness to imperfect channel knowledge. The advanced algorithms under development will profoundly alter MIMO technology and directly impact the robustness and bandwidth efficiency of wireless multi-antenna communications.","title":"Waveform Diversity for Wireless Communications with Joint Transceiver Multipath Exploitation and Interference Avoidance","awardID":"0515032","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["434205","564051"],"PO":["564898"]},"107847":{"abstract":"Proposal ID: 0524643<br\/>Title: CT-ISG: Computer Log Anonymization and Information Sharing<br\/>PI: Adam Slagell<br\/><br\/>To make computer and network log anonymization most useful, solutions must be customizable. Current tools are inflexible and do not support more than a single type of log. This, in turn, has inhibited log sharing, making the detection and response to modern, distributed and coordinated attacks more difficult. The main goal of this project is to create an anonymization framework that allows users to choose from multiple levels of anonymization that make different trade-offs between information loss and the protection of sensitive logging information. This research accomplishes this by addressing the following 5 research problems: (1) create a classification of logs by the types of security events that can be detected, (2) create a metric of utility based upon the different fields within a set of logs, (3) determine how different anonymization algorithms affect this metric, (4) determine how the choice of anonymization algorithms affects the strength of the anonymization scheme itself, and (5) create an architecture that provides multi-level anonymization for logs while optimizing the log utility metric for a given security requirement. The results of this research project will remove barriers that have prevented many organizations from sharing logs, thus making it more likely to detect and understand broad and coordinated attacks.","title":"CT-ISG: Computer Log Anonymization and Information Sharing","awardID":"0524643","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4089","name":"NETWORK CENTRIC MIDDLEWARE SVC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["464413","562328",284888],"PO":["529429"]},"107858":{"abstract":"From Bluetooth transceivers to the NASA Mars Rover, reconfigurable circuits have become one of the mainstays of embedded design. Combining the high computational performance of specialized circuits with the re-programmability of software, these devices are quickly becoming ubiquitous. Unfortunately, if unprotected, this reconfigurability could be exploited to disrupt critical operations, snoop on supposedly secure channels, or even to physically melt a device. However, a new approach to controlling changes to the hardware logic promises to overcome these problems. In addition, the innate malleability of this hardware presents the opportunity for hardware enforcement of adaptive security policies. For example, in an emergency, trusted individuals may need to override the nominal security policy. Thus, the reconfigurable component may provide a highly trusted mechanism for secure functionality in changing environments.<br\/><br\/>This research aims to close a gaping security hole in our nation's information infrastructure by enhancing the logical structure and internal management of reconfigurable hardware to enforce a dynamic information protection policy. Specifically, this research will: (1) discover hardware synthesis and static validation methods that will ensure that only secure and non-destructive configurations can be loaded, (2) develop new reconfigurable structures capable of securely mediating run-time access to shared resources through the use of hardware-compiled formal access policy languages, and (3) establish a firm foundation for trustworthy dynamic policy enforcement through ontological analysis, formal modeling and the development of management mechanisms integrating the results of the first two activities.","title":"Collaborative Research: CT-T: Adaptive Security and Separation in Reconfigurable Hardware","awardID":"0524707","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["528231",284923,284924],"PO":["497499"]},"100830":{"abstract":"0447594<br\/>Jane Huang<br\/>DePaul University <br\/><br\/>CAREER: Goal Centric Traceability for Supporting Impact Analysis of Systemic Qualities<br\/><br\/>Software products are increasingly deployed within safety critical applications in which system failure could lead to loss of life, environmental damage, or significant financial loss. Unfortunately numerous accounts of failures cite the root cause as problems related to the incorrect implementation of non-functional qualities such as safety, security, or reliability. The objective of this research is to develop a new technique known as Goal-Centric Traceability (GCT) that will provide effective support for managing systemic qualities. GCT is designed to prevent systemwide non-functional qualities from being adversely impacted during the change process and to ensure the long-term quality of the system. The work will involve developing dynamic traceability techniques between functional and non-functional requirements using probabilistic and constraint-based methods. Heuristics and supporting methods will be defined for identifying the transitive impact of a change upon systemic goals and for automating the re-evaluation of existing assessment models such as performance simulations and executable scenarios. A standard interface for interfacing GCT with popular architectural assessment tools will be developed. The work has potential for significant impact as it addresses a complex yet practical problem commonly experienced in industry.","title":"CAREER: Goal Centric Traceability for Managing Systemic Requirements","awardID":"0447594","effectiveDate":"2005-08-01","expirationDate":"2011-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["550521"],"PO":["564388"]},"100863":{"abstract":"Many data-intensive systems, such as commercial or scientific database systems, store vast amounts of data. Some metrics (e.g., performance) of answering user queries on stored data can be improved by using derived data, such as indexes or materialized views. The goal of this project is to develop an extensible framework for designing and using derived data in answering database queries efficiently. The outcomes of the project are expected to be general and independent of a specific data model (e.g., relational or XML), while giving guarantees with respect to query-performance improvement.<br\/><br\/>The approach consists of developing and evaluating mathematical models and algorithms for designing and using views and indexes for common types of queries on relational and XML data. The techniques will be experimentally evaluated using an open-source implementation of a database-management systems, on synthetic and real relational and XML databases. Expected outcomes of the project include automated tuning of data-access characteristics in a variety of applications, thus enhancing the quality of user interactions with data-intensive systems.<br\/><br\/>The PI will develop and disseminate practical solutions to the problem. Expected outcomes of the project include sequences of research-oriented exercises that would increase novelty and excitement in the curriculum and enhance learning experience for students. By working on the proposed project, the diverse body of students at NCSU will obtain a unique set of skills that will position them competitively in the modern workplace.<br\/><br\/>The PI will make the results of the project widely available, including software she will develop.<br\/><br\/>http:\/\/research.csc.ncsu.edu\/selftune\/","title":"CAREER: Adaptive Automated Design of Stored Derived Data","awardID":"0447742","effectiveDate":"2005-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}}],"PIcoPI":[266102],"PO":["469867"]},"103756":{"abstract":"The desire for integrated services in networking research often results in significant shortfalls in service provision for some specialized applications. While the applications may be able to directly compensate with additional code, the placement of many necessary resources in protected address spaces with rigid interfaces (e.g., the operating system) may make effective compensation difficult if not impossible in practice. The experimental research proposed here, in Multiple restricted Virtual Machines (MrVM), takes a high-risk but potentially very promising approach to addressing this problem. The basic notion is to construct an application-specific complement of virtual machine services to embed into a conventional operating system. In this way, the application-specific requirements can be met, while the restrictions placed on MrVM interpretation insulate applications from each other and from the shared services.","title":"SGER: Prototyping Application-Specific Kernel Virtual Machines for Networking","awardID":"0504159","effectiveDate":"2005-08-15","expirationDate":"2007-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["402531"],"PO":["7594"]},"105945":{"abstract":"Intellectual Merit<br\/><br\/>-Develop theortetical techniques to analyze greedy heuristics with non-submodular potentail functions.<br\/>-Develop new techniques, design approximations with better performance ratio for some classical optimization<br\/>-Find new heuristics with better computational performance<br\/><br\/>Broader Impact<br\/><br\/>-Enhance advanced theory of optimization and algorithmic study in optimization<br\/>-Research will involve graduate students","title":"NSG: Studies in Optimizations with Applications","awardID":"0514796","effectiveDate":"2005-08-15","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["450698"],"PO":["432103"]},"106924":{"abstract":"Much of the research on mobile ad hoc networks (MANETs) has focused on simulation and testbed studies, while plans for actual deployment of large-scale MANETs remain limited primarily to military and single-vendor public safety applications. There is uncertainty, in fact, as to whether a large-scale distributed ad hoc network created with hardware and software from many different vendors and controlled by many different administrative entities is even viable. The emergence of software-defined and cognitive radios in an ad hoc environment also opens new questions regarding how to ensure interoperability and cooperation among nodes.<br\/>This project establishes a multi-institution competition, The MANIAC Challenge, that allows us to study the tension between the desire of nodes to focus only on delivery of their team's packets (in order to preserve battery life and competitive advantage) and the need for nodes in a MANET to cooperate in order to permit the delivery of packets across the heterogeneous network.<br\/>The MANIAC Challenge provides an unprecedented opportunity to observe the operation of an uncoordinated ad hoc network. By studying the outcome of the challenge, we will identify emergent behaviors in a MANET and assess whether resource sharing is an incentive-compatible strategy in the absence of monetary rewards and explicit reputation schemes. In its second year, the competition will permit observation of the effectiveness of physical and link layer adaptation schemes.<br\/><br\/>Ultimately, this will impact how research in networking is conducted, stimulating hands-on and prototyping efforts that complement the currently-favored methods of simulation and mathematical analysis.","title":"NeTS-NBD: Mobile Ad Hoc Networking Interoperability And Cooperation Challenge (MANIAC Challenge)","awardID":"0519825","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["540088","540078"],"PO":["557315"]},"105714":{"abstract":"Experience building and fielding data integration systems has shown that they are brittle in a very fundamental way: they cannot handle uncertainty about data or about how data is combined to provide answers. This limitation is especially pronounced in scientific applications, where data is inherently uncertain and the models of the domain are constantly evolving. From the users' perspective, the inability to model uncertainty can result in loss of relevant answers, an explosion of irrelevant answers and in no justification of answers. The limitation is deeply rooted in the deterministic paradigm underpinning data management systems today, which is designed to support scalability to large data instances, but is incapable of representing and reasoning about uncertainty.<br\/><br\/>A new approach to data integration, where uncertainties are handled explic-<br\/>Itly, is proposed. Over the past few years, the BioMediator system, which<br\/>integrates about a dozen public data sources on genes and proteins, has been available. The group has observed and documented the types of uncertainty that limit the power of any mediator-based integration system like BioMediator. These uncertainties occur at three levels: at the data instance level, at the schema level, and at the user query level. In the new approach, all uncertainties will be made explicit in the system, and represented in a uniform way, using a probabilistic data model. The mediator system supports a query language with SQL but with a modified semantics: the answers to each query are annotated with a probability score, and a lineage<br\/>information.<br\/><br\/>The new work will involve the design of a probabilistic data model, the development of probabilistic query processing and optimization techniques, and the design of user feedback methods. They will build a system, U2 (short for UII { Uncertain<br\/>Information Integration ) that will model uncertainty at all levels of the system, including the query language, mediated schema, source mappings and source data. U2 will explain its results to the user and will actively seek to resolve uncertainty when it arises, incorporating feedback from the user where possible. They will extend the BioMediator System and collaborate with the current users of the system.<br\/><br\/>There are three areas of broader impact. Issues of information integration will be integrated more tightly into the undergraduate and graduate database curriculum<br\/>Second, the research will fuel collaboration with biomedical computing research, and will<br\/>extend the BioMediator system that is currently in use by practitioners in the field. Finally, tools and services will be made available for public use.","title":"II: Information Integration in the Presence of Uncertainty","awardID":"0513877","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7373","name":"ITR-INFORMATION INTEGRATION"}}],"PIcoPI":["531543","531543",278933,278934],"PO":["565136"]},"105978":{"abstract":"Collaborative Research: Signal Processing in Wireless Ad Hoc Networking<br\/><br\/>Project Abstract<br\/><br\/>Mobile ad hoc networks (MANETs) are highly desirable in areas\/situations where base stations are unavailable or too expensive to establish. Lack of infrastructure, intermittent connectivity, and frequent changes in topology due to mobility present significant challenges in the research of MANETs. While most prior efforts were primarily focused on the networking layer, there are compelling reasons supported by recent information theory and signal processing studies which indicate that an integrated approach seeking cross-layer diversity exploitation can lead to more fruitful results. This project follows that path.<br\/><br\/>The objective of this project is to develop a framework of communication, networking, and signal processing techniques for MANETs by exploiting node cooperation at the physical, medium access, and networking layers. Three different but intertwined research directions are involved, Specifically, the first is to develop a distributive modulation theory over wireless relay channels, covering investigation of the characteristics of wireless relay channels, modulation and detection for coherent, differential and non-coherent communications for relay networks, power allocation and placement of relay nodes. The second direction is to develop bandwidth-efficient and delay-tolerant cooperative coding schemes to address several issues that are unique in coded cooperation, including bandwidth expansion caused by repetitive transmissions from relays, node asynchronism due to distributive locations of relays, and scalability (viz., the capability to degrade gracefully when some cooperating nodes that implement a distributive code fail because of fading). The third direction is to explore networking by parallel relays and develop networking protocols that take into account the realistic characteristics of radio signals and provide a flexible framework for diversity exploitation.","title":"Collaborative Research: Signal Processing in Wireless Ad Hoc Networking","awardID":"0514938","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["399103"],"PO":["564898"]},"104306":{"abstract":"This proposal was received in response to Nanoscale Science and Engineering initiative, NSF 04-043, category NIRT. The Divisions of Chemistry (CHE), Materials Research (DMR), and Physics (PHY) are supporting Mark I. Stockman of Georgia State University, Moungi G. Bawendi and Keith A. Nelson of the Massachusetts Institute of Technology, and Hrvoje Petek of the University of Pittsburgh in a research effort as a Nanoscale Interdisciplinary Research Team (NIRT). Their work will focus on developing devices and methods enabling the manipulation of electromagnetic fields at the nanometer scale. Specifically, the group will work on developing a structural element (a portal) that will enable a shaped, femtosecond laser pulse to individually address single quantum dots in proximity to the portal. The NIRT brings together a multidisciplinary team that can address all aspects of the project from quantum dot synthesis to femtosecond coherent control and time-resolved plasmonic imaging to theoretical modeling and design. The results of this research should contribute broadly to nanotechnology and nanoscience by enabling direct access to the nanoscale with optical methods that have long been restricted to micron length scales. This work will provide the participating graduate and postdoctoral researchers with opportunities to develop expertise in a unique combination of methods in modern optics and advanced materials research: ultrafast optics, coherent control, plasmonics, nanomaterials fabrication and design, and sophisticated modeling. Moreover, high school and undergraduate research students will be introduced to the work of this NIRT project through expansion of existing efforts at MIT and the REU program at University of Pittsburgh.","title":"NIRT: Full Spatio-Temporal Coherent Control on Nanoscale","awardID":"0507147","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0301","name":"Division of PHYSICS","abbr":"PHY"},"pgm":{"id":"1248","name":"PHYSICS-OTHER"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1674","name":"NANOSCALE: INTRDISCPL RESRCH T"}},{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1978","name":"PROJECTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"1710","name":"CONDENSED MATTER PHYSICS"}}],"PIcoPI":["545104","338494","492200","546983"],"PO":["532169"]},"107705":{"abstract":"This grant supports theoretical research on fundamental issues related<br\/>to the implementation of quantum computation in solid-state<br\/>devices. Since the discovery that certain tasks could be performed<br\/>with great efficiency by algorithms based on quantum mechanics, an<br\/>intense effort has been made to find suitable quantum<br\/>hardware. Although several proposed implementations, such as those<br\/>based on nuclear magnetic resonance and atomic trapping, have passed<br\/>the proof-of-principle, few-qubit phase, the path to achieving a<br\/>reliable multi-qubit quantum computer is still undefined.<br\/><br\/>In this proposal we investigate the physical limitations to resilient<br\/>computation with solid-state quantum bits (qubits), such as<br\/>semiconductor quantum dots and superconductor junctions. While<br\/>solid-state qubits seem easily scalable from the fabrication<br\/>viewpoint, they also present high decoherence rates as compared to<br\/>other implementations. One major concern is that such strong <br\/>decoherence may lead to errors occurring at a rate too large to <br\/>be controlled.<br\/><br\/>However, differently from other nuclear, atomic, and optical qubits, <br\/>the interaction of solid-state quantum devices with the environment <br\/>can introduce strong memory effects. As a result, temporal correlations <br\/>may appear during the operation of multi-qubit systems. Current quantum <br\/>error correction codes are not designed to cope with this situation, <br\/>which may then invalidate any error threshold estimate for solid-state <br\/>qubits based on the efficiency of those codes.<br\/><br\/>We will explore these issues in a comprehensive way. Starting from a<br\/>thorough study of the mechanisms of decoherence in single- and<br\/>double-qubit systems, we will study a model of multi-qubit systems in<br\/>the presence of correlated noise in a variety of realistic<br\/>conditions. Our results will help set up new strategies for the<br\/>operation of multi-qubit systems. They will also let us understand<br\/>what are the constraints that error correction codes will need to<br\/>satisfy in order to achieve fault-tolerant quantum computation in<br\/>large-scale solid state implementations. To achieve our goals, we have<br\/>put together a team of researchers with expertise in nanoscale physics <br\/>and computer science. The final outcome of our project will be a much<br\/>better understanding of how a real solid-state quantum computer would<br\/>behave.","title":"QnTM: Collaborative Research: Is Resilient Quantum Computing in Solid State Systems Possible?","awardID":"0523509","effectiveDate":"2005-08-15","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":["512020"],"PO":["474792"]},"107837":{"abstract":"Proposal ID: 0524540<br\/>Title: CT-ISG: Printer and Sensor Forensics<br\/>PIs: Jan P. Allebach, George T. Chiu, and Edward J. Delp<br\/><br\/>This research addresses the need for a means to assure the authenticity of digital media consisting of image content. The work investigates both intrinsic signatures that are an inherent characteristic of the imaging device and extrinsic signatures that can be introduced by the manufacturer with the possibility including additional user-controlled information. The intrinsic signature represents artifacts that are due to optical, electrical, or mechanical limitations of the imaging device. The extrinsic signature is generated by modulating parameters that control the intrinsic signature of the device. The same algorithms that detect the intrinsic signature will form the basis for detecting and decoding the extrinsic signature.<br\/><br\/>This research will result in a new understanding of the relation between imaging devices and artifacts produced by those devices. It will lead to new knowledge regarding image analysis for feature extraction and the design of classifiers based on those features. In calculating error control codes and channel capacities for extrinsic signatures, it will extend the application of classical communications theory to a new domain. This work will be of direct benefit to society by providing law enforcement and government agents new tools for combating counterfeiting, forgery, and other criminal and terrorist activities.","title":"CT-ISG:Printer and Sensor Forensics","awardID":"0524540","effectiveDate":"2005-08-15","expirationDate":"2010-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["563744",284859,"385369"],"PO":["529429"]},"108816":{"abstract":"This project is supporting the annual Cyber Corps Symposium, which brings together the current scholarship students, faculty at participating institutions, and government officials for a four-day event. This symposium provides a mechanism for building community across the programs, helping to develop that strong cadre of IA programs. In addition, the symposium provides opportunities for students, faculty, and agency officials to interact regarding myriad IA topics, including the future of IA in the government, curricular matters, and employment opportunities. An important aspect of the symposium will be a series of invited talks and panels regarding technical, legal, and policy aspects of IA. The symposium also provides networking opportunities for faculty at participating SFS institutions and for representatives of federal agencies. These interactions improve faculty's understanding of the government's needs and challenges, which in turn helps them develop curricula that better address those needs.","title":"CyberCorps 2005 Symposium","awardID":"0530777","effectiveDate":"2005-08-15","expirationDate":"2006-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"T070","name":"HOMELAND SECURITY-FED CYBER SE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1104","name":"Division of UNDERGRADUATE EDUCATION","abbr":"DUE"},"pgm":{"id":"1668","name":"FED CYBER SERV: SCHLAR FOR SER"}}],"PIcoPI":["530674","530676","456455"],"PO":["288365"]},"107738":{"abstract":"When solving the factorization problem, Peter Shor illustrated that a new type of computer using principles of quantum mechanics could far exceed the efficiency <br\/>of today's computers. Since then quantum computer scientists have been focusing on finding the group of computational problems where speed-ups of this magnitude can be obtained. The present proposal targets a systematic study of this question using two different methods.<br\/><br\/>The first method is based on earlier work by the PI, where he showed that the process of quantum walks (i.e. the repeated use of a quantum operator made by \"quantizing\" <br\/>a classical operator) can be used to speed up a large class of classical algorithms. The proposal describes new kinds of quantum walks that could solve more problems than previous ones and attain larger speed-ups. This topic comprises many open questions, which the PI and his students wish to investigate.<br\/><br\/>The second method relates to the black box model, which captures the speed of a quantum algorithm by finding out how many times it has to use different subroutines. At present, this model has proved to be the most successful tool in the comprehensive study of quantum algorithms and is often a faithful indicator of their complexities. The PI's previous works on the black box model are well-known in the quantum computing community. The proposal raises several new research ideas that aim at a better understanding of the black box model.<br\/><br\/>Quantum computation research efforts often lead to interesting discoveries about classical algorithms, which is an unexpected new development in the theory of computing. While researching the two methods above, the PI and his students plan to keep a watchful eye on this new benefit.<br\/><br\/>The PI is currently building a quantum computing group at Rutgers, The State University of NJ. The PI has been approached by several interested students both at the graduate and undergraduate levels, and several colleagues have indicated as well that they would be ready to participate in the research efforts of such a group. The PI gained support from the deans and DIMACS, and built partnerships with quantum groups at Lucent Technologies' Bell Labs and NEC. Support from NSF is needed primarily for student research assistantships.","title":"QnTM: Quantum Speed-up of Classical Algorithms","awardID":"0523866","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["531145"],"PO":["565157"]},"110676":{"abstract":"Threaded event networks are hypertext-like structures which the PI has previously explored in three domains (science education, narratives, and digital preservation) employing distinct but related formalisms: qualitative causal maps, plot units, and role-activity diagrams. While these formalisms appear to be generally effective, they are considerably more complex than simple node-and-link hypertext. Moreover, event networks are often associated with text passages and are often intended for educational uses where repeated navigation through the network following different paths is to be expected. In these contexts, ease of user access is crucial in order for the formalisms to be effective for users. In this project, the PI will extend his work on threaded event scenarios to address interface issues and develop supporting navigational landmarks, by applying composite hypertexts to complex domains to further develop timeline toolkits that his earlier work have created or suggested. He will develop more interactive and adaptive presentations, which allow users to branch from established guided tours within the event networks, and which, as the users gain greater familiarity with the network, enable adjustment of the landmarks that are presented to provide context for the users. Authors and other users will also be able to add annotations, and even structured meta-comments, to the event networks, for which dialog management and rhetorical structures will be supported. The PI will conduct three levels of evaluations: for the low-level tools, for the use of the tools in high-level tasks, and as the basis for developing communities discussing science and history. <br\/><br\/>Broader Impacts: Because discussion of causation is so common in areas as diverse as science and history, tools to highlight and explore event networks potentially have a wide range of applications. These tools should help students to develop richer conceptual understanding of events in science and history. The tools may also be used by the public to create discussion sites about history, and they should be useful for practicing scientists, historians, archivists, and genealogists.","title":"Interacting with Threaded Event Scenarios","awardID":"0541637","effectiveDate":"2005-08-23","expirationDate":"2006-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6845","name":"HUMAN COMPUTER INTER PROGRAM"}}],"PIcoPI":["374273"],"PO":["565227"]},"104802":{"abstract":"Pervasive computing systems are on the brink of revolutionizing our world - computation will be embodied in everyday objects, not 'traditional' computers, and will seamlessly enhance our lives. This embedding, and cost considerations, suggest that pervasive computing applications will consist of a dynamic set of system components (smart objects) executing on a distributed fabric consisting of shared storage, computing, and sensing resources, adapting to environmental changes, and opportunistically cooperating and exploiting third party services. At the same time, RFID technologies continue their fast paced evolution towards enabling cost effective embedding of tags, storage (i.e., read\/write memories) and thus context within the environment and 'space' of objects. As such, these technologies provide a concrete setting in which to explore the system-level challenges unique to pervasive computing. This research lays the foundations for novel system-level technologies and design methodologies for next generation pervasive computing applications. It includes three key thrusts. First, it addresses the organization and management of novel RFID-based distributed memory subsystems. The key idea is that these might be used as a public asset, shared by multiple applications, leveraging a collective of anonymous mobile smart objects to gather and efficiently disseminate in situcontextual information. The second thrust focuses on maintaining data integrity and consistency across such RFID-based distributed memory subsystems. The last thrust investigates how to <br\/>generate and store context for novel pervasive computing applications, involving a formal study of the characteristics and scaling properties of different policies to generate and capture distributed space-time <br\/>contextual data.","title":"CSR-EHS: Novel Mobile and Distributed Embedded Systems for Pervasive Computing Applications","awardID":"0509355","effectiveDate":"2005-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["560221","560221","293070"],"PO":["561889"]},"104846":{"abstract":"One of the key problems confronting computer system designers is the management and conservation of energy sources. This challenge is evident in a number of ways. The goal may be to extend the battery lifetime in a computer system comprising of a processor and a number of memory modules, I\/O cores, and bridges. This is especially important in light of the fact that power consumption in a typical portable electronic system is increasing rapidly whereas the gravimetric energy density of its battery source is improving at a much slower pace. Other goals may be to limit the cooling requirements of a computer system or to reduce the financial burden of operating a large computing facility. The objective of this research is to develop system-wide power optimization algorithms and techniques that eliminate waste or overhead and allow energy-efficient use of the various memory and I\/O devices while meeting an overall performance requirement. More precisely, this project tackles two related problems: dynamic voltage and frequency scaling targeting the minimization of the total system energy dissipation and global power management in a system comprising of modules that are potentially managed by their own local power management policies, yet must closely interact with one another in order to yield maximum system-wide energy efficiency. The broader impacts of this project include the development of energy-aware computer systems as the key for cost-effective realization of a large number of high-performance applications running on battery-powered portable platforms and the education and training of young researchers and engineers to be able to address complex and intertwined energy efficiency\/performance challenges that arise in the context of designing next-generation information technology products and services.","title":"CSR-EHS: System-Wide Dynamic Voltage Scaling and Power Management in Battery-Powered Embedded Systems","awardID":"0509564","effectiveDate":"2005-08-15","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["518663"],"PO":["561889"]},"105704":{"abstract":"ABSTRACT<br\/><br\/>Advances in video technology are being incorporated into today's healthcare practice. For example, various types of endoscopes are used for colonoscopy, upper gastrointestinal endoscopy, enteroscopy, bronchoscopy, and cystoscopy. In addition, a rapidly expanding number of formerly open surgical procedures now are being converted to endoscopic procedures including resection of gallbladders, retrieval of donor kidneys, resection of tumors of colon and pancreas, correction of hiatal hernias, coronary artery bypass grafting and minimal invasive neurosurgeries. Despite a large body of knowledge in medical image analysis, endoscopy videos are not systematically captured for real-time or post-procedure reviews and analyses. They are recorded occasionally to magnetic video-tapes (i.e., VHS). No hardware and software tools have been developed to capture, analyze, and provide user-friendly and efficient access to the medical, scientific, or educational content on such videos. This project aims to develop an Endoscopic Multimedia Information System (EMIS) to capture high quality endoscopy videos, analyze the captured videos, and provide efficient access to the content of these videos.<br\/><br\/>Images of endoscopy videos significantly differ from medical still images as studied<br\/>in the literature of medical image processing. The project will develop a new capturing system for endoscopy videos designed for patient's privacy and is non-disruptive to endoscopic procedures<br\/>and non-restrictive to a particular endoscope vendor. New algorithms for automatic classification of informative and non-informative frames. The technique does not need any predefined parameters or thresholds. New algorithms for automatic content analysis for protruding lesions such aspolyps. Many protruding lesions are clustered together. A new region segmentation technique that can identify isolated lesions will be developed first. To handle clustered<br\/>lesions, algorithms using a region pattern graph that captures important characteristics of relevant regions will be developed..<br\/><br\/>The proposed system will directly benefit endoscopic research, education, and training. Contributions to research-based training of graduate students who will contribute to research-based advanced training of students in graduate and undergraduate programs in computer science and medical informatics at PI and CoPI's institutions. The project will contribute to training of a new generation of computer scientists with a unique skill set supplement to traditional<br\/>medical imaging. This research will enhance research opportunities for junior high and high school students participating in various university programs (UTA Summer Science Camp, Program for Women in Science and Engineering) and national programs. (4) Broaden the participation of under-represented groups.","title":"SEI: Collaborative Research: Endoscopic Multimedia Information System (EMIS)","awardID":"0513809","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7294","name":"SCIENCE & ENGINEERING INFORMAT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["425154","433896"],"PO":["565136"]},"106969":{"abstract":"The goal of the WaveScope project is to build a sensor computing platform for high data-rate, disconnected sensor network applications. It is motivated by industrial, structural, and automotive monitoring applications that use many embedded, wireless, and high-rate vibration, pressure, magnetic, and sound sensors to detect faults, anomalies, and trends. Timely detection and diagnosis can reduce maintenance costs and assist with troubleshooting. The project employs three classes of methods to facilitate the construction of such applications: <br\/><br\/>1. It provides a general-purpose information processing architecture that combines continuous query processing and signal processing on sensor data streams. <br\/>2. It employs in-network processing algorithms based on probabilistic models and wavelets that adapt to prevailing network conditions. <br\/>3. It uses a multi-hop wireless data delivery system that provides efficient and fair network resource allocation in the face of intermittent connections via diverse links (e.g., 802.15.4, 802.11, BlueTooth, and others)<br\/><br\/>The WaveScope infrastructure allows engineers and scientists to rapidly develop high data rate sensor-based applications. Target WaveScope-based systems include automotive sensor networks and industrial equipment monitoring.<br\/><br\/>In addition to research papers and impact on courses, expected results include:<br\/> High data-rate protocols and information processing architecture.<br\/> Open-source software that runs on commercial hardware and allows others to quickly develop and deploy high data-rate sensing applications. <br\/> A real-world deployment and evaluation in one of the target application areas.","title":"NeTS-NOSS: WaveScope - An Adaptive Wireless Sensor Network System for High Data-Rate Applications","awardID":"0520032","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["508283","525076"],"PO":["564777"]},"107717":{"abstract":"Intellectual merit<br\/>The proposed research described in the .rst bullet above will substan-<br\/>tially extend our current understanding of the information-theoretic capabilities and limitations of<br\/>quantum learning algorithms.The proposed research described in the second bullet will extend our<br\/>understanding of what can be learned by computationally e .cient quantum algorithms.<br\/><br\/>Broader Impact<br\/>This research plan is closely integrated with a plan to achieve broader impact through<br\/>education which involves training students and broadly disseminating research results.Highlights<br\/>of the plan to achieve broader impacts include:(a)developing advanced courses in computational<br\/>learning theory at Columbia University (these courses will be largely project-oriented,and the<br\/>PI will encourate students to pursue projects at the intersection of learning theory and quantum<br\/>computation);(b)advising and guiding graduate students in their development as researchers and<br\/>educators,and actively working with advanced Ph.D.students to perform the proposed research;(c)<br\/>widely disseminating the results of the PI 's research through a range of di .erent mediums,including<br\/>talks,conference and journal publications,and survey articles on quantum computational learning.","title":"QnTM: Quantum Computational Learning","awardID":"0523664","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":["550576"],"PO":["565157"]},"107838":{"abstract":"ABSTRACT<br\/>0524545<br\/>Zhong Shao<br\/>Yale University<br\/><br\/>CT-ISG: Modular Verification of Concurrent Assembly Code<br\/><br\/>Proof-carrying code (PCC) is a general framework that can in principle verify safety properties of arbitrary machine-level programs. Existing PCC systems and typed assembly languages (TAL), however, can only handle sequential programs. This severely limits their applicability since many real-world systems use some forms of concurrency in their core software. This proposed research focuses on developing new techniques for certifying low-level concurrent programs. The PI will also develop new instructional material and tools for disseminating his research results to the general public.<br\/>Hoare logic can be combined with the assume-guarantee paradigm to reason about high-level concurrent programs but it does not support well low-level features such as first-class code pointers, unbounded<br\/> dynamic thread creation and termination, sharing of code between threads, and non-atomic machine code blocks. Typed assembly language provides a more modular and scalable framework but it can certify simple type safety only. The proposed research will show how to combine the strengths of the two to build a powerful new framework for specifying, composing, and verifying advanced properties on low-level concurrent code. The results from this research will provide a foundation for certifying realistic multi-threaded programs and make an important advance toward generating proof-carrying concurrent code.","title":"CT-ISG: Modular Development of Certified Concurrent Code","awardID":"0524545","effectiveDate":"2005-08-15","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}}],"PIcoPI":["550517"],"PO":["564388"]},"107728":{"abstract":"Carnegie Mellon University Better understanding of the processes involved in the physiology of bacteria can potentially have tremendous impact on both therapeutic approaches to infectious diseases and metabolic engineering applications in biotechnology.<br\/><br\/>In this project, Drs. David L. Wild and Matthew J. Beal, of the Keck Graduate Institute in Claremont, California and the State University of New York in Buffalo, New York, respectively, are proposing to build statistical models of time series data, with a view to leveraging sophisticated Bayesian methods to \"reverse-engineer\" an organism's complex genetic regulatory networks from the raw measurements of gene expression and metabolite concentration.<br\/><br\/>Drs. Wild and Beal will apply their techniques to an ideal experimental system: the response of the bacterium E coli to acid stress, which enables pathogenic E. coli to survive passage through the acidic environment of the stomach and gastro-intestinal tract. They will collaborate with experimentalists Drs. Francesco Falciani and Mark Viant at the University of Birmingham, UK, who will provide data from both pathogenic and non-pathogenic strains of this bacterium. Predictions made by Wild and Beal's models can then be tested and explored back in the laboratory.<br\/><br\/>Recent advances in functional genomics technologies have given biologists unprecedented access to measurements of the inner workings of complex biological organisms. Using microarray expression profiling, it is now possible to measure the expression levels of tens of thousands of genes in just a single biological experiment, conducted over several days in the form of a time series. Contrast this to the situation only ten years ago when it was rather unusual for a biologist to measure the expression of more than just one or two carefully chosen genes. As well as high-throughput gene expression methods, the new technology of \"metabolomics\" has opened the door to measuring even more information in the form of the concentration of hundreds of metabolites that are also crucial players in the complex cellular processes under study.<br\/><br\/>This overwhelming amount of data challenges traditional methods of analysis, especially when one considers the element of time, because now one must consider how certain genes regulate the expression of other genes from one time point in the experiment to the next. <br\/><br\/>A key ingredient in Drs. Wild and Beal's models is the inclusion of \"hidden factors\" that help to explain the correlation structure of the observed measurements. These factors may correspond to unmeasured quantities that were not captured during the experiment and often reduce the number of direct gene-to-gene dependencies, leaving the resulting networks much more interpretable for the biologist. A natural question arises: how many hidden factors should be used to account for the dependencies in the observed data? This is answered by employing Bayesian model selection, a well-founded principle used in machine learning and statistics to choose between models of differing complexities. Their models also use a technique called Automatic Relevance Determination to further simplify the models so that only those genes and metabolites that are participating players in the process are retained in the final model.<br\/><br\/>Another advantage of the Bayesian framework is that existing information about known network connections and interactions, derived from the literature or commercial databases, can be included in the model. The output of the modeling procedure is a probabilistic reckoning of which genetic regulatory networks are plausible or not. These probabilities can be used to design future biological experiments targeted at specific genes, with a view to corroborating the model's in silico predictions or to simply probe a relatively uncharted network.","title":"BiComp: Nonparametric Bayesian Models for Genetic Variations and Their Associations to Diseases and Population Demography","awardID":"0523757","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["518220"],"PO":["565223"]},"100964":{"abstract":"Disruptions occur often even in well-managed wired networks due to various failures and more so in emerging wireless networks due to additional causes such as external interference. With the increased use of the Internet for many real-time business applications, there is a growing demand for high service availability despite such disruptions. The goal of this project is to develop resilient routing schemes that mitigate the impact of disruptions on mission-critical applications and services. Specifically, this project aims to: (i) devise a failure inferencing based fast rerouting scheme to provide failure protection similar to MPLS fast-reroute without altering the destination-based forwarding paradigm prevalent in IP networks; (ii) design scalable localized on-demand link state routing schemes that ensure reliable delivery in spite of disruptions without requiring accurate state at each node; (iii) explore ways to utilize both topology and position information for handling disruptions; (iv) extend and apply these methods for load balancing and resilient multicasting. This project will boost the robustness of the Internet and enable services that improve the lives of everyone. It also has significant educational value in addition to facilitating knowledge sharing and distance education as a result of an always-available Internet. The results from this project will be assimilated into advanced graduate level and introductory undergraduate level theory courses as well as hands-on laboratory courses. Furthermore, the research advances will also be disseminated to the industry and the broader community through several publications and prototypes.","title":"CAREER: Disruption Tolerant Routing in Wired and Wireless Networks","awardID":"0448272","effectiveDate":"2005-08-15","expirationDate":"2011-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4090","name":"ADVANCED NET INFRA & RSCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["451202"],"PO":["557315"]},"105925":{"abstract":"Mobile ad hoc networks (MANETs) are highly desirable in areas\/situations where base stations are unavailable or too expensive to establish. Lack of infrastructure, intermittent connectivity, and frequent changes in topology due to mobility present significant challenges in the research of MANETs. While most prior efforts were primarily focused on the networking layer, there are compelling reasons supported by recent information theory and signal processing studies which indicate that an integrated approach seeking cross-layer diversity exploitation can lead to more fruitful results. This project follows that path. <br\/><br\/>The objective of this project is to develop a framework of communication, networking, and signal processing techniques for MANETs by exploiting node cooperation at the physical, medium access, and networking layers. Three different but intertwined research directions are involved, Specifically, the first is to develop a distributive modulation theory over wireless relay channels, covering investigation of the characteristics of wireless relay channels, modulation and detection for coherent, differential and non-coherent communications for relay networks, power allocation and placement of relay nodes. The second direction is to develop bandwidth-efficient and delay-tolerant cooperative coding schemes to address several issues that are unique in coded cooperation, including bandwidth expansion caused by repetitive transmissions from relays, node asynchronism due to distributive locations of relays, and scalability (viz., the capability to degrade gracefully when some cooperating nodes that implement a distributive code fail because of fading). The third direction is to explore networking by parallel relays and develop networking protocols that take into account the realistic characteristics of radio signals and provide a flexible framework for diversity exploitation.","title":"Collaborative Research: Signal Processing in Wireless Ad Hoc Networking","awardID":"0514736","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["400450"],"PO":["564898"]},"104726":{"abstract":"The technological frontier and performance barrier for control and management of<br\/>many present-day engineering systems lie in their complexity. Consequently,<br\/>complexity management has become an essential part of control systems design. An<br\/>emerging approach to controlling such systems consists of a decomposition of the<br\/>control actions into a sequence of modes, each of which is defined for a<br\/>particular task, operating point, or data source. The central question is how to<br\/>schedule the various modes in order to optimize the system's performance.<br\/>Related questions concern the development of real-time algorithms for<br\/>performance improvement and the tradeoff between the size of the mode set and<br\/>the system's performance.<br\/><br\/>This project will answer the above questions by casting them in the setting of<br\/>optimal control, by combining techniques from hybrid systems, motion description<br\/>languages, and numerical optimization. This new approach to controlling complex<br\/>systems will advance the state of the art of supervisory controller design by<br\/>providing effective algorithms for off-line computation of optimal controls, and<br\/>for real-time implementation of suboptimal controls.<br\/><br\/>On the technological side, the framework for optimal timing control of<br\/>multi-modal systems has far-reaching implications in a variety of application<br\/>domains. For instance, optimal scheduling of robotic tasks can be crucial for<br\/>the success of autonomous planetary explorations, mine sweeping,<br\/>search-and-rescue applications, and other military missions. On the educational<br\/>side, a new controls curriculum will be developed at the Georgia Institute of<br\/>Technology that combines computer-science techniques with the traditional<br\/>approach to signals and systems.","title":"CSR-EHS: Optimal, Multi-Modal Control of Complex Systems","awardID":"0509064","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["526932","553540"],"PO":["561889"]},"106926":{"abstract":"We will soon enter an era of 10 GHz processors and 10 Gigabit networks, but will have no effective means of using that processing power to deliver that level of throughput for regular applications. This gap lays in the \"service stack,\" which includes the application-level software, the operating system, and the networking code used by networked systems (Web servers, mail servers, caching Web proxy servers, Grid services, etc). Ideally, the performance of these systems would improve correspondingly with faster CPUs, but actual improvements may be significantly lower for a variety of reasons, including operating system limitations, slower improvements in disk performance, poor use of the CPU's features, etc. This work examines the service stack from the CPU's perspective to understand how the mismatches between it and modern processor features reduce delivered performance. The ultimate goal is to develop Grid\/networking applications that scale with processor speed, and to make it easier for developers to improve performance. <br\/><br\/>Intellectual Merit: The principal investigators (PIs) focus on three areas:<br\/>1. Improving the tools for finding performance problems across boundaries, such as those between programs and the operating system, and between the operating system and the device drivers for the hardware; <br\/>2. Using these tools to reorganize the networking code in the operating system to optimize common cases; and<br\/>3. Developing a system that can automatically customize the operating system to be tailored for specific programs, without requiring the developer to manually modify the operating system's source code. <br\/><br\/>The main area of investigation will be what the PIs term cross-boundary problems, where two systems\/layers that have no obvious flaws can perform poorly when used together. The performance problems generally stem from a mismatch of usage assumptions in the different layers. For example, Web servers using the \"event-driven\" design approach have shown exceptional performance and scalability on \"static\" content, such as regular files, and images. However, under such conditions, their design also requires the operating system to store a large amount of information about the program, and this information must be duplicated during certain processing steps when the server handles \"dynamic\" content, such as customized Web pages. This interaction causes their performance on dynamic content to drop, as the server gets busier. Other design assumptions in filesystems can cause all types of servers to unnecessarily block when searching for files, if other searches are in progress. This results in the CPU being underutilized for the duration of a disk access, which is a problem since CPU speeds have been growing faster than disk access times. As a result, the relative performance loss from this kind of problem will grow over time. Beyond developing a new network service stack that has higher performance and is designed to better utilize the CPU, the other focus of this work is examining the process of this kind of optimization. <br\/><br\/>Broader Impact: The PIs will use summer internships both to attract traditionally under-represented groups, but also to have developers the PIs can closely observe as they use the system. The focus of the internships will be either to optimize some open-source system, like the MySQL database or the Squid Web proxy server, or to expand the tools themselves. By using developers with various levels of expertise, the PIs can gauge how the tools can reveal the most promising areas for further investigation. The PIs can also incorporate the feedback from these experiments into the interface for using these tools, not only to increase interactivity, but also to help non-experts use them.","title":"NeTS-NBD: Bridging the 10 GHz \/ 10 Gbit Gap: Whole-system approaches for scalable networked services","awardID":"0519829","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["533299","560046"],"PO":["565090"]},"105716":{"abstract":"The computational comparison of variations among genomic sequences sampled from a large number of unrelated individuals in a population is a very powerful way to address both fundamental and applied biological questions. The best known questions concern the location of genes and mutations that contribute to disease incidence and to variation in economically important traits. Nature and history have created a large variety of mosaic genomes among individuals and populations who can be studied today. The grand challenge is to exploit these natural experiments by finding patterns in and among the different mosaic genomes (the genotypes) that have significant and biologically meaningful associations with important traits (the phenotypes) of interest. With genomic level technologies, the needed data on population level variation is becoming available but challenging problems remain in the analysis of the data.<br\/><br\/>This proposal focuses on novel, critical computational problems that arise in population-scale genomic data acquisition and analysis. The algorithmic problems of concern are divided into biology-based problems and technology-based problems, but the interplay of technology and biology is critical. This research will be conducted by an interdisciplinary group of computer scientists, mathematicians and geneticists. The main biology-based algorithmic problems concern the computational deduction of the frequency, location, and the full temporal structure of historical recombination, gene-conversion and lateral gene- transfer. The main technology-based problems are concerned with important problems of missing data or error-prone data, and with the deduction of haplotype data from genotype data. The problems of missing or error-prone data are approached through the use of optimization techniques. The haplotype deduction problem uses a variety of techniques, based on exploiting more complete and realistic biological models of how the underlying haplotypes have evolved. One element is the incorporation of recombination into the models, connecting previous work on constructing histories of recombinations with work on deducing haplotypes from genotypes.<br\/><br\/>The algorithms and software will allow biologists to better understand the history<br\/>and role of recombination, gene-conversion and lateral gene transfer, and to cope with problems in the data. As just two examples, the tools could facilitate the tasks of gene finding by association mapping, and in understanding how lateral gene-transfer helps bacteria to rapidly develop antibiotic resistance. The impact will be enhanced by our associated educational and outreach efforts.","title":"SEI(BIO): Computational Population Genomics: Using Variation to Connect Genotypes to Phenotypes","awardID":"0513910","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7294","name":"SCIENCE & ENGINEERING INFORMAT"}}],"PIcoPI":["518691","447975"],"PO":["565136"]},"106948":{"abstract":"Effective methods of network congestion control and avoidance are important for the successful co-existence of different applications running on a time-varying network with shared resources such as the current Internet. In particular, congestion control and avoidance methods govern the means by which an application sending packets across a network adjusts its sending bit-rate in order to avoid or react to network congestion indicators in the form of lost packets, increased packet transport delay, or explicit congestion warnings from internal network routers. Clearly, with different applications all sharing the same network resources, congestion control methods that allow for the efficient and fair allocation of network resources without requiring a centralized authority for determining the allocations are needed. <br\/><br\/>This research takes a new approach to the analysis and design of congestion control. Through the use of network calculus notions, which do not impose overly restrictive modeling assumptions on traffic and service endemic in stochastic modeling approaches, this research will provide new analytical guidance and practical methods for congestion control. Through the use of recently developed methods for handling feedback with time-variant delays, optimal congestion controllers for different delay-based congestion avoidance control objectives can be derived. Preliminary results indicate that an approximation of a certain optimal non-causal controller provides better throughput or fairness than any other existing delay-based scheme. Building upon this preliminary work, the investigators will examine the basic problem of delay-based congestion avoidance for both window and variable bit-rate controllers, providing analytical guidance in the form of optimal controllers that can be approximated, leading to practical implementations. In addition, the research will explore the joint use of both delay and packet loss\/marking feedback together within a congestion control context. This will ultimately lead to the consideration of the joint design of congestion window controllers and active queue management methods within the network calculus framework. As the network calculus based controllers must be approximated, the issue of control parameter estimation will be considered within a statistical signal processing context. <br\/><br\/>Broader Impact: This research will be performed at the University of Miami, which is recognized as a Hispanic Serving Institution and Postsecondary Minority Institution, meaning that the investigators will be able to involve underrepresented minorities in their efforts to integrate research and education.","title":"NeTS-NBD: Illuminating Congestion Control: Analytical Guidance and Practical Implementations","awardID":"0519933","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["560250","560252","385516"],"PO":["565090"]},"105529":{"abstract":"ABSTRACT<br\/>0513179<br\/>Wei Cai<br\/>U of NC @ Charlotte<br\/><br\/>This interdisciplinary proposal is to develop high order numerical methods to provide accurate modeling capabilities of light propagation through microphotonics. The problems of light propagation through such devices are closely related to the design of delay lines, optical buffering devices. Due to small scales of those devices and wave nature of the light signals, the accuracy of the numerical methods, especially the phase accuracy of the numerical methods, is critical in obtaining the speed and phase information of light signals through photonic devices. <br\/>The development of algorithms for modeling of microphotonics such as resonant waveguides will result in advanced capabilities in solving linear and nonlinear Maxwell equations in inhomogeneous media for a wide range of engineering problems. The potential technology applications of this research will provide integration of optical elements on a single chip, to control velocity of light, to provide routing and switching functionality on a micro-scale by incorporating nonlinear optical material into the microspheres\/microcylinders. These are the fundamental questions being addressed in the research communities of modern photonics. The major challenge will be the development of highly accurate and efficient numerical algorithms for the solution of linear and nonlinear Maxwell equations in layered and inhomogeneous media. The following topics will be studied: (a) Discontinuous spectral element methods for time dependent nonlinear Maxwell equations, (b) Upwinding Embedded Boundary Methods, (c) Modeling with the developed algorithms for coupled resonator optical waveguide devices.<br\/>As a main goal of this proposal, we plan to find solutions to the current bottleneck problems in the designing of coupled resonator waveguides with significant impact on the development of next generation optical technologies. This includes optimization of the nanometric separation between microspheres or microcylinders to achieve a trade-off between reduced group velocity of light (desirable property for optoelectronic applications) and reduced efficiency of optical transport. This also includes understanding of the role of the size disorder existing in the presently available ensembles of microspheres and microcylinders. The results of this proposal will be directly implemented into the manufacturing of photonic devices of coupled microspheres or microcylinders in our laboratory. In addition, publicly available codes will be created for calculating of all types of optical spectra (transmission, reflection and scattering) and photonic band structures of coupled resonator waveguides. <br\/>One graduate student will conduct research toward a Ph.D. degree in either optics or\/and applied mathematics, and his\/her participation will contribute to the educational components of the newly established Center of Optoelectronics and Optical Communications at the UNC Charlotte. Research results from this proposal, in the area of new physics and mathematical modeling tools, will be incorporated into the optics curriculum now under development at the Center, potential technology transfer of the results to the area optics industrial will be explored through the existing partnership between the Center and area optics companies. The PIs will also actively participate in the Center's technology training programs with the area high schools.","title":"High Order Numerical Methods for Light Propagation in Micro-Photonics","awardID":"0513179","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":["542982","377744"],"PO":["432103"]},"107718":{"abstract":"The scientific goal of this proposal is to demonstrate and study techniques that will be important for building networks that can reliably transmit quantum information over macroscopic distances. Neutral atoms that have been laser cooled and stored in optical traps have low decoherence rates and are well suited for storing quantum information in the form of qubits. In order to transfer information between macroscopically separated neutral atom qubits the information can be mapped onto photons that are emitted by one atom and detected by the receiving atom. As individual atoms have small cross sections for photon generation and detection we have proposed and will demonstrate coupling of single atoms to mesoscopic many atom qubits, followed by efficient generation of photonic qubits by the mesoscopic ensemble. The emitted photon is then detected by a receiving ensemble, which is in turn coupled to a single atom qubit. Combining these elements will enable a quantum channel that connects single atom qubits. The quantum channel can be used for transmitting quantum information and for creating distant bell states that can be used for teleportation of the atomic states. <br\/><br\/>Our experimental approach is based on using single atoms as well as ensembles of atoms stored at high densities in optical traps. The atoms are laser cooled to kinetic energies of a few micro Kelvins. Coherent laser techniques are used for qubit manipulation as well as excitation of strongly interacting Rydberg states. The Rydberg states are used to couple information between single atoms and many atom ensembles. Coherent manipulation of the ensembles with several laser beams results in deterministic emission of photons in a desired direction. <br\/><br\/>The intellectual merit of the proposed activity is in the study and demonstration of many particle entanglement. The presence of entanglement provides a sharp distinction between classical and quantum phenomena, and is fundamental to the computational power of quantum mechanical systems. This research will extend our ability to create and harness entanglement for controlling the flow of information and will demonstrate the possibility of entangling single atom qubits with mesocopic qubits.<br\/><br\/>The broader impact of the proposed activity will include contributions to the development of quantum techniques for computing and communication. These quantum mechanical approaches have the potential for unprecedented computational power, as well as secure transmission of information. In addition the research to be performed at The University of Wisconsin - Madison will expose undergraduate students, graduate students, and postdoctoral researchers to state of the art techniques and tools, and train them to contribute to the technological and scientific development of society. We have consistently had strong involvement of undergraduate students in our research on this topic and will continue to do so in the proposed work.","title":"QnTM: Entanglement in mesoscopic atomic clouds and quantum networking","awardID":"0523666","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":["544345","515545"],"PO":["565157"]},"102901":{"abstract":"ABSTRACT<br\/>ARC #0457662<br\/><br\/>This project studies patterns of migration of North American arctic indigenous people between rural communities, larger regional centers, and urban areas over the past several decades. It has four primary<br\/>research objectives: (1) develop improved methods for analyzing migration decisions of individuals participating in mixed subsistence and cash economies; (2) apply these methods to improve understanding of<br\/>Inuit migration decisions in a comparative multi-decadal study of Alaska and arctic Canada; (3) develop and make available to other researchers metadata for research and policy applications; and (4) involve arctic local governments in policy-relevant research.<br\/><br\/>The PIs address questions about the causes and consequences of migration such as the roles of subsistence opportunities and community quality of life amenities, gender differences, and national policies on migration decisions. Comparing the Inupiat regions in Alaska to the Nunavut Territory of Canada, the researchers ask whether Canadian Inuit are less mobile than Alaska Inupiat; and if so, to what extent can this be attributed to differences in policies in the two nations? They also investigate the long-term consequences of migration decisions: is mobility on balance improving living conditions in arctic communities, especially the poorest places, or is it draining leadership to larger settlements and exacerbating inequalities?<br\/><br\/>Working with local participating organizations, the researchers are developing research protocols for analyzing microdata collected from the late 1970s to the present, including the US Census, the Survey of Living Conditions in the Arctic, North Slope Borough Censuses, Statistics Canada's Aboriginal People's Survey, and other household survey data from Nunavut and Alaska. A key step in the research is the creation of a new large-sample household-level dataset from 1990 and 2000 Decennial Census Long Form data, in cooperation with the US Census Center for Economic Studies.","title":"Migration in the Arctic: Subsistence, Jobs, and Well-Being in Urban and Rural Communities","awardID":"0457662","effectiveDate":"2005-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"I331","name":"Defense Intelligence Agency"}},{"dir":{"id":"14","name":"Office of OFFICE OF POLAR PROGRAMS                ","abbr":"OPP"},"div":{"id":"1401","name":"Division of ARCTIC SCIENCES DIVISION","abbr":"ARC"},"pgm":{"id":"5205","name":"ARCTIC RESRCH SUPPRT & LOGISTI"}},{"dir":{"id":"14","name":"Office of OFFICE OF POLAR PROGRAMS                ","abbr":"OPP"},"div":{"id":"1401","name":"Division of ARCTIC SCIENCES DIVISION","abbr":"ARC"},"pgm":{"id":"5221","name":"ARCTIC SOCIAL SCIENCES"}}],"PIcoPI":["560038","497236"],"PO":["564332"]},"104837":{"abstract":"The proposed work will develop, implement, and evaluate new techniques that help to automate performance analysis of modern software systems. The methodology pursued breaks down the problem of automating performance analysis into three components: identifying performance anomalies, detecting covariation between performance metrics, and determining causality between covariant metrics. The proposed approach uses statistical data mining and machine learning techniques to automate the three components. The proposed system works on a collection of traces, with each trace containing one or more streams of measurements from a performance metric.<br\/><br\/>The project will bring techniques from the statistical data mining and machine learning techniques to bear on the problem of automating performance analysis. The fundamental insight of the proposed approach is that there is significant information in the time-varying contours of a stream. Previous statistical approaches to performance analysis have ignored this information in favor of examining the covariation across metrics at a particular snapshot of time.","title":"CSE--SMA: Understanding the Performance of Modern Systems","awardID":"0509521","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7469","name":"ITR-HEC"}}],"PIcoPI":[276782,"461768",276784],"PO":["493916"]},"107807":{"abstract":"Proposal Number: 0524269<br\/>Title: DoS Prevention in Shared Channels<br\/>PI: Carl Gunter<br\/><br\/>Methods for analyzing and preventing Denial of Service (DoS) threats are of fundamental value for designing robust Internet protocols. Much work has been done to develop pragmatic solutions to protocol-specific DoS threats, but there is a lack of realistic theoretical models for studying DoS and of broad paradigms for designing DoS-resilient protocols. This project develops theoretical models based on a \"shared channel model\" which describes how adversaries and valid senders share the network bandwidth of attack targets. It exploits this model to design counter-measures based on a paradigm in which asymmetries in protocol workloads that are exploited by adversaries are systematically converted to the advantage of trusted parties. Specific project goals include developing (1) general techniques for obtaining DoS-resilience that can be used to adapt existing protocols or create new ones; (2) ways to automate DoS analysis of protocols to reduce the effort required to confirm practical availability properties theoretically and find unexpected attacks before protocols are deployed; and (3) a unified model of integrity, confidentiality, and availability based on both existing algebraic techniques and new probabilistic techniques.","title":"Collaborative Research: CT-T: DoS Prevention in Shared Channels","awardID":"0524269","effectiveDate":"2005-08-15","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["496297","497006",284770],"PO":["529429"]},"104739":{"abstract":"As software continues to grow in complexity, an important requirement that is emerging is software must be able to change at run-time. Unfortunately, software engineering techniques that are useful in building robust dynamically modified software are woefully lacking. In general, extensive testing and debugging are the traditional ways of ensuring the robustness of sofware. The problem of debugging a program where the underlying code is changing at run-time makes the problem all the more difficult. <br\/><br\/>The aim of this proposed research is to address this situation and develop new techniques for debugging dynamically modified software. The key idea is that debugging for such software must extend through the code modification and allow debugging on code that has been changed and to see how past and future adaptations has\/can affect execution. The proposed work considers several types of dynamic code modifications, including dynamically optimized code, dynamically applied code patches, components and dynamically linked libraries. New debug directives and queries will be developed specifically to address the challenges of debugging such code. Novel techniques based on code analysis, reverse execution, checkpointing, and instrumentation optimization will be used to enable these new directives and queries.<br\/><br\/>This research has both fundamental and software contributions, including: (1) a better understanding of the constraints and trade-offs that exist for developing robust dynamically modified software, (2) a framework that supports the construction of debugging techniques and tools that are useful when developing dynamically software, (3) the development of debug strategies, directives and queries that are designed to handle the special challenges of dynamically modified code, and (4) the development of a set of debugging tools for dynamic software that will be widely distributed. The techniques, tools and algorithms that will be developed through the course of this research will contribute significantly to understanding how modern software development techniques can be incorporated with dynamic code modifications.","title":"CSR-AES: Collaborative Research: Debugging Dynamic Code Modifications","awardID":"0509115","effectiveDate":"2005-08-01","expirationDate":"2007-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["560516"],"PO":["551712"]},"107709":{"abstract":"Membrane computing, a relatively new branch of molecular computing, holds great promise for new paradigms for computation suggested by processes that occur in live cells. This area was recently selected by the Institute for Scientific Information (ISI) as a fast \"Emerging Research Front\" in Computer Science. The project has three major directions of research using tools and techniques from the membrane computing area. The first concerns the simulation of cells using concepts from P systems implemented in the E-cell package. To help the simulation of cells the second research area focuses on defining models of P systems that mimic the processes in the cell. The PIs have already, as a preliminary result, one such example of a model of P systems that has several features not found in any other previous P system models: binding of objects and genetic information. Another major novelty of the proposed model is that time is associated with each rule. The PIs plan to search for other suitable models, and also work on the more theoretical aspects of the P systems: developing efficient algorithms (e,g., considering reachability between system configurations), and investigating important and fundamental computational issues in the models considered. The third area complements the first two: research in the area of P systems simulators. While in the first area the interest is in simulating cells using, among other ideas, the models developed in the second area; the third research focus goes in the opposite direction - simulation of P systems rather than the simulation of cells. This area of research complements the first two, since such a simulator for a bio-relevant P system could answer practical questions raised from biology. An ambitious goal of the whole project is to capture in these mathematical models the way the cells self-configure and self-maintain and view it as computation. These investigations could yield new ideas and useful models for computation inspired by the extraordinary system that is a cell. The proposed project topic is new, with great promise to significantly impact several areas. In Biology the project will help explore, model and simulate differential gene expression during various biological processes and transcriptional and signaling networks on a larger scale than the current capabilities; in Computer Science the research could yield new paradigms and new computing techniques (as has happened in the discovery of genetic algorithms and neural networks). One of the major contributions of this project will be the applications of a good cell-simulator in many areas - such a software would be an invaluable tool for designing and testing new drugs; for predicting the behavior of a specific type of cell in a specific environment; and, lastly, for teaching and gaining more knowledge and insight about the extraordinary systems that we call \"the cells\".","title":"BioComp: Collaborative Research: P Systems: Theory and Applications to Modeling and Simulation of Cells","awardID":"0523572","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["485879"],"PO":["565223"]},"110515":{"abstract":"This project is developing novel, scalable techniques to mine massive astronomical survey databases (66 million lightcurves now available, growing to ~100 billion in a decade) for the signatures of planets around other stars. Giant astronomical surveys will soon monitor much of the sky regularly, detecting vast numbers of interesting, variable astronomical objects. These objects will range from small solar system bodies to distant quasars; in the middle of the range of scales the surveys will find planets around other stars when they transit (pass in front of) their parent stars. The objective of this proposal is to get ready for this new treasure trove. <br\/><br\/>Specifically, this project is: (1) developing new, scalable techniques to identify extra-solar planets in synoptic survey data, and (2) preserving novelty detection when the volume of data becomes so large that the scientists are \"removed\" from the data. The goal is to discover new extra-solar planets.<br\/><br\/>The outcome of this project will be new techniques for mining these extremely massive databases, and a preliminary survey for candidate planets. This will enable follow-up work to scour new data as they become available. The datamining tools will have impact that extends beyond the needs of astronomy. The screening of time-series data for interesting events occurs in many arenas, ranging from national defense to the tracking of commercial transactions.","title":"Collaborative Research: SGER: Mining for Planets","awardID":"0540921","effectiveDate":"2005-08-01","expirationDate":"2006-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7294","name":"SCIENCE & ENGINEERING INFORMAT"}}],"PIcoPI":["471355"],"PO":["565136"]},"110669":{"abstract":"Computer system reliability and robustness depends on operating system<br\/>correctness. An incorrect operating system is vulnerable to random crashes<br\/>or, worse, attack, where one program corrupts another program's execution.<br\/>Thus, the longstanding goal of a verified operating system: one whose<br\/>correctness is proved beyond doubt. Although aspects of operating systems,<br\/>such as interactions with memory hardware, are currently hard for even<br\/>advanced automatic verifiers, coordinated kernel interface changes and<br\/>verification advances may be able to break this impasse. This exploratory<br\/>research program addresses several basic issues in kernel verification.<br\/>Advances are made both to the BLAST lazy predicate abstraction verification<br\/>tool, and to a small, readable kernel specially designed for verification.<br\/>Particular advances include specialized types and abstractions for<br\/>bit-packed structures and for unbounded data structures, and transactional<br\/>kernel interfaces. All tool advances will be made publicly available.<br\/>This program will clear the obstructions to a more ambitious project: the<br\/>construction of a fully verifiable kernel. Its success will connect the<br\/>operating systems and verification communities, leading to more reliable,<br\/>dependable systems and system designs.","title":"(SGER) Preliminary Steps Toward a Verifiable Kernel","awardID":"0541606","effectiveDate":"2005-08-15","expirationDate":"2006-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"9199","name":"UNDISTRIBUTED PANEL\/IPA FUNDS"}}],"PIcoPI":["541961","460594","529115"],"PO":["309350"]},"110328":{"abstract":"This research investigates a way to efficiently produce realistic computer graphics images. Applications of computer graphics are placing an ever-increasing demand on the ability to produce accurate, convincing images of complex scenes. Examples of such applications include architectural and engineering design, virtual training, telecollaboration, and game and movie rendering. However, much of the scene complexity is usually irrelevant to how humans perceive the rendered image. The approach explored by this research is<br\/>to exploit the limitations of the human visual system by automatically focusing computational effort on the visual features that are important for convincing the eye, while saving time where the eye would be insensitive to the difference. This research develops new, feature-based computer graphics rendering techniques that more efficiently handle the large, complex, realistic scenes needed by future applications.<br\/><br\/>The goal of the research is a scalable, feature-based graphics pipeline that exposes features explicitly at every level from modeling to the final rendered image. If computational work is proportional to visual features, the computational cost is proportional to the intrinsic visual complexity of the output image rather than to other measures of scene complexity such as polygon count. Such a graphics pipeline is fundamentally more scalable. The researchers are investigating efficient algorithms for finding visually important features, new feature-based scene and display representations for high-quality modeling and display, and new rendering algorithms that exploit features to provide scalable, efficient, high-quality image synthesis.","title":"Feature-based Rendering","awardID":"0539996","effectiveDate":"2005-08-15","expirationDate":"2006-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}}],"PIcoPI":["508159"],"PO":["532791"]},"104839":{"abstract":"Security holes and crashes at the operating system (OS) level<br\/>represent a serious infrastructure problem. These OS bugs are often<br\/>the result of memory management errors, which are very hard for<br\/>developers to avoid. In the long run, a broad class of<br\/>memory-management errors can be prevented by using a high-level,<br\/>type-safe language, but whether these languages are suitable for<br\/>kernel implementation remains an open question. Unfortunately, any<br\/>given experiment to answer the question (i.e., using a particular<br\/>high-level language) involves many differences in implementation<br\/>compared to a conventional kernel, making it difficult to draw<br\/>conclusions about which parts of the high-level approach work and<br\/>which parts do not. This project is an experiment specifically about<br\/>garbage collection for legacy OS kernels, changing as few other<br\/>implementation issues as possible. Concretely, the research is about<br\/>developing tools to automatically transform the C source code of a<br\/>conventional kernel so that it is compatible with a variety of precise<br\/>garbage-collection strategies. The project's ultimate goal is to check<br\/>whether the OS becomes usefully more reliable as a result of garbage<br\/>collection, and to measure the performance costs in both desktop and<br\/>embedded environments. If the experiment shows that the OS improves at<br\/>a reasonable cost, then the research will have shown how to increase<br\/>the reliability of a major part of today's computer systems<br\/>infrastructure. Tools generated by the project will be made publicly<br\/>available for use in research, for application to practical software,<br\/>and for pedagogical purposes.","title":"PDOS: Experimenting with Garbage Collection in an Otherwise Conventional OS","awardID":"0509526","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["518130","550238"],"PO":["535244"]},"106919":{"abstract":"Traditional networks, both wired and wireless, have the property that end-to-end paths between nodes are relatively stable. However, not all environments that require communication will allow the creation of stable end-to-end paths. For example, the aftermath of a severe earthquake disaster will include collapsed buildings, persons trapped in debris, damaged utilities and roads, as well as fires and secondary explosions. Under this situation, the ability to communicate, even at low rates, is extremely valuable for sharing vital information, such as the number and location of survivors and the activities of rescue workers. To provide communication in these challenged environments, the network protocols must be explicitly designed to perform despite frequent disruptions in the availability and performance of network components (i.e., links and nodes). The resulting systems are termed Disruption Tolerant Networks (DTNs).<br\/><br\/>This work focuses on the construction of DTNs that go far beyond the task of finding unicast paths. Instead, these DTNs are robust under uncertainty and attack, and are highly efficient in their use of the node and link resources. Specifically, this project focuses on the following fundamental functions: group communication, single-hop transfers, and power management. The work is grounded by experimentation in testbeds at the collaborating universities. The project provides algorithms, protocols, and platforms to create robust and efficient DTNs enabling communication in the most critical of environments, as well as exposes cost-performance tradeoffs that might have broader applicability in less challenged networks.","title":"Collaborative Research: NeTS-NBD: Construction of Robust and Efficient Disruption Tolerant Networks","awardID":"0519784","effectiveDate":"2005-08-15","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["550428","550429"],"PO":["557315"]},"105709":{"abstract":"The Earth System Curator collaboration will unify the treatment of models and datasets relating to climate change by developing a common language - a metadata formalism - with which to describe the two, and by prototyping a set of tools based on that formalism that allows researchers to manipulate models and datasets seamlessly and with ease. The goal, in the end, is to increase the productivity of climate researchers and understanding of the Earth system.<br\/><br\/>Collaborators include computer science and Earth science researchers at MIT, Princeton University, the Georgia Institute of Technology, and the National Center for Atmospheric Research. The work proposed builds on two ongoing community efforts, the Earth System Modeling Framework (ESMF) and the Earth System Grid II (ESG). The ESMF is a national initiative to develop common modeling infrastructure for the nation's climate and weather models, including coupling tools and standard modeling utilities. The primary objective of ESG is to make the output of high-resolution, long-duration simulations performed with climate models available to global change impacts researchers nationwide, through the use of Grid, data\/metadata, and portal technologies. The team will explore those aspects of ESMF and ESG that can be usefully aligned, and will prototype a new entity, the Earth System Curator, that spans the gap between the two.<br\/><br\/>The Curator begins with a crucial insight: that the descriptors used for comprehensively specifying a model configuration are needed for a scientifically useful description of the model output data as well. The development of a common metadata schema that describes both will be the basis for this unique and powerful community resource. The Curator will provide a community database from which researchers can archive and query a wide class of Earth system models, experiments, model components, and model output data and results. Researchers will subsequently be able either to analyze model output from pre-existing runs, or to access a model and modify and run it themselves, either on a local computer or on the virtualized resources of the computational Grid. In addition to the query function, the project will prototype a tool that will test if sets of model components or datasets can interact to form an application. Finally, as part of the Curator effort, tools for auto-generation of component wrappers and applications will also be prototyped.<br\/><br\/>The Curator is part of a community vision for the use of information technology in climate and related research. The Curator prototype will help to further suggest and define the form of next-generation modeling and data management tools, by offering a concrete representation to add credibility to innovative ideas. Further, the ESMF and ESG co-investigators, by virtue of their projects' emphases on production software and extensive customer bases, are in an excellent position to transition the Curator tools into a viable product following an NSF-funded prototype stage. Over the course of the Curator effort, many researchers associated with ESMF and ESG will be encouraged to try out and offer feedback on the Curator software. Advances achieved with the Curator project will influence other domains through conferences and publications, and through a web of relationships founded on a shared need for multi-component HPC modeling, ease of information archival and access, and similarities in simulation numerics. While the advances in climate prediction due directly to the Curator effort itself may be both difficult to track and modest, an integrated environment for Earth system research is critical to addressing world climate issues in the near future, and the Curator is a definitive step in that direction.<br\/> <br\/>The Curator will also be incorporated into the project infrastructure for software engineering courses at Georgia Tech. Appropriate introductory material will be prepared and project opportunities defined. Many of the project opportunities will take the form of making climate data accessible to the general public. In this way the Curator not only provides software engineering students an opportunity to participate in an actual ongoing engineering development effort, but the resultant projects will make Earth science more available to the general public.","title":"Collaborative Research: Earth System Curator: Spanning the Gap Between Models and Datasets","awardID":"0513841","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7373","name":"ITR-INFORMATION INTEGRATION"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"7231","name":"CYBERINFRASTRUCTURE"}}],"PIcoPI":["450514","560505"],"PO":["565136"]},"110539":{"abstract":"------<br\/>The proposed study will conduct a pilot research on facial expression representation, detection, tracking and classification in a complete 3D space. To prove the concept of 3D face expression recognition, the study will investigate the issue on how and what 3D facial features could make the 3D-based expression recognition better than 2D-based recognition. The analysis of 3D facial expressions will facilitate the examination of both the fine structural changes inherent in and the precise time course of spontaneous expressions. Although 3D modeling techniques have been extensively used for 3D face recognition and 3D face animation, barely any research on 3D facial expression recognition has been reported. A primary reason why such research has not yet been undertaken is that no 3D expression database exists. As a proof-of-concept, a state-of-the-art 3D dynamic facial expression database will be constructed to test the algorithms and make it public to the research community to foster the research in the field. The project aims to achieve a high rate of accuracy in identifying a wide range of facial expressions, with the ultimate goal of increasing the general understanding of facial behavior and 3D structure of facial expressions on a detailed level. As a result, this exploratory research will lead to building a humanized system for recognizing spontaneous facial expression, which is crucial to the next generation of human computer interaction, security, human affect recognition, biomedicine, law-enforcement and psychology research.<br\/><br\/>This project will facilitate research on the next generation of human-computer interaction through automatic 3D facial expression recognition. It will help increase the scientific understanding and modeling of cognitive processes of the human visual system. The resulting system and the 3D facial expression database will be valuable for applications in security, law-enforcement, biomedicine, behavior science, entertainment and education. The 3D face expression database will be made available to the entire research community. As an academic institution, SUNY Binghamton has a large and diverse student and staff population. This will facilitate scanning many people of diverse ethnic backgrounds for the database. The results of this research can be integrated into graduate and undergraduate courses at SUNY Binghamton related to image processing, computer vision, and computer graphics.","title":"SGER: Analyzing Facial Expression in Three Dimensional Space","awardID":"0541044","effectiveDate":"2005-08-01","expirationDate":"2007-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7339","name":"COMPUTER VISION"}}],"PIcoPI":["511701"],"PO":["317663"]},"110508":{"abstract":"This project is developing novel, scalable techniques to mine massive astronomical survey databases (66 million lightcurves now available, growing to ~100 billion in a decade) for the signatures of planets around other stars. Giant astronomical surveys will soon monitor much of the sky regularly, detecting vast numbers of interesting, variable astronomical objects. These objects will range from small solar system bodies to distant quasars; in the middle of the range of scales the surveys will find planets around other stars when they transit (pass in front of) their parent stars. The objective of this proposal is to get ready for this new treasure trove. <br\/><br\/>Specifically, this project is: (1) developing new, scalable techniques to identify extra-solar planets in synoptic survey data, and (2) preserving novelty detection when the volume of data becomes so large that the scientists are \"removed\" from the data. The goal is to discover new extra-solar planets.<br\/><br\/>The outcome of this project will be new techniques for mining these extremely massive databases, and a preliminary survey for candidate planets. This will enable follow-up work to scour new data as they become available. The datamining tools will have impact that extends beyond the needs of astronomy. The screening of time-series data for interesting events occurs in many arenas, ranging from national defense to the tracking of commercial transactions.","title":"Collaborative Research: SGER: Mining for Planets","awardID":"0540902","effectiveDate":"2005-08-01","expirationDate":"2006-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7294","name":"SCIENCE & ENGINEERING INFORMAT"}}],"PIcoPI":["365889"],"PO":["565136"]},"120838":{"abstract":"The objective of this project is to support the Data Grid infrastructure by developing new and original techniques for efficient storage, retrieval, and analysis of complex scientific data. The main focus is on the development of a highly scalable data engine geared toward the needs of analytical computing in Data Grid environments. However, keys to the realization of the project are the advances in the areas of indexing and clustering data in multi-dimensional spaces.<br\/><br\/>While the main goal of analytical computing in Data Grid environments is to facilitate hypothesis formulation or to test the validity of a postulated scientific model, its primary method is usually that of data clustering. Since typical analytical tasks also rely on ad-hoc data exploration, any data engine for Grid-enabled analytical computing must support an integrated set of different retrieval and clustering techniques. The data engine developed in this project will feature: an efficient and scalable indexing technique for data in high-dimensional spaces, which will include a practical solution for handling data with missing information; a new and original access method for similarity searching in multi-dimensional spaces; and an original technique for clustering large volumes of multi-dimensional data, which will require no dimensionality reduction.","title":"ITR: Development of a Data Engine for Grid-Enabled Analytical Computing","awardID":"0635365","effectiveDate":"2005-08-16","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":[321055],"PO":["563751"]},"95380":{"abstract":"This collaborative project \"TANGO\" is leveraging strengths of research teams at Brigham Young University (IIS-0414644, PI: David Embley) and Rensselaer Polytechnic Institute (IIS 0414854, PI: George Nagy). TANGO is a framework for organizing domain-specific factual data appearing in independently generated web pages. Algorithms and software are developed for extracting and interpreting individual lists and tables and integrating them with the contents of other tables that present partially overlapping information. The input pages may be HTML, PDF, PostScript, or scanned document image files. The output of the system is an ontology that provides the conceptual framework for the domain. The ontology evolves as web pages with additional relevant data are harvested and processed. Constructing ontologies is currently a labor and skill intensive process. The TANGO framework automates much of the ontology construction task for a domain when information exists in lists and tables that describe the domain. The results of this project build stepping stones to the Semantic Web, for which ontologies are a central component. Therefore, this project will have an impact on users of the Semantic Web, who assisted by the automated software agents developed in this project, can accomplish information-based tasks for research, scholarship, planning, design, education, and entertainment. This cross-disciplinary and cross-university endeavor introduces graduate and undergraduate students to cutting-edge research. Developed tools (including source programs), a corpus of tables both in raw and normalized forms, collected test data, and technical reports can be found at the project website (http:\/\/www.tango.byu.edu).","title":"Collaborative Research: TANGO: Table Analysis for Semiautomatic Generation of Ontologies","awardID":"0414854","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["371109"],"PO":["563751"]},"109073":{"abstract":"The ACM\/USENIX Conference on Mobile Systems, Applications, and Services, to be held in Seattle, WA from June 6th through the 8th, has established itself as the preeminent venue for mobile and wireless systems work. It is a single-track conference, with rigorous peer review, and an acceptance rate of less than 25%. A \"systems\" perspective is particularly important in the mobile setting, because (1) the changing mobile environment requires each component to adapt; doing so in isolation leads to inefficiencies or worse, and (2) applications at this level are generally dependent on context; something that cannot be determined at a single system layer. Training students in these and related issues is critical in developing the workforce expertise required to maintain leadership in this emerging area. Participation in events like this is an extremely important part of the graduate school experience, providing the opportunity to interact with more senior researchers and to be exposed to leading edge work in the field. The support provided enables the participation of students who would otherwise be unable to attend MobiSys 2005.","title":"Student Travel Support: ACM\/USENIX MobiSys 2005","awardID":"0532149","effectiveDate":"2005-08-15","expirationDate":"2006-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["530391"],"PO":["7594"]},"106050":{"abstract":"ABSTRACT<br\/>0515320<br\/>Shivkumar Chandrasekaran<br\/>U of California - SB<br\/><br\/>Collaborative Research: Super-fast direct sparse solvers<br\/><br\/>The numerical solution of partial differential equations (PDE) is a key enabling technology in all<br\/>disciplines of engineering and science. Nevertheless the numerical solution of three-dimensional<br\/>PDEs is a critical bottle-neck that prevents this potential from being realized. This proposal<br\/>advances techniques that can be used to overcome this bottle-neck. Discretized elliptic PDEs are normally solved by iterative schemes since the fill-in during sparse Gaussian elimination is excessive. This proposal observes that the fill-in, in a certain ordering, has low numerical rank in the off-diagonal blocks, and that this structure can be computed and exploited to construct direct solvers that are linear in the number <br\/>of unknowns. The outcome of the proposed research has the potential to create a novel class of pre-conditioners that in conjunction with iterative solvers can become powerful weapons for solving difficult elliptic PDEs.<br\/>The intellectual merit of the proposal stems from the complicated structure in the fill-in that<br\/>must be first inferred from regularity results for Green's functions in elliptic PDE theory and then<br\/>converted into effective linear-time algorithms to both capture the structure on the fly during sparse<br\/>Gaussian elimination, and then exploited to speed up the very same Gaussian elimination. The impact of the proposal will be to provide new solvers for difficult PDEs. In particular thesoftware that is developed will be made available to the community, and should enable scientists and engineers to have a new tool for their difficult problems. It will also infuse fresh ideas into the field of sparse direct solvers and unify it with the field of iterative methods.","title":"Collaborative Research: Super-fast Direct Sparse Solvers","awardID":"0515320","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["381444"],"PO":["381214"]},"107293":{"abstract":"This project, developing experimental prototypes of intelligent energy-efficient embedded systems for use in nodes of sensor networks, plans to acquire the hardware equipment that <br\/>-Builds energy efficient embedded sensor systems;<br\/>-Characterizes their energy consumption parameters; and<br\/>-Verifies and evaluates new designs.<br\/>The goal is to translate ongoing research on energy efficiency to development of advanced sensors that incorporates as many energy-saving features as possible while at the same time remain light, small, inexpensive, and relatively flexible. The investigation covers the spectrum: energy-efficient circuitry, energy-aware architectures, energy-aware embedded systems compilers, energy-efficient system integration, and energy-efficient communication and networking. The research needs to<br\/>-Compare analog and digital circuit implementations, Modify standard processor architectures,<br\/>-Develop dynamic memory management schemes, Integrate the components, and<br\/>-Tailor those (chosen for overall efficiency) to carefully chosen algorithms for processing and transmission<br\/>Virtually all wireless networks are crucially dependent on portable finite energy sources. In particular, sensor networks for a large number of applications (such as event detection, monitoring, and data collection) depend on non-renewable energy sources. Both dominant forms of energy used in sensor networks, RF Transmission and Processing, exhibit expenditure modes that depend on a large number of factors, from the bottom layer up to the network protocol stack. Clearly, there is a fundamental tradeoff between energy spent at transmission and energy spent for processing. For example, data collected by sensors can undergo complex compression that reduces the number of transmitted bits and hence saves transmission energy at the expense of increases processing energy; thus the component of the sensor that includes the communication system must also be optimized with respect to energy efficiency. Hence, this work experiments with different scenarios of network communication. Through both simulation and prototype implementation, two particular issues related to communication and networking are studied: To<br\/>-Integrate communication functions such as modulation and coding with hardware design (e.g., power amplifier) <br\/>-Evaluate power consumption of transmission vs. processing and longer vs. shorter hops with actual physical tests<br\/>-To calculate the energy consumption tradeoffs, measurements and display equipment are necessary. Specific research projects include<br\/>-Energy-efficient circuit design; Micro-controller architectures; Memory hierarchies; <br\/>-Component integration, and Communication protocols.<br\/><br\/>Broader Impact: The project should have a huge educational impact. While training the next generation of researchers, this work contributes a freely available tool for cycle-level simulation to accurately estimate energy consumption of the design. The need for deployment of long-lived sensor networks is well documented and affects directly the commercial and military sectors. An opportunity exists for significant reductions in energy consumption.","title":"MRI:Development of Energy-Efficient Embedded Systems for Wireless Sensor Networks","awardID":"0521227","effectiveDate":"2005-08-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"9199","name":"UNDISTRIBUTED PANEL\/IPA FUNDS"}}],"PIcoPI":["501350",283200,"540124","309324",283203],"PO":["557609"]},"109163":{"abstract":"This special meeting will bring together people from the Computational<br\/>Statistics and Numerical Linear Algebra communities with people from the<br\/>Theoretical Computer Science community to promote cross-fertilization of<br\/>ideas. This will hopefully lead to a better understanding of modern<br\/>massive data sets as well as invention of new techniques and algorithms<br\/>for their modeling and analysis. Most conferences and workshops on<br\/>information retrieval and data mining are dominated by studies of<br\/>traditional data sets and traditional algorithmic techniques.<br\/>Nevertheless, modern data sets have become extremely complex and have<br\/>rendered traditional approaches for processing them ineffective in many<br\/>instances. The difficulties occur primarily in two ways: modern data sets<br\/>are often high-dimensional and massive, and thus novel dimensionality<br\/>reduction or sampling-based techniques have to be employed in processing<br\/>them, and modern data sets often have complicated nonlinear structures,<br\/>and must be modeled with appropriate mathematical objects such as tensors,<br\/>symmetric spaces, Lie groups, etc. This special meeting will study data<br\/>sets with such complex structures and explore novel techniques for<br\/>analyzing such data.<br\/><br\/>The techniques that will be discussed at the meeting have applicability in<br\/>numerous fields of practical and important interest. For example:<br\/>(1) Applications in Bioinfomatics and the Medical Sciences include<br\/>improved methods to analyze large microarray and high-throughput chemical<br\/>data sets for developing new medications, identifying novel gene products,<br\/>elucidating protein folding pathways, and the improved detection and<br\/>classification of cancer.<br\/>(2) Applications in the Social Sciences include improved methods to model<br\/>the combinatorial structure of large social networks; this has been used<br\/>and will continue to be used to, e.g., identify potential terrorist cells<br\/>hidden in a communications network.<br\/>(3) Applications in the Geosciences include remote sensors in air-and<br\/>spacecraft that produce more data than can currently be stored, but which<br\/>must be analyzed for environmental planning, weather forecasting, and<br\/>public health contamination issues.<br\/>(4) Applications in Computer Vision include novel ways to identify noisy<br\/>images of targets and faces for improved target identification and<br\/>individual identification in more realistic settings.","title":"Special Meeting: Workshop on Algorithms for Modern, Massive Datasets","awardID":"0532668","effectiveDate":"2005-08-15","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["320271"],"PO":["565027"]},"95430":{"abstract":"We are currently witnessing a proliferation of database-powered Web applications that interact with external users or programs according to workflows of considerable complexity. This complexity calls for the development of static analysis techniques capable of exposing bugs (such as non-conformance to a given secure data access protocol, or to the intended business process) to increase the confidence in the security, robustness and correctness of Web applications.<br\/><br\/>The general objective of this project is to develop new tools and techniques for the high-level specification and automatic verification of database-powered Web applications. The project investigates the trade-off between the expressiveness of the Web service specification language and the feasibility of verification tasks. It also aims at establishing tractability boundaries and at developing practical algorithms and heuristics for verification. The technical problems raised bring into play techniques from logic, automata theory, computational complexity, algorithms, and computer-aided verification.<br\/><br\/>The project's broader impact consists in advancing the technology required by the development of trustworthy interactive, database-powered Web applications. This will benefit a wide variety of applications ranging from digital government to e-commerce to infrastructure for scientific applications. This research will encourage closer collaboration between the database and computer-aided verification communities. The project will contribute to the development of human resources by training doctoral and MS students in the critical area of secure and robust Web application development.<br\/><br\/>The results of the project will be widely disseminated through high-quality conference and journal publications. The developed tools will be made available via the project's Web site (http:\/\/www.cs.ucsd.edu\/~lsui\/project) for academic, research, and non-commercial purposes.","title":"Specification and Verification of Data-driven Web Services","awardID":"0415257","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["409520","518657"],"PO":["563727"]},"95342":{"abstract":"This collaborative project \"TANGO\" is leveraging strengths of research teams at Brigham Young University (IIS-0414644, PI: David Embley) and Rensselaer Polytechnic Institute (IIS 0414854, PI: George Nagy). TANGO is a framework for organizing domain-specific factual data appearing in independently generated web pages. Algorithms and software are developed for extracting and interpreting individual lists and tables and integrating them with the contents of other tables that present partially overlapping information. The input pages may be HTML, PDF, PostScript, or scanned document image files. The output of the system is an ontology that provides the conceptual framework for the domain. The ontology evolves as web pages with additional relevant data are harvested and processed. Constructing ontologies is currently a labor and skill intensive process. The TANGO framework automates much of the ontology construction task for a domain when information exists in lists and tables that describe the domain. The results of this project build stepping stones to the Semantic Web, for which ontologies are a central component. Therefore, this project will have an impact on users of the Semantic Web, who assisted by the automated software agents developed in this project, can accomplish information-based tasks for research, scholarship, planning, design, education, and entertainment. This cross-disciplinary and cross-university endeavor introduces graduate and undergraduate students to cutting-edge research. Developed tools (including source programs), a corpus of tables both in raw and normalized forms, collected test data, and technical reports can be found at the project website (http:\/\/www.tango.byu.edu).","title":"Collaborative Research: TANGO: Table Analysis for Semiautomatic Generation of Ontologies","awardID":"0414644","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[250003,250004,250005],"PO":["563751"]},"107790":{"abstract":"Proposal Number: NSF-0524156<br\/><br\/>TITLE: A Framework for Defending against Node Compromises in<br\/> Distributed Sensor Networks <br\/><br\/>PI: Sencun Zhu (szhu@cse.psu.edu), Co-PI: Guohong Cao (gcao@cse.psu.edu)<br\/><br\/>Securing wireless sensor networks is a significant challenge because of <br\/>network scale, highly constrained system resources, and the fact that<br\/>sensor networks are often deployed in unattended and hostile environments.<br\/>The objective of this project is to develop a framework for defending<br\/>against node compromises in unattended sensor networks. The framework<br\/>consists of a suite of security mechanisms spanning three phases:<br\/>prevention, detection, and reaction. This research seeks to provide fundamental<br\/>security services covering key management, authentication, compromise<br\/>detection, and revocation. These services are essential for the successful<br\/>deployment of sensor networks. In addition, the research seeks solutions that<br\/>are designed and implemented in a distributed manner, where no central<br\/>authority is involved. This distributed property is critical for unattended<br\/>sensor networks deployed in adversarial environments, because a central<br\/>authority is a single point of failure from both security and performance<br\/>perspectives. The success of this project will have broad impact, making sensor networks more trustworthy and amenable to commercial, civilian, and military applications. The results of the project are disseminated through publications and talks, and the research is integrated into the computer science and engineering curriculum at Penn State.","title":"CT-ISG: A Framework for Defending Against Node Compromises in Distributed Sensor Networks","awardID":"0524156","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["550918","550785"],"PO":["561889"]},"106030":{"abstract":"Many optimization problems that arise in various industries, including logistics, production planning, transportation and telecommunication, have to be solved repeatedly and in automated fashion. In many cases, the underlying mathematical optimization problem is provably hard, and cannot be solved to optimality in a reasonable amount of time. As a result, the task of the algorithm designer is to develop algorithms that are efficient and provide good solutions on every single run, as a far from optimum solution even just once might be catastrophic, either in terms of the cost of the solution obtained or in terms of its inability to meet demands or other constraints. This stresses the importance of designing efficient algorithms for hard combinatorial optimization problems that deliver solutions guaranteed to be probably close to the optimum. This area of approximation algorithms has seen a tremendous growth in the last decade, with a host of new results. <br\/><br\/>The intellectual merit of this proposal is to develop new methodologies and techniques to provide the algorithm designer with the tools to design approximation algorithms and is also to focus on crucial problems. These problems include fundamental problems such as the traveling salesman problem and other network problems which arise as building blocks in many industrial settings. A special emphasis will be given to settings in which the data evolves over time and the algorithm's task is to provide a constantly changing solution to meet the fluctuating requirements. This solution needs to be robust against these fluctuations, and remain close to optimum at any time.<br\/><br\/>The broader impact of this proposal is to provide industry with the tools to be<br\/>more productive and more efficiently use the available resources, and this in turn will have an impact on the economy. The proposal also seeks funds for the training of graduate students and this is important to maintain the competitively of our workforce and guarantee the best possible training for the next generation of faculty members teaching on our college campuses.","title":"Design and Analysis of Algorithms - New Paradigms, Methodologies and Applications","awardID":"0515221","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["485468"],"PO":["562944"]},"107251":{"abstract":"This project, enabling, enhancing, and supporting high performance parallel computer graphics based research and education, aims at providing researchers and academic institutions access to parallel computer graphics and visualization technology. Visualizaton Graphics Processing Units (GPUs) clusters (VGCs) will be purchased and placed at the Institute for Scientific Research (ISR), Alderson Broaddus College (A-B College), and at West Virginia University (WVU). The VGCs tightly coupled GPUs will allow investigation of algorithms that could not be implemented efficiently in prior systems. As high-speed networking made cluster computing practical, the bandwidth to the GPUs will make cluster visualization practical. Due to slow networking speeds, and even slower display buffer read-backs, prior clusters were forces to operate as a collection of poorly connected PCs. However, the VGC, with its fast GPU-to-GPU connectivity, will be able to operate as a single, integrated entity for performing graphics- and GPU-oriented operations. The acquisition of the VGC will allow the investigation of new visualization and analysis techniques: ultra-high resolution visualizations, image processing of extremely high-resolution video streams, image based reconstruction of complex real-world scenes through multiple video cameras, and the use of GPUs in-the-large to perform scientific calculations and simulations. The VGC provides the high-speed communication required between GPUs. Currently, there are no available systems that scale-up both screen and depth resolution at a reasonable price-performance ratio. Building a VGC can demonstrate that non-specialized commodity, multi-purpose hardware can replace expensive, single-task hardware while offering more functionality and flexibility.<br\/><br\/>Broader Impact: VGCs will be readily available at three institutions to service students, researchers, educators, and small businesses in West Virginia, an EPSCoR state with relatively poor high-technology infrastructure. An organization without access to significant funds or funding sources will be able to learn from this system and software design, and will be able to build its own VGC machine.","title":"MRI: Acquisition of a Visualization GPU Cluster (VGC)","awardID":"0521109","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"7368","name":"SCI TESTBEDS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[283026,283027,283028],"PO":["557609"]},"107262":{"abstract":"This project, enabling a new class of communications applications that can support future transportation systems, aims at establishing a laboratory for interdisciplinary experimental research in wireless networking and mobile computing in order to develop expertise in theoretical and applied aspects within the context of the automotive applications. The research focuses in the development of:<br\/>-Physical layer prototypes for inter-vehicle communications;<br\/>-Routing protocols for inter-vehicle communications;<br\/>-Real-time traffic information system; <br\/>-In-vehicle database management system; and <br\/>-Cooperative driving system.<br\/>In vehicle computing presents a challenge for mobile computing research due to the very large number of nodes on the road and the high rate of topological changes. This work develops efficient and scalable algorithms for wireless communication, routing, mobile data management, information dissemination, and querying in the highly dynamic vehicular environments. Through collaborations with local industrial partners, integration of the research should result in commercial systems such as co-operative driving and in-vehicle data management systems.<br\/><br\/>Broader Impact: Creating new collaborative opportunities, this infrastructure should help in the areas of mobile computing and automotive applications. The concepts and procedures developed from this project will serve as topics for theses and senior design projects. New courses will continue to be developed; stronger ties with industry will surface. Moreover, the application of wireless technology into the automobile should result in vehicles with increased productivity, safety, and security.","title":"MRI:Acquisition of Instruments for Mobile Computing Research in the Context of Automotive Applications","awardID":"0521142","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[283069,"550712",283071,283072,283073,"442797"],"PO":["557609"]},"107461":{"abstract":"This NSF grant will provide funding for the development of algorithms and supporting theory for numerical analysis and approximation of general arrival\/service-process queueing networks having time-dependent input parameters, characteristics and protocols. The result will be QNATS, the Queueing Network Approximator for Time-Dependent Systems---an addition to the NSF Cyberinfrastructure of web-useable software that is available to the engineering and scientific community. QNATS will include the facility to analyze systems in which arrival and service mechanisms change with time, as well as queueing nodal capacity and the number of servers. Also considered will be deterministic additions\/deletions of entities from queueing nodes as a function of time. QNATS will allow for time-dependent interarrival- and service-time distributions that are general in shape, rather than the often unrealistic time-dependent exponential\/Poisson distributions of textbook models. QNATS models will be easy to build and QNATS analysis will be able to deliver results quickly enough to be used interactively via web services <br\/><br\/>Queueing models are among the most widely used techniques to design and improve the performance of manufacturing, service-sector, digital-telecommunications and computer systems. In fact, queueing models are useful for assessing the performance of additions to the Cyberinfrastructure itself. Virtually any system in which discrete entities contend for resources from one or more service nodes can be represented as a network of queues. For example, time-dependent, infinite-server queueing networks have become a standard model for analysis of cellular telecommunication systems. Other applications come from a large variety of fields, including population processes in biology, migration\/immigration processes, and epidemiology. QNATS will allow engineers and scientists in many fields to avoid the disastrous consequences that can occur when time-varying system characteristics are ignored or are approximated by their time-averaged values, leading to severe underestimation of system congestion.","title":"Collaborative Research: QNATS--The Queueing Network Approximator for Time-Dependent Systems","awardID":"0521945","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"7368","name":"SCI TESTBEDS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"5514","name":"OPERATIONS RESEARCH"}}],"PIcoPI":[283840],"PO":["423737"]},"107252":{"abstract":"This project, enabling and fabricating low-cost, high-performance interconnected spatially immersive visualization facilities, perfects and exploits the technology of immersive environments. The aim is to build and operate systems economically using commodity elements, and to run software that readily supports the development of highly interactive immersive applications. Supporting cross-campus collaboration, the system will utilize commodity computers, commodity projectors, and open source software. In each system, a networked visual computer cluster will drive a surrounding high-resolution polyhedral display surface formed from many identical modular components. Cutting across many disciplines, the facility enables the following research activities:<br\/>-Perceptual optimization for data visualization, Modeling watershed flooding, <br\/>-Importance-based visualization, Brain network visualization, <br\/>-Empirical study of creative cognition, Spatial hypertext,<br\/>-Robust operation on curved geometry, 4D construction visualization, <br\/>-Architectural design environments, Physically-based simulation, Gesture-based interfaces, <br\/>-Fundamental human interaction design, Historic anthropological site reconstruction, <br\/>-Interactive science education, Self-relevance of social cognition, <br\/>-Auditory\/visual perceptual integrating and comprehension, and <br\/>-Immersive telecommuting.<br\/><br\/>Broader Impact: The project expects to considerably lower cost by building and operating an immersive system constructed entirely using commodity components. The operating experience and design expertise gained in this project should make construction of immersive visualization systems possible at institutions with more modes means. The design guidelines, and software will be made available through a project website. The immersive systems will be used for the development of informal educational experiences.","title":"MRI: Development of Spatially Immersive Visualization Facilities","awardID":"0521110","effectiveDate":"2005-08-01","expirationDate":"2010-05-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"7368","name":"SCI TESTBEDS"}}],"PIcoPI":[283030,"557686","410034","515569","492962"],"PO":["557609"]},"107373":{"abstract":"This project, acquiring a large-scale, interactive display for visualization, human computer interaction, and biological imaging research, services two primary research areas: human computer interaction (HCI) and biological imaging. The empirical investigation for HCI examines how a display wall changes user's perceptive and cognitive work patterns with visual data, and how these effects can be leveraged to increase user effectiveness in a variety of domains. This knowledge is then used to improve user efficiency through tailored interaction methods and user interfaces for display walls. For the latter, the high-resolution capabilities of the display wall in visualization and biomedical imaging are exploited. This application dove-tails the recent acquisition of a 3T MRI for the purpose of analyzing cognitive processes during problem solving.<br\/><br\/>Broader Impact: Housed in the Institute for Neurocognitive Science and Technology (INST), the equipment will contribute to provide graduates hands-on training in its use, capabilities, and limitations. The research training and outreach efforts utilizing the wall would educate students and the community in the technology and the ongoing research efforts at the university.","title":"MRI: Acquisition of A Display Wall for Human Systems Research and Biological Imaging","awardID":"0521564","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["551043","497247"],"PO":["557609"]},"107285":{"abstract":"This project, supporting experimental research methods for large speech recognition tasks, aims at purchasing a large Symmetric Multi-Processor (SMP) system. The research involves the development of models and algorithms that will reduce automatic speech recognition (ASR) errors for natural conversations, which may be exacerbated by realistic but difficult acoustic conditions. Major improvement in algorithm robustness opens a wider range of future applications, including voice access to networked information and information retrieval and extraction for meetings. Head-mounted microphones or microphone arrays may not be feasible due to low Signal to Noise Ratio (SNR) and the effects of reverberation. Integration of multiple estimators, either at the level of probability streams or hypothesized word sequences, with associated confidence measures, can greatly improve overall performance. Research has shown that such properties can significantly increase recognition accuracy, even for high SNR tasks that require the transcription of informal conversational speech. For problems of scale, training of even a single-stream system can take weeks using a 2005-generation PC or workstation. A fast multi-processor system might overcome these resource limitations and greatly enhance the ability to explore promising solutions to the current constraints on performance. Hence, research requiring multiple probability streams or more computationally intensive algorithms should benefit from this new multi-layered system infrastructure.<br\/><br\/>Broader Impact: The planned research supports technical explorations that become the basis of PhD dissertations. Other areas, such as computational biology, natural language processing, digital communications, computer vision, human activity modeling, and human computer interaction, might also benefit by the research. ICSI involves many female researchers; has a high school outreach program, and trains students, and other investigators.","title":"OIA\/MRI: Acquisition of a Computational Server for Large Vocabulary Connectionist Speech Recognition","awardID":"0521210","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}}],"PIcoPI":["550775",283164,283165],"PO":["557609"]},"107770":{"abstract":"Abstract<br\/><br\/>Proposal Number: 0524047<br\/>PI: Blaze, Matthew <br\/>Institution: University of Pennsylvania<br\/>Title: Trustworthy Network Eavesdropping & Countermeasures (TNEC)<br\/><br\/>The TNEC project examines the problem of Internet traffic interception from the point of view of the eavesdropper. It investigates whether the very properties that make it unwise to depend on networks for confidentiality can be turned on their head to effectively frustrate eavesdropping.<br\/><br\/>Network interception is usually framed as a problem of sensitivity -- preventing traffic \"evasion.\" In TNEC, on the other hand, the focus is on selectivity against misleading noise. The project introduces a new concept, called \"confusion,\" in which a second- or third- party directs artificial noise at the eavesdropper that is difficult to distinguish from targets' traffic. Confusion has the novel property that it does not require end-to-end cooperation by the communicating parties in order to achieve eavesdropping resistance.<br\/><br\/>TNEC will formalize confusion and study specific networks and eavesdropping configurations for susceptibility to confusion techniques, leading to tools for measuring and quantifying the \"fidelity\" of intercepted traffic streams even in the presence of active countermeasures. More broadly, the project will examine the network architectures for exacerbating confusion, possibly leading to networks that inherently resist eavesdropping without the need for end-to-end encryption.","title":"CT-ISG: Trustworthy Network Eavesdropping and Countermeasures (TNEC)","awardID":"0524047","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["402531"],"PO":["521752"]},"107781":{"abstract":"NSF 0524096<br\/><br\/>CT-T: A Laboratory Workbench for Security Research<br\/><br\/>Jay Lepreau<br\/><br\/>This research develops new techniques and technologies that directly improve the study of security-related software systems. A primary barrier to studying both trustworthy and malicious software, especially software that runs on networks of computers, is providing realistic but controlled environments. The environments must provide not only strong containment guarantees, but also strong connections to the outside world, both for realism and to enable analysis and control of the systems under test.<br\/><br\/>This research develops and deploys new technologies to make such environments publicly available, in three \"timely\" ways. The first is technology that allows distributed systems to be paused, checkpointed, and moved forward and backward in time. This research is leading to advancements in mixing real and virtual time in networked systems. The second way is a workbench for experiment workflows, allowing people to move forward and backward through the experimentation process. This is yielding advances in many areas including distributed system control, scientific workflow management, large-scale data storage, data analysis, and visualization. The third \"timely\" result is the regular deployment of this infrastructure as part of the popular Web-based \"Emulab\" testbed software. The project's tools are continuously deployed in online network testbeds, and delivered to the research, education, and security community for use and extension.","title":"CT-T: A Laboratory Workbench for Security Research","awardID":"0524096","effectiveDate":"2005-08-15","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["344532","521992","521992","557575"],"PO":["529429"]},"102094":{"abstract":"Abstract<br\/><br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: Collaborative Research: WORKIT: A Universal Wireless Open Research KIT <br\/><br\/>Lead Proposal: CNS 0454288<br\/>PI: Henning Schulzrinne<br\/>Institution: Columbia University <br\/><br\/>Proposal CNS 0453830<br\/>PI: Thomas LaPorta<br\/>Institution: Pennsylvania State Univ University Park <br\/><br\/>Proposal CNS 0454329<br\/>PI: Elizabeth M. Belding-Royer<br\/>Institution: University of California-Santa Barbara <br\/><br\/>Proposal CNS 0454174<br\/>PI: Scott C. MIller<br\/>Institution: Lucent Technologies, Bell Labs <br\/><br\/> <br\/>This project addresses the need for wireless network tools and platforms as recommended in the 2003 NSF Wireless Network Workshop report. The project will build on the IOTA (Integration of Two Access Technologies) project at Bell Labs. The PI's will enhance and develop IOTA for a software and systems package in a distributable form called the Wireless Open Research Kit (WORKIT). WORKIT will include source code and documentation and also be embodied in low-cost off the shelf hardware. WORKIT will be an enabler for research in mobility management, interlayer awareness, software algorithms for optimal network selection, reconfiguration, security, accounting, authentication, policy download and enforcement, and hybrid wireless networking. Broader impacts of this project include use of WORKIT in education and enabling stronger university\/industry collaborations in this area of emerging importance.<br\/>at colleges and universities.","title":"CRI: Collaborative Research: WORKIT: A Universal Wireless Open Research KIT","awardID":"0453830","effectiveDate":"2005-08-15","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["518380"],"PO":["434241"]},"107671":{"abstract":"Proposal Number: 0523249<br\/>Title: Protecting TCP Congestion Control: Tools for Design, Analysis and Emulation<br\/>PI: Sonia Fahmy<br\/><br\/>The increasing volume of non-conforming and malicious traffic flows poses a serious challenge to the stability of the Internet. Such traffic flows could significantly throttle the data rates sustainable by TCP flows, and could affect millions of users who rely on the Internet for their daily business. The following three types of misbehaving flows: unresponsive TCP sessions, low-rate TCP-targeted attacks, and randomly scanning TCP worms, can be easy to launch and are enormously damaging.<br\/><br\/>This research takes an ambitious step in systematically developing: (i) dynamic router-based quarantine schemes to penalize unresponsive TCP flows; (ii) defense strategies for low-rate TCP-targeted attacks; (iii) router-based designs to effectively control indiscriminate TCP worms; and (iv) tools and methodologies for the evaluation of the proposed schemes, specifically using the DETER\/Emulab emulation platform. The research will enable in-depth characterization of the misbehaving flows and the design of effective solutions for minimizing the vulnerability of the Internet to such flows.<br\/><br\/>This work will have an enormous practical impact, will foster new research directions towards a trustworthy Internet, will accelerate security research by streamlining the experimental process, and will train security students in both theory and hands-on experimentation.","title":"CT-T: Collaborative Research: Protecting TCP Congestion Control: Tools for Design, Analysis, and Emulation","awardID":"0523249","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["548182","550639"],"PO":["529429"]},"106021":{"abstract":"ABSTRACT<br\/>0515170 <br\/>Adrian Sandu<br\/>Virginia Polytechnic Institute and State University<br\/><br\/>MULTIRATE TIME INTEGRATION ALGORITHMS FOR ADAPTIVE SIMULATIONS OF PDES<br\/><br\/>Large scale simulations of time-dependent partial differential equations (PDEs) often involve grids of multiple resolutions covering different subdomains. When explicit temporal integration is employed, stability requirements restrict the global simulation time step. The time step bound is driven by the finest mesh patch or by the highest wave velocity, and is typically (much) smaller than necessary for other variables in the computational domain. Improvements in the efficiency and overall simulation capabilities require the development of new, adaptive, multirate time integration methods. The development of multirate<br\/>integration is challenging due to the conservation and stability constraints which time stepping schemes need to satisfy.<br\/><br\/>The overall goal of the proposed project is to develop efficient time stepping methods for parallel simulation of large-scale time-dependent PDEs. Multirate algorithms will be constructed such that: (1) different<br\/>time steps can be used in different subdomains to achieve efficiency; (2) the methods can be constructed with high order of temporal accuracy; (3) linear and nonlinear stability impose only local restrictions of the step<br\/>size (e.g., local Courant numbers); (4) the methods are conservative; and (5) different methods can be applied to different processes in multi-physics simulations. The research approach is to employ the<br\/>framework of multirate integration for both Runge-Kutta and linear multistep methods. The multirate integration techniques will inherit the strong stability properties of the corresponding single rate integrators.<br\/>Moreover, implicit-explicit multirate methods will be constructed, which are appropriate for multiphysics multiscale simulations. The methods will be illustrated in real-life, multi-scale, multi-physics simulations<br\/>arising in the prediction of atmospheric pollution.","title":"Multirate Time Integration Algorithms for Adaptive Simulations of PDEs","awardID":"0515170","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["518306"],"PO":["565157"]},"106054":{"abstract":"ABSTRACT<br\/>0515358<br\/>Irving S. Reed<br\/>U of Southern California<br\/><br\/>In this research we will study a methodology for solving a class of convex optimization problems (COPs) in analytically feasible and computationally efficient ways. These problems are frequently encountered in many information processing applications, especially in wireless communication systems equipped with multiple transmit (Tx) and receive (Rx) antennas for beamforming, equalization, joint optimal design of Tx-Rx and power control. Two types of convex optimization will be investigated in this research: (1) un-constrained<br\/>or constrained with equalities, and (2) constrained with mixed equalities and inequalities. Most of<br\/>conventional beamformings (diversity combining) at Rx fall into the first type, whereas, when joint Tx-Rx optimizations with power control are involved, the problems tend to be complicated with mixed constraints, and become the second type. The closed-form solutions to related COPs of the first type are known in many cases, but difficult to compute in real time, whereas some COPs of the second type may not be analytically solvable.","title":"TF: Dimension Reduction Approach to Convex Optimization: Theory, Algorithms, and Applications","awardID":"0515358","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7351","name":"THEORETICAL FOUNDATIONS (TF)"}}],"PIcoPI":[279768,279769],"PO":["564898"]},"109574":{"abstract":"Learning visual object categories, and recognizing objects in images, is perhaps the most difficult and exciting problem in machine vision today. In light of the fast growing data deluge in science, engineering, industry and society, recognition systems must be able to operate without human supervision. This poses new challenges: How can one learn automatically models of a large number of object classes from unlabelled images? How can one represent these object classes such that they can be searched efficiently? How can one leverage the learnt models to learn new object classes from very few examples?<br\/><br\/>It is proposed that these challenges may be met by inferring hierarchical representations of object classes from unlabelled image data. Object classes are represented as constellations of parts, where each part extracts shape and appearance information.<br\/>Non-parametric Bayesian techniques may be employed to organize these object classes into tree-structured representations. The richness of this representation grows incrementally as more data is presented to the system. New similarity measures between object classes naturally derive from this representation facilitating recognition.<br\/><br\/>Outreach to the local community is established through a collaboration with the California State University Northridge where students, often minorities who are the first in the family to obtain a university degree, will have the opportunity to engage in visual recognition problems proposed by and relevant to local companies.","title":"Collaborative Research: Learning Taxonomies of the Visual World","awardID":"0535278","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7339","name":"COMPUTER VISION"}}],"PIcoPI":["516952"],"PO":["500200"]},"109585":{"abstract":"Current video-based trackers can track robustly over thousands of video frames. This work seeks to develop trackers that operate over two orders of magnitude more time (hours). Such long-term object tracking must be resilient to large changes in appearance of both the object and the surrounding environment. This requires raising the level of abstraction at which the tracker represents its target -- the goal must be tracking \"objects\", not image templates or distributions of color. The intellectual merit of this effort is to achieve persistent object tracking through novel research that spans the areas of on-line feature selection, foreground\/background segmentation, and object model learning and recognition. Flexible appearance-based object descriptors are developed that automatically adapt to changes in object and background appearance. Shape-constrained figure\/ground segmentation is performed to avoid model drift during adaptation. Object models are learned on-the-fly during tracking and used to search for and recognize the same object again after occlusion or tracking failure. Development of this technology for persistent object tracking has broad impact in commercial applications such as traffic monitoring, motion capture and automated surveillance, as well as law enforcement and military applications in trailing suspects and combatants. This project promotes scientific repeatability, code-sharing and dissemination of results by maintaining a tracking evaluation web site that provides open source tracking code, benchmark datasets, and a mechanism for online evaluation.","title":"Persistent Tracking","awardID":"0535324","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7339","name":"COMPUTER VISION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["518440"],"PO":["500200"]},"109266":{"abstract":"PROPOSAL: 0533232<br\/>INSTITUTION: U. of Connecticut<br\/>PI: Peters, Thomas J<br\/>TITLE: Computational Topology Workshop -- Six Years and Growing<br\/><br\/>ABSTRACT<br\/> Computational Topology Workshop -- Six Years and Growing<br\/><br\/>A scientific workshop on computational topology will be held on July 14, 2005 at Denison University in Granville, Ohio. This is an added activity to the 2005 annual Summer Conference on Topology and Its Applications, which is scheduled for July 10 - 13 at this same site. The integration with a major topology conference should provide perspective and rigor to this nascent sub-discipline. The format is <br\/>novel, leaving ample discussion time to engage the attendees in expanding and refining this emerging interdisciplinary research area. A main theme for computational topology is the development of efficient algorithms for maintaining the integrity of objects with respect to a variety of topological characteristics.<br\/><br\/>Already, this workshop has garnered led to its organizer, T. J. Peters, being invited to serve as an editor on computational topology for the forthcoming monograph, \"Open Problems in Topology, II\", a volume which is expected to steer research activities for several years. Attendees will be invited to contribute problems.<br\/>Graduate students and post-doctoral associates, from both the traditional topology and computational topology communities, have aggressively been recruited, with an attractive response rate. Papers from this workshop will also be considered for a special issue of the journal, Applied General Topology.","title":"Computational Topology Workshop -- Six Years and Growing","awardID":"0533232","effectiveDate":"2005-08-01","expirationDate":"2006-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["482465","494001","452967"],"PO":["321058"]},"102160":{"abstract":"Abstract<br\/><br\/>Proposal: CNS 0454259<br\/>PI: John C. Kelly<br\/>Institution: North Carolina Agricultural & Technical State University<br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: Advanced Sensor Network Applications for Environmental Monitoring Systems <br\/><br\/> <br\/>Investigators at North Carolina A&T will develop a sensor network infrastructure with environmental monitoring for water quality management as an application focus. The proposal will support research and education in information systems in conjunction with an application in environmental science. Computer science issues to be addressed include wireless sensor networks, wireless network security, wireless network protocols, data collection for environmental monitoring, and data mining. Broader impacts include using sensor network technology to explore new sensor application in water quality management. The investigators have a strong educational program integrated with their research and plans to increase minority participation in computer science and engineering education and career paths.","title":"CRI: Advanced Sensor Network Applications for Environmental Monitoring Systems","awardID":"0454259","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["541775","381508","536010",269338,"561257"],"PO":["402055"]},"107793":{"abstract":"Proposal: 0524172<br\/><br\/>Title: CT-ISG Collaborative Research: DNS Security Revisited: Enabling Cryptographic Defenses in Large-Scale Distributed Systems<br\/><br\/>PIs: Lixia Zhang (UCLA), Songwu Lu (UCLA), and Dan Massey (Colorado <br\/>State)<br\/><br\/>The Domain Name System (DNS) is a core Internet protocol and virtually all Internet applications rely on some form of DNS data. This project is identifying and addressing fundamental technical challenges in deploying the DNS Security Extensions (DNSSEC) in the global Internet. DNSSEC aims at enhancing DNS with data origin authentication and data integrity checking by applying well defined cryptographic solutions, however a number of system issues have arisen in the process of moving the cryptographic solution to real deployment. This project is first conducting a systematic assessment of the gap between the DNSSEC specification and the deployment constraints. For each identified technical challenge, the project is proposing, implementing, and evaluating specific solutions and then integrating such solutions into a unified design improvement.<br\/><br\/>DNSSEC deployment is critical to enhanced security in cyberspace, and this effort will help move it forward by overcoming existing roadblocks, foreseeing new obstacles on the road, and developing enabling techniques to clear these obstacles. The project will also extrapolate a set of lessons and principles on major challenges in deploying cryptographic protection in large scale systems, which will hopefully provide input into other cryptographic deployment effort, such as the global routing system.","title":"CT-ISG: Collaborative Research: DNS Security Revisited: Enabling Cryptographic Defenses in Large-Scale Networks","awardID":"0524172","effectiveDate":"2005-08-15","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["559197"],"PO":["521752"]},"95423":{"abstract":"Information exchange via XML documents is a rapidly growing technology. However, to date, complex constraints of trust and confidentiality often prohibit the dissemination of data. Data that could safely be disseminated to others remains hidden behind firewalls. This project aims at producing lightweight tools that allows publication and dissemination of data while at the same time controlling how data is accessed. New data management techniques are developed that use cryptographic primitives in order to enforce access control policies in published XML documents. In cryptographically enforced access control, the data owner publishes a single data instance, which is partially encrypted, and which enforces all access control policies. The project develops a declarative language for access policies, based on XQuery, and a method for applying these policies to an XML data instance to produce a single, multiply-encrypted XML view. This view can then be published by the data owner on the Internet, and everyone can freely download and disseminate it. The crucial aspect is that only users having the right keys can access encrypted parts of the XML document. Different users holding different set of keys will have access to different parts of the document. The project also develops an XQuery interpreter to enable authorized users, holding the right keys, to execute queries on the encrypted view. The interpreter decrypts data on the fly, and only that data required to answer the query. A novel kind of data model, called protection tree, is developed which captures how various keys protect different parts of the XML data. The protection tree is central to the proposed approach: once the security policies are applied to the XML data instance, they produce a protection tree; and the data model that forms the input to the user queries is also modeled as a protection tree. The results of this research will be applicable to providing secure access to XML documents. The project Web site (http:\/\/www.cs.washington.edu\/homes\/pjallen\/cryptography.html) will be used for free dissemination of results; specifically, a policy query evaluator (which produces the encrypted view), a decrypting XQuery interpreter (which is used to query the encrypted view), and a consistency checker. This project will also provide educational and research experience in the Cyber Trust area.","title":"Using Cryptography to Control Access in Published Data","awardID":"0415193","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["531543"],"PO":["427499"]},"107210":{"abstract":"This project, enhancing a humanoid bipedal robot with vestibular and vision sensors, aims at creating Bipedal Robot Facility for research on making robots walk in a dynamically stable, and thus more human fashion. The ability to keep gaze concentrated on a point of interest during continuous bipedal locomotion (when the body might pitch, yaw, or roll in response to uneven terrain) is an automatic function that humans perform rather efficiently. Modeling the fundamental sensorimotor strategies associated with head and body control during walking and turning has led to understanding basic human functions. Bipedal robots are expected to maneuver more efficiently over uneven terrain; however, such terrain is extremely challenging. The next generation of robot vehicles will have to operate in conditions that require gaits that involve dynamic stability; thus, serious consideration needs to be given to dynamically stable motion stabilization. This work utilizes gaze stabilization as a control strategy for a dynamically stable gait, a difficult unsolved problem in robotics. This strategy is supported by recent psychological research. To date even the most advanced legged-robots do not attempt to combine this type of sensory information. The team consisting of two computer scientists and a psychology researcher hypothesizes, based on extensive work in human locomotion, that doing so will have a revolutionary effect on the stability of the robot gait. Mimicking the sensory capability of a wide range of animals-including humans, birds, kangaroos, reptiles like the basilisk-and the way that these animals use this sensory capacity, the project aims at improving the bipedal robot gaits. Moreover, the research will be enhanced by three longer projects:<br\/>-Building humanoid robots that can walk quickly over uneven terrain while maintaining stability and performing a human-like gait;<br\/>-Understanding human gait; and<br\/>-Designing prostheses that can help humans who are otherwise unable to attain a normal gait due to pathologies such as Parkinson's disease.<br\/><br\/>Broader Impact: Active collaboration with other faculty in CS and neurology at many campuses encourage effective multi-disciplinary and multi-entitity use of the instrumentation. Reaching many students, the robots will be used as demonstration systems in three classes. Moreover, an early High School program for participation for math and science, involving minorities, is in place. OpenPINO, a center of a linux-like open-source user community, and a Web-site encourage dissemination.","title":"MRI: Acquisition of Bipedal Robot Facility to Support Research into Improvement of Orientation and Stability of Locomotion","awardID":"0520989","effectiveDate":"2005-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}}],"PIcoPI":["320075",282861,"505729"],"PO":["557609"]},"106000":{"abstract":"ABSTRACT<br\/>051034<br\/>Ming Gu<br\/>U of California - Berkeley<br\/><br\/>Collaborative Research: Super-fast direct sparse solvers<br\/><br\/>The numerical solution of partial differential equations (PDE) is a key enabling technology in all disciplines of engineering and science. Nevertheless the numerical solution of three-dimensional PDEs is a critical bottle-neck that prevents this potential from being realized. This proposal advances techniques that can be used to overcome this bottle-neck. Discretized elliptic PDEs are normally solved by iterative schemes since the fill-in during sparse Gaussian elimination is excessive. This proposal observes that the fill-in, in a certain ordering, has low numerical rank in the off-diagonal blocks, and that this structure can be computed and exploited to construct direct solvers that are linear in the number of unknowns. The outcome of the proposed research has the potential to create a novel class of pre-conditioners that in conjunction with iterative solvers can become powerful weapons for solving difficult elliptic PDEs.<br\/>The intellectual merit of the proposal stems from the complicated structure in the fill-in that must be first inferred from regularity results for Green's functions in elliptic PDE theory and then converted into effective linear-time algorithms to both capture the structure on the fly during sparse Gaussian elimination, and then exploited to speed up the very same Gaussian elimination. The impact of the proposal will be to provide new solvers for difficult PDEs. In particular thesoftware that is developed will be made available to the community, and should enable scientists and engineers to have a new tool for their difficult problems. It will also infuse fresh ideas into the field of sparse direct solvers and unify it with the field of iterative methods.","title":"Collaborative Research: Super-fast Direct Sparse Solvers","awardID":"0515034","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["550348"],"PO":["399214"]},"107364":{"abstract":"This project, focusing on methodologies and architectures for extracting information and integrating heterogeneous information systems, and mining multimedia\/multimodality data collected in real time, services a large community of users with common interests in the areas of biosignal analysis and biocomputation. The infrastructure is a hybrid partly Windows- and partly Unix-based system, designed to acquire, analyze, integrate, securely store, and visualize large volumes of multimodal\/multisensor data obtained from an experimental subject, all in real time. The data are generated locally by sensing systems that currently exist in the laboratories, and include thermal cameras, 3D stereo video cameras, and brain activity scanners. Capable of integrating data collected at remote collaborating institutions, the system may include MRI and CT scans or live neurophysiological activity. The facility compliments the systems already available where the existing high-performance computers, primarily devoted to number crunching, are intended to run for a long time without interruption. Currently, these researchers have separate labs, each specializing in a different image modality. The proposal seeks to unify these labs, extend the range of modalities, and add computational and visualization resources. Relying on an interdisciplinary team of experts to integrate the best existing tools and practice of information technology, and to develop software tools specific to the common needs of real-world biomedical applications, the project addresses the needs of the ever-increasing complexity of biomedical data collection, and analysis and distribution of digital information upon which computational biosciences are dependent today. The infrastructure supports ongoing projects in<br\/>-Functional imaging (computational tracking of human learning),<br\/>-Thermal and Optical Imaging, and<br\/>-Distributed computing.<br\/><br\/>Broader Impact: This work should significantly advance the state-of-the-art in computational biomedicine and bioengineering and should provide answers to complex problems currently under investigation. It may lead to new applications in the areas of human-computer interface and biometrics-based security. The facilities, opened to researchers from academia and industry, serve as research and training grounds for scientists, impacting directly the educational activities by providing hand-on experience to students.","title":"MRI: Acquisition of a Hybrid System and Research Infrastructure for Large-Scale Integration of Biomedical Data","awardID":"0521527","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7294","name":"SCIENCE & ENGINEERING INFORMAT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}}],"PIcoPI":[283513,"554348","540798","371194","532471"],"PO":["557609"]},"107375":{"abstract":"This project, supporting interdisciplinary computational projects in bioinformatics and computational biology, computational genomics, and plant sciences, enables several research projects, including:<br\/>-Assembly, validation, and annotation of the maize genome,<br\/>-Large-scale Express Sequence Tag (EST) clustering with applications to gene identification,<br\/>-Detection of functional and regulatory clusters of proteins through whole organism protein network analysis, and<br\/>-Design and simulation of FPGA-based engines for computational genomics.<br\/>Enabling solution of large-scale applications of current relevance, the infrastructure contributes to develop, demonstrate, and disseminate high performance computing techniques and comprehensive software systems capable of solving such applications. The team consists of researchers with expertise in parallel algorithms, architectures, high-performance software development, bioinformatics and computational biology, molecular biology, functional genomics, maize genetics, and protein structural biology. The instrumentation will be used to perform clustering of the largest-scale human and mouse EST collections, and perform EST-based gene discovery at unprecedented sale and speed. Whole organism protein systems biology studies will be used to uncover functional and regulatory clusters of proteins. Design simulation of FPGAs for computational genomics applications will enable other researchers to solve large-scale problems with modest size cluster equipped with FPGAs.<br\/><br\/>Broader Impact: The project contributes web-based community resources and\/or infusion of new knowledge into existing web-based community resources. The instrumentation benefits research and educational activities in the areas addressed. Collaborative ties with New Mexico State University, a Hispanic serving doctoral extensive institution ensures involvement of underrepresented students. Iowa offers Women in Science and Engineering summer program for high school female students","title":"MRI: Acquisition of a 512-node BlueGene\/L Supercomputer for Large-Scale Applications in Genomics and Systems Biology","awardID":"0521568","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}}],"PIcoPI":["531658","521840","565135","453310"],"PO":["557609"]},"108112":{"abstract":"This project investigates the Next Generation Network Technology and Systems capable of understanding and learning the high-level perspective of the network. The proposed approach pursues a new cognitive intelligent networking paradigm that maintains the success of today's Internet but which also incorporates cognitive intelligence in the network--a new networking technique that provides the ability for the network to know what it is being asked to do, so that it can step-by-step take care of itself as it learns more. In particular, we explore new networking architecture and network elements that will lead to a future network with (a) improved robustness and adaptability, (b) improved usability and comprehensibility, (c) improved security and stability, and (d) reduced human intervention for operation and configuration. This project pursues a set of comprehensive studies that seek innovations through the design and modeling of a new brain-reflex cognitive intelligence architecture, an intelligent programmable network elements architecture, and an intelligent network control and management design. <br\/><br\/>Broader Impact: The team approach covering neuroscience, datamining, computer science, systems engineering, artificial intelligence, and networking will provide rich opportunities for students to learn beyond their primary fields of study. New courses developed by the faculty members will disseminate the new material covering neuroscience and information technology.","title":"NeTS-NBD: Collaborative Research: Intelligent and Adaptive Networking for the Next Generation Internet","awardID":"0526016","effectiveDate":"2005-08-15","expirationDate":"2006-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["527078"],"PO":["565090"]},"102161":{"abstract":"Abstract<br\/><br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: PlanetLab: A Community Resource Development Program <br\/>Proposal: CNS-0454278 <br\/>PI: Larry L. Peterson<br\/>Institution: Princeton University <br\/><br\/>This community resource project will extend the PlanetLab testbed to approximately 1000 nodes at approximately 350 sites. PlanetLab is an overlay network providing an environment for both research innovation in network services and a platform for introducing new services to client communities. As a research testbed, PlanetLab provides a geographically distributed set of machines, a realistic network substrate that experiences conjestion, failures, and diverse behavior at links, and realistic workloads at client computers. As a deployment platform, PlanetLab provides researchers with a technology transfer path for services that have community support and provides users with access to these innovative new services. Users of PlanetLab are active in areas such as network architectures, measurement and modeling, content distribution, and education. Broader impacts of this project include enabling a wider community opportunities to engage in cutting edge networking research, extending the host sites to EPSCOR states, HBCU's, and liberal arts colleges; and engaging both industry and international participation in this effort.","title":"CRI: PlanetLab: A Community Resource Development Program","awardID":"0454278","effectiveDate":"2005-08-01","expirationDate":"2011-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["560046"],"PO":["564778"]},"107871":{"abstract":"NSF 0524771<br\/><br\/>CT-T: Collaborative Research: Adaptive Security and Separation in Reconfigurable Hardware<br\/><br\/>PI: Timothy Sherwood, University of California-Santa Barbara, Cynthia Irvine, Naval Postgraduate School<br\/><br\/>From Bluetooth transceivers to the NASA Mars Rover, reconfigurable circuits have become one of the mainstays of embedded design. Combining the high computational performance of specialized circuits with the re-programmability of software, these devices are quickly becoming ubiquitous. Unfortunately, if unprotected, this reconfigurability could be exploited to disrupt critical operations, snoop on supposedly secure channels, or even to physically melt a device. However, a new approach to controlling changes to the hardware logic promises to overcome these problems. In addition, the innate malleability of this hardware presents the opportunity for hardware enforcement of adaptive security policies. For example, in an emergency, trusted individuals may need to override the nominal security policy. Thus, the reconfigurable component may provide a highly trusted mechanism for secure functionality in changing environments.<br\/><br\/>This research aims to close a gaping security hole in our nation's information infrastructure by enhancing the logical structure and internal management of reconfigurable hardware to enforce a dynamic information protection policy. Specifically, this research will: (1) discover hardware synthesis and static validation methods that will ensure that only secure and non-destructive configurations can be loaded, (2) develop new reconfigurable structures capable of securely mediating run-time access to shared resources through the use of hardware-compiled formal access policy languages, and (3) establish a firm foundation for trustworthy dynamic policy enforcement through ontological analysis, formal modeling and the development of management mechanisms integrating the results of the first two activities.","title":"Collaborative Research: CT-T: Adaptive Security and Separation in Reconfigurable Hardware","awardID":"0524771","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["560817","527086"],"PO":["529429"]},"105693":{"abstract":"ABSTRACT<br\/><br\/>Treatment of HIV infection is challenging because of both rapidly evolving therapeutic strategies and the complex social problems affecting patients. Tools that extract information from databases and provide information to resolve some of the involved uncertainties in selection of therapy strategies are necessary. Unfortunately,existing databases are marred with imperfections ranging from missing information to data entry errors and subjective evaluations. In machine learning, data imperfections have received inadequate attention,which is surprising in view of the abundance of mathematical frameworks developed by the decades of research in the .eld of uncertainty processing. In the proposed work, a team of three researchers will address the research tasks that will enhance the physician 's decision-making capabilities by gleaning actionable knowledge from relevant databases. These tasks include detection of frequently co-occurring diseases that require association mining in time-varying domains with ambiguities and uncertainties, prediction of the success of specific treatments, with special attention to induction from sparse and unreliable data, prediction of a patient non-compliance with a focus on ambiguous attributes and statistical and medical validation of the knowledge.<br\/><br\/>The proposed research will contribute to computer science along the following three lines. 1) Modification of existing techniques for association mining so that they can work with ambiguities and can quantify the uncertainty of the results. Techniques that reflect the time-varying nature of the induced knowledge will also be developed. 2) Development of a novel clustering algorithm (based on collaborative filtering) capable of ignoring the descriptions of the training examples, and modification of existing collaborative-filtering techniques so that they can handle data imperfections. 3) Development of machine-learning techniques for classifier induction from ambiguously described examples. All of these three contributions can be used in knowledge discovery from imperfect databases. In the medical domain, the induced knowledge will provide new hypotheses as well as new treatment strategies.<br\/><br\/>This research project involves a multidisciplinary collaboration of professionals from<br\/>different disciplines. The medical students involved will learn to appreciate how modern computer science techniques can enhance medical practice,while the engineering students will learn about the complications encountered in medical applications of computer science. Outreach activities to develop the participation of high school and community college students will be developed. The University of Miami is a Hispanic Serving Institution; the proposed research will involve under-represented student groups in engineering research. Activities on broad dissemination of research results via publications and presentations, incorporation of theoretical and experimental work into courses, and by contribution to relevant Internet sites.","title":"SEI: Diagnosis and Treatment of HIV Patients using Data-Mining Techniques: Making Inferences from Imperfect Data","awardID":"0513702","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7294","name":"SCIENCE & ENGINEERING INFORMAT"}}],"PIcoPI":["560250","461718",278871],"PO":["565136"]},"104296":{"abstract":"ABSTRACT<br\/><br\/>NSF-0507056<br\/><br\/>Pevzner, Pavel<br\/><br\/>Research on Computation in Molecular Biology (RECOMB) is the leading meeting in bringing computer scientists and molecular biologists together to focus on the computational aspects in the interdisciplinary field. The steering committee are internationally recognized leaders in computer science, statistics and computational molecular biology. The meeting provides peer reviewed plenary presentations, poster sessions and publication of papers and abstracts of posters. The engaging atmosphere enables all level of researchers to engage in discussion and exploration. Funds enable graduate students to participate in the meeting. The recruiting focus is on junior researchers and members of under-represented groups.","title":"RECOMB Conference Support","awardID":"0507056","effectiveDate":"2005-08-15","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1629","name":"BE: NON-ANNOUNCEMENT RESEARCH"}}],"PIcoPI":[275326],"PO":["565136"]},"107211":{"abstract":"This collaborative project with WVU (05-21034, Ross), supporting research in biometric authentication for identity assurance, complements ongoing work as part of an NSF-ITR grant that is cooperatively funded by the US Department of Homeland Security. The research develops new biometric authentication technologies and involves five tasks:<br\/>-Improvement of facial recognition that requires a 3D digitizer,<br\/>-Investigation of large-scale distributed systems that requires powerful servers and PCs;<br\/>-Simulation of matching performance for system scaling that requires powerful computers;<br\/>-Investigation of saccades as biometrics that requires Dual-Image eye-trackers; and<br\/>-Exploration of multi-spectral iris imaging that requires a multi-spectral camera.<br\/>The work, developing new techniques for distributed biometric systems as well as performing experiments with scaling up large biometric databases, aims at improving the acceptance of biometrics for trusted and ubiquitous computing. By rendering a decision about a user's identity from physiological measurements made on their \"image,\" biometric authentication systems uniquely provide the means for binding an individual's presence with their cyber action so that intent can be firmly established. Hence, the system allows the physical association of a person to the identification data in a database to enhance security protection or abuse\/misuse of personal data.<br\/><br\/>Broader Impact: A strong biometric curriculum already in place will be expanded, thus preparing many in areas serviced by Homeland Security. As these trusted systems multiply, interdisciplinary education for engineers is better serviced. The work addresses increasingly important problems in homeland security.","title":"MRI:Acquisition of Instrumentation for Biometric Authentication Research: Collaborative Research","awardID":"0520990","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}}],"PIcoPI":["535233"],"PO":["557609"]},"107695":{"abstract":"Recent advances in our understanding of quantum information suggest that computational devices based on fundamental quantum principles, such as interference and entanglement, could perform certain computational tasks much more quickly than any classical computer. The potential ramifications of computing devices based on these principles have inspired a great deal of effort aimed at determining the information processing power of such devices and possible methods for physically realizing them. A particularly promising direction of research has focused on the development of the quantum random walk (QRW). This basic algorithmic building-block is an especially attractive candidate for physical realization, as it is significantly less complex than general-purpose quantum computation and still has interesting algorithmic applications.<br\/><br\/>We shall study QRWs, focusing both on implementation and theoretical issues. We will explore realistic physical systems in which QRWs could be realized, such as ultracold Rydberg atoms, and pay special attention to explicit experimental issues, such as laser excitation sequences and redundant detection to allow error rejection. We will also develop theoretical models of decoherence, specifically focusing on the effect that decoherence has on the properties of QRWs that underly current algorithmic applications. In particular, we will study the effect of imperfect walk lattices (that is, the situation where the walk takes place on an imperfect combinatorial structure, like a grid with some nodes removed), the effect of unintentional partial measurement on QRWs, the effect of non-uniform site-site interaction on QRW, and the effect of multiple particles.<br\/><br\/>Intellectual Merit: We describe how ensembles of ultracold atoms can be used to process quantum information, and describe a new scheme based on ultracold Rydberg atoms in optical lattices to implement a continuous-time QRW. The proposed research covers new avenues, such as models of decoherence in QRWs due to imperfect walk lattices or effect of unintentional partial measurements, and a novel form of conditional interaction (the van der Waals blockade) to prepare qubits and execute quantum gates. <br\/><br\/>Broader Impact: The proposed activities will promote teaching and training at all levels. In addition, the vibrant collaborative efforts, at the national and international levels, will enhance the flow of ideas and information, as well as promote training via exchange of students and postdoctoral researchers. The multidisciplinary nature of the proposed activities (research, training, and outreach) will cross-fertilize numerous subfields including computer science, applied mathematics, electrical engineering, and physics. Finally, beyond the immediate impact of the proposed research in quantum information processing, a deeper understanding of many-body qubits will impact development of new schemes and studies to implement quantum information processing.","title":"QnTM: Quantum Information Processing with Quantum Random Walks","awardID":"0523431","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":["477856","486281"],"PO":["565157"]},"107057":{"abstract":"The focus of this effort is the development and distribution of an open source software communications architecture (SCA), referred to as OSSIE. We provide the software defined radio (SDR) research community the tools and software necessary to explore and develop software-defined wireless systems across multiple layers of the OSI stack, resulting in more efficient SDR design in terms of power consumption and data throughput. This allows rapid prototyping and testing of new waveforms with minimal hardware dependencies. Underlying research issues being investigated include algorithm development, cross-layer optimization, and the rapid development of highly portable code. The resulting suite of tools allows us to address a number of previous SCA limitations. Specifically we are (a) developing SCA code having greatly enhanced portability, especially with regards to FPGAs in radios; (b) reducing overhead by circumventing the use of CORBA; (c) increasing the flexibility of the radio to support the dynamic adaptation necessary for implementing cognitive radios; and (d) reducing power consumption by designing software enabled power management techniques that take advantage of signal properties. In addition we are developing a new radio design methodology in which the radio under development can be emulated within the framework using test instruments and \"software wrappers\" to facilitate over-the-air testing. Test instruments are then replaced with SCA-compatible hardware to create the prototype radio, a technique that significantly reduces development time. Radios not having a SCA compliant hardware structure can still be developed using this methodology through the use of OSSIE's well-defined Application Program Interfaces.","title":"NeTS ProWiN: An Open Systems Approach for Rapid Prototyping Waveforms for Software Defined Radio","awardID":"0520418","effectiveDate":"2005-08-15","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["432103","560140","364912"],"PO":["564777"]},"102162":{"abstract":"Abstract<br\/><br\/>Proposal: CNS 0454279<br\/>PI: Gregory R. Ganger<br\/>Institution: Carnegie-Mellon University<br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: A Cost-Effective, Large-scale Storage Infrastructure for CMU Researchers <br\/><br\/><br\/>Investigators at Carnegie-Mellon University will build a prototype large-scale storage infrastructure from cost-effective components and explore computer science and applications research with this equipment. Researchers will acquire approximately 100 Terabytes of storage; it will be deeply instrumented to acquire information on workload, faults, administrative tasks, power and other parameters to support research on fault tolerance and self-management. Applications, including network intrusion research, design and testing of circuits, and nanotechnology will provide realistic workloads as well as be supported to enable advances in these research areas. Broader impacts of this include creation of cost effective storage architectures to support data intensive science and engineering applications.","title":"CRI: A Cost-Effective, Large-Scale Storage Infrastructure for CMU Researchers","awardID":"0454279","effectiveDate":"2005-08-01","expirationDate":"2006-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["342331","486330","358529","311936","486331"],"PO":["297837"]},"102185":{"abstract":"Abstract<br\/><br\/>Proposal: CNS 0454432<br\/>PI: David E. Culler<br\/>Institution: University of California-Berkeley <br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI:Scalable Embedded Wireless Sensor Network Evaluation Facility<br\/><br\/>Investigators at UC Berkeley will build an inter-related family of flexible, large-scale evaluation facilities for wireless embedded sensor networks that will enable empirical evaluation of critical protocols at a scale and completeness that is not possible today. This will allow rigorous testing of protocols and distributed data processing algorithms under real conditions of noise, interference, loss, and variations in connectivity that are extremely difficult to capture accurately under simulation. They will build an indoor, large-scale set of over a thousand heterogeneous wireless nodes with extensive logging capabilities. The testbed will utilize the newest generation of Berkeley motes, redesigned for these testbeds, as well as newly developed, embedded Linux-class nodes. This will be complemented with a dedicated server side infrastructure to schedule experiments, operate nodes, store and analyze the experimental data, and emulate enterprise capabilities that interact with sensor networks. Traces of sensor data streams will also be stored in the infrastructure and replayed into the wireless network to allow repeatable experimentation with realistic in-network data processing algorithms. In addition to the sheer number of nodes, the facility will stress the heterogeneity likely to be encountered in practical deployments by including higher-powered nodes, higher bandwidth networks, and mobile motes. <br\/><br\/>The infrastructure will enable research in Wireless Sensor Networks which represent a very new and important class of networked systems. Issues to be addressed include routing, information dissemination techniques, energy efficient communication protocols and reliable communication with intermittent connectivity, the use of vision and high bandwidth sensors, mobile and high power nodes in heterogeneous WSN, and networked information theory. Finally, approaches to security in WSN such as analysis of key pre-distribution protocols, secure routing, secure aggregation and privacy research will be highlighted on the testbed. In addition to supporting about 25 researchers at Berkeley, the testbed network will become available to the broader research community through Internet login or on-site visits. <br\/><br\/>Broader Impacts of the project include exploring technology for eventual deployment of a new information infrastructure to a large number of critical physical infrastructures, including electric power, heating and cooling of buildings, water, oil and gas pipelines, refineries, etc. This project will examine high confidence services: reliable, fault tolerant and resistant to attack. These same capabilities represent a potential breakthrough for many areas of science, including environmental monitoring, civil engineering, eco-physiology and others.","title":"CRI: Scalable Embedded Wireless Sensor Network Evaluation Facility","awardID":"0454432","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["526900","527079","508219"],"PO":["564777"]},"107773":{"abstract":"ABSTRACT<br\/>0524051<br\/>Reps, Thomas<br\/>U of Wisconsin Madison<br\/><br\/>The goal of the proposed project is to create techniques that (i) provide better predictions of<br\/>the behavior of computer systems, and (ii) make computer systems less vulnerable to attack.<br\/>Speci.cally, we propose to develop improved security-analysis technology to be applied in two<br\/>areas:<br\/>Access control of shared computing resources<br\/>Finding security vulnerabilities in programs<br\/>Intellectual Merit: Solutions to the problems addressed in this proposal would provide<br\/>1. Better methods for access control of shared computing resources. Issues that will be addressed<br\/>include:<br\/>Enabling collaboration across separate administrative domains, while preserving privacy.<br\/>Creating better methods for identifying access-control vulnerabilities and defects in accesscontrol<br\/>policies.<br\/>2. Better tools for identifying security vulnerabilities in programs.<br\/>While these two topics might, at .rst blush, seem unrelated, they turn out to be closely related at the<br\/>technical level: problems in both areas can be formulated using the same machinery an automatatheoretic<br\/>formalism called weighted pushdown systems (WPDSs). WPDS solvers represent a united technology for the key algorithms required in both areas. Consequently, the study of WPDSs and related formalisms provides intellectual leverage for making advances in both areas. Moreover, studying them together is likely to bring added bene.ts: past history has shown that improvements motivated by the needs in one area have had unanticipated bene.ts in the other area. Broader Impact: As the Internet has become pervasive, security and reliability issues have become enormously important to society. New security exploits are announced daily, power-grid failures are caused by bugs in software, and multi-hundred-million-dollar space projects are interrupted by software glitches. Better tools for identifying vulnerabilities in programs will lead to<br\/>software systems with enhanced security and reliability.<br\/>The growth of the Internet also o.ers the promise of an improved platform for cross-organization<br\/>interaction and collaboration. However, the decentralized nature of the Internet presents an obstacle:<br\/>currently, organizations maintain their own namespaces and impose their own access-control policies. Cross-domain interactions can be hindered by the need to set up access-control mechanisms that incorporate (in whole or part) those of the individual organizations, as well as by conficts in the structure and contents of existing namespaces and access-control policies. Better methods for access control of shared computing resources would provide improved .exibility for supporting cross-domain interactions via the Internet. A related objective is to provide better methods for predicting the behavior and consequences of an access-control policy that crosses organizational and trust boundaries.<br\/>The proposed project aims to make fundamental advances in science and engineering that address<br\/>these issues, all of which are relevant to the goals of NSF's Cybertrust program.<br\/>Our tools and implementations will be made available for other researchers to download over the<br\/>web and use in their own security-analysis work.","title":"CT-ISG: Advanced Methods for Checking Information-Security Properties","awardID":"0524051","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2880","name":"SOFTWARE ENGINEERING AND LANGU"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["401760","521593"],"PO":["564388"]},"107663":{"abstract":"The ability to redesign and even build completely new biological entities offers revolutionary opportunities for using biology to solve human problems. The dream, however, far outstrips the reality: engineering new biological machines, programming cells behaviors, and building new forms of life pose huge technical and conceptual challenges. Our research is designed to make some important first steps, by developing generally-applicable engineering methods for building synthetic gene networks. Implementation of even the most simple circuits in a biological system requires tedious optimization of a large number of poorly-understood parameters, many of which can neither be measured nor easily manipulated. Simulations can sometimes guide optimization, but we believe that biological systems are best optimized using nature's editing strategy, evolution. We believe that a combined approach of rational design based on computational predictions coupled with directed evolution-making mutations in the laboratory and selecting those organisms exhibiting the desired behaviors--will be fundamental to the progress of synthetic biology. We will in effect learn how to 'breed' useful synthetic gene networks, just as we have learned how to breed useful plants and animals.<br\/>This approach mimics natural evolution in exploring the vast and complex landscape of functions available to a set of molecules making up an engineered regulatory pathway. Importantly, it circumvents our near-complete ignorance of how a DNA sequence encodes a specific set of biological functions, a detailed understanding that is required for any 'rational' design approach. By introducing random mutations into the DNA and screening for different functions that might be expressed by the mutant circuits, we can identify which functions are possible as well as the ranges of function available to the specific search process (e.g. random point mutation targeted to a specific gene). With further analysis, e.g. sequencing to identify the mutations and biochemical analysis of circuit components, we gain insights into the molecular mechanisms by which the overall function is achieved or modified. <br\/>In this project we have three specific aims. The first is to validate a 'selection module' by which we can efficiently evolve components and circuits in the laboratory. This module connects proper circuit function to the ability of cells that express it to survive and grow. Cells with functioning circuits survive and grow; those that have not solved the problem do not. To program complex behaviors, we will also need components that respond to predefined ranges of input parameters with predictable output parameters. Thus our second aim is to use directed evolution to create a range of transcriptional activators based on the well-characterized framework protein, LuxR. Laboratory-evolved LuxR variants will activate gene transcription at different, nonnatural promoter sites on the DNA. Finally, we propose to investigate the range of circuit functions available to a predefined set of components via evolutionary exploration. Specifically, we will evolve a series of 'band detect' circuits that respond to a prespecified range of acyl-HSL concentrations. These circuits will be used to construct synthetic systems that form patterns of gene expression in the solid phase.<br\/>Our ultimate goal is to develop a fundamental enabling technology for synthetic biology as well as for developing bio-inspired modes and architectures for computing. We envision that evolved circuits and the synthetic multicellular systems that can be constructed from them will be useful to researchers developing quantitative models of gene regulation, quorum sensing, and other aspects of cellular computing.","title":"BIC: Collaborative Research: Evolutionary Optimization of Biological Circuits: Towards Cellular Programming","awardID":"0523195","effectiveDate":"2005-08-01","expirationDate":"2010-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["496117"],"PO":["565223"]},"107784":{"abstract":"NSF 0524118 <br\/><br\/>Cryptography for Constained Environments <br\/><br\/>John Black <br\/><br\/>Sensor nodes and RFIDs are small networked devices with highly constrained resources; specifically, they often have limited amounts of processing power, radio power, memory, and battery life. Conventional cryptographic algorithms work very well on today's desktop machines, but are too resource-intensive for use in highly-constrained environments. This research investigates novel ways of creating cryptographic objects tailored for constrained environments. The research will build objects such as cryptographic hash functions, one-way hash chains, blockciphers, and other objects using lightweight primitives. It will also investigate methods of obtaining suites of these objects building them all from a single underlying primitive, thereby conserving memory. <br\/><br\/>A second thrust of the proposal involves curricular development to enhance educational awareness of security at the PI's home institution and in local industry. Increased awareness of security issues among those who will enter the workforce as software developers should decrease the frequency of code vulnerabilities in production software.","title":"CT-ISG: Cryptography for Constrained Environments","awardID":"0524118","effectiveDate":"2005-08-15","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["517575"],"PO":["521752"]},"107223":{"abstract":"This collaborative project with St Lawrence U (05-20990, Schuckers), supporting research in biometric authentication for identity assurance, complements ongoing work as part of an NSF-ITR grant that is cooperatively funded by the US Department of Homeland Security. The research develops new biometric authentication technologies and involves five tasks:<br\/>-Improvement of facial recognition that requires a 3D digitizer,<br\/>-Investigation of large-scale distributed systems that requires powerful servers and PCs;<br\/>-Simulation of matching performance for system scaling that requires powerful computers;<br\/>-Investigation of saccades as biometrics that requires Dual-Image eye-trackers; and<br\/>-Exploration of multi-spectral iris imaging that requires a multi-spectral camera.<br\/>The work, developing new techniques for distributed biometric systems as well as performing experiments with scaling up large biometric databases, aims at improving the acceptance of biometrics for trusted and ubiquitous computing. By rendering a decision about a user's identity from physiological measurements made on their \"image,\" biometric authentication systems uniquely provide the means for binding an individual's presence with their cyber action so that intent can be firmly established. Hence, the system allows the physical association of a person to the identification data in a database to enhance security protection or abuse\/misuse of personal data.<br\/><br\/>Broader Impact: A strong biometric curriculum already in place will be expanded, thus preparing many in areas serviced by Homeland Security. As these trusted systems multiply, interdisciplinary education for engineers is better serviced. The work addresses increasingly important problems for homeland security.","title":"MRI:Acquisition of Instrumentation for Biometric Authentication Research: Collaborative Research","awardID":"0521034","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["564474","560285","525557"],"PO":["557609"]},"106024":{"abstract":"Digital signal processing has been one of the technological revolutions of highest impact. At the core, this technology relies fundamentally on approximating analog signals of the real world by discrete representations that can be stored, transmitted and processed without loss of information by digital circuits and computers. Despite the ubiquity of applications and products that process signals digitally, the interface between the analog and the digital worlds, i.e. the quantization paradigm, is still only partially understood from a fundamental point of view. While the classical approach of high-resolution discretization of independent signal samples has remained dominant in the theory, the actual technology has evolved in a completely different direction in which highly correlated signal samples are quantized very coarsely using as few as 1 bit per sample. Everyday technological products such as CD players, cell phones and laser printers have employed this alternative approach very successfully for over a decade already, yet the current theoretical understanding of coarsely quantized overcomplete expansions is still highly underdeveloped. The goal of this project is to set forth the theoretical foundations of this subject with the motivations of better analyzing existing methods, introducing new designs and creating new potential applications by extending the concept to other areas of signal processing.<br\/><br\/>Two specific and related methods for coarse quantization of redundant (overcomplete) signal expansions are sigma-delta modulation for A\/D conversion of audio signals and error-diffusion for digital halftoning of images. In both cases, a given target analog signal is represented by a judiciously chosen one-bit (or few-bit) sequence which approximates the signal in a suitable low-pass subspace. Both methods employ specially designed nonlinear feedback dynamical systems to generate these representations, exact analyses of which are very challenging. Central questions of input-output error signal analysis -- a somewhat alien subject to dynamical systems -- have been traditionally addressed via empirical linear modeling. This research introduces radically new tools of analysis and design by incorporating dynamical systems and signal processing methods through a genuine study of the nonlinearity. A secondary contribution is the development of new signal processing applications in communications theory inspired by coarse quantization ideas in redundant systems.","title":"Collaborative Research: Foundations of Coarsely Quantized Overcomplete Signal Expansions","awardID":"0515187","effectiveDate":"2005-08-01","expirationDate":"2009-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":[279698],"PO":["564898"]},"107036":{"abstract":"This project investigates the Next Generation Network Technology and Systems capable of understanding and learning the high-level perspective of the network. The proposed approach pursues a new cognitive intelligent networking paradigm that maintains the success of today's Internet but which also incorporates cognitive intelligence in the network--a new networking technique that provides the ability for the network to know what it is being asked to do, so that it can step-by-step take care of itself as it learns more. In particular, we explore new networking architecture and network elements that will lead to a future network with (a) improved robustness and adaptability, (b) improved usability and comprehensibility, (c) improved security and stability, and (d) reduced human intervention for operation and configuration. This project pursues a set of comprehensive studies that seek innovations through the design and modeling of a new brain-reflex cognitive intelligence architecture, an intelligent programmable network elements architecture, and an intelligent network control and management design. <br\/><br\/>Broader Impact: The team approach covering neuroscience, datamining, computer science, systems engineering, artificial intelligence, and networking will provide rich opportunities for students to learn beyond their primary fields of study. New courses developed by the faculty members will disseminate the new material covering neuroscience and information technology.","title":"Collaborative Research: NeTS-NBD: Intelligent and Adaptive Networking for the Next Generation Internet","awardID":"0520333","effectiveDate":"2005-08-15","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["521752","507838","483816","542107","551132","521753"],"PO":["565090"]},"107157":{"abstract":"This project, developing a highly integrated and modular system for affective sensing research, aims at designing, developing, verifying, and disseminating an instrumentation setup that can be used to sense, record, and identify physiological changes that signal affective shifts relevant to human-computer interaction (HCI). A large number of biological signal sensors will be integrated for the purpose of developing HCI with the ability to respond to the state of the user's autonomic nervous system (including user emotion and affect, and states related to exercise and health). Affective computing implies that a computer system should be able to assess the emotional state of the subject, i.e., perform affective sensing based on real-time monitoring of the physiological expression of the user's affective state. Non-invasive\/unobtrusive measurements in this monitoring process ultimately yield enhancements in HCI. Physiological manifestations of sympathetic activation associated with affective shifts in a highly integrated sensory platform may reveal inconspicuous but relevant features that could be overlooked when the signals are observed in isolation. The work involves five sets of experiments:<br\/>-DSP for affective sensing,<br\/>-Real-time measurement of eye gaze tracking,<br\/>-Analysis of the relationship between exercise and blood volume, <br\/>-EEG as a way to assess the quality of HCI, and<br\/>-Autonomic nervous system monitoring as a way to assess HCI.<br\/><br\/>Broader Impact: This affective sensing system can be used as an evaluation platform toward the design of the next generation of human computer interfaces that would improve access and functional capabilities of persons with disabilities. The development of the instrument will impact the training and motivation of future professionals and researchers. This work contributes in the creation of new initiatives to engage students in an institution serving a large number of minority students; thus strengthening the student pipeline towards graduate degrees.","title":"MRI: Development of a Highly Integrated Instrumentation Setup for Affective Sensing Research","awardID":"0520811","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}}],"PIcoPI":["558105","435839"],"PO":["557609"]},"109225":{"abstract":"The proliferation of Internet technologies, services and devices, have made the current networked system designs, and management tools incapable of designing reliable, secure networked systems and services. In fact, we have reached a level of complexity, heterogeneity, and dynamism that our information infrastructureis becoming unmanageable and insecure. A series of preceeding workshops have focused on the research issues and challenges facing the development of<br\/>autonomic computing systems. The objectives of this conference are two-fold. First, by bringing together researchers in this field, this meeting further understand and address the challenging research issues facing the development and deployment of autonomic systems and applications and support dynamic, data-driven application systems, i.e., applications that dynamically interact with others by sharing processing and data in novel ways. Second, its output can be helpful in focusing and crystallizing potential directions of future NSF emphasis areas to maximize the scientific and technological returns from those research investments.","title":"Proposal for Support of The 2nd Annual International Conference on Pervasive Services (ICPS 2005)","awardID":"0533041","effectiveDate":"2005-08-15","expirationDate":"2006-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"9199","name":"UNDISTRIBUTED PANEL\/IPA FUNDS"}}],"PIcoPI":["561892","316568"],"PO":["301532"]},"102152":{"abstract":"Abstract<br\/><br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: Collaborative Research: WORKIT: A Universal Wireless Open Research KIT <br\/><br\/>Lead Proposal: CNS 0454288<br\/>PI: Henning Schulzrinne<br\/>Institution: Columbia University <br\/><br\/>Proposal CNS 0453830<br\/>PI: Thomas LaPorta<br\/>Institution: Pennsylvania State Univ University Park <br\/><br\/>Proposal CNS 0454329<br\/>PI: Elizabeth M. Belding-Royer<br\/>Institution: University of California-Santa Barbara <br\/><br\/>Proposal CNS 0454174<br\/>PI: Scott C. MIller<br\/>Institution: Lucent Technologies, Bell Labs <br\/><br\/> <br\/>This project addresses the need for wireless network tools and platforms as recommended in the 2003 NSF Wireless Network Workshop report. The project will build on the IOTA (Integration of Two Access Technologies) project at Bell Labs. The PI's will enhance and develop IOTA for a software and systems package in a distributable form called the Wireless Open Research Kit (WORKIT). WORKIT will include source code and documentation and also be embodied in low-cost off the shelf hardware. WORKIT will be an enabler for research in mobility management, interlayer awareness, software algorithms for optimal network selection, reconfiguration, security, accounting, authentication, policy download and enforcement, and hybrid wireless networking. Broader impacts of this project include use of WORKIT in education and enabling stronger university\/industry collaborations in this area of emerging importance.<br\/>at colleges and universities.","title":"CRI: Collaborative Research: WORKIT: A Universal Wireless Open Research KIT","awardID":"0454174","effectiveDate":"2005-08-15","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["486344","564558"],"PO":["434241"]},"105672":{"abstract":"ABSTRACT<br\/><br\/>Advances in video technology are being incorporated into today's healthcare practice. For example, various types of endoscopes are used for colonoscopy, upper gastrointestinal endoscopy, enteroscopy, bronchoscopy, and cystoscopy. In addition, a rapidly expanding number of formerly open surgical procedures now are being converted to endoscopic procedures including resection of gallbladders, retrieval of donor kidneys, resection of tumors of colon and pancreas, correction of hiatal hernias, coronary artery bypass grafting and minimal invasive neurosurgeries. Despite a large body of knowledge in medical image analysis, endoscopy videos are not systematically captured for real-time or post-procedure reviews and analyses. They are recorded occasionally to magnetic video-tapes (i.e., VHS). No hardware and software tools have been developed to capture, analyze, and provide user-friendly and efficient access to the medical, scientific, or educational content on such videos. This project aims to develop an Endoscopic Multimedia Information System (EMIS) to capture high quality endoscopy videos, analyze the captured videos, and provide efficient access to the content of these videos.<br\/><br\/>Images of endoscopy videos significantly differ from medical still images as studied<br\/>in the literature of medical image processing. The project will develop a new capturing system for endoscopy videos designed for patient's privacy and is non-disruptive to endoscopic procedures<br\/>and non-restrictive to a particular endoscope vendor. New algorithms for automatic classification of informative and non-informative frames. The technique does not need any predefined parameters or thresholds. New algorithms for automatic content analysis for protruding lesions such aspolyps. Many protruding lesions are clustered together. A new region segmentation technique that can identify isolated lesions will be developed first. To handle clustered<br\/>lesions, algorithms using a region pattern graph that captures important characteristics of relevant regions will be developed..<br\/><br\/>The proposed system will directly benefit endoscopic research, education, and training. Contributions to research-based training of graduate students who will contribute to research-based advanced training of students in graduate and undergraduate programs in computer science and medical informatics at PI and CoPI's institutions. The project will contribute to training of a new generation of computer scientists with a unique skill set supplement to traditional<br\/>medical imaging. This research will enhance research opportunities for junior high and high school students participating in various university programs (UTA Summer Science Camp, Program for Women in Science and Engineering) and national programs. (4) Broaden the participation of under-represented groups.","title":"SEI: Collaborative Research: Endoscopic Multimedia Information System (EMIS)","awardID":"0513582","effectiveDate":"2005-08-01","expirationDate":"2009-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7294","name":"SCIENCE & ENGINEERING INFORMAT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[278808],"PO":["565136"]},"107642":{"abstract":"ABSTRACT<br\/>0523101<br\/>PI: Dae-Kyoo Kim<br\/>Oakland University<br\/><br\/>TITLE: An Aspect-Oriented Approach to Developing UML Models of Access <br\/>Control Systems<br\/><br\/>The proposed research is concerned with developing aspect-oriented design (AOD) techniques that facilitate the development of UML models of secure software systems based on access control mechanisms. This requires <br\/>addressing the following issues: (1) developing a rigorous notation for describing access control mechanisms in a form that promotes their reusability and composibility with UML models, (2) developing weaving <br\/>algorithms to systematically incorporate access control properties into an application model, (3) analyzing woven models to verify correct realization of the access control mechanism applied, (4) developing a <br\/>prototype tool that supports weaving and analysis of woven models,(5) validating the effectiveness of the techniques and prototype tool developed in this research.<br\/><br\/>Major merits of the research include that (1) the proposed approach increases the reusability and understandability of access control mechanisms, (2) the weaving techniques provide systematic ways of incorporating access control mechanisms into UML models, (3) the evaluation techniques allow one to rigorously check security assurance for woven models, and (4) woven models can be served as a basis for generating implementation code. The results of the research will be validated in collaboration with industries, <br\/>disseminated via research papers, and incorporated in software engineering and security courses.","title":"CT-ISG: An Aspect-Oriented Approach to Developing UML Models of Access Control Systems","awardID":"0523101","effectiveDate":"2005-08-15","expirationDate":"2009-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":[284307],"PO":["564388"]},"107774":{"abstract":"This research aims to offer a technically feasible and legally sound solution to the unique protection of children's online privacy, whose trustworthiness displays a strong potential for adoption, a subject long ignored by past research. The technical solutions must be infused with feedback from legal and business professionals, information systems analysts, and potential users in order to improve usability and bolster perceived trustworthiness. This proof-of-concept is named POCKET - Parental Online Consent for Kids' Electronic Transactions. POCKET utilizes a modularized architecture, which is very effective in protecting the information exchanged online while minimizing the user's burden. The tasks of this project include: study parental knowledge of and attitudes toward children sharing information online and document causes of the failure of previous solutions; design a framework for integrating legal, trust, and adoption factors into POCKET; build a prototype that will provide a technical, trustworthy solution to obtaining parental consent and protecting children's privacy; finally test and verify the effectiveness of POCKET. <br\/><br\/>The intellectual merit of this project lies in both the strength of the interdisciplinary project team and the significance of the problem addressed. The team includes a broad range of expertise need to approach the problem from not only a technical but also a social and legal perspective. The PI and others involved all have significant expertise to offer to the project, which is high risk and difficult but also has high potential for return. The project offers new tools that can be used to protect children online as well as new insights into the practices and concerns of parents and others with respect to privacy issues. In addition, there is a potential for new technical results in the area of model-checking security protocols. <br\/><br\/>The broader impacts of the project lie in the potential to increase the online safety of children. In additional the project develops and tests a prototype of a new tool for parents as well as raises the awareness among parents of the serious issues involved. The study involves a diverse population and includes significant community outreach and dissemination. This research provides an opportunity for multidisciplinary collaboration, involving parents and children from a wide demographic in terms of gender, ethnicity, degree of internet experience, and advancing the interests of children as an under-represented group.","title":"CT-ISG: POCKET: A Technical and Behavioral Concept for Protecting Children's Online Privacy","awardID":"0524052","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["494757","548204",284675,284676],"PO":["529429"]},"108511":{"abstract":"This proposal describes an aggressive program of research in computational topology with a focus on computing and characterizing shortest paths in a variety of domains relevant to applications in robotics, coordination, and locomotion. We will investigate efficient descriptions of shortest-path information and geodesic structures in spaces with different types of constraints. The three main goals of our project are (1) developing algorithms to compute optimal paths, cycles, and other one-dimensional substructures, primarily in two-dimensional surfaces; (2) applying tools from Alexandrov geometry and topology to more efficiently characterize and compute shortest paths in non-positively curved spaces; and (3) developing languages to characterize spaces of optimal paths for motion systems with mechanical and\/or nonholonomic constraints. Our proposed work draws on techniques from low-dimensional geometric and algebraic topology, combinatorial group theory, computational geometry, and non-holonomic motion planning.<br\/><br\/>At a high level, our research focuses on techniques for computing the cheapest way to move from one point to another in a variety of interesting spaces. Consider, for example, a collection of robots moving around a factory floor. The positions of the robots can be encoded as a single point in a high-dimensional configuration space. The geometry of this space is governed by certain mechanical and\/or kinematic constraints; for example, robots must never collide with each other, and they have a limited rate of acceleration. A shortest path in the configuration space describes a set of motions of the robots from one set of locations to another that is as efficient as possible. We plan to develop algorithms that compute such shortest paths quickly, by exploiting the overall ``shape\" of the underlying space.","title":"MSPA-MCS: Fundamental Geodesic Problems in Computational Topology","awardID":"0528086","effectiveDate":"2005-08-15","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"7454","name":"MSPA-INTERDISCIPLINARY"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}}],"PIcoPI":["468851","553248","423419"],"PO":["565286"]},"107444":{"abstract":"This NSF grant will provide funding for the development of algorithms and supporting theory for numerical analysis and approximation of general arrival\/service-process queueing networks having time-dependent input parameters, characteristics and protocols. The result will be QNATS, the Queueing Network Approximator for Time-Dependent Systems---an addition to the NSF Cyberinfrastructure of web-useable software that is available to the engineering and scientific community. QNATS will include the facility to analyze systems in which arrival and service mechanisms change with time, as well as queueing nodal capacity and the number of servers. Also considered will be deterministic additions\/deletions of entities from queueing nodes as a function of time. QNATS will allow for time-dependent interarrival- and service-time distributions that are general in shape, rather than the often unrealistic time-dependent exponential\/Poisson distributions of textbook models. QNATS models will be easy to build and QNATS analysis will be able to deliver results quickly enough to be used interactively via web services <br\/><br\/>Queueing models are among the most widely used techniques to design and improve the performance of manufacturing, service-sector, digital-telecommunications and computer systems. In fact, queueing models are useful for assessing the performance of additions to the Cyberinfrastructure itself. Virtually any system in which discrete entities contend for resources from one or more service nodes can be represented as a network of queues. For example, time-dependent, infinite-server queueing networks have become a standard model for analysis of cellular telecommunication systems. Other applications come from a large variety of fields, including population processes in biology, migration\/immigration processes, and epidemiology. QNATS will allow engineers and scientists in many fields to avoid the disastrous consequences that can occur when time-varying system characteristics are ignored or are approximated by their time-averaged values, leading to severe underestimation of system congestion.","title":"Collaborative Research: QNATS - The Queueing Network Approximator for Time-Dependent Systems","awardID":"0521857","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"7368","name":"SCI TESTBEDS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"5514","name":"OPERATIONS RESEARCH"}}],"PIcoPI":["476824"],"PO":["423737"]},"107697":{"abstract":"Large scale automation of chemical and biological processing is an emerging area of research in engineering and science. Integrated nanotransport systems are miniaturized chips containing large-scale integrated network of channels. Integrated nanotransport systems have the potential to not only revolutionize chemical and biological processing but also impact other application areas such as memory and storage, sensing, nanomanufacturing, and biomimetics. In the area of chemical and biological processing, integrated nanotransport systems, compared to conventional systems, are attractive as they require reduced consumption of samples and reagents and can provide much shorter analysis times, greater sensitivity, and portability that allows in situ and real-time analysis and disposability. Currently, integrated nanotransport systems are designed using a trial-and-error experimental approach. The design process and a proper understanding of the fundamental issues in nanotransport systems can be greatly improved with insight from theory and computational modeling.<br\/>The objective of this research is to establish the computational foundations and to develop computational design tools to accelerate the design of integrated nanotransport systems. Specifically, the aims of this research are to (i) develop computational tools employing hierarchical physical models (e.g. quantum-mechanical, atomistic and classical models) to understand fundamental issues governing fluid and ion transport through a single nanochannel; (ii) develop bio-inspired channel designs using the computational tools developed as part of this research; (iii) extract compact, circuit or reduced-order models from detailed single channel analysis as well as from appropriate simplifications of the theoretical models describing nanotransport; (iv) use the circuit models to develop system level simulation tools for analysis of large arrays of channels and integrated systems. The system level design tools will be used to design integrated nanotransport systems for chemical and biological analysis and other interesting applications. <br\/>The proposed research is at the cross-roads of several engineering and science disciplines. As a result, the development of computational design tools for integrated nanotransport systems will impact several disciplines and application areas. The main efforts of this project will result in the education of graduate students in the highly interdisciplinary area of nanotransport systems. Other educational activities planned as part of this project include the training of undergraduate students and incorporation of research results from this project into summer schools offered at UIUC.","title":"Computational Foundations of Integrated Nanotransport Systems","awardID":"0523435","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["539633"],"PO":["565157"]},"106014":{"abstract":"Information Theoretic Secret Key Generation in a Network:<br\/>Principles and Constructions<br\/>Abstract<br\/>Information security is a crucial requirement in current and emerging communication networks serving<br\/>commercial as well as military applications. Advances in network theory and design, combined with strides in information theory and communication theory, have helped enable the deployment of multiterminal systems in which broadband information exchanges take place among fixed or mobile users over wired or wireless channels. In such systems, issues of secure communication have thrust themselves to the forefront of network operation, and of research in information theory. These developments emphasize the need for the study of network models of information security, which are of significantly greater scope and complexity than their point-to-point predecessors. The prime objective is to establish and maintain secure communication over public channels in networks of fixed or changing configuration.<br\/>This investigation constitutes a new research area in information-theoretic cryptography, which focuses<br\/>on code construction for secret key generation for encrypted communication in a network. The investigators<br\/>study new constructive schemes for secret key generation, which are based on a provably secure notion of<br\/>information theoretic secrecy. The main features of this research are: (i) the development of information theoretic principles which govern the modeling, quantitative performance assessment, and design of secret key cryptosystems for a network, with different secret keys being assigned to different groups of terminals for information security in group communication; (ii) an investigation of the relationship between information theoretic secrecy generation and distributed network data compression without secrecy constraints; and (iii) the resulting construction of secret keys from correlated observations at the terminals using new, practical constructions of linear data compression codes which, in turn, rely on families of emerging efficient linear error-correcting channel codes.","title":"Information Theoretic Secret Key Generation in a Network: Principles and Constructions","awardID":"0515124","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["561385","518024"],"PO":["432103"]},"106036":{"abstract":"Digital signal processing has been one of the technological revolutions of highest impact. At the core, this technology relies fundamentally on approximating analog signals of the real world by discrete representations that can be stored, transmitted and processed without loss of information by digital circuits and computers. Despite the ubiquity of applications and products that process signals digitally, the interface between the analog and the digital worlds, i.e. the quantization paradigm, is still only partially understood from a fundamental point of view. While the classical approach of high-resolution discretization of independent signal samples has remained dominant in the theory, the actual technology has evolved in a completely different direction in which highly correlated signal samples are quantized very coarsely using as few as 1 bit per sample. Everyday technological products such as CD players, cell phones and laser printers have employed this alternative approach very successfully for over a decade already, yet the current theoretical understanding of coarsely quantized overcomplete expansions is still highly underdeveloped. The goal of this project is to set forth the theoretical foundations of this subject with the motivations of better analyzing existing methods, introducing new designs and creating new potential applications by extending the concept to other areas of signal processing.<br\/><br\/>Two specific and related methods for coarse quantization of redundant (overcomplete) signal expansions are sigma-delta modulation for A\/D conversion of audio signals and error-diffusion for digital halftoning of images. In both cases, a given target analog signal is represented by a judiciously chosen one-bit (or few-bit) sequence which approximates the signal in a suitable low-pass subspace. Both methods employ specially designed nonlinear feedback dynamical systems to generate these representations, exact analyses of which are very challenging. Central questions of input-output error signal analysis -- a somewhat alien subject to dynamical systems -- have been traditionally addressed via empirical linear modeling. This research introduces radically new tools of analysis and design by incorporating dynamical systems and signal processing methods through a genuine study of the nonlinearity. A secondary contribution is the development of new signal processing applications in communications theory inspired by coarse quantization ideas in redundant systems.","title":"Collaborative Research: Foundations of Coarsely Quantized Overcomplete Signal Expansions","awardID":"0515252","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":[279723],"PO":["564898"]},"107015":{"abstract":"This research is directed at filling a critical need in building secure sensor networks. Using techniques other than cryptographic, the research develops intrusion detection and response systems (IDRS) that are suited especially for the challenges of a deployed wireless sensor network.<br\/><br\/>Specific topics being pursued include (i) developing lightweight specification-based IDS detectors that apply to various sensor network protocols, (ii) employing formal reasoning and verification techniques on these detectors to develop a formal framework for proving that, for a given set of assumptions, the IDS will trigger an alarm whenever the policy is violated, regardless of the correctness of the protocol or its implementation, (iii) building a cooperative, distributed, lightweight ID architecture to explore issues in data acquisition, aggregation, correlation and analysis as well as appropriate dissemination of IDS alerts and response directives suitable for this domain, and (iv) employing the NSF-NeTS UCDavis SENSES project software infrastructure that facilitates the task of developing, deploying and managing sensor network applications in order to generate IDS schemas from the above developed security specifications. This will be used to develop common IDS components for building the IDS middleware. The key here is to develop very fine-grained and scalable components that can be synthesized to fit on a wide range of devices.<br\/><br\/>This research will lead to the development and deployment of new architectures and new IDRS methodologies suitable for the realization of effective and evaluatable security for sensor networks.","title":"NeTS-NOSS: SNIDS: Sensor Network Intrusion Detection Systems","awardID":"0520269","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["521752","560876","507838","284993",282233,"521753"],"PO":["564777"]},"107378":{"abstract":"This project, augmenting capabilities for systematic analysis and evaluation of security vulnerabilities and developing new methodologies for prevention of security threats and attacks on information networks and networked systems, aims at improving end-to-end security of information infrastructures and realizing various security applications. The infrastructure services the following research:<br\/>-Security vulnerabilities characterization, testing and measurement for end-to-end networks and information systems;<br\/>-New anomaly detection algorithms utilizing outlier detection techniques;<br\/>-New hardware implementation methodologies and evaluation for intrusion detection\/prevention schemes;<br\/>-New methodologies and non-proprietary test-suite for evaluation of performance of firewall and intrusion detection systems;<br\/>-Innovative schemes for computer network forensics;<br\/>-Analysis and prevention of security vulnerabilities and security monitoring with various wireless data networks.<br\/>The enhanced capabilities will contribute in the development of new methodologies and algorithms for security-vulnerability characterization and prevention. Additionally, non-proprietary test suites and a benchmark for a standardized security assessment and evaluation should be created.<br\/><br\/>Broader Impact: While servicing a large number of underrepresented individuals, this infrastructure should promote more exchanges facilitating more collaboration and contribute to hands on projects. Direct impact in retaining and retraining students should be felt from the new courses related to security analysis and evaluation and the summer internships.","title":"MRI: Acquisition of Instrumentation for Security Research and Training with Wireline and Wireless Information Networks","awardID":"0521585","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4096","name":"COMMUNICATIONS RESEARCH"}}],"PIcoPI":[283575,"434480","462903",283578],"PO":["557609"]},"109578":{"abstract":"Learning visual object categories, and recognizing objects in images, is perhaps the most difficult and exciting problem in machine vision today. In light of the fast growing data deluge in science, engineering, industry and society, recognition systems must be able to operate without human supervision. This poses new challenges: How can one learn automatically models of a large number of object classes from unlabelled images? How can one represent these object classes such that they can be searched efficiently? How can one leverage the learnt models to learn new object classes from very few examples?<br\/><br\/>It is proposed that these challenges may be met by inferring hierarchical representations of object classes from unlabelled image data. Object classes are represented as constellations of parts, where each part extracts shape and appearance information.<br\/>Non-parametric Bayesian techniques may be employed to organize these object classes into tree-structured representations. The richness of this representation grows incrementally as more data is presented to the system. New similarity measures between object classes naturally derive from this representation facilitating recognition.<br\/><br\/>Outreach to the local community is established through a collaboration with the California State University Northridge where students, often minorities who are the first in the family to obtain a university degree, will have the opportunity to engage in visual recognition problems proposed by and relevant to local companies.","title":"Collaborative Research: Learning Taxonomies of the Visual World","awardID":"0535292","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7339","name":"COMPUTER VISION"}}],"PIcoPI":["517415"],"PO":["564316"]},"107840":{"abstract":"Current-day electronic computers are not fundamentally different from purely mechanical computers: the operation of either can be described fully in terms of classical physics. By contrast, computers could in principle be built to profit from actual quantum phenomena that have no classical analogue, such as entanglement and interference, sometimes providing exponential speed-up compared with classical computers. Every quantum algorithm requires the implementation of a quantum oracle (logic circuit), whose function is to recognize solutions to a given problem. To completely exploit the \"quantum parallelism,\" this oracle should be realized by using quantum gates because it must be able to handle an arbitrary superposition of basis vectors (quantum states.) A key problem is thus how to construct a minimum-cost realization of this kind of quantum logic circuit. This research focuses on the development of an efficient synthesis framework for quantum logic circuits. The proposed synthesis algorithm and flow can generate a quantum circuit using the most basic quantum operators, i.e., the rotation and controlled-rotation primitives in the Bloch Sphere Representation. More importantly, this work introduces the notion of quantum factored forms, and develops a canonical and concise representation of quantum logic circuits, called a quantum decision diagram (QDD). The QDDs are amenable to efficient manipulation and optimization including recursive unitary functional bidecomposition. Subsequently, an effective QDD-based algorithm is developed and applied to automatic synthesis of quantum logic circuits. If successful, this research will pave the way toward building quantum computing circuits and eventually systems. Its impacts can thus be broad and substantial.","title":"QnTM: Efficient Synthesis of Quantum Logic Circuits by Rotation-Based Quantum Operators and Unitary Functional Bi-Decomposition","awardID":"0524602","effectiveDate":"2005-08-01","expirationDate":"2006-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":["518663"],"PO":["521045"]},"104221":{"abstract":"NIRT: Chemically Tunable Nanoelectronic and Nanoelectromechanical Systems<br\/><br\/>Abstract<br\/><br\/>The interdisciplinary research program aims to establish a fundamental understanding of interfacial\/surface effects on the electronic and mechanical properties of nanoscale materials incorporated into devices. Surface chemistry to control the highly environment-sensitive responses of nanoscale materials, specifically single-walled carbon nanotubes will be developed, and synergistic computational and experimental studies of nanoscale surfaces and interfaces will be conducted. It is anticipated that this will lead to simple but effective chemical means to tune the electronic characteristics of carbon nanotube transistors and resonators Furthermore, these chemically tunable nanotubes will provide versatile building blocks suitable for developing engineering concepts and designs for novel nanoelectronic and electromechanical architectures.<br\/><br\/> The success of this NIRT project will provide diverse resources for research and education in multiple fields of science and engineering as well as new materials and devices from which integrated electronics and sensor technologies with unprecedented capabilities may arise. Enrichment of undergraduate and graduate course curricula and development of computational\/modeling tools are also anticipated.","title":"NIRT: Chemically Tunable Nanoelectronic and Nanoelectromechanical Systems","awardID":"0506660","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1674","name":"NANOSCALE: INTRDISCPL RESRCH T"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0705","name":"Division of ENGINEERING EDUCATION AND CENT","abbr":"EEC"},"pgm":{"id":"1480","name":"ENGINEERING RESEARCH CENTERS"}}],"PIcoPI":["539633","560968","531740","503634","544178"],"PO":["559883"]},"102164":{"abstract":"Abstract<br\/><br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/>Title: CRI: Collaborative Research: WORKIT: A Universal Wireless Open Research KIT <br\/><br\/>Lead Proposal: CNS 0454288<br\/>PI: Henning Schulzrinne<br\/>Institution: Columbia University <br\/><br\/>Proposal CNS 0453830<br\/>PI: Thomas LaPorta<br\/>Institution: Pennsylvania State Univ University Park <br\/><br\/>Proposal CNS 0454329<br\/>PI: Elizabeth M. Belding-Royer<br\/>Institution: University of California-Santa Barbara <br\/><br\/>Proposal CNS 0454174<br\/>PI: Scott C. MIller<br\/>Institution: Lucent Technologies, Bell Labs <br\/><br\/><br\/>This project addresses the need for wireless network tools and platforms as recommended in the 2003 NSF Wireless Network Workshop report. The project will build on the IOTA (Integration of Two Access Technologies) project at Bell Labs. The PI's will enhance and develop IOTA for a software and systems package in a distributable form called the Wireless Open Research Kit (WORKIT). WORKIT will include source code and documentation and also be embodied in low-cost off the shelf hardware. WORKIT will be an enabler for research in mobility management, interlayer awareness, software algorithms for optimal network selection, reconfiguration, security, accounting, authentication, policy download and enforcement, and hybrid wireless networking. Broader impacts of this project include use of WORKIT in education and enabling stronger university\/industry collaborations in this area of emerging importance.<br\/>at colleges and universities.","title":"CRI: Collaborative Research: WORKIT: A Universal Wireless Open Research KIT","awardID":"0454288","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["518545"],"PO":["434241"]},"107775":{"abstract":"NSF 0524059<br\/><br\/>Resource-guided Implementation of Secure Embedded Software<br\/><br\/>Stephan Zdancewic, Rajeev Alur, Andre Scedrov<br\/><br\/>This project aims to advance the technology for designing and<br\/>implementing security-sensitive software for networked, embedded<br\/>systems. The strategy unifies two approaches that can contribute key<br\/>elements to the needed technology. The first approach provides<br\/>security for distributed systems by using language-based information<br\/>flow controls, a technique for annotating code with confidentiality and<br\/>integrity requirements and using these requirements to determine the<br\/>necessary placement of code in a networked system characterized by<br\/>complex trust relations between distributed agents. The second<br\/>approach, model-based design, expresses protocols and systems at a<br\/>high level of abstraction suitable for automated analysis techniques<br\/>that can reveal design errors, and provides synthesis tools that<br\/>automate low-level platform-specific details, thereby reducing coding<br\/>errors. The approach, called resource guided model<br\/>transformation, will allow the designer to express systems and their<br\/>security requirements at a high level, including annotations for<br\/>resource constraints of the target platform. This research program<br\/>focuses on the development of foundational theories and tools for correctness<br\/>preserving transformations of these models and protocols, facilitating<br\/>the synthesis of secure distributed software. The project<br\/>investigates the effectiveness of this methodology by implementing<br\/>secure transaction systems using Java Card technology.","title":"CT-T: Resource-Guided Implementation of Secure Embedded Software","awardID":"0524059","effectiveDate":"2005-08-15","expirationDate":"2010-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["381704","497082","556654"],"PO":["561889"]},"107786":{"abstract":"Membrane computing, a relatively new branch of molecular computing, holds great promise for new paradigms for computation suggested by processes that occur in live cells. This area was recently selected by the Institute for Scientific Information (ISI) as a fast \"Emerging Research Front\" in Computer Science. The project has three major directions of research using tools and techniques from the membrane computing area. The first concerns the simulation of cells using concepts from P systems implemented in the E-cell package. To help the simulation of cells the second research area focuses on defining models of P systems that mimic the processes in the cell. The PIs have already, as a preliminary result, one such example of a model of P systems that has several features not found in any other previous P system models: binding of objects and genetic information. Another major novelty of the proposed model is that time is associated with each rule. The PIs plan to search for other suitable models, and also work on the more theoretical aspects of the P systems: developing efficient algorithms (e,g., considering reachability between system configurations), and investigating important and fundamental computational issues in the models considered. The third area complements the first two: research in the area of P systems simulators. While in the first area the interest is in simulating cells using, among other ideas, the models developed in the second area; the third research focus goes in the opposite direction - simulation of P systems rather than the simulation of cells. This area of research complements the first two, since such a simulator for a bio-relevant P system could answer practical questions raised from biology. An ambitious goal of the whole project is to capture in these mathematical models the way the cells self-configure and self-maintain and view it as computation. These investigations could yield new ideas and useful models for computation inspired by the extraordinary system that is a cell. The proposed project topic is new, with great promise to significantly impact several areas. In Biology the project will help explore, model and simulate differential gene expression during various biological processes and transcriptional and signaling networks on a larger scale than the current capabilities; in Computer Science the research could yield new paradigms and new computing techniques (as has happened in the discovery of genetic algorithms and neural networks). One of the major contributions of this project will be the applications of a good cell-simulator in many areas - such a software would be an invaluable tool for designing and testing new drugs; for predicting the behavior of a specific type of cell in a specific environment; and, lastly, for teaching and gaining more knowledge and insight about the extraordinary systems that we call \"the cells\".","title":"BioComp: Collaborative Research: P Systems: Theory and Applications to Modeling and Simulation of Cells","awardID":"0524136","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["499391"],"PO":["565223"]},"109865":{"abstract":"The Internet has become an indispensable infrastructure for our economy, society, and government. However, despite its critical importance, today's Internet is extremely fragile and suffers from frequent attacks. One of the main reasons for the security vulnerabilities of today's Internet is that the Internet protocols and architecture were designed for a trustworthy environment. This assumption is clearly no longer valid in today's Internet, connecting millions of people, computers, and corporations distributed throughout the world. Many researchers have studied how to secure the Internet, mostly by proposing patches to address current vulnerabilities. However, partial solutions and ad hoc mechanisms often do not address the root cause of the problems, and hence will not be able to eradicate the current problems and prevent them from manifesting in different forms in the future. Moreover, security patches and ad hoc security solutions increase network complexity, which in turn increases vulnerability. Thus, we need a radical new design for a next-generation Internet, which is designed ground-up from sound principles. <br\/><br\/>Intellectual Merit: The principal investigators (PIs) plan a series of efforts to engage the community in systematically exploring this important question of how to provide the next-generation Internet with a set of fundamental security design principles and mechanisms that will provide the next generation Internet with provable security guarantees. One part of the planning effort is one or more workshops. Starting from a clean-slate approach, participants in the workshops will investigate the fundamental issues in designing a secure next generation Internet, which not only removes many of the current security problems but also provides provable security guarantees against unforeseen future attacks. The deployment of a next-generation secure Internet will likely start out as a research test bed. Such an infrastructure will attract applications requiring high-assurance, resulting in a transition of hosts to the secure Internet. The success of such a test bed critically depends on the collaboration of researchers in networking, architecture, and security. A distributed effort with many small projects is unlikely to have the same impact as a coordinated collaborative research effort. This planning effort will provide coordination of research efforts and establishing community consensus for promising research directions.<br\/><br\/>Broader Impacts. This effort will involve the networking and security communities to establish a consensus on what security properties to provide in the network, and to establish promising research directions for designing a secure next-generation secure Internet. Included in this effort is a workshop with 30-40 networking and security experts from industry and academia to further explore and define a research direction for the next-generation secure Internet. The PIs will produce a report to communicate the results from this planning activity to the broader research community. Through a series of meetings, organized discussions, presentations, this effort will build a roadmap and help the community to reach consensus on what the important research directions are and how the different research efforts work together to achieve the desired goal of designing and implementing a testbed of an architecture for a next-generation secure Internet.","title":"Collaborative Research: Planning Grant: A Clean-Slate Design for the Next-Generation Secure Internet","awardID":"0537246","effectiveDate":"2005-08-01","expirationDate":"2007-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["526903","496764"],"PO":["565090"]},"106026":{"abstract":"PROPOSAL: 0515197<br\/>INSTITUTION: U of Delaware<br\/>PI: Saunders, B. David<br\/>TITLE: Integer Linear Algebra, LinBox Applications and Extensions<br\/><br\/>ABSTRACT<br\/>Linear algebra lies at the core of computation and modeling in science and engineering. Almost all of it done today is numeric linear algebra in which the data are measured values and the results are approximate and subject to varying amounts of error. This research addresses EXACT linear algebra computation in which the data are whole numbers and the results are exact - computed without any error whatsoever. In the past two decades, significant new methods have arisen and are enabling solution of large problem instances where it could not before be dreamed of. We will broaden the range of problem types solvable by the new methods, devise further new and better methods, and, above all, provide high performance implemented programs in a software library, LinBox, readily available to all. <br\/>We will study (1) extremely sparse systems, with just a few nonzero entries per matrix row, (2) symmetric matrices: matrix signature and positive definiteness, (3) hybrid algorithms for Smith normal forms. The corresponding application areas are image rendering, study of symmetry (Lie groups), and combinatorics. The intellectual heart of the activity lies in two major areas. First, we probe the performance limits for sparse linear algebra, striving to create algorithms which are optimal, in the sense of computational complexity. This requires both novel algorithms and new insights concerning the absolute limits to speedup. Secondly, we program the library in a way providing for both high performance and genericity with respect to the many variants of matrix representation and underlying arithmetic. Thus the implementation elegantly solves the vexing problem of software reusability. The fact that our programs are not simply academic demonstrations is very important, giving the work much broader impact. Mathematicians and scientists may now use LinBox and for the first time perform exact linear algebra computations on large problems.","title":"Integer Linear Algebra, LinBox Applications and Extensions","awardID":"0515197","effectiveDate":"2005-08-01","expirationDate":"2010-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["451563"],"PO":["562944"]},"107016":{"abstract":"This project aims to develop an adaptive sensor network architecture that enables the efficient, large-scale, long-term, low-cost, on-demand monitoring of a variety of physical phenomena with high fidelity. The core theme is that distributed signal processing and data assimilation of sensor data, as well as network management and monitoring, should be performed inside the sensor network in order to reduce energy consumption and global communication needs, leading to dramatically increased sensor lifetimes and much higher fidelity in the tracking of the physical phenomena of interest. The goal is to develop a flexible, self-monitoring architecture for this type of in-network processing and sensor networking that exploits adaptivity to significantly improve the network's efficiency, robustness, and usefulness. Two kinds of adaptivity are considered: (1) data adaptivity, where the network topology is adapted to align communications with the natural data flows; and (2) resource adaptivity, where the network topology is adapted based on computational, battery, or bandwidth resources. The expected results include the development of adaptive communication protocols and routing topology, the development of network management tools for sensor communication performance monitoring and inference, and for sensor distribution monitoring, as well as the experimental deployment of the adaptive sensor network architecture in a small-scale testbed of sensor nodes on the Rice University campus. Results will be disseminated through technical reports posted on the project web page, through papers presented at professional meetings, as well as through journal publications.","title":"NeTS-NOSS: Adaptivity in Sensor Networks for Optimized Distributed Sensing and Signal Processing","awardID":"0520280","effectiveDate":"2005-08-01","expirationDate":"2011-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["302291","485219","497441",282239,"543600"],"PO":["557315"]},"108127":{"abstract":"Trading zones, interactional expertise and interdisciplinary collaboration<br\/>Michael E. Gorman, University of Virginia<br\/>Project Summary<br\/>Intellectual merit<br\/>Recent NSF initiatives like nanotechnology and convergent technologies emphasize collaboration across not only scientific and engineering disciplines, but also social sciences and humanities (Roco & Bainbridge, 2001, 2002). Science-technology studies (STS) is evolving new theoretical frameworks based on trading zones, shared expertise and moral imagination that show promise for understanding and facilitating<br\/>these collaborations (Gorman, 2002, 2003a, 2003b).<br\/><br\/>This proposal has three goals:<br\/>a) Developing a framework for analyzing and facilitating interdisciplinary<br\/>collaboration in science and technology, based upon linking the concepts of<br\/>trading zones and interactional expertise;<br\/>b) Adding an ethics\/values component via the concept of moral imagination;<br\/>c) Applying the framework to the convergence between nanoscience,<br\/>biotechnology, information technology, and cognitive science (\"NBIC\").<br\/>The method will be a workshop that brings together participants from STS, ethics and<br\/>industry. It will be held at Arizona State University's Decision Theater, a facility<br\/>designed to encourage collaboration among different stakeholders.<br\/><br\/>Broader Impact<br\/>This workshop includes an interdisciplinary group of participants (see below) who<br\/>would never appear at a society meeting together. Their work will have an impact on how<br\/>converging technologies can be introduced in ways that promise social as well as<br\/>technological progress. Results will also be relevant to the emerging fields of Earth<br\/>Systems Engineering Management and Service Science.<br\/>Results will be disseminated via:<br\/>o A volume of edited papers from the workshop;<br\/>o Articles in journals like Science, Technology & Human Values and IEEE<br\/>Technology & Society.<br\/>o On a web-site maintained at Arizona State University","title":"Trading Zones, Interactional Expertise and Interdisciplinary Collaboration, Arizona State University; 6\/05-8\/06.","awardID":"0526096","effectiveDate":"2005-08-01","expirationDate":"2007-01-31","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0405","name":"Division of OF SOCIAL AND ECONOMIC SCIENCE","abbr":"SES"},"pgm":{"id":"1353","name":"Hist & Philosophy of SET"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"7373","name":"ITR-INFORMATION INTEGRATION"}}],"PIcoPI":["447804"],"PO":["563324"]},"109579":{"abstract":"The research in this project will develop robust and effective algorithms for detecting and analyzing meaningful image discontinuities, which are critical cues that play an important part in building general image understanding systems. The project will build on promising preliminary research on multi-flash imaging, which uses active illumination to achieve robust detection and labeling of depth discontinuities in images. These ideas will be generalized by varying various aspects of the illumination parameters and developing methods to handle a much wider variety of imaging conditions. The research will develop a framework for robust multi-view stereo by integrating viewpoint variation with active illumination, and it will extend the concept to develop methods for detecting and analyzing other kinds of meaningful discontinuities (in surface normal, illumination, motion, etc.). To pursue these objectives, extensive data collection and experimentation will be performed with a number of image capture designs and methods for detecting and analyzing discontinuities. The research will enable a variety of both short-term and long-term applications in engineering, medicine, art, and entertainment. Real and synthetic data for experiments will be shared publicly with the research community via the web, as will metrics and software for comparing and evaluating techniques. The educational impacts of the project include the involvement of graduate and undergraduate students and new seminar courses that will be developed related to the research theme. The project will address diversity issues by recruiting undergraduate researchers and short-term graduate students through programs that aim to increase the involvement of underrepresented students in science and engineering, and also by the planned collaborations with colleagues and students in areas outside of Computer Science where women are currently better represented.","title":"Detecting and Analyzing Discontinuities in Computer Vision","awardID":"0535293","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7339","name":"COMPUTER VISION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["558448"],"PO":["564316"]},"105982":{"abstract":"Abstract<br\/>0514955<br\/>Yi Ma<br\/>U of Illinois @ Urbana<br\/><br\/>Estimation of Hybrid Models as Algebraic Sets<br\/><br\/>With the rapid advance in information technologies, all types of new data images, videos, audios, texts, economic data, and genomic data have been produced at an unprecedented rate and scale. How efficiently to represent, classify, and analyze a large quantity of high-dimensional data that have complex geometric or statistical structures becomes one of the most fundamental and challenging problems in our modern information-technology era. One of the main difficulties with modeling complex data is that a given data set may consist of many different components (subsets), each of which may have different topological, geometric or statistical structures. In different contexts, such data are called mixed, or multi-modal <br\/>or piecewise, or heterogeneous,or hybrid.<br\/><br\/>The goal of the proposed project is to investigate a novel approach that models hybrid data as algebraic sets instead. This readily relates the hybrid-model estimation problem with the study of algebraic sets in algebraic geometry. It offers rather refreshing technical tools, different from the conventional statistical learning methods, which can help resolve the apparent chicken-and-egg problem in the estimation of hybrid models as well as lead to efficient and robust algorithms for estimating and decomposing hybrid <br\/>models.<br\/><br\/>In the past few years, hybrid model estimation has become a fundamental problem in many important applications, ranging from image\/video\/motion segmentation in computer vision, sparse image <br\/>representation and approximation in image processing, to hybrid system identification in systems theory. The proposed interdisciplinary research project provides ample opportunities for students from both mathematics and engineering to interact with each other. Engineering students are traditionally weaker in training in abstract algebra than in analysis, geometry, and statistics. This project will significantly improve the visibility of algebraic geometry to engineering students and researchers. It will help to <br\/>develop new cross-discipline research programs between mathematics and engineering. Furthermore, visual examples and demonstrations developed from this project will also enhance the teaching of abstract algebraic concepts to undergraduate mathematical students or even to the general public.","title":"Estimation of Hybrid Models as Algebraic Sets","awardID":"0514955","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":[279598,"311520"],"PO":["562944"]},"104772":{"abstract":"Large scale parallel systems are critical to our computational infrastructure to take on the challenges imposed by applications whose scale and demands exceed the capabilities of machines available in the market today. Pushing the limits of hardware and software technologies to extract the maximum performance, in turn, exacerbates other problems. Notable amongst these problems is the susceptibility to failures, which arises as a consequence of growing hardware transient errors, hardware device failures, software complexity, and the complex hardware\/software inter-dependencies between the nodes of a parallel system. These failures can have substantial consequences on system performance, in addition to impacting the costs of maintenance\/operation, thereby putting at risk the very motivation behind deploying these large scale systems.<br\/><br\/>This research is expected to make three broad contributions towards developing a runtime infrastructure, called PROGNOSIS, for failure data collection and online analysis. The first set of contributions will be on collecting and analyzing system events and failure data from an actual BlueGene\/L system over an extended period of time. In addition to presenting the raw system events, the research will be developing filtering techniques to remove unimportant information and identifying stationary intervals, together with defining the attributes for logging and their frequency. The second set of contributions will be models for online analysis and prediction of evolving failure data by exploiting correlations between system events over time, across the nodes, and with respect to external factors such as imposed workload and operating temperature. The third set of contributions will be on demonstrating the uses of PROGNOSIS. Tools such as PROGNOSIS can help substantially in the development of self-healing systems, which has been noted to be an important goal in the emerging area of Autonomic Computing by several computer vendors.","title":"Collaborative Research: CSR-SMA+AES: PROGNOSIS to Enhance the Runtime Health of Large Scale Parallel Systems","awardID":"0509234","effectiveDate":"2005-08-01","expirationDate":"2007-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["542015"],"PO":["551712"]},"107830":{"abstract":"Project Summary In 2004,the 90 nm node for CMOS-based Si integrated circuits was commercialized.90<br\/>nm refers to the 'half-pitch ' between the most closely spaced metal lines the actual pitch of those lines is 180 nm..Assuming that the current scaling trends continue,technology nodes within the 10-15 nm range would be commercialized around the timeframe 2020 or so.It has previously not been possible to even explore circuitry at these dimensions,since no patterning method for creating such ultra-high density semiconductor circuitry existed.However,the SNAP (superlattice nanowire pattern transfer)method has been recently demonstrated as capable of producing relatively large scale,highly conducting Si nanowire circuits at these dimensions.<br\/>The work proposed herewill utilize these circuits,and will focus on addressing some of the most fundamental,chemical,and materials issues that are associated with scaling semiconductor computational circuitry to near molecular dimensions.The intellectual merit of this work will be to establish whether or not it is even possible to scale CMOS circuitry to such extremes.The broader impact is that,regardless of what computational paradigm follows the current one,a high levelof manufacturing perfection at the atomic scale is likely to be necessary.<br\/>The work described in this proposal will lay much of the foundation for achieving such perfection.<br\/>In the spirit of the RFA,certain approaches described here require manufacturing at a near atomic level of<br\/>control,although parallel fabrication approaches for achieving such perfection are proposed,rather than atom by atom assembly approaches.Also,in the spirit of the RFA,architectural approaches for novel omputational schemes,such as those that can take advantage of highly regular circuit structures,or that can bridge length scales from the nano-scale of the logic circuits to the sub-micron scale of standard lithography,will be exploited.<br\/>In fabricating and utilizing ultra-dense silicon circuitry,several chemical and materials issues become im-<br\/>portant.For example, as Si wire widths are reduced to a few nm,the role that surface states play in the conductivity characteristics of the nanowires becomes increasingly important.Since oxide passivation of Si reduces the mobility of charge carriers near the surface,we want to replace the oxide with an atomically perfect (and very thin)surface passivant.We propose to explore the use of methyl termination of Si(111)for applications to these circuits,an alternative that has been demonstrated to be air-stable with atomically complete passivation that dramatically reduces the surface charge carrier recombination velocities.<br\/>Silicon conductors with a thin,high-k gate dielectrics and metal gate electrodes are envisioned to become<br\/>important by decade 's end.Equally important for more extreme scaling,will be low-k dielectrics that serve to electronically isolate one nanowire from its nearest neighbor,so that the field-gating can be localized to individual nanowires within a high density logic circuit.These issues will be addressed by combining theoretical modeling to determine effective dielectric constants of ultra-thin materials and molecular films with experimental studies incorporating atomic-layer deposition of high-k gate dielectrics (i.e.HfO2)coupled with the incorporating low-k dielectrics for separating the Si nanowire conductors.<br\/>Finally,ultra-high density patterning methods will likely be limited in terms of the physical complexity<br\/>achievable in a circuit design.This requires the incorporation of novel approaches for bridging the length scales between the sub-micron world of lithography and the nanometer world of ultra-high density circuits.It also requires novel architectural concepts to take advantage of highly-or quasi-regular patterning methods.Architectural approaches that solve these issues will provide a driver for much of the fundamental science described herein.","title":"NANO: The Fundamental Science of Ultra-High Density Logic Circuitry","awardID":"0524490","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1954","name":"QUANTUM CALCULATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["516311","293096","554822"],"PO":["565223"]},"107720":{"abstract":"Quantum information forms a new concept in information technology, which has the potential to impact many aspects of modern life, from finance to secure communications to scientific research. The quantum mechanical property known as \"spin\" lies at the heart of many quantum information technologies. The spin of an electron, for example, can carry a quantum bit of information (known as a \"qubit\"), which takes the values 0 or 1, or a superposition of the two. Like an ordinary computer, the quantum computer performs operations on its qubits. This is accomplished by bringing the qubits (e.g. electrons) into proximity, and letting them interact over a prescribed time period. Typically, the interaction distances are very small, on the order of nanometers (one billionth of a meter). This distance requirement can cause communication problems for a quantum computer, which must access thousands of qubits. Indeed, in many architecture proposals, information must be transmitted to distant qubits by iterated swapping between neighboring spins in a one-dimensional array. There is considerable overhead associated with such swap protocols, both in time and efficiency. <br\/>The present proposal puts forward a new concept for long-range, effective interactions between spin qubits, based on a \"spin-bus.\" The bus is formed from a long chain of closely spaced spins, which are fixed in place so that their interactions are constant and strong. The bus acts as a fast conduit for quantum information. Additional, external spin qubits can be coupled to the bus, by means of controlled electrical gates. Thus, the bus provides a mechanism for indirect qubits interactions at large distances. The spin-bus architecture has pros and cons that should be weighed and optimized. A particular advantage of the spin-bus approach is that it does not require any resources besides those normally assumed for spin quantum computing. Further, it intentionally exploits the most robust feature of spin-based architectures: strong coupling at short distances. <br\/>The objective of the work is to characterize and develop all aspects of the spin-bus architecture for quantum computing. The proposal involves three separate and interdisciplinary research components: Bus Operation. A thorough understanding of the spin-bus energy spectrum is needed, to characterize the possible operating modes. Inevitable, inequivalent couplings between the neighboring spins reduce the bus speed. Theoretical analysis is needed, to determine the importance of such variability, and to optimize the bus operation in terms of bus size and speed. Decoherence Analysis. Quantum information is particularly susceptible to environmental influences, which lead to errors in the quantum computer. Because the spin-bus is much larger than a single spin, unique types of errors may affect it. Analysis will focus on fluctuating and spatially varying magnetic and electrostatic fields, vibrations within the matrix containing the spins, and \"leakage\" of information from the bus into undesired spin states. Quantum Error Correction. Error correction in the spin-bus is quite different from conventional quantum computers. The special challenges to be addressed include trade-offs between long-range coupling and parallelizability, leakage control, and the tailoring of established error correction techniques for the spin-bus architecture.","title":"QnTM: EMT: Spin Bus for Quantum Information Processing","awardID":"0523675","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":["512529","494164","441771"],"PO":["565223"]},"107610":{"abstract":"The ability to redesign and even build completely new biological entities offers revolutionary opportunities for using biology to solve human problems. The dream, however, far outstrips the reality: engineering new biological machines, programming cells behaviors, and building new forms of life pose huge technical and conceptual challenges. Our research is designed to make some important first steps, by developing generally-applicable engineering methods for building synthetic gene networks. Implementation of even the most simple circuits in a biological system requires tedious optimization of a large number of poorly-understood parameters, many of which can neither be measured nor easily manipulated. Simulations can sometimes guide optimization, but we believe that biological systems are best optimized using nature's editing strategy, evolution. We believe that a combined approach of rational design based on computational predictions coupled with directed evolution-making mutations in the laboratory and selecting those organisms exhibiting the desired behaviors--will be fundamental to the progress of synthetic biology. We will in effect learn how to 'breed' useful synthetic gene networks, just as we have learned how to breed useful plants and animals.<br\/>This approach mimics natural evolution in exploring the vast and complex landscape of functions available to a set of molecules making up an engineered regulatory pathway. Importantly, it circumvents our near-complete ignorance of how a DNA sequence encodes a specific set of biological functions, a detailed understanding that is required for any 'rational' design approach. By introducing random mutations into the DNA and screening for different functions that might be expressed by the mutant circuits, we can identify which functions are possible as well as the ranges of function available to the specific search process (e.g. random point mutation targeted to a specific gene). With further analysis, e.g. sequencing to identify the mutations and biochemical analysis of circuit components, we gain insights into the molecular mechanisms by which the overall function is achieved or modified. <br\/>In this project we have three specific aims. The first is to validate a 'selection module' by which we can efficiently evolve components and circuits in the laboratory. This module connects proper circuit function to the ability of cells that express it to survive and grow. Cells with functioning circuits survive and grow; those that have not solved the problem do not. To program complex behaviors, we will also need components that respond to predefined ranges of input parameters with predictable output parameters. Thus our second aim is to use directed evolution to create a range of transcriptional activators based on the well-characterized framework protein, LuxR. Laboratory-evolved LuxR variants will activate gene transcription at different, nonnatural promoter sites on the DNA. Finally, we propose to investigate the range of circuit functions available to a predefined set of components via evolutionary exploration. Specifically, we will evolve a series of 'band detect' circuits that respond to a prespecified range of acyl-HSL concentrations. These circuits will be used to construct synthetic systems that form patterns of gene expression in the solid phase.<br\/>Our ultimate goal is to develop a fundamental enabling technology for synthetic biology as well as for developing bio-inspired modes and architectures for computing. We envision that evolved circuits and the synthetic multicellular systems that can be constructed from them will be useful to researchers developing quantitative models of gene regulation, quorum sensing, and other aspects of cellular computing.","title":"BIC: Collaborative Research: Evolutionary Optimization of Biological Circuits: Towards Cellular Programming","awardID":"0522831","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["401360"],"PO":["474792"]},"107731":{"abstract":"The goal of this project is to parse video from a moving platform in real-time to produce retinotopic maps that reveal the spatial layout of the scene as well as any independently moving objects present. The project proposes to duplicate the function of the early visual system in a multichip neuromorphic system with two hundred and thirty thousand silicon neurons (three quarters the number of pixels in a VGA image) and three billion synaptic interactions several orders of magnitude larger than anything built to date. The outcome of this project will be SEER, a subwatt, paperback-size seeing machine. <br\/><br\/>The intellectual merit of the proposed research stems from the tight synergy between the <br\/>computational theory, based on the principle of compositionality ,and the neuromorphic implementation, based on reentrant networks. Compositionality dictates that the various parts of the vision problem should be attacked simultaneously and reentrancy gives us the capability to do exactly that. <br\/><br\/>The broader impact of the proposed effort could be enormous. A multichip neuromorphic system performing multimodal segmentations would reinvigorate robotics and computer vision. By providing the infrastructure for early vision, it will facilitate the study of cognition, which will most likely generate a flood of new theories and experiments, in the Neurosciences and in the sciences of the artificial.","title":"Collaborative Proposal: Seer: A Gigascale Neuromorphic Visual System","awardID":"0523788","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["460460","531850"],"PO":["565223"]},"116575":{"abstract":"The project will explore novel software technologies to improve the time to solution of applications by increasing memory efficiency on emerging distributed systems. The approaches to be pursued include combination of simulation and hardware assists. The memory performance of distributed simulations must improve if the benefits of emergent high-end systems are to be realized. To hide the effects of memory latency, the complexity of systems used as building blocks (e.g. SMPs) in large-scale clusters has increased substantially. The combination of memory speed, node complexity and system scale often leads to poor performance. Years of intense research aimed at improving the performance of applications in parallel and distributed systems have led to average efficiencies of 5-10%. The continued exponential increase in complexity makes maintaining these efficiencies through tuning challenging. Improving efficiency dramatically will require innovation. The project will improve distributed simulation performance in SMP-based clusters through creation of a new set of software tools that enable access to coordinated hardware counter information across system components (e.g. CPU and NIC). Preliminary results indicate these multi-component techniques are inherently scalable while providing system-wide memory performance information previously unavailable without the aid of sophisticated, intrusive software profiling or simulation.<br\/><br\/>The project will leverage hardware profiling, parallel and distributed performance evaluation, statistical data reduction analysis, analytical modeling techniques and tool development with emerging hardware counter technologies on commodity CPUs and NICs to produce a software framework for locality-aware application profiling, analysis and optimization, and create a framework that provides a complete picture of local and remote memory accesses in a largescale, high-end distributed systems .","title":"Collaborative Research: CSR(SMA): Scalable performance modeling and analysis framework","awardID":"0613461","effectiveDate":"2005-08-15","expirationDate":"2007-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7469","name":"ITR-HEC"}}],"PIcoPI":["563791"],"PO":["551712"]},"107798":{"abstract":"Proposal Number: 0524402<br\/>Title: Protecting TCP Congestion Control: Tools for Design, Analysis and Emulation<br\/>PI: George Kesidis<br\/><br\/>The increasing volume of non-conforming and malicious traffic flows poses a serious challenge to the stability of the Internet. Such traffic flows could significantly throttle the data rates sustainable by TCP flows, and could affect millions of users who rely on the Internet for their daily business. The following three types of misbehaving flows: unresponsive TCP sessions, low-rate TCP-targeted attacks, and randomly scanning TCP worms, can be easy to launch and are enormously damaging.<br\/><br\/>This research takes an ambitious step in systematically developing: (i) dynamic router-based quarantine schemes to penalize unresponsive TCP flows; (ii) defense strategies for low-rate TCP-targeted attacks; (iii) router-based designs to effectively control indiscriminate TCP worms; and (iv) tools and methodologies for the evaluation of the proposed schemes, specifically using the DETER\/Emulab emulation platform. The research will enable in-depth characterization of the misbehaving flows and the design of effective solutions for minimizing the vulnerability of the Internet to such flows.<br\/><br\/>This work will have an enormous practical impact, will foster new research directions towards a trustworthy Internet, will accelerate security research by streamlining the experimental process, and will train security students in both theory and hands-on experimentation.","title":"CT-T: Collaborative Research: Protecting TCP Congestion Control: Tools for Design, Analysis, and Emulation","awardID":"0524202","effectiveDate":"2005-08-15","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7456","name":"ITR-CYBERTRUST"}}],"PIcoPI":["550859","562302"],"PO":["529429"]},"106005":{"abstract":"We propose to develop a theoretical framework and algorithms for recommendations system, where the task is to recommend products to users based on their past choices. In this proposal we present preliminary results that are strikingly better than previous ones. Specifically, the best previously known algorithm was centralized, had time complexity O(mn) (where m and n denote the number of users and products, respectively), and relied on some strong assumptions about the preferences of users. We demonstrate the following results. First, a centralized algorithm whose time complexity is O(m + n). This algorithm improves on the previous one both in complexity and in removing key \"gap\" and \"separability\" assumptions by abandoning the algebra-based approach of prior algorithms. Second, we present the first distributed algorithm for recommendation systems, which also solves an open problem posed in previous work. The algorithm is resilient against adaptive Byzantine users, which is important in the context of e-commerce. The collective work done by the users using the distributed algorithm under any asynchronous schedule is O(n + mlogm). Our algorithms can also, without increase in complexity, completely characterize each user, if the users satisfy the \"separability\" assumption.<br\/><br\/>Intellectual merit and broader impact. Our algorithms demonstrate that the methods traditionally used to attack these problems (in particular, singular value decomposition) were not necessary. In addition, we believe that our methods are sufficiently simple to be implementable, thus having a real impact on society at large.<br\/><br\/>Specifically, Hewlett Packard will be collaborating with JHU and Tel-Aviv University on technological transfer. The project will advance knowledge and education by tightly integrating undergraduate and graduate students into the project and by disseminating the results via lectures notes and publications.","title":"Collaborative Filtering and Learning","awardID":"0515080","effectiveDate":"2005-08-15","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["343097"],"PO":["550329"]},"109525":{"abstract":"Learning visual object categories, and recognizing objects in images, is perhaps the most difficult and exciting problem in machine vision today. In light of the fast growing data deluge in science, engineering, industry and society, recognition systems must be able to operate without human supervision. This poses new challenges: How can one learn automatically models of a large number of object classes from unlabelled images? How can one represent these object classes such that they can be searched efficiently? How can one leverage the learnt models to learn new object classes from very few examples?<br\/><br\/>It is proposed that these challenges may be met by inferring hierarchical representations of object classes from unlabelled image data. Object classes are represented as constellations of parts, where each part extracts shape and appearance information.<br\/>Non-parametric Bayesian techniques may be employed to organize these object classes into tree-structured representations. The richness of this representation grows incrementally as more data is presented to the system. New similarity measures between object classes naturally derive from this representation facilitating recognition.<br\/><br\/>Outreach to the local community is established through a collaboration with the California State University Northridge where students, often minorities who are the first in the family to obtain a university degree, will have the opportunity to engage in visual recognition problems proposed by and relevant to local companies.","title":"Collaborative Research: Learning Taxonomies of the Visual World","awardID":"0534984","effectiveDate":"2005-08-01","expirationDate":"2008-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7339","name":"COMPUTER VISION"}}],"PIcoPI":[289957],"PO":["564316"]},"106038":{"abstract":"This research concerns protocols used for the efficient transmission of data in the Internet and other distributed communication networks, such as wireless networks. The research focuses on routing, congestion control and heterogeneity. Routing concern the choice of routes for messages and the enforcement of those routes by local decisions governed by routing tables stored at the routers. It is a fundamental constraint<br\/>that the sizes of these tables must be small. Congestion control strives to reduce congestion by selectively dropping packets from flows that are receiving more than their fair share of bandwidth. The issue of heterogeneity is whether it is better to have the capacity of a distributed system divided evenly or unevenly among its processors.<br\/><br\/>Geographic routing assigns coordinates to each processor, placing it in a metric space. The research investigates the correctness and efficiency of geographic routing schemes. Special cases of geographic routing include greedy routing, in which each hop is chosen to minimize distance to the destination, and beacon routing, in which each coordinate of a host gives the minimum number of hops from the host to a selected beacon node. The research explores the loss of efficiency imposed by routing constraints of the type arising in the Border Gateway Protocol. The ability of overlay nodes to improve routing efficiency is explored. The construction of compact multicast routing tables is investigated. Efficient methods are given for identifying high-rate flows and dropping their packets preferentially.<br\/>Finally, evidence is provided that systems are more robust when their capacity is distributed heterogeneously among the processors.","title":"Design and Analysis of Internet Algorithms: Routing, Filtering, Overlay Design and Influence of Heterogeneity","awardID":"0515259","effectiveDate":"2005-08-15","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["561682","558880","508219"],"PO":["432103"]},"110581":{"abstract":"Distributed applications and the hardware\/software platforms on which they<br\/>run are becoming increasingly complex. As a result, it is also becoming<br\/>increasingly difficult to understand and then manage application behavior,<br\/>particularly for applications that are critical to the ability of<br\/>organizations to provide services for which they have contractual obligations.<br\/><br\/>Georgia Tech's `Service Paths' project is developing technologies for<br\/>(1) dynamically discovering the critical `paths' applications follow<br\/>through distributed sets of services, and (2) managing such paths in<br\/>order to improve an application's ability to meet dynamic service level<br\/>agreements. Service path technologies will substantially improve systems'<br\/>abilities to provide end-to-end guarantees and enhance the underlying<br\/>systems' capabilities to provide such guarantees. Concrete outcomes include<br\/>new system-level methods and abstractions to better understand current<br\/>application behavior, new middleware abstractions suitable both for<br\/>constructing and managing large-scale software overlays across many<br\/>machines, and experimental results attained with virtualized platforms<br\/>like PlanetLab and with novel hardware support for runtime platform<br\/>virtualization.<br\/><br\/>Broader significance: the costs of developing and changing modern IT<br\/>infrastructures are now dominated by personnel rather than hardware.<br\/>Technology providers have reacted to this fact by making it increasingly<br\/>easy to develop complex distributed applications. Without also<br\/>understanding and then being able to manage and control such applications,<br\/>cost savings attained at development time cannot be translated to the<br\/>continuing cost savings required to make U.S. industry competitive with<br\/>low-cost, international providers.","title":"SGER: Service Paths -- Optimizing end-to-end Behaviors in Distributed Service Architectures","awardID":"0541199","effectiveDate":"2005-08-01","expirationDate":"2007-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["523077"],"PO":["543507"]},"104773":{"abstract":"As software continues to grow in complexity, an important requirement that is emerging is software must be able to change at run-time. Unfortunately, software engineering techniques that are useful in building robust dynamically modified software are woefully lacking. In general, extensive testing and debugging are the raditional ways of ensuring the robustness of sofware. The problem of debugging a program wherethe underlying code is changing at run-time makes the problem all the more difficult. <br\/><br\/>The aim of this proposed research is to address this situation and develop new techniques for debugging dynamically modified software. The key idea is that debugging for such software must extend through the code modification and allow debugging on code that has been changed and to see how past and future adaptations has\/can affect execution. The proposed work considers several types of dynamic code modifications, including dynamically optimized code, dynamically applied code patches, components and dynamically linked libraries. New debug directives and queries will be developed specifically to address the challenges of debugging such code. Novel techniques based on code analysis, reverse execution, checkpointing, and instrumentation optimization will be used to enable these new directives and queries.<br\/><br\/>This research has both fundamental and software contributions, including: (1) a better understanding of the constraints and trade-offs that exist for developing robust dynamically modified software, (2) a framework that supports the construction of debugging techniques and tools that are useful when developing dynamically software, (3) the development of debug strategies, directives and queries that are designed to handle the special challenges of dynamically modified code, and (4) the development of a set of debugging tools for dynamic software that will be widely distributed. The techniques, tools and algorithms that will be developed through the course of this research will contribute significantly to understanding how modern software development techniques can be incorporated with dynamic code modifications.","title":"Collaborative Research: CSR--AES--Debugging Dynamic Code Modifications","awardID":"0509237","effectiveDate":"2005-08-01","expirationDate":"2007-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["438734"],"PO":["551712"]},"104322":{"abstract":"NIRT: Semiconductor nanostructures and photonic crystal microcavities for quantum information processing at Terahertz frequencies<br\/>Abstract<br\/>This grant will explore the fundamental physics of elements in a proposed semiconducting quantum information processor (QIP) which is potentially scalable to ~1000 quantum bits (qubits). The qubits in the envisioned QIP are the two lowest orbital states of electrons bound to shallow donors (D0) in GaAs or bound in elongated self-assembled quantum dots called quantum dashes (QDAs). QDAs will be grown by molecular beam epitaxy on patterned substrates. The resonance frequency of D0 and QDA-based qubits will be between 1 and 4 Terahertz (THz). The energy relaxation and decoherence rates of these qubits will be measured, and are predicted to be slow because the resonant frequencies are well below that of an optical phonon. GaAs and Si Photonic crystal resonators for THz frequencies will also be fabricated and characterized. Finally, qubits will be incorporated into resonators and reversible coupling of energy between the resonator and qubits will be investigated. <br\/><br\/>This research program works at two scientific and technological frontiers: harnessing quantum mechanics for information processing, and developing the portion of the electromagnetic spectrum between 1 and 10 THz (THz-1 THz=one trillion cycles\/s). The research will explore a new approach to quantum information processing in semiconductors, enhance our fundamental understanding of the transfer of information and energy between simple quantum systems and their semiconducting hosts, and create new materials and structures in which to store THz light and control its interaction with matter.","title":"NIRT: Semiconductor nanostructures and photonic crystal microcavities for quantum information processing at Terahertz frequencies","awardID":"0507295","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0301","name":"Division of PHYSICS","abbr":"PHY"},"pgm":{"id":"1248","name":"PHYSICS-OTHER"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1674","name":"NANOSCALE: INTRDISCPL RESRCH T"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"1710","name":"CONDENSED MATTER PHYSICS"}}],"PIcoPI":["529906","399761",275423,"455512"],"PO":["565157"]},"107842":{"abstract":"Quantum computing offers powerful new ways to solve cryptographic problems far more efficiently than classical computers can. This project focuses on the development of new quantum algorithms for problems such as Graph Isomorphism, for which there is no known efficient algorithm on classical computers. We focus on the Hidden Subgroup Problem (HSP) and its relatives. The HSP framework made its first appearance in the seminal work of Simon and Shor, where it led to efficient quantum algorithms for several basic problems in computational number theory, including integer factoring and computing discrete logarithms. In particular, Shor's algorithm for the HSP on the cyclic group makes it possible to break the RSA public-key cryptosystem.<br\/><br\/>The hidden subgroup problems appearing in Simon's and Shor's algorithms take place over commutative groups, a case of the HSP that is now well-understood. The noncommutative hidden subgroup problem is intimately linked to several problems of major interest, including Graph Isomorphism, hidden shift problems, and cryptographically important cases of the Shortest Lattice Vector problem. Despite these incentives, however, the noncommutative HSP has largely resisted the quantum computing community's advances thus far. Efficient algorithms are only known for a few families of groups, and even the information--theoretic aspects of the problem are poorly understood. <br\/><br\/>This project will seek both efficient quantum algorithms and query-complexity lower bounds for the symmetric group---the case of the HSP relevant to Graph Isomorphism---and other groups of algorithmic interest. Our approach applies the rich mathematical tools of representation theory, adapted bases, and entangled measurements over multiple coset states.","title":"QnTM: Collaborative Research: The Quantum Complexity of Algebraic Problems","awardID":"0524613","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":["531365"],"PO":["565157"]},"107887":{"abstract":"An interdisciplinary approach involving concepts from computer science, mathematics, statistical physics, and communication sciences, is proposed to develop an integrated analytical and software platform that would simultaneously facilitate the analysis and understanding of several types of biological and social networks, including DNA-protein, protein-protein, metabolic, and inter-cellular signaling networks, and the design of information and computational networks, including fault-tolerant and attack-resistant federated databases and ad hoc communication networks, and distributed systems such as critical infrastructure networks (e.g., national power grids). <br\/><br\/>The field of complex networks has evolved in a highly synergistic fashion, involving the physical and life sciences, computing and engineering disciplines. This proposal aims to develop a common set of tools for the diverse communities, thus promoting cross-pollination of ideas, and spurring the development of integrated technologies and innovations. In the past, most work on complex networks has been analysis-oriented; that is, identifying and measuring different characteristics of existing systems (e.g., the Internet, protein-protein interactions, empirical social networks) and proposing potential dynamics that could have led to the emergence of such systems. This proposal, focuses on a more balanced approach, where in addition to the modeling work, it also addresses the issue of how the modeling work can be harnessed to design information networks, and how to formulate systematic methods for mining the information available from complex networks. Specific topics and issues pursued include: (1) Network Tomography - Inferring hidden structures from observed data: What are the structural characteristics of biological networks and how have they evolved and formed? The recent flood of data concerning large-scale biological networks are just beginning to be analyzed, and the PI proposes to design a set of complex network tools to address these issues. (2) Information Dynamics In Complex Networks: How is information processed and communicated in complex networks? Can synchronization or stable communication among nodes arise in networks where the connectivity is itself time varying and the same pair of nodes are only intermittently connected? (3) Net-Modeler: development of a comprehensive complex network modeler to be used for both static and dynamical explorations of biological networks; (4) Designer Complex Systems - Designing Dynamic Information and Computing Networks: How to harness the structure and functionalities of the biological and other complex networks to design local protocols of a distributed and ad hoc networked system, so that the system has the desired global properties, such as low-latency and robust functionality? <br\/><br\/>The methodologies and software tools developed in the proposal will be integrated into a virtual web-based laboratory. This will lead to (i) a nationwide resource for the analysis of biological and social networks, and (ii) interdisciplinary undergraduate and graduate courses and training programs on complex networks and their applications.","title":"BIC: From Cellular and Gene Networks To Principles of Robust Communication and Distributed Systems Design","awardID":"0524843","effectiveDate":"2005-08-15","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["455118"],"PO":["565223"]},"107799":{"abstract":". Under NSF EMT Proposal \"Scalable DNA molecular computation\" a team of scientists and students at the University of California, Riverside will investigate the possibility of making a large-scale neural network computer based on the interactions of DNA molecules. The research team includes the principle investigator, Professor Allen Mills from the Department of Physics, Professor Jerome Schultz from the Department of Chemical and Environmental Engineering and several graduate and undergraduate students. The possibility of doing practical computations using groups of a few molecules as computing elements is intriguing because one might be able to make molecular computers much more powerful than any electronic computer known today. This is because DNA molecules of the required size are billions of times smaller in volume than the smallest feature on a modern computer chip, so that the DNA equivalent of a powerful microprocessor could in principle fit into a tiny droplet of liquid. One can easily project that a DNA computing machine with the physical size of a laptop could contain millions of such droplets that would have a composite computing power exceeding that of a human brain. <br\/>The first indication that computing could be done with DNA molecules came in 1994 when Prof. Len Adleman, working at UCLA, dramatically demonstrated that certain reactions, in which various DNA molecules in solution find, recognize and bind to complementary DNA molecules, may be utilized for the purpose of doing computations with molecules. It is encouraging that Adleman and his coworkers have recently used a related scheme to solve a problem involving one million different DNA molecules.<br\/>One stumbling block on the path to the enormous computing potential of DNA is the anticipated accumulation of errors in a computation due to imperfect molecular reactions. The new NSF proposal seeks to achieve \"Scalable DNA molecular computation\" by configuring a DNA molecular computer as a brain-like neural network, a computing architecture that uses redundancy to make it inherently fault tolerant and therefore possibly scalable to virtually any size. <br\/>The first practical neural network architecture, and one that is perhaps ideal for testing a DNA neural network, is the content-addressable memory neural network invented by Prof. John Hopfield at Cal Tech and Bell Labs in the early 1980's. In the first phase of the proposal, the students under the guidance of their mentors will learn how to use biochemical reactions naturally occurring in living cells to make the necessary components for a DNA neural network. In the second phase of the work, a small-scale molecular computer will be made to demonstrate the principle of operation of a DNA neural network. Plans will be drawn up for scaling this network up by an order of magnitude to search for size limitations in this type of molecular computing. <br\/>The prospect of making neural networks with a size approaching that of a human brain is very intriguing and raises interesting questions about how one would program or \"train\" a very large network, how it would communicate, whether its operation could be autonomous and whether it could exhibit self-organization and emergent behavior.<br\/>The proposed work will demonstrate whether DNA neural networks could be competitive with other technologies in certain niches. One of the most important applications could be the use of DNA neural networks trained to recognize the DNA signatures of certain cancer cell types and for performing inexpensive genetic profiling tests to determine the susceptibility of people to a spectrum of the most common diseases such as various cancers, asthma, diabetes and hemochromatosis where dozens of genetic defects in different combinations may be involved. Thus even if giant networks prove to be infeasible, the eventual beneficiaries of relatively small networks may be patients that would receive improved therapy based on the availability of an inexpensive and rapid diagnostic tool, and the general populace who will be able to adjust their lifestyles based on genetic knowledge. The prospects for large DNA neural networks are uncertain at this time, but in principle they could be the basis for an entirely new computing paradigm. In any case, the proposed research will provide excellent training for two PhD students and several undergraduate students in the emerging field of biophysics at a University noted for its large component of students from minority backgrounds.","title":"NANO: EMT: Scalable DNA Molecular Computation","awardID":"0524203","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7353","name":"EMERGING MODELS & TECHNOLOGIES"}}],"PIcoPI":["512072"],"PO":["565223"]},"107337":{"abstract":"This project, enabling high performance computing, aims at purchasing and operating a supercluster to complement current federal and state investments and help drive related research and educational activities. The institution has initiated a collaborative enterprise among the Department of Computer Science and Engineering and five other PhD-granting departments from the Colleges of Engineering and Science (Biomedical Science, Chemistry, Electrical Engineering, Mathematics, Physics, and local industries) and has established a Center High Performance Computing. Housed in the Center, the cluster involves research groups from computer systems and scientific computing. The Center services:<br\/>-Research: on high performance computing;<br\/>-Development: tools for parallel programming on the supercluster; <br\/>-Service: other departments on campus, and also local community, such as area high schools;<br\/>-Education: computing environment enables courses for students to gain hand-on experience; and<br\/>-Collaboration: joint research with local industries to develop advanced tools for designing and executing parallel programs.<br\/><br\/>The research projects, many involving extensive computation, include:<br\/>-Porting of TICC-PP software infrastructure to Linux<br\/>-Video coding,<br\/>-Biometrics,<br\/>-Mathematical problems, Physics of materials, and<br\/>-Genomics.<br\/><br\/>Broader Impact: The platform fosters collaborations with local industries, higher and K-12 education, and government agencies. Moreover, it contributes to train a diverse population of students in high performance computing.","title":"MRI: Acquisition of a NUMA-based Supercluster for High Performance Computing","awardID":"0521410","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7399","name":"CISE MINOR INST INFRA (MII) PR"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"9199","name":"UNDISTRIBUTED PANEL\/IPA FUNDS"}}],"PIcoPI":["523055",283387,"459379","7594",283390,"523055","523055","294774"],"PO":["557609"]},"109658":{"abstract":"Teresa Head-Gordon and Martin Head-Gordon of the University of California at Berkeley, Vijay Pande of Stanford University, and Jay Ponder of Washington University are supported by the NSF Divisions of Chemistry and Shared Cyberinfrastructure, under the Cyberinfrastructure and Research Facilities Program. This collaborative project includes extensive development and validation of polarizable force fields for use in biomolecular simulations, public distribution of resulting software components and parallelized computer codes, and workshops to disseminate these tools to the research community.<br\/><br\/>Biomolecular simulations lie at the heart of physically-driven, atomistic approaches to protein\/water dynamics. Empirical force fields are at the core of such simulations, and together with the computer programs that employ them, they define the central cyberinfrastructure in this field. Outcomes of this project include publicly available parameter sets and software, along with workshops for dissemination.","title":"Collaborative Research: Cyberinfrastructure for Next Generation Biomolecular Modeling","awardID":"0535710","effectiveDate":"2005-08-15","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1938","name":"CHEMICAL INSTRUMENTATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"7368","name":"SCI TESTBEDS"}}],"PIcoPI":["540363","540364"],"PO":["563091"]},"110681":{"abstract":"The National Science Foundation (NSF) and the Defense Advanced Projects Research Agency (DARPA) jointly sponsor this workshop on the Future Directions in Network Modeling, Simulation, and Measurement research. The goal of this workshop is to bring together both developers and potential users of advanced network modeling, simulation, and measurement technology in order to both assess the current state of the art and to provide recommendations concerning the next steps that should be taken to maximize the impact of such technologies within the networking research community and industry","title":"Workshop on Future Directions in Network Modeling, Simulation, and Measurement, August 15-16, 2005, Arlington, VA","awardID":"0541670","effectiveDate":"2005-08-15","expirationDate":"2007-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"T113","name":"DARPA-FUTURE DIRE IN NETWORKIN"}}],"PIcoPI":["545655","536648","434457"],"PO":["565090"]},"105984":{"abstract":"This project aims to establish an altogether new paradigm in computer design verification by synergistically integrating polynomial algebra, ring theory and algorithm development, all within a VLSI-CAD based verification framework. As digital designs proceed through various synthesis and optimization stages, it is required to verify the functional equivalence of different design implementations. However, the growing complexity of digital systems is limiting the scope and applications of contemporary verification tools. This has particularly affected efficient verification of polynomial signal processing and multimedia applications where arithmetic datapath computations are implemented at register-transfer-level (RTL). For such designs, the verification problem can be modeled as that of proving polynomial equivalence over finite integer rings of the form Z_{2^k}, where k is the size of the datapath operands. In this project properties of these integer rings are being thoroughly investigated, and used to investigate algorithms for verification of digital circuits modeled at the register-transfer-level.<br\/><br\/>In this collaborative research, the PIs will: (1) study and derive new mathematical techniques to verify equivalence of multi-variate polynomials over finite rings of the form Z_{2^k}; (2) derive algorithmic procedures to prove polynomial equivalence in Z_{2^k}, within a CAD-based RTL verification framework; and (3) explore the above concepts in the context of efficient RTL synthesis of polynomial datapaths. The novelty of the problem lies in its mathematical challenge and in its engineering applications to RTL datapath verification. Successful completion of this project would broadly impact RTL datapath verification technology and enhance the understanding of some of the unresolved problems in classical mathematics.","title":"Collaborative Research: A New Theoretical and Algorithmic Framework for RTL Datapath Verification using Polynomial Algebra over Finite Integer Rings","awardID":"0514966","effectiveDate":"2005-08-01","expirationDate":"2006-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["550799"],"PO":["321058"]},"107810":{"abstract":"Proposal 0524286<br\/>CT-ISG: Tracing VoIP Calls Through The Internet<br\/>Xinyuan Wang, George Mason University<br\/><br\/>In order to combat crime and terrorism, law enforcement agencies (LEA) need wiretap and call identifying capability to conduct lawful electronic surveillance. However, the proliferation of VoIP calls has imposed significant new challenges in providing the same call-identifying and wiretap capability as that exist in traditional circuit-switched networks. For example, VoIP calls could be setup by proprietary signaling protocols, and the VoIP traffic could easily be encrypted by publicly available encryption software.<br\/><br\/>The objective of this project is to investigate how VoIP calls can be effectively identified and traced in the Internet and develop efficient tracing methods with sound scientific foundation. The research activities will be focused on (1) developing a new form of call-identifying information for VoIP calls that is applicable to any VoIP call no matter what VoIP signaling protocols are used; (2) establishing the quantitative tradeoff models and the ultimately achievable call-identifying capability in the presence of adversary; (3) developing proof of concept prototype that can transparently embed unique call-identifying information to VoIP calls at real-time. <br\/><br\/>The outcome of this research will help to answer fundamental questions about the technical feasibility of identifying VoIP calls, and provide the necessary scientific and engineering foundations for a critical but currently missing capability in the fight on crime and terrorism.","title":"CT-ISG: Tracing VoIP Calls Through The Internet","awardID":"0524286","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["390316"],"PO":["402055"]},"105995":{"abstract":"This project aims to establish an altogether new paradigm in computer design verification by synergistically integrating polynomial algebra, ring theory and algorithm development, all within a VLSI-CAD based verification framework. As digital designs proceed through various synthesis and optimization stages, it is required to verify the functional equivalence of different design implementations. However, the growing complexity of digital systems is limiting the scope and applications of contemporary verification tools. This has particularly affected efficient verification of polynomial signal processing and multimedia applications where arithmetic datapath computations are implemented at register-transfer-level (RTL). For such designs, the verification problem can be modeled as that of proving polynomial equivalence over finite integer rings of the form Z_{2^k}, where k is the size of the datapath operands. In this project properties of these integer rings are being thoroughly investigated, and used to investigate algorithms for verification of digital circuits modeled at the register-transfer-level.<br\/><br\/>In this collaborative research, the PIs will: (1) study and derive new mathematical techniques to verify equivalence of multi-variate polynomials over finite rings of the form Z_{2^k}; (2) derive algorithmic procedures to prove polynomial equivalence in Z_{2^k}, within a CAD-based RTL verification framework; and (3) explore the above concepts in the context of efficient RTL synthesis of polynomial datapaths. The novelty of the problem lies in its mathematical challenge and in its engineering applications to RTL datapath verification. Successful completion of this project would broadly impact RTL datapath verification technology and enhance the understanding of some of the unresolved problems in classical mathematics.","title":"Collaborative research: a new theoretical and algorithmic framework for RTL datapath verification using polynomial algebra over finite rings","awardID":"0515010","effectiveDate":"2005-08-01","expirationDate":"2007-04-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["550826"],"PO":["321058"]},"104785":{"abstract":"CNS-0509297 CSR\/PDOS: Security and Incentives for Overlay Network<br\/>Infrastructure<br\/>Abstract: Proposed Research<br\/>Overlay network systems have largely been engineered to operate in a world where every<br\/>node cooperatively executes the protocol as specified. Nodes either operate correctly, or<br\/>cease to work altogether (i.e., fail-stop). This model is quite successful when nodes<br\/>can trust one another, but this limits overlay systems from operating in the more general<br\/>case when they might be operating on untrusted, end-user computers. Such users will be<br\/>economically rational, meaning they will seek to get as much benefit from the network as<br\/>possible while contributing as little of their own resources. Our research will examine<br\/>extensions to these systems where nodes perform accounting, allowing them to track<br\/>which nodes are and are not performing correctly, making it possible to offer preferential<br\/>service to nodes that offer preferential service in return. These and other related<br\/>techniques will ultimately result in incentives-compatible designs that enable a new<br\/>generation of peer-to-peer applications, including distributed backup systems, distributed<br\/>file systems, and distributed email systems. Our work will be disseminated as part of the<br\/>FreePastry project, which provides a high-quality and scalable implementation of a<br\/>variety of overlay network services and is available under a BSD-style license. Likewise,<br\/>our work will become part of the next-generation Tor anonymous communication system.","title":"CSR\/PDOS: Security and Incentives for Overlay Network Infrastructure","awardID":"0509297","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["282239","548177"],"PO":["493916"]},"107700":{"abstract":"Quantum computing offers powerful new ways to solve cryptographic problems far more efficiently than classical computers can. This project focuses on the development of new quantum algorithms for problems such as Graph Isomorphism, for which there is no known efficient algorithm on classical computers. We focus on the Hidden Subgroup Problem (HSP) and its relatives. The HSP framework made its first appearance in the seminal work of Simon and Shor, where it led to efficient quantum algorithms for several basic problems in computational number theory, including integer factoring and computing discrete logarithms. In particular, Shor's algorithm for the HSP on the cyclic group makes it possible to break the RSA public-key cryptosystem.<br\/><br\/>The hidden subgroup problems appearing in Simon's and Shor's algorithms take place over commutative groups, a case of the HSP that is now well-understood. The noncommutative hidden subgroup problem is intimately linked to several problems of major interest, including Graph Isomorphism, hidden shift problems, and cryptographically important cases of the Shortest Lattice Vector problem. Despite these incentives, however, the noncommutative HSP has largely resisted the quantum computing community's advances thus far. Efficient algorithms are only known for a few families of groups, and even the information--theoretic aspects of the problem are poorly understood. <br\/><br\/>This project will seek both efficient quantum algorithms and query-complexity lower bounds for the symmetric group---the case of the HSP relevant to Graph Isomorphism---and other groups of algorithmic interest. Our approach applies the rich mathematical tools of representation theory, adapted bases, and entangled measurements over multiple coset states.","title":"QnTM: Collaborative Research EMT: The Quantum Complexity of Algebraic Problems","awardID":"0523456","effectiveDate":"2005-08-01","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":["486281"],"PO":["565157"]},"118832":{"abstract":"","title":"Integrating Models of Trust, Gossip, and Emotion for Artifical Agents","awardID":"0623951","effectiveDate":"2005-08-24","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6850","name":"DIGITAL SOCIETY&TECHNOLOGIES"}}],"PIcoPI":["397037"],"PO":["564456"]},"107711":{"abstract":"This grant supports theoretical research on fundamental issues related<br\/>to the implementation of quantum computation in solid-state<br\/>devices. Since the discovery that certain tasks could be performed<br\/>with great efficiency by algorithms based on quantum mechanics, an<br\/>intense effort has been made to find suitable quantum<br\/>hardware. Although several proposed implementations, such as those<br\/>based on nuclear magnetic resonance and atomic trapping, have passed<br\/>the proof-of-principle, few-qubit phase, the path to achieving a<br\/>reliable multi-qubit quantum computer is still undefined.<br\/><br\/>In this proposal we investigate the physical limitations to resilient<br\/>computation with solid-state quantum bits (qubits), such as<br\/>semiconductor quantum dots and superconductor junctions. While<br\/>solid-state qubits seem easily scalable from the fabrication<br\/>viewpoint, they also present high decoherence rates as compared to<br\/>other implementations. One major concern is that such strong <br\/>decoherence may lead to errors occurring at a rate too large to <br\/>be controlled.<br\/><br\/>However, differently from other nuclear, atomic, and optical qubits, <br\/>the interaction of solid-state quantum devices with the environment <br\/>can introduce strong memory effects. As a result, temporal correlations <br\/>may appear during the operation of multi-qubit systems. Current quantum <br\/>error correction codes are not designed to cope with this situation, <br\/>which may then invalidate any error threshold estimate for solid-state <br\/>qubits based on the efficiency of those codes.<br\/><br\/>We will explore these issues in a comprehensive way. Starting from a<br\/>thorough study of the mechanisms of decoherence in single- and<br\/>double-qubit systems, we will study a model of multi-qubit systems in<br\/>the presence of correlated noise in a variety of realistic<br\/>conditions. Our results will help set up new strategies for the<br\/>operation of multi-qubit systems. They will also let us understand<br\/>what are the constraints that error correction codes will need to<br\/>satisfy in order to achieve fault-tolerant quantum computation in<br\/>large-scale solid state implementations. To achieve our goals, we have<br\/>put together a team of researchers with expertise in nanoscale physics <br\/>and computer science. The final outcome of our project will be a much<br\/>better understanding of how a real solid-state quantum computer would<br\/>behave.","title":"BioComp: Collaborative Research: Is Resilient Quantum Computing in Solid State Systems Possible?","awardID":"0523603","effectiveDate":"2005-08-15","expirationDate":"2009-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1708","name":"QuBIC"}}],"PIcoPI":["379546","486193"],"PO":["565157"]},"102178":{"abstract":"ABSTRACT<br\/>Proposal: CNS 0454404<br\/>PI: Dirk C. Grunwald<br\/>Institution: University of Colorado at Boulder <br\/>Title: CRI: Wireless Internet Building Blocks for Research, Policy, and Education<br\/>Program: NSF 04-588 CISE Computing Research Infrastructure<br\/><br\/> <br\/>This project addresses sharing in the radio spectrum; current estimates are that about 10% of allocated spectrum is actually used. The PI's will address this problem with methods for agile spectrum use, including use of non-contiguous spectrum, reservation of spectrum as an alternative to ownership, and smart radios. This award will fund equipment used for three broad purposes: measurement of spectrum use which will enable developing models of use and diagnostic tools for experimentation, research on wireless networking with a wireless testbed, and providing an infrastructure for real-time capture and analysis of the wireless testbed spectrum use. In addition to supporting several research activities at the University of Colorado, broader impacts include possible impacts on telecommunications policy through technology investigations and demonstrations and impacts on education at the university.","title":"CRI: Wireless Internet Building Blocks for Research, Policy, and Education","awardID":"0454404","effectiveDate":"2005-08-01","expirationDate":"2010-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["543625","529582",269399,"543626","518457"],"PO":["565090"]}}