{"197791":{"abstract":"This INSPIRE award is jointly funded by the Information Integration and Informatics Program in the Information and Intelligent Systems Division of the Computer and Information Sciences Directorate, the Marine Geology and Geophysics Program in the Ocean Sciences Division of the Geosciences Directorate, the Arctic Natural Sciences Program in the Arctic Sciences Division and the Antarctic Glaciology Program in the Antarctic Sciences Division in the Office of Polar Programs, and the Office of Cyberinfrastructure. <br\/><br\/>The critical first step in the analysis of paleoclimate records like ice or sediment cores is the construction of an age model, which relates the depth in a core to the calendar age of the material at that point. The reasoning involved in age-model construction is complex, subtle, and scientifically demanding because the processes that control the rate of material accumulation over time, and that affect the core between formation and sampling, are unknown. Geoscientists approach this problem by treating the core like a crime scene and asking the question: \"What physical and chemical processes could have produced this situation, and what does that say about the timeline?\" However, the sheer number of possibilities, coupled with the volume and complexity of the climatology data that is currently available and is continually collected, severely limit the scope of these investigations. The goal of this project is to remove this roadblock. This research will lead to an integrated software tool called CScibox, that uses automated reasoning techniques to help scientists analyze ice and sediment cores. It employs a cyberinfrastructure that provides powerful, intuitive tools on a scientist's desktop while taking full advantage of modern data- and computation-intensive computing and networking infrastructure -- including workflow-based computation, parallel execution, distributed systems, cluster machines and the cloud. <br\/><br\/>CScibox will not only improve the ability of individual geoscientists analyze single cores; it has the potential to transform the field of paleoclimatology by facilitating rapid, reproducible analysis and synthesis of the information in the diverse collections of raw data available in data archives to foster understanding and improved scientific decision making. This will have broad impacts for society by allowing scientists to develop deeper insights into the roles of various factors in the complex relationships that give rise to geological records of the earth's climate that are used in today's models of environmental change. This project also has a broad educational impact. Students involved in the development and implementation of CScibox will develop skills in interdisciplinary research and will learn how to apply computational methodology in a challenging scientific context that has not yet significantly benefitted from developments in information technology. CScibox is designed to be easy to install and use; see the project web site (http:\/\/www.cs.colorado.edu\/~lizb\/cscience.html) for source code, documentation, and examples of its use. Future steps include extending the work to other paleoclimate data, working with geoscientists to make the user interface as intuitive as possible, and holding demos and workshops at geosciences conferences to educate that community about what the tool can do and how to use it.","title":"INSPIRE: Automating Reasoning in Interpreting Climate Records of the Past","awardID":"1245947","effectiveDate":"2012-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0405","name":"Division of OF SOCIAL AND ECONOMIC SCIENCE","abbr":"SES"},"pgm":{"id":"8078","name":"INSPIRE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"06","name":"Directorate for DIRECTORATE FOR GEOSCIENCES             ","abbr":"GEO"},"div":{"id":"0602","name":"Division of DIV ATMOSPHERIC & GEOSPACE SCI","abbr":"AGS"},"pgm":{"id":"1530","name":"PALEOCLIMATE PROGRAM"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0707","name":"Division of INDUSTRIAL INNOVATION & PARTNE","abbr":"IIP"},"pgm":{"id":"7231","name":"CYBERINFRASTRUCTURE"}},{"dir":{"id":"10","name":"Office of OFFICE OF BUDGET, FINANCE, & AWARD MANAG","abbr":"BFA"},"div":{"id":"1001","name":"Division of BUDGET DIVISION","abbr":"DOB"},"pgm":{"id":"1620","name":"MARINE GEOLOGY AND GEOPHYSICS"}},{"dir":{"id":"14","name":"Office of OFFICE OF POLAR PROGRAMS                ","abbr":"OPP"},"div":{"id":"1403","name":"Division of ANTARCTIC SCIENCES DIVISION","abbr":"ANT"},"pgm":{"id":"5116","name":"ANTARCTIC GLACIOLOGY"}},{"dir":{"id":"14","name":"Office of OFFICE OF POLAR PROGRAMS                ","abbr":"OPP"},"div":{"id":"1403","name":"Division of ANTARCTIC SCIENCES DIVISION","abbr":"ANT"},"pgm":{"id":"5280","name":"ARCTIC NATURAL SCIENCES"}},{"dir":{"id":"14","name":"Office of OFFICE OF POLAR PROGRAMS                ","abbr":"OPP"},"div":{"id":"1403","name":"Division of ANTARCTIC SCIENCES DIVISION","abbr":"ANT"},"pgm":{"id":"5407","name":"POLAR CYBERINFRASTRUCTURE"}}],"PIcoPI":["542878",530767,530768,530769],"PO":["563751"]},"208725":{"abstract":"Developed economies are largely based on impersonal exchange; trade does not require high levels of information about others' past behavior. Consequently, lack of trust, social distance, the possibility to break promises and other trade frictions all hinder economic activity. This project employs theoretical and experimental methodologies to study individual and group behavior in economies populated by individuals who do not know each other and may not trust each other. These strangers interact in pairs with randomly changing opponents whose identities and past behaviors are unknown. In each trade encounter, individuals can either cooperate or behave opportunistically. The sequence of encounters is indefinite, so many outcomes are possible. Theoretically, self-regarding individuals can overcome opportunistic temptations, and attain the best outcome, by adopting community-based punishment schemes. The research will pursue a systematic study of small and large laboratory economies based on these indefinitely repeated matching games. It will advance knowledge in the social and economic sciences by identifying behavioral elements and endogenously-arising institutions that are associated to the emergence, sustainability, and breakdown of cooperation among individuals who may neither know nor trust each other. New insights about cooperation will have applications to disciplines in which the study of social decision problems is an active area of research, such as political science, accounting, and sociology.","title":"ICES: Small: Institutions for Cooperation in Large Economies of Strangers","awardID":"1341805","effectiveDate":"2012-08-01","expirationDate":"2014-04-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8052","name":"Inter Com Sci Econ Soc S (ICE)"}}],"PIcoPI":[559522],"PO":["565251"]},"198572":{"abstract":"The PI's goal in this exploratory research is to tackle the fundamental obstacle preventing high quality, interactive animation of natural phenomena, namely the enormous number of degrees of freedom involved. Because hand animating a typical mesh is a tedious and time consuming task, computer graphics has turned to physics and simulation to animate most natural phenomena. In this context, the promise of simulation is generality; an infinite space of material properties and initial conditions can be explored. This generality is also simulation's greatest limitation; the space of possible animations is vast, while the space of desirable animations is a great deal smaller. The PI's approach is to use simulation's strength (its ability to create rich animation data under a variety of conditions) to combat its greatest limitations (high dimensionality and computational expense).<br\/><br\/>To this end, he will develop machine learning tools for finding new and more expressive low-dimensional representations, which do not describe all possible animations but rather succinctly describe the space of desirable animations. Previous attempts to apply machine learning to the animation of natural phenomena have shown promise, but also significant limitations. These approaches have suffered from over-fitting, have sacrificed locality, and have not allowed artistic control over the space of possible animations. Furthermore, these approaches have been too data-driven, failing to allow for the input of valuable human knowledge and intuition or mathematical and physical models. Until these limitations are addressed, the promise of high-quality interactive computer animation of natural phenomena will remain out of reach. For concreteness the PI will focus on cloth and fluids as test bed domains (initially assuming an algorithmic paradigm of coarse simulation enhanced by data-driven upsampling operators), for which he will explore questions of sparseness, expanded feature sets, combining operators, and artistic control. <br\/><br\/>Broader Impacts: Simulation is a powerful technique whose usefulness is not limited to computer animation. So while the test bed domains fall within the realm of traditional computer graphics, project outcomes will allow for high-quality, interactive computer animation of natural phenomena across all of science and engineering, with particular applicability to film, video games, virtual reality, medical training, etc. Moreover, the unique context of computer animation will necessarily require new machine learning algorithms that will feed back into that community as well. The PI plans to develop and release the majority of his source code under free BSD licenses.","title":"EAGER: Learning Upsampling Operators for Animation of Cloth and Fluids","awardID":"1249756","effectiveDate":"2012-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["548348"],"PO":[532791]},"189761":{"abstract":"This award provides funding for a new collaborative Research Experiences for Teachers (RET) Site focused on Signal and Image Processing at the University of Central Florida and Florida Institute of Technology. Each year 13 high school teachers from school districts in four Central Florida counties will participate in research projects at the universities. The teachers will also develop modules related to their research which they will implement in their classes in the following school year. The teachers will organize annual science events to share their work with the communities around their schools and they will participate in the Florida Science Olympiad. Through participation in the RET Site the teachers will have an enhanced knowledge base in engineering and computer science and the skills to translate this into their classroom practices, thus impacting their students and motivating them towards science, technology, computing, and engineering disciplines.<br\/><br\/>The intellectual merit of this project revolves around the expertise of the research team and outstanding research environment in which the teachers will work. The research projects are compelling and are in areas that are of current interest. <br\/><br\/>The broader impacts of this project include substantial impact on the area schools and dissemination to a broad community. Teachers will incorporate new computing and engineering topics into their classes and develop hands-on ways to impart these topics to secondary students. The teachers will also engage in public outreach to convey the concepts and appreciation of computer science and engineering to the public. Through this project, a long-term relationship between the university and the schools will be forged and cemented. The partners will work together to build a foundation of outstanding computing and engineering education in the region.","title":"Collaborative Research: RET in Engineering and Computer Science Site: Research Experiences for Teachers Focused on Applications of ImagEs and SiGnals In High Schools (AEGIS)","awardID":"1200552","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1359","name":"RES EXP FOR TEACHERS(RET)-SITE"}}],"PIcoPI":["538958"],"PO":["564181"]},"198473":{"abstract":"The Global Lambda Integrated Facility (GLIF) 12th Annual Global LambdaGrid Workshop is being held in Chicago, October 11-12, 2012, collocated with the 8th IEEE International e-Science Conference, the Open Grid Forum, and the Microsoft e-Science Workshop. The GLIF organization's annual workshop provides an international forum for Research & Education network managers and engineers, computer scientists and discipline scientists who are involved in the design, development and use of advanced optical networks to meet, discuss, develop and demonstrate innovative solutions to complex societal problems. These optical networks are the underpinnings of an evolving global high-performance computing and communications ecosystem, and provide the \"big pipes\" that enable scientists to access complex collections of digital \"big data\" in timely fashion so that they can extract knowledge and insight.<br\/><br\/>Global networking is a vital component of global cyberinfrastructure, and it is the GLIF community that focuses on how to optimally design and develop on-demand LambdaGrids of interconnected distributed computing, sensor and instrument resources to best enable data-intensive science, thus representing intellectual merit.<br\/><br\/>GLIF 2012 presents an opportunity to better architect how high-performance networks, or \"big pipes\", can best enable access to \"big data\" in order to provide scientists with timely information for analysis and ultimately knowledge. Nascent work in these areas is already impacting scientific disciplines, from bioinformatics to metagenomics to geosciences, and it is the general public that will ultimately benefit.","title":"GLIF 12th Annual Global LambdaGrid Workshop","awardID":"1249280","effectiveDate":"2012-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"7369","name":"INTERNATIONAL RES NET CONNECT"}}],"PIcoPI":["558517"],"PO":["564246"]},"187242":{"abstract":"Due to the massive proliferation of mobile devices like smart phones and tablets, mobile cloud computing (MCC) has emerged as a new paradigm and extension of cloud computing, with which users can access a plethora of unique features. Unfortunately, since mobile users offload most computation and storage to the cloud and are usually battery-powered, high performance connections to the cloud tend to become the bottleneck, hindering the efficiency of MCC. Thus, throughput and energy efficiency for the connections to the cloud are two critical and challenging design issues in MCC, which should be addressed carefully. In this project, the PI proposes a novel Multi-radio Multi-channel Multi-hop Cellular Network architecture to facilitate MCC, and investigates throughput and energy optimization by focusing on the following three main research thrusts: (1) throughput optimization for communications between cloud users and base stations (U2B) in hybrid mode, (2) path capacity optimization for communications among cloud users (U2U) with cooperation, and (3) energy optimization for U2B\/U2U communications. The results of this project will advance the state of the art in network optimization for MCC and enrich the scientific knowledge of network design. The proposed research is thus crucial to the success of MCC, which can provide great opportunities for job creation, greater productivity, and economic growth. The results of the project will be disseminated through publications and talks. The proposed research will also be integrated with curriculum development, minority student recruitment, training and mentoring, and outreach to K-12 students.","title":"CAREER: Multi-Radio Multi-Channel Multi-Hop Cellular Networks: Throughput and Energy Consumption Optimization","awardID":"1149786","effectiveDate":"2012-08-01","expirationDate":"2017-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["560137"],"PO":["565303"]},"198044":{"abstract":"This proposed task provides the support for a community meeting by the Federal agencies, whose mission is to advance the science and technology of cyber security, with leading experts and researchers in academia, industry, and government at the Gaylord National Hotel and Convention Center, National Harbor, Maryland. This meeting will focus on recent progress in developing the scientific foundation for the design and analysis of trusted systems.","title":"2012 SoS Community Meeting","awardID":"1247384","effectiveDate":"2012-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"L525","name":"National Security Agency"}}],"PIcoPI":["564430",531487],"PO":["565239"]},"200684":{"abstract":"Emerging trends suggest that a significant share of Internet traffic will be due to real-time streaming applications. However, given the unpredictable nature of popularity of any content and the significant swings seen in user demand, provisioning a centralized content distribution system that can guarantee the strict quality of service constraints needed for real-time streaming is a very difficult task. Therefore, this project studies modeling paradigms and systems design for utilizing end-user resources for real-time streaming in a peer-to-peer (P2P) fashion. The rationale is that P2P networks can dynamically adapt to changing network conditions and service requirements more easily than centralized content distribution systems.<br\/><br\/>This research effort takes a holistic approach in which, on one hand, we study fundamental performance limits of peer-to-peer streaming and design distributed algorithms to achieve these limits. On the other hand, we use these algorithms as building blocks to construct practical protocols that are implemented and validated on an emulation testbed. We bring together a rich set of analytical tools drawn from stochastic processes, optimization and control theory to merge them with real-life multimedia issues of video coding, frame dropping and trans-coding as well as practical implementation issues. Besides the technological impact of developing viable video-streaming ideas for an increasingly networked world, our project also informs and educates both students and the broader society with curriculum development, outreach and integration of under-represented groups.","title":"NeTS: Medium: Collaborative Research: Modeling, Design and Emulation of P2P Real-Time Streaming Networks","awardID":"1261429","effectiveDate":"2012-08-16","expirationDate":"2014-05-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["539541"],"PO":["565090"]},"193490":{"abstract":"Mobile computing devices such as cellular phones, tablets, and laptop computers have become permanent fixtures of modern-day life. Moreover, users have become accustomed to consistent advances in technology that continue to improve features and usability. For decades, such technological improvements could largely be attributed to Moore's Law, which predicts consistent doubling of transistor count and reductions in costs. For almost as long, people have repeated the now well-known saying, \"no exponential lasts forever,\" and also predicted the eventual end of Moore's Law. It appears that the end might finally be near, which motivates research that identifies inefficiencies and finds opportunities at the intersection of traditionally disparate research areas to improve performance and energy efficiency of devices that span the computing spectrum from mobile to servers in the cloud. This project builds on existing technologies that control the voltage and frequency of microprocessors, but takes them to higher levels of integration and granularity.<br\/><br\/>There are several key challenges that stand to obstruct continued growth in computing. Limitations in power budgets, whether due to high cooling costs in high-performance computing systems or to fixed energy capacity in mobile devices, require innovations that will allow future systems with larger numbers of cores to dynamically adapt to time-varying needs of modern workloads. Integrated voltage regulators provide one of the most promising approaches to address energy and scalability constraints. Integrated voltage regulators offer advantages of nanosecond-scale voltage transition times, more efficient current delivery to the load at high power levels, and significant benefits in form-factor and system-level power management. This project aims to develop a systematic approach to answer questions surrounding the benefits and overheads associated with embedding integrated voltage regulators in future chips.","title":"SHF: Small: Exploration of energy-optimized computing architectures using integrated voltage regulators","awardID":"1218298","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[518228,518229],"PO":["366560"]},"193270":{"abstract":"Energy consumption is now emerging as a dominant performance measure in computer systems. In recent years, significant progress in improving energy efficiency has been accomplished by a combination of better hardware design and software tools. Yet the design of future energy-efficient computer systems will ultimately require the development of fundamental models and algorithmic tools that can be used to guide practical solutions.<br\/><br\/>This project is to study algorithmic methods for improving energy efficiency of data processing and storage in computer systems. The basic approach is to model the operation of various system components in the language of combinatorial optimization, with the objective function representing energy consumption, and to solve these problems using exact or approximate efficient algorithms. Many of those problems can be formulated in terms of task scheduling, where the objective is to optimize the CPU energy consumption required to complete a collection of tasks, while meeting some performance requirements. Other examples include minimizing energy consumption of memory systems, both the internal and external memories, by optimizing power levels and sophisticated paging or caching strategies. In addition to addressing some specific energy optimization problems, this work is expected to produce new algorithmic techniques, as well as deeper understanding of the adequacy of standard performance enhancement tools, like caching and load balancing, for improving energy efficiency. The study on energy complexity will also shed some light on the relation between computation and energy.<br\/><br\/>Some algorithms developed in the course of this research will be implemented, tested empirically on the FreeBSD-based platform, and made available to practitioners. The educational component includes research projects for graduate and undergraduate students, and developing a course on sustainable computing.","title":"AF: Small: Collaborative Research: Algorithmic Approaches to Energy-Efficient Computing","awardID":"1217314","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[517719],"PO":["565251"]},"202137":{"abstract":"The objective of this project is to develop geometric algorithms with quality and performance guarantees for constructing road networks from geo-referenced trajectory data. This is a new type of geometric reconstruction problem in which the task is to extract the underlying geometric structure sampled by a a set of noisy, movement-constrained trajectories. Different models for trajectories and for road networks will be investigated. Ideas from geometric shape matching, probabilistic modeling, and trajectory clustering will be applied to develop geometric algorithms with quality and performance guarantees that exploit the continuity of the input trajectories. Proof-of-concept implementations will be developed to test and validate the algorithms on real data.<br\/><br\/>Vast amounts of geo-referenced trajectory data are being collected due to the ubiquitous availability of positioning technologies such as the Global Positioning System (GPS). This project will help address the very timely challenge of analyzing this data. It will also provide novel algorithms for construction and maintenance of digital street maps, which are among the most valuable digital data resource in today's society. The results of this project will benefit a wealth of applications ranging from a variety of location-based services on street maps to the analysis of tracking data for hiking trail map generation or for studying social behavior in animals. Students will be tightly integrated into research projects, providing them with collaborative research experience.","title":"AF: Small: Geometric Algorithms for Constructing Road Networks from Trajectories","awardID":"1301911","effectiveDate":"2012-08-02","expirationDate":"2015-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["558984"],"PO":["565157"]},"196373":{"abstract":"The 21st IEEE International Conference on Computer Communications and Networks (ICCCN 2012) is to be held in Munich, Germany during July 30 to August 2, 2012 (http:\/\/icccn.org\/icccn12\/). This award provides partial assistance to approximately 10 United States-based graduate students to attend this important conference.","title":"NSF Travel Grant Support for IEEE ICCCN 2012 Conference","awardID":"1238494","effectiveDate":"2012-08-15","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["550728","564222"],"PO":["565255"]},"208748":{"abstract":"This research focuses on integrating M-ary distributed detection and <br\/>communications, and makes a significant impact on the theory and <br\/>practice of wireless sensor networks (WSNs) and cognitive radio networks <br\/>(CRNs). In particular, the following four problems are tackled.<br\/><br\/>(1) M-ary distributed detection: The PI develops fusion rules and local <br\/>decision rules for M-ary distributed detection systems with different <br\/>wireless channel models, and studies how the erroneous channels <br\/>fundamentally limit the performance of these systems. Furthermore, two <br\/>composite M-ary distributed detection problems that have applications in <br\/>WSNs and CRNs are investigated: joint detection and localization of <br\/>multiple acoustic sources and spectrum sensing in a multi-channel <br\/>system. (2) Sequential methods for wireless distributed detection <br\/>systems: Striving to gain a profound understanding of the tradeoffs <br\/>between static and sequential distributed detection in wireless <br\/>networks, the PI designs two sequential distributed detection systems <br\/>with i) sequential tests only at the sensors and ii) sequential test <br\/>only at the fusion center, and studies how the detection reliability and <br\/>delay of static and sequential detection systems are affected by the <br\/>adopted wireless channel model. (3) Impact of imperfect channel state <br\/>information on the performance of wireless distributed detection <br\/>systems: The PI studies the effects of channel estimation errors on the <br\/>designs of static and sequential distributed detection systems and <br\/>investigates how these errors further limit the system detection <br\/>performance. (4) Cooperation in wireless distributed detection systems: <br\/>For both static and sequential distributed detection systems cooperative <br\/>schemes are developed, based on local information passing among <br\/>neighboring nodes, and conditions under which cooperation provides gain, <br\/>in terms of both detection reliability and delay, are explored and the <br\/>relationships between the cooperative gain and different channel models <br\/>are sought.","title":"CAREER: M-ary Distributed Detection in Wireless Sensor Networks","awardID":"1341966","effectiveDate":"2012-08-31","expirationDate":"2016-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":[559577],"PO":["564898"]},"189344":{"abstract":"This collaborative research project (IIS-1160894, W. Bruce Croft, University of Massachusetts Amherst and IIS-1160862, Jamie Callan, Carnegie-Mellon University) addresses the complex issues of ephemeral information that is generated as part of social interactions is different in terms of time scale, quantity, and quality to archival information found on the web. This project investigates the hypothesis that, because of the context provided, searching either ephemeral or archival information is enhanced using the connections between them. It develops new retrieval models and features for ranking functions in a range of search tasks that can exploit an integrated ephemeral\/archival network. Some search tasks are based on previous TREC blog, microblog, and web activities. It also investigates two new tasks, conversation retrieval and aggregated social search. Conversation retrieval targets information units in the form of \"conversations\" or \"events\" instead of simply retrieving social postings or web pages. Aggregated social search ranks information in different granularities, such as sentence, posting, conversation, or thread, based on the underlying query intent. <br\/><br\/>Research that explores the connections between ephemeral and archival information requires a dataset that contains both types of information. A crucial part of this project extends the archival ClueWeb12 dataset with ephemeral microblog, blog, and discussion forum data that links to the web data. This extension is distributed to the research community as the ClueWeb12++ dataset. This project (http:\/\/ciir.cs.umass.edu\/research\/ephemeral\/) is the first to address the full possibilities of search that exploits all the connections and contexts created by bringing together the two \"worlds\" of information. It also develops and distributes a unique new dataset that supports the development of a new generation of tools to access a broad range of information. Students at collaborating institutions, University of Massachusetts Amherst and Carnegie-Mellon University will be involved in educational activities and benefit from research experience.","title":"III: Medium: Collaborative Research: Connecting the Ephemeral and Archival Information Networks","awardID":"1160862","effectiveDate":"2012-08-01","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["541869"],"PO":["563751"]},"198177":{"abstract":"This EArly-concept Grant for Exploratory Research (EAGER) supports an exploratory study to evaluate model components for prediction of human speech recognition in the presence of noise. Such a model has the potential to predict confusions between fine phonetic distinctions in different levels of background noise and at different speaking rates. The study takes advantage of modern physiological results that indicate that the primary auditory cortex performs spectro-temporal filtering; that is, that there are cells that are sensitive to particular spectro-temporal modulations at each auditory frequency. In this project, perceptual experiments in the presence of both stationary and non-stationary additive noise and at different signal-to-noise ratios for a database of CVC syllables recorded at 2 different speaking rates yield confusion statistics. These statistics are then compared to those resulting from an auditory model enhanced by elements incorporating these spectro-temporal filters. <br\/><br\/>Successful results from this study will suggest enhancements to current hearing models and ultimately, after a broader study for which this EAGER is a pilot, advance the understanding of human speech perception. Background noise presents a challenging problem for a variety of speech and hearing devices including hearing aids and automatic speech recognition (ASR) systems. Since normal-hearing human listeners are extremely adept at perceiving speech in noise, this improved understanding of human models could lead to better artificial systems for speech processing. The databases and tools developed for this study will be disseminated to the research community.","title":"EAGER: Collaborative Research: Towards Modeling Human Speech Confusions in Noise","awardID":"1248047","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["550775"],"PO":["565215"]},"193491":{"abstract":"Software-intensive systems are typically designed around a set of architectural decisions. These decisions are often based on well-known architectural tactics, which work together to shape the structure, behavior, properties, processes, and governance of the delivered solution. Unfortunately, architectural quality often degrades over time as ongoing maintenance activities are undertaken to correct faults, improve performance, or to adapt the system in response to changing requirements. This research presents a novel and practical approach for addressing the problem of architectural degradation based on a new concept of tactic Traceability Information Models (tTIMs). tTIMS provide semantically typed, reusable, and extensible traceability links, and are designed to significantly simplify the traceability process, reduce its costs, scale up to support traceability and architectural preservation in large and complex projects, and facilitate the visualization of underlying design rationales to the software maintainer. The proposed techniques will have the potential to impact a wide variety of high-performance and safety critical software systems.<br\/><br\/>The project makes several important contributions. First, it provides a potentially transformative solution for integrating existing techniques from the field of architectural assessment and trace retrieval into the maintenance process. Second it develops new algorithms and techniques for capturing and reusing traceability knowledge across projects in the form of a reusable infrastructure of semantically typed traceability links. Third, it provides new visualization techniques for communicating underlying architectural decisions and their interrelationships to developers and maintainers within the context of common software engineering tasks. Fourth, it advances cutting edge trace-retrieval solutions by utilizing a combination of trace retrieval, machine learning, and structural analysis techniques in order to detect classes related to architectural tactics and then to classify those classes according to specific roles in the tactic. Finally, it utilizes this information to automatically generate architecturally significant traceability links.","title":"SHF: Small: Tactic-Centric Traceability Models for Preserving Architectural Quality","awardID":"1218303","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["550521"],"PO":["564388"]},"193381":{"abstract":"This project develops technologies of dynamic 3D reconstruction with applications to broader areas, such as free-viewpoint video, markerless motion capture, special effects for 3D and conventional films, and augmented reality. While image and video-based 3D reconstruction of static scenes is well-understood and is among the most active research areas in computer vision, the current 3D reconstruction methods are not be able to reconstruct dynamic scenes containing non-rigidly moving people, animals or objects well. Furthermore, these methods are unable to self-assess their output. This research effort casts dense multi-view 3D reconstruction as an estimation problem with explicit uncertainty modeling distinguishing between geometric and correspondence uncertainty which are due to very different causes. Other innovations include the combined use of viewpoint-based and world-based processing with explicit and implicit representations and uncertainty-driven regularization.<br\/><br\/>The outcomes of this project improve 3D reconstruction quality and reduce cost for the above applications which have broader impact on different research areas. Ongoing outreach efforts focus on improving Science, Technology, Engineering and Mathematics (STEM) education in several ways: by teaching high school students during the summer, by mentoring them as interns and by training graduate students working with high school STEM teachers. The project also includes a plan of creating the first publicly available dataset of multiple-view dynamic scenes with ground truth depth.","title":"RI: Small: Uncertainty-driven Dynamic 3D Reconstruction","awardID":"1217797","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["551285"],"PO":["564316"]},"192072":{"abstract":"Millions of people used social media technologies to organize themselves to produce information and artifacts of value, such as Wikipedia, GNU\/Linux, and PatientsLikeMe. However, these communities have significant problems: (1) They work so well to let people maintain existing relationships and find others who are similar to them that they can promote self-segregation and opinion polarization. (2) Knowledge production has been democratized, but at the price of devaluing expert participation. (3) They open people to fresh ideas from far-off places, but few sites incorporate the rich local knowledge of home communities. This project addresses these problems by creating algorithms and user interfaces that nudge individuals and communities into more effective patterns of social connections and interaction. <br\/><br\/>The intellectual merit includes: (1) Identification of principles people use to build their social networks, the range of potential contacts they consider, and the contexts in which they encounter potential contacts. (2) Empirical knowledge of the effectiveness of different social network structures for solving a range of realistic decision-making tasks. (3) Novel social recommendation algorithms that maximize network effectiveness and that recommend interaction opportunities that serve as contexts for communication and connection.<br\/><br\/>Broader impacts: This research will be performed in the context of two domains of societal interest, parenting and bicycling, enabling significant broader impacts. Providing people information that gives them confidence to bicycle more increases their personal health, decreases air pollution, and improves community cohesion. Research shows that parents with larger and more diverse social networks make better parenting decisions, and that parents also seek expert advice for specific issues. Therefore, a system that helps people form effective social networks and that provides access to expert educators offers direct benefits to parents. Better parents also tend to be more involved in their communities; thus, improved community health is a bonus effect. Finally, through the research team's connections with parent educators, the project will explicitly target parents from low-income families, who have the greatest needs for support and information.","title":"SoCS: Collaborative Research: Novel Algorithms and Interaction Mechanisms to Enhance Social Production","awardID":"1210863","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7953","name":"SOCIAL-COMPUTATIONAL SYSTEMS"}}],"PIcoPI":[514760,"518491"],"PO":["565342"]},"189620":{"abstract":"For many decades, Moore's Law has allowed exponential growth in computing capability while simultaneously reducing the power consumed by digital devices. Due to fundamental material properties and engineering challenges, in the future the power and energy efficiency of transistors that are the building blocks of digital devices will not improve significantly. Thus to continue providing performance improvements without increasing power consumption, new techniques to design microprocessors are required. This research project looks at a new approach to build microprocessors to make them more energy efficient. The main idea in this research project is to develop techniques allowing microprocessors to efficiently predict without having to expend power-hungry resources to recover in case the prediction is wrong. The research leverages the mathematical principle of idempotence (doing something multiple times producing the same result) in a novel way. In this project, this principle is applied to microprocessor design to develop a class of processors called Idempotent Processors. The research addresses formal theoretical analysis of the technique, ways to build software compilers, and microprocessor designs spanning CPUs to GPUs to exploit this principle. <br\/><br\/>The core idea of this project is to use the property of idempotence: performing an idempotent operation many times produces the same result. The research builds upon the following insight: applications naturally decompose into a continuous series of idempotent regions; i.e., their execution can be broken down into a set of regions, where each region is idempotent - re-execution has no side-effects. The research develops the idea of Idempotent Processors, whose fundamental abstraction is executing idempotent regions of code. This allows novel modifications to the microprocessor pipeline and allows many forms of speculation without the need to restore any state prior to re-execution. This design approach unifies speculation for performance, reliability, and energy efficient execution under one principled approach. The static analysis research formalizes the notion of idempotence and investigates mechanisms for determining idempotent regions. The compiler implementation for various ISAs (instruction set architectures), CPUs (central processing units), and GPUs (grahics processing units) evaluates the approach quantitatively. <br\/><br\/>The project's end-to-end solutions across multiple synergistic directions have potential for disruptive impact. The project involves collaborative work between UW-Madison and UT-San Antonio and involves under-graduate researchers, exchanges visits between institutions, and explores integrated curriculum enhancement and outreach across UW and UTSA. The project's multi-disciplinary and multi-institution collaboration provides distributed impact.","title":"SHF: Medium: Title: Idempotent Processing and Architectures","awardID":"1162215","effectiveDate":"2012-08-01","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["521593","554979","521594"],"PO":["366560"]},"200752":{"abstract":"Correctness and performance are two of the most fundamental concerns in<br\/>software development. In particular, the increasing complexity of modern <br\/>computing environment has made it extremely difficult for software applications to be both correct and efficient. Software programs are frequently found to be flawed, and existing technology has fallen behind in providing the necessary programming language and tool support to ensure high quality software development. This research develops programming language as well as compiler analysis and optimization techniques to support the automated translation of software from high-level design to low-level efficient implementations. <br\/><br\/>This research develops a multilayer code synthesis framework that systematically produces high-quality software by effectively combining software verification techniques with program analysis and compiler optimization in a three-phase translation process. First, starting from the software design phase, the framework automatically translates formal software semantic specifications into object-oriented or procedural implementations based on strategies selected by programmers. Then, based on knowledge from the software-design phase, a sequence of domain-specific optimizations is applied to the implementation to improve algorithm efficiency. Finally, architecture-specific optimizations are applied to performance-critical routines, and the optimized routines are empirically tuned as the application is ported to different machines. Different design and programming languages may be used in each translation phase, and software verification technology will be used to ensure the correctness of each translation. The research focuses on scientific computing and system software applications, where both correctness and performance are of critical concern. The integrated research is expected to significantly improve both the trustworthiness and performance of existing software development.","title":"CAREER: Multilayer Code Synthesis For Correctness and Performance","awardID":"1261811","effectiveDate":"2012-08-13","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7329","name":"COMPILERS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[538267],"PO":["565272"]},"205911":{"abstract":"Despite many energy efficient communication techniques developed in the past few years, energy supply of sensor nodes remains the main challenge in the design of wireless and body sensor networks. Harvesting energy from ambient sources such as light, wind or vibration is the most attractive solution to this problem. Due to the stochastic nature of these ambient sources of energy, the amount of harvested energy can no longer be described by simple and deterministic models, rather sophisticated stochastic models are necessary that accurately describe the variation and correlation of the harvested energy in time and space. Such models are not available today.<br\/><br\/>This project develops detailed stochastic energy models based on long term empirical measurements of three different ambient sources of energy, namely human motion for body sensor networks and solar and wind for wireless sensor networks. For human motion source, the measurements take place at multiple locations on the subject?s body using wearable accelerometers, for the duration of two days. For wind and solar sources the measurements are performed using multiple pyranometers and wind sensors, for the duration of one year. <br\/><br\/>Analogous to the channel and traffic models widely in use today, the energy models resulting from this research will provide a basis upon which many research projects concerning design and performance analysis of harvesting-aware communication techniques can be built. These models will be disseminated through publications as well as code provided freely on the Internet.","title":"EAGER: Stochastic Energy Harvesting Models for Wireless and Body Sensor Networks","awardID":"1321403","effectiveDate":"2012-08-01","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[551277],"PO":["565303"]},"193360":{"abstract":"The proliferation of wireless technology constitutes one of the most remarkable success stories in the history of human intervention. This growth however raises the question of whether technology will capitulate to its own success owing to the scarcity of spectrum. Fortunately the problem is more artificial than fundamental: the scarcity has been precipitated by the exclusive licensing model, while large swaths of licensed spectrum remain underutilized. A conjecture therefore is that unlicensed access of idle (but licensed) spectrum, commonly referred to as secondary spectrum access, can help avert the impending crisis. This however can only be realized if secondary access is profitable for the license holders. This is the main motivation behind this research, which seeks to design incentives to induce license holders to allow secondary access. Specifically, the research studies a free secondary market where the licensed and unlicensed users trade access rights based on bilateral agreements, and seek to optimize their interaction strategies. The resulting decision problems are analyzed using tools from game theory, optimization, graph theory and stochastic control. <br\/><br\/>The profit perspective adopted in this research incentivizes greater participation of wireless carriers in the secondary markets, enabling a quantum leap in more efficient spectrum utilization. Students at both undergraduate and graduate levels are trained in subjects from wireless technology to game theory and optimization theory. This research will generate algorithms for computing the decision strategies of the license holders. They will be evaluated through rigorous analysis and extensive simulations. Results will be disseminated through conferences and journals.","title":"NeTS: Small: Collaborative Research: Playing the Devil's Advocate: The Profit Perspective in Secondary Spectrum Markets","awardID":"1217689","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[517933],"PO":["557315"]},"200257":{"abstract":"With the growth of the Web and improvements in data collection technology in Science, datasets have been rapidly increasing in size and complexity, necessitating comparable scaling of machine learning algorithms. However, designing and implementing efficient parallel machine learning algorithms is challenging and time consuming. To address this challenge, we recently released GraphLab, a framework providing an expressive and efficient high-level abstraction satisfying the needs of a broad range of machine learning algorithms. The performance of our system has attracted significant attention, receiving thousands of downloads from many universities and companies.<br\/><br\/>Currently, GraphLab only addresses batch processing in multicore settings. In this project, we are developing GraphLab 2: addressing the much more challenging online and distributed settings, tackling: 1) Cloud-based distributed machine learning. 2) Natural graphs, with very high-degree vertices that are not amenable to graph partitioning methods. 3) Online tasks, where data and queries are streaming over time. 4) Off-core computation, since huge problems may not fit into memory, even across the cloud.<br\/><br\/>One of the key contributions of the project is the continual dissemination and transfer of our technology. Our open-source software releases will continue to enable large-scale machine learning applications in science and engineering.<br\/><br\/>Our ambitious broader impact goals, beyond theory and systems, include the development of a new curriculum focused on preparing students for the industrial and scientific needs in this field. Our proposed courses include \"Machine Learning on the Web\" and \"Cloud Computing for Big Machine Learning and Data Mining.\"","title":"RI: Small: GraphLab 2: An Abstraction and System for Large-Scale Parallel Machine Learning on Natural Graphs","awardID":"1258741","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"6892","name":"CI REUSE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["549916"],"PO":["564318"]},"196781":{"abstract":"This workshop activity supports US participation and leadership in an international collaborative event around global federation development in support of collaboration in the research community. The workshop centers on Virtual Organizations (VO?s) in research and education, their needs in identity management (IdM), sharing of best practices and current deployments of federated identity systems, how to leverage global networking infrastructure in support of these federated systems, how to leverage recent cybersecurity advances, and issues in integrating some of these concepts and approaches in federated identity with distributed scientific applications and collaboration software. The event is to be held in the Netherlands in September, 2012.","title":"Virtual Organizations and the New Identity Cyberinrastructure: A Workshop for VO Architects in Middleware Planning (VAMP)","awardID":"1240657","effectiveDate":"2012-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"7369","name":"INTERNATIONAL RES NET CONNECT"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1109","name":"Division of RESEARCH ON LEARNING","abbr":"DRL"},"pgm":{"id":"7477","name":"CI-TEAM"}}],"PIcoPI":[527882],"PO":["564246"]},"193162":{"abstract":"Natural language processing tasks like question answering or machine translation require sophisticated parsers: systems that extract grammatical dependency relations between words. But traditional supervised methods of training parsers rely on very expensive hand-labeled datasets, and generalize poorly to new words, grammar, languages, or genres of text. This project is pursuing three directions to significantly augment current unsupervised models of grammar induction. First is a new mathematical model of dependency parsing that draws on linguistic intuitions of constituency. Second is an architecture that jointly learns grammar and parts-of-speech, eliminating the need for supervised part-of-speech tags and hand-labeled datasets, and making grammar induction possible on a vast number of languages and genres. Third are ways to exploit new sources of data for unsupervised learning, including anchor text in web data, vastly expanding the scope of the problem from the small clean annotated treebanks commonly used in current work.<br\/><br\/>Language understanding by machine is a crucial tool for our nation: machine translation makes international web sites broadly accessible, sentiment analysis helps newspapers make politics more transparent, question answering systems help people disseminate knowledge, and information extraction helps corporations and people draw insights from vast databases of documents. By improving the fundammental parsing technology that underlies each of these tasks, and making it possible to parse new languages and genres that have not been parsable before, this project has the power to vastly increase both the power and scope of these key applications.","title":"RI: Small: Learning Meaning and Grammar from Interaction, Context, and the World","awardID":"1216875","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[517444],"PO":["565215"]},"197200":{"abstract":"In current designs of computer chips, electrical wires are used for communication between the different components of the chip. As technology scales down into the nanometer domain, the signal delay and power consumption caused by electrical wires start to dominate the overall delay and power consumption on the chip, mainly because wires do not scale as well as other logic components. A promising alternative to using electrical wires is to use optical waveguides for communication. Using nanophotonics for on-chip communication may lead to faster signal propagation, increased bandwidth density and reduced power consumption. However, many fundamental challenges face the integration of optical devices into commercial chips. This project addresses some of these challenges and its success will have a significant impact on the semiconductor industry.<br\/><br\/>The two major challenges addressed in this project are process variations and thermal sensitivity of optical devices. The former refers to the drifts in resonance wavelengths of optical devices due to fabrication errors during the manufacturing process. The latter refers to similar drifts that result during operation due to temperature fluctuations within the chip. Both drifts are inevitable with current technology and cause the optical network to lose significant bandwidth. Instead of relying on device level innovations, the proposed research takes an architectural approach to endure and tolerate drifts in wavelength resonance. Specifically, it investigates different techniques to maximize the effective bandwidth at run-time in the presence of defects and changes in operating temperatures. These techniques treat bandwidth as a resource that is allocated, on-demand, to different nodes in a way that masks the resonance shifts of optical devices. Since the aggregated available on-chip bandwidth is usually larger than the instantaneous demand for bandwidth, the effect of the imperfect hardware is mitigated by appropriately assigning wavelengths to nodes, thus offering a reliable and near perfect optical communication layer to the other components of the system.","title":"EAGER: Tackling the Variations and Instability of Nanophotonic Interconnection Network via Architecture Techniques","awardID":"1242657","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["533851","508314"],"PO":["366560"]},"187223":{"abstract":"Understanding how genetic material determines the observable characteristics (phenotypes) of an organism relies on knowledge of phenotype-gene relations. From high-throughput genomic data, it is now possible to apply computational approaches to identify the associations between individual phenotypes and genes. Since the number of determined phenotype-gene associations is still very limited, no computational framework has been developed to perform large-scale cross-species analysis of the association between the whole collection of phenotypes (phenome) and genes (genome). The objective of this CAREER proposal is to develop new computational methods for predicting and understanding phenome-genome association across multiple species. With the prediction tools, a biologist or disease researcher could more reliably prioritize genes to test their association with phenotypes in the laboratory. The availability of the tools will greatly expedite the process of discovering new associations, especially for studying rare phenotypes. The developed methods will be applied to study phenome-genome associations for the analysis of several cancer tumor phenotypes and the growth phenotypes of Arabidopsis thaliana in collaboration with oncologists and biologists. The study of the plant growth phenotypes aims to identify genes that govern seedling de-etiolation and seed development. The collaboration should generate a potential increase in seed yield and concomitant increases in the contents of proteins and oil per seed for the crop plants. The study of the ovarian cancer and lung cancer tumor phenotypes will help reveal the driving pathways of chemoresistance, and result in useful prediction tools and drug targets for the treatment of ovarian cancer and lung cancer. The research in this proposal will deliver a web portal called Phenome-Genome Explorer with a collection of computational tools that utilize known phenotype-gene associations to predict new associations, find conserved associations and conserved modules of associations across species. The PI has a long-term commitment to teach a summer class in the BioSMART program for Minnesota high school students. He will also create a new course Computational Phenomics and Genomics to support two graduate programs for training students in biomedical\/health informatics with knowledge in genomics and computer science. The education plan in the proposal aims to promote high school students' early interest in careers in computing science and biomedical\/health informatics and integrate the research development on phenome-genome analysis into training graduate students to meet the need of workforce in the growing biomedical and health informatics industry, with a focus on recruiting students in minority and under-represented groups.<br\/><br\/>This proposal targets a systematic computational study of phenome-genome association in a network context. The comparative analysis across multiple species will expand the current scope of understanding evolutionary relation between phenome and genome. The proposed research work focuses on 1) How to discover patterns and predict new associations by learning with the sparse connections in a large heterogeneous network composed of phenotype network, gene network and their association network; and 2) How to compare multiple heterogeneous networks to find conserved patterns and modules, and to infer new associations. Both scenarios require development of scalable new algorithms to deal with multiple large heterogeneous networks.","title":"CAREER: Predicting and Mining Phenome-genome Association across Species","awardID":"1149697","effectiveDate":"2012-08-01","expirationDate":"2017-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0807","name":"Division of MOLECULAR AND CELLULAR BIOSCIE","abbr":"MCB"},"pgm":{"id":"8011","name":"Networks and Regulation"}}],"PIcoPI":[502199],"PO":["565136"]},"193680":{"abstract":"The field of machine learning is extremely successful in solving classification problems where the inputs are fixed size feature vectors and the outputs are a small fixed number of classes. However, many applications such as natural language understanding and visual scene interpretation involve inputs and outputs of variable size that have rich internal structure. This project will study new approaches for such structured prediction problems. For example, the inputs may be natural language documents or visual scenes and the outputs may be formal representations of their semantic content, such as entities inferred or observed and the relationships between them. Most approaches to structured prediction learn a cost function to score potential structured outputs. Finding the correct output for the given structured input then consists of inferring the least cost output. Unfortunately, the computational cost of this inference is prohibitive for expressive cost functions; this forces the use of either simpler cost functions or approximate inference. In either case, prediction accuracy can suffer.<br\/><br\/>The current project aims to address this issue by integrating learning and search in a new framework that allows for the development of novel algorithms for structured prediction. In particular, this project will address three topics: (1) A generic framework will be developed for cost function learning by imitating the decisions of an optimal time-bounded search procedure on the training data. This will allow for a wide range of state-of-the-art search algorithms to be leveraged for structured prediction. (2) A theory and framework will be developed to learn to speed up the search for a global optimum by compressing the search into a shorter time-frame. This will allow for learning to address not only accuracy but also the computational efficiency of the predictor. (3) Both the cost function learning and speedup learning will be instantiated in multiple search algorithms and evaluated in different applications.<br\/><br\/>The project seeks to make contributions to a variety of applications of broad impact including natural language understanding, tracking objects in video, and personalized scheduling. The frameworks, algorithms and testbeds for learning to search and structured prediction will be integrated into the Weka tool-box so that they can be easily combined with different supervised learning algorithms and used in further research. The results and benchmark domains will be publicly distributed through the project's web pages. A special topics graduate course will be taught on the topic of this proposal at Oregon State University.","title":"RI: Small: Integrating Learning and Search for Structured Prediction","awardID":"1219258","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[518680,"551065"],"PO":["565035"]},"193570":{"abstract":"The exponential growth of mobile data is a major challenge to the operators of cellular networks. Looking beyond conventional capacity-improving approaches such as adding more cells and acquiring more spectrum, this project seeks to fundamentally improve the spectral efficiency of cellular data networks. The project investigates LAWN (Large number of Antenna based Wireless Networking), a radically new cellular network architecture, in which a large number of antennas simultaneously serve a relatively much smaller number of wireless terminals using multiuser beamforming. The project has two inter-related thrusts. The first investigates a novel LAWN base station design and prototype that can cost-effectively scale to hundreds of antennas and exploit physical-layer tradeoffs between computational complexity and network capacity. The second thrust studies the resulting new network architecture that efficiently schedules terminals, intelligently allocates transmission power, and coordinates pilot signal transmissions to mitigate inter-cell interference.<br\/><br\/>The project targets improving the spectral and power efficiency of cellular networks by many fold, leading to not only fast wireless data networks but also longer battery lifetime of mobile terminals. Results from the project are likely to provide fresh insights for new theoretical development, bringing large-scale multi-user beamforming one significant leap closer to practical deployment in cellular data networks. In addition to academic publications, the project will produce an open platform, including hardware, software, and documentation available on-line, for teaching and researching base station design. It will actively involve undergraduate students as well as students from under-represented populations.","title":"NeTS: Small: Collaborative Research: LAWN: Scaling Up Cellular Data Networks using a Large Number of Antennas","awardID":"1218700","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["548312"],"PO":["565303"]},"193350":{"abstract":"Battery power is a scarce resource on phones and is one of the main factors that limit the deployment of rich new kinds of mobile applications. There is presently no comprehensive solution to this power management problem, and existing solutions are narrowly focused on specific applications or specific device components. In this research, we will develop a system called PowerMedic that makes device-wide energy decisions across applications. The approach is to implement power management as a fundamental primitive. Much like other network primitives such as send and receive, applications need only specify their requirements; they no longer need to perform low-level energy optimizations. PowerMedic takes into account the interactions between device components such as network, CPU, screen, and sensors, to optimize power consumption, while satisfying the application requirements. The runtime decisions to save power are dynamic and driven by the user's application patterns, mobility, network connectivity, and other environmental factors. For evaluation purposes, it is expected that a user study will be conducted to gather mobile usage data. The data can be used to build predictive models of user behavior, such as charging patterns, to further improve the system.<br\/><br\/>This research is relevant to a broad segment of the population, as it is projected that there will be 1 billion smartphones worldwide by 2013. PowerMedic will enable a wide range of critical applications on phones, ranging from healthcare applications to providing assistive technology for the disabled. Today, these applications have limited success on phones because of the power bottleneck. This research will also benefit the research community through the release of the PowerMedic software and the user study data.","title":"NeTS: Small: Mobile Power Management as a Network Primitive","awardID":"1217644","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["543394",517912],"PO":["565303"]},"193240":{"abstract":"Jamming attack is one of the most commonly used techniques for limiting the effectiveness of an opponent's communication in wireless networks. Along with the advent of user configurable intelligent devices, such as cognitive radios, jamming is no longer limited to military related events, but has become an urgent and serious threat to civilian communications as well. Motivated by this observation, this project is devoted to the development of multi-layer anti-jamming techniques for secure communications over wireless networks. First, we introduce a systematic 2D model for cognitive jamming characterization, detection and classification. We also introduce the concept of disguised jamming, and point out that it can be much more harmful than the traditional strong jamming. Second, we investigate efficient anti-jamming system design for both point-to-point (one-hop) communications and multi-hop communications. For one-hop communications, we enhance jamming resistance by incorporating advanced cryptographic techniques into network-centric spectrum access control and the physical layer transceiver design. For multi-hop communications, we develop efficient and unconditionally secure cryptographic algorithms and protocols to maximize routing diversity and routing anonymity. <br\/><br\/>The proposed highly efficient anti-jamming techniques lay a solid foundation for real-time jamming detection and reliable information transmission under dynamic jamming scenarios. It can greatly improve the efficiency and reliability of high speed wireless services. This project also includes a significant education component aimed at integrating frontier research with undergraduate and graduate curricula. Research results will be presented at international conferences, meetings and published in scientific journals.","title":"NeTS: Small: Anti-Jamming Techniques for Secure Communications in Wireless Networks","awardID":"1217206","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["523634","523780"],"PO":["557315"]},"193361":{"abstract":"Software engineers need improved tools and methods for translating complex legal regulations into workable information technology systems. Compliance with legal requirements is an essential element in trustworthy systems. The research proposed herein will advance the cutting edge for creating more accurate, efficient, and reliable RCSE (Regulatory Compliance Software Engineering), resulting in compliant software systems. System specifications typically concentrate on system-level entities, whereas legal discussions emphasize fundamental rights and obligations discursively. This work bridges three cultures of scholarship and research: software specification, law, and access control. By empowering software developers and policy makers to better understand regulatory texts and the access controls specified within these texts, current and future software systems will be better aligned with the law.<br\/><br\/>There are three main expected results of this work: (1) Framework, methodology and heuristics to identify UCONLEGAL components in legal texts; (2) extended TLA (Temporal Logic of Actions) rules from UCONABC and mapping of predicates, actions, states, variables and obligations between UCONLEGAL and UCONABC; (3) validated and extended role-based access controls to meet healthcare and financial legal requirements through further development of UCONLEGAL. The impacts of this work are expected to be far reaching; law and regulations govern the collection, use, transfer and removal of information from software systems in many sectors of society, and this research explicitly calls for models and theories for analyzing and reasoning about security and privacy in a regulatory and legal context.","title":"SHF: Small: Towards Regulatory Compliance Software Engineering with UCON_LEGAL","awardID":"1217699","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[517935,517936],"PO":["564388"]},"193482":{"abstract":"Searching for relevant code is a common task among programmers, one that is becoming more crucial to their productivity as open code repositories grow in size and programmers have the opportunity to reuse the resources therein. Yet, the code search tools for programmers to leverage such massive repositories have barely evolved in the last decade. Programmers still search by specifying keywords, an approach that may perform very inconsistently as it depends on the programmer's ability to provide terms that match how the programs in the repository are implemented or documented. This is problematic because a slightly wrong keyword could miss relevant results and return mostly spurious results, reducing programmer's productivity.<br\/><br\/>This research aims to address these limitations by exploring a code search that is novel in two aspects. First, it allows programmers to search for code by specifying desired code behavior in the form of inputs and outputs. This removes the need for the programmer to imagine how the solution to a problem may have been implemented, and instead the programmer can concentrate on defining what the software should do. Second, given the inputs and outputs describing the desired behavior and the set of programs in a code repository, both automatically encoded as constraints, the proposed approach employs a constraints solver to identify what programs in the repository could satisfy the programmers' specifications, effectively solving the search. This guarantees the search results behave as specified. Developing this approach requires tackling several fundamental challenges including the definition of mappings to automatically and efficiently encode programs as constraints so that solvers can find suitable solutions. Systematic strategies must also be developed to refine the programmers' specifications when they are too weak and return too many matches, and to relax the constraints in order to find partial matches when exact ones are not available. Techniques to support the composition of partial matches will also be necessary to scale the approach to larger and more diverse search queries. Last, infrastructure must be built and studies conducted to determine whether the proposed approach can be cost-effective in practice. Use of these techniques will change the way programmers operate, directly impacting their productivity by enabling them to truly leverage existing code in increasingly rich code repositories.","title":"SHF: Small: Solving the Search for Relevant Code in Large Repositories with Lightweight Specifications","awardID":"1218265","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[518212],"PO":["564388"]},"193273":{"abstract":"The investigator studies algorithms for problems that combine graph theory with geometry.<br\/><br\/>One component of the proposal concerns sets of circles that do not overlap, but meet at points of tangency like coins pushed together on the surface of a table. The investigator will seek out efficient algorithms for constructing circle packings and use them to find \"Lombardi drawings\", a form of information visualization in which graph edges are represented as circular arcs. These drawings resemble soap bubbles and the PI plans to use them to characterize the patterns of adjacency that soap bubbles can have.<br\/><br\/>A second component of the proposal concerns the graphs formed from geometric objects by linking nearby pairs of objects by edges; these graphs have many applications in data clustering, collision detection in physical simulation, and architectural design. The investigator will develop efficient data structures for maintaining mutual nearest neighbors of dynamic points, and study nearest neighbors in tree-like non-Euclidean metric spaces for which previously known methods are inapplicable.<br\/><br\/>In a third component of the award, the investigator will study efficient algorithms for constructing cartograms, visualizations of geographic data in which geographic regions are shown with stylized shapes and distorted areas that represent numeric information such as population. In addition he will study compact three-dimensional visualizations of graphs as the sets of vertices and edges of three-dimensional polyhedra with axis-parallel edges and to study Barnette's conjecture on Hamiltonian cycles for the graphs of axis-parallel polyhedra.<br\/><br\/>As well as the applications of these problems in information visualization, scientific computing, and engineering, the broader impacts of this proposal include open-source implementations of algorithms and the public dissemination of knowledge via the creation of relevant Wikipedia articles.","title":"AF:Small:Geometric graph algorithms","awardID":"1217322","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[517727],"PO":["565157"]},"193042":{"abstract":"Demands for wireless access to the Internet and voice communications<br\/>keep growing exponentially, while the available spectrum remains scarce.<br\/>As a result, cellular, WiFi, mesh, and cognitive networks are<br\/>increasingly interference-limited.<br\/><br\/>Despite significant efforts over the last decade, key aspects of the<br\/>interference are still not well understood. In particular, the spatial<br\/>and temporal correlation of the interference has been largely ignored,<br\/>despite its profound impact on the performance. With the proper<br\/>mathematical and numerical tools from stochastic geometry and spatial<br\/>statistics, the impact of protocol decisions on the interference as a<br\/>random field in space and time can be assessed, and, even more<br\/>importantly, the question of how to engineer the interference for<br\/>optimum performance can be addressed.<br\/><br\/>This project aims at taking a major step in this direction. It focuses<br\/>on developing a fundamental understanding of the structure of the<br\/>interference using a rigorous analytical approach. While the outcomes of<br\/>the project will be applicable to and relevant for most modern wireless<br\/>systems, they are particularly pertinent for cognitive systems, where<br\/>interference between primary and secondary users is not just a technical<br\/>problem leading to a performance reduction, but also a regulatory and<br\/>legal issue.","title":"CIF: Small:Interference Engineering in Wireless Systems","awardID":"1216407","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["540624"],"PO":["564924"]},"198510":{"abstract":"In robotics activities, students are theorized to benefit from \"learning-by-doing\" activities where they set their own goals, but in practice, these activities have failed to produce the expected effects on STEM outcomes. To improve learning from robotics, this project will leverage teachable agent technologies, where students learn about a domain by teaching a computer agent. The agent interacts with students to expose misconceptions and encourage them to persist in the face of failure. By integrating the structure of teachable agents with the exploratory and engaging features of learning from robotics, the project will enhance the benefits of both approaches. The investigators will implement a robot that students can teach about the concepts they are learning in their middle school mathematics class. They will engage in design exercises with teachers and students to identify features of the robot that might be particularly important for improving student motivation and learning. Finally, they will conduct two studies, one in the laboratory and one in the classroom, to explore how students react to the robot in a realistic setting. <br\/><br\/>Intellectual merit: The project will improve understanding of what features students respond to in a teachable robot for mathematics, and of the potential educational benefits such a robot might have. The project will make a technological contribution by inventing methods for integrating teachable agents and robotic learning environments.<br\/><br\/>Broader impacts: The project will impact middle school students from underrepresented groups who will help design and test the teachable robot, exposing them to new technologies and engaging aspects of STEM careers. The project will also lead to a better understanding of the impact of teachable robots in the classroom and pave the way for these technologies to be adopted more widely in education.","title":"EAGER: A Teachable Robot for Mathematics Learning in Middle School Classrooms","awardID":"1249406","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["552834","552861"],"PO":["565227"]},"197311":{"abstract":"The US is growing older because of millions of baby boomers who already started turning 65. Since susceptibility to diseases increases with age, studying molecular causes of aging gains importance. Human lifespan is long, which, in addition to ethical constraints, makes studying human aging difficult. Therefore, aging is studied in simpler ?model? species, e.g., baker?s yeast. Then, the knowledge about aging is transferred from model species to human. Thus far, this transfer has been restricted to genomic sequence comparison, by identifying regions of similarity between sequences of genes in different species (which are believed to be a consequence of functional relationships between the sequences), and by transferring the knowledge from a gene in model species to a sequence-similar gene in human. However, genes (that is, their protein products) carry out biological function by interacting in complex networked ways with one another, instead of acting alone. Hence, it has been argued in the post-genomic era that the wirings among genes in cellular networks could give biological insights over and above sequences of individual genes. Thus, this project hypothesizes that, analogous to genomic sequence research, biological network research will impact our understanding of aging. For example, since not all genes implicated in aging in model species have sequence-similar genes in human, restricting comparison to sequence may limit the transfer of aging-related knowledge to human. Network comparison can help, as it can find regions of similarities between networks of different species and allow for a transfer of the knowledge between such regions.<br\/><br\/>Intellectual merit: Unlike genomic sequence research, biological network research is in its infancy, for the following reasons. Many network problems (including network comparison) are computationally intractable, and hence, efficient approximate (or heuristic) solutions are needed. The function of many genes remains unknown, and hence, it must be discovered from other, better-characterized genes. Even though cells evolve over time, current methods for analyzing systems-level biological networks deal only with their static representations, because dynamic biological network data can not be obtained easily with current biotechnologies, and because there is a lack of efficient methods for dynamic network analysis. Current biological networks are noisy, with many missing and spurious links, due to limitations of biotechnologies as well as human biases during data collection; thus, methods for network de-noising need to be developed. Hence, this project aims to use sensitive measures of network structure (or topology) to develop new heuristic computational methods for efficient network analysis, which can cope with the complexity of functionally uncharacterized, dynamic, and noisy biological networks. Also, it aims to help in understanding the processes of human aging by enabling exploitation of biological network data. Specifically, the new methods will be used to: transfer the knowledge about aging from model species to human to complement the knowledge obtained from sequence; study dynamic human biological networks (obtained computationally by combining current static networks with age-specific gene expression data) to learn about how cells change with age; and de-noise current networks to produce higher-confidence results.<br\/><br\/>Broader impacts: Understanding aging is of societal importance. Since network research spans many domains, the proposed methods will be implemented into open-source research software, which will also serve as an educational tool. Integration of research and education will be promoted further by training interdisciplinary scientists through novel courses on network research. Research supervision will be offered to K-12, undergraduate, and graduate students, focusing on minorities and women. Interdisciplinary collaborations will be encouraged to allow for wide distribution of the proposed ideas and results.","title":"What Can Networks Tell Us About Aging?","awardID":"1243295","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7931","name":"COMPUTATIONAL BIOLOGY"}}],"PIcoPI":["550423"],"PO":["565223"]},"198455":{"abstract":"The fundamental research question posed by this project is how to provide an integrated, flexible, and scalable storage repository for early design sketches. Early design is characterized by informal, unstructured, and heavily collaborative work processes involving multiple participants, multiple forms of representational media, and a plethora of interactions between these. However, there currently exist no storage mechanisms capable of supporting the full scope of sketching for creative design. In practice, designers either resort to low-tech physical artifacts such as pen, paper, and whiteboards that lack the persistence of digital storage, or general purpose software such as e-mail or shared file systems where the storage mechanisms are not well-integrated with the creative process and also lack provenance, history, and versioning information. <br\/><br\/>Intellectual Merit: The contribution of this project is to apply the concept of a Wiki to sketches. Wikis are collections of freely editable web documents and have become popular due to their capacity for drawing upon the crowd to create content. Wikis have also been shown to be particularly useful in early design, but existing Wiki software makes integrating visual representations burdensome. This research effort will be focused on designing, building, and evaluating a sketch-based Wiki (skWiki) that allows for combining vectorized sketches with text. <br\/><br\/>Broader Impacts: Unlike prototyping, manufacturing, or even engineering, innovation cannot easily be outsourced and is now recognized as the single most important ingredient in our economy. Consequently, there has never been a clearer imperative to both create designs which will fundamentally change the competitive landscape, and also to improve the innovative capacity of designers, particularly those in engineering. To facilitate such synergistic effects, the skWiki framework designed in this project will be made available as Open Source. We will also use the tool for teaching engineering design.","title":"EAGER: skWiki - A Sketch-based Wiki","awardID":"1249229","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["553653","559568"],"PO":["564456"]},"198587":{"abstract":"This is funding for a two-day workshop entitled \"Articulating the Computing Research Agenda in Social Computing Research\" that will be held in the fall of 2012. Social Computing is an important new area of research that has given rise to a number of new journals, conferences and funding opportunities. A wide variety of topics and disciplinary approaches have been grouped under the label of \"social computing,\" some of which have strong computer and information science components but others of which contribute to other academic disciplines. The goal of this workshop is to bring together experts in social computing to develop a framework for conceptualizing social computing as an area of computer and information sciences. <br\/><br\/>Specifically, the workshop will (a) consider what constitutes a social computing contribution in the information and computational sciences; (b) consider intellectual relationships between types of social computing contributions; (c) identify standards necessary for advancing social computing research; and (d) identify infrastructures that would help develop and support social computing research as part of a computing disciplines. Outcomes will include a published overview of the current state of social computing research, a set of educational goals for developing a future community of social computing scholars, the identification of publishing and other venues that can stimulate the exchange of ideas among the social computing researchers, and the identification of infrastructure needs.<br\/><br\/>The workshop will be organized as a series of discussions around six themes in social computing research within the computer and information sciences: (a) AI, machine learning and data mining; (b) social networking and network science; (c) quantitative methods; (d) qualitative methods; (e) systems and experimentation; and (f) design and broader impacts. One or more experts will lead each thematic discussion, during which the workshop attendees will consider issues such as social computing successes in that thematic area, critical research questions, and infrastructural needs.<br\/><br\/>Participants in the workshop will include invited experts in the thematic topic areas and researchers selected on the basis of position papers submitted in response to the call for participation. In addition, a number of slots will be reserved for advanced graduate students. <br\/><br\/>Intellectual Merit: The workshop will advance computer science and engineering by providing a framework for understanding current social computing research and for identifying future key research questions. It will also advance social computing research by identifying methods for community building and exchange of research findings.<br\/><br\/>Broader Impacts: Social computing is central to many areas critical to society, including emergency response, collective action, crowd-sourcing, sustainability, and health applications. The outputs of this workshop will help researchers identify computer and information science contributions that will advance these and related areas of public concern. The workshop will also include outreach efforts to engage underrepresented groups in STEM research.","title":"HCC: Workshop: Articulating the Computing Research Agenda in Social Computing Research","awardID":"1249835","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7953","name":"SOCIAL-COMPUTATIONAL SYSTEMS"}}],"PIcoPI":["543781"],"PO":["565342"]},"189765":{"abstract":"This award provides funding for a new collaborative Research Experiences for Teachers (RET) Site focused on Signal and Image Processing at the University of Central Florida and Florida Institute of Technology. Each year 13 high school teachers from school districts in four Central Florida counties will participate in research projects at the universities. The teachers will also develop modules related to their research which they will implement in their classes in the following school year. The teachers will organize annual science events to share their work with the communities around their schools and they will participate in the Florida Science Olympiad. Through participation in the RET Site the teachers will have an enhanced knowledge base in engineering and computer science and the skills to translate this into their classroom practices, thus impacting their students and motivating them towards science, technology, computing, and engineering disciplines.<br\/><br\/>The intellectual merit of this project revolves around the expertise of the research team and outstanding research environment in which the teachers will work. The research projects are compelling and are in areas that are of current interest. <br\/><br\/>The broader impacts of this project include substantial impact on the area schools and dissemination to a broad community. Teachers will incorporate new computing and engineering topics into their classes and develop hands-on ways to impart these topics to secondary students. The teachers will also engage in public outreach to convey the concepts and appreciation of computer science and engineering to the public. Through this project, a long-term relationship between the university and the schools will be forged and cemented. The partners will work together to build a foundation of outstanding computing and engineering education in the region.","title":"Collaborative Research: RET in Engineering and Computer Science Site: Research Experiences for Teachers Focused on Applications of ImagEs and SiGnals In High Schools (AEGIS)","awardID":"1200566","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1359","name":"RES EXP FOR TEACHERS(RET)-SITE"}}],"PIcoPI":["511959",508775],"PO":["564181"]},"198488":{"abstract":"This is funding to support participation by about 12 graduate students (10 from U.S. institutions and 2 from non-U.S. institutions) and 5-6 senior members of the ICMI community (faculty and industry researchers) in a Doctoral Consortium (workshop) to be held in conjunction with and immediately preceding the 14th International Conference on Multimodal Interaction (ICMI 2012), which will take place October 23-26, 2012, in Santa Monica, California, and which is organized by the Association for Computing Machinery (ACM). The ICMI conference series is the premier international forum for multidisciplinary research on multimodal human-human and human-computer interaction, interfaces, and system development. The conference focuses on theoretical and empirical foundations, component technologies, and combined multimodal processing techniques that define the field of multimodal interaction analysis, interface design, and system development. Topics of special interest to the conference this year include multimodal interaction processing, interactive systems and applications, modeling human communication patterns, and data, evaluation and standards for multimodal interactive systems. ICMI 2012 will feature a single-track main conference which includes: keynote speakers, technical full and short papers (including oral and poster presentations), special sessions, demonstrations, exhibits and doctoral spotlight papers. The ICMI 2012 proceedings will be published by ACM as part of their series of International Conference Proceedings. More information about the conference may be found at http:\/\/www.acm.org\/icmi\/2012. <br\/><br\/>The goal of the ICMI Doctoral Consortium is to provide PhD students with an opportunity to present their work to a group of mentors and peers from a diverse set of academic and industrial institutions, to receive feedback on their doctoral research plan and progress, and to build a cohort of young researchers interested in designing multimodal interfaces. Student participants will present their ongoing thesis research as a short talk at the Consortium and also as a poster at the conference Doctoral Spotlight Session. This year, the organizers seek to expand the scope of the Doctoral Consortium to provide more opportunities for interaction between the students and senior members of the field; to this end, the program will also include a lunch on the day of the workshop for students and mentors, a career panel that will provide the students and mentors the opportunity to ask and answer questions and discuss challenges and opportunities in the field, and a dinner that will provide the students with the opportunity to hold informal conversations among themselves as well as with the organizers and mentors.<br\/><br\/>Broader Impacts: The Doctoral Consortium will give student participants exposure to their new research community, both by presenting their own work and by observing and interacting with established professionals in the field. It will encourage students at this critical time in their careers to begin building a social support network of peers and mentors. The organizers will take steps proactively to achieve a diversity of research topics, disciplinary backgrounds, methodological approaches, and home institutions among the students. Priority will be given first to minority students, female students, students from geographically underrepresented states, and finally to students whose advisors or departments have insufficient funds to support their participation in the conference. To further increase diversity up to two student participants may be invited from abroad, and no more than two will be invited from any given U.S. institution of higher learning.","title":"Workshop: Doctoral Consortium at the 14th International Conference on Multimodal Interaction","awardID":"1249319","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["563326"],"PO":["565227"]},"189556":{"abstract":"Mobile devices are increasingly being relied on for a number of services that go beyond simple connectivity and require more complex processing. Such applications have become an indispensable part of everyday life. This has been made possible by two trends. First, truly portable mobile devices, such as smartphones and tablets, are increasingly capable devices with processing and storage capabilities that make significant step improvements with every generation. A second trend is the availability of improved connectivity options for mobile devices which have enabled applications that transcend an individual device's capabilities by making use of remote processing and storage.<br\/><br\/>The primary approach in wide-use today to enable such remote processing makes use of standard cloud computing resources to off-load the 'heavy lifting' that may be required in some mobile applications to specially designated servers or server clusters. Another technique for remote processing of mobile applications proposes the use of cloudlets which provide software instantiated in real-time on nearby computing resources using virtual machine technology. This previous work assumes the availability of infrastructure-based compute resources and additionally requires good, constant connectivity to such resources for the duration of a computational task. This leaves the important and challenging question of handling intermittent connectivity of mobile devices unaddressed. <br\/><br\/>This work will generalize the remote mobile computing paradigm to enable its operation in the presence of intermittent connectivity. This, in turn, provides the important ability to leverage more compute resources including those provided by intermittently-connected cloudlets and even those available in other mobile devices. Our work can be viewed as enabling a truly general vision of cyber foraging which envisions mobile applications 'living off the land' by exploiting nearby computational resources.<br\/><br\/>The research work will be conducted by an interdisciplinary team which includes researchers with expertise in mobile wireless networks, in program partitioning and execution in mobile environments, and in building applications that involve analysis and interpretation of rich multimedia data. The work will include efforts in 1) development of a remote execution framework for a number of scenarios with varying availability of remote compute resources and a diversity of intermittent connectivity features, 2) development of application profiling and partitioning tailored to intermittently-connected environments, 3) experimentation with sequential and embarrassingly parallel applications, and 4) evaluation through proof-of-concept prototypes as well as extensive emulation.<br\/><br\/>Intellectual Merit: This work generalizes the mobile computing paradigm to include full consideration of the increased computational power of mobile devices together with their inherent intermittent connectivity. In doing so, we draw upon and push forward recent advances in program analysis, networking in intermittently connected environments, and mobile multimedia applications. The unified framework developed within this research has conceptual and pragmatic value; it sets the stage for a future in which mobile applications can leverage and dynamically adapt to any and all available resources.<br\/><br\/>Broader Impacts: The work is multi-disciplinary, and the students working on the project will be exposed to problems that lie in the intersection of computer science disciplines such as networking, programming languages, and distributed processing. Our research will also enable applications that run at the edge of the network and without fixed infrastructure, thus empowering citizens where they are, with limited external dependencies.","title":"NeTS: Medium: Mobile Computing over Intermittently Connected Networks","awardID":"1161879","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["550428","550429",508256,"534450"],"PO":["564993"]},"203515":{"abstract":"Given the rate of fossil fuel depletion, alternative energy driven systems are poised to determine our planet's future. However, there is still a lack of computer systems that can intelligently use harvested energy for a wide range of applications. To fill this void, this CAREER research is developing techniques for designing renewable energy systems spanning small devices (powered by indoor light), disaster-relief sensors (solar powered), and large systems such as green homes. This project meets the above goal through a four fold research agenda. First, to better understand the variability in energy harvested this research uses low power side channel measurement infrastructure to profile energy generation and compression techniques to determine appropriate model granularity for harvesters. Second, this project uses multi-tiered software and hardware architectures that can function under variable power inputs. Third, to tame the uncertainty associated with energy harvested, this research is designing a multi-scale self-adaptive optimization framework tuned to the computational capability of the associated device. Finally, this research is designing replayer systems that is used to playback renewable energy traces for rapid development of such systems in the lab.<br\/><br\/>With target applications such as healthcare, emergency relief, and green homes, this CAREER research has profound societal impact. Additionally, the project meets other broader goals, including undergraduate research and education and maintaining student diversity in Arkansas. Through Green Computing courses, this research is instrumental in percolating renewable energy device design into the graduate and undergraduate curricula at the University of Arkansas, and through distance education to the rest of the world. Also through support from industrial partners, the research prototypes are being made publicly accessible.","title":"CAREER: System Support for Renewable Energy-driven Devices","awardID":"1308723","effectiveDate":"2012-08-24","expirationDate":"2016-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["547994"],"PO":["565255"]},"193373":{"abstract":"Troubleshooting undesirable network events, such as poor connectivity or performance, is difficult at best. The high-speed link techniques that work on LANs, such as dumping packets and analyzing the detailed traffic, are impossible due to massive data volume. This project will explore mathematical techniques and network tools that will reduce the amount of data that has to be captured and stored while still allowing network operators to troubleshoot their networks. The project's objective is to extract from high-speed packet streams on individual network links an approximate and highly compressed representation of the link traffic that is orders of magnitude smaller in size than the raw traffic stream but which permits almost the same degree of troubleshooting as the raw data. The project will develop the algorithms and mathematical theory needed to design intelligent sampling algorithms for compressing network traffic. Specific areas to be studied for purposes of developing sampling techniques include identifying what constitutes the representative flows for troubleshooting purposes and investigating how to best encode and decode the sampled data, and how the samples can be gracefully shrunk over time so as to reclaim space for new data as they arrive.<br\/><br\/>Broader Impact:<br\/>The project will provide research experience for undergraduates. Undergraduates at Georgia Tech, Denison and other institutions will be recruited via undergraduate workshops and research symposiums. Additionally, the project will integrate education and research via inclusion of the research into courses. In addition to publishing in appropriate scientific venues, the PIs will expand the Wikipedia entries on topics related to data streaming algorithms as part of the process of disseminating general information about the topic area to the scientific community. In terms of commercial impact, the project will lead to better methods for the diagnosis of large-scale networks, thereby reducing the cost to maintain and operate them. As part of the transfer of research findings into commercial practice the PIs will collaborate with members of AT&T's Network Management and Engineering Department.","title":"NeTS: Small: Collaborative Research: Towards Principled Network Troubleshooting via Efficient Packet Stream Processing","awardID":"1217758","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[517961],"PO":["564993"]},"193395":{"abstract":"This project takes a novel look at Complex Networks and related problems in data mining and signal processing. Issues arising in Complex Networks are formulated as intrinsically topological problems which, when studied using algebraic topological tools, affords highly efficient and effective solutions. The algebraic approach yields distributed computations, and naturally unveils topological information about the underlying network for strategic planning of mitigation. In particular, this unified view allows tackling problems of dynamics and evolution of networks, where time-varying properties may in turn be used for network healing. <br\/><br\/>The efficient developed solutions will have an impact on data mining analysis, and provide more insight into the associated strengths and limitations, as well as other network related applications in a broad variety of networks.","title":"CIF: Small: Complex Networks: A Unifying Topological View","awardID":"1217874","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":[518008],"PO":["564924"]},"193187":{"abstract":"Energy consumption is now emerging as a dominant performance measure in computer systems. In recent years, significant progress in improving energy efficiency has been accomplished by a combination of better hardware design and software tools. Yet the design of future energy-efficient computer systems will ultimately require the development of fundamental models and algorithmic tools that can be used to guide practical solutions.<br\/><br\/>This project is to study algorithmic methods for improving energy efficiency of data processing and storage in computer systems. The basic approach is to model the operation of various system components in the language of combinatorial optimization, with the objective function representing energy consumption, and to solve these problems using exact or approximate efficient algorithms. Many of those problems can be formulated in terms of task scheduling, where the objective is to optimize the CPU energy consumption required to complete a collection of tasks, while meeting some performance requirements. Other examples include minimizing energy consumption of memory systems, both the internal and external memories, by optimizing power levels and sophisticated paging or caching strategies. In addition to addressing some specific energy optimization problems, this work is expected to produce new algorithmic techniques, as well as deeper understanding of the adequacy of standard performance enhancement tools, like caching and load balancing, for improving energy efficiency. The study on energy complexity will also shed some light on the relation between computation and energy.<br\/><br\/>Some algorithms developed in the course of this research will be implemented, tested empirically on the FreeBSD-based platform, and made available to practitioners. The educational component includes research projects for graduate and undergraduate students, and developing a course on sustainable computing.","title":"AF: Small: Collaborative Research: Algorithmic Approaches to Energy-Efficient Computing","awardID":"1216993","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["485283"],"PO":["565251"]},"194067":{"abstract":"The objective of this project is to help developers in making applications? usage of personal information transparent to mobile phone users, system integrators, and other evaluators. Recent well-publicized mobile privacy incidents have demonstrated all these parties have lost count of what information mobile devices collect, store, and transmit. A successful project would lead to improved privacy and application transparency, and would help prevent future privacy compromises. Project results could be adopted into mobile operating systems and could guide FTC policy on mobile privacy. The project includes close collaboration with industry stakeholders to facilitate dissemination of new ideas. Work will be conducted with graduate and undergraduate students, which will not only provide them with training in research methodology, but also expose them to important ethical questions surrounding privacy issues in mobile applications.<br\/><br\/>This project will aid developers and nudge them to follow privacy principles by making their usage of personal information transparent. Towards this end, this project will develop guidelines for privacy-aware system APIs that encourage developers to employ privacy-by-design techniques. For example, system APIs designed from a privacy perspective will make it easier to obtain more general information, rather than potentially more sensitive, fine-grained personal information. Through case studies with developers and users, this project is expected to lead to novel insights into the effectiveness of these techniques.","title":"TWC: Small: Redesigning Mobile Privacy: Helping Developers to Protect Users","awardID":"1223977","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["553648","521585"],"PO":["565264"]},"189634":{"abstract":"Cloud computing offers IT organizations the ability to create<br\/>geo-distributed, and highly scalable applications while providing<br\/>attractive cost-saving advantages. Yet, architecting, configuring, and<br\/>adapting cloud applications (latency-sensitive web applications and<br\/>bulk data processing applications) to meet their stringent performance<br\/>requirements is a challenge given the rich set of configuration<br\/>options, shared multi-tenant nature of cloud platforms, and dynamics<br\/>resulting from activities such as planned maintenance.<br\/><br\/>This project is developing novel methodologies, algorithms, and<br\/>systems that can enable application architects to (1) judiciously<br\/>architect applications across multiple cloud data-centers while<br\/>considering application performance requirements, cost saving<br\/>objectives, and cloud pricing schemes guided by performance and cost<br\/>models of cloud components; (2) automatically learn effective<br\/>application configurations and configuration-to-performance prediction<br\/>models through statistical machine learning techniques; and (3) create<br\/>applications that can adapt to ongoing dynamics in cloud environments<br\/>through transaction reassignment over shorter time-scales, and<br\/>application migration over longer time-scales.<br\/><br\/>The impact of this research is multi-fold: (1) Enable IT organizations<br\/>to significantly reduce costs by optimally moving their operations to<br\/>the cloud; (2) create benchmarks based on operationally deployed<br\/>applications and collecting workload traces which will be made<br\/>available to the research community; (3) make developed algorithms and<br\/>systems widely available as open source software; (4) inform the<br\/>design of a nation-wide health-care cloud in Thailand; (4) introduce<br\/>cloud computing related topics in the undergraduate and graduate<br\/>curriculum; and (6) train multiple Ph.D., M.S., and undergraduate<br\/>students, with explicit effort to recruit and train students from<br\/>under-represented minority groups.","title":"CSR: Medium: Collaborative Research: Architecting Performance Sensitive Applications for the Cloud","awardID":"1162333","effectiveDate":"2012-08-15","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["524799",508447],"PO":["565255"]},"189667":{"abstract":"The main goal of this proposal is to understand the safety problems introduced by realtime online video-based services, and to develop highly scalable and accurate solutions and systems that protect the safety of interactive video users. While realtime, online video-based services are newly emerging services, they have already gained tremendous popularity with millions of users. These services introduce new ways for people to interact and have the potential to significantly impact, even revolutionize, pervasive computing and online social networking services. Unfortunately, the presence of misbehaving users may jeopardize the entire utility of these services and bring harm to minors. <br\/><br\/>This proposal investigates innovative ideas including fine-grained cascaded classification and a rule-based pre-classifier with logistic regression to improve scalability while preserving accuracy, incorporating contextual information from smartphone sensors and application scenarios to improve accuracy, and mechanisms to evaluate and circumvent cyber-bullying. The complete system will be deployed and evaluated using real-world data. With the growing popularity of realtime, online video-based services and the threat to minors of being exposed to unsuitable content or cyber-bullying attacks, developing effective techniques for detecting misbehaving users of these services will have significant and far-reaching beneficial impacts on society. <br\/><br\/>All software developed from this project will be released as open source. The project will develop and integrate educational course modules on Internet safety topics into relevant middle and high school courses via an on-going NSF GK-12 project. Undergraduate and minority students will be recruited via the Colorado Diversity Institute and annual REU supplements.","title":"CSR: Medium: Highly Scalable and Accurate System Support for Detecting Misbehaving Users and Mitigating Criminal Activities in Realtime Online Video-Based Services","awardID":"1162614","effectiveDate":"2012-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":["555576","556808",508532],"PO":["565255"]},"192461":{"abstract":"The memory system continues to be a major performance and power<br\/>bottleneck in nearly all computing systems. And, it is becoming<br\/>increasingly more so with major application, architecture, and<br\/>technology trends. Embedded applications that acquire and<br\/>process real-time data, Internet and cloud applications that have to<br\/>analyze large databases, and the exa-scale era HPC applications that<br\/>need to crunch voluminous data sets are just a few examples of<br\/>increasingly data-intensive applications that require high memory<br\/>capacity, performance, and energy efficiency. Thus, the well-known <br\/>memory wall problem has become even more difficult to surmount and<br\/>needs a fundamental rethinking of the memory hierarchy design for future<br\/>computing platforms.<br\/><br\/>The goal of this proposal is to fundamentally and holistically<br\/>rethink the design of the entire memory hierarchy taking into consideration<br\/>the emerging device\/memory technologies and to exploit the design trade-offs<br\/>at different layers of the system stack -- from devices to micro-architecture,<br\/>compilers and runtime systems. The solution will cover innovations in<br\/>architecting and optimizing the entire memory path consisting of the caches,<br\/>on-chip networks, memory controller and main memory. The objective<br\/>is to enable 100X improvement in memory capacity over the next<br\/>decade, while providing 5X improvement in performance and<br\/>10X improvement in energy efficiency. The proposed research has the potential to transform the design of<br\/>next-generation memory systems for the multi-core era, which is expected<br\/>to be a ubiquitous part of the entire IT sector.<br\/>The cross-cutting nature of this research can foster new research directions<br\/>in several areas, spanning technology\/energy-aware design, computer architecture,<br\/>compilers, and system\/application software. With the memory system forming<br\/>the backbone of nearly every envisioned future application domain, the<br\/>broader impact of this research can accelerate the design and deployment<br\/>of future applications. This project will enable transfer of research<br\/>results to industry, enhance undergraduate and graduate student training<br\/>including under-represented students, and contribute to the development of new<br\/>research and teaching tools.","title":"SHF: Large:Collaborative Research: Architecting the Next Generation Memory Hierarchy - A Holistic Approach","awardID":"1213052","effectiveDate":"2012-08-01","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["550859","542015","549542","542016","518504"],"PO":["366560"]},"200678":{"abstract":"The security of pervasive computing devices relies on cryptographic engines which are usually considered the most trusted part of the system. An immanent threat to embedded cryptographic engines are physical attacks. Practical countermeasures against physical attacks are not completely fail-safe and overly expensive for most applications. Theoretical approaches, however, still tend to have imperfect leakage models and wrong or impractical assumptions about the abilities of cryptographic sub-primitives. Yet, the theoretical concepts of leakage resilience, which have been mostly disregarded by practitioners, carry a great potential to construct cryptographic primitives that resist physical attacks and allow for more resource-efficient implementations.<br\/><br\/>The project investigates solutions for basic cryptographic services that are (i) secure in the presence of physical attacks and (ii) are comparable in performance and costs to state-of-the-art implementations of cryptography. This is achieved by enhancing the concepts of leakage resilience to make them applicable in the constrained regimes of embedded pervasive systems. Theoretical concepts are advanced and brought into practice by actual implementation. Practical evaluation uncovers remaining weaknesses in the currently used leakage models. By combining the advantages of both approaches ? a thorough practical evaluation of the applied methods and the well-defined leakage-resilience of the theoretical approaches ? stronger, more reliable, and practical solutions are derived. Besides an increased security for the wide range of embedded products, the findings give valuable feedback to both theoretic cryptographers and practical security architects. <br\/><br\/>Only security solutions that are leakage resilient, withstand practical evaluation and match economic expectations guarantee a widespread use and hence more secure pervasive systems.","title":"CAREER: Practical Leakage Resilience: Provable Side-Channel Resistance for Embedded Systems","awardID":"1261399","effectiveDate":"2012-08-13","expirationDate":"2015-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["550154"],"PO":["565264"]},"193440":{"abstract":"Numerical results of scientific computations are stored in computers as<br\/>floating-point numbers, an approximation of real numbers that accounts for<br\/>the fact that a computer's storage is limited. This need for approximation<br\/>has the unfortunate side effect that floating-point numbers don't abide by<br\/>common laws of arithmetic known from high school, such as the associativity<br\/>of addition. As a consequence, apparently equivalent implementations of<br\/>floating-point operations on computer hardware may produce very different<br\/>results, such as when the order of operands of an addition is changed by a<br\/>compiler. Programs generically written for high-performance parallel computing<br\/>platforms are likely to be compiled using different floating-point<br\/>implementations and schedulings, as the executable resulting from the<br\/>compilation depends on the available hardware. Such parallel scientific<br\/>programs are therefore susceptible to reliability and portability issues<br\/>that can range from simple deviations in precision to drastic changes of<br\/>program control flow when moving from one architecture to another.<br\/>The results of this research will be tools and techniques to help scientists <br\/>find bugs more effectively in such programs. This research has important implications <br\/>for the reliability of important scientific programs such as those used in biomedical <br\/>imaging applications, climate modelling, and vehicle design. <br\/><br\/>This project develops rigorous methods for analyzing parallel scientific<br\/>code, specifically written using the now emerging OpenCL parallel<br\/>programming standard. The goal is to detect potential sources of<br\/>reliability and portability deficiencies in such code that are due to<br\/>dependencies of the floating-point behavior on the underlying hardware,<br\/>which may be unknown to the programmer. Traditional reliability methods<br\/>such as program testing and debugging are ineffective for parallel OpenCL<br\/>programs, because program behavior may vary across runs, making after-test<br\/>behavior uncertain. For these reasons, the investigators will use rigorous<br\/>analysis methods that are not solely based on program execution. Instead,<br\/>the program is formally modeled as a transition system; the model is<br\/>encoded symbolically, using logical formula representations that can often<br\/>compactly represent the set of executions of the program without executing<br\/>it. The program model is then analyzed for portability violations and<br\/>program errors using floating point-capable decision procedures and model<br\/>checkers. To achieve scalability, the investigators plan to exploit the<br\/>highly symmetric and parametric form of OpenCL programs, where identical<br\/>operations are performed by many computational threads in Single<br\/>Instruction Multiple Data (SIMD) style.","title":"SHF: Small: Ensuring Reliability and Portability of Scientific Software for Heterogeneous Architectures","awardID":"1218075","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}}],"PIcoPI":["534268","557274"],"PO":["565264"]},"192472":{"abstract":"Future scientific and technological efforts to achieve better understanding of oceans and water-related applications will rely heavily on our ability to jointly consider communications, actuation and sensing in a unified system that includes instruments, vehicles, human operators and sensors of all types. The goals of this project are to design networking tools for mobile underwater networks, develop novel navigation mechanisms for communication-constrained autonomous underwater vehicles and to ultimately integrate sensing and classification to provide solutions for the exploration-exploitation tradeoff.<br\/><br\/>This project will lead to development of underwater communication methods with applications to science, security, and industry in the areas of environmental monitoring, aquatic eco-system analysis, ocean accident remediation, surveillance for defense applications, homeland security, oil and gas, aquaculture, geological and oceanographic science, and marine biology. It will also contribute to the training of new information technology professionals and scientists with expertise in interdisciplinary research spanning underwater networks, oceanography and computer science.","title":"NeTS: Large: Collaborative Research: Exploration and Exploitation in Actuated Communication Networks","awardID":"1213128","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[515828,"560335",515830],"PO":["565303"]},"193462":{"abstract":"Hopefully in the near future quantum computers will be built. These devices could outperform conventional digital computers, not by having faster operations but by reducing the number of operations required to obtain a result. However to date there are only a few known examples of computational tasks where this speedup has been established. The PIs of this proposal are looking for other ways that quantum computers could speed up computationally intensive tasks that now challenge the limits of conventional computers. They are looking at several specific methods for achieving quantum speedup. One such method is the Quantum Adiabatic Algorithm, which relies on a fundamental concept in quantum physics that tells us how to keep a quantum system in its lowest energy state. The PIs are investigating the efficacy of this method on general optimization problems which arise often in practice. Another method uses the physics of quantum coherence on geometric structures to seek algorithmic speed up for search problems. In addition to algorithms, the PIs will investigate quantum cryptographic protocols that might be used on a future quantum internet. Applications of these ideas might include more secure identity protection as well as more secure financial transactions.","title":"AF: Small: Physics Based Approaches to Quantum Information Science","awardID":"1218176","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[518163,518164],"PO":["565157"]},"193473":{"abstract":"Sensors on mobile phones can enable attractive sensing applications in different domains such as environmental monitoring, social network, healthcare, etc. However, fundamental energy-efficient resource management problems have not been well studied for mobile phone sensing. In addition, how to provide incentives to attract user participation has not been well addressed. The objective of this project is to develop a unified and green platform for mobile phone sensing, optimize its performance by designing energy-efficient algorithms for sensing task management, and develop game-theoretic incentive mechanisms to attract user participation. The proposed research is organized into four cohesive research thrusts: 1) Design and implement a unified software architecture to enable support for various sensing applications. 2) Develop both platform-centric and user-centric incentive mechanisms to attract user participation. 3) Develop energy-efficient algorithms to manage sensing tasks. 4) Test the developed platform and algorithms via simulation and real experiments. This research will result in a unified and green mobile phone sensing system. Fundamental resource management problems will be solved by theoretically-sound and practical algorithms. The project will also result in novel incentive models and mechanisms for mobile phone sensing. In addition, the proposed platform can create a completely new type of online marketplace. The proposed energy-efficient algorithms can benefit both mobile users and the environment. The project is also expected to advance public understanding of mobile phone sensing via publications, seminars and workshops.","title":"NeTS: Small: Collaborative Research: A Green and Incentive Platform for Mobile Phone Sensing","awardID":"1218203","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["445979"],"PO":["565303"]},"193121":{"abstract":"This research investigates four fundamental questions in quantum information processing. <br\/>1. The power and limitations of two quantum provers, in particular the relation of this model with the classical complexity classes of Polynomial Space and Exponential Time. <br\/>2. The optimal rate at which mixed state quantum information can be compressed, in particular if a well-known lower bound on the rate by Holevo quantity is tight. <br\/>3. Strong lower bound on quantum one-way communication. <br\/>4. The power and limitations of short quantum refereed games, in particular the relation of those models with other quantum and classical complexity classes.<br\/><br\/>Quantum information processing is of fundamental importance to the security, prosperity, and welfare of the nation, as it may lead to revolutionary technologies for super-fast computing and secure methods for communication. The successful completion of this project will illuminate the power and limitations of quantum information processing capabilities, thus contribute to the realistic deployment of quantum information technologies. PI will actively involve undergraduate and graduate students in his research.","title":"AF: CIF: Small: Theoretical Problems in Quantum Cmputation and Cmmunication","awardID":"1216729","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[517335,"549690"],"PO":["565157"]},"193363":{"abstract":"This project develops a theory for characterizing the performance of parallel data structures and parallel algorithms that use parallel structures. Standard metrics for parallel algorithms, such as \"work\" (total amount of computation) and \"span\" (critical-path length), do not naturally generalize in the presence of contention on shared data. Moreover, standard approaches for analyzing sequential data structures, such as amortization, do not seem to generalize when data structures are parallel, in part because the performance depends on the properties of the underlying parallel task schedulers.<br\/><br\/>The specific research goals are as follows: <br\/>(1) Investigate a methodology for designing and analyzing parallel algorithms that use data structures, especially amortized ones. <br\/>(2) Design parallel schedulers that ameliorate the contention on parallel data structures. <br\/>(3) Design parallel data structures that perform provably well with these schedulers.<br\/><br\/>Today parallel computing is ubiquitous. Modern computation platforms---smartphones to network routers, personal computers to large clusters and clouds---each contain multiple processors. Writing parallel code that provably scales well is challenging and techniques for analyzing sequential algorithms and data structures generally do not apply to parallel code. This project will develop a theoretical foundation for characterizing the scalability of parallel programs that contend for access to shared data.","title":"AF: SMALL: Collaborative Research: Data Structures for Parallel Algorithms","awardID":"1217708","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["533341"],"PO":["565157"]},"193275":{"abstract":"Physical networks (such as communications networks and road networks) are dynamic objects, prone to sudden failures, congestion, and malicious attacks. Nonetheless, we must be able to compute basic functions of the current state of these networks, i.e., as network components fail and recover, we need dynamic algorithms and data structures to route along shortest paths and to calculate distances, flow\/cut capacities, and point-to-point connectivity. Currently deployed systems typically deal with the problem of transient failures by periodically recomputing from scratch all the network properties of interest. Solutions of this type are insufficient in two ways. They are computationally inefficient, the extent to which depends on the given network property, and they do not respond to network component failures dynamically.<br\/><br\/>The technical aims of this project are two-fold. The first is to develop abstract representations of graphs and graph properties, along the lines of cactus trees and Gomory-Hu trees. These graph representations are useful in the design of efficient algorithms and data structures, but are also of interest to the broader mathematics community. The second is to develop data structures in the dynamic subgraph model and d-failure model. These abstract models capture the situation found in many real world applications: there is a fixed substrate network, which can be processed in advance by possibly inefficient algorithms, which is subject to a sequence of node\/link failures and recoveries intermixed with queries. A pervasive theme of the project is to determine what gains in efficiency can be had (in running time, space consumption) by accepting approximate solutions, e.g., approximately shortest routes or approximately minimum cuts that are guaranteed to be accurate up to some fixed multiplicative or additive error.<br\/><br\/>In addition to its specific research goals, the aims of this project are to (i) train undergraduate and graduate students in the design and rigorous analysis of efficient data structures, and (ii) incorporate modern data structures into the computer science curriculum by developing course materials appropriate to students at<br\/>the graduate or advanced undergraduate level.","title":"AF:Small:Data Structures for Dynamic Networks","awardID":"1217338","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["549839"],"PO":["565251"]},"197884":{"abstract":"Computational systems biology is a highly inter-disciplinary research area that seeks to understand interplays of molecules and cells in time and space to determine functioning of a complex biological system using sophisticate mathematical and statistical tools. Fostering a new generation of computational biologists that possess the interdisciplinary expertise of molecular biology and computational science will significantly impact future biological and biomedical research and industry. <br\/><br\/>IEEE Workshop on Genomic Signal Processing and Statistics, or GENSIPS, is a workshop sponsored by the IEEE Signal Processing Society. The goals of GENSIPS 2012 are 1) to address the systems biology issues in the emerging omics fields, especially the next-generation sequencing technologies; 2) to significantly increase the participation of graduate students and post-doctoral researchers, especial woman and minority students from signal processing, computer science, biostatistics, and biological science community. This request for fund from NSF will provide travel awards to qualified graduate students and doctoral researchers to support them to present their research papers. Particularly, 20-30% of the awards will be allocated to woman and minority students. Ultimately, GENSIPS 2012 will contribute to fostering student education on computational systems biology.","title":"GENSIPS'12 Conference: Fostering Interdisciplinary Research and Education in Computational Biology","awardID":"1246395","effectiveDate":"2012-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["559861","559862",531038,"559863","334663"],"PO":["565223"]},"207719":{"abstract":"The emerging cognitive radio network (CRN) paradigm has a great potential to solve what seems to be a spectrum crisis, by allowing the unlicensed or secondary users (SUs) to opportunistically and dynamically utilize the white spaces within the licensed bands, without causing harmful interference to the licensed or primary users (PUs). This research investigates two essential components of CRNs: spectrum sensing and spectrum access and sharing. More specifically, the PIs study: 1) novel integrated signal processing and communication designs for data fusion in cooperative spectrum sensing, and 2) novel cooperative spectrum sharing and communication schemes that benefit both PUs and SUs.<br\/><br\/>In contrast to the existing data fusion rules that assume error-free communication channels with capacity constraints, this research involves novel integrated designs that consider the deteriorating effects of communication channels between the radios and the fusion point and therefore are robust against channel errors and provide higher detection reliability. The robustness can further be improved by employing distributed space-time coding and harvesting diversity gain. Novel cooperative communication schemes are developed based on modern coding and enable SUs to relay PUs? rateless coded data packets in a fashion that is completely seamless to PUs. The schemes have mutual benefits for both PUs and SUs and differ from the existing ones in which SUs are silent during PUs? transmission. Broader impacts include (1) bonding the research groups from OSU and the UR and enhancing research and education through this partnership, (2) making a significant impact on the theory and practice of CRNs, (3) increasing the participation of under-represented students in PIs? research groups and promoting engineering among high school students, and (4) integrating research and education through development of new courses.","title":"CIF: Small: Collaborative Research: Cooperative Sensing and Communications for Cognitive Radio Networks","awardID":"1336123","effectiveDate":"2012-08-31","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7935","name":"COMM & INFORMATION THEORY"}}],"PIcoPI":["559577"],"PO":["564924"]},"198556":{"abstract":"CRYPTO is the Annual International Cryptology Conference, sponsored by IACR (International Association of Cryptologic Research), in cooperation with the Computer Science Department of the University of California, Santa Barbara. The first CRYPTO was held in 1981, and since then CRYPTO has been IACR's flagship conference, attracting researchers in the field of cryptology from all over the world.<br\/><br\/>The scope of research papers accepted to CRYPTO range from theoretical foundation of cryptology to application and implementation of cryptographic schemes. This grant supports student attendance at CRYPTO. Students will have the opportunity to interact with many of the luminaries in the field during conference presentations, special tutorial sessions on hot topics, as well as in more informal settings including a reception and banquet dinners.","title":"Conference Funding Proposal: Advances in Cryptology - CRYPTO 2012","awardID":"1249666","effectiveDate":"2012-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[532751,532752,"562062"],"PO":["565327"]},"197258":{"abstract":"This award provides funding for approximately one hundred undergraduate and graduate students each year to attend the 2012 - 2014 Grace Hopper Celebration of Women in Computing conferences. The Grace Hopper Celebration is designed to bring the research and career interests of women in computing to the forefront. Leading researchers present their current work and special sessions focus on the role of women in today's technology fields. NSF scholarship support allows students who could not otherwise attend to participate and become part of the broad Hopper community of young computing professionals that is nurtured through the event.<br\/><br\/>The intellectual merit of this project lies in the access to the many technical presentations and researchers at the conference. In addition, students have the opportunity to participate in poster sessions and receive feedback from discipline specialists. The Grace Hopper Celebration provides a unique, supportive environment for intellectual discourse that is particularly appealing for women and other under-represented groups. <br\/><br\/>The broader impacts of this project deal with providing resources and support to students that will help them persist in their programs as well as access to avenues to continue through the IT workforce pipeline. Students who attend the Grace Hopper Celebration will be exposed to leaders who are creating, improving, and studying computer and information technologies who serve as role models and mentors. This travel scholarship support provides a unique opportunity to directly affect the careers of future computer scientists and impact the serious problem of broadening participation in computing.","title":"Undergraduate and Graduate Student Scholarship and Travel Grants for 2012-2014 Grace Hopper Women in Computing Conference","awardID":"1242967","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":[529248,529249],"PO":["564181"]},"200855":{"abstract":"Mobile content distribution (MCD) is an important emerging application in vehicular ad hoc networks (VANETs). Its high and stable data rate requirement puts significant stress on the severely bandwidth-limited and highly mobile VANETs. Symbol level network coding (SLNC) provides better error-resilience and in turn higher spatial reusability for wireless transmission. It thus is a promising approach for the very bandwidth-hungry MCD networks. This project focuses on two main fundamental problems related to exploiting SLNC for MCD in VANETs. First, a theoretic study on the network capacity and performance bounds achievable by SLNC in mobile wireless networks is carried out. A new model is proposed to characterize the SLNC operation in a flow network setting. Key factors that impact the achievable throughput are identified. Methods to derive accumulated throughput for a mobile receiver following certain mobility pattern are investigated. The second research focus is on the design of distributed and localized algorithms and protocols for MCD networks using SLNC. A suite of new network protocols for SLNC-based MCD network are proposed in order to maximize the throughput of the MCD network.","title":"NeTS: Small: Collaborative Research: Mobile Content Distribution in Vehicular Ad Hoc Networks","awardID":"1262275","effectiveDate":"2012-08-13","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["565164"],"PO":["557315"]},"192451":{"abstract":"The memory system continues to be a major performance and power<br\/>bottleneck in nearly all computing systems. And, it is becoming<br\/>increasingly more so with major application, architecture, and<br\/>technology trends. Embedded applications that acquire and<br\/>process real-time data, Internet and cloud applications that have to<br\/>analyze large databases, and the exa-scale era HPC applications that<br\/>need to crunch voluminous data sets are just a few examples of<br\/>increasingly data-intensive applications that require high memory<br\/>capacity, performance, and energy efficiency. Thus, the well-known <br\/>memory wall problem has become even more difficult to surmount and<br\/>needs a fundamental rethinking of the memory hierarchy design for future<br\/>computing platforms.<br\/><br\/>The goal of this proposal is to fundamentally and holistically<br\/>rethink the design of the entire memory hierarchy taking into consideration<br\/>the emerging device\/memory technologies and to exploit the design trade-offs<br\/>at different layers of the system stack -- from devices to micro-architecture,<br\/>compilers and runtime systems. The solution will cover innovations in<br\/>architecting and optimizing the entire memory path consisting of the caches,<br\/>on-chip networks, memory controller and main memory. The objective<br\/>is to enable 100X improvement in memory capacity over the next<br\/>decade, while providing 5X improvement in performance and<br\/>10X improvement in energy efficiency. The proposed research has the potential to transform the design of<br\/>next-generation memory systems for the multi-core era, which is expected<br\/>to be a ubiquitous part of the entire IT sector.<br\/>The cross-cutting nature of this research can foster new research directions<br\/>in several areas, spanning technology\/energy-aware design, computer architecture,<br\/>compilers, and system\/application software. With the memory system forming<br\/>the backbone of nearly every envisioned future application domain, the<br\/>broader impact of this research can accelerate the design and deployment<br\/>of future applications. This project will enable transfer of research<br\/>results to industry, enhance undergraduate and graduate student training<br\/>including under-represented students, and contribute to the development of new<br\/>research and teaching tools.","title":"SHF: Large:Collaborative Research: Architecting the Next Generation Memory Hierarchy - A Holistic Approach","awardID":"1212962","effectiveDate":"2012-08-01","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":["550874"],"PO":["565264"]},"193430":{"abstract":"This project develops a theory for characterizing the performance of parallel data structures and parallel algorithms that use parallel structures. Standard metrics for parallel algorithms, such as \"work\" (total amount of computation) and \"span\" (critical-path length), do not naturally generalize in the presence of contention on shared data. Moreover, standard approaches for analyzing sequential data structures, such as amortization, do not seem to generalize when data structures are parallel, in part because the performance depends on the properties of the underlying parallel task schedulers.<br\/><br\/>The specific research goals are as follows: <br\/>(1) Investigate a methodology for designing and analyzing parallel algorithms that use data structures, especially amortized ones. <br\/>(2) Design parallel schedulers that ameliorate the contention on parallel data structures. <br\/>(3) Design parallel data structures that perform provably well with these schedulers.<br\/><br\/>Today parallel computing is ubiquitous. Modern computation platforms---smartphones to network routers, personal computers to large clusters and clouds---each contain multiple processors. Writing parallel code that provably scales well is challenging and techniques for analyzing sequential algorithms and data structures generally do not apply to parallel code. This project will develop a theoretical foundation for characterizing the scalability of parallel programs that contend for access to shared data.","title":"AF: SMALL: Collaborative Research: Data Structures for Parallel Algorithms","awardID":"1218017","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7926","name":"ALGORITHMS"}}],"PIcoPI":["556689"],"PO":["565157"]},"193551":{"abstract":"Information theory describes what is possible in reliable communication and data compression, and it generally does so via asymptotic results. While existing asymptotic results are rightly celebrated, they have two limitations. First, none of these results is directed at the asymptotic regime that is the most practically important. In the case of channel coding, this is the regime in which the error probability tends to zero and the rate approaches capacity as the blocklength increases. Second, existing results are often too coarse to be useful as benchmarks for practical codes because they hide nuisance factors that could be significant at moderate blocklengths.<br\/><br\/>This research will address both of these issues. The project will characterize the performance of optimal channel codes for the regime in which the rate approaches capacity and the error probability simultaneously tends to zero. This will be followed by characterizing the nuisance factors in both this regime and the classical ones by establishing more refined \"exact-asymptotic\" bounds. These results will also be extended to data compression and multiuser problems, leading to a more precise understanding of the fundamental limits of coding in practical regimes of interest.<br\/><br\/>In additional to advancing the field of information theory, this research will strengthen its connections to coding theory and probability theory, the former by providing more useful performance bounds for moderate blocklengths and the latter by cross-fertilizing ideas between the two fields. The educational component will revamp course offerings in information theory in order to allow the inclusion of recent research results, including those from this project, and to make the subject accessible to a wider audience.","title":"CIF: Small: Moderate Deviations and Exact Asymptotics in Information Theory","awardID":"1218578","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":[518371],"PO":["564924"]},"193672":{"abstract":"This research involves theoretical and applied research on learning and representation of high-<br\/>dimensional data. The term high dimensionality refers to the property that the number of variables<br\/>or ?unknowns? is typically much larger than the number of observations available at hand. A key<br\/>challenge is being able to represent and learn such phenomena with sample and computational<br\/>requirements scaling favorably in the number of dimensions. This project addresses these challenges<br\/>through a graphical approach by exploiting the inherent graphical structure present in many large<br\/>data-sets.<br\/><br\/>This research considers modeling high-dimensional data through probabilistic graphical models,<br\/>also known as Markov random fields. An important research thrust of this proposal is to develop<br\/>novel algorithms for learning and inference under the framework of graphical models. Another<br\/>important thrust of this proposal is to develop efficient scalable models for representing high-<br\/>dimensional data beyond the traditional framework of graphical models. This research establishes<br\/>strong theoretical guarantees for the developed methods, as well as applies them to real data in<br\/>various domains, including genetic and financial data, and data from large online social networks<br\/>such as Facebook and Twitter.","title":"Graphical Approaches to Modeling High-Dimensional Data","awardID":"1219234","effectiveDate":"2012-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["534528"],"PO":["564898"]},"191494":{"abstract":"The project aims at increasing the ability to respond to large-scale disasters and manage emergencies by including robots and agents as teammates of humans in search and rescue teams. The project focuses on large teams of humans and robots that have only incomplete knowledge of the disaster situation while they accomplish the mission to rescue people and prevent fires.<br\/><br\/>The methodology to achieve cooperation within the teams will be based on the development of mental models shared by team members. The shared mental models will facilitate the interactions among robots and humans by providing a suitable level of abstraction enabling them to share beliefs, desires, and intentions as they work to accomplish their tasks.<br\/><br\/>The performance of teamwork models will be measured by comparing various task performance metrics (such as time to save people), system level metrics (such as computation time or message traffic), and amount of sharedness of the mental models. The experimental work will be conducted using the open source RoboCup Search and Rescue Simulator.<br\/><br\/>Broader impacts include integration of research results in undergraduate courses, availability of the software produced as open source, outreach activities to expose K-12 students to research issues and to excite them about using computing methods for real-world problems. The long term objective is to improve preparadeness for emergency situations, which will help saving lives and minimizing loss of properties.","title":"NRI-Small: Mixed Human-Robot Teams for Search and Rescue","awardID":"1208413","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":["561562"],"PO":["565035"]},"193573":{"abstract":"The Global Positioning System (GPS) provides absolute positioning with meter-scale accuracy anywhere on Earth. However, centimeter-scale precision can only be obtained using expensive and specialized equipment, such as differential GPS. The key observation of the project is that many applications of wireless nodes, such as sensor arrays, require accurate relative node locations only and not precise absolute coordinates. The proposed effort utilizes information fusion in a network of GPS-equipped mobile nodes (sensors, smartphones, ground or aerial robots, etc.) to derive relative geographic location information that is more accurate than what traditional GPS is able to provide. In this project, the nodes share their raw GPS satellite measurements with each other and solve for their pairwise relative location vectors as opposed to individual absolute positions. The proposed work is clearly very challenging. GPS measurements correspond to ranges thousands of kilometers long obtained by utilizing radio signals traveling at the speed of light. All the measurement errors traditional GPS must handle will need to be addressed by our approach as well. The proposed unique combination of known methods from literature and novel techniques of our own shows great promise in achieving centimeter scale accuracy in GPS-based relative localization.<br\/><br\/>If successful, the project will have a truly broad impact through the numerous applications it will enable that are either not economical or simply not possible today. These include tight formation flying of UAVs, an urban guidance system for the visually impaired, collision avoidance and platoon control in autonomous vehicles, consumer grade surveying equipment, and mobile distributed microphone (and other sensor) arrays. Moreover, the project will focus on involving undergraduate students in our research. The Vanderbilt Mobile Application Team (VMAT) is a young volunteer self-organizing undergraduate student club with the primary purpose of teaching\/involving students in the field of mobile phone application development. The PIs will encourage VMAT students to develop apps using our precise relative localization feature by providing them hardware and a library of our prototype system early on, and will place special emphasis on involving students from traditionally under-represented groups in VMAT in general and in this research project in particular.","title":"NETS: Small: Relative Localization with GPS","awardID":"1218710","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["551714","527012"],"PO":["565303"]},"193342":{"abstract":"Sensors on mobile phones can enable attractive sensing applications in different domains such as environmental monitoring, social network, healthcare, etc. However, fundamental energy-efficient resource management problems have not been well studied for mobile phone sensing. In addition, how to provide incentives to attract user participation has not been well addressed. The objective of this project is to develop a unified and green platform for mobile phone sensing, optimize its performance by designing energy-efficient algorithms for sensing task management, and develop game-theoretic incentive mechanisms to attract user participation. The proposed research is organized into four cohesive research thrusts: 1) Design and implement a unified software architecture to enable support for various sensing applications. 2) Develop both platform-centric and user-centric incentive mechanisms to attract user participation. 3) Develop energy-efficient algorithms to manage sensing tasks. 4) Test the developed platform and algorithms via simulation and real experiments. This research will result in a unified and green mobile phone sensing system. Fundamental resource management problems will be solved by theoretically-sound and practical algorithms. The project will also result in novel incentive models and mechanisms for mobile phone sensing. In addition, the proposed platform can create a completely new type of online marketplace. The proposed energy-efficient algorithms can benefit both mobile users and the environment. The project is also expected to advance public understanding of mobile phone sensing via publications, seminars and workshops.","title":"NeTS: Small: Collaborative Research: A Green and Incentive Platform For Mobile Phone Sensing","awardID":"1217611","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[517891],"PO":["565303"]},"193364":{"abstract":"Recent advances in program verification show that we are on the verge of being able to prove correctness of safety and security critical software systems. But the proofs only establish correctness with respect to a model of the underlying processor on which the code executes. Unfortunately, the community lacks high-fidelity, carefully tested specifications of widely-used processors, such as Intel's x86 family of processors. This severely limits efforts in making software reliable and secure, from software assurance to malware analysis to sandboxing technologies. The goal of this project is to provide tools for building, reasoning about, and validating models of widely-used processors. The proposed research will result in public specifications of common processors, which will benefit a wide range of software applications. It will help improve the dependability and security of critical software applications.<br\/><br\/>The investigators' approach to building processor models is carefully designed to support reuse of components across different architectures and different applications. In particular, they propose to formalize two domain-specific languages that will make it easy to specify decoders and instruction semantics. The tools for these languages will include support for efficient execution so that the models can be tested against implementations. To demonstrate the efficacy of these tools, the investigators will build and validate models of both the x86 and ARM families of processors. They will also investigate applications of these models by building correctness proofs of verifiers for inlined reference monitors and by integrating them as the target languages of a verified compiler. The investigators plan to expend efforts on building a community of researchers for formal processor models and to involve this community to give feedback, improve, and use the models. The project will also provide excellent opportunities for training undergraduate students and for developing new curriculum materials on formal methods.","title":"SHF: Small: Collaborative Research: Reusable Tools for Formal Modeling of Machine Code","awardID":"1217710","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7943","name":"PROGRAMMING LANGUAGES"}}],"PIcoPI":[517943],"PO":["564588"]},"193254":{"abstract":"Geometric spaces arise in computer science through a number of avenues. The most obvious of these occurs when the input data for a problem possesses an inherent metric structure, like the hop-distance between nodes in a network, or the similarity distance between pairs of genomic sequences. But there are other, more subtle examples, like the geometry of sparse vectors, which arises prominently in coding theory, signal recovery, and quantum information, or the effective resistance distance between nodes in an electrical network, which has proven to be a powerful algorithmic tool in attacking both algebraic and combinatorial problems.<br\/><br\/>Perhaps most strikingly, in the setting of combinatorial optimization, high-dimensional geometry often presents itself in an unexpected and profound manner. A basic example is the use of convex optimization in the solution---exact or approximate---to a variety of combinatorial problems. In many cases, a problem with an a priori purely combinatorial structure is shown to involve rich geometric phenomenon. Furthermore, we now realize that often this structure is inherent and fundamental, in the sense that any solution to the problem must confront its geometric core.<br\/><br\/>The PI will seek to understand these connections and develop new algorithmic techniques to exploit them. This work will employ techniques from high-dimensional geometry and probability, functional analysis, spectral geometry, and combinatorics to attack problems at the forefront of computer science. This includes addressing fundamental gaps in our understanding of theoretical issues, as well as developing solutions to practical problems that arise from the need to analyze and manipulate massive data sets. To achieve these goals, the investigator intends to address central, important open problems in the fields of approximation algorithms, high-dimensional information theory, and discrete asymptotic convex geometry.","title":"AF: Small: Metric Geometry for Combinatorial Problems","awardID":"1217256","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[517683],"PO":["565157"]},"198831":{"abstract":"In a large-scale destructive event one of the great challenges for public health workers and rescue teams is to have stable and accessible emergency communication systems that are compatible with the needs of the users. Mobile technology is rapidly being integrated into the to first responder environment, yet little research currently exists regarding the use of communication platforms and Internet social networks for emergency response; as a consequence, there exists a gap in the body of knowledge regarding the most effective and efficient use of mobile communication platforms in disaster response. The PI's objective in this exploratory research is to lay the foundation for a human-centered methodology to evaluate critical human factors and ergonomic issues that can be addressed through the use of mobile technology in the preparation and response phases of emergency management. Because similarities exist between the 2011 earthquake in the Northern Virginia region of the United States and the Christchurch, New Zealand earthquake of 2010, her approach is to initiate an international research collaboration between the US and NZ to address research questions such as: What are the current and emerging technologies used by emergency management personnel (including first responders and volunteers) in the preparation and response phases of disaster management? What are the human factors and ergonomics challenges to emergency management personnel that can be addressed through the use of mobile technology? How does the use of emerging technology and social networking impact the performance of emergency management personnel from a human factors perspective? Can a mathematical model be developed to measure the usability, relative importance and reliability of essential technology in the preparation and response phases of disaster management? The work will include review of data from the Christ Church earthquakes and on-site data collection in New Zealand using mock scenarios and observation analysis of emergency managers during training exercises in the Emergency Operations Center Lab at Massey University.<br\/><br\/>Broader Impacts: Technology can be rendered useless in extreme situations absent appropriate communication and task performance protocols. So establishing guidelines and communication protocols for ergonomically sound task performance in emergency management has the potential to transform the response to human needs in catastrophic events. Project outcomes will lay the foundation for maximizing communication resources for first responders during relief efforts while significantly impacting the quality of aid rendered.","title":"EAGER: Initiation of a US-New Zealand Human Factors in Disaster Management Research Collaboration","awardID":"1251425","effectiveDate":"2012-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[533425],"PO":["565227"]},"193276":{"abstract":"At the heart of most algorithms for distributed and randomized algorithms is a Markov chain that 'visits and samples' a subset of the nodes in the graph and which satisfies the following three properties: the answer derived from walking the graph converges, the transition probabilities for next state only depend on locally available information and the convergence is 'fast-mixing' or efficient. The proposal will develop approaches for improving the mixing time of Markov Chain Monte Carlo (MCMC) algorithms. These approaches are then to be applied as a building block for algorithms in three distinct areas -- (1) sampling large graphs (2) wireless multihop scheduling and (3) duty cycling in wireless sensor networks.<br\/><br\/>Broader Impact: MCMC-like approaches underly algorithms for a wide range of socially and economically important problems and that progress in improving the mixing time will have significant impacts. Additionally, the proposal highlights student mentorship and interdisciplinary course development.","title":"NeTS: Small: Distributed and Efficient Randomized Algorithms for Large Networks","awardID":"1217341","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[517733],"PO":["564993"]},"195223":{"abstract":"This award will support a Research Coordination Network aimed at creating models for Arctic urban sustainability. It is a multi-disciplinary, international effort examining the interconnections among resource development, climate change, and evolving demographic patterns in an effort to provide advice to U.S., Russian, and other policy-makers on how to develop Arctic oil and natural gas deposits and their related infrastructure in a way that produces minimal impact on the environment. The five-year project will convene an annual meeting of scientists working on these issues in Washington and Russia (alternating yearly) in order to facilitate collaboration across disciplines and institutes and to spur better communication between the researchers and policy-making community. Between meetings, the network will engage its participants through webinars hosted at George Washington University, place-based exercises to develop recommendations for specific cities, and coordinating on-going research projects. Russia is the central focus of this project because its territory holds most of the Arctic's energy resources and is the site of the most extensive urban development in the high north. How Russia develops its Arctic energy resources and how its northern cities evolve over the next several decades will have a major impact on the environmental health of the entire Arctic region. The output of the project will be policy advice on how to improve Arctic sustainability in the crucial urban areas associated with energy resource development.<br\/><br\/>The project bridges disciplinary and national divides by bringing together geographers, political scientists, and sociologists to study the interaction of human and natural systems in the Arctic. Project personnel include researchers with a wide range of expertise, including knowledge of energy resource development; migration and employment patterns in Eurasia; and scientific measurement of permafrost thickness throughout Arctic regions. The project will provide additional enrichment for a) the graduate students and early career scholars who are involved in the networking activities, b) residents of Arctic urban developments who will receive area-specific advice on improving sustainability, c) and policy-makers who benefit from input on how infrastructure sites, resource exploitation, and social urban environments can be made more robust in light of forthcoming climate and socio-economic changes. Additionally, the project will serve underrepresented groups by assessing urban impacts on the indigenous peoples of the Arctic and examining the role that foreign laborers play in Arctic infrastructure development.","title":"RCN-SEES: Building a Research Network for Promoting Arctic Urban Sustainability in Russia","awardID":"1231294","effectiveDate":"2012-08-15","expirationDate":"2017-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2878","name":"SPECIAL PROJECTS - CCF"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0808","name":"Division of BIOLOGICAL INFRASTRUCTURE","abbr":"DBI"},"pgm":{"id":"1664","name":"Research Coordination Networks"}}],"PIcoPI":[523332,523333,523334,523335],"PO":["564332"]},"196202":{"abstract":"The ACM SIGCOMM Conference will be held from August 13 to August 17, 2012 in Helsinki, Finland. ACM SIGCOMM is the flagship annual conference of the Special Interest Group on Data Communications, a special interest group of the Association for Computing Machinery. This proposal requests funding to assist approximately 10 graduate students and 3 postdoctoral students from US institutions to attend this conference. Participation in conferences such as SIGCOMM is an extremely important part of the graduate school experience, provides students with an opportunity to interact with more senior researchers and exposes them to leading edge research in the field. The support requested in this proposal will enable people in the above categories to attend the main SIGCOMM 2012 conference, who would be otherwise unable to do so. The travel grant chairs are committed to encouraging the participation of women and under-represented minorities.<br\/><br\/>Intellectual Merit: This project proposes to provide travel support for US-based graduate and postdoctoral students to attend the main SIGCOMM 2012 conference. SIGCOMM is the flagship conference in the networking community and a highly competitive venue (with acceptance rate around 10%). Its rich program exposes participants to new ideas and cutting-edge research and allows for interaction between researchers from all over the world.<br\/><br\/>Broader Impact: This project integrates research and education of students through exposure to a premier technical meeting in computer networks and communications. Students will have the opportunity to observe high-quality presentations and interact with senior researchers in the field. The proposed student participation is expected to have a positive impact on the students' research interests and quality. Second, the project will promote diversity by encouraging and enabling women and other under-represented minorities to participate. Furthermore, the truly international flavor of SIGCOMM as an annual conference is well-known and reflected in the composition of the Technical Program Committee as well as in the authors of the submitted and accepted papers. As such, it cultivates international research interactions and presents a tremendous opportunity to students to increase their breadth of ideas, research approaches, and technical perspectives.","title":"Student Travel Grants for SIGCOMM 2012 Conference","awardID":"1237809","effectiveDate":"2012-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["562066"],"PO":["565090"]},"197545":{"abstract":"This award provides travel grants to 10 US students, in the amount of $1,000 each, to attend the 2012 International Green Computing Conference (IGCC-12) which is to be held in San Jose, California in June 5-8, 2012. The travel grant recipients are expected to report on their learning experiences after the conference ends.<br\/><br\/>IGCC is the premier conference in the area of Green or Sustainable Computing. This conference aims to enable researchers and students to the study and practice the efficient use of computing resources, which in turn can impact a spectrum of economic, ecological, and social objectives.","title":"Student Travel Sponsorship for the 2012 International Green Computing Conference","awardID":"1244773","effectiveDate":"2012-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["559680","559679"],"PO":["565255"]},"197314":{"abstract":"This project will deploy a pilot Software-Defined Science Network (SDSN) to interconnect research computing resources on the Duke campus and link them to national WAN circuit fabrics. This pilot deployment will support both Computer Science applications such as GENI and domain sciences such as Duke's Institute for Genome Sciences and Policy (IGSP) and Duke's high-energy physics group (HEP). Additionally, the SDSN will connect to the Duke Shared Cluster Resource (DSCR), a 4300-core shared batch job service used by domain scientists all over the Duke campus. The key goal is to enable domain scientists to request virtual networks that are 'simple and scalable enclaves' for science networking, and that link selected resources on campus with selected resources outside, while excluding unrelated traffic.<br\/><br\/>The project will experiment with OpenFlow controllers on a trial basis within isolated flowspace slices of the SDSN, including OpenFlow-enabled traffic engineering policies that offload science traffic onto the SDSN. The project plan is that initial trial demos will exercise cloudbursting capability to expand computing service into a cloud site, and (potentially) to support virtual machine migration among the OpenStack cloud testbeds on campus. The PIs will report on their experience in technical papers. Travel budget is included for presentation of learnings to other university CIO\/CTOs. Although software-defined networking (SDN) technologies are currently being widely discussed and are key elements in the GENI architecture, there is little operational or campus-level architectural experience with using them. The project will advance the state of the art in integrating SDN technologies into campus networks, and in enabling safe, controlled interconnection of science resources and GENI resources within and across campuses. The project seeks to devise and implement practical solutions that are easily reproducible beyond the initial prototype, scalable to wider use, and grounded in technologies that are (or soon will be) solid, manageable, and commercially available for deployment throughout production campus networks. The project outcomes include reporting of results and lessons to other campus network operators and to SDN researchers and industry<br\/><br\/>Broader Impact:<br\/>The pilot will provide an opportunity to gain experience with architectural, deployment, administrative and operational issues of OpenFlow in campus settings to serve research and education needs beyond the Computer Science domain. The Duke campus OpenFlow model (GENI-derived technology) offers domain sciences on-demand access to ultra-high-speed networks without performance limiting firewalls. As such it will provide direct benefit to the domain sciences. The PIs will report on issues and operational experience associated with the deployment. These reports and the PI's willingness to share their experience with other universities will reduce barriers to use of GENI from campuses, establish GENI technologies and OpenFlow as building blocks for science networking, enhance support for computational science on the Duke campus, and facilitate sharing of resources and data among science researchers and their collaborators on and off campus.","title":"EAGER: Duke ON-RAMPS: OpenFlow-Enabled Network Resource Access that is Manageable, Programmatic, and Safe","awardID":"1243315","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["553906","530835"],"PO":["564993"]},"198535":{"abstract":"Natural language inference (NLI) and data-driven paraphrasing share the related goals of being able to detect the semantic relationship between two natural language expressions, and being able to re-word an input text so that the resulting text is meaning-equivalent but worded differently. On the one hand, work in recognizing textual entailment (RTE) within NLI has attempted to formalize the process of determining whether a natural language hypothesis is entailed by a natural language premise, sometimes called \"natural logic\". Research in data-driven paraphrasing, on the other hand, attempts to extract paraphrases at a variety of levels of granularity including lexical paraphrases (simple synonyms), phrasal paraphrases, phrasal templates (or \"inference rules\"), and sentential paraphrases, for various downstream applications such as question answering, information extraction, text generation, and summarization.<br\/><br\/>This EAGER award explores bridging the gap, through analysis of sentential paraphrasing via synchronous context free grammars (SCFGs), and how they may be coupled to formal constraints akin to recent work in phrase-based formulations of natural logic for RTE. Data-driven paraphrasing has largely neglected semantic formalisms, and NLI has relied heavily on hand-crafted resources like WordNet. If this project is successful it will potentially lead towards NLI systems that are more robust, and paraphrasing systems that are better formalized. Taken together, these improvements will allow better RTE systems to be developed. Moreover, this project has the potential to impact widely used human language technologies such as web search and natural language interfaces to mobile devices, and to further the connection between computational semantics and formal linguistics.","title":"EAGER: Combining natural language inference and data-driven paraphrasing","awardID":"1249516","effectiveDate":"2012-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[532696,"560763"],"PO":["565215"]},"197116":{"abstract":"Many real-world phenomena can be modeled by dynamic networks whose connectivity as well as activity changes over time. Hence, there is a growing interest in elucidating the structure and dynamics of such networks. Existing approaches to this problem focus mainly on utilizing either coarse network properties or global structural features to comprehend network dynamics. Such methods often rely on extensions of network features of static networks to understand dynamic networks and fail to capture the rich dynamics of real-world networks. <br\/><br\/>This exploratory project explores a hierarchical approach to decomposition of network structure and dynamics that can explain changing dynamics at multiple scales ranging from node-level to community-level. The approach is novel, and because of its untested nature, somewhat risky. The research is organized around three aims:(i) Develop information-theoretic flow based approaches that can extract multiple layers of dynamics by simultaneously optimizing for explicit community structures and partial flow dynamics in complex networks. (ii) Develop a computational framework for dynamics-aware network summarization that preserves the flow dynamics of graphs and provides a summary of the large-scale graph dynamics.<br\/><br\/>The project advances the current state-of-the-art in network data analytics. The resulting tools for elucidating the structure and dynamics of complex networks at multiple scales could potentially transform the way we understand, design, engineer, and control complex networks. The project enriches research-based training and outreach activities at Wayne State University.","title":"EAGER: Efficient Methods for Characterizing Large-Scale Network Dynamics","awardID":"1242304","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["550519"],"PO":["565136"]},"198447":{"abstract":"In this project the PI will explore a novel experimental and methodological framework for measuring and understanding the role of stress in student performance on exams. Stress is defined here as sympathetic arousal, an ever-present mechanism that helps humans cope with perceived threats or challenges. The planned experiments, in which criticality and computer-mediation define the two axes of the experimental space, will cover a rich set of combinations that include bi-weekly exams each of which counts for a small percentage of the grade vs. midterm and final exams that each count for a large percentage of the grade, alternately offered in paper and iPad forms. Subjects will be randomly divided into control and interventional groups; in the interventional group the time will be relaxed, to determine the effect of stress reduction on the evolution of competency. Unobtrusive measurements (to ensure non-interference with the observed behaviors) will span the genetic, physiological and psychological levels, while micro to macro measurements will afford a deep understanding of the issues. While most HCI-related projects aim to change the design of the interface, this research instead investigates the possibility of \"changing\" the human (student) by revolutionizing relevant aspects of the educational system. Improving the human-computer interface is a secondary aim where, depending upon the success of the intervention, stress-reduction designs can be conceived to counter-balance the time stressor.<br\/><br\/>Broader Impacts: This research tries to identify and suggest ways to correct problems in paper- vs. computer-mediated exams, precipitated by strong sympathetic responses. To this end the PI explores the role of the time stressor, and in so doing questions certain aspects of the prevailing educational philosophy.","title":"EAGER: The Effect of Stress and the Role of Computer Mediation on Exam Performance","awardID":"1249208","effectiveDate":"2012-08-01","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[532471],"PO":["565227"]},"189625":{"abstract":"Cloud computing offers IT organizations the ability to create<br\/>geo-distributed, and highly scalable applications while providing<br\/>attractive cost-saving advantages. Yet, architecting, configuring, and<br\/>adapting cloud applications (latency-sensitive web applications and<br\/>bulk data processing applications) to meet their stringent performance<br\/>requirements is a challenge given the rich set of configuration<br\/>options, shared multi-tenant nature of cloud platforms, and dynamics<br\/>resulting from activities such as planned maintenance.<br\/><br\/>This project is developing novel methodologies, algorithms, and<br\/>systems that can enable application architects to (1) judiciously<br\/>architect applications across multiple cloud data-centers while<br\/>considering application performance requirements, cost saving<br\/>objectives, and cloud pricing schemes guided by performance and cost<br\/>models of cloud components; (2) automatically learn effective<br\/>application configurations and configuration-to-performance prediction<br\/>models through statistical machine learning techniques; and (3) create<br\/>applications that can adapt to ongoing dynamics in cloud environments<br\/>through transaction reassignment over shorter time-scales, and<br\/>application migration over longer time-scales.<br\/><br\/>The impact of this research is multi-fold: (1) Enable IT organizations<br\/>to significantly reduce costs by optimally moving their operations to<br\/>the cloud; (2) create benchmarks based on operationally deployed<br\/>applications and collecting workload traces which will be made<br\/>available to the research community; (3) make developed algorithms and<br\/>systems widely available as open source software; (4) inform the<br\/>design of a nation-wide health-care cloud in Thailand; (4) introduce<br\/>cloud computing related topics in the undergraduate and graduate<br\/>curriculum; and (6) train multiple Ph.D., M.S., and undergraduate<br\/>students, with explicit effort to recruit and train students from<br\/>under-represented minority groups.","title":"CSR: Medium: Collaborative Research: Architecting Performance Sensitive Applications for the Cloud","awardID":"1162270","effectiveDate":"2012-08-15","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["543600"],"PO":["565255"]},"198117":{"abstract":"This EArly-concept Grant for Exploratory Research (EAGER) supports an exploratory study to evaluate model components for prediction of human speech recognition in the presence of noise. Such a model has the potential to predict confusions between fine phonetic distinctions in different levels of background noise and at different speaking rates. The study takes advantage of modern physiological results that indicate that the primary auditory cortex performs spectro-temporal filtering; that is, that there are cells that are sensitive to particular spectro-temporal modulations at each auditory frequency. In this project, perceptual experiments in the presence of both stationary and non-stationary additive noise and at different signal-to-noise ratios for a database of CVC syllables recorded at 2 different speaking rates yield confusion statistics. These statistics are then compared to those resulting from an auditory model enhanced by elements incorporating these spectro-temporal filters. <br\/><br\/>Successful results from this study will suggest enhancements to current hearing models and ultimately, after a broader study for which this EAGER is a pilot, advance the understanding of human speech perception. Background noise presents a challenging problem for a variety of speech and hearing devices including hearing aids and automatic speech recognition (ASR) systems. Since normal-hearing human listeners are extremely adept at perceiving speech in noise, this improved understanding of human models could lead to better artificial systems for speech processing. The databases and tools developed for this study will be disseminated to the research community.","title":"EAGER: Collaborative Research: Towards Modeling Human Speech Confusions in Noise","awardID":"1247809","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["558131",531681],"PO":["565215"]},"192991":{"abstract":"The desire to hide the transmission of information has existed since antiquity. This has included the need to conceal the very existence of transmissions; exposing the presence of transmissions may reveal the location of the sender or an increase in the frequency of transmissions may indicate the impending occurrence of an event. Research is underway to determine if communications can be hidden in the midst of the RF emissions of current and future broadband packet-based wireless networks. These systems use Orthogonal Frequency Division Multiplexing (OFDM) along with protocols that adapt to the environment, e.g., adaptive modulation and coding (AMC), Hybrid ARQ (HARQ), and opportunistic scheduling. This research is based on the key insight that the adaptive nature of the protocols used in wireless systems makes the networks vulnerable to exploitation. This research is discovering those weaknesses and exposing how those vulnerabilities can be used to enable communications by covert ad hoc networks. Theoretical systems analysis combined with laboratory prototyping is quantitatively demonstrating the information transfer and low probability of detection capabilities that covert systems can achieve as well as their impact on the performance of the target wireless system. The effort could lead to new covert communications techniques. Conversely, exposing the vulnerabilities of wireless networks has to potential to help to guide the creation of new protocols or motivate the modification of existing techniques to make them less susceptible to exploitation.","title":"NeTS: Small: Exploiting Adaptive Protocols in Packet-Based Broadband Wireless Networks","awardID":"1216132","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["402055"],"PO":["557315"]},"200856":{"abstract":"The economics of Cloud Computing Cloud Computing impels a fundamental shift in how data services are deployed and delivered, enabling flexible, dynamic outsourcing while reducing capital cost commitments for hardware and software. However, cloud computing also deprives customers of direct control over the systems that manage their data, raising security and privacy concerns. This research develops the means to provide secure and privacy-assured data service outsourcing that is usable, scalable, and meets performance goals, through: (1) an encrypted cloud data service, including fuzzy and ranked keyword search with strong privacy-assurance for large numbers of users and files, (2) scalable and owner-controlled cloud data sharing service with fine-grained data access enforcement and scalability, and (3) privacy-preserving secure cloud storage auditing that maintains strong guarantees of storage correctness.<br\/>Enabling secure and privacy-assured data service outsourcing is fundamental to the success of cloud computing deployment. This project also develops curricula and teaches and supervises students. Materials of this project will be made available online as tutorials, software packages, and publications of general interest.","title":"CAREER: Secure and Privacy-assured Data Service Outsourcing in Cloud Computing","awardID":"1262277","effectiveDate":"2012-08-13","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["565164"],"PO":["543481"]},"200867":{"abstract":"State dependent resource allocation is critical for improving the network efficiency. Over the past two decades, remarkable progress has been made on the design of wireless networks with maximum throughput and low latency by using state dependent resource allocation algorithms. However, most of these works assume the network is fully observable and the network state information is perfectly known. This assumption is becoming increasingly questionable because of the multi-carrier technology, which makes it extremely expensive to obtain the complete network state information, and transmission delays and measurement errors, which make it impossible to know the exact network state. With wireless networks become pervasive in our daily life, new theories and algorithms are needed for partially observable wireless networks.<br\/><br\/>In this project, the PI models a partially observable wireless network as a partially observable Markov process, and then applies the framework of Markov decision processes (MDP). While important structure properties may be discovered using the MDP framework, finding optimal solutions in general is an NP-hard problem. To overcome this difficulty, this project uses drift-based competitive analysis and drift-based large-deviations analysis for quantifying fundamental limits and deriving optimal or near optimal algorithms. The project is expected to lead to breakthroughs in managing partially observable wireless networks. Fundamental limits of partially observable wireless networks, novel resource allocation algorithms with provable throughput and latency guarantees, and implementations on a real-world test bed will have a significant impact on the design and implementation of future wireless networks.","title":"NeTS: Small: Control of Partially Observable Wireless Networks: Fundamental Limits, Optimal Algorithms and Practical Implementation","awardID":"1262329","effectiveDate":"2012-08-16","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["539541"],"PO":["557315"]},"193530":{"abstract":"Over the past decade, energy efficiency emerged as a first-order design challenge to developers across all layers of the computing stack, from microprocessor architects to large-scale datacenter operators. Unfortunately, in the future, operating a system close to its efficient design point for power will make the system susceptible to unreliability, whereas allowing margins will make the system inefficient. Therefore, understanding the interplay between power, performance and reliability is the next essential step for building sustainable computing systems in the future.<br\/><br\/>To this end, research under this proposal investigates new techniques to build computing systems that achieve both high energy efficiency and high reliability, but at a low cost to enable broader adoption of the techniques by our society. The investigators propose a resilient hardware\/software co-designed machine organization that eliminates inefficiencies that arise from circuit- and architectural-level techniques that focus only on energy efficiency. The investigators' system optimizes reliability, energy, and performance in a coordinated manner. In this system, software continuously monitors execution and it dynamically adapts hardware resources based on feedback. The system automatically makes calculated efforts to characterize operational inefficiencies, and attempts to eliminate the inefficiencies by carefully relaxing the robustness of the system without compromising correctness. To characterize these inefficiencies and train the system, the investigators study and develop various algorithms, tools, and methodologies across the hardware and software boundaries.","title":"SHF: Small: Cross-Layer Solutions for Sustainable and Reliable Computing Systems","awardID":"1218474","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["556800","535200"],"PO":["366560"]},"193651":{"abstract":"There are two approaches to preventing localized bottlenecks in datacenter networks: 1) building an over-provisioned network capable of supporting worse-case dataflows, or 2) providing rapid (microseconds to millisecond) on-demand provisioning of links, usually optical, between communication hotspots in the datacenter. This projects studies and prototypes the second approach via a novel interconnect design for datacenter networks (DCNs) with good fault-tolerance and scalability features, termed WaveCube. In particular, optical wavelength selective switch (WSS) technology is used to achieve cost savings while implementing multipathing and dynamic bandwidth scheduling for improved performance. The WaveCube topology is a multi-dimension cube with fiber carrying multiple wavelengths (Wavelength Division Multiplexing or WDM). WDM dramatically cuts down on the number of fibers needed in a datacenter but gives rise to a wavelength assignment problem. Thus the project will study wavelength assignment algorithms needed to provide connections via multiple WSSs taking into account fault-tolerance and performance. The project will compare WaveCube wiring complexity to that of other suggested approaches. The project will also study the performance of WaveCube-based data centers under a variety of processing loads, some derived from actual measurements and some synthetic.<br\/><br\/>Broader Impact:<br\/>The PI will create undergraduate datacenter networking courses. The PI also describes a plan to increase participation of underrepresented minorities and women in the proposed research activities and had previous demonstrations of such outreach via work with programs such as AGEP, REU, CRA CREW. The PI's are collaborating with an industrial organization.","title":"NeTS: Small: WaveCube: A Scalable, Fault-Tolerant, High-Performance Optical Data Center Architecture","awardID":"1219116","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["477110"],"PO":["564993"]},"193563":{"abstract":"The exponential growth of mobile data is a major challenge to the operators of cellular networks. Looking beyond conventional capacity-improving approaches such as adding more cells and acquiring more spectrum, this project seeks to fundamentally improve the spectral efficiency of cellular data networks. The project investigates LAWN (Large number of Antenna based Wireless Networking), a radically new cellular network architecture, in which a large number of antennas simultaneously serve a relatively much smaller number of wireless terminals using multiuser beamforming. The project has two inter-related thrusts. The first investigates a novel LAWN base station design and prototype that can cost-effectively scale to hundreds of antennas and exploit physical-layer tradeoffs between computational complexity and network capacity. The second thrust studies the resulting new network architecture that efficiently schedules terminals, intelligently allocates transmission power, and coordinates pilot signal transmissions to mitigate inter-cell interference.<br\/><br\/>The project targets improving the spectral and power efficiency of cellular networks by many fold, leading to not only fast wireless data networks but also longer battery lifetime of mobile terminals. Results from the project are likely to provide fresh insights for new theoretical development, bringing large-scale multi-user beamforming one significant leap closer to practical deployment in cellular data networks. In addition to academic publications, the project will produce an open platform, including hardware, software, and documentation available on-line, for teaching and researching base station design. It will actively involve undergraduate students as well as students from under-represented populations.","title":"NeTS: Small: Collaborative Research: LAWN: Scaling Up Cellular Data Networks Using a Large Number of Antennas","awardID":"1218668","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[518400],"PO":["565303"]},"193332":{"abstract":"The goal of this project is to develop solutions to reduce congestion problem in urban area. Each vehicle will be equipped with GPS and dedicated short-range communications (DSRC) capability in the near future. In congested urban areas, Road Side Units (RSUs) and Relay Nodes (RNs) are deployed. Each vehicle entering these areas will report its trajectory (i.e., a selective path to a destination) based on GPS to a Control Center via an RSU. RSUs can monitor the vehicles passing by and assist the data delivery between Control Center and vehicle or vehicle to RSUs. The Control Center will be responsible for deciding how to smooth the traffic based on the current traffic load by adjusting the trajectories of some vehicles or sending warning message to vehicles in certain areas. This project is to investigate the feasibility and three associated research issues of such an infrastructure. The research issues to be addressed are 1) How to cost effectively deploy the minimum numbers of RSUs and RNs in a given roadmap to guarantee the delay and performance of data dissemination in such an infrastructure? 2) How to effectively deliver data from vehicles to RSU and from RSU to vehicles? 3) How to disseminate traffic related warning messages to vehicles that may be affected by a given traffic incidence in a timely and effective manner? This project is expected to advance the understanding of vehicle-to-vehicle and vehicle-to-infrastructure communications to a higher level. Moreover, this project has a substantial impact to the society. Not only can the commute of each person be shortened, but also the total energy saving and its associated environmental impact can be huge.","title":"NeTS: Small: Information Dissemination in Vehicular Networks for Reducing Traffic Congestion","awardID":"1217572","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["543507"],"PO":["557315"]},"193453":{"abstract":"In non-cooperative resource sharing and exchange systems, users compete for available resources aiming to optimize their individual objectives. Operation by self-interested users often results in suboptimal performance from a system designer's point of view. This research involves the development of a new incentive design framework for resource sharing and exchange systems where the designer can perfectly or imperfectly monitor the actions of self-interested users and intervene in their interaction. <br\/><br\/>The investigators systematically study what the designer can achieve in terms of improving the performance of various non-cooperative networks and systems by designing suitable protocols and how it can achieve these improvements, given its abilities to monitor the users and its capability to intervene. This research formulates and solves the designer's problem of finding an optimal intervention rule that optimizes the designer's objective while explicitly considering the selfish nature of users. The results obtained characterize the extent to which the designer can shape the incentives of users depending on its ability to monitor the actions of users and to impact the payoffs of users.<br\/><br\/>The investigators systematically study both multi-user interactions scenarios where the users interact sporadically, which will be modeled as one-shot games with intervention, as well as the case where users establish long-term relationships, which will be modeled as repeated games with intervention. In both scenarios, the cases of perfect and imperfect monitoring of the users? actions will be investigated. Overall, this research provides a novel framework to evaluate the performance gain from having a designer that has a monitoring and intervention ability, and to figure out the best way to utilize its capabilities.","title":"CIF: Small: Intervention: A Design Framework for Resource Sharing and Exchanges Among Self-interested Users","awardID":"1218136","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["558132"],"PO":["564898"]},"193365":{"abstract":"The proliferation of wireless technology constitutes one of the most remarkable success stories in the history of human intervention. This growth however raises the question of whether technology will capitulate to its own success owing to the scarcity of spectrum. Fortunately the problem is more artificial than fundamental: the scarcity has been precipitated by the exclusive licensing model, while large swaths of licensed spectrum remain underutilized. A conjecture therefore is that unlicensed access of idle (but licensed) spectrum, commonly referred to as secondary spectrum access, can help avert the impending crisis. This however can only be realized if secondary access is profitable for the license holders. This is the main motivation behind this research, which seeks to design incentives to induce license holders to allow secondary access. Specifically, the research studies a free secondary market where the licensed and unlicensed users trade access rights based on bilateral agreements, and seek to optimize their interaction strategies. The resulting decision problems are analyzed using tools from game theory, optimization, graph theory and stochastic control. <br\/><br\/>The profit perspective adopted in this research incentivizes greater participation of wireless carriers in the secondary markets, enabling a quantum leap in more efficient spectrum utilization. Students at both undergraduate and graduate levels are trained in subjects from wireless technology to game theory and optimization theory. This research will generate algorithms for computing the decision strategies of the license holders. They will be evaluated through rigorous analysis and extensive simulations. Results will be disseminated through conferences and journals.","title":"NetS: Small: Collaborative Research: Playing the Devil's Advocate: The Profit Perspective in Secondary Spectrum Markets","awardID":"1217730","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[517945],"PO":["557315"]},"193376":{"abstract":"This project endeavors to answer the fundamental research question whether it is possible to alter the appearance of specific reflectance properties in a photograph of a scene by only partially characterizing the appearance of the scene via measurements. Specifically, the research team investigates novel appearance editing techniques that minimize the modeling effort, while maintaining maximum editing flexibility and at the same time impose minimal restrictions on the scene, while guaranteeing a physically plausible editing result. A common application of appearance modeling is to digitally clone a real-world scene, then change some scene properties, and finally revisualize the scene. Despite the enormous advances in appearance modeling, it still requires significant effort and expertise to create a digital clone of a physical scene. The effort needed to make the desired alterations to the scene often pales in comparison to the required appearance modeling effort. This project focuses on three reflectance phenomena suited for measurement-based editing: translucency, surface albedo, and surface reflectance.<br\/><br\/>This research program has a far-reaching impact beyond computer graphics. It has the potential to impact product development and pre-visualization, virtual reconstruction of archaeological artifacts, forensics, virtual cosmetics, fine-arts and entertainment, and other applications that require the visualization of an object\/subject with altered reflectance conditions. Besides supporting the traditional research roles of graduate students, undergraduate education is enhanced by hands-on research experience and the incorporation of research results into existing courses.","title":"CGV: Small: Measurement-based Editing of Reflectance Properties in Photographs","awardID":"1217765","effectiveDate":"2012-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["562850"],"PO":["564316"]},"193266":{"abstract":"This project investigates a fundamental and critical, but largely unexplored issue: automatically identifying visual contexts and discovering visual patterns. Many contemporary approaches that attempt to divide and conquer the video scenes by analyzing the visual objects separately are largely confronted. Exploring visual context has shown its promise for video scene understanding. Discovering visual contexts is a challenging task, due to the content uncertainty in visual data, structure uncertainty in visual contexts, and semantic uncertainty in visual patterns. The goal of this project is to lay the foundation of contextual mining and learning for video scene understanding, by pursuing innovative approaches to discovering collocation visual patterns, to empowering contextual matching of visual patterns, and to facilitating contextual modeling for visual recognition. The research team develops a unified approach to mining visual collocation patterns and learning visual contexts, and to provide methods and tools that facilitate contextual matching and modeling. <br\/><br\/>This research significantly advances video scene modeling and understanding, and produces an important enabling technology for a wide range of applications including image\/video management and search, intelligent surveillance and security, human-computer interaction, social networks, etc. This research program contributes to education through curriculum development, student involvements, and workshops and tutorials outside the vision community. This project also outreaches to K-12 education, and it provides datasets and software on its website to the community.","title":"RI: Small: Mining and Learning Visual Contexts for Video Scene Understanding","awardID":"1217302","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["182691"],"PO":["564316"]},"193035":{"abstract":"Advances in semiconductor technology in recent years allow billions of transistors to be integrated on the same chip, which makes power consumption the most critical design constraint. While the prevailing clocked synchronous logic has started to face more and more challenges as the transistor feature size continues to shrink, this research is to develop an innovative asynchronous digital processor design methodology and platform for energy efficiency, which provide a balanced power-speed tradeoff to deliver \"just right\" performance under all circumstances across a large variety of applications. <br\/><br\/>In our novel design methodology, given information for an application scenario and design constraints, appropriate asynchronous processing elements are selected and the number of cores in a parallel architecture is determined. This throughput analysis step is optimized by including a wide range of processing elements and by employing a fine-grained characterization process. During processor operation, the input data rate and the system utilization are monitored and compared with user preference to determine how many cores should remain active and also to adjust the supply voltage accordingly. A history-based forecasting mechanism is implemented for accurate yet efficient workload prediction. <br\/><br\/>Centralized and distributed voltage architectures for dynamic voltage scaling will be compared across wide dynamic ranges, along with the evaluation of various low voltage gate designs in facilitating extreme voltage scaling for power reduction. A series of prototype circuits will be designed, laid out, and simulated under various constraints and user settings. The results will be summarized and analyzed for performance evaluation and further optimization in the design methodology. Outcomes of this research have the potential to impact the battery-powered mobile computing and communication device industry, by enabling products that offer longer battery life, enhanced reliability, and reduced cost.","title":"SHF: Small: ADAPT: an Adaptive Delay-insensitive Asynchronous PlaTform for energy efficiency across wide dynamic ranges","awardID":"1216382","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}}],"PIcoPI":["525965"],"PO":["366560"]},"193156":{"abstract":"In a heterogeneous wireless network, interference has become the dominant performance limiting factor due to frequency sharing between the second tier low powered micro\/pico\/femto base stations and the high powered macro base stations. This project deals with several theoretical and practical aspects of interference management for the heterogeneous wireless networks.<br\/><br\/>The proposed research will first identify the classes of interference management problems that are computationally intractable, and then show how well they can be approximately solved with a complexity that is polynomial in the problem size and the solution accuracy. Special emphasis is given to the distributed implementation of the proposed algorithms. A unified co-tier and cross-tier approach will be developed for joint user admission, user-base station association, power control, user grouping as well as transceiver design. Central to this study is to jointly optimize user-base station association, transmit\/receive beamformers, user scheduling as well as partial joint transmission among a subset of base stations. The proposed approach will transform the interference management problem into an equivalent higher dimensional optimization problem and explore efficient update of iterates that can be decomposed across base stations, users and frequency slots.<br\/><br\/>Throughout this research, advanced optimization techniques will be the key to the development of various interference management algorithms. The methodologies developed in this research will provide effective means to mitigate interference in heterogeneous networks and significantly improve the network throughput as well as the users quality of service.","title":"CIF: Small: A Cross-Tier Approach to Interference Management in Wireless Heterogeneous Networks","awardID":"1216858","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":[517430],"PO":["564924"]},"198986":{"abstract":"This award will help to support student attendance at the 53rd IEEE Annual Symposium on the Foundations of Computer Science (FOCS) in New Brunswick, New Jersey, on October 19 through October 23, 2012, as well as to support attendance by some qualified postdoctoral fellows who do not have other sources of travel funding. FOCS and its sister conference, the ACM STOC meeting, are the premier broad-based conferences on the Theory of Computing. FOCS has a record of strongly encouraging participation by students, for whom the conference serves as a valuable educational experience, both for the technical content of the talks and for the opportunities for networking that it provides. In recent years, the annual FOCS conference has been attended by eighty to one hundred students who have comprised more than a third of all attendees. This award will provide partial support to twenty or more students, covering shared hotel rooms and travel. (Student registration will be supported by other means.)","title":"IEEE Symposium on Foundations of Computer Science (FOCS) 2012, New Brunswick, New Jersey Oct 19-23, 2012","awardID":"1252272","effectiveDate":"2012-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8052","name":"Inter Com Sci Econ Soc S (ICE)"}}],"PIcoPI":[533831],"PO":["565251"]},"197326":{"abstract":"The NSF Secure and Trustworthy Cyberspace (SATC) program provides an opportunity for its principal investigators and the larger community of cyber security researchers to discuss the latest advances in cyber security, demonstrate technical accomplishments, and advise NSF on promising future research directions. This proposal defines a technical program agenda that highlights the contributions of the participating NSF directorates, and addresses the research themes, established in the OSTP Strategy for the Federal Cybersecurity Research and Development Program, especially including accelerating transition to practice, developing scientific foundations, and strengthening education and training in cyber security.","title":"Stimulating SaTC Research Through PI Meeting","awardID":"1243386","effectiveDate":"2012-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[529428,529429],"PO":["565239"]},"198569":{"abstract":"This research aims to bridge the gap between the current reality and the potential for database system deployments on large clusters of servers in a data center or large numbers of virtual machines in the cloud. There does not exist a scalable, elastic, ACID-compliant database system implementation today. In general, applications that require elastic scalability are forced to program around the lack of ACID guarantees of the database system, and many applications are too complicated to be rewritten to work around these issues. The goal of this project is to overcome these issues using the following approaches: (1) Implementing a database system using an innovative deterministic architecture that guarantees that nondeterministic processing events will not affect database state, (2) Leveraging this new architecture to avoid \"commit protocols\" for distributed transactions in a cluster, (3) Designing a scalable preprocessor for the deterministic database that collects, analyzes, and dispatches transactions to the database cluster in order to further improve scalability, and (4) Developing a new lazy transaction evaluation approach in order to spread out load and avoid damaging effects of database load spikes. Overall, this research enables thousands of applications written for many different use-cases (such as e-commerce, telecommunications, and online auctions) to achieve scalability \"for free\" without having to rewrite the application code. This research involves both Ph.D. students and undergraduates, with significant outreach efforts to encourage undergraduates to get involved in research. Open source code, publications, and technical reports from this research will be disseminated via the project web site http:\/\/db.cs.yale.edu\/determinism\/.","title":"EAGER: Scaling the Preprocessor and Making it More Intelligent in Deterministic Database Systems","awardID":"1249722","effectiveDate":"2012-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[532784],"PO":["563727"]},"196149":{"abstract":"This award will support student travel to the ACM SIGPLAN International Conference on Functional Programming (ICFP 2012) and associated workshops. The conference is being held in Copenhagen, Denmark. ICFP is the premier conference on all topics related to functional and higher-order programming. Supporting student travel to attend professional conferences and workshops is a very important mission of the NSF. Broader impacts include building the next generation of researchers in this research area, as well as providing international experiences to build a globally-aware workforce.","title":"Support for Student Attendance at ICFP 2012","awardID":"1237499","effectiveDate":"2012-08-01","expirationDate":"2013-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["563565"],"PO":["564588"]},"193410":{"abstract":"The conventional software currently used to handle input in nearly all modern graphical user interfaces (GUIs) is effective and highly evolved. This has the advantages of promoting reuse rather than reinvention of interaction techniques, and making it easy to create GUis, even for those with limited programming ability. However, these successful software abstractions assume the inputs reported to the system accurately reflect the actions of the user - that input is certain rather than uncertain. Unfortunately, this does not hold for some of the most interesting new input technologies including naturalistic inputs such as free space gestures (e.g., as sensed by the Kinect depth camera), pen input (including handwriting, gestures, and free hand drawing), touch input, sensors for context, and voice input. Some of these new technologies contain inherent uncertainty, such as when a finger touch area (that the user cannot see) is much larger than the pixels of a display. Others make use of recognizers for input and typically produce estimates of what might have occurred. Since conventional methods of input handling have no way to manage uncertainty in input, many of them force uncertainty to be resolved before input processing even starts. For example, the location of input from a touch screen may be represented as certain using a single point (its centroid). But when uncertainty information is thrown away, interfaces can quickly become brittle; small recognition errors can derail the interaction and destroy the user experience. As a result, these new and very promising forms of input have often proven difficult to use to their full potential. The PI's goal in this project is to overcome this problem by creating a redesigned input-handling infrastructure, which will robustly model, and make use of, inputs with uncertainty. It will do this by treating all input, and all UI actions stemming from that input, on a probabilistic basis, entertaining multiple possible interpretations of input (and all its consequences over time), along with estimates of the likelihood of each interpretation. As a result, when decisions need to be made and irreversible actions undertaken, systems will have a sound basis for choosing among interpretations. Rather than starting with completely new input concepts, the PI's approach is to extend conventional input abstractions with support for uncertainty. Normally, a single certain input event is dispatched to a single interactor, which interprets its meaning to track its own interactive state and eventually request actions. Now, each of these parts of the input process will be done probabilistically. An estimated probability distribution will be tracked over input alternatives that might have occurred, interactors which might have received that input, states that interactors might be in, and actions that interactors might request as a result. These probability distributions can then be used to make informed decisions about when, whether, and which actions to actually undertake. To hide the complexity of maintaining each of these distributions over time from the UI programmer, the PI will employ a Monte Carlo representation of a probability distribution (i.e., a weighted set of samples each indicating the probability of one definite value). Crucially, this representation will allow the code to simply execute traditional (certain) input processing steps multiple times - once for each sample in the relevant probability distribution(s). This hides nearly all the complexity associated with uncertainty, and allows programmers to use their current conceptual models, and even code nearly identical to their current practices, for most aspects of input handling.<br\/><br\/>Broader Impacts: Project outcomes will radically change the ease with which readily available new input technologies can be incorporated into interactive systems, and thus will have wide impact in expanding our ability to build and deploy interfaces with new forms of input. As part of this research, the PI will develop working solutions for both graphical user interfaces and context-aware applications. He will also create and widely distribute a full teaching toolkit which embodies these concepts (where the term \"teaching\" is used in the same spirit that Pascal was a teaching programming language - it used good concepts and the best practices of the time; it was conceptually clean, yet suitable for real work). This teaching toolkit will be integrated into educational activities at the PI's institution, and curricular modules will be developed which should allow this to be carried to other universities.","title":"HCC: Small: New Infrastructure Concepts for Robust Handling of Inputs with Uncertainty","awardID":"1217929","effectiveDate":"2012-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[518043,518044],"PO":["565227"]},"193454":{"abstract":"Accurate details in 3D structures of RNA molecules are important for understanding RNA function, which can in turn help us understand biological systems, develop new medicines, and improve human health. One issue in RNA 3D structure analysis is that the structures obtained from biological experiments often contain errors and need to be corrected. The main reason for the errors is because RNA 3D structures are highly complex. While there are existing automatic tools for obtaining protein 3D structures from experimental data, such tools are not yet available for RNAs.<br\/><br\/>Previously the PI developed a program called RNA Backbone Correction (RNABC) that uses geometric algorithms and robotics to search for error-free RNA structures. While RNABC has been used to correct structural errors in existing RNA structures and has been integrated into the MolProbity web service for RNA structure validation, further advancement is needed. Research shows that RNABC corrects errors in 40% to 80% of RNA structures tested.<br\/><br\/>This project develops a new and improved computer program that scientists can use to correct structural errors and obtain accurate details of RNA 3D structures. The goal is to correct errors in over 80% of RNA structures and maintain the same running time. The new program will combine methods in machine learning, data mining, robotics and numerical analysis to search for error-free RNA structures. The project will advance algorithmic research in multiple ways and help us better understand the details of RNA structures. This project provides research opportunities to students interested in computer science, mathematics, and biology, and helps educate the next generation of scientists.","title":"AF: Small: RUI: A new and improved algorithm for fitting RNA backbone in crystallographic data","awardID":"1218145","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["542594"],"PO":["565223"]},"193465":{"abstract":"This project addresses the reduction of energy consumption in optical and wireless access networks via a series of approaches such as capacity-adaptive network design, and energy-efficient resource allocation and traffic scheduling. The capacity-adaptive access network stays in the 'high-capacity high-power' mode when the network is heavily loaded with bandwidth-hungry applications such as video streaming, and switches into the 'low-capacity low-power' mode when the network is lightly loaded with less bandwidth demanding applications such as voice and future smart grid traffic. The project generalizes the approaches currently being used in data centers to communication links, and focuses on formalizing the problem and obtaining optimal or near-optimal solutions. It will not only improve energy efficiency of current access networks, but will also provide insights and guidelines on how to upgrade the capacity of access networks efficiently.<br\/><br\/>The concept of designing capacity-adaptive access networks introduces new requirements of 'capacity adaptive' on the design of access networks as opposed to simply provisioning for peak utilization. This project's energy-aware network control, resource allocation, and traffic scheduling schemes consider an additional key performance metric, energy consumption, in addition to network quality of service, while conventional schemes merely consider network performance such as delay and loss. Research results, via theoretical analysis, simulations, and testbed experiments, will not only demonstrate improved energy efficiency of access networks, but will also be potentially tailored for core networks, datacenter networks, and other wireless systems.<br\/><br\/>Broader Impact:<br\/>Reducing the energy consumption of access equipment will not only have positive environment impacts during normal operation but it will provide public benefit during emergencies. Remote access equipment is currently powered by a combination of public power, battery backup, and varying degrees of local generation. Techniques such as those being developed by this project that extend operation when on emergency backup help protect the public health and safety. The project will integrate research and education through participation of both undergraduate and graduate students in the project and by incorporation of research outcomes into undergraduate and graduate course work. The PI will utilize the New Jersey Educational Opportunity Program (EOP), which provides educational opportunities to under-represented populations, as part of the process for recruiting students into the research program.","title":"NeTS: Small: GATE: Greening At The Edges","awardID":"1218181","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["550852"],"PO":["564993"]},"193476":{"abstract":"Many deaf and hard of hearing students use real-time captioning to participate in education. Generally, real-time captions are provided by skilled professional captionists (stenographers) who use specialized keyboards or software to keep up with natural speaking rates of up to 225 words per minute. But professional captionists are expensive and must be arranged in advance in blocks of at least an hour. Automatic speech recognition (ASR) is improving, but still experiences high error rates in real classrooms. In this collaborative effort involving the University of Rochester and Rochester Institute of Technology, the PIs will address these issues by blending human- and machine-powered captioning to produce captions on demand, in real time, for low cost. The PIs' approach is for multiple non-experts and ASR to collectively caption speech in under 5 seconds, with the help of interfaces which encourage quick, incomplete captioning of live audio. Because non-experts cannot keep up with natural speaking rates, new algorithms will merge incomplete captions in real time. (While the sequence alignment problem can be solved exactly with dynamic programming, existing approaches are too slow, are not robust to input error, and do not incorporate natural language semantics.) Systematically varying audio saliency will encourage complete coverage of speech. Non-expert captions will train ASR engines in real time, so that ASR may improve during a lecture. (Traditional approaches for ASR training assume that training occurs offline.) The quikCaption mobile application will embody these ideas and will be iteratively designed with deaf and hard of hearing students at the National Technical Institute of the Deaf (NTID) via design sessions, lab studies and in-class deployments. Non-expert captionists can be drawn from broad sources: volunteers willing to donate their time, classmates with relevant domain knowledge, or always-available paid workers. They may be local (in the classroom) or remote. Captionists may have experience from prior quikCaption sessions, or novice crowd workers recruited on demand from existing marketplaces (e.g., Mechanical Turk). A flexible worker pool will allow real-time captions to be available on demand at low cost and for only as long as needed.<br\/><br\/>Broader Impacts: This research will dramatically improve education for deaf and hard of hearing students by enabling access to serendipitous opportunities, such as conversations after class or last-minute guest lectures for which no interpreter or captionist was arranged. Real-time captioning will also be useful in other settings such as school programs, artistic performances, and political events. Older hard of hearing adults usually prefer captioning, and represent a sizable and growing population; hearing people may benefit because captioning is a first step in automatic translation of aural speech. The algorithms developed as part of this project for real-time merging of incomplete natural language will likely be adaptable for other applications such as collaborative translation or communication over noisy mediums.","title":"HCC: Small: Collaborative Research: Real-Time Captioning by Groups of Non-Experts for Deaf and Hard of Hearing Students","awardID":"1218209","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[518197,"557943"],"PO":["565227"]},"193377":{"abstract":"Self-assembly is a process by which simple objects connect with each other to form complex structures under very little external control. Given that self-assembly is very common in nature, it is almost certain that self-assembly technologies will ultimately permit precise and efficient fabrications of nanostructures. There are many kinds of self-assembly. This project chooses to focus on algorithmic DNA self-assembly with the goal of understanding self-assembly in general from programming (i.e., algorithmic) and mathematical perspectives.<br\/><br\/>Algorithmic DNA self-assembly takes advantage of the facts that the four bases of DNA (i.e., A, C, G, T) can be used to encode information and that A binds with T and C binds with G. Small molecules consisting of multiple DNA strands (i.e., more than double strands) have been designed to act as four-sided building blocks (which are called tiles) for algorithmic DNA self-assembly. Experimental work has demonstrated that these building blocks can effectively perform computation as well as assemble crystals. Some key aspects of the self-assembly process of such building blocks have already been used to formulate a fundamental mathematical model called the abstract tile assembly model. This model extends a mathematical theory of two-dimensional tilling by adding a natural mechanism to grow a structure formed by tiles. The model consists of a set of square tiles. The four sides of a tile are each associated with a glue (which is implemented as a DNA strand). A special tile in the tile set is designated as the initial seed structure. Self-assembly proceeds by starting with the initial seed structure and then gluing copies of tiles from the tile set one by one to the growing seed structure whenever the total glue binding strength between a tile and the seed structure is no less than a threshold (which is implemented as the temperature in the tube).<br\/><br\/>Under the abstract tile assembly model, algorithmic DNA self-assembly is both a form of nanotechnology and a model of computation. As a computational model, algorithmic DNA self-assembly first encodes a computer program and an input data set for a given computational problem into the glues of DNA tiles. The tiles then bind with each other through DNA complementarity to execute the program on the input data set to produce a DNA nanostructure, which in turn encodes the desired output of the computational problem. As a nanotechnology, the goal of algorithmic DNA self-assembly is to design glues to program a set of tiles to assemble into the desired nanostructure. <br\/><br\/>The project will investigate interconnected research directions in algorithmic DNA self-assembly to explore new ways (1) to minimize the cost of manufacturing the glues and tiles used to assemble a structure, (2) to minimize the amount of time needed to assemble a structure as well as the amount of defects in the assembled structure, and (3) to impose desirable structural properties on the assembly process as well as on the assembled structure. A common theme across these directions is to automate the design of DNA self-assembly systems. <br\/><br\/>The project will continue the efforts of the Principle Investigator (PI) to introduce this emerging interdisciplinary field to the theoretical computer science community by formulating research problems of interest to that community. With this project, the PI will continue to recruit members of under-represented groups into this field in particular and into computer science in general. The PI has introduced and taught a course in this field in the past two years at the level of advanced undergraduate students and first-year graduate students. This course attracted two undergraduate students from outside computer science to decide to apply to PhD programs in this field and a postdoctoral fellow in Chemistry to collaborate with the PI and undergraduate summer research students. With this project, the PI will continue to teach this course on a regular basis to attract students and researchers into this emerging interdisciplinary computer science field.","title":"AF: Small: Combinatorial Algorithms and Computational Complexity for DNA Self-Assembly","awardID":"1217770","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[517969],"PO":["565223"]},"204619":{"abstract":"Natural emergency and disaster scenarios are unfortunate occurrences with far reaching after effects. In addition to human casualties, natural calamities can destroy the power grid, telephone networks, and mobile phone towers. The result being, there is no alternative for disseminating critical updates such as disease and safety alerts, locations, and directions to survivors. The above problem is an outcome of the lack of network systems that are designed to survive and serve after natural disasters. In order to remedy this problem, this project is designing, implementing, and deploying a solar powered self-sustainable emergency mesh to provide critical communication infrastructure to survivors and rescue personnel. <br\/><br\/>This work employs a mesh architecture that is energy-efficient and self-sustainable. It relies on renewable energy sources such as solar to provide near-perpetual lifetime to mesh nodes while serving critical updates to survivors. Since renewable energy scavenging is notoriously unpredictable, this project uses a clean-slate low power hardware and software systems design for the nodes. Additionally, in the event of node failures due to variability inherent in renewable energy scavenging and extreme environmental conditions during the aftermath of a natural calamity, the mesh automatically redistributes the data on the failed nodes to maintain sufficient redundancy. Finally, the mesh design uses common wireless technology such as Wi-Fi and light weight web-based services for compatibility with off-the-shelf laptops, mobile phones, and PDAs carried by disaster survivors. The culminating goal of this project is to save human lives. With the mesh infrastructure in place, human casualties during post-disaster times can be minimized. Additionally, during non-emergency scenarios, the system can be used to disseminate \\police alerts\" and \\disease alerts\" (e.g. swine-flu) that keep individuals well informed and cautious. In addition to a broad societal impact, this project, through educational courses provides hands-on experience to undergraduates and graduates in building mobile, embedded, and geospatial systems.","title":"CSR: Small: Self-sustainable Solar-powered Emergency Mesh Design","awardID":"1314024","effectiveDate":"2012-08-24","expirationDate":"2014-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[547994],"PO":["565255"]},"197755":{"abstract":"This award is to support student participation in workshops that are affiliated with the annual ACM symposium on Computer and Communication Security conference (CCS), which are to be during October 15-19th 2012, in Raleigh NC. <br\/><br\/>The annual ACM Computer and Communications Security Conference is a leading international forum for information security researchers, practitioners, developers, and users to explore cutting-edge ideas and results, and to exchange techniques, tools and experiences. Since 2001, CCS started accommodating affiliated workshops to explore security issues in a variety of emerging areas. The CCS workshops have become active forums for researchers to form focus groups, discuss and collaborate on emerging and critical security problems, and disseminate fresh, revolutionary (and sometimes even controversial) ideas. These workshops also serve as natural venues to bring together researchers from multiple disciplines to address security issues in specific domains, such as health care, cloud computing and national critical infrastructures. The workshop selection process is rigorous and well coordinated. All workshop proposals are carefully reviewed by the CCS steering committee and evaluated based on their topic, intellectual merits, and relevance to academia and industry.","title":"Workshop Organization Supplement: A Series of Workshops on Security in Emerging Areas at the 2012 ACM Conference on Computer and Communications Security","awardID":"1245825","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["534031"],"PO":["565327"]},"193278":{"abstract":"Energy and power consumption have become a critical issue ranging from microarchitectures to large-scale data centers and supercomputers. Conservative estimates suggest that the information technology industry world-wide energy consumption is in excess of 400 TWh and growing, generating roughly the same carbon footprint as the airline industry, accounting for 2% of global emissions. At the same time, the power constraints of chips hamper their performance, and the shrinking transistor geometries and low supply voltages increase the severity of processor variations resulting in higher timing error rates. High error rates lead to a significant drop in yield and increased manufacturing costs, calling for designs that are able to withstand them.<br\/><br\/>This project seeks to understand and explore the novel paradigm of elastic fidelity computing. Elastic fidelity computing capitalizes on the observation that many applications can naturally tolerate errors, and that not all of them need to run at 100% fidelity all the time. Specifically, the goal of this work is to understand the error models of various hardware components as they relate to data movement, storage, and computation, and simultaneously to understand the error resiliency of applications and re-architect them to leverage elastic fidelity.<br\/><br\/>Elastic fidelity offers potentially transformative effects for science and society, by challenging conventional wisdom and taking a fresh look at the interplay of errors, output quality and energy efficiency for an important class of pervasive streaming and data-intensive applications. More specifically, elastic fidelity promises significant energy savings that can put computing on an environmentally sustainable path, by lowering the operational costs in major economic sectors, and making the manufacturing of future chips cheaper by relaxing the accuracy requirements of hardware components. The results of this research will be disseminated through publications, workshops, advanced curriculum, and releases of the developed infrastructure in the public domain. To accelerate broad societal effects, the project participants will seek to foster technology transfer by promoting collaboration and industry involvement through presentations and site visit","title":"SHF:Small:Collabroative Research: Elastic Fidelity: Trading off Computational Accuracy for Energy Efficiency","awardID":"1217353","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["527873"],"PO":["366560"]},"195104":{"abstract":"Walking for a healthy adult seems easy. However, underlying this apparent simplicity our nervous system is performing a task of astounding complexity. Using sensory information about the body's movement, the nervous system coordinates the activation of dozens of muscles so that we stably and efficiently move through our environment. For example, if our nervous system senses that our foot will strike the ground too soon, it will adjust muscle activations so we do not stumble and fall. In this project, an interdisciplinary team of investigators aims to uncover the rules the nervous system uses to make such adjustments. Using a general computational and theoretical framework taken from engineering (used to understand, for example, the rhythmic control of the angle of attack of rotating helicopter blades), the method depends on gently perturbing a person's senses and body in various ways and observing how the nervous system adjusts muscle activations in response. The investigators will first test their methods on a simpler type of rhythmic movement, repetitive hitting of a virtual ball with a paddle, then extend the findings to coordination during walking. <br\/><br\/>By constructing a general approach to understanding the control of rhythmic movements, including swimming in fish, flying in insects and birds, and walking in people and robots, the investigators may provide a foundation for understanding how control breaks down for people with neurological conditions such as stroke and incomplete spinal cord injury. This has the potential to advance neuromuscular rehabilitation and the design of assistive devices. <br\/><br\/>[Co-funded by CISE and SBE]","title":"Collaborative Research: Understanding the Rules for Human Rhythmic Motor Coordination","awardID":"1230493","effectiveDate":"2012-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[522960],"PO":["563458"]},"193289":{"abstract":"The overarching goal of this project is to improve our understanding of the power of randomness in designing efficient algorithms, constructing graphs that behave like \"random\" graphs, and in proving certain mathematical results. One of the basic questions in randomness is that of obtaining \"pure\" random bits from real-world sources of randomness. Procedures developed for this purpose are called extractors and have found many applications, some of which are completely unrelated to their original motivation. A recent line of research (by the PI and others) has resulted in new techniques, especially algebraic, being introduced to this area. This resulted in new constructions of extractors that go beyond previous barriers. One of the main goals of this project is to further study these techniques and obtain better constructions of extractors producing more random bits of higher quality than known before.<br\/><br\/>Another central question studied in this project is Polynomial Identity Testing (PIT) - testing whether a given arithmetic expression is an identity or not. This can be done efficiently using randomness but a deterministic algorithm is not known. The importance of finding deterministic algorithms for this problem (or even to special cases of it) is of major importance and attracted a lot of attention (including work by the PI with co-authors). A third major goal of this project is to obtain deterministic PIT algorithms for classes of circuits for which only randomized algorithms are known. This is tightly related to proving new computational hardness results, being a yet another target for this project.<br\/><br\/>This project aims at expanding our understanding of randomness as a computational resource while developing new and transformative mathematical techniques and concepts for attacking long-standing problems in this area. Progress on could lead to new practically useful insights into algorithms, coding theory and cryptography among others. The PI will be involved in organizing seminars, reading groups and writing survey articles aimed at disseminating knowledge gained during the proposed research to the wider academic community. In addition, the PI will give public talks, including to high school students, and those aimed at a non-technical audience.","title":"AF: Small: Randomness in Computation - New Directions and Techniques","awardID":"1217416","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[517762],"PO":["565157"]},"198855":{"abstract":"Behavioral training interventions have received much interest as an efficient and cost effective way to maintain brain fitness or enhance skilled performance with impact ranging from health to job training. In particular, neuroscience and behavioral research has documented the importance of explicitly training attentional control, in order to enhance perceptual and cognitive fitness, physical exercise, to promote physical fitness and rehabilitation as well as kindness and compassion, to produce changes in adaptive emotion regulation and well-being. During this same time period, video game play has become pervasive throughout all layers of society providing a potentially unique vehicle to deliver such controlled training at home in a cost-efficient manner. Given the potential impact of these interactive systems, it is necessary to understand the underlying scientific principles that govern the development and deployment of interactive computer games in healthcare and education. The search for the deeper understanding requires the involvement of expertise from a wide variety of areas ranging from neuroscience to computer science and engineering. To this end, the workshop brought together experts from a wide range of academic disciplines with engineers and software developers. The goals of the workshop was to identify the remaining challenges and roadblocks that represent barriers in making these sophisticated interactive systems to be among the key components supporting the future healthcare delivery with emphasis on prevention and quality of life. This workshop provided a unique opportunity to define new approaches to the design, assessment and distribution of video games for enhancing well-being and attention, opening a unique path to addressing key health care and societal concerns in a user-friendly and cost effective manner","title":"Workshop: Enhancing Well-being and Attentional Control through Games and Interactive Media: A Neuroscientific Approach","awardID":"1251546","effectiveDate":"2012-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8018","name":"Smart Health & Wellbeing"}}],"PIcoPI":[533487,533488,"548160",533490],"PO":["564768"]},"193058":{"abstract":"A fundamental challenge in machine learning is the design of computational agents that, rather than being explicitly programmed, autonomously learn complex tasks in stochastic real-world environments. Past approaches, such as reinforcement learning algorithms for solving Markov decision processes, scale poorly to large state spaces. The proposed research addresses this curse of dimensionality by investigating a novel framework combining reinforcement learning and online convex optimization, in particular mirror descent and related algorithms. Mirror descent scales significantly better than classical first-order gradient descent in high-dimensional state spaces, by using a distance-generating function specific to a particular state space geometry.<br\/><br\/>The proposed framework enables several significant algorithmic advances in the design of autonomous machine learning agents: a new class of first-order mirror-descent based methods for learning sparse solutions to Markov decision processes will be developed that scale significantly significantly better than previous second-order methods; novel hierarchical methods for solving semi-Markov decision processes will be investigated; and finally, applications to a variety of high-dimensional Markov decision processes will be explored.<br\/><br\/>The anticipated outcomes of the proposed work include foundational advances in designing autonomous agents that learn to solve sequential decision-making problems, which will impact a large number of target applications from manufacturing to robotics and scheduling. The educational goal includes the development of a graduate-level course in online convex optimization for sequential decision-making, as well as interdisciplinary tutorials to enhance the cross-fertilization of ideas from applied mathematics and optimization to machine learning and artificial intelligence.","title":"RI: Small: Reinforcement Learning by Mirror Descent","awardID":"1216467","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["544475"],"PO":["562760"]},"193664":{"abstract":"The gap between the speed of processors and that of memory has been widening for decades and will continue to widen with current technology forecasts. This major obstacle to high-performance computing is traditional overcome using techniques that exploit data locality in memory, such as cache memories. However, a very important class of applications commonly referred to as irregular applications, do not exhibit any locality. Examples of such applications include: (1) sparse linear algebra, widely used in science, engineering, medicine, finances, economic modeling etc; and (2) graph algorithms, used in the modeling and analysis of large data such as social networks and the ab-initio construction of genomes from sequenced material etc. Hardware supported multithreaded execution has been shown to mask the latency to memory, and hence can boost the effective parallelism, by suspending a thread waiting for the result of a memory operation and resuming it when the results are available. By doing so the utilization of the computational units is raised to near 100% resulting in a tremendous speedup of the computation.<br\/><br\/>This research aims at generating customized hardware for multithreaded execution on configurable devices such as FPGAs. FPGAs (Field Programmable Gate Arrays) are integrated circuits on which arbitrary digital hardware circuits can be configured and reconfigured under software control. Toward this goal, CHAT (Customized Hardware Accelerated Threads) is being developed as a tool that generates a custom multithreaded FPGA processor design tailored for a particular application, based on the C programming language specification of the application. Preliminary results show a potential speedup greater than 10x over traditional memory hierarchy approaches for some irregular applications. The technical deliverables of this project will be: (1) an open-source distributed version of CHAT implemented on high-performance machines with FPGA accelerators; and (2) a detailed analysis of the performance benefits of various compile-time optimizations on various applications.","title":"SHF:Small: Automatic Generation of Hardware Threads on Programmable Fabrics","awardID":"1219180","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7941","name":"COMPUTER ARCHITECTURE"}}],"PIcoPI":[518640],"PO":["366560"]},"192454":{"abstract":"Future scientific and technological efforts to achieve better understanding of oceans and water-related applications will rely heavily on our ability to jointly consider communications, actuation and sensing in a unified system that includes instruments, vehicles, human operators and sensors of all types. The goals of this project are to design networking tools for mobile underwater networks, develop novel navigation mechanisms for communication-constrained autonomous underwater vehicles and to ultimately integrate sensing and classification to provide solutions for the exploration-exploitation tradeoff.<br\/><br\/>This project will lead to development of underwater communication methods with applications to science, security, and industry in the areas of environmental monitoring, aquatic eco-system analysis, ocean accident remediation, surveillance for defense applications, homeland security, oil and gas, aquaculture, geological and oceanographic science, and marine biology. It will also contribute to the training of new information technology professionals and scientists with expertise in interdisciplinary research spanning underwater networks, oceanography and computer science.","title":"NeTS: Large: Collaborative Research: Exploration and Exploitation in Actuated Communication Networks","awardID":"1212999","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[515773],"PO":["565303"]},"193675":{"abstract":"This research is concerned with the refractive elements of the eye and the errors or artifacts produced in the process of focusing light onto the retina. As the popularity of mobile hand-held devices continues to grow, a limiting factor that may emerge as an increasing impediment to their adoption among sizeable segments of the population is the prevalence of vision problems. Effective use of these devices is predicated upon reasonable visual performance by a user who must interact with a small area. This is particularly problematic for the population of older users, who face increasing incidence of vision ailments as they age. But even for younger people, there is evidence that the prevalence of myopia is increasing, especially in Asian populations. Furthermore, some visual impairments involve higher order optical aberrations (sometimes referred to as \"irregular astigmatism\"), which are impossible to correct with spectacle lenses.<br\/><br\/>In prior work, the PI developed Vision-Realistic Rendering (VRR) to simulate an individual's vision system from measuring his or her optical system. Given these same optical measurements for that individual, the PI's goal in the current project is to achieve vision correction algorithmically and digitally rather than optically; that is, given a user with refractive error corrected by spectacles or with high order optical aberrations, compute an individualized \"inverse blur\" transformation to be applied to a sharp image such that when the resulting transformed image is then viewed by this individual, the inverse blur is canceled by the optical aberrations of his or her vision and this blurred version of the image appears in sharp focus to this individual.<br\/><br\/>The problem with the inverse blurring process is that it tends to produce an image whose dynamic range is much larger than that of the original image (due to a weak frequency response in the blurring kernel, and division by weak response creates large values). There are usually many negative pixels and a bright spot in the pre-deconvolved image. Computation of the pre-deconvolved image involves using inverse-filtering or a spatial domain solver. However, the situation is fundamentally different from that of performing the image de-blurring as a post-process; since the blurring convolution is the final step, there is a loss of frequency information that cannot be recovered by adding prior knowledge. The PI's approach in this project is to address the large dynamic range of the pre-deconvolved image by using a high dynamic range display system, and the loss of frequency information by the concept of a multi-layered display that does not lose any frequency content even after the blurring.<br\/><br\/>Broader Impacts: Eyeglasses cannot correct higher order optical aberrations that arise in the vision system of many patients who have certain types of corneal pathologies or who experience side effects of corneal refractive surgeries (such as LASIK and PRK). If successful, this research will significantly impact vision correction technology by laying the foundations for a variety of new display algorithms and devices which transcend this limitation and provide vision correction for patients whose vision problems are related to either low or higher order optical aberrations. <br\/><br\/>The PI makes a concerted effort to involve undergraduate students and minorities in his research. He offers independent study for students at all levels, includes undergraduate students in research group meetings, and offers freshman and sophomore seminars on related topics. He works closely with the Black Graduate Engineering and Science Students and Latino Association of Graduate Students in Engineering and Science, and supervises students in the Summer Undergraduate Program in Engineering Research at Berkeley which brings underrepresented minorities from around the country to do research at Berkeley during the summer.","title":"HCC: Small: Individualized Inverse-Blurring and Aberration Compensated Displays for Personalized Vision Correction with Applications for Mobile Devices","awardID":"1219241","effectiveDate":"2012-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[518667],"PO":["565227"]},"193444":{"abstract":"Troubleshooting undesirable network events, such as poor connectivity or performance, is difficult at best. The high-speed link techniques that work on LANs, such as dumping packets and analyzing the detailed traffic, are impossible due to massive data volume. This project will explore mathematical techniques and network tools that will reduce the amount of data that has to be captured and stored while still allowing network operators to troubleshoot their networks. The project's objective is to extract from high-speed packet streams on individual network links an approximate and highly compressed representation of the link traffic that is orders of magnitude smaller in size than the raw traffic stream but which permits almost the same degree of troubleshooting as the raw data. The project will develop the algorithms and mathematical theory needed to design intelligent sampling algorithms for compressing network traffic. Specific areas to be studied for purposes of developing sampling techniques include identifying what constitutes the representative flows for troubleshooting purposes and investigating how to best encode and decode the sampled data, and how the samples can be gracefully shrunk over time so as to reclaim space for new data as they arrive.<br\/><br\/>Broader Impact:<br\/>The project will provide research experience for undergraduates. Undergraduates at Georgia Tech, Denison and other institutions will be recruited via undergraduate workshops and research symposiums. Additionally, the project will integrate education and research via inclusion of the research into courses. In addition to publishing in appropriate scientific venues, the PIs will expand the Wikipedia entries on topics related to data streaming algorithms as part of the process of disseminating general information about the topic area to the scientific community. In terms of commercial impact, the project will lead to better methods for the diagnosis of large-scale networks, thereby reducing the cost to maintain and operate them. As part of the transfer of research findings into commercial practice the PIs will collaborate with members of AT&T's Network Management and Engineering Department.","title":"NeTS: Small: Collaborative Research: Towards Principled Network Troubleshooting via Efficient Packet Stream Processing","awardID":"1218092","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["349186"],"PO":["564993"]},"204818":{"abstract":"Abstract for EAGER: Self-Assembly of Complex Systems <br\/>Intellectual Merit: <br\/>Self-assembly is a model for how individual components arrange themselves through local interactions to form organized structures. It originated as a model for construction of nanotechnology. The theory of complex systems describes many phenomena in nature, from human intelligence and evolving communities of organisms to human social networks, economies, and cultures. In these systems, complexity emerges in ways that is not immediately obvious from an understanding of the component parts and the relationships between them. In this project, self-assembly will be investigated as a mechanism for creation of complex systems. Self-assembly can be shown to be equivalent to other models of complex systems. In addition, it can be programmed like a computer. A goal of this project is to develop efficient ways to program self-assembly to produce interesting complex systems, which have practical applications. As a beginning to this, we have developed a mapping of self-assembly onto graphs that enables us to use an efficient algorithm to determine the system that is constructed. Thus, self-assembly should be able to generate complex systems and to provide efficient and realistic simulation of those types of systems. In the project, the self-assembly algorithms will be applied to automatic content generation for games, in which the self-assembly automatically creates situations and non-player characters with which players of the game interact. The conjecture is that this will provide more dynamic and realistic game environments, and moreover, will be an interesting test-bed for investigation of the relationship between self-assembly and complex systems.<br\/>Broader Impacts: <br\/>This research integrates ideas from chemistry, physics, biology, and computer science to relate self-assembly to complex systems, and to produce potentially transformative tools that will not only improve understanding of complex systems, but also form the basis for innovative complex systems in a variety of application domains. These include nanotechnology, artificial intelligence, art, literature, and computer games. There are many natural phenomena (i.e. human intelligence, living systems) for which traditional symbolic models of computation are only able to capture a part of their essential capabilities and characteristics. Human language is an example. This research conceivably could result in software that is able to produce target systems that capture some of the capability, adaptability, and complexity that is observed in nature. If successful, the project could result in a new paradigm for realistic and complex behavior through computer programs, and would potentially impact not only nanotechnology, but also applications that require automatic generation of realistic content. Moreover, our models of self-assembly can generate this content in tractable ways. In addition, under the direction of the investigator, graduate and undergraduate students will work together in a team on this project, and will be educated in the unique multidisciplinary approach that has been proposed.","title":"EAGER: Self-Assembly of Complex Systems","awardID":"1315129","effectiveDate":"2012-08-10","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7946","name":"BIO COMPUTING"}}],"PIcoPI":[548478],"PO":["565223"]},"193466":{"abstract":"This research involves novel techniques for capturing natural human manipulation actions in all of their complexity. Consider how we prepare food, pick up and use a common tool such as a wrench, or plant a flower. We would like to learn from human examples of such complex actions, so that we can portray convincing graphical renditions of human characters for applications ranging from education, training, and entertainment to exposition and scientific study. Broader impact includes use of databases and the capture tool developed as part of this research for development of dexterous robotic manipulation, design of prosthetics, and understanding of human grasping and manipulation. The visual nature and appeal of this research makes it especially suited for mentoring of undergraduates and women, as well as for outreach to spark interest in science for K-12 students.<br\/><br\/>Traditional and established approaches to motion capture, such as optical approaches, cannot handle the complex, detailed, high degree of freedom interactions that must be observed to capture dexterous manipulation actions. The alternative pursued in this research is simulation motion capture, where a user interacts with and guides a running simulation. Enabling technologies include (1) real-time simulation of a skeleton driven deformable hand in close interaction with simulated objects, (2) novel control algorithms and interface techniques to allow direct control of such simulations, (3) a new language for segmentation and control law development, based on user actions during the simulation, and (4) novel manipulation objective functions learned from direct observation of human actions. Benefits of simulation capture over current capture technologies include little or no postprocessing, precise representation of contact forces, fast trial and error, ability to make use of a variety of means for control, and ability to use a variety of input devices. The resulting capture system will be inexpensive and broady accessible.","title":"CGV: Small: Simulation Motion Capture of Dexterous Manipulation","awardID":"1218182","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["560865"],"PO":["565227"]},"194797":{"abstract":"Proposal #: 12-29081<br\/>PI(s): Somani, Arun K; Aluru, Srinivas; Fox, Rodney O; Gordon, Mark S; Takle, Eugene S<br\/>Institution: Iowa State University<br\/>Title: MRI: Acquisition of a HPC System for Data-Driven Discovery in Science and Engineering<br\/>Project Proposed:<br\/>This project, acquire an HPC instrument (HPC cluster with large storage and a fast Infiniband network), aims to support 17 projects from 8 departments in a broad range of computational disciplines, including bioscience, ecology, fluid dynamics, earth and atmospheric science, materials science, and energy systems. The inclusion of GPUs and emphasis on large-scale memory units constitutes the key novelty of the proposed instrument. The proposed research involves a mix of algorithm development for parallel architectures and computational modeling, while pursuing compelling applications in biological, material, energy and climate sciences, i.e.:<br\/>- Biosciences. Bioinformatics tools will be developed to focus on research such as error-correcting algorithms for next-gen sequencers, resequencing, genome assembly, genome-wide association, biological network interference analysis, and metabolomics.<br\/>- Multiscale methods for grand challenge problems. Methods that can address ?grand challenge? problems, such as simulation of atmospheric aerosol formation and design of new materials. Coarse-graining will be used, starting with high-level quantum mechanics methods that are computationally expensive and mapping the high-level potential onto a new potential is much simpler.<br\/>- Computational fluid dynamics modeling. The new HPC platform will enable cutting edge research in fluid mechanics and multiphase flows. Particle-resolved direct numerical simulations of multiphase flow with fluid and surface reactions will be first-of-its-kind simulations. Algorithmic developments will have broad applications in sprays, bubbly flows and device-scale simulations of gas-solid flow applications that employ quadrature-based moment methods to treat the solid phase.<br\/>- Coupled dynamics of land use change and regional climate extremes. The long-term goal is to integrate policy and climate projection models to capture dynamic coupling between policy-driven agricultural land use change and regional climate, including the novel climate and regional agricultural projection systems and simulations. <br\/>Broader Impacts: <br\/>The impact should be felt both regionally and nationally. At the national level, the instrument should initiate transformative advances in computational algorithms proposed and will be made available to the broader research community in the form of open-source codes. Simulations made possible by these algorithms will have broad national and societal impact ranging from climate change scenarios to wind power generation to plant biotechnology and improved animal breeding. At the regional level the proposed HPC cluster will greatly enrich the institution?s research infrastructure. Use of the instrument will be incorporated into advanced courses and time will be allocated to train undergraduates, graduate students, and postdoctoral fellows in computational modeling and algorithm development. The HPC cluster will make time available to primarily undergraduate institutions and, coupled with active recruitment plans, should help attract women, underrepresented minorities, and first generation college students, who might otherwise not be encouraged to attend the institution.","title":"MRI: Acquisition of a HPC System for Data-Driven Discovery in Science and Engineering","awardID":"1229081","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[521837,"526234","556512",521840,"565135"],"PO":["557609"]},"195534":{"abstract":"This award funds the travel of 15 US student to travel to the 21st ACM International Symposium on High Performance Distributed Computing (HPDC 2012). HPDC 2012 is to be held, along with nine workshops, at the campus of the Delft University of Technology in Delft, The Netherlands on June 18 through June 22, 2012. HPDC is the premier forum for presenting the latest research findings on the design and use of parallel and distributed systems for high end computing, collaboration, data analysis, and other innovative applications. <br\/><br\/>HPDC 2012 will build on the success of its predecessors, which date back to 1992, and which have typically drawn 150-250 participants from among the academic research, national lab, standards, vendor, and user communities in the field of high performance distributed computing. HPDC seeks to increase student participation in the symposium as well as the field in general.<br\/><br\/>This award will facilitate participation of unfunded student authors, and encourage participation of students from traditionally underrepresented groups, underfunded institutions, and undergraduates.","title":"Student Travel Support for ACM HPDC 2012","awardID":"1233209","effectiveDate":"2012-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[524138],"PO":["565255"]},"193367":{"abstract":"Society increasingly depends on networks in general and the Internet in particular for just about every aspect of daily life. As the Internet increases its reach in global scope, services traditionally implemented on separate networks are increasingly subsumed by the Internet, either as overlays, via gatewayed access, or as a replacement for legacy networks. With this growing dependence on and integration of services in the Internet, come increasingly severe consequences of disruption in networked services.<br\/><br\/>This collaborative project conducts systemic research on fundamental understanding on network design for massive failures or attacks such as power blackouts and natural disasters, as well as attacks against the physical, and protocol infrastructure by intelligent adversaries. The novelty of the work is a systemic approach to designing resilient and survivable networks by developing a network topology generation, design, and analysis methodology that incorporates a number of critical factors, with an optimization formulation that incorporates geodiversity. Methodologies will assess vulnerabilities of existing networks and to propose modifications to make them more resilient and survivable. Furthermore, a novel approach is taken to understand resilient transport for massive failures in a traffic-engineering framework so that applications can exploit the structural diversity introduced in the network.<br\/><br\/>Broader Impact: The research aims to gain new understanding on how to design and deploy resilient networks, with emphasis on surviving massive disruptions as a result of large-scale disasters and coordinated attacks. The specific techniques and mechanisms may be applied to improve the resilience of the current Internet and its parts to better serve society, as well as to help drive the deign of the Future Internet. The research will advance the state-of-the art in network science and engineering. The principal investigators will make all results, including analytic and simulation models and source code, publicly available to the research community on a public wiki. Results will be disseminated in conferences, journal papers, and books. The project will support the research of two Ph.D. students with an effort to recruit students from under-represented groups. The project results will contribute to the education of other students, with direct input and class project participation in several courses at the undergraduate and graduate levels, including a senior undergraduate and graduate special topics courses on Science of Communication Networks and Resilient and Survivable Networks at both The University of Kansas and the University of Missouri-Kansas City.","title":"NeTS: Small: Collaborative Research: Resilient Network Design for Massive Failures and Attacks","awardID":"1217736","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["541801"],"PO":["565090"]},"199714":{"abstract":"","title":"CS 10K Online Communities of Practice","awardID":"1256310","effectiveDate":"2012-08-01","expirationDate":"2017-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7382","name":"Computing Ed for 21st Century"}}],"PIcoPI":[535451],"PO":["560704"]},"196459":{"abstract":"This three-year, multi-path collaboration program between U.S. and Chinese researchers, scientists, educators, and leaders in higher education focuses on key issues regarding network deployment, global cyberinfrastructure, and interoperability. The project builds on engagement between Internet2, CERNET, CTSNET, and the Chinese Academy of Sciences, ongoing collaborations and remote campus deployments between US universities and China, scientific interaction between the two countries, and findings derived from prior annual meetings of the Chinese-American Network Symposium (CANS) group.<br\/><br\/>The intellectual merit of this work is based on the international exchange of ideas it will foster. By bringing together network experts and leading researchers, and by building on the momentum established by campuses and the continuum of CANS events, these activities lead to sustainable and ongoing linkages and interactions to foster global cyberinfrastructure. As research and education increasingly acquire more globalparameters, interactions like this are critical. The broader impact results from the global interactions and ongoing collaborations to be translated to multiple scientific disciplines and the continued deployment of US satellite campuses to promote<br\/>global exchange of educational findings and methodology.","title":"Framework Program for US-China Collaboration in Scientific Research and Education","awardID":"1239009","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"7369","name":"INTERNATIONAL RES NET CONNECT"}}],"PIcoPI":["560043",526814,526815],"PO":["564246"]},"188605":{"abstract":"The MetroBotics REU Site project provides funded academic year research opportunities for students from public urban institutions within the New York City area. The project engages undergraduates in robotics research in the Agents Lab at the City University of New York (CUNY). The Agents Lab focuses on effective coordination of robust human\/multi-robot teams. A rough-and-ready, multi-agent approach is taken, in which small, low-cost robots collectively perform exploration tasks. This team-based strategy differs from state-of-the-art systems in which a single expensive, high-end robot is fielded---but if that robot fails, then the entire operation halts. The multi-agent alternative provides redundancy and requires a system that can coordinate in dynamic, uncertain environments, that can adapt to changing team composition, and can interact with an untrained human operator. The MetroBotics project provides opportunities for undergraduates to participate in this research and to experience the investigative process within a vibrant group setting over an extended timeframe. The target population for the MetroBotics project consists of students underserved by traditional REU Sites, frequently underrepresented in computing, who work while taking classes, who are first in their families to pursue higher education, and who take care of parents or who are parents themselves. These aspects present particular difficulties for students interested in research. MetroBotics strives to get motivated students out of mundane jobs and into the lab during the academic year, giving them a context to deepen technical skills acquired in the classroom and direct access to mentors who can support a dialogue concerning research career options.","title":"REU Site: Academic-year Robotics Research for Urban Public College Students","awardID":"1156827","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1359","name":"RES EXP FOR TEACHERS(RET)-SITE"}}],"PIcoPI":[505729,"523503",505731,505732],"PO":["563751"]},"187318":{"abstract":"Handing dynamics has long since been a unique challenge in wireless network resource management. While the accelerated proliferation of smartphones and tablets has drawn considerable attention to the issue of efficient wireless resource management, the problem actually gets further exacerbated, resulting in inferior application performance at massive scale and negative societal impact to a larger user base. The objective of this project is to establish a solid theoretical foundation for wireless resource management that handles dynamics in its core design. It is based on a novel time-scale decomposition approach, in which dynamics play the foundational role in the clean-slate formulation of a new resource management optimization framework. Through diverse educational efforts and channels, including course development, on-line education systems, research and outreach programs, this project will reach a diverse group of students, influence and inspire them with inter-disciplinary thinking and enrich them with hands-on experiences.<br\/><br\/>The project will establish a significant step towards the relief of \"wireless data network congestion\" problem. The results from this project will enrich and transform the field of wireless resource management from heuristic adaptations of static solutions into one that is oriented towards wireless network dynamics. In addition to the traditional knowledge distribution channel such as academic publications, this project will also disseminate its results through distribution of software packages and the collected network traces. The impact of this project will also be demonstrated in a remote medical care system.","title":"CAREER: A Time-Scale Decomposition Approach to Dynamic-Oriented Resource Management for Wireless Networks","awardID":"1150169","effectiveDate":"2012-08-01","expirationDate":"2017-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[502392],"PO":["557315"]},"192840":{"abstract":"The goal of the project is to be able to replace human agents with<br\/>artificial agents in studying two-player games. This project, if<br\/>successful, will greatly enhance the ability of economists to test<br\/>economic theories by partially replacing laboratory experiments with<br\/>simulations. Over the last decades laboratory studies have proven<br\/>invaluable both for the validation (and invalidation) of economic<br\/>theories, and for the practical purpose of testing mechanisms (such as<br\/>auctions) in the laboratory prior to practical implementation. The<br\/>ability to use simulations with artificial agents in place of<br\/>laboratory experiments with live human beings will both reduce the<br\/>cost of validation and testing, and make it possible to explore<br\/>quickly a much wider range of theories and policy alternatives. It<br\/>will also enhance our understanding of human behavior and enrich our<br\/>knowledge of the connection between human and artificial intelligence.<br\/><br\/>Agent-based modeling is an emerging and attractive approach to<br\/>validating economic theories. Existing research has focused on simple<br\/>and naive agents. This project proposes as the next step to develop<br\/>artificial human (economic) agents capable of mimicking the behavior<br\/>of human laboratory subjects in the context of two player simultaneous<br\/>move games. Substantial and detailed data is available on human play<br\/>under these conditions. Existing algorithms fall only slightly short<br\/>of the ability to mimic human play but do not yet implement fully<br\/>autonomous agents. Based on hidden Markov models, the PIs propose to<br\/>develop and investigate a framework of belief learning that is broad<br\/>enough to encompass many existing learning algorithms including<br\/>reinforcement learning, fictitious play, and smooth fictitious play.<br\/>Moreover, based on this framework, the PIs propose to derive more<br\/>sophisticated learning methods to fully develop artificial agents. This<br\/>research will pursue two important directions. First, the PIs will<br\/>introduce the initial calibration of priors based on available<br\/>information and a cognitive hierarchy model. Second, the PIs will allow for<br\/>the reconsideration of the existing model when \"surprises\" occur. The<br\/>project will lead to the next stage of research in both economics and<br\/>computer science in broadening the class of artificial agents to<br\/>attack broader and more economically important tasks. It will also<br\/>enrich the research on graphical model learning for artificial<br\/>intelligence.","title":"ICES: Small: Artificial Human Agents for Virtual Economies","awardID":"1215302","effectiveDate":"2012-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8052","name":"Inter Com Sci Econ Soc S (ICE)"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"1320","name":"ECONOMICS"}}],"PIcoPI":[516651,"560534"],"PO":["565251"]},"192400":{"abstract":"Future scientific and technological efforts to achieve better understanding of oceans and water-related applications will rely heavily on our ability to jointly consider communications, actuation and sensing in a unified system that includes instruments, vehicles, human operators and sensors of all types. The goals of this project are to design networking tools for mobile underwater networks, develop novel navigation mechanisms for communication-constrained autonomous underwater vehicles and to ultimately integrate sensing and classification to provide solutions for the exploration-exploitation tradeoff.<br\/><br\/>This project will lead to development of underwater communication methods with applications to science, security, and industry in the areas of environmental monitoring, aquatic eco-system analysis, ocean accident remediation, surveillance for defense applications, homeland security, oil and gas, aquaculture, geological and oceanographic science, and marine biology. It will also contribute to the training of new information technology professionals and scientists with expertise in interdisciplinary research spanning underwater networks, oceanography and computer science.","title":"NeTS: Large: Collaborative Research: Exploration and Exploitation in Actuated Communication Networks","awardID":"1212597","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[515627],"PO":["565303"]},"193401":{"abstract":"Recent advances in program verification show that we are on the verge of being able to prove correctness of safety and security critical software systems. But the proofs only establish correctness with respect to a model of the underlying processor on which the code executes. Unfortunately, the community lacks high-fidelity, carefully tested specifications of widely-used processors, such as Intel's x86 family of processors. This severely limits efforts in making software reliable and secure, from software assurance to malware analysis to sandboxing technologies. The goal of this project is to provide tools for building, reasoning about, and validating models of widely-used processors. The proposed research will result in public specifications of common processors, which will benefit a wide range of software applications. It will help improve the dependability and security of critical software applications.<br\/><br\/>The investigators' approach to building processor models is carefully designed to support reuse of components across different architectures and different applications. In particular, they propose to formalize two domain-specific languages that will make it easy to specify decoders and instruction semantics. The tools for these languages will include support for efficient execution so that the models can be tested against implementations. To demonstrate the efficacy of these tools, the investigators will build and validate models of both the x86 and ARM families of processors. They will also investigate applications of these models by building correctness proofs of verifiers for inlined reference monitors and by integrating them as the target languages of a verified compiler. The investigators plan to expend efforts on building a community of researchers for formal processor models and to involve this community to give feedback, improve, and use the models. The project will also provide excellent opportunities for training undergraduate students and for developing new curriculum materials on formal methods.","title":"SHF: Small: Collaborative Research: Reusable Tools for Formal Modeling","awardID":"1217891","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[518022],"PO":["564588"]},"193544":{"abstract":"This proposal aims to develop algorithms for decentralized task allocation among multiple intelligent agents in uncertain environments with a focus on provable performance bounds. Four task settings and their combinations are to be considered: (1) constraints among tasks (e.g., disjoint task groups, precedence relations among tasks); (2) constraints among agents (e.g., maintenance of a communication network); (3) constraints among groups of agents (e.g., requiring a group to include agents with specialized skills); (4) on-line arrival of additional tasks. Existing algorithms for task allocation that have provable performance bounds usually do not consider the realistic constraints stated above. On the other hand, approaches that consider some of the constraints above do not have performance bounds. Furthermore, current algorithms often assume the existence of a centralized coordinator (e.g., an auctioneer in market-based approaches) and may not be scalable. Thus, there is a gap between the existing literature and the practical requirements in multi-robot applications. Hence, there is a need to design distributed task allocation methods that take into consideration practical constraints and have formal performance guarantees. The evaluation plan will use simulated and real robots in search and rescue contexts. The project will use the USARSim environment with approximately 50 simulated robots.<br\/><br\/>The mathematical techniques upon which this project will rely to develop algorithms for task allocation will depend on the problem characteristics. When there are constraints among tasks, the project will use techniques from combinatorial optimization and linear programming. For tasks where each task can be performed by multiple agents, the project will use concepts from cooperative game theory and coalition formation in conjunction with integer optimization techniques. For dynamically arising tasks, the project will explore the use of stochastic programming techniques with the key idea being use of the dual of the integer program model of the task allocation problem to design \"bidding rules\" for agents that ensure a performance guarantee for the overall system.<br\/><br\/>A wide range of application domains -- including emergency response, homeland security, environmental monitoring, hazardous waste cleanup and manufacturing -- stand to benefit from the task allocation techniques proposed in this project. Results from this project will enable application domains to more fully reap the benefits of emerging robotic technology by providing techniques that allow robots to autonomously and efficiently coordinate and allocate tasks among themselves. Graduate students will play a major role in conducting the proposed research. Additionally, this project will provide research opportunities to undergraduate students both within Carnegie Mellon University and from other institutions through the Robotics Institute Summer Scholar program.","title":"RI: Small: Multi-Agent (Multi-Robot) Task Allocation with Formal Guarantees in Dynamic Environments under Realistic Constraints","awardID":"1218542","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["553661"],"PO":["565035"]},"193665":{"abstract":"Computing infrastructure has been a driving force for our socio-economic progress in the past several decades. From drug discovery to space exploration, every scientific and engineering domain relies on computer systems to accurately analyze complex datasets. Historically, computational accuracy has been taken for granted in all these disciplines, but this notion is changing. While rapidly shrinking transistor dimensions lead to exponential power and performance benefits, the trend is also creating several unwanted side effects in computer system reliability. There are two types of errors that will become prevalent in the near future: (1) multi-bit soft errors where alpha particles and neutrons cause multiple bits to flip at the same time, and (2) intermittent errors that occur due to stress accumulation over the lifetime of a computer. Thus it is critical to benchmark the impact of these errors on the lifetime of a computer chip. Only when the impact is accurately measured is it possible to judiciously deploy solutions to improve reliability. Since any protection scheme comes with a cost, it is necessary to understand when a particular protection scheme being considered, such as parity or single-error-correcting double-error-detecting code, is too much or too little. <br\/><br\/>This project presents two solutions for benchmarking multi-bit soft errors and intermittent errors. This project will develop a unified methodology to benchmark the impacts of single-bit and multi-bit soft errors on caches protected with an arbitrary protection scheme, such as an inter-leaved, block-level or word-level error correcting code. Such a benchmarking framework will significantly enhance a computer designer's ability to objectively evaluate the performance, power, and reliability tradeoffs of various protection schemes proposed for protecting caches. <br\/><br\/>This research also develops a methodology to benchmark the vulnerability of an instruction set architecture (ISA) to intermittent errors. Each instruction in an ISA specification is enhanced to quantify the amount of stress that it is expected to cause on the underlying microarchitecture of a chip. The stress level information from the ISA is combined with operating conditions of the chip to continuously monitor intermittent error probability during application execution. Any unwanted degradation in chip reliability is then tackled by software exception handlers, which trigger redundant execution of vulnerable code. <br\/><br\/>Broader societal impact will result from these research solutions. Benchmarking is essential to objectively evaluate the cost-benefit tradeoffs of various solutions currently being proposed to tackle reliability concerns. Without benchmarking, building a system to meet reliability specifications is a guessing game. By providing the right set of tools to initiate just-in-time error correction and recovery mechanisms, a computer designer can significantly lower the cost of providing reliable computations.","title":"SHF:Small: Benchmarking of Transient and Intermittent Errors and Their Application to Microarchitecture","awardID":"1219186","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[518642,518643],"PO":["366560"]},"193786":{"abstract":"Increasingly, firms and organizations are creating structured challenges to spur innovation and address both economic and societal issues. Open innovation virtual organizations are formed to respond to a \"challenge call\" describing the need, providing specifications for desired solutions and setting a deadline for submissions. Usually, this challenge call is delivered on a public forum with an invitation to those with expertise to share their solutions and an incentive for the best solution. In addition, a cyberinfrastructure is established to support the development and delivery of solutions. Solution proposers often participate in an online discussion forum where a social infrastructure is established with norms governing forum behavior. At the end of the challenge, an award is announced to the best solution provider.<br\/><br\/>The incentives provided in most challenges inhibit knowledge sharing among contributors and collaboration is rare. Consequently, there is limited potential to involve diverse and unexpected innovation sources in solution development. This project will develop a sociotechnical cyber infrastructure that manages the tensions inherent in open innovation virtual organizations to encourage collaboration among open innovation participants while mobilizing the energy and focus created by the competition of the open innovation challenge. Through a series of open innovation natural experiments we will compare various interventions aimed at informing participants about collaborative ways to contribute and identify effective techniques.<br\/><br\/>Challenge calls can be an effective innovation technique applying science and engineering to important societal issues. These new platforms are being used to find solutions for such diverse problems as developing an economical low energy use lightbulb, focused drug discovery, and reducing barriers facing returning veterans in higher education. However, the methods currently used in challenges discourage the kind of collaboration that would be needed to leverage diverse ideas and generate greater innovation. This project will develop methods to encourage collaboration among challenge participants and help public and private sector organizations find new and creative solutions to complex problems.","title":"VOSS: Collaborative Research: Impact of In-Process Moderation on Open Innovation Collaboration","awardID":"1219829","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7642","name":"VIRTUAL ORGANIZATIONS"}},{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0405","name":"Division of OF SOCIAL AND ECONOMIC SCIENCE","abbr":"SES"},"pgm":{"id":"8031","name":"Science of Organizations"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7953","name":"SOCIAL-COMPUTATIONAL SYSTEMS"}}],"PIcoPI":[518933],"PO":["565342"]},"193555":{"abstract":"Computers, and implicitly programming languages, are used in many<br\/>critical applications these days, where correct behavior is necessary.<br\/>Rigorous, formal semantic definitions of the employed programming<br\/>languages are therefore necessary in order to verify computing systems.<br\/>Unfortunately, in spite of more than forty years of research in<br\/>programming language semantics, most program verifiers are not directly<br\/>based on a formal semantics, but rather on complex and ad-hoc hardwired<br\/>models of their target programming languages. This has at least two<br\/>negative consequences: first, it makes the development and maintenance<br\/>of program verifiers hard and uneconomical, particularly for new<br\/>programming languages or languages which evolve fast; second, it allows<br\/>room for subtle bugs in program verifiers themselves. This research<br\/>project aims at developing a generic program verification framework<br\/>that takes a programming language given through its formal semantics as<br\/>input, and yields a program verifier for that language as output.<br\/>Moreover, the language semantics will be executable, so testable,<br\/>and public, so will serve as a reference implementation for the language<br\/>and as a formal basis for language understanding.<br\/><br\/>Specifically, this projects builds upon recent advances in matching logic<br\/>and its use for verifying reachability properties. A language-independent<br\/>sound and relatively complete proof system takes a programming language<br\/>operational semantics as a set of axioms, and can be used to derive<br\/>any reachability property about any program in the given language. This<br\/>is in sharp contrast to the existing verification approaches based on<br\/>Hoare logic and on dynamic logic, since these approaches are<br\/>language-specific. This research will therefore lead to the development<br\/>of semantic and verification techniques and algorithms that will work for<br\/>any language, provided a formal semantics of the language is given.<br\/>Consequently, the broader impact of this research is that it will increase<br\/>the quality and robustness of software systems, and will narrow the gap<br\/>between the specification and the implementation of computer systems.","title":"SHF: Small: Usable Verification using Rewriting and Matching Logic","awardID":"1218605","effectiveDate":"2012-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["549774"],"PO":["565264"]},"193203":{"abstract":"Algorithms for solving multivariate polynomial systems will be investigated with a particular focus on parametric polynomial systems in which indeterminates are classified into two disjoint subsets-- one consisting of parameters and other consisting of variables. Such polynomial systems are used to model or approximate problems generically in many application domains, where a generic problem has parameters such that for every parameter value, the generic problem becomes specific. The objective is to study the structure of solutions for different specializations of parameters. This research project will investigate the use of the framework of Groebner basis computations for this analysis. Particularly, comprehensive Groebner bases and comprehensive Groebner systems are elegant mathematical objects which represent all the solutions of a parametric polynomial system for all possible parameter values. The project will explore theoretical foundations as well as develop efficient and effective algorithms for computing comprehensive Groebner systems and comprehensive Groebner bases for parametric polynomial systems. The concept of a minimal canonical comprehensive Groebner basis will be developed and its significance will be explored for studying problems in polynomial ideal theory and algebraic geometry. An efficient algorithm to compute a minimal canonical comprehensive Groebner basis will be investigated.<br\/><br\/>Parametric multivariate polynomial systems are a powerful tool for modeling many problems in various application domains. The problems of (i) determining whether a given polynomial equation system has a common solution, (ii) deriving conditions on symbolic parameters appearing in polynomial equations such that they have a common solution, and (iii) developing an efficient representation of common solutions are of fundamental significance. These problems arise in diverse applications, including engineering design, robotics, inverse kinematics, graphics, solid modeling, CAD-CAM design, geometric construction, drug-design, control theory, and program verification and analysis. Given that many problems in various application domains can be generically modeled using parametric polynomials, fast methods for solving parametric polynomial systems are useful for those applications. The proposed research will lead to the development of theory and algorithms related to comprehensive Groebner computations and investigation of their effective use in many application domains, with a particular focus on geometric design and modeling, as well as program analysis and verification. The algorithms developed during the research project will be implemented in computer algebra systems including Magma and Singular, and experimented with on a variety of problems arising from different application domains. Heuristics will be developed and analyzed to make these algorithms and their implementations efficient.","title":"Math: Algorithms for Parametric (Comprehensive) Groebner Computations","awardID":"1217054","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0304","name":"Division of MATHEMATICAL SCIENCES","abbr":"DMS"},"pgm":{"id":"1271","name":"COMPUTATIONAL MATHEMATICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["531861"],"PO":["565211"]},"193566":{"abstract":"Approximation techniques are valuable in the design of algorithms for combinatorial optimization problems. Mathematical programming relaxations provide tractable versions of hard optimization problems that are useful in the design of good algorithms -- in some cases, these relaxations and their duals serve as a guide for the design of algorithms and lower bound proofs. Such approximation techniques are useful not just in traditional optimization problems, but also for problems in online algorithms and other areas. This project proposes to study a variety of problems where approximation techniques play a crucial role -- many of these questions are about basic problems in approximation algorithms, but many are about questions where insights from mathematical relaxations and other approximation techniques play an important role. The broad goals of this project include (a) Obtaining a better understanding of the use of lift-and-project relaxations for optimization problems like coloring and the closely related question of developing algorithmic techniques for graphs of low threshold rank, (b) Attempting to close gaps in our understanding of classical optimization problems like the traveling salesman problem and bin packing, and (c) Shedding light on newer problems like online versions of weighted matching and new flow formulations.<br\/><br\/>Optimization problems are ubiquitous and for many such problems of interest, we have strong evidence that it is impossible to obtain exact efficient solutions. To circumvent this intractability, we design efficient heuristics that may not find the best solution necessarily, but have guarantees that the solution they produce is not far from the optimal (i.e. is approximately optimal). Mathematical programming is a very important tool in designing such approximation algorithms. Successfully achieving the project goals will require advances in our knowledge of this area, and especially new insights into the powerful and versatile mathematical programming toolkit. As part of this project, graduate and undergraduate students will be trained by involving them in these research activities. Course materials for graduate and undergraduate courses will be developed distilling research results of this project, as well as new developments in the field.","title":"AF: Small: Approximation Techniques for Combinatorial Optimization","awardID":"1218687","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["542007"],"PO":["565251"]},"193478":{"abstract":"Modern Graphic Processing Units (GPUs) offer more parallelism and higher memory bandwidth than CPUs. This project aims to take advantage of these properties by developing a system to efficiently process database queries over GPU-resident datasets. To achieve this goal this project employs the following approaches: (a) The development of novel indexing techniques that combine multidimensional partitioning with block-oriented bitmaps, and whose parameters are sensitive to the query distribution;(b) The optimization of memory bank contention and value contention between threads; (c) The efficient implementation of a complete set of relational database operators, including aggregation, joins, and indexed selections; and (d) The evaluation of the performance of the system on query-intensive workloads, using real applications and standard benchmarks. Improvements in database system performance would have wide-ranging impact on the efficiency of many enterprises that employ database systems for analytics. The project supports PhD students working on database system implementation techniques. The innovations and software created during the project will be used to enhance the curriculum of the Database Systems Implementation course at Columbia University. Publications, software, and other project data will be disseminated via the web at our project web site (http:\/\/www.cs.columbia.edu\/~kar\/gpuproject.html).","title":"III: Small: Database Processing on GPUs","awardID":"1218222","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["531167"],"PO":["563727"]},"196218":{"abstract":"This Ethics Education in Science and Engineering award addresses growing calls for ethics as a core competency in engineering education. The fundamental questions addressed by this research are:<br\/><br\/>(1) What is the impact of a scaffolded, integrated, and reflexive analysis (SIRA) approach to ethical analysis of cases concerning emerging technology on the development of students' ethical reasoning and satisfaction and engagement with ethics education?<br\/>(2)What characteristics of this SIRA approach most contribute to change in moral reasoning ability and to student satisfaction and engagement with engineering ethics education?<br\/>This will be studied by examining the relationship between graduate and senior-level undergraduate students' participation in and experience of the online learning module and their development of ethical reasoning and satisfaction and engagement with ethics education. The result will be several SIRA modules that will have been piloted in the online format with increasingly larger groups of engineering seniors and graduate students. Their effect will be assessed through analysis of reasoning, interactivity, discourse, and reflectivity.<br\/><br\/>Broader Impacts: The results of this project will be communicated in engineering education and ethics communities by presentation at professional engineering education, national engineering, and ethics conferences. This will include publishing findings in peer reviewed publications with potential to reach collegiate educators. In addition, the interactive case-based modules will be made available through the NSF-sponsored website for online ethics, Ethics CORE at http:\/\/nationalethicscenter.org, and the NAE Online Ethics Center at http:\/\/www.onlineethics.org. SIRA learning modules with established assessment instruments that have proven to be effective in developing ethical reasoning abilities for engineers will be an attractive option for embedding into a wide variety of engineering courses and curricula across disciplines and within a variety of engineering programs at both undergraduate and graduate levels.","title":"SIRA Modules for Effectively Engaging Engineers in Ethical Reasoning About Emerging Technologies","awardID":"1237868","effectiveDate":"2012-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7787","name":"EESE"}}],"PIcoPI":["540666",526013,526014,"553347"],"PO":["564922"]},"200706":{"abstract":"A key challenge in developing multi-threaded applications on modern architectures is correctly synchronizing data shared among the threads while avoiding excessive performance penalties. Unsafe low-level synchronization mechanisms can easily introduce errors (e.g. race conditions and deadlock) that are extremely difficult to debug. At the same time, application performance and scalability are frequently compromised due to inefficient implementations of synchronous operations on shared data. <br\/><br\/>This research develops a library of highly concurrent scalable data containers with associated programming interface and optimization support to significantly enhance the productivity and performance of multi-threaded C\/C++ applications on multicore architectures. The library provides an easy to use and composable interface similar to that of C++ Standard Template Library (STL) and enhances each container type with internal support for non-blocking synchronization of their data accesses, thereby providing better safety and performance than traditional blocking synchronization by eliminating hazards such as deadlock, livelock, and priority inversion, and by being highly scalable in supporting large numbers of threads. A higher level programming interface, similar to that of OpenMP, is supported by a preprocessing compiler associated with the runtime to ease the transition of existing sequential or multi-threaded C\/C++ applications to using the non-blocking synchronous template library and to provide optimization and tuning support for the use of the library abstractions. The developed deliverables are expected to demonstrate a seamless integration of developer input, compiler optimization, and multicore runtimes to support systematic migration of C\/C++ applications to continuously evolving architectures. <br\/><br\/>The scalable template library and the associated programming interface and tuning support is expected to provide an immense productivity and performance boost for developers of high-end scientific and systems applications, including branch and bound, graph analysis, complex scene rendering, and goal propagation in autonomous embedded systems. The developed programming techniques and tools can enable the transformation of such applications into software that is substantially more reliable, efficient, and scalable than existing state of the art. The software techniques is also expected to be employed as an educational toolkit in the teaching of programming languages, compilers, systems software, and parallel programming courses.","title":"SHF: Small: Collaborative Research: Programming Interface And Runtime For Self-Tuning Scalable C\/C++ Data Structures","awardID":"1261584","effectiveDate":"2012-08-13","expirationDate":"2015-05-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["538267"],"PO":["565272"]},"191774":{"abstract":"A unique interdisciplinary team of computer scientists, information scientists, ornithologists, project managers, and programmers will develop a novel network between machine learning methods and human observational capacity to explore the synergies between mechanical computation and human computation. This is called a Human\/Computer Learning Network, and while the focus is to improve data quality in broad-scale citizen-science projects, the network has the potential for wide applicability in a variety of complex problem domains. The core of this network is an active learning feedback loop between machines and humans that dramatically improves the quality of both, and thereby continually improves the effectiveness of the network as a whole. The Human\/Computer Learning Network will leverage the contributions of broad recruitment of human observers and process their contributed data with artificial intelligence algorithms leading to a total computational power far exceeding the sum of their individual parts. This work will use the highly successful eBird citizen-science project as a testbed to develop the Human\/Computer Learning Network. eBird engages a global network of volunteers who submit tens of millions of bird observations annually to a central database.<br\/>This research addresses three fundamental data quality challenges in citizen-science. These are: 1) reducing errors in identification or classification of objects; 2) identifying and quantifying the differences between individual observers; 3) reducing the spatial bias prevalent in many citizen-science projects. To address these challenges, the project will build on advances in artificial intelligence that now provide the opportunity to study systems through the generation of models that can account for enormous complexity. Preliminary work on observer classification will be extended by developing new multi-label machine learning classification algorithms that provide better ecological interpretations and more accurate predictions. In addition, the research will develop new active learning algorithms by constructing sampling paths that will optimize volunteer survey efforts to maximize overall spatial coverage, and incentivize participation via crowdsourcing techniques. Finally, it will study how participants can improve the quality of their observations based on the feedback and information provided by the artificial intelligence. <br\/><br\/>Broad-scale citizen-science projects can recruit extensive networks of volunteers, who act as intelligent and trainable sensors in the environment to gather observations. Artificial intelligence processes can dramatically improve the quality of the observational data that volunteers can provide by filtering inputs based on observers' expertise, a judgment that is based on aggregated historical data. By guiding the observers with immediate feedback on observation accuracy and customization of observation worksheets, the artificial intelligence processes contribute to advancing expertise of the observers, while simultaneously improving the quality of the training data on which the artificial intelligence processes make their decisions. The results of the project will have significant benefit for all citizen science and broader impact in an emerging world of ubiquitous computing in which human-machine partnerships will become increasingly common.","title":"SoCS: Collaborative Research: A Human Computational Approach for Improving Data Quality in Citizen Science Projects","awardID":"1209589","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7953","name":"SOCIAL-COMPUTATIONAL SYSTEMS"}}],"PIcoPI":["541699",514003,"560670"],"PO":["564456"]},"192764":{"abstract":"It is intuitively clear that, in many situations, agents (in the sense of game theory) do not make a best response because determining what the best response should be would require a great deal of effort. This does not make agents irrational; chess players are not being irrational when they fail to play optimally. Indeed, their response could be viewed as perfectly rational, given their computational limitations. Standard solution concepts such as Nash equilibrium do not explicitly take computation into account. <br\/><br\/>The importance of considering computational issues in game theory has been recognized since at least the work of Simon. The PIs have developed a general framework for taking computation into account. A player is viewed as choosing a Turing machine (TM). With each TM M and input is associated a complexity. The complexity can represent the running time of or space used by M on that input; it can also be used to capture the complexity of M itself (e.g., the number of states) or to model the cost of searching for a new strategy to replace one that the player already has. The approach captures important intuitions, and can deal with a number of well-known problematic examples. Moreover, there are deep connections between this approach and cryptographic protocol security. Thus, thinking in terms of computational games can lead to new insights and new approaches in security. <br\/><br\/>The goal of this project is to investigate the implications of adding computational cost to game theory more broadly. This is expected to involve issues of language, awareness, and cryptography. Topics to be addressed include the following: <br\/>- The extent to which this approach can capture important notions of secure computation and suggest new notions will be studied.<br\/>- In the model, it is implicitly assumed that agents \"understand\" the costs associated with each TM and \"understand\" what the best response is. Thus, if the complexity is taken to be running time, agents know (or at least have subjective beliefs about) the running time of each TM. But where are these beliefs coming from? Clearly they, too, must be computed. Thus, the model will be extended so that it takes into account the cost of computing beliefs and best responses. Doing this will force consideration of issues of language in a serious way. <br\/>- The earlier model considered only normal-form games. In extensive-form games, computation becomes an explicit part of the game. Intuitively, as the agent does computation, his understanding of the game improves, and he may become aware of more options. Thus, there should be deep connections between computation in extensive-form games and awareness, a topic that has attracted a great deal of recent attention in game theory and computer science; these will be explored.<br\/>- To further understand the role of language, the interplay between the choice of language, computational cost, and rationality will be considered. <br\/><br\/>Progress in this area could have broad impact on theoretical models in game theory and computer science, and on applications of game theory both to the social sciences and to more practical work in security and networks, where game-theoretic models are becoming more and more prevalent.","title":"ICES: Large: Computation, Language, and Awareness in Games","awardID":"1214844","effectiveDate":"2012-08-01","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8052","name":"Inter Com Sci Econ Soc S (ICE)"}}],"PIcoPI":[516488,"517989"],"PO":["565251"]},"194711":{"abstract":"This project is developing tools and techniques for cost-effective evaluation of the trustworthiness of mobile applications (apps). The work focuses on enterprise scenarios, in which personnel at a business or government agency use mission-related apps and access enterprise networks.<br\/><br\/>In such scenarios there are incentives and resources for much more substantive evaluations and controls on information flow than are currently found in commodity app marketplaces. The project aims to advance the science needed for static techniques to be usable by professional development and evaluation teams and useful for achieving dramatically improved assurance. The project's goals are to: (a) find flexible and expressive ways to specify information flow requirements for apps, (b) find effective ways to specify what is assumed about the Android platform, and (c) find practical static analysis and verification techniques to check security of apps with respect to given policies and the platform. Results include specification techniques and theory - models and algorithms. These are applied in case studies with prototype tools that the project develops, to evaluate how well the goals are achieved.<br\/><br\/>The project's techniques can be deployed by certification organizations to provide scientifically sound techniques for assurance, thus<br\/>enabling the full benefits of highly-integrated mobile software in mission-critical situations. Software designers will benefit from being able to precisely specify end-to-end requirements as well as component interfaces. Software developers will benefit from reliable means to detect design flaws and bugs, malware in third-party software, and unintended functionality that exposes vulnerabilities. Beyond the specific target of mobile software, the techniques will be of use in other settings, especially web applications, where it is crucial to reason about interfaces between mutually untrusting parties making heavy use of callbacks. The project could help improve security in government agencies and private sector, indirectly benefitting national security and the general population.","title":"TWC: Medium: Collaborative: Flexible and Practical Information Flow Assurance for Mobile Apps","awardID":"1228695","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[521544],"PO":["549626"]},"194601":{"abstract":"Proposal #: 12-28291<br\/>PI(s): Majumdar, Kingshuk<br\/> Kremar, Maja; Szarecka, Agnieszka<br\/>Institution: Grand Valley State University<br\/>Title: MRI\/Acq.: High-Performance Computing Cluster for Research and Education<br\/>Project Proposed: <br\/>This project, acquiring a computing cluster with multiple nodes, aims to have the equipment serve as a central resource for high performance computing at the institution. Complementing a small 8-node computing cluster, this cluster supports research and training of faculty and students in physics and cellular and molecular biology and provides the opportunity to foster an emerging community of interdisciplinary researchers from other fields interested in solving complex computational problems. Some research activities include:<br\/>- Order-disorder phases of two and three dimensional magnetic spin systems;<br\/>- First principle calculations using density functional theory; and<br\/>- Quantum mechanical modeling of the binding pocket and antibiotic resistance in a set of beta-lactamases.<br\/>The instrumentation provides a means to test, optimize, and carry out some computations locally.<br\/>Broader Impacts: <br\/>Serving as a powerful stimulus for the recruitment of undergraduates that include first generation, minority and non-traditional students, the instrumentation should also leverage individual research funding in a mainly undergraduate-serving institution and foster further the integration of research and education. Moreover, this acquisition should promote an exciting learning environment for the next generation of students providing an accessible entry to research and education at the frontiers of Computational Science, Information Technology, and Cluster Computing.","title":"MRI: Acquisition of a High-Performance Computing Cluster for Research and Education","awardID":"1228291","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}}],"PIcoPI":[521221,521222,521223],"PO":["557609"]},"193655":{"abstract":"This project investigates the nature of crowd-based human analytics at various scales, specifically how the concentrated efforts of a few contributors differ from the summed micro contributions of many. Automated approaches are good at handling huge amounts of data, but they lack the flexibility and sensitivity of human perception when making decisions or observations, especially when computational challenges revolve around visual analytics. Networks of humans, as an alternative, can scale up human perception by facilitating massively parallel computation through the distribution of micro-tasks, but human data interpretation is variant between individuals. Wide variability in the amount of participation of individuals in crowd-based computation creates non-uniform representations of a crowd, which is an important discrepancy that could significantly impact the validity of the term \"crowd\" in crowdsourcing. The research will explore data generated from the extreme ends of the participation curve and quantify the quality of data produced from a broad sampling of a crowd versus concentrated voice of the few \"super users.\" <br\/><br\/>As one measure of comparison, the researchers will observe how characteristically variant samplings of human generated analysis alter the outcome when used as training data in a machine learning framework. This investigation will utilize data generated from a crowdsourcing effort that tapped over 10,000 volunteer participants to generate over 2 million human annotations on ultra-high resolution satellite imagery in search for tombs across Mongolia. Image tiles were distributed at random to participants who tagged anomalies of interest, while crowd consensus on points of interest provided a field survey team with locations to ground truth in Mongolia. Participation ranged widely, as illustrated by the fact that 20 percent of the data came from the most active 1 percent of participants, while at the other extreme 20 percent of the data came from the 80 percent of participants who were least active. While consensus of the crowd provided one metric to measure the quality of anomaly identifications, ground truth observations showed actual validation tended to correspond with identifications made from higher interest participants. This study will explore the nature of data generated from experts versus crowds of non-experts, starting from the discrepancies in participation levels.<br\/><br\/>Crowd-based human analytics has been welcomed as a potential solution to some of the world?s largest data challenges. Examples of crowdsourcing have shown that the power of distributed microtasking can engage challenges as overwhelming as categorizing the galaxies, or as complicated as folding proteins. However this concept depends upon the recruitment of human help, often at whatever levels of participation an individual is willing to contribute. The variation in contributions, and thus impact levels, between individuals can be staggering, with participation typically distributed across a longtail curve. That fundamental aspect of a recruited crowd should be recognized and understood when extracting knowledge from the data that is generated. This project will contribute to the necessary understanding by determining how the distributed inputs from a crowd differ from the concentrated efforts of an individual. Insight into the effects of crowd dynamics on results will determine how we pool and retain participation and, thus, have transformative impact on the development of crowdsourcing as a concept for analytics.","title":"HCC: Small: Examining the Super User versus the Crowd in Human-Centered Computation","awardID":"1219138","effectiveDate":"2012-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["539187","557605"],"PO":["564456"]},"193545":{"abstract":"Time banking refers to community-based volunteering in which participants provide and receive services; for example, one neighbor might be a competent handyman, another has a heavy-duty pickup truck. Each can provide community service doing what he\/she can do for other members. For this, they receive time credits that can be exchanged for other services, say, gardening. A community brokering entity, a time bank, keeps track of time credits earned and redeemed. Time banks have pervasive implications for conceptions of work, citizenship, volunteering, community engagement, and social inclusion. Time banking has spread rapidly in recent years throughout the world.<br\/><br\/>Current time banking focuses on asynchronous transactions. This project will incorporate mobile computing and Web 2.0 services to carry out a design research investigation of mobile time banking. The work will support finer grain, near-real time scheduling of neighborhood service exchanges. This approach allows many new time banking possibilities. For example, a child comes home from school with a headache. His mother posts a request, and a neighbor who is stopping at the local drug store anyway gets the aspirin.<br\/><br\/>Broader impacts: This project will fundamentally extend the concept of time banking, and support for time banking throughout the world, by directly extending current time banking infrastructures. It can provide domestic and community infrastructure to ease the busyness stresses of too many to-dos, while enhancing quality of life through improved health and sense of community. It can increase the visibility and valuing of informal quasi-work activities that are often devalued in money-based economies, and facilitate the transition from volunteer activity to paid work, a key policy strategy for reducing chronic unemployment. This project integrates education, research, and community service for student participants. The PIs will work with campus programs to ensure that a diversity of students is aware of our work.","title":"HCC: Small: Socio-technical Issues in Mobile Time Banking","awardID":"1218544","effectiveDate":"2012-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":["549541"],"PO":["564456"]},"193787":{"abstract":"Increasingly, firms and organizations are creating structured challenges to spur innovation and address both economic and societal issues. Open innovation virtual organizations are formed to respond to a \"challenge call\" describing the need, providing specifications for desired solutions and setting a deadline for submissions. Usually, this challenge call is delivered on a public forum with an invitation to those with expertise to share their solutions and an incentive for the best solution. In addition, a cyberinfrastructure is established to support the development and delivery of solutions. Solution proposers often participate in an online discussion forum where a social infrastructure is established with norms governing forum behavior. At the end of the challenge, an award is announced to the best solution provider.<br\/><br\/>The incentives provided in most challenges inhibit knowledge sharing among contributors and collaboration is rare. Consequently, there is limited potential to involve diverse and unexpected innovation sources in solution development. This project will develop a sociotechnical cyber infrastructure that manages the tensions inherent in open innovation virtual organizations to encourage collaboration among open innovation participants while mobilizing the energy and focus created by the competition of the open innovation challenge. Through a series of open innovation natural experiments we will compare various interventions aimed at informing participants about collaborative ways to contribute and identify effective techniques.<br\/><br\/>Challenge calls can be an effective innovation technique applying science and engineering to important societal issues. These new platforms are being used to find solutions for such diverse problems as developing an economical low energy use lightbulb, focused drug discovery, and reducing barriers facing returning veterans in higher education. However, the methods currently used in challenges discourage the kind of collaboration that would be needed to leverage diverse ideas and generate greater innovation. This project will develop methods to encourage collaboration among challenge participants and help public and private sector organizations find new and creative solutions to complex problems.","title":"VOSS: Collaborative Research: Impact of In-Process Moderation on Open Innovation Collaboration","awardID":"1219832","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"7642","name":"VIRTUAL ORGANIZATIONS"}},{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0405","name":"Division of OF SOCIAL AND ECONOMIC SCIENCE","abbr":"SES"},"pgm":{"id":"8031","name":"Science of Organizations"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7953","name":"SOCIAL-COMPUTATIONAL SYSTEMS"}}],"PIcoPI":[518935],"PO":["565342"]},"193556":{"abstract":"Smart phones, tablet computers, and other mobile devices are transforming the way people work, play, and interact. Their mobility, connection to the cloud, and integration of sensors, computation and communication have inspired unique applications unforeseen just a short time ago. The future holds even greater promise, as applications fuse diverse inputs to deliver new levels of situation awareness and embedded intelligence. To fuel this rapidly growing market, the processors that power mobile devices must be designed more quickly than processors in conventional computers, even as the complexity of mobile processors approaches that of desktop and server processors. Product development is accelerated by licensing hardware descriptions of the latest processors - called soft cores - from third parties, and integrating them with proprietary designs into an overall system-on-chip. Unfortunately, there is an additional step that erodes the productivity gains won by licensing soft cores. A soft core must be converted into a hard core, i.e., a circuit layout that can be fabricated in a semiconductor foundry. Producing a high-quality layout is a painstaking, manual process requiring niche expertise. Alternatively, automated synthesis and place-and-route (SPR) tools can be used, but they produce poor layouts with sub-par performance and power consumption.<br\/><br\/>This research combines the convenience of automated layout with the quality of manual layout. The key innovation is to not compromise on automation - SPR must be used - but rather to modify the design of mobile processors so that SPR is able to produce a quality layout on par with manual layout. That is, the mobile processor is designed with the knowledge that SPR is going to be used. This new paradigm is called Design for Competitive Automated Layout (DCAL). DCAL applies a novel regimen of design strategies at multiple levels that enables SPR to produce competitive layouts. A common theme across all levels is to restructure or eliminate sources of processor complexity that SPR handles poorly. (1) Circuit-level DCAL: Highly-ported memory structures traditionally require intense manual layout. Making matters worse, there are many of them in a modern processor. These are restructured to achieve quality layouts without manual effort. (2) Microarchitecture-level DCAL: The most challenging processor units are restructured so that aggressive circuit and layout optimizations for meeting timing closure are rendered unnecessary. (3) Core-level DCAL: Designing a single microarchitecture that performs well across arbitrary program phases is a significant source of complexity. Core-level DCAL divides program behaviors into useful classes and provides dedicated core designs for these classes; the cores are streamlined for the targeted behaviors, enabling SPR to produce quality physical designs. (4) ISA-level DCAL: For portability across many different processors, mobile device software is often distributed using a virtual instruction-set-architecture (ISA) that does not correspond to any particular processor ISA. The prerequisite for translating the virtual ISA into a processor ISA on-the-fly opens the door to improvising on the processor's ISA for one or multiple core types, with the aim of further streamlining cores for SPR to generate quality physical designs.<br\/><br\/>The economic and societal benefits of DCAL are tangible. Automation accelerates innovation by allowing companies to focus more on developing richer user experiences and less on low-level technology that makes it all possible. Moreover, automation puts this technology into the hands of more people, including folks without niche expertise and small nimble design teams. Both companies and everyday users profit from the fact that more innovative products are being delivered to market sooner.","title":"SHF: Small: Design for Competitive Automated Layout (DCAL) of Mobile Application Processors","awardID":"1218608","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[518384],"PO":["366560"]},"193567":{"abstract":"Supervised semantic parsers, which learn to map language to relational data, perform poorly on texts that differ in vocabulary or style from the training text, and on databases that differ from the database used in training. Today's semantic parsers have only been tested on narrowly-circumscribed domains like geography, but the ideal semantic parser would generalize to the many and incredibly diverse relational databases available on the Web. <br\/><br\/>This project develops semantic parsers that approach this ideal system. The project divides the overall task into two parts: mapping named-entities in text to database constants in any domain, and mapping full sentences and questions to logical forms written in a variant of the lambda calculus. Techniques for resolving named-entities make use of domain-independent contextual information around the named-entity for disambiguation. To connect words like \"directed\" with a database relation listing directors of movies, the project relies on schema-matching techniques from database integration. The system extracts a relational view of a corpus, and then generates alignments between these extracted alignments and the fixed relational structure of existing databases. The project uses transfer-learning and co-training approaches to estimate parameters for statistical models for named-entity disambiguation and schema matching across domains and databases.<br\/><br\/>The project is expected to produce new methods and software systems for connecting human language with relational data. It enables language-based queries to the broad array of structured data available on the Web, making it easier to find information than ever before.","title":"RI: Small: Learning Open Domain Semantic Parsers","awardID":"1218692","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[518409],"PO":["565215"]},"193215":{"abstract":"In this project, the PI and his team study the relationship between widely-used satisfiability algorithms (SAT solvers) and the complexity of proofs, with the goal of characterizing the proof strength of complete SAT solvers that use conflict-directed clause learning. The project also focuses on the inherent tradeoffs between the running time and storage (space) required to derive proofs and the impact that this has on SAT solvers. SAT solvers are among the most important and useful tools in a wide range of applications, from finding solutions to problems under constraints to checking the correctness and safety of software and hardware systems where their<br\/>role as methods of proof is required.<br\/><br\/>This project will also explore resource tradeoffs inherent in solving specific computational problems such as computing order statistics and encoding data using good quality error-correcting codes that are resilient to worst-case errors. Such error-correcting codes are becoming increasingly important in a networked world in which adversaries may deliberately corrupt network traffic.","title":"AF: Small:Tradeoffs among Measures in Computational and Proof Complexity","awardID":"1217099","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[517581],"PO":["565157"]},"193336":{"abstract":"Bug fixing is time-consuming and error-prone. In the current multi-core era, widespread multi-threaded software and concurrency bugs make things even worse. Developers struggle to release correct patches for concurrency bugs on time. Much progress has been made in detecting concurrency bugs. Unfortunately, software reliability does not improve until the detected bugs are actually fixed.<br\/><br\/>This project aims to build an automated bug-fixing framework that enables self-healing multi-threaded software by combining the strengths of concurrency-bug detection, static analysis, and multi-threaded software testing. Specifically, the proposed framework will include four automated components: (1) a testing and bug-detection component that helps understand concurrency bugs and designs fix strategies; (2) a static analysis and code transformation component that inserts synchronization into software and generates high-quality patches; (3) a component that evaluates and refines patches; (4) a component that provides ad-hoc patches for bugs with incomplete information. This research will help lower the costs of software development, failure diagnosis, and bug repair. It will also improve software users everyday experience through faster and more reliable software on a wide spectrum of platforms.","title":"SHF: Small: A Framework for Self-Healing Multi-Threaded Software","awardID":"1217582","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["549944",517878],"PO":["564388"]},"193578":{"abstract":"This award aims to increase our understanding about the power of quantum computers. The goal is to determine which problems have efficient quantum algorithms and which do not. One source of problems relates to the simulation of quantum mechanical systems. Another direction is determining which cryptosystems are secure against quantum computers. The most commonly used cryptosystems, such as systems based on RSA and elliptic curves, can be broken by quantum computers and an important question is to determine which systems to use instead. This question involves both understanding which problems are hard for quantum computers as well as how to design secure protocols.<br\/><br\/>The PI and his graduate students will consider three types of problems. The first comes from Hamiltonian complexity, which addresses computing properties of physical systems. Depending on the setup, these systems range from being solvable classically to being very hard even for quantum computers. The second set of problems comes from proposals for quantum-resistant cryptography using the Learning With Errors problem. The PI will study the recent assumptions that have been made to make these systems more efficient to see if they are secure against quantum computers. The third set of questions involves determining when the security arguments work for classical protocols in the presence of quantum computers.","title":"AF: Small: The Quantum Complexity of Physical and Algebraic Problems","awardID":"1218721","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[518436],"PO":["565157"]},"193468":{"abstract":"This project develops a theory for characterizing the performance of parallel data structures and parallel algorithms that use parallel structures. Standard metrics for parallel algorithms, such as \"work\" (total amount of computation) and \"span\" (critical-path length), do not naturally generalize in the presence of contention on shared data. Moreover, standard approaches for analyzing sequential data structures, such as amortization, do not seem to generalize when data structures are parallel, in part because the performance depends on the properties of the underlying parallel task schedulers.<br\/><br\/>The specific research goals are as follows: <br\/>(1) Investigate a methodology for designing and analyzing parallel algorithms that use data structures, especially amortized ones. <br\/>(2) Design parallel schedulers that ameliorate the contention on parallel data structures. <br\/>(3) Design parallel data structures that perform provably well with these schedulers.<br\/><br\/>Today parallel computing is ubiquitous. Modern computation platforms---smartphones to network routers, personal computers to large clusters and clouds---each contain multiple processors. Writing parallel code that provably scales well is challenging, and techniques for analyzing sequential algorithms and data structures generally do not apply to parallel code. This project will develop a theoretical foundation for characterizing the scalability of parallel programs that contend for access to shared data.","title":"AF: SMALL: Collaborative Research: Data Structures for Parallel Algorithms","awardID":"1218188","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["548222"],"PO":["565157"]},"193589":{"abstract":"Energy and power consumption have become a critical issue ranging from microarchitectures to large-scale data centers and supercomputers. Conservative estimates suggest that the information technology industry world-wide energy consumption is in excess of 400 TWh and growing, generating roughly the same carbon footprint as the airline industry, accounting for 2% of global emissions. At the same time, the power constraints of chips hamper their performance, and the shrinking transistor geometries and low supply voltages increase the severity of processor variations resulting in higher timing error rates. High error rates lead to a significant drop in yield and increased manufacturing costs, calling for designs that are able to withstand them.<br\/><br\/>This project seeks to understand and explore the novel paradigm of elastic fidelity computing. Elastic fidelity computing capitalizes on the observation that many applications can naturally tolerate errors, and that not all of them need to run at 100% fidelity all the time. Specifically, the goal of this work is to understand the error models of various hardware components as they relate to data movement, storage, and computation, and simultaneously to understand the error resiliency of applications and re-architect them to leverage elastic fidelity.<br\/><br\/>Elastic fidelity offers potentially transformative effects for science and society, by challenging conventional wisdom and taking a fresh look at the interplay of errors, output quality and energy efficiency for an important class of pervasive streaming and data-intensive applications. More specifically, elastic fidelity promises significant energy savings that can put computing on an environmentally sustainable path, by lowering the operational costs in major economic sectors, and making the manufacturing of future chips cheaper by relaxing the accuracy requirements of hardware components. The results of this research will be disseminated through publications, workshops, advanced curriculum, and releases of the developed infrastructure in the public domain. To accelerate broad societal effects, the project participants will seek to foster technology transfer by promoting collaboration and industry involvement through presentations and site visits.","title":"SHF:Small:Collaborative Research: Elastic Fidelity: Trading-off Computational Accuracy for Energy Efficiency","awardID":"1218768","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[518459,518460],"PO":["366560"]},"197406":{"abstract":"This EAGER proposal, augmenting basic materials and device research in integrated polymer\/living tissue structures, opens an exciting new area in engineered materials. Upgrading from the planned inkjet printer to the new generation of materials plotters in this exploratory project will enable the rapid integration of organic structural, sensor, actuator and electronic components into fully functional devices at many scales. The applications for robotics and cyber-physical systems include 3D polymer MEMs with integrated organic electronics, neurons and even muscle tissue into polymer mechanical devices, variable-focus vision systems and new approaches to polymer muscles.<br\/><br\/>Broader Impacts: This equipment will advance discoveries in multiple materials and device studies in projects across a number of academic departments. The research will broaden participation by providing the critical hardware for three female PhD students in the PI's labs, two of which are funded by NSF GRFs. The equipment will be made broadly available through a new user-service facility, enhancing infrastructure in multiple disciplines.","title":"EAGER: Shared Materials Plotter for Organic Robotics","awardID":"1243871","effectiveDate":"2012-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["544824"],"PO":["543539"]},"191522":{"abstract":"This project develops the technology for unconstrained measurement of human grasp forces. Measurement of multi-fingered grasp forces typically requires a human to grasp an object at predefined sensor locations or to wear instrumented gloves that impede haptic sensations. The objective of this project is to characterize the ability to estimate three-dimensional grasp forces at the fingertips by measuring the color change of the fingernail. This fingernail imaging technique allows the human subject to freely choose where to place the fingers on the object, allowing for completely unconstrained multi-finger grasping. A magnetic levitation device is used to apply a range of 3-D forces to the human fingertip while collecting images of the fingernail. Various image processing techniques are being explored to register the fingernail images to a standard template, and various mathematical models relating pixel intensity to force are being investigated to determine an optimal method. A robotic motion-tracking technique is being implemented to keep the fingers in view of the camera as the hand moves during grasping experiments. The fingernail imaging technique is first validated using constrained grasping experiments, and then applied to unconstrained grasping experiments.<br\/><br\/>This research enables a co-robot to detect the individual finger forces of a human partner using a technique that does not interfere with the human's haptic sense. A co-robot trained with the appropriate calibration data could recognize and emulate or adapt to a human partner's grasp forces, measured using only vision. Research efforts are being integrated into the Robotics education and outreach at the University of Utah.","title":"NRI-Small: Measuring Unconstrained Grasp Forces Using Fingernail Imaging","awardID":"1208626","effectiveDate":"2012-08-01","expirationDate":"2017-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":["513354",513346],"PO":["564316"]},"193656":{"abstract":"This research seeks to push the frontiers of coreference resolution research via achieving two objectives. First, it addresses the Winograd Schema Challenge by examining a class of difficult-to-resolve anaphors whose resolution requires commonsense knowledge involving the roles played by the participants in an event and their causal relationships. It adopts a deep text-understanding approach to this problem. Specifically, it ventures into unexplored areas of coreference research, including the use of script-like knowledge and sentiment analysis, as well as an examination of the role of discourse connectives. Second, it enables the acquisition of coreference resolvers for a substantially larger number of natural languages and domains than is currently possible. One of the major obstacles to the deployment of coreference technologies across a large number of languages and domains is the high cost associated with coreference-annotating data in a language and domain. It investigates two cost-effective approaches to data annotation, one involving translation-based projection and the other bootstrapping.<br\/><br\/>As coreference is an enabling technology for many traditional and emerging text-processing applications, the project has the potential to improve these applications. Through the construction of a multi-lingual, multi-domain coreference resolver and the availability of annotated data produced in the course of this investigation, the project may stimulate research in under-studied languages and domains by a broader community of researchers. Equally importantly, the use of commonsense knowledge in the resolver mimics the human coreference resolution process, bringing artificial intelligence researchers one step closer to building an intelligent agent that can truly understand natural language.","title":"RI: Small: Semantics-Based, Weakly-Supervised Coreference Resolution","awardID":"1219142","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[518623],"PO":["565215"]},"193425":{"abstract":"Energy consumed by computing and communication devices has become a significant cost of<br\/>conducting business today. Indeed, as we move to higher speed networks and increase the<br\/>deployment of the Internet to all corners of the world, this cost is expected to grow dramatically.<br\/>Major contributors to the size and energy cost of the deployed infrastructure include enterprise<br\/>networks and data center networks. Enterprise networks are deployed in the workplace and have<br\/>been shown to be heavily under-utilized with peak utilizations typically a fraction of capacity.<br\/>Similarly, data center networks have also been shown to be very lightly loaded for significant<br\/>lengths of time. In both these cases, the energy consumed by the networking infrastructure is very<br\/>high, in relation to the actual data transmitted. The reasons for this are twofold. First, network<br\/>switches are designed to always run at peak rates even in the absence of traffic and second, switch<br\/>ports map one-to-one to end systems even though these systems load the ports at a tiny fraction of<br\/>available capacity. The outcome of this research project is a fundamental re-design of the network<br\/>switch in a way that preserves user experience but results in dramatic energy savings.<br\/><br\/>The key idea in this work is the development of a hardware device called a merge network, which<br\/>sits between the switch and connected systems. Traffic coming into the switch is merged together<br\/>and fed to a smaller number of ports allowing either the replacement of large port-density<br\/>switches with smaller switches or the powering off of large portions of the switch such as line<br\/>cards and memory banks. While the benefit of this idea is evident, the challenges in realizing it<br\/>are many. At the hardware level, the challenge is in merging traffic with minimal energy cost and<br\/>data loss. The approach adopted in this project is to build the merge network using analog<br\/>components with a digital control for managing Layer 2 protocol issues. Modern switches<br\/>implement a suite of Layer 2 protocols that expect a 1-1 mapping of switch ports to end-systems.<br\/>By breaking this mapping using a merge network, one effectively causes incorrect protocol<br\/>behavior. This issue is addressed by utilizing port virtualization within each switch. Thus, the<br\/>Layer 2 protocols remain unaffected while the network sees tremendous reduction in energy<br\/>usage. The project uses prototyping, experimentation and simulation to fully characterize this new<br\/>switch architecture in enterprise and data center networks. Experimental systems based on Click<br\/>Modular Router will be inserted into the College network to measure the energy savings in real<br\/>application as well as to understand side effects of such a redesign on overall network behavior.<br\/>Traffic statistics collected at a data center network and the University-level enterprise network<br\/>will be utilized in a simulator to analyze protocol behavior at a larger scale. This project is<br\/>transformative in that it redesigns the basic building block of the Internet resulting in dramatic<br\/>energy savings.<br\/><br\/>The broader impact of this work ranges from fundamentally redesigning the key component of<br\/>the Internet (the switch) to directly influencing the energy usage in the communications<br\/>infrastructure. The impact on industry is significant in that the work here will influence the design<br\/>of future high-speed switches where the merge network is incorporated into the architecture<br\/>rather than as an external add on. Traffic statistics collected during this project will be made<br\/>available to the research community and will thus provide a much-needed source of enterprise-level<br\/>traces. The research community will also benefit by the implementation of IEEE 802.1<br\/>protocols into the Click software base. The project will enhance the training of the future<br\/>workforce via the development of a new class and including students in hands on measurement<br\/>and development of the experimental platform.","title":"NeTS: Small: Merging Traffic for Energy Conservation in Enterprise & Datacenter Networks","awardID":"1217996","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[518079],"PO":["565090"]},"193546":{"abstract":"Optimization problems that are NP-hard, and thus conjectured not to have efficient algorithms, arise in many of the areas of human knowledge. Reconstructing phylogeny, finding market equilibrium, scheduling flights and detecting bottlenecks in networks are all examples of such problems. Algorithmists are often able to design algorithms that approximate the optimum efficiently. However, for many problems the approximation is quite crude. Moreover, finding better approximation often turns out to be NP-hard.<br\/><br\/>Understanding the approximability of NP-hard problems is intimately related to the deep theory of probabilistically checkable proofs (PCPs). This research aims at tackling a sequence of challenging open problems in PCP that include and generalize the Sliding Scale Conjecture of Bellare, Goldwasser, Lund and Russell from 1993. The latter conjecture is that a PCP verifier that uses r random bits can achieve soundness error that is exponentially small in r. Proving the conjecture will imply the first hardness results for polynomially large approximation factors. The other conjectures in the sequence, which consider special kinds of verifiers: projection, smooth, linear etc, imply more hardness of approximation results. <br\/><br\/>Proofs of PCP theorems often involve a large array of techniques: algebraic, combinatorial, information theoretic, coding theoretic and analytic. PI will employ and add to this toolbox. In particular, she will look into constructions of new low degree tests, composition methods, and amplification techniques for PCPs.<br\/><br\/>The broader impact contribution include the development of a comprehensive course in probabilistically checkable proofs, writing popular articles (e.g., about PCP), participation in the CS theory questions and answers website \"CS Stack Exchange\", and working with students of diverse backgrounds.","title":"AF: Small: Sliding Scale Problems in Probabilistic Checking of Proofs","awardID":"1218547","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[518359],"PO":["565157"]},"193315":{"abstract":"Interference in wireless networks often prevents nodes from receiving messages that are within transmission range. This is because communication occurs through broadcast channels rather than point-to-point links -- a node that is close to more than one transmitter receives a signal that is a mixture of the signals originally transmitted. Avoiding collisions basically requires some type of symmetry breaking among the nodes that want to transmit, to prevent them from transmitting at the same time, and has been the subject of many important studies.<br\/><br\/>This project will focus on wireless network scenarios in which the contention is bounded and introduce a framework for coping with collisions rather than avoiding them. It will consist of two main contributions. First, the PI will introduce a new coding framework for additive networks called Bounded-Contention Coding (BCC), which allows a receiver to uniquely decode colliding transmissions, and the PI will devise efficient codes within this framework. Second, the PI will design algorithms that use these codes for communicating in wireless networks, in order to solve many fundamental distributed problems. The PI will complement this study with corresponding lower bounds in order to understand the limitations on communicating and computing in such an environment. The PI will also consider several extensions and modifications of the basic model, determine their impact on the algorithms, and propose new solutions for problems in these models as well.<br\/><br\/>The results of this project can have impact on the design of real wireless networks, in which bounded-contention scenarios are common. For example, in a large-scale system of phones, only a few calls are typically active at one time. Another example is when nodes in a wireless sensor network receive measurement data from the environment periodically and send it to a central location in the network. The times at which the sensor nodes transmit their measurements could be scheduled in a way that bounds the number of concurrently active transmissions. In both of these examples, although the contention is limited, interference may still occur, implying the necessity to resolve colliding transmissions. For examples such as these, our coding methods and algorithms can lead to new, efficient implementations, and our lower bound results can lead to a better understanding of the inherent limitations.","title":"AF: Small: Bounded-Contention Coding for Wireless Networks","awardID":"1217506","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[517826],"PO":["565251"]},"193568":{"abstract":"The computing industry recently experienced a major shift in CPU architectures with the advent of multicore chips. This shift has necessitated the adoption of new programming models, algorithms, and analysis methods to fully exploit the parallelism inherent in multicore chip designs. While advances in these areas are well underway, industry has already begun yet another architectural shift towards systems with heterogeneous processing elements. Heterogeneity creates new challenges because the availability of different types of processing elements means that nontrivial choices must be made when allocating hardware resources to software components.<br\/><br\/><br\/>One of the most successful applications of heterogeneity today is in architectures in which powerful graphics processing units (GPUs) are used alongside general-purpose CPUs. Though originally intended as special-purpose graphics accelerators, GPUs are now being widely used for non-graphics processing in numerous application domains, including many domains in which real-time constraints (e.g., deadline requirements) exist. For example, envisioned automated automotive systems will require real-time sensing and tracking features that GPUs can accelerate. The goal of this project is to determine which resource allocation methods best facilitate the support of such real-time applications on heterogeneous platforms that may have multiple CPUs and GPUs. This goal is being met by undertaking a broad study of issues affecting the deployment and analysis of real-time applications implemented on GPU-enabled multicore platforms. Broader impacts include joint research with industry colleagues, and the development of publicly-available open-source software that can be used by other institutions for research and teaching purposes.","title":"CSR: Small: Real-Time Computing Using GPUs","awardID":"1218693","effectiveDate":"2012-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["31436",518412],"PO":["565255"]},"197935":{"abstract":"This award supports approximately 20 US-based students to attend the 2012 USENIX Security Symposium, the premier forum for advanced professionals from academic and industrial backgrounds to meet and discuss the newest advances in the security of computer systems and networks. Participation in Security 12 and similar conferences is a valuable and important part of the graduate school experience. It provides students with the opportunity to interact with more senior researchers in the field and exposes students to leading work in the field. This award will enable students to participate who would otherwise be unable to attend the conference. Security 12 will be held August 8-10, 2012, in Bellevue, WA. The conference will consist of a three-day technical program, with numerous workshops co-located. The goal of the conference is to bring together researchers, practitioners, system administrators, system programmers, and others interested in the latest advances in the security of computer systems and networks. This cross-disciplinary emphasis makes it well suited as a target for student participation, and this event has always drawn a large number of student attendees. Students receiving assistance will be strongly encouraged to present posters and works in progress as part of the selection process.","title":"Student Travel Support for the 21st USENIX Security Symposium 2012","awardID":"1246725","effectiveDate":"2012-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":["559051"],"PO":["564223"]},"198519":{"abstract":"This is funding to support a doctoral research symposium (workshop) of approximately 10 promising doctoral students from the United States and abroad (up to 2 international students), along with 5 high profile faculty and industrial researchers. The event will take place in conjunction with and immediately following the 14th ACM International Conference on Ubiquitous Computing (UbiComp 2012), to be held September 5-8, in Pittsburgh, and which is sponsored by the Association for Computing Machinery. The annual UbiComp conference is the premier international forum for the presentation and discussion of cutting edge research relating to both the technical and applied aspects of ubiquitous computing technologies, systems and applications. This is an interdisciplinary field of research and development that utilizes and integrates pervasive, wireless, embedded, wearable and\/or mobile technologies to bridge the gaps between the digital and physical worlds. Thus, the conference brings together researchers and practitioners from diverse areas that include human-computer interaction, pervasive computing, distributed and mobile computing, real world modeling, sensors and devices, middleware and systems, programming models and tools, and human-centric validation and experience characterization. More information about the conference may be found at http:\/\/www.ubicomp.org\/ubicomp2012. <br\/><br\/>The three goals of the full-day doctoral consortium are to increase the exposure and visibility of the participants' work within the community, to help establish a sense of community among this next generation of researchers, and to help foster their research efforts by providing highly constructive feedback and guidance from senior researchers in a supportive and interactive environment. To these ends, student participants will each make a formal presentation of their work to the group, with ample time allotted for questions and feedback from the faculty panel as well as from the other student participants. The feedback is geared to helping students understand and articulate how their work is positioned relative to other research, whether their topics are adequately focused for thesis research projects, whether their methods are correctly chosen and applied, and whether their results are appropriately analyzed and presented. Additional opportunities for more informal discussion and networking will be during the doctoral consortium's lunch and dinner events. Extended abstracts of the students' work (up to 4 pages in length) will be included in the supplemental proceedings which are distributed to all conference attendees.<br\/><br\/>Broader Impacts: The doctoral colloquium will help expand the participation of young researchers pursuing graduate studies in the various fields associated with ubiquitous computing, by affording them an opportunity to gain wider exposure in the community for their innovative work and to obtain feedback and guidance from senior members of the research community. It will further help foster a sense of community among these young researchers, by allowing them to create a social network both among themselves and with senior researchers at a critical stage in their professional development. Several participants in past UbiComp doctoral consortia have since gone on to high profile research careers. The event organizers have committed to accept no more than one student from any given institution, and they will make a special effort to attract students who are diverse across a number of dimensions (e.g., research interests, gender and ethnicity), so that the participants' horizons are broadened to the future benefit of the field.","title":"Workshop: The Doctoral Colloquium at UbiComp 2012","awardID":"1249461","effectiveDate":"2012-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[532662],"PO":["565227"]},"190863":{"abstract":"MASON is an open source massive multiagent simulation toolkit, that is, a set of software tools designed to make it easy to build simulations of large numbers of agents -- robots, people, organizations, animals, etc. -- interacting with one another in a complex fashion. Such toolkits are used by scientists to model everything from schools of fish, to swarms of flying robots, to the rise of historic trade routes in Asia. These kinds of models are becoming increasing popular both in the scientific and engineering communities as computer power is enabling the study of large numbers of \"agents\" to interact with one another in interesting and nontrivial ways. MASON is considered to lie at the \"high performance\" end of software of this type: it runs on everything from laptops to supercomputers. As such it has become a popular tool for simulating thousands, even potentially millions of agents interacting with one another. <br\/>MASON is now at a turning point in its development. Models are being developed which are pushing it to its limits, MASON's complexity is proving daunting to newcomers, and it is increasingly attractive to use MASON in the context of massive model optimization. A MASON workshop is intended to solicit community and expert feedback and input on extending and improving MASON in at least three potential ways: 1. A massively distributed version of MASON enables the library to harness large clusters of computers for even bigger and more costly models. 2. Connecting MASON with Integrated Development Environments, and developing a lightweight model development language can make it much easier for non-experts to use MASON. 3. Integration with large-scale stochastic optimization toolkits enables scientists to use MASON as a kind of modeler's assistant: the modeler specifies the high-level rules he believes to be true or wishes to test, and provides an assessment procedure for model output, and then the optimizer proceeds to \"fill in the gaps\", hunting for the remaining model rules which produce models which optimize this assessment procedure. Similar tools enable sweeps of model parameters. Output from the workshop will go towards development of a community-based plan and methodology to improve MASON in these and other aspects.<br\/><br\/>Tools and techniques like MASON are having an increasing and unusually broad impact on society at large: they're used in large economic and financial models, social models of terrorist networks, game development and special effects, air traffic control, and ecological models ranging from coral reef damage to the impacts of climate change.","title":"CI-P: Workshop on Enhancing a Large-scale Multiagent Simulation Tool","awardID":"1205626","effectiveDate":"2012-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[511673,"549627","541913",511676],"PO":["565272"]},"192964":{"abstract":"Extensive work over the last decade, done within theoretical computer science, has provided deep insights into the computability of market equilibria for various market models and utility functions, using the powerful tools of the modern theories of algorithmic design and algorithmic complexity. This follows up on a century-long work, within mathematical economics, on obtaining a mechanism that converges to equilibrium -- a goal that had to be eventually abandoned due to certain negative results on efficient computability of the equilibrium. The work in TCS was motivated in part by applications to markets on the Internet. <br\/><br\/>The current project will extend this work along several exciting directions. Recent work of the lead Principle Investigator (PI) on using complementary pivot algorithms, for obtaining usable algorithms for certain market models that are unlikely to have efficient algorithms in the usual sense of polynomial worst-case running time, opens up the possibility of extending this approach to broader classes of markets, in particular, markets with production. A major new challenge is to address dynamically evolving markets. <br\/><br\/>In terms of applications of markets, the team brings to this project a wealth of experience on electricity markets, gained from work done with researchers in computer science and control dynamics. The PIs plan on bringing their expertise in mechanism design to bear on the problems of integrating renewable energy sources into the smart grid and providing better approaches to the pricing and allocation of ancillary services to guarantee reliability and stability. Another new challenge is to extend general equilibrium theory, the undisputed crown jewel of mathematical economics, to the digital economy. The traditional notion of equilibrium is not applicable to digital goods -- once produced, an unbounded number of copies of such goods are available. The digital realm is very rich and is increasingly occupying a larger share of our economy. It is imperative, therefore, to achieve the same depth of understanding of pricing for digital goods as was obtained for conventional goods. <br\/><br\/>This project will provide algorithms and insights into the computational aspects of markets, including electricity markets and transactions on the Internet, thereby helping make their operation more efficient. Hence, it is expected to contribute to advances in science and engineering, as well as to promote economic prosperity.","title":"ICES: Large: Collaborative Research: Markets, Algorithms, Applications and the Digital Economy","awardID":"1216019","effectiveDate":"2012-08-01","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8052","name":"Inter Com Sci Econ Soc S (ICE)"}}],"PIcoPI":[516928],"PO":["565251"]},"190797":{"abstract":"This project provides infrastructure to support research concerning the investigation, design, and integration of reusable sensory-motor services that proactively monitor patient health status and provide flexible tools for intervention. The shortage of skilled allied healthcare professionals and their finite capacity prevents the continuous monitoring of patient vital statistics. Leveraging an aging population?s preference to remain at home while managing chronic illness, this project enables research into the use of machine perception and sensory motor learning for recognition and intervention of degrading health status in a home care environment.<br\/><br\/>The investigators are building a new infrastructure consisting of compute nodes, robotic platforms, sensing systems, and novel service framework for research and education in Machine Perception and Sensory Motor Learning for healthcare applications at Delaware State University. By factoring perceptual systems and making them accessible services on the network, a robot becomes a virtual assemblage that recruits arbitrary resources at run-time. Such service compositions provide a richer suite of perceptual and robotic services because it draws from the power-set of available sensory-motor apparatus. Service compositions include methods, such as ensembles in machine learning, that construct high fidelity models for pattern learning by combining a number of simpler component models. <br\/><br\/>The infrastructure built from this project enables research in many areas, such as perceptual computing, robotics, and machine learning. The broader impact of the project includes more proactive tools for detection and intervention in healthcare and training of undergraduate and graduate students (including underrepresented students in STEM) in interdisciplinary research.","title":"II-NEW: Infrastructure for Research and Education in Machine Perception and Sensory Motor Learning for Home Health Care","awardID":"1205426","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["528676"],"PO":["564316"]},"192986":{"abstract":"The fields of Algorithmic Game Theory and Mechanism Design study the strategic interaction of multiple agents. The growth of available data and computational power enable these fields to make a real impact in understanding the agents' incentives and improving their decisions and the resulting social outcomes. A grand challenge is the presence of uncertainty and risk, which calls for new game theoretic models and algorithmic techniques. As a brief example, consider going to the airport to catch a flight. In uncertain traffic and a deadline, risk considerations are critical and a less risky, albeit longer, route might be preferable over one that minimizes expected delay. But the fundamental questions do not depend merely on one individual's preferences, but on a broader social model that captures many individuals and their interactions. For instance, how does strategic risk-aware routing by multiple users affect route selection and the overall congestion in the network? And how different is the individually optimal (equilibrium) route selection of multiple risk-averse users from a socially optimal traffic allocation?<br\/><br\/>This research program outlines an agenda for investigating how uncertainty and risk aversion transform traditional models and methodologies employed by Algorithmic Game Theory and Mechanism Design. It will import models and tools from areas with a rich tradition in studying risk, such as Financial Engineering, and adapt them to multi-user computational environments. Ultimately, the goal is to develop new game theoretic models and analysis, as well as new algorithms for solving the computational questions in these models. <br\/><br\/>The program focuses on problems that arise in complex multi-user systems in the real world and has the potential to improve a variety of applications that involve uncertainty and risk-averse users, for example, reduce congestion in transportation and telecommunication networks, enhance the quality of streaming applications over the Internet, increase the safety and security in military operations, autonomous and robot navigation, etc. From a research standpoint, the transformative potential of the proposed research is to fundamentally shift thinking about uncertain multi-user environments away from optimizing simple expected performance and instead towards a systematic treatment of contingencies and risk.","title":"ICES: Small: Risk Aversion in Algorithmic Game Theory and Mechanism Design","awardID":"1216103","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8052","name":"Inter Com Sci Econ Soc S (ICE)"}}],"PIcoPI":["554523"],"PO":["565251"]},"193525":{"abstract":"The exponential growth of mobile data is a major challenge to the operators of cellular networks. Looking beyond conventional capacity-improving approaches such as adding more cells and acquiring more spectrum, this project seeks to fundamentally improve the spectral efficiency of cellular data networks. The project investigates LAWN (Large number of Antenna based Wireless Networking), a radically new cellular network architecture, in which a large number of antennas simultaneously serve a relatively much smaller number of wireless terminals using multiuser beamforming. The project has two inter-related thrusts. The first investigates a novel LAWN base station design and prototype that can cost-effectively scale to hundreds of antennas and exploit physical-layer tradeoffs between computational complexity and network capacity. The second thrust studies the resulting new network architecture that efficiently schedules terminals, intelligently allocates transmission power, and coordinates pilot signal transmissions to mitigate inter-cell interference.<br\/><br\/>The project targets improving the spectral and power efficiency of cellular networks by many fold, leading to not only fast wireless data networks but also longer battery lifetime of mobile terminals. Results from the project are likely to provide fresh insights for new theoretical development, bringing large-scale multi-user beamforming one significant leap closer to practical deployment in cellular data networks. In addition to academic publications, the project will produce an open platform, including hardware, software, and documentation available on-line, for teaching and researching base station design. It will actively involve undergraduate students as well as students from under-represented populations.","title":"NeTS: Small: Collaborative Research: LAWN: Scaling Up Cellular Data Networks using a Large Number of Antennas","awardID":"1218457","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["555996"],"PO":["565303"]},"193415":{"abstract":"This project develops an axiomatic theory of hierarchical clustering for asymmetric networks as is typical of trust propagation. Say, for example, that Miranda trusts Billy who trusts Ariel who trusts Miranda, but there has not been enough interactions in the opposite direction to establish trust. When these three people meet, shall they trust each other? The answer to this question is equivalent to the determination of whether Miranda, Billy, and Ariel are part of the same cluster: their circle of trust. The axioms of value, influence, and transformation are postulated. The Axiom of Value says that in a two-node network the nodes cluster at resolution equal to the maximum dissimilarity between them. The Axiom of Influence says that no clusters are formed at resolutions that do not allow bidirectional paths to be formed. The Axiom of Transformation states that if we consider a network and reduce all pairwise dissimilarities, the level at which two nodes become part of the same cluster is not larger than the level at which they were clustered together in the original network. Generic properties of any method that satisfies these axioms are explored and specific methods that abide by these axioms are derived. To enrich the axiomatic exploration of asymmetric clustering an application thrust and three theory thrusts are pursued. The application thrust explores the formation of circles of trust in social networks and the design of protocols to establish trust in technological networks. The theory thrusts will study alternative axiomatic formulations, stability of asymmetric hierarchical clustering algorithms, and the determination of algorithms to compute hierarchical clusters.<br\/><br\/>The educational agenda is integrated into the Market and Social Systems Engineering (MKSE) program at the University of Pennsylvania. The MKSE program is an undergraduate course of study that fully integrates the disciplines needed to design and analyze the complex networks that are reshaping our society. Given the importance of trust in these networks the research undertaken in the context of this project is incorporated into classes in the MKSE program. The excitement an idea like the formalization of trust propagation generates is further exploited to draw attention to the MKSE and Systems Engineering programs from the wider academic community. These ideas are part of a long term effort on the part of the PI to contribute to the closing of the excitement, challenge, and discipline gaps.","title":"CIF: SMALL: Circles of Trust: An Axiomatic Construction of Clustering in Asymmetric Networks","awardID":"1217963","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["541883"],"PO":["564924"]},"194867":{"abstract":"Proposal #: 12-29306<br\/>PI(s): Mahmood, Akhtar H; <br\/> Alam, Mohammad S; Severini, Horst; Zain, Samya; <br\/>Institution: Bellarmine University<br\/>Title: MRI\/Acq.: High Resolution Visualization System to Enable Large-Scale Data-Intensive Collaborative Research and Cyber-Learning using Grid Computing<br\/>Project Proposed:<br\/>This project from an EPSCoR state and a non-PhD granting institution, acquiring a high resolution visualization instrument, aims to set up an Advanced Visualization and Computational Lab (AVCL) to enable large scale data-intensive collaborative research (data analyses, simulation, and visualization tasks) in high energy physics. In collaboration with University of Oklahoma, State University of New York at Albany, and Susquehanna University, the project spans four geographically dispersed institutions. The acquisition targets a cost-effective high-resolution tiled display system providing a collaborative visualization environment. This visualization system will be constructed as a tiled display wall (16- foot wide by 4.6-foot tall) over an area that fills a user?s field of view to provide a display that looks nearly borderless. It will be connected to several Tier-4 data analyses workstations via a wall processor, so that the scientists can simultaneously display multiple ATLAS data events all at once. 2D and 3D visualization tasks will be carried out using Atlantis, GraXML and VP1 visualization software package. The studies and research projects in high energy physics will be enabled, including, but not limited, to the research for new types of subatomic particles, such as the charged Higgs boson and Supersymmetric (SUSY) particles at the Large Hadron Collider (LHC), and ?Search for Charged Higgs Boson from tt Production?. <br\/>Broader Impacts: <br\/>This instrumentation increases institutional capacity to conduct cutting-edge research in cost-effective visualization system. Since the large amount of data cannot be otherwise analyzed and studied, visualization is fundamental for high energy physics. The acquisition of this visualization system should significantly increase the research capability within all the participating institutions. Moreover, the acquisition of this visualization system will serve as a campus-wide resource and provide an exceptional opportunity to engage students in research at the forefront of data-intensive computational science, visualization, and high-energy physics. Through this effort, student training (including underrepresented groups) will be emphasized, as well as the knowledge dissemination. Its impact may be significant.","title":"MRI: Acquisition of a High Resolution Visualization System to Enable Large-Scale Data-Intensive Collaborative Research and Cyber-Learning using Grid Computing","awardID":"1229306","effectiveDate":"2012-08-15","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[522160,522161,522162,522163],"PO":["557609"]},"193657":{"abstract":"Automatic Content Identification is an emerging technology that has found applications to broadcast monitoring, connected audio, content tracking, digital asset management, near-duplicate identification, contextual advertising, and as a filtering technology for file sharing. Content identification algorithms must be robust to common signal degradations. They operate on highly compressed data (robust hashes, aka content fingerprints) to meet storage, communication, and computing constraints.<br\/><br\/>The goal of this project is to develop an analytical framework for content identification based upon fundamental principles and modern methods of statistical inference and information theory and to develop novel content identification algorithms. The project focuses on the following four research topics:<br\/><br\/>1. Hash-Based Inference.<br\/>2. Information-Theoretic Analysis: Content identification is formulated as a communication problem with storage constraints and its fundamental performance limits are investigated.<br\/>3. Code Design: A learning-theoretic approach is developed for statistical modeling of content fingerprints and degradation channels from training data, and for designing hashing codes and decoding metrics that are optimally matched to these statistics.<br\/>4. Applications: to audio, images, and video are explored, as well as forensic analysis and security.<br\/><br\/>The project is synergistic and trains graduate students for leadership roles in information technology. The project benefits other areas, including but not limited to content retrieval, clustering, database indexing, pattern recognition, biometrics, and human\/computer interaction.","title":"CIF: Small: Theory and Algorithms for Statistical Content Identification","awardID":"1219145","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":[518625],"PO":["564898"]},"194757":{"abstract":"This project is developing tools and techniques for cost-effective evaluation of the trustworthiness of mobile applications (apps). The work focuses on enterprise scenarios, in which personnel at a business or government agency use mission-related apps and access enterprise networks.<br\/><br\/>In such scenarios there are incentives and resources for much more substantive evaluations and controls on information flow than are currently found in commodity app marketplaces. The project aims to advance the science needed for static techniques to be usable by professional development and evaluation teams and useful for achieving dramatically improved assurance. The project's goals are to: (a) find flexible and expressive ways to specify information flow requirements for apps, (b) find effective ways to specify what is assumed about the Android platform, and (c) find practical static analysis and verification techniques to check security of apps with respect to given policies and the platform. Results include specification techniques and theory - models and algorithms. These are applied in case studies with prototype tools that the project develops, to evaluate how well the goals are achieved.<br\/><br\/>The project's techniques can be deployed by certification organizations to provide scientifically sound techniques for assurance, thus<br\/>enabling the full benefits of highly-integrated mobile software in mission-critical situations. Software designers will benefit from being able to precisely specify end-to-end requirements as well as component interfaces. Software developers will benefit from reliable means to detect design flaws and bugs, malware in third-party software, and unintended functionality that exposes vulnerabilities. Beyond the specific target of mobile software, the techniques will be of use in other settings, especially web applications, where it is crucial to reason about interfaces between mutually untrusting parties making heavy use of callbacks. The project could help improve security in government agencies and private sector, indirectly benefitting national security and the general population.","title":"TWC: Medium: Collaborative: Flexible and Practical Information Flow Assurance for Mobile Apps","awardID":"1228930","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"8060","name":"Secure &Trustworthy Cyberspace"}}],"PIcoPI":[521696],"PO":["549626"]},"192337":{"abstract":"Millions of people used social media technologies to organize themselves to produce information and artifacts of value, such as Wikipedia, GNU\/Linux, and PatientsLikeMe. However, these communities have significant problems: (1) They work so well to let people maintain existing relationships and find others who are similar to them that they can promote self-segregation and opinion polarization. (2) Knowledge production has been democratized, but at the price of devaluing expert participation. (3) They open people to fresh ideas from far-off places, but few sites incorporate the rich local knowledge of home communities. This project addresses these problems by creating algorithms and user interfaces that nudge individuals and communities into more effective patterns of social connections and interaction. <br\/><br\/>The intellectual merit includes: (1) Identification of principles people use to build their social networks, the range of potential contacts they consider, and the contexts in which they encounter potential contacts. (2) Empirical knowledge of the effectiveness of different social network structures for solving a range of realistic decision-making tasks. (3) Novel social recommendation algorithms that maximize network effectiveness and that recommend interaction opportunities that serve as contexts for communication and connection.<br\/><br\/>Broader impacts: This research will be performed in the context of two domains of societal interest, parenting and bicycling, enabling significant broader impacts. Providing people information that gives them confidence to bicycle more increases their personal health, decreases air pollution, and improves community cohesion. Research shows that parents with larger and more diverse social networks make better parenting decisions, and that parents also seek expert advice for specific issues. Therefore, a system that helps people form effective social networks and that provides access to expert educators offers direct benefits to parents. Better parents also tend to be more involved in their communities; thus, improved community health is a bonus effect. Finally, through the research team's connections with parent educators, the project will explicitly target parents from low-income families, who have the greatest needs for support and information.","title":"SoCS: Collaborative Research: Novel Algorithms and Interaction Mechanisms to Enhance Social Production","awardID":"1212338","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7953","name":"SOCIAL-COMPUTATIONAL SYSTEMS"}}],"PIcoPI":[515452],"PO":["565342"]},"193437":{"abstract":"Many deaf and hard of hearing students use real-time captioning to participate in education. Generally, real-time captions are provided by skilled professional captionists (stenographers) who use specialized keyboards or software to keep up with natural speaking rates of up to 225 words per minute. But professional captionists are expensive and must be arranged in advance in blocks of at least an hour. Automatic speech recognition (ASR) is improving, but still experiences high error rates in real classrooms. In this collaborative effort involving the University of Rochester and Rochester Institute of Technology, the PIs will address these issues by blending human- and machine-powered captioning to produce captions on demand, in real time, for low cost. The PIs' approach is for multiple non-experts and ASR to collectively caption speech in under 5 seconds, with the help of interfaces which encourage quick, incomplete captioning of live audio. Because non-experts cannot keep up with natural speaking rates, new algorithms will merge incomplete captions in real time. (While the sequence alignment problem can be solved exactly with dynamic programming, existing approaches are too slow, are not robust to input error, and do not incorporate natural language semantics.) Systematically varying audio saliency will encourage complete coverage of speech. Non-expert captions will train ASR engines in real time, so that ASR may improve during a lecture. (Traditional approaches for ASR training assume that training occurs offline.) The quikCaption mobile application will embody these ideas and will be iteratively designed with deaf and hard of hearing students at the National Technical Institute of the Deaf (NTID) via design sessions, lab studies and in-class deployments. Non-expert captionists can be drawn from broad sources: volunteers willing to donate their time, classmates with relevant domain knowledge, or always-available paid workers. They may be local (in the classroom) or remote. Captionists may have experience from prior quikCaption sessions, or novice crowd workers recruited on demand from existing marketplaces (e.g., Mechanical Turk). A flexible worker pool will allow real-time captions to be available on demand at low cost and for only as long as needed.<br\/><br\/>Broader Impacts: This research will dramatically improve education for deaf and hard of hearing students by enabling access to serendipitous opportunities, such as conversations after class or last-minute guest lectures for which no interpreter or captionist was arranged. Real-time captioning will also be useful in other settings such as school programs, artistic performances, and political events. Older hard of hearing adults usually prefer captioning, and represent a sizable and growing population; hearing people may benefit because captioning is a first step in automatic translation of aural speech. The algorithms developed as part of this project for real-time merging of incomplete natural language will likely be adaptable for other applications such as collaborative translation or communication over noisy mediums.","title":"HCC: Small: Collaborative Research: Real-Time Captioning by Groups of Non-Experts for Deaf and Hard of Hearing Students","awardID":"1218056","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[518104],"PO":["565227"]},"196759":{"abstract":"This proposal requests funds to support student travel to the IEEE International Conference on Software Maintenance (ICSM 2012) that will be hosted in Trento, Italy in Sept 2012. This is an international conference and a top venue for topics related to software development lifecycle. The conference is an educational opportunity for the students and will help build the next generation of researchers in this field. The international experience has the additional broader impact of building a global workforce for science and engineering.","title":"Supporting student travel from underrepresented groups to the 28th IEEE International Conference on Software Maintenance (ICSM 2012)","awardID":"1240505","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["534438"],"PO":["564388"]},"190820":{"abstract":"The OpenRAM project aims to provide an open-source memory compiler framework for Random-Access Memories (RAMs). Most academic Integrated Circuit (IC) design methodologies are inhibited by the availability of memories since commercially available memory compilers are often black box, not modifiable, and have limited available memory configurations. However, memories are the largest barrier to future scaling and require urgent solutions to lower leakage power consumption, improve reliability in shrinking supply voltages, and incorporate post-CMOS technologies. The OpenRAM compiler will enable research in these areas by providing a completely modifiable, verified, technology independent compiler for single-port and multi-port RAMs and many-port register files. The availability of the OpenRAM will enable research in a variety of IC-related disciplines<br\/>including: computer architecture and system-on-chip (SOC) design, memory circuit and device research, and computer-aided design (CAD).<br\/>Computer architects and SOC designers need access to a variety of memory configurations including specialized many-ported register files to prototype system-level implementations. Similarly, memory circuit and device researchers need a framework to prototype new circuits and devices in the context of these many configurations. Last, CAD researchers need a framework to study the optimization and analysis of power and yield of on-chip memories. In addition to the broader impact of enabling future memory system scaling, OpenRAM will also be an important education tool by allowing anyone to synthesize and utilize memory macros in standard-cell design flows and implement custom extensions.","title":"Collaborative Research: CI-ADDO-NEW: An Open Memory Array Compiler Framework to Support Devices, Circuits and Systems Research","awardID":"1205493","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":[511551],"PO":["565255"]},"190897":{"abstract":"The California Social Science Experimental Laboratory (CASSEL) is perhaps the world's most advanced research laboratory devoted to experimental work in the Social Sciences. CASSEL has been and continues to be used for research and training by faculty and graduate students from UCLA (Departments of Economics, Political Science and Anthropology, Anderson Graduate School of Management, School of Law) and Caltech (Division of Humanities and Social Sciences) and many other institutions both nearby and far away, including Brown University, Claremont Center, UC Irvine, and the University of Beijing. The Division of Social Sciences at UCLA has made a long-term commitment to support CASSEL, providing space, maintenance, operating funds. In this proposal the PI is asking NSF to provide funds for major equipment upgrades (server, subject computers, and ancillary equipment) in order to continue and expand CASSEL's vital role in experimental work in the Social Sciences. Equipment in CASSEL is in need of upgrade for a number of reasons: 1) Computers are due for replacement as they are close to the end of their normal useful lives (current subject computers and servers are not fast enough to carry out the computation necessary to conduct some experiments); 2) In order to run large experiments, the method of subject signup and payment needs to be expedited (to do this, the lab needs to set up electronic signup and payment systems); 3) Current equipment does not permit monitoring of communications between subjects or the monitoring of such communications; 4) CASSEL?s outreach capabilities need to be enhanced so that experiments can be run remotely; and 5) Ancillary equipment in CASSEL including projectors, copiers, etc. is reaching or has already exceeded its normal useful life.<br\/><br\/>Intellectual Merit<br\/>Experiments are used to develop and improve the organization and performance of private and public institutions. The requested funds will yield a high return on investment because, as documented in detail in the Project Description, CASSEL is extensively used by a large number of students and researchers from a wide range of institutions. The proposed enhancement will maintain and expand the usability of CASSEL for broadly-based communities of researchers and educators that extend well beyond our institution, so that it becomes the premier social science research laboratory in the world.<br\/><br\/>Broader Impact<br\/>CASSEL is organized around the \"shared resource\" principle. It provides a user-friendly facility enabling researchers without laboratory access to become involved in experimental research, and enabling all researchers to engage in large-scale experiments. It is both an interdisciplinary and an inter-university facility. One indication of the lab's outreach is that in the last two years over 50% of the usage was by non-UCLA researchers. Research conducted at CASSEL broadens our understanding of decision-making, markets and institutions. This research leads to advances in theoretical models and in the connection of those models to empirical data, and to advances in experimental methodology. In addition, CASSEL is an important training ground for graduate student researchers, and hosts large numbers of freshmen seminars using experimental methods. Finally, CASSEL plays an important role in developing and testing of new experimental software (such as Multistage and jMarkets) and recruiting\/scheduling software that is used in many other experimental laboratories and serves to facilitate experimental research in social science.","title":"CI-ADDO-EN: Enhancement and Operation of the California Social Science Experimental Laboratory - CASSEL","awardID":"1205748","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"1320","name":"ECONOMICS"}}],"PIcoPI":[511780,511781],"PO":["565227"]},"192965":{"abstract":"Networking research on Internet Quality of Service techniques has resulted in standards that have been widely incorporated into routers. And yet, use of Quality of Service practices is very limited. Internet Service Providers have implemented Quality of Service practices to support their own voice and video services, but do not offer Quality of Service techniques to their subscribers for use by any other Internet applications. Internet Service Providers have not incorporated Quality of Service practices into their agreements with other Internet Service Providers, and hence Quality of Service techniques are not available across the Internet. The limited deployment of Quality of Service practices has ignited vigorous debate over Net Neutrality, one of the most contentious telecommunications public policy issues in decades.<br\/><br\/><br\/>This project will address the lack of availability of Quality of Service techniques widely across the Internet. It will adopt an interdisciplinary approach that integrates network architecture, economics, and law. The project will have three goals. First, it will create and analyze network architecture and economic models that can give insight into when an Internet Service Provider will choose to offer Quality of Service techniques to application providers other than itself. Second, it will consider how communications law may affect the development of Quality of Service practices. Third, it will illustrate potential technical and economic arrangements between users, Internet Service Providers, and application-providers that could enable widespread deployment of Quality of Service practices in a manner consistent with new laws regulating traffic management. <br\/><br\/><br\/>It is hoped that the results will encourage the widespread deployment of Quality of Service practices and thus enable further development of novel Internet applications. The research will also encourage interdisciplinary teaching of networking technology, economics, and law through further development of a novel undergraduate course on The Internet and Public Policy. The research, if successful, will inform ISPs and regulators, and potentially improve both the functional character of the Internet and the revenue opportunities of the ISPs.","title":"ICES:Small: An architectural, economic, and legal approach to encouraging end-to-end Quality of Service in the Internet","awardID":"1216023","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8052","name":"Inter Com Sci Econ Soc S (ICE)"}}],"PIcoPI":[516930],"PO":["565251"]},"191799":{"abstract":"A unique interdisciplinary team of computer scientists, information scientists, ornithologists, project managers, and programmers will develop a novel network between machine learning methods and human observational capacity to explore the synergies between mechanical computation and human computation. This is called a Human\/Computer Learning Network, and while the focus is to improve data quality in broad-scale citizen-science projects, the network has the potential for wide applicability in a variety of complex problem domains. The core of this network is an active learning feedback loop between machines and humans that dramatically improves the quality of both, and thereby continually improves the effectiveness of the network as a whole. The Human\/Computer Learning Network will leverage the contributions of broad recruitment of human observers and process their contributed data with artificial intelligence algorithms leading to a total computational power far exceeding the sum of their individual parts. This work will use the highly successful eBird citizen-science project as a testbed to develop the Human\/Computer Learning Network. eBird engages a global network of volunteers who submit tens of millions of bird observations annually to a central database.<br\/>This research addresses three fundamental data quality challenges in citizen-science. These are: 1) reducing errors in identification or classification of objects; 2) identifying and quantifying the differences between individual observers; 3) reducing the spatial bias prevalent in many citizen-science projects. To address these challenges, the project will build on advances in artificial intelligence that now provide the opportunity to study systems through the generation of models that can account for enormous complexity. Preliminary work on observer classification will be extended by developing new multi-label machine learning classification algorithms that provide better ecological interpretations and more accurate predictions. In addition, the research will develop new active learning algorithms by constructing sampling paths that will optimize volunteer survey efforts to maximize overall spatial coverage, and incentivize participation via crowdsourcing techniques. Finally, it will study how participants can improve the quality of their observations based on the feedback and information provided by the artificial intelligence. <br\/><br\/>Broad-scale citizen-science projects can recruit extensive networks of volunteers, who act as intelligent and trainable sensors in the environment to gather observations. Artificial intelligence processes can dramatically improve the quality of the observational data that volunteers can provide by filtering inputs based on observers' expertise, a judgment that is based on aggregated historical data. By guiding the observers with immediate feedback on observation accuracy and customization of observation worksheets, the artificial intelligence processes contribute to advancing expertise of the observers, while simultaneously improving the quality of the training data on which the artificial intelligence processes make their decisions. The results of the project will have significant benefit for all citizen science and broader impact in an emerging world of ubiquitous computing in which human-machine partnerships will become increasingly common.","title":"SoCS: Collaborative Research: A Human Computational Approach for Improving Data Quality in Citizen Science Projects","awardID":"1209714","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7953","name":"SOCIAL-COMPUTATIONAL SYSTEMS"}}],"PIcoPI":[514058],"PO":["564456"]},"193636":{"abstract":"Society increasingly depends on networks in general and the Internet in particular for just about every aspect of daily life. As the Internet increases its reach in global scope, services traditionally implemented on separate networks are increasingly subsumed by the Internet, either as overlays, via gatewayed access, or as a replacement for legacy networks. With this growing dependence on and integration of services in the Internet, come increasingly severe consequences of disruption in networked services.<br\/><br\/>This collaborative project conducts systemic research on fundamental understanding on network design for massive failures or attacks such as power blackouts and natural disasters, as well as attacks against the physical, and protocol infrastructure by intelligent adversaries. The novelty of the work is a systemic approach to designing resilient and survivable networks by developing a network topology generation, design, and analysis methodology that incorporates a number of critical factors, with an optimization formulation that incorporates geodiversity. Methodologies will assess vulnerabilities of existing networks and to propose modifications to make them more resilient and survivable. Furthermore, a novel approach is taken to understand resilient transport for massive failures in a traffic-engineering framework so that applications can exploit the structural diversity introduced in the network.<br\/><br\/>Broader Impact: The research aims to gain new understanding on how to design and deploy resilient networks, with emphasis on surviving massive disruptions as a result of large-scale disasters and coordinated attacks. The specific techniques and mechanisms may be applied to improve the resilience of the current Internet and its parts to better serve society, as well as to help drive the deign of the Future Internet. The research will advance the state-of-the art in network science and engineering. The principal investigators will make all results, including analytic and simulation models and source code, publicly available to the research community on a public wiki. Results will be disseminated in conferences, journal papers, and books. The project will support the research of two Ph.D. students with an effort to recruit students from under-represented groups. The project results will contribute to the education of other students, with direct input and class project participation in several courses at the undergraduate and graduate levels, including a senior undergraduate and graduate special topics courses on Science of Communication Networks and Resilient and Survivable Networks at both The University of Kansas and the University of Missouri-Kansas City.","title":"NeTS: Small: Collaborative Research: Resilient Network Design for Massive Failures and Attacks","awardID":"1219028","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["536602"],"PO":["565090"]},"196925":{"abstract":"This grant provides support to broaden participation in the Twenty-first International Conference on Parallel Architecture and Compilation Techniques (PACT12), to be held in Minneapolis, Minnesota on September 19-23, 2012. PACT is the premiere forum to bring together researchers from architecture, compilers, applications and languages to present and discuss innovative research of common interests. The grant will primarily fund US citizens\/residents, and the highest priority will be given to under-represented groups. Attending the conference is an important educational\/training experience for the participants and helps create the next generation of researchers in this area.","title":"Student Travel Support for the Twenty-First International Conference on Parallel Architectures and Compilation Techniques (PACT), 2012","awardID":"1241490","effectiveDate":"2012-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["539562"],"PO":["564388"]},"193537":{"abstract":"The success of cloud computing as a business model has led to several commercial cloud services for storage, data mining and searching, and multimedia content delivery. As cloud services penetrate emerging markets -- notably the smart-phone market -- conventional utility services such as television are becoming ?cloudized.? <br\/><br\/>As the economy becomes more dependent on the efficient operation of cloud data centers (DCs), there?s an increasingly urgent need to design ?green? and energy-adaptive management software that optimizes the operation of a geographically distributed set of data centers under varying workload and energy conditions.<br\/><br\/>Cyber-physical, proactive and predictive decision making is essential for a DC operating system that efficiently predicts, e.g., the workload and the power consumption when running multiple jobs on the same server and makes decisions, e.g., for redistributing the servicing virtual machines among varying number of available DCs, electricity cost, percentage of renewable energy supply and capacity of energy storage. <br\/><br\/>This project is addressing the scientific and engineering challenges involved in decision making of such DC management arising from an array of complicating factors -- e.g., server heterogeneity, cyber-physical interactions between DCs and their environment, varying workload and electricity pricing -- and trade-offs -- e.g., energy-delay, time-accuracy, and cooling-computing . The project is developing a Green cyber-physical Data Center Simulator, GDCSim, for studying the tradeoffs in practical scenarios with several complicating factors. <br\/><br\/>Project aims to enable a more sustainable cloud computing and to train engineers for a greener economy. The project findings are available through ASU-IMPACT lab?s website (http:\/\/impact.asu.edu\/).","title":"CSR: Small: Understanding and Modeling the Trade-Offs in Data Centers for Next-Generation Sustainable Management","awardID":"1218505","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["535238",518338],"PO":["565255"]},"193427":{"abstract":"At the heart of any computer program is the data that the program is designed to manipulate. Data, in turn, is only useful when organized into some sort of structure ---be it as simple as a list or as complex as a network or hierarchical tree. The mathematical theory of \"combinatorial species\" studies many variations of such structure, but it has not often been applied in a computer science context. The goal of this project is a new understanding of data structure through the abstractions of combinatorial species. This understanding will lead to new classes of data structures as well as practical tools for working with existing ones. Ultimately, this work will lead to more expressive programming languages, allowing computer programs to be written at a higher level and with fewer errors.<br\/><br\/>More specifically, the project seeks to extend the existing computational theory of algebraic data types with constructs derived from the theory of combinatorial species. In particular, the project will explore the specification of data types with nontrivial symmetries and sharing, such as cycles, bags, and simple graphs. The project will develop libraries and language extensions to support the use of species data types in the Haskell programming language. As a significant test case, it will explore the application of species data types to property-based testing. Although the work is to be carried out in the context of the Haskell programming language, the results will be applicable to any language, including those with direct support for algebraic datatypes (such as OCaml, F#, Racket or Scala), and those without. All libraries and tools developed under this project will be made available as open source.","title":"CCF-SHF Small: Beyond Algebraic Data Types: Combinatorial Species and Mathematically-Structured Programming","awardID":"1218002","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["550615"],"PO":["564588"]},"193559":{"abstract":"Interference Alignment is a breakthrough paradigm for wireless networks where several users share the same channel resources. Today's commercially available networks are built so as to avoid interference, so that a given resource is used by only one user at a time. With interference alignment, all users access the channel simultaneously and each user obtains half the interference-free rate, thereby achieving a network throughput that scales linearly with the number of users. The two most commonly studied approaches to interference alignment are the linear and non-linear schemes. In the former case the alignment problem can be formulated as that of solving an overdetermined system of equations with respect to a subset of the unknowns, and can be cast into the familiar language of vector spaces. The latter requires deep results in number theory on approximating an irrational number by a rational one. At present no unifying framework exists that encompasses both approaches. This research uses the theory of modules (on residue rings or polynomial rings) from modern algebra as a unifying framework for interference alignment. First the framework is introduced and shown to reduce to well known linear and non-linear interference alignment by choosing proper rings. Then, channels are classified in terms of their submodule decomposition and their degrees of freedom evaluated. Finally, actual codes are designed through a novel method where a destination decodes the lattice code that is the closet (in a nested lattice chain) to the received signal. This latter task has connections with the recently proposed compute-and-forward method of relaying. Connections among the two are also explored.<br\/><br\/>The central impact of the proposed work stems from its unified theoretical treatment of interference alignment. No such unified framework or methodology exists at present. Although it has been often observed that classical linear algebra tools are insufficient for network problems, the theory of modules has not emerged so far as a tool to explain, analyze and design codes for interference networks. This research, by exploring novel foundations of interference alignment, will provide the community with an extremely rich toolset to design and analyze codes, which is expected to impact and benefit other network problem such as network coding and compute-and-forward relaying.<br\/>From a technical perspective the new paradigm proposed in this research will enlarge the scope of information processing in interference networks, and provide an important step forward into the consolidation of a comprehensive theory of network science for distributed, decentralized, interfering networks. The results of this research will be presented at major national and international professional venues, in the information theory and communication networks communities and are expected to be of immediate use to the industrial sector.","title":"CIF: Small: Modules as a Framework for Interference Alignment in Networks","awardID":"1218635","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":[518391],"PO":["564924"]},"197816":{"abstract":"Response of cells to their changing environment is governed by intricate regulations of gene expression by regulating molecules in cells including, most importantly, transcription factors (TFs) and microRNAs (miRNAs). Understanding how TF and miRNA regulations define cellular states such as cell survival, cell proliferation, and cell death, and eventually phenotypes including various diseases is a major challenge facing computational systems biologists. <br\/><br\/>Intellectual Merit<br\/> This EAGER project will develop and validate a novel computational model called Semi-parametric Bayesian FActor Regulatory Model (SB-FARM) for TF and miRNA regulation of gene expression. The advantages of SB-FARM over other existing models are that it integrates existing knowledge about gene regulations in the model and enables the discovery of specific regulations by TFs and miRNAs under unmeasured conditions or contexts. The investigators will examine the modeling details as well as algorithms for reconstructing the model from a set of gene expression data. They will apply SB-FARM to study the context-specific regulations in E-coli and human cancer. <br\/><br\/>The long term goal of the investigators is to develop signal processing and statistical learning methods for the system-level understanding of gene regulatory networks underlying different biological processes and apply them to better understand the genomic basis for diversity in organisms and development of diseases. The SB-FARM has a general structure that permits integration of additional aspects of gene regulation. The success of the SB-FARM will have long lasting impact on gene regulation research and is expected to also significantly advance statistical signal processing and Bayesian learning.<br\/><br\/>Broader Impact <br\/>This research is highly interdisciplinary, cross-cutting science and engineering. It will provide an environment for advanced interdisciplinary learning and education in the area of genomics signal processing and computational biology. The PIs will also actively involve graduate and undergraduate students in research activities. Particularly, they will utilize the minority institution status of UTSA and UTHSCSA to recruit and involve minority students in this research. The developed computational methods will be implemented into publically available software to aid computational biology researchers to investigate context specific gene regulations. The computational methods and tools will enhance the signal processing and machine learning research and ultimately lead to development of new theory and methods.","title":"AF: EAGER: Bayesian Factor Modeling of Context-Specific Gene Regulation","awardID":"1246073","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["559862","531038"],"PO":["565223"]},"190876":{"abstract":"The OpenRAM project aims to provide an open-source memory compiler framework for Random-Access Memories (RAMs). Most academic Integrated Circuit (IC) design methodologies are inhibited by the availability of memories since commercially available memory compilers are often black box, not modifiable, and have limited available memory configurations. However, memories are the largest barrier to future scaling and require urgent solutions to lower leakage power consumption, improve reliability in shrinking supply voltages, and incorporate post-CMOS technologies. The OpenRAM compiler will enable research in these areas by providing a completely modifiable, verified, technology independent compiler for single-port and multi-port RAMs and many-port register files. The availability of the OpenRAM will enable research in a variety of IC-related disciplines<br\/>including: computer architecture and system-on-chip (SOC) design, memory circuit and device research, and computer-aided design (CAD).<br\/>Computer architects and SOC designers need access to a variety of memory configurations including specialized many-ported register files to prototype system-level implementations. Similarly, memory circuit and device researchers need a framework to prototype new circuits and devices in the context of these many configurations. Last, CAD researchers need a framework to study the optimization and analysis of power and yield of on-chip memories. In addition to the broader impact of enabling future memory system scaling, OpenRAM will also be an important education tool by allowing anyone to synthesize and utilize memory macros in standard-cell design flows and implement custom extensions.","title":"Collaborative Research: CI-ADDO-NEW: An Open Memory Array Compiler Framework to Support Devices, Circuits and Systems Research","awardID":"1205685","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":[511725],"PO":["565255"]},"192966":{"abstract":"Extensive work over the last decade, done within theoretical computer science, has provided deep insights into the computability of market equilibria for various market models and utility functions, using the powerful tools of the modern theories of algorithmic design and algorithmic complexity. This follows up on a century-long work, within mathematical economics, on obtaining a mechanism that converges to equilibrium -- a goal that had to be eventually abandoned due to certain negative results on efficient computability of the equilibrium. The work in TCS was motivated in part by applications to markets on the Internet. <br\/><br\/>The current project will extend this work along several exciting directions. Recent work of the lead Principle Investigator (PI) on using complementary pivot algorithms, for obtaining usable algorithms for certain market models that are unlikely to have efficient algorithms in the usual sense of polynomial worst-case running time, opens up the possibility of extending this approach to broader classes of markets, in particular, markets with production. A major new challenge is to address dynamically evolving markets. <br\/><br\/>In terms of applications of markets, the team brings to this project a wealth of experience on electricity markets, gained from work done with researchers in computer science and control dynamics. The PIs plan on bringing their expertise in mechanism design to bear on the problems of integrating renewable energy sources into the smart grid and providing better approaches to the pricing and allocation of ancillary services to guarantee reliability and stability. Another new challenge is to extend general equilibrium theory, the undisputed crown jewel of mathematical economics, to the digital economy. The traditional notion of equilibrium is not applicable to digital goods -- once produced, an unbounded number of copies of such goods are available. The digital realm is very rich and is increasingly occupying a larger share of our economy. It is imperative, therefore, to achieve the same depth of understanding of pricing for digital goods as was obtained for conventional goods. <br\/><br\/>This project will provide algorithms and insights into the computational aspects of markets, including electricity markets and transactions on the Internet, thereby helping make their operation more efficient. Hence, it is expected to contribute to advances in science and engineering, as well as to promote economic prosperity.","title":"ICES: Large: Collaborative Research: Markets - Algorithms, Applications and the Digital Economy","awardID":"1216024","effectiveDate":"2012-08-01","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8052","name":"Inter Com Sci Econ Soc S (ICE)"}}],"PIcoPI":[516932],"PO":["565251"]},"193604":{"abstract":"This project aims to build large scale distributed syntactic, semantic, and lexical language models that are trained by corpora with up to Web-scale data on a supercomputer in order to substantially improve the performance of machine translation and speech recognition systems. It is conducted under the directed Markov random field paradigm to integrate both topics and syntax to form complex distributions for natural language, and uses hierarchical Pitman-Yor processes to model long-tail properties of natural language. By exploiting this particular structure, the complex statistical estimation and inference algorithms are decomposed and performed in a distributed environment. The language models are put into one-pass decoders of machine translation systems, and the lattice rescoring decoder into a speech recognition system. In addition, a principled solution to a long-standing open problem, smoothing fractional counts due to latent variables in Kneser-Ney's sense, might be found. <br\/><br\/>This project fits into the NSF's strategic long term vision of a Cyber-infrastructure Framework for 21st Century Science and Engineering (CIF21). The project integrates various kinds of known language models and provides a way to overcome the limitations of existing combination methods for language models and to deploy algorithmically interesting methodologies that are scalable to data sets available on the Web. The project provides an environment for interdisciplinary education in information technology that bridges areas of language and speech processing, machine learning, and data-intensive science and engineering to benefit students at several levels.","title":"RI: Small: Developing Large Scale Distributed Syntactic, Semantic and Lexical Language Models for Machine Translation and Speech Recognition","awardID":"1218863","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[518498,518499],"PO":["565215"]},"192108":{"abstract":"Displays of anger or joy can profoundly shape the behavior of those around us. This project brings a functionalist perspective to the study of such influences on human decision-making. Rather than treating affective displays as mere manifestation of internal experience, they can be viewed as strategic other-directed signals that facilitate social goals, for example by conveying specific internal beliefs, desires and future intentions. The project explores the theory of \"reverse appraisal\" as a framework for explaining and computationally modeling this social process. Specifically, this theory will be used to unpack the mechanisms by which affective displays influence outcomes in social dilemmas and negotiations. This functional analysis also informs the design of human-computer and computer-mediated collaboration systems. The project uses a framework based on \"virtual confederates\" - i.e., anthropomorphic computer characters - to study social affect. It builds on reverse appraisal's insight that what matters is the information (function) not the display in itself (form) and studies how the social functions of affect apply to systems that extend beyond virtual humans.<br\/><br\/>Broader Impact. The project promotes synergies between social and computational sciences: it contributes to understanding the psychology of how affective expressions impact people's decisions in social, economic and daily life; and, it advances the state of human-computer systems for collaboration in practical domains such as negotiation, and has implications for designing collaborative computer-mediated systems. The research results in new tools -- virtual confederates -- or studying social processes, which will be offered freely to the scientific community. Finally, the project supports research and mentoring activities for graduate students and postdoctoral scholars towards a multidisciplinary specialization in affect and decision making.","title":"SoCS: Achieving the Interpersonal Function of Affect in Human-Machine Collaboration","awardID":"1211064","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7953","name":"SOCIAL-COMPUTATIONAL SYSTEMS"}}],"PIcoPI":[514859,514860],"PO":["565215"]},"193439":{"abstract":"Network routing is the ensemble of technologies for determining optimal paths for data delivery. These paths are selected as a result of complex policy-driven interactions between multiple networks and protocols. It is hard to understand the consequences of adopting particular policy configurations, especially since each individual operator's view of the global network is incomplete. Some combinations of policy can lead to loss of connectivity, the adoption of sub-optimal paths, or protocol convergence failure.<br\/><br\/>This research project aims to provide theoretical and practical methods by which operators can manage the interaction of policy in the absence of complete information. The central formalism is a protocol-agnostic model of partial policy specifications. This model supports a variety of tasks, such as the synthesis of unspecified parts of local policy according to high-level objectives, configuration repair, and planned migration from one policy to another. Each of these applications is to be given a rigorous algorithmic basis in terms of the policy model, and implemented on simulated and real routing platforms. Additional research goals include the development of higher-level policy representations on top of the basic model, and providing tools for assessing policy privacy using the incomplete information framework.<br\/><br\/>The broader impacts of this research include the development of practical tools for policy analysis and synthesis, on standard network platforms. This will allow important routing policy management tasks to be carried out with confidence. Student projects connected to the development and validation of these tools will be of direct importance to their education.","title":"NeTS: Small: Routing Design and Analysis with Incomplete Information","awardID":"1218066","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[518109,518110],"PO":["565090"]},"196838":{"abstract":"The annual ACM SIGKDD conference is the premier international forum for data mining researchers and practitioners from academia, industry, and government to share their ideas, research results and experiences. KDD 2012 will feature keynote presentations, oral paper presentations, poster sessions, workshops, tutorials, panels, exhibits, demonstrations, and the KDD Cup competition. KDD-2012 will be held at Beijing, China from August 12 to 16, 2012. Strong student participation is a long running tradition of the KDD conference. This project seeks to encourage and support graduate students enrolled in doctoral programs at US institutions to participate in KDD. Approximately 20 Student Travel Awards will be made to enable the students to attend KDD 2012 and participate in a Doctoral Forum.<br\/><br\/>Intellectual Merit: The field of Knowledge Discovery and Data Mining has grown rapidly in recent years. Massive data sets have driven research, applications, and tool development in business, science, government, and academia. The continued growth in data collection in all of these areas ensures that the fundamental problem which KDD addresses, namely how does one understand and use one's data, will continue to be of critical importance across a large range of organizations. Due to the recent data explosion in many applications, governments and companies can easily collect data spanning terabytes, petabytes or more. Several traditional data mining algorithms need to be replaced, or drastically re-designed, to handle such volumes. SIGKDD conference focuses on innovative research on all aspects of knowledge discovery and data mining. Examples of topic of interest include (but are not limited to): association analysis, classification and regression methods, semi-supervised learning, clustering, factorization, transfer and multi-task learning, feature selection, social networks, mining of graph data, temporal and spatial data analysis, scalability, privacy, security, visualization, text analysis, Web mining, mining mobile data, recommender systems, bioinformatics, e-commerce, online advertising, anomaly detection, and knowledge discovery from big data, including the data on the cloud. Papers emphasizing theoretical foundations, novel modeling and algorithmic approaches to specific data mining problems in scientific, business, medical, and engineering applications are particularly encouraged. It is important to attract students working on these topics to become the future leaders of the field.<br\/><br\/>Broader Impacts: The student travel awards will support will enable US students to participate in one of the leading data mining conferences, and encourage them to pursue research on topics that are at the forefront of the current state of the art. The awards will help broaden the participation of female and minority students who are currently under-represented within the data mining research community.","title":"Student Travel Support for the 2012 ACM Conference on Knowledge Discovery and Data Mining (KDD 2012).","awardID":"1241017","effectiveDate":"2012-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":["541857"],"PO":["565136"]},"193109":{"abstract":"The computing industry is currently producing and developing chips that include processor cores of different types. The move towards heterogeneous chips is due to two trends: the ability to squeeze more transistors on each chip and the improved power-efficiency of using specific cores for specific purposes. This research project extends the inter-core communication systems and precise specifications of homogeneous chips to heterogeneous chips. The benefits of extending homogeneous communication paradigms include improved performance and improved power-efficiency, and preliminary results show orders of magnitude improvements in both metrics. Society relies upon computers for ever more purposes, and qualitatively improving their performance and power-efficiency are critical to future computing platforms. Furthermore, by extending homogeneous specification methodologies, this research project facilitates chip testing and validation for heterogeneous chips, which is a significant fraction of industry effort in chip development, and eases the programming of these chips, which is among the biggest challenges in computing today. <br\/><br\/>This research project consists of two technical thrusts. The first thrust is the development of memory systems for heterogeneous chips. Rather than treat a heterogeneous chip as two distinct systems that happen to share a chip, the goal is to closely integrate the heterogeneous cores in a systematic fashion that enables most communication between cores to be performed via on-chip caches rather than via off-chip memory. The second research thrust is to extend the notion of a computer architecture--an implementation-independent specification of a processor's behavior--from homogeneous systems to chip-wide heterogeneous systems. This research project involves the design and experimental evaluation of heterogeneous processor chips, and the bulk of the experimental work uses full-system simulation and experiments on purchased hardware. The contributions of this project will be heterogeneous processor chips that have significantly greater performance and power-efficiency, as well as implementation-independent architectures for heterogeneous chips. Society will be using heterogeneous processor chips in personal computers, in smartphones, and via cloud-provided services. Qualitatively improving these chips will enhance user computing experiences and enable new and exciting computing applications.","title":"SHF: Small: Shared Memory Architectures and Microarchitectures for Heterogeneous General-Purpose Chips","awardID":"1216695","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["536996"],"PO":["366560"]},"190800":{"abstract":"Healthcare systems are facing an imminent crisis precipitated by the confluence of current social, economic and demographic trends. According to the United States Census Bureau our total expenditures on healthcare reached $2.5 trillion in 2009 and are projected to reach 20% of the nation's Gross Domestic Product in 2020. These statistics suggest the need to seek more scalable and affordable healthcare solutions. A focus on proactive management of wellness \/ prevention, on early detection of disease, and on optimal maintenance of chronic conditions has the potential to reduce healthcare costs while increasing quality of life. Recent technological advances in sensors, integration and miniaturization of low-power electronics, wireless networking, mobile computing, and cloud computing allow us to modernize and change the way health care services are deployed and delivered.<br\/><br\/>Mobile health monitoring systems that integrate wireless body area networks (WBANs) with a range of intelligent and miniature sensors, personal devices like smart phones, and servers that can be accessed over the Internet emerge as a promising technology for real-time, unobtrusive health and wellness monitoring of individuals during normal daily activities. Unprecedented proliferation of personal computing devices and smart sensors makes such solutions practical and affordable. But further research is needed to explore the design space and create optimal solution for a given application, and also to understand the full scope of the opportunities offered by the new technology while addressing the challenges.<br\/><br\/>This project will provide funds to create mHealth, a new computing infrastructure to support research and education in computer systems for mobile health and wellness monitoring at the University of Alabama in Huntsville. The mHealth infrastructure consists of: a) A variety of wearable wireless sensors for monitoring users' physiological signals, body movement and activity levels, along with general environmental conditions; b) Personal devices that collect data from the sensors, analyze it, compile personalized health status information, and upload data over the Internet to a server; and c) An mHealth server running databases and services for logging and analysis of health records from multiple users. The mHealth infrastructure will directly support several research efforts conducted by the investigators, their students, and their collaborators at the University of Alabama in Huntsville's College of Nursing, at the Mayo Clinic, at the University of Alabama at Birmingham's School of Health Sciences, and at the Lakeshore Foundation in Birmingham. In addition, mHealth will be used in a senior design course sequence and two graduate courses at the University of Alabama in Huntsville to improve the student academic experience and to help with recruiting efforts through outreach activities.<br\/><br\/>Intellectual Merit<br\/>The mHealth infrastructure will enable the investigators to pursue the following research goals: 1) Exploring critical design issues in the next generation of wireless wearable body area networks for health monitoring including their functionality, reliability, and energy-efficiency; 2) Creating annotated public data repositories with vital signs and physical activity parameters during normal daily activities to promote further research in WBANs for health monitoring; 3) Building WBAN research prototypes and developing algorithms, firmware and software artifacts for applications such as monitoring and managing physical activity in people with spinal cord injury (with UAB, Lakeshore), monitoring and managing ambulatory rehabilitation of people with coronary disease (with Mayo), health status assessment of people with coronary disease (with Mayo), and monitoring of the occupational stress of nurses (with UAHuntsville Nursing).<br\/><br\/>Broader Impact<br\/>Acquisition of the mHealth infrastructure will help the investigators and their collaborators pursue research in several multidisciplinary areas. If successful, these efforts promise to improve our understanding of the design space of wearable ubiquitous platforms for health monitoring, to improve quality of life of people with disabilities, to promote healthy behaviors through use of technology in the general population, to enable affordable solutions for ambulatory rehabilitation, and to enable early detection of health conditions. The mHealth infrastructure will also be used directly for education and training in several graduate and undergraduate courses.","title":"II-NEW: mHealth - Computing Infrastructure for Mobile Health and Wellness Monitoring","awardID":"1205439","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[511500,"517808"],"PO":["565227"]},"192978":{"abstract":"Cloud computing centers are used by most major corporations to store business data and provide flexibility in computing services allowing more agile business responses in this rapidly evolving economy. Individuals also benefit from cloud computing services ranging from data storage (e.g. dropbox.com) to mobile computation (e.g. the iPhone's Siri computes in the cloud). These cloud computing centers must adapt rapidly and efficiently to complex and intensive requests from many simultaneous users. One emerging mechanism for such complex real time allocations is to use pricing -- provide prices for various resources and allow an internal market to find good allocations. The goal of this project is to carefully analyze several important properties of price mechanisms. First and foremost is the accuracy and stability of prices and the allocations that they induce -- the so called market equilibria. Second, is their manipulability by users, a problem that has been observed in several such systems. Third is their relationship to external pricing and their relationship to various methods used to increase the income of cloud computing providers. <br\/><br\/>The project seeks to analyze price mechanisms for cloud computing, studied within the more general framework of resource allocation. In the first special case (the Leontief economy) it is assumed that the commodities are perfect complements. The second special case is the \"machine based model\" in which some machines are of use to only a subset of all users. The project will study the fairness properties of such schemes and particular their relation to Nash bargaining theory, the convergence rate to the clearing prices, and the potential for users to manipulate the market.<br\/><br\/>This research will have direct impact on the design and development of modern cloud computing centers, allowing them to provide lower cost service and avoid many pitfalls that have been problematic in the past. In addition, the study of prices and markets in such a controlled environment will allow us to develop mathematical and computational tools that will be useful in understanding the national and international economies. In particular, the study of price manipulations and the development of systems that limit such manipulations are of great importance. Also, the development of better mechanisms for managing cloud computing centers will improve the environment by reducing the number of computers required by these centers, saving resources and energy. The PIs will participate in education outreach at local high schools and with women and under-represented groups at the Berkeley Foundation for Opportunities in Information Technology (BFOIT) summer workshop.","title":"ICES: Small: Evaluating Price Mechanisms for Clouds","awardID":"1216073","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"8052","name":"Inter Com Sci Econ Soc S (ICE)"}}],"PIcoPI":["560562",516972],"PO":["565251"]},"193506":{"abstract":"The current trend in computer hardware is towards increasing numbers of parallel, independent processing units (cores). This trend necessitates a widespread transition from traditional sequential programming to parallel programming. But because parallel programming is notoriously difficult, adoption has been slow. A fundamental reason for this difficulty is that programs can often yield inconsistent answers, or even crash, due to unpredictable interactions between parallel tasks. Certain classes of programs, however, admit strong mathematical guarantees that they will behave the same in spite of parallel execution. This research is extending the mathematical foundation underlying such deterministic programs. It studies a family of programming languages that allow both parallel computations, and communications between them in the form of restricted modifications to, and observations of, shared data. The communication allowed is of a more general nature than previous work in the area.<br\/><br\/>This project is centered around a variant of the lambda-calculus that includes shared variables whose states occupy a join semilattice and change monotonically within that lattice. A number of deterministic programming models, both recent (Intel CnC), and older (Kahn-MacQueen Process Networks), can be mapped into this framework. In addition to constructing proofs of determinism for the language, this project explores various extensions, including limited forms of nondeterminism (i.e., which admit failures but never wrong answers). Finally this project will use its formal language as a tool for reasoning about practical parallel programs. That is, determinism can be demonstrated by verifying that the program's shared states form a semilattice and state changes are monotonic.","title":"SHF: Small: Generalizing Monotonic Data Structures for Expressive, Deterministic Parallel Programming","awardID":"1218375","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["556710"],"PO":["564588"]},"193418":{"abstract":"This project focuses on the issues of network design, provisioning, and traffic engineering for dynamic backbone and metropolitan access networks needed to support a \"follow the wind\/sun\" computing paradigm for green cloud computing. This paradigm is based on the observation that it is more energy efficient to move data and computations to a site with local green energy than it is to connect green energy sources to a power grid and then transmit the green power to distant data centers. <br\/><br\/>The project will study network design and traffic engineering when placement of the elements generating the traffic load is a possible degree of freedom. The sub-problems include 1) placement of data centers to both be close to green energy and aware of existing network deployment, 2) network capacity planning and rate capacity assignment for networks where data centers may be dynamically powered up\/down based on energy availability, 3) routing of user service requests to data centers based on network considerations and estimates of green energy availability.<br\/><br\/>Broader Impact:<br\/>This project will explore and quantify better use of green-renewable energy by data-centers, a significant consumer of electric power. The project will produce insights that assist industry in the future design, placement and operation of green data centers. The PIs will train undergraduate (via the REU program) and graduate students in the interdisciplinary area of energy management. As part of the training the PIs will hold highly-interactive weekly meetings in which students actively learn from each other.","title":"NeTS: Small: Design and Provisioning of Low-Carbon Optical Datacenter Networks","awardID":"1217978","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[518062,518063],"PO":["564993"]},"197609":{"abstract":"The University of Wisconsin will evaluate the impact of the Alliance for the Advancement of African American Researchers in Computing (A4RC). A4RC was formed in Spring 2006 with the goal of increasing the number of African Americans obtaining advanced degrees in computing, particularly at the Ph.D. level, by establishing student pipelines from Historically Black Colleges and Universities (HBCUs) to Research Universities that offer advanced degrees in computing. A4RC collaborations are centered around research pods that link students and faculty from HBCUs and research institutions on a particular area of computer science research. The A4RC Evaluation Team will examine the five years of A4RC evaluation data to determine what program attributes are best suited to serve as national models for creating a computing education pathway for African Americans. The project staff will utilize existing data and original data collection to assess the effectiveness of past and current A4RC initiatives. The PI will disseminate best practices and research results on increasing the participation of African Americans in computing.","title":"RAPID: Understanding the Impact of the Alliance for the Advancement of African-American Researchers in Computing (A4RC)","awardID":"1245103","effectiveDate":"2012-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7382","name":"Computing Ed for 21st Century"}}],"PIcoPI":[530190],"PO":["560704"]},"193617":{"abstract":"This project concerns a machine learning technique known as reinforcement learning, which is related to, but distinct from, the notion of reinforcement learning used in psychology. The common element is that both views study changes in behavior that result from experience. In the machine learning case, the behaviors are often decision making in dynamic environments, such as controlling a robot, a factory, inventory levels for a warehouse or even drug dosage levels. Current theoretical development in this area guarantees that optimal decisions can be made by reinforcement learning algorithms, but only under restrictive assumptions that are difficult to ensure in practice. Efforts to apply reinforcement learning to significant practical problems have enjoyed some success, but such efforts often forgo theoretical guarantees and rely upon tedious parameter adjustments by experts (human trial and error) to achieve success.<br\/><br\/>This research seeks to reduce the amount of human trial and error needed to make reinforcement learning successful, thereby making it a more accessible tool to a wider range of people. Specifically, it will focus on algorithms for domains described by continuous variables, seeking to provide stronger theoretical guarantees for such domains as well as an approach that balances the anticipated benefit of trying new things with the benefit of sticking to what is already known about a problem (exploration vs. exploitation). A practical benefit of success in this area would be improved techniques that make it easier for people to deploy algorithms that learn and improve performance in a variety of practical tasks like those mentioned above: robot or factory control, inventory management, or drug delivery.<br\/><br\/>This project plans to use a model helicopter as a challenge domain, but it is not about helicopter control per se. Rather, it seeks to develop general techniques that can apply to many problems, including helicopters, and will use model helicopters as an inexpensive and fun way to motivate students. The project aims to develop a model helicopter simulator (to reduce the cost and risk of trying everything on an actual helicopter) and plans to make this simulator available to the research community, providing a fun and challenging benchmark problem.","title":"RI: Small: Non-parametric Approximate Dynamic Programming for Continuous Domains","awardID":"1218931","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":[518533],"PO":["562760"]},"193628":{"abstract":"While TCP was designed to work in any environment, it has a long history of problems whenever introduced in newer environments (e.g., wireless networks, satellite networks, high-capacity networks) that it was not explicitly designed and tested for. A similar new frontier that challenges TCP is the data center and cloud environments that have become popular in recent times. This project will conduct research on exposing end-to-end challenges that TCP faces in the modern cloud computing and data center environments and propose solutions to address them. <br\/><br\/>Specifically, it focuses on three major issues surrounding TCP in data center and cloud environments: First, in virtualized cloud environments, when multiple VMs share the CPU, CPU access latency for each VM (i.e., the interval during which a VM waits for the CPU) is in the order of tens\/hundreds of milliseconds and can be orders of magnitude higher than the typical sub-millisecond network RTTs. This high RTT causes significant reduction in TCP throughput. Second, in multirooted data center networks, under certain conditions, TCP connections sharing a common link exhibit severe unfairness. Finally, in data center networks today, equal-cost multipath routing (ECMP) is often used to split traffic across multiple paths, but it can potentially cause significant load imbalance. Researchers have refrained from suggesting packet-level multipath routing in the past because of its ability to cause reordering that may reduce TCP throughput. It is however not clear whether this poor interactions with multipath routing exists even under symmetric topologies such as fattrees, or more generally, multi-rooted tree topologies. <br\/><br\/>Intellectual Merit: The goal of this project is to comprehensively investigate these afore-mentioned three major issues discussed above, propose new solutions to address the problems, and finally, validate these solutions and hypotheses using real prototype implementations and through extensive evaluations. (1) To address TCP's performance deterioration in virtualized cloud environments, the project will explore a new approach called transport function delegation, where certain TCP functions are delegated to the driver domain or hypervisor. (2) It will conduct extensive experimentation using real testbeds to expose, and characterize the unfairness issue, and also propose and evaluate classic solutions (e.g., RED) and a new routing algorithm called equal-length routing to mitigate this problem. (3) It will revisit the conventional wisdom that fine-grained multi-path traffic splitting protocols interact poorly with TCP, in the context of data center networks which have regular topologies such as multi-rooted trees. <br\/><br\/>Broader Impact: The broader impact of this research comprises of the following: (1) It will help improve the key aspects of TCP such as performance and fairness in data center and cloud environments, that will benefit all data center systems and applications. (2) Results of this research will be transferred to industry. (3) Results of this research will be integrated into courses such as operating systems, computer networks, and cloud computing. It will also provide training to graduate students and several Ph.D. theses are expected to come out of this research. (4) It will include participation of minorities (under-represented minorities and women).","title":"NeTS: Small: Towards Exposing and Mitigating End-to-End TCP Performance and Fairness Issues in Data Center Networks","awardID":"1219004","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[518557,518558],"PO":["565090"]},"193408":{"abstract":"Most multicore data structures being designed and used in parallel software today are concurrent versions of the sequential data structures of years past. They continue to work reasonably well because the level of parallelism offered by mainstream multicore machines is still low. As machines grow in size, however, the limitations of these traditional structures will become clear: they have inherent sequential bottlenecks, require tight synchronization, and are not easily distributed. This project will develop new classes of parallel data structures that combine relaxed specifications with highly decentralized randomized implementations, overcoming the drawbacks of existing structures and providing a better fit with tomorrow's massively parallel many-core machine designs. <br\/><br\/>This research will combine theoretical algorithmic work with empirical evaluation on real machines and applications, and will result in a library of concurrent data structures and a collection of design approaches and specification methodologies. Commercial software developers are in desperate need of data structures that scale while still remaining easy to understand and modify. Making this project's developed structures widely available will greatly help in making tomorrow's applications, whether they run on a cell phone or on a server in the cloud, make full use of the parallelism offered by multicore technology.","title":"SHF: Small: Multicore Data-Structures: Relaxed, Flat, and Randomized","awardID":"1217921","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":["541744"],"PO":["564588"]},"190813":{"abstract":"User interactive event driven software is pervasive today. End users interact with applications by pointing, clicking or touching the interface, and as this happens, the programs respond. Ensuring the dependability of these systems through software testing is paramount, because insufficient testing techniques currently cost the US economy billions of dollars annually. Yet the flexibility of these systems, which make them appealing to users, also increases the difficulty of testing them. This difficulty has fueled a large body of research on user interactive event driven testing, but the newly developed techniques are often evaluated using isolated case studies and experiments. The user interactive event driven testing community lacks a common set of benchmarks and tools for evaluating their new methods, leading to experimental mismatch; the results of one study are difficult to compare with another. The lack of common benchmarks also means that we cannot easily combine results of multiple studies to build a larger body of knowledge.<br\/><br\/>This project reduces the mismatch and is advancing user interactive event driven testing research by developing a shared research and experimentation web infrastructure called COMET. An initial proof of concept for COMET was developed through earlier support from NSF. Factors that contribute to the mismatch include the development of platform specific test methods, test harnesses that require customizations for each test subject application and each operating system, and models that are incompatible with one another. The research will devise new techniques to control the testing environment, contextualizing factors that may affect experimental outcome and will allow for evolution and change of the artifacts over time. It is building a shared and extensible web infrastructure of benchmarks, tools, models and test artifacts that will enable scientific discovery in the state-of-the-art of user interactive event driven testing. COMET is a public resource that will be available to a broader community. Its impact will extend not only to the user interactive event testing research community, but also to others that work with user interfaces such as those who study usability, and to industry and the software testing community at large. The project work will involve both graduate and undergraduate students. Artifacts from the COMET website will be utilized for educational purposes in the classroom.","title":"II-NEW: Collaborative Research: COMET: A Web Infrastructure for Research and Experimentation in User Interactive Event Driven Testing","awardID":"1205472","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":[511534],"PO":["564588"]},"190824":{"abstract":"User interactive event driven software is pervasive today. End users interact with applications by pointing, clicking or touching the interface, and as this happens, the programs respond. Ensuring the dependability of these systems through software testing is paramount, because insufficient testing techniques currently cost the US economy billions of dollars annually. Yet the flexibility of these systems, which make them appealing to users, also increases the difficulty of testing them. This difficulty has fueled a large body of research on user interactive event driven testing, but the newly developed techniques are often evaluated using isolated case studies and experiments. The user interactive event driven testing community lacks a common set of benchmarks and tools for evaluating their new methods, leading to experimental mismatch; the results of one study are difficult to compare with another. The lack of common benchmarks also means that we cannot easily combine results of multiple studies to build a larger body of knowledge.<br\/><br\/>This project reduces the mismatch and is advancing user interactive event driven testing research by developing a shared research and experimentation web infrastructure called COMET. An initial proof of concept for COMET was developed through earlier support from NSF. Factors that contribute to the mismatch include the development of platform specific test methods, test harnesses that require customizations for each test subject application and each operating system, and models that are incompatible with one another. The research will devise new techniques to control the testing environment, contextualizing factors that may affect experimental outcome and will allow for evolution and change of the artifacts over time. It is building a shared and extensible web infrastructure of benchmarks, tools, models and test artifacts that will enable scientific discovery in the state-of-the-art of user interactive event driven testing. COMET is a public resource that will be available to a broader community. Its impact will extend not only to the user interactive event testing research community, but also to others that work with user interfaces such as those who study usability, and to industry and the software testing community at large. The project work will involve both graduate and undergraduate students. Artifacts from the COMET website will be utilized for educational purposes in the classroom.","title":"II-NEW: Collaborative Research: COMET: A Web Infrastructure for Research and Experimentation in User Interactive Event Driven Testing","awardID":"1205501","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7944","name":"SOFTWARE ENG & FORMAL METHODS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7359","name":"COMPUTING RES INFRASTRUCTURE"}}],"PIcoPI":["550281"],"PO":["564588"]},"191506":{"abstract":"To enable natural and productive human-robot interaction (HRI), a co-robot must both understand and control \"proxemics\" -- the social use of space -- in order to communicate in ways commonly used and understood by humans. This project focuses on answering the question: How do social (speech and gesture), environmental (loud noises and low lighting), and personal (hearing and visual impairments) factors influence positioning and communication between humans and co-robots, and how should a co-robot adjust its social behaviors to maximize human perception of its social signals?<br\/><br\/>The project will develop principled computational models for the recognition and control of proxemic co-robot behavior in HRI using both telepresence and autonomous co-robots. The research will establish a foundational component of HRI for co-robotics, with specific impact on special needs users in socially assistive contexts -- particularly the elderly, both aging in place and in institutions -- with the goal of mitigating isolation and depression, and encouraging exercise and socialization. <br\/><br\/>Broader impacts: The work will inform robot design and control, and provide software and a corpus of public HRI data for use by researchers worldwide. Beyond robotics, the project promises to inform, validate, and extend longstanding research in the social sciences. This project also includes a strong public and K-12 outreach component consisting of weaving the HRI themes being developed into annual regional and international outreach events. The events feature large-scale open houses and educational workshops with interactive demonstrations and hands-on activities that highlight human factors in computational systems as an effective means of increasing interest in STEM-related activities.","title":"NRI-Small: Spacial Primitives for Enabling Situated Human-Robot Interaction","awardID":"1208500","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"8013","name":"National Robotics Initiative"}}],"PIcoPI":[513294,"558935"],"PO":["564456"]},"193519":{"abstract":"Efficient algorithms are fundamental to various areas in computer science. This project will study new techniques for designing efficient algorithms for hard combinatorial problems. The interaction between theoretical analysis and practical implementations of such hard problems will be explored. Furthermore, this project will advance the study of algorithms and systems in a minority-serving institution where under-represented students will gain valuable experience in foundational cutting-edge research. <br\/><br\/>More precisely, this project will investigate novel measures for some classical and important NP-hard problems, algebraic techniques for designing exact and parameterized algorithms, and parallel\/distributed implementations of exact and parameterized algorithms. The first part is devoted to studying the effects of new measures on designing algorithms for well-known NP-hard problems such as the Boolean Satisfiability Problem. The second part is focused on expanding the power of algebraic techniques to those seemingly difficult problems involving vertex deletion. The last part is to devise a clever way to implement resulting algorithms on a parallel\/multi-core machines.","title":"AF: Small: RUI:Applications and New Techniques for Exact Parameterized Computation","awardID":"1218423","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":["563325"],"PO":["565251"]},"190935":{"abstract":"The field of optomechanics has progressed rapidly in the past few years from proof-of-principle demonstrations of coupling between optical and mechanical systems to the observation of quantum effects in a handful of these devices. During the same period, connections between optomechanical systems and other AMO systems such as ultracold atoms and diamond NV centers have emerged as promising routes towards controlling quantum information in hybrid systems. In this project, we are working to extend both of these efforts by realizing a new type of optomechanical system capable of reaching the quantum regime while also playing host to cold molecules strongly coupled to an optical cavity. The devices we are building will also be substantially more robust and compact than existing optomechanical devices.<br\/><br\/>To achieve these goals, we are using high finesse optical cavities filled with superfluid helium. The cavities are formed between a pair of optical fibers. Precision-machined glass ferrules are being used to align the fibers and to contain the superfluid. The mechanical element will be the vibrational modes of the superfluid helium filling the space between the two fibers. This design eliminates all mechanical alignments, and should result in a very stable device. Numerical estimates suggest that this device will provide access to a range of quantum phenomena, including observations of the zero-point motion of the superfluid modes, the generation of squeezed light, and measurements of the quantum back action of an optical displacement measurement. In addition, we are exploring the optomechanical properties of superfluid-vacuum interfaces by filling the cavity only partially with superfluid.<br\/><br\/>Superfluid helium can play host to a variety of atom-like systems (such as metastable helium dimer moledules and electron bubbles) that can interact with both the optical cavity and the sound waves in the helium. Combining optomechanics with such atom-like systems would greatly enhance the versatility of these devices. We are using these superfluid optomechanical devices to study the quantum limits of measurements, and to produce ultrastable light for use in instruments operating at the quantum limit of sensitivity. This work is also providing valuable training for graduate students in laser optics, cryogenics, low-noise measurements, signal processing, data analysis, and quantum optics. This training will allow them to pursue basic and applied research in a wide variety of settings.","title":"Superfluid Optomechanics","awardID":"1205861","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0301","name":"Division of PHYSICS","abbr":"PHY"},"pgm":{"id":"1291","name":"ATOMIC & MOLECULAR DYNAMICS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0503","name":"Division of SHARED CYBERINFRASTRUCTURE","abbr":"SCI"},"pgm":{"id":"1710","name":"CONDENSED MATTER PHYSICS"}}],"PIcoPI":[511869],"PO":["564326"]},"193509":{"abstract":"This research focuses on the development of data-adaptive algorithms and error bounds for spectral estimation, which is a stealth technology in diverse application fields. Spectral estimation plays a critical role in many applications, civil as well as military. Strengthening the theoretical underpinnings and the quality and robustness of spectral estimation algorithms enables the discovery of new technologies for information acquisition which would not have been possible using traditional methods. The results of this study can provide new opportunities in a wide range of studies ranging from building on the fundamentals of inverse problems for signal processing to devising practically applicable and reliable spectral estimation algorithms.<br\/><br\/>This research leverages the recent advances in compressive sensing and is aimed at addressing the underlying technical challenges associated with the development of large scale spectral estimation algorithms. Moreover, quantifying uncertainty and assessing error bounds for current and new methods is also of significant importance. A metric needs to be well defined and naturally is a key element in any quantitative scientific theory. This research seeks to advance fundamental knowledge in novel data-adaptive spectral estimation algorithm design and to apply mathematical and engineering principles to address error bounding, while advancing mathematical and engineering knowledge on multiple fronts through the objectives listed below: 1) development of data-adaptive high resolution spectral estimation methods, 2) computationally efficient implementations of the algorithms for large scale problems, 3) theoretical quantitative assessment of the methods by deriving error bounds in suitable metrics, and 4) application of the methods to diverse real-world problems.","title":"CIF: Small: Adaptive Spectral Estimation and Error Bounding","awardID":"1218388","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["554666"],"PO":["564898"]},"198191":{"abstract":"This NSF award supports a planning workshop on Innovation in Medical Cyber-Physical Systems. This activity arises from an interagency exploration of emerging CPS research directions. The meeting will develop plans for further assessment by the CPS community of biomedical research needs and opportunities. The meeting and subsequent community-wide assessment are conducted in the context of the interagency High Confidence Software and Systems Coordinating Group.","title":"Conference Grant Proposal for Planning Meeting of Workshop on Medical Device Innovation Using Cyber Physical Systems","awardID":"1248083","effectiveDate":"2012-08-15","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7918","name":"CYBER-PHYSICAL SYSTEMS (CPS)"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"L509","name":"National Security Agency"}}],"PIcoPI":[531882],"PO":["565274"]},"204693":{"abstract":"The objective of this research is to develop a comprehensive theoretical and experimental cyber-physical framework to enable intelligent human-environment interaction capabilities by a synergistic combination of computer vision and robotics. Specifically, the approach is applied to examine individualized remote rehabilitation with an intelligent, articulated, and adjustable lower limb orthotic brace to manage Knee Osteoarthritis, where a visual-sensing\/dynamical-systems perspective is adopted to: (1) track and record patient\/device interactions with internet-enabled commercial-off-the-shelf computer-vision-devices; (2) abstract the interactions into parametric and composable low-dimensional manifold representations; (3) link to quantitative biomechanical assessment of the individual patients; (4) facilitate development of individualized user models and exercise regimen; and (5) aid the progressive parametric refinement of exercises and adjustment of bracing devices. This research and its results will enable us to understand underlying human neuro-musculo-skeletal and locomotion principles by merging notions of quantitative data acquisition, and lower-order modeling coupled with individualized feedback. Beyond efficient representation, the quantitative visual models offer the potential to capture fundamental underlying physical, physiological, and behavioral mechanisms grounded on biomechanical assessments, and thereby afford insights into the generative hypotheses of human actions.<br\/><br\/>Knee osteoarthritis is an important public health issue, because of high costs associated with treatments. The ability to leverage a quantitative paradigm, both in terms of diagnosis and prescription, to improve mobility and reduce pain in patients would be a significant benefit. Moreover, the home-based rehabilitation setting offers not only immense flexibility, but also access to a significantly greater portion of the patient population. The project is also integrated with extensive educational and outreach activities to serve a variety of communities.","title":"CPS:Medium:Quantitative Visual Sensing of Dynamic Behaviors for Home-based Progressive Rehabilitation","awardID":"1314484","effectiveDate":"2012-08-29","expirationDate":"2015-11-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["54165"],"PO":["565136"]},"205365":{"abstract":"We are entering an Industrial Revolution in the production of information. While in the past data was \"handmade\" by typing on keyboards, today data are increasingly manufactured by machines: sensors, cameras, software logs, etc. When harnessed in a timely manner, these data can have significant positive impact in many contexts, including early warning and rapid response in natural disasters, air quality monitoring, and improved Internet security. To provide useful information in these contexts, computers in multiple locations must coordinate over networks, because the data are both widely distributed and massive, and cannot be \"warehoused\" at a single location in a timely manner. Worse, sensor data is typical \"noisy\" or erroneous in various ways, so statistical methods must be employed to convert the raw \"evidence\" data into probabilistically reliable information. <br\/><br\/>In this project we develop new techniques to integrate statistical inference methods from AI with overlay network algorithms developed for peer-to-peer and wireless settings. We design new overlay network algorithms customized for distributed inference. We also develop network-aware inference algorithms that can trade off inference approximation quality for communication efficiency and robustness to network failure. Finally, we explore the use of a high-level declarative language for programming both the networking and inference logic. The high-level language enables us to investigate compilation techniques to co-optimize the inference and overlay network tasks for maximal utility. We prototype and evaluate our ideas via open-source implementations deployed on testbeds like Emulab and Planetlab. Software and research papers are disseminated at http:\/\/declarativity.net.","title":"NGNI-Medium: Collaborative Research: MUNDO: Managing Uncertainty in Networks with Declarative Overlays","awardID":"1318441","effectiveDate":"2012-08-31","expirationDate":"2013-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[549916],"PO":["563727"]},"198194":{"abstract":"A vast array of broadcasting protocols has been developed to alleviate the Broadcast Storm Problem for single-radio single-channel and single-rate wireless networks. The emergence of Multi-radio Multi-channel and Multi-rate Mesh (M4) networks, however, brings a lot of new challenges, such as channel assignment, adjacent-channel interference, and network capacity. This project focuses on the design, analysis, and implementation of distributed broadcasting protocols for M4 networks. Challenges such as channel assessment and assignment, interference-aware metric design, transmission rate control, broadcasting tree construction, etc., are to be considered. In-depth theoretical analysis, simulations, and real-world network experiments are also conducted to evaluate the broadcasting protocols. This project also explores several educational innovations. These include the development of a novel three-layer teaching structure, efficient pedagogical approaches, and effective means of incorporating research into learning and education. As one important component of this project, outreach activities are developed for underrepresented minorities from local high schools with the aim of improving their academic performance. <br\/><br\/>This research advances knowledge and understanding in the areas of wireless mesh networks, network optimization, and information dissemination. The problems studied are pragmatically and intellectually important and their solutions are critical to several areas such as the modeling of wireless communication links, networking traffic theory, and network performance analysis. The techniques developed in this project will benefit a broad spectrum of applications, including homeland security and military network deployment. Research results will be disseminated through a number of channels including international conferences, academic journals, seminars, workshops, and websites.","title":"CAREER: Distributed Broadcasting Protocols for Multi-Radio Multi-Channel and Multi-Rate Ad Hoc Mesh Networks","awardID":"1248092","effectiveDate":"2012-08-01","expirationDate":"2013-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["557315"],"PO":["564924"]},"202287":{"abstract":"This collaborative research brings together computer scientists from University of Maryland Baltimore County (UMBC) and Brown University and neuroscientists from the University of Mississippi Medical Center (UMMC) to study the design of a scientific visualization language (SVL). Despite the numerous visualization approaches already devised, visualization remains more of an art than a science. Grounded in theories and methods from human-centered computing, machine learning, and cognitive psychology, this work is to develop and evaluate a scientific visualization language (SVL) to provide a principled way to help scientists understand how and why visualizations work. Tools and theories developed in this project can lead to efficient knowledge discovery to help neuroscientists study brains using diffusion tensor magnetic resonance imaging (DTI).<br\/><br\/>This work has the following specific objectives and outcomes: (1) close collaboration with scientists to discover, refine, and verify a symbol space, (2) a semantic space that describes the relationship among symbols, (3) a testbed that implements SVL for neuroscientists to compose visualizations, (4) development of new and enhanced courses at University of Southern Mississippi and Brown University, and (5) wide dissemination of the research outcomes through open-source software, experimental data, open labs, publications, and presentations.<br\/><br\/>This project is expected to have broad impact. It may lead to significantly better approaches to human knowledge discovery and decision making in many disciplines where visualizations have found successful application, including neuroscience, biomedicine, bioinformatics, biology, chemistry, geosciences, business, economics, and education. Undergraduate and graduate students are expected to participate in the research through our courses, and student exchanges are planned between USM and Brown. K-12 students can visit the USM lab while the project is in progress. Software and results will be disseminated via the project Web site (https:\/\/sites.google.com\/site\/simplevisualizationlanguage).","title":"GV: Small: Collaborative Research: Supporting Knowledge Discovery through a Scientific Visualization Language","awardID":"1302755","effectiveDate":"2012-08-31","expirationDate":"2014-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["292127"],"PO":["563751"]},"199130":{"abstract":"This is funding to support a doctoral research symposium (workshop) of approximately 12 promising doctoral students from the United States and abroad (up to 2 international students), along with 4 high profile faculty and industrial researchers. The event will take place in conjunction with and immediately preceding the 7th Interactive Tabletops and Surfaces Conference (ITS ?12), to be held November 11-14, 2012, in Cambridge Mass., and which is sponsored by the Association for Computing Machinery. The technological advances of the past decade have given rise to an increasing number of creative research areas that seek to overcome the long-standing separation between the physical and digital worlds. The ITS Conferences are a premiere venue for presenting research in the design and use of new and emerging tabletop and interactive surface technologies, bringing together about 200 top researchers, engineers and practitioners who are interested in both the technical and human aspects of ITS technology; so topics of interest include not only innovations in ITS hardware and software but also expanding our understanding of design considerations for ITS technologies and their applications. More information about the conference may be found at http:\/\/its2012conf.org. <br\/><br\/>The primary goal of the ITS Doctoral Symposium is to provide mentorship for future leaders in this growing field. To these ends the event is designed so as to encourage young scholars from diverse backgrounds encompassed by ITS to attend the conference, meet with more senior researchers, and pursue a career in this field. The day-long program will help establish a sense of community among the next generation of researchers, while increasing the exposure and visibility of the participants' work and at the same time fostering their research efforts by providing constructive feedback and guidance from senior researchers in a supportive and interactive environment. Student participants will each make a formal presentation of their work to the group, with ample time allotted for questions and feedback from the faculty panel as well as from the other student participants. The feedback is geared to helping students understand and articulate how their work is positioned relative to other research, whether their topics are adequately focused for thesis research projects, whether their methods are correctly chosen and applied, and whether their results are appropriately analyzed and presented. Additional opportunities for more informal discussion and networking will be during the doctoral symposium's lunch and dinner events. Students' short papers will appear in the ITS Proceedings and they will be indexed in the ACM Digital Library. Students will also be invited to present their work at the main conference during the Poster Session; this will also afford the opportunity for students to talk one-on-one with peers and other more senior researchers, in addition to the focused mentoring of the Doctoral Symposium itself.<br\/><br\/>Broader Impacts: Student participants will be selected on the basis of written submissions in the form of a six page ACM format paper submitted to the Doctoral Symposium chairs. These will be reviewed by the faculty panel and external reviewers. Participants will be expected to be pursuing a doctoral degree and will be selected primarily from the field of human-computer interaction, which could span a variety of disciplines including computer science, engineering, or a design background. The review and decision of acceptance will balance many factors including the quality of the proposals. The event organizers are committed to diversity across backgrounds, gender, topics, and institutions, as well as to the inclusion of students from underrepresented minorities. To assure broad coverage of topics and backgrounds, no more than two students will be accepted from any given institution and no more than one with any given advisor. NSF funds will be used primarily to support student participants from U.S. institutions, with no more than two educational institutions from abroad.","title":"Workshop: Doctoral Symposium at the 2012 Interactive Tabletops and Surfaces Conference (ITS 2012)","awardID":"1253138","effectiveDate":"2012-08-01","expirationDate":"2013-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7367","name":"HUMAN-CENTERED COMPUTING"}}],"PIcoPI":[534174],"PO":["565227"]},"189494":{"abstract":"As companies, governments, and individual users adopt increasingly diverse computing platforms, from outsourced cloud computations to personal laptops and mobile devices, enforcing uniform security policies across these platforms becomes unwieldy.<br\/><br\/>Similarly, regulatory compliance and business auditing requires tracking the history of this data in a comprehensive, secure, and platform-independent manner. Unfortunately, technology has not kept pace with these practical concerns, and several systems and security research challenges must be addressed to make this vision a reality.<br\/><br\/>There is a natural and under-explored connection between understanding the origins of data and using that data's history to enforce security policies. To leverage this connection, this project is developing a comprehensive, general framework for automatically tracking the history of data and enforcing associated security policies in cloud computing environments. The research focuses on three key research challenges. First, the project investigates novel applications of virtualization technologies to transparently infer data provenance by inspecting a guest operating system (OS) and applications. Second, this project is developing techniques to securely store, manage, and query provenance data at cloud scale. Finally, the project combines the first two technologies to transparently and collaboratively enforce security policies throughout the cloud and end-user systems.<br\/><br\/>The prototype system is designed to allow individual users and organizations to rapidly adopt new technology platforms, from clouds to novel end-user systems, without having to worry about the interaction of these new systems with security policies and regulatory compliance concerns.","title":"CSR: Medium: CloudTracker: Transparent, Secure Provenance Tracking and Security Policy Enforcement in Clouds","awardID":"1161541","effectiveDate":"2012-08-01","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":["553485","521618"],"PO":["551712"]},"199053":{"abstract":"CAREER: Research on Real-time Robust and Secure Communications for Vehicular Ad Hoc Networks<br\/><br\/>This CAREER project is motivated by the belief that Vehicular Ad Hoc Networks (VANETs) based inter-vehicle communications could enhance traffic safety and traffic operation. VANET networks differ from general mobile ad hoc networks (MANET) because of the stringent requirements on real-time, robust, and secure communications and coordination in a critical highly dynamic environment. Building on research concerning run-time static relative-position relation among neighboring vehicles, this project addresses the major challenges in access technology, dynamic power control, robust multi-hop communication, and security and privacy provisioning. In particular, this project will develop new approaches to access technology enabling high channel availability over dynamic multi-path wireless channels, delay-bounded dynamic power control augmenting real-time communications over high mobility, robust multi-hop message disseminations in the presence of frequent fragmentations, and security implementation promoting cooperative communication and balancing privacy and security. It is envisioned that VANET communications will open the door for many new applications such as traffic safety, vehicular mobility, traffic operation, information sharing, vehicle data acquisition, and opportunistic pervasive communications.","title":"CAREER: Research on Real-time Robust and Secure Communications for Vehicular Ad Hoc Networks","awardID":"1252638","effectiveDate":"2012-08-01","expirationDate":"2013-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4090","name":"ADVANCED NET INFRA & RSCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["534006"],"PO":["565327"]},"201242":{"abstract":"Wireless technology has emerged as a low-cost and infrastructure-free method to deploy communication networks and has inspired a wide range of applications such as wireless mesh networks for public safety, wireless sensor networks for unmanned surveillance, and vehicular networks for accident warnings. Many of these applications require effective delay control for desired performance, which, however, is one of the most difficult problems in wireless network design due to the inherent weaknesses of wireless communication such as limited bandwidth, channel fading, and interference.<br\/><br\/>In the past few years, a major breakthrough in wireless network research has been to harness the power of optimization theory and stochastic network theory for network design. These advances, however, shed little light on communication latency (or delay) because the focus is almost exclusively on the long-term throughput. This project takes a bold step to break away from today's throughput-first mentality, and embraces a delay-oriented approach where delay is a primary design objective, not a byproduct of throughput-oriented designs. <br\/><br\/>The expected results of this project include: (i) new network theories that quantify fundamental delay and throughput limits of wireless networks; (ii) transformative algorithms (functionalities spanning multiple network layers and their interactions) that are optimized for communications requiring delay guarantees; and (iii) distributed and low-complexity implementations. Theoretically, this project will lead to a union of stochastic analysis and optimization for quality of service control in wireless networks. Practically, this project will produce transformative algorithms for supporting delay constrained applications.","title":"CAREER: Meeting Deadlines: Theories and Algorithms to Support Delay Constrained Communication in Wireless Networks","awardID":"1264012","effectiveDate":"2012-08-16","expirationDate":"2014-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":[539541],"PO":["557315"]},"198042":{"abstract":"The research community's understanding of the speech-to-text problem has reached a point at which most challenges can in principle be met, given a baseline system, enough data from the target domain, and an expert, who knows how to develop or adapt a recognizer for the target context-of-use. Unfortunately, this approach does not scale: despite the growing interest in speech-user interfaces, there are a limited number of experts equipped to analyze and develop an accurate speech recognizer.<br\/><br\/>This Early Grant for Exploratory Research explores the possibility of formalizing a speech recognition expert's implicit knowledge of the required analysis and development steps in a rule-based knowledge base, which can help a speech recognition non-expert develop a speech recognizer as part of an application, such as a dialog system in a rare dialect. Speech recognition experts adapt and improve recognizers by listening to data, aggregating error reports, and then adjusting parameters, retraining models, or applying adaptation techniques, based on their assessment of the mismatched context of use. This project extracts intuition from contextual interviews with such experts, develops a proof-of-concept expert system to predict the gains a system would see from specific adaptation techniques, and explores the factors which will make this approach feasible.<br\/><br\/>This project creates ways to make development of speech-enabled applications more accessible to a broader class of researchers, students, and practitioners, particularly from the user interface area. It will make joint development of user interface and speech recognition feasible, without requiring large teams with varied skill-sets.","title":"EAGER: A Research Infrastructure for Analyzing Speech-based Interfaces","awardID":"1247368","effectiveDate":"2012-08-01","expirationDate":"2014-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["543578",531482],"PO":["565215"]},"198053":{"abstract":"Machine learning is a powerful tool for artificial intelligence and data mining problems. However, its success critically relies on a good feature representation of the data; therefore, the problem of feature construction poses a fundamental challenge. In recent years, representation learning has emerged as a promising method for learning useful feature representations from data. However, the current state-of-the-art methods are still limited in building intelligent agents that can learn and interact with complex environments and large amounts of sensory input. Specifically, the majority of the existing methods cannot scale well to large-scale data.<br\/><br\/>The goal of this project is to fill this gap by formulating a new framework that can effectively learn representations from complex environments and scale to large data. Specifically, we propose novel approaches for learning robust representations from large-scale data by (1) controlling the complexity of the feature representations and (2) adaptively modeling relevant patterns in the presence of significant amounts of irrelevant patterns or noise.<br\/><br\/>Key intellectual contributions of this project will be (1) a novel framework of representation learning that provides robust representations from large amounts of unlabeled data and relatively small amounts of labeled data, and (2) theoretical and algorithmic advances for inference, learning, and related optimization problems in representation learning for large-scale, complex sensory information processing.<br\/><br\/>This work will serve as a catalyst leading to applications, such as multimedia processing and search, medical image processing, speech recognition, and autonomous navigation. The results will be disseminated through publications and free software.","title":"EAGER: Toward Scalable Life-long Representation Learning","awardID":"1247414","effectiveDate":"2012-08-01","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7495","name":"ROBUST INTELLIGENCE"}}],"PIcoPI":["540682"],"PO":["562760"]},"199054":{"abstract":"The emerging wireless paradigm of dynamic spectrum access via cognitive radio technology has been increasingly recognized for its great potential in drastically enhancing spectrum utilization efficiency. The basic requirements of cognitive radio networks (CRNs) are to protect licensed primary users and provide reliable dynamic spectrum access to secondary cognitive users, which give rise to a new fundamental issue in spectrum access related security. This project develops a comprehensive security system that lays down a secure backbone for CRNs that coexist with primary networks under various network architectures and spectrum coexistence paradigms. The developed security measures are coherently embedded into the entire CRN, from the very beginning of the spectrum sensing stage to the dynamic spectrum access process until the data communication stage. <br\/><br\/>Targeting three main sources of CRN security vulnerability, our research objectives and thrusts include: 1) systematically identify the unique primary user related attacks in CRNs and develop a suite of attack detection and defense mechanisms; 2) develop secure and robust strategies of dynamic spectrum access for benign cognitive users; 3) design confidential and anonymous mechanisms to solve the distinct challenges in privacy protection, taking full advantage of the unprecedented flexibility that CRNs offer in dynamic spectrum utilization. This project lays out the foundation for the development of enabling security technologies for the new paradigm of dynamic spectrum sharing, which in turn can substantially improve the spectrum utilization efficiency of wireless networks, offering a multitude of new cognitive radio devices and wireless services with secure and reliable spectrum access.","title":"TC: Small: Security Provisioning for Cognitive Radio Networks","awardID":"1252643","effectiveDate":"2012-08-01","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7795","name":"TRUSTWORTHY COMPUTING"}}],"PIcoPI":[534006],"PO":["565303"]},"193082":{"abstract":"The most recent trend in chip design is to integrate general purpose central processing units (CPUs) with graphics processing units (GPUs) onto a single microprocessor chip. Looking beyond the obvious benefits of simply putting components closer together, such integration presents an unprecedented opportunity for the CPU and GPU to collaborate, yielding a system whose performance far exceeds the sum of its parts. Whereas, currently, the CPU and GPU are delegated different tasks that each is suited for, this project explores new ways for the CPU and GPU to tackle and collaborate on the same task. The collaboration is fundamentally different from conventional parallel processing, because the CPU and GPU have radically different architectures. In particular, the CPU performs novel meta-computation that assists a GPU task, or vice versa. This innovative approach uncovers new opportunities for emerging heterogeneous architectures. <br\/><br\/>The project investigates CPU\/GPU collaborative execution paradigms to overcome fundamental limitations of both CPU and GPU computing tasks. The GPU achieves high computational throughput and energy efficiency by executing a single instruction on many data items. Its efficiency is severely degraded if some data items are not available due to long memory access latency or different data items require different operations. The CPU\/GPU collaboration leverages the CPU to run far ahead of the GPU to prefetch the data and reorganize the operations needed for different data items so as to drastically improve the GPU efficiency. Conversely, on the CPU side, the CPU\/GPU collaboration leverages the GPU's parallel processing power to accelerate auxiliary computations that greatly enrich the CPU program. Locality analysis, for instance, reveals the nature of memory accesses but requires high computation time when running on a CPU. GPU acceleration makes it possible to perform locality analysis simultaneously with the CPU program and adapt the memory hierarchy on-the-fly to improve CPU performance. The research cuts through software and hardware layers. From the software perspective, the project develops automated approaches to generate code for collaborative execution. From the hardware perspective, future architectures are defined to facilitate more effective CPU\/GPU collaboration. The automated software approach adds value to current and upcoming microprocessors by enabling them to run more efficiently. The performance improvement and energy savings translate directly into enhanced user experience.","title":"SHF: Small: CPU-GPU Collaborative Execution in Fusion Architectures","awardID":"1216569","effectiveDate":"2012-08-01","expirationDate":"2015-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[517241],"PO":["366560"]},"205468":{"abstract":"Due to the open nature of wireless communications, it is important to provide a comprehensive security solution to protect information exchanged over the wireless medium. Two approaches have been adopted in the existing studies. In the first approach, physical layer properties of wireless channels are exploited to achieve provably secure information transmission. However, the study of practical models that involve active attackers and dishonest legitimate users has been limited. In the second approach, cryptographic schemes that can deal with general practical models are adopted. However, these schemes ignore the physical properties of wireless channels and rely on unproved intractability assumptions. It is critical to create synergies between these two approaches and design secure wireless systems that can deal with general scenarios while providing analytical security guarantees.<br\/><br\/>This project aims to systematically exploit wireless channel physical layer resources to design secure wireless systems that provide provable confidentiality, integrity and privacy. Two main thrusts are investigated in this project. In the first thrust, a layered design approach, which uses physical layer resources to generate information theoretically secure keys needed for upper layer security primitives, is developed. In the second thrust, a cross-layer design approach that explicitly exploits physical layer resources in the design of upper layer security primitives is studied. This approach involves the development of novel performance metrics that have elements from both information theory and cryptography and the design of schemes under these new metrics. The research is fully integrated with three phases of engineering education: a) outreach to high school students; b) project- and research-oriented undergraduate student education; and c) graduate students training through new course development, mentoring and industry collaboration.","title":"CAREER: Building Secure Wireless Communication Systems via Physical Layer Resources","awardID":"1318980","effectiveDate":"2012-08-01","expirationDate":"2016-04-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":["551202"],"PO":["564924"]},"187340":{"abstract":"Markov chain simulation is a very general technique applied to a wide spectrum of problems in the physical sciences. From explicit simulations of physical and dynamical processes, such as fluid dynamics and spin systems, to algorithms for sampling from probability distributions over enormous sets of combinatorial objects, Markov chains are ubiquitous. This project will seek to improve the design and analysis of such algorithms, leading to faster running times.<br\/><br\/>Understanding the performance of a Markov Chain Monte Carlo algorithm involves proving bounds on how quickly it approaches its limiting, or \"stationary\", distribution. Due to the inherent randomness in any Markov chain simulation, there is often no reliable empirical criterion for measuring this convergence; rather, one must rely on theoretical guarantees. The PI will focus on the twin long-standing problems of how to redesign Markov chains to actually converge faster, and of proving better convergence guarantees, both of which allow us to safely terminate MCMC simulations sooner. Over the course of this work, these goals will be approached using techniques from three main thematic groupings:<br\/><br\/>1. In the \"Coupling Method\" for proving convergence bounds, one seeks to show that two copies of a Markov chain can be \"made to approach each other\" under some metric on the state space. To improve this kind of analysis, the PI seeks to find better metrics, i.e., better definitions of the distance between two states.<br\/><br\/>2. Several different mathematical notions of duality have played important roles in the analysis of Markov chains. For example, the duality between the spin system and the cluster characterization of the Ising model, a standard model of magnetic materials, the high-temperature\/low-temperature duality for the Potts model on a planar graph, and strong stationary duality, which underlies a recently introduced technique called the Evolving Sets method.<br\/><br\/>3. The PI will attempt to convert reversible Markov chains into non-reversible \"lifted\" Markov chains, by adding additional \"momentum\" information to the states. These lifted chains allow sampling from the original distribution, but can run quadratically faster.<br\/><br\/>The project will include the creation and deployment of a free web resource, \"Markov Chains Central,\" which will include a collection of new and existing laboratory applets for simulating and experimenting with Markov chains and various measures of convergence. These applets will help students visualize Markov chains and understand them through experimentation and play. The project also features an integrated educational plan, which provides for wide dissemination of generated knowledge and educational materials. This work will support undergraduate and graduate student research and mentoring. Effort will be made to maximize involvement of women and minority students.","title":"CAREER: Innovations in Markov Chains: Metrics, Duality and Liftings","awardID":"1150281","effectiveDate":"2012-08-01","expirationDate":"2017-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["518608"],"PO":["565251"]},"198054":{"abstract":"This award is a travel grant to fund underrepresented groups to attend the 2012 Software Engineering Educators Symposium (SEES12) and the 2012 ACM SIGSOFT Conference on Foundations of Software Engineering (FSE12). This proposal aims to help address problems of the low representation of women and minorities in computer science graduate degree programs in the United States. It will supply travel awards to educators from minority-serving institutions (MSIs), liberal arts colleges, and primarily teaching universities to attend the 2012 Software Engineering Educators? Symposium (SEES12) and the 2012 ACM SIGSOFT Conference on Foundations of Software Engineering (FSE12). SEES12 aims to (1) expose educators to tools and techniques for teaching software engineering and programming that they can use to interest their students and better prepare them for graduate work in SE; and (2) establish connections between educators at PhD granting research universities and those at target institutions that will serve as avenues for recruitment of underrepresented minorities and women.","title":"Group Travel Grant for Faculty at Colleges and Universities Serving Minorities and Women: 2012 Software Engineering Educators' Symposium","awardID":"1247416","effectiveDate":"2012-08-15","expirationDate":"2014-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7798","name":"SOFTWARE & HARDWARE FOUNDATION"}}],"PIcoPI":[531512,531513],"PO":["564388"]},"189353":{"abstract":"This collaborative research project (IIS-1160894, W. Bruce Croft, University of Massachusetts Amherst and IIS-1160862, Jamie Callan, Carnegie-Mellon University) addresses the complex issues of ephemeral information that is generated as part of social interactions is different in terms of time scale, quantity, and quality to archival information found on the web. This project investigates the hypothesis that, because of the context provided, searching either ephemeral or archival information is enhanced using the connections between them. It develops new retrieval models and features for ranking functions in a range of search tasks that can exploit an integrated ephemeral\/archival network. Some search tasks are based on previous TREC blog, microblog, and web activities. It also investigates two new tasks, conversation retrieval and aggregated social search. Conversation retrieval targets information units in the form of \"conversations\" or \"events\" instead of simply retrieving social postings or web pages. Aggregated social search ranks information in different granularities, such as sentence, posting, conversation, or thread, based on the underlying query intent. <br\/><br\/>Research that explores the connections between ephemeral and archival information requires a dataset that contains both types of information. A crucial part of this project extends the archival ClueWeb12 dataset with ephemeral microblog, blog, and discussion forum data that links to the web data. This extension is distributed to the research community as the ClueWeb12++ dataset. This project (http:\/\/ciir.cs.umass.edu\/research\/ephemeral\/) is the first to address the full possibilities of search that exploits all the connections and contexts created by bringing together the two \"worlds\" of information. It also develops and distributes a unique new dataset that supports the development of a new generation of tools to access a broad range of information. Students at collaborating institutions, University of Massachusetts Amherst and Carnegie-Mellon University will be involved in educational activities and benefit from research experience.","title":"III: Medium: Collaborative Research: Connecting the Ephemeral and Archival Information Networks","awardID":"1160894","effectiveDate":"2012-08-01","expirationDate":"2016-07-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"7364","name":"INFO INTEGRATION & INFORMATICS"}}],"PIcoPI":[507668],"PO":["563751"]},"200188":{"abstract":"","title":"Program Evaluation Feasibility Study for Two OneNSF Investments: Secure Trustworthy Cyberspace, and Cyber-enabled Materials, Manufacturing and Smart Systems","awardID":"1258430","effectiveDate":"2012-08-15","expirationDate":"2014-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}}],"PIcoPI":["564427"],"PO":["564428"]}}