{"209999":{"abstract":"A sketch of a massive dataset is some compression of it which still allows for answering, sometimes only approximately, some pre-specified types of queries about the data. For many query types of interest, it turns out that sketches exist that provide exponentially smaller compressions. This feature has made sketching methods pervasive in coping with recent trends in data explosion to reduce both communication bandwidth and required storage capacity. Sketching has also been applied to obtain algorithmic speedup for certain high-dimensional problems such as nearest neighbor search, clustering, and low-rank approximation for large matrices, as well as to enable more efficient signal acquisition in a field that has come to be known as compressed sensing. This research plans to further the state of knowledge concerning three intertwined subtopics of sketching: streaming, dimensionality reduction, and compressed sensing.<br\/><br\/>A fundamental question the PI will investigate is whether one can design sketches that are moderately \"universal\", in that the same sketch can be used to answer many different types of queries. Dimensionality reduction has been successfully used to circumvent the so-called \"curse of dimensionality\" in many problems, where the best known algorithms have running times that scale poorly with dimension. This research plans to study the tradeoffs between approximation quality, number of vectors in the data set, and target dimension, and to close gaps between known upper and lower bounds. Compressed sensing has found applications in a diverse range of areas, such as magnetic resonance imaging and photography. This research plans to investigate more efficient compressed sensing schemes for providing various types of approximate recovery guarantees.","title":"CAREER: Sketching Algorithms for Massive Data","awardID":"1350670","effectiveDate":"2014-05-01","expirationDate":"2019-04-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7796","name":"ALGORITHMIC FOUNDATIONS"}}],"PIcoPI":[562913],"PO":["565251"]},"209996":{"abstract":"In recent years, we have witnessed an explosion in the amounts of data being acquired and analyzed in a wide variety of contexts. Data-driven techniques are increasingly applied, not only in the traditional quantitative sciences, but also throughout the social sciences and in a variety of other non-traditional scenarios that challenge many of our common assumptions. For example, in contexts such as collaborative filtering, personalized and predictive medicine, and personalized learning systems, we face a variety of challenges due largely to the fact that an important -- often the only -- source of data is people. In these and many other modern applications, we want to learn about people using the data that people supply.<br\/><br\/>This presents several difficulties, including the fact that such data is often very \"coarse\" or heavily \"quantized\". It might even be binary or entirely nonmetric data consisting of categories or comparisons. Moreover, in many of these cases it is impossible to fully sample the data, and the underlying data of interest may be constantly changing, necessitating approaches that can handle incomplete observations and dynamic data models. This research confronts these difficulties by building on recent progress in the design of efficient algorithms for exploiting low-dimensional structure to perform inference, often using highly incomplete and coarse observations. This research addresses a number of fundamental theoretical and algorithmic questions in the context of low-rank matrix recovery, nonmetric multidimensional scaling, unfolding, and low-dimensional dynamic models. It has applications in contexts such as collaborative filtering, personalized and predictive medicine, and personalized learning systems.","title":"CAREER: Learning from Coarse, Nonmetric, and Incomplete Data","awardID":"1350616","effectiveDate":"2014-05-15","expirationDate":"2019-04-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7797","name":"COMM & INFORMATION FOUNDATIONS"}}],"PIcoPI":[562907],"PO":["564898"]},"210827":{"abstract":"Authentication is the first step in securing a networked application. Traditional authentication relies on cryptographic algorithms that verify users identify based on certain pre-configured secrets. In a world with \"trillions\" of mobile objects (\"computation in everything\") rigorous key management techniques are difficult, the traceability of pre-configured secrets is compromised, and crypto-based authentication schemes become ineffective.<br\/><br\/>This project will investigate an alternative approach, referred to as cognitive security, to address this compelling problem. Cognitive security aims to supplement the crypto-based solutions with unique, unforgeable, and robust credentials that are inherent to the network entities such as mobile devices and users. For example, a location claim can be verified by a location \"fingerprint\" constructed from the ambient radio signals presented at the said location at the said time. A user's identity can be verified by knowledge the verifier learned from the user's online social networks. These credentials are physical properties of the mobile devices or knowledge naturally known to the users, which do not have to be pre-configured or remembered. Mechanisms will be developed so that these credentials can be collected or learnt through the normal network operations and be used to securely verify a device or a user's identity or claims. <br\/><br\/>This is an international collaborative project, between US and Japan. The project will involve multiple areas of information technology, including security, wireless communications and networking, and machine learning. Cognitive security is a promising new approach to mobile network security where the security of the device or secrets stored in the device cannot be guaranteed.","title":"NeTS: JUNO: Cognitive Security: A New Approach to Securing Future Large Scale and Distributed Mobile Applications","awardID":"1405747","effectiveDate":"2014-05-01","expirationDate":"2017-04-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1714","name":"SPECIAL PROJECTS - CISE"}}],"PIcoPI":[564847,564848],"PO":["564993"]},"209986":{"abstract":"Cloud computing is pervasive, but cloud service outages still take place. This proposal addresses how to ensure that failure recovery will work robustly in many deployment scenarios. To address this important question, this project proposes drill-ready cloud computing (DrCloud), a new dependability paradigm that advocates cloud systems to routinely perform \"failure drills\" in real deployments (i.e., deliberately schedule real failures rather than waiting for unexpected real failures to happen). This practice can unearth in-production recovery issues and prevent real outages. <br\/><br\/>This project will create five building blocks of drill-ready cloud computing: methodology, safety, efficiency, usability, and generality. Specifically, these five sub-projects will substantiate a new research methodology via a formal study of hundreds of in-production recovery issues, devise mechanisms that guarantee safety (no data loss and performance disruptions) analogous to a proper fire drill preparation, develop techniques that maximize resource and monetary efficiency of drill execution, design a specification language and its runtime that simplifies drill usability, and finally boost drill generality beyond failure drills (e.g., supporting software upgrade and configuration change drills). <br\/><br\/>The DrCloud project will enrich decades of research and literature in fault-tolerant computing. The project will also bring many direct benefits to the society; users from many areas increasingly use large-scale storage and computation services, depending on high availability and predictability that drill-ready cloud computing will facilitate. The project will also involve state-of-the-art scale-out cloud systems (Hadoop, Cassandra, HBase, etc.). Adding drill-readiness to these systems will provide prototypes of next-generation reliable systems.","title":"CAREER: DrCloud: Drill-Ready Cloud Computing","awardID":"1350499","effectiveDate":"2014-05-01","expirationDate":"2019-04-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7354","name":"COMPUTER SYSTEMS"}}],"PIcoPI":[562887],"PO":["565319"]}}