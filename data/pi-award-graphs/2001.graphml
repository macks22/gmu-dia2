<?xml version="1.0" encoding="UTF-8"?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
<!-- Created by igraph -->
  <key id="v_name" for="node" attr.name="name" attr.type="double"/>
  <key id="v_label" for="node" attr.name="label" attr.type="double"/>
  <key id="e_effectiveDate" for="edge" attr.name="effectiveDate" attr.type="string"/>
  <key id="e_title" for="edge" attr.name="title" attr.type="string"/>
  <key id="e_abstract" for="edge" attr.name="abstract" attr.type="string"/>
  <key id="e_pgm" for="edge" attr.name="pgm" attr.type="string"/>
  <key id="e_label" for="edge" attr.name="label" attr.type="double"/>
  <key id="e_expirationDate" for="edge" attr.name="expirationDate" attr.type="string"/>
  <key id="e_div" for="edge" attr.name="div" attr.type="string"/>
  <key id="e_awardID" for="edge" attr.name="awardID" attr.type="double"/>
  <key id="e_dir" for="edge" attr.name="dir" attr.type="string"/>
  <graph id="G" edgedefault="undirected">
    <node id="n0">
      <data key="v_name">539892</data>
      <data key="v_label">539892</data>
    </node>
    <node id="n1">
      <data key="v_name">462214</data>
      <data key="v_label">462214</data>
    </node>
    <node id="n2">
      <data key="v_name">438098</data>
      <data key="v_label">438098</data>
    </node>
    <node id="n3">
      <data key="v_name">496031</data>
      <data key="v_label">496031</data>
    </node>
    <node id="n4">
      <data key="v_name">507725</data>
      <data key="v_label">507725</data>
    </node>
    <node id="n5">
      <data key="v_name">188405</data>
      <data key="v_label">188405</data>
    </node>
    <node id="n6">
      <data key="v_name">324990</data>
      <data key="v_label">324990</data>
    </node>
    <node id="n7">
      <data key="v_name">528136</data>
      <data key="v_label">528136</data>
    </node>
    <node id="n8">
      <data key="v_name">124950</data>
      <data key="v_label">124950</data>
    </node>
    <node id="n9">
      <data key="v_name">124951</data>
      <data key="v_label">124951</data>
    </node>
    <node id="n10">
      <data key="v_name">522819</data>
      <data key="v_label">522819</data>
    </node>
    <node id="n11">
      <data key="v_name">421169</data>
      <data key="v_label">421169</data>
    </node>
    <node id="n12">
      <data key="v_name">183505</data>
      <data key="v_label">183505</data>
    </node>
    <node id="n13">
      <data key="v_name">554461</data>
      <data key="v_label">554461</data>
    </node>
    <node id="n14">
      <data key="v_name">146169</data>
      <data key="v_label">146169</data>
    </node>
    <node id="n15">
      <data key="v_name">342332</data>
      <data key="v_label">342332</data>
    </node>
    <node id="n16">
      <data key="v_name">499748</data>
      <data key="v_label">499748</data>
    </node>
    <node id="n17">
      <data key="v_name">143931</data>
      <data key="v_label">143931</data>
    </node>
    <node id="n18">
      <data key="v_name">148028</data>
      <data key="v_label">148028</data>
    </node>
    <node id="n19">
      <data key="v_name">179385</data>
      <data key="v_label">179385</data>
    </node>
    <node id="n20">
      <data key="v_name">516924</data>
      <data key="v_label">516924</data>
    </node>
    <node id="n21">
      <data key="v_name">554383</data>
      <data key="v_label">554383</data>
    </node>
    <node id="n22">
      <data key="v_name">436244</data>
      <data key="v_label">436244</data>
    </node>
    <node id="n23">
      <data key="v_name">535238</data>
      <data key="v_label">535238</data>
    </node>
    <node id="n24">
      <data key="v_name">521130</data>
      <data key="v_label">521130</data>
    </node>
    <node id="n25">
      <data key="v_name">169508</data>
      <data key="v_label">169508</data>
    </node>
    <node id="n26">
      <data key="v_name">539354</data>
      <data key="v_label">539354</data>
    </node>
    <node id="n27">
      <data key="v_name">173452</data>
      <data key="v_label">173452</data>
    </node>
    <node id="n28">
      <data key="v_name">179385</data>
    </node>
    <node id="n29">
      <data key="v_name">516924</data>
    </node>
    <node id="n30">
      <data key="v_name">553537</data>
      <data key="v_label">553537</data>
    </node>
    <node id="n31">
      <data key="v_name">393307</data>
      <data key="v_label">393307</data>
    </node>
    <node id="n32">
      <data key="v_name">558560</data>
      <data key="v_label">558560</data>
    </node>
    <node id="n33">
      <data key="v_name">564777</data>
      <data key="v_label">564777</data>
    </node>
    <node id="n34">
      <data key="v_name">547675</data>
      <data key="v_label">547675</data>
    </node>
    <node id="n35">
      <data key="v_name">550149</data>
      <data key="v_label">550149</data>
    </node>
    <node id="n36">
      <data key="v_name">172780</data>
      <data key="v_label">172780</data>
    </node>
    <node id="n37">
      <data key="v_name">163585</data>
      <data key="v_label">163585</data>
    </node>
    <node id="n38">
      <data key="v_name">154611</data>
      <data key="v_label">154611</data>
    </node>
    <node id="n39">
      <data key="v_name">333704</data>
      <data key="v_label">333704</data>
    </node>
    <node id="n40">
      <data key="v_name">466602</data>
      <data key="v_label">466602</data>
    </node>
    <node id="n41">
      <data key="v_name">335395</data>
      <data key="v_label">335395</data>
    </node>
    <node id="n42">
      <data key="v_name">564821</data>
      <data key="v_label">564821</data>
    </node>
    <node id="n43">
      <data key="v_name">451890</data>
      <data key="v_label">451890</data>
    </node>
    <node id="n44">
      <data key="v_name">94260</data>
      <data key="v_label">94260</data>
    </node>
    <node id="n45">
      <data key="v_name">468851</data>
      <data key="v_label">468851</data>
    </node>
    <node id="n46">
      <data key="v_name">530056</data>
      <data key="v_label">530056</data>
    </node>
    <node id="n47">
      <data key="v_name">292937</data>
      <data key="v_label">292937</data>
    </node>
    <node id="n48">
      <data key="v_name">539728</data>
      <data key="v_label">539728</data>
    </node>
    <node id="n49">
      <data key="v_name">316455</data>
      <data key="v_label">316455</data>
    </node>
    <node id="n50">
      <data key="v_name">146648</data>
      <data key="v_label">146648</data>
    </node>
    <node id="n51">
      <data key="v_name">168683</data>
      <data key="v_label">168683</data>
    </node>
    <node id="n52">
      <data key="v_name">146650</data>
      <data key="v_label">146650</data>
    </node>
    <node id="n53">
      <data key="v_name">146651</data>
      <data key="v_label">146651</data>
    </node>
    <node id="n54">
      <data key="v_name">168685</data>
      <data key="v_label">168685</data>
    </node>
    <node id="n55">
      <data key="v_name">322427</data>
      <data key="v_label">322427</data>
    </node>
    <node id="n56">
      <data key="v_name">455489</data>
      <data key="v_label">455489</data>
    </node>
    <node id="n57">
      <data key="v_name">146255</data>
      <data key="v_label">146255</data>
    </node>
    <node id="n58">
      <data key="v_name">146256</data>
      <data key="v_label">146256</data>
    </node>
    <node id="n59">
      <data key="v_name">538637</data>
      <data key="v_label">538637</data>
    </node>
    <node id="n60">
      <data key="v_name">146921</data>
      <data key="v_label">146921</data>
    </node>
    <node id="n61">
      <data key="v_name">208531</data>
      <data key="v_label">208531</data>
    </node>
    <node id="n62">
      <data key="v_name">209556</data>
      <data key="v_label">209556</data>
    </node>
    <node id="n63">
      <data key="v_name">479049</data>
      <data key="v_label">479049</data>
    </node>
    <node id="n64">
      <data key="v_name">558485</data>
      <data key="v_label">558485</data>
    </node>
    <node id="n65">
      <data key="v_name">287642</data>
      <data key="v_label">287642</data>
    </node>
    <node id="n66">
      <data key="v_name">228065</data>
      <data key="v_label">228065</data>
    </node>
    <node id="n67">
      <data key="v_name">521208</data>
      <data key="v_label">521208</data>
    </node>
    <node id="n68">
      <data key="v_name">555419</data>
      <data key="v_label">555419</data>
    </node>
    <node id="n69">
      <data key="v_name">497006</data>
      <data key="v_label">497006</data>
    </node>
    <node id="n70">
      <data key="v_name">508477</data>
      <data key="v_label">508477</data>
    </node>
    <node id="n71">
      <data key="v_name">539633</data>
      <data key="v_label">539633</data>
    </node>
    <node id="n72">
      <data key="v_name">531018</data>
      <data key="v_label">531018</data>
    </node>
    <node id="n73">
      <data key="v_name">433896</data>
      <data key="v_label">433896</data>
    </node>
    <node id="n74">
      <data key="v_name">517543</data>
      <data key="v_label">517543</data>
    </node>
    <node id="n75">
      <data key="v_name">550936</data>
      <data key="v_label">550936</data>
    </node>
    <node id="n76">
      <data key="v_name">531861</data>
      <data key="v_label">531861</data>
    </node>
    <node id="n77">
      <data key="v_name">546946</data>
      <data key="v_label">546946</data>
    </node>
    <node id="n78">
      <data key="v_name">491422</data>
      <data key="v_label">491422</data>
    </node>
    <node id="n79">
      <data key="v_name">169508</data>
    </node>
    <node id="n80">
      <data key="v_name">169508</data>
    </node>
    <node id="n81">
      <data key="v_name">409074</data>
      <data key="v_label">409074</data>
    </node>
    <node id="n82">
      <data key="v_name">547712</data>
      <data key="v_label">547712</data>
    </node>
    <node id="n83">
      <data key="v_name">551055</data>
      <data key="v_label">551055</data>
    </node>
    <node id="n84">
      <data key="v_name">486529</data>
      <data key="v_label">486529</data>
    </node>
    <node id="n85">
      <data key="v_name">237056</data>
      <data key="v_label">237056</data>
    </node>
    <node id="n86">
      <data key="v_name">563571</data>
      <data key="v_label">563571</data>
    </node>
    <node id="n87">
      <data key="v_name">129387</data>
      <data key="v_label">129387</data>
    </node>
    <node id="n88">
      <data key="v_name">129388</data>
      <data key="v_label">129388</data>
    </node>
    <node id="n89">
      <data key="v_name">148178</data>
      <data key="v_label">148178</data>
    </node>
    <node id="n90">
      <data key="v_name">533355</data>
      <data key="v_label">533355</data>
    </node>
    <node id="n91">
      <data key="v_name">550026</data>
      <data key="v_label">550026</data>
    </node>
    <node id="n92">
      <data key="v_name">533299</data>
      <data key="v_label">533299</data>
    </node>
    <node id="n93">
      <data key="v_name">518152</data>
      <data key="v_label">518152</data>
    </node>
    <node id="n94">
      <data key="v_name">462739</data>
      <data key="v_label">462739</data>
    </node>
    <node id="n95">
      <data key="v_name">393313</data>
      <data key="v_label">393313</data>
    </node>
    <node id="n96">
      <data key="v_name">483825</data>
      <data key="v_label">483825</data>
    </node>
    <node id="n97">
      <data key="v_name">553596</data>
      <data key="v_label">553596</data>
    </node>
    <node id="n98">
      <data key="v_name">547675</data>
    </node>
    <node id="n99">
      <data key="v_name">550484</data>
      <data key="v_label">550484</data>
    </node>
    <node id="n100">
      <data key="v_name">445172</data>
      <data key="v_label">445172</data>
    </node>
    <node id="n101">
      <data key="v_name">445173</data>
      <data key="v_label">445173</data>
    </node>
    <node id="n102">
      <data key="v_name">168075</data>
      <data key="v_label">168075</data>
    </node>
    <node id="n103">
      <data key="v_name">300026</data>
      <data key="v_label">300026</data>
    </node>
    <node id="n104">
      <data key="v_name">444397</data>
      <data key="v_label">444397</data>
    </node>
    <node id="n105">
      <data key="v_name">486066</data>
      <data key="v_label">486066</data>
    </node>
    <node id="n106">
      <data key="v_name">397660</data>
      <data key="v_label">397660</data>
    </node>
    <node id="n107">
      <data key="v_name">148284</data>
      <data key="v_label">148284</data>
    </node>
    <node id="n108">
      <data key="v_name">525589</data>
      <data key="v_label">525589</data>
    </node>
    <node id="n109">
      <data key="v_name">460591</data>
      <data key="v_label">460591</data>
    </node>
    <node id="n110">
      <data key="v_name">560602</data>
      <data key="v_label">560602</data>
    </node>
    <node id="n111">
      <data key="v_name">140761</data>
      <data key="v_label">140761</data>
    </node>
    <node id="n112">
      <data key="v_name">530762</data>
      <data key="v_label">530762</data>
    </node>
    <node id="n113">
      <data key="v_name">550462</data>
      <data key="v_label">550462</data>
    </node>
    <node id="n114">
      <data key="v_name">518384</data>
      <data key="v_label">518384</data>
    </node>
    <node id="n115">
      <data key="v_name">159584</data>
      <data key="v_label">159584</data>
    </node>
    <node id="n116">
      <data key="v_name">406745</data>
      <data key="v_label">406745</data>
    </node>
    <node id="n117">
      <data key="v_name">323015</data>
      <data key="v_label">323015</data>
    </node>
    <node id="n118">
      <data key="v_name">159587</data>
      <data key="v_label">159587</data>
    </node>
    <node id="n119">
      <data key="v_name">549853</data>
      <data key="v_label">549853</data>
    </node>
    <node id="n120">
      <data key="v_name">156824</data>
      <data key="v_label">156824</data>
    </node>
    <node id="n121">
      <data key="v_name">147466</data>
      <data key="v_label">147466</data>
    </node>
    <node id="n122">
      <data key="v_name">553259</data>
      <data key="v_label">553259</data>
    </node>
    <node id="n123">
      <data key="v_name">475435</data>
      <data key="v_label">475435</data>
    </node>
    <node id="n124">
      <data key="v_name">257046</data>
      <data key="v_label">257046</data>
    </node>
    <node id="n125">
      <data key="v_name">146313</data>
      <data key="v_label">146313</data>
    </node>
    <node id="n126">
      <data key="v_name">146314</data>
      <data key="v_label">146314</data>
    </node>
    <node id="n127">
      <data key="v_name">550629</data>
      <data key="v_label">550629</data>
    </node>
    <node id="n128">
      <data key="v_name">147996</data>
      <data key="v_label">147996</data>
    </node>
    <node id="n129">
      <data key="v_name">550373</data>
      <data key="v_label">550373</data>
    </node>
    <node id="n130">
      <data key="v_name">148885</data>
      <data key="v_label">148885</data>
    </node>
    <node id="n131">
      <data key="v_name">507668</data>
      <data key="v_label">507668</data>
    </node>
    <node id="n132">
      <data key="v_name">283200</data>
      <data key="v_label">283200</data>
    </node>
    <node id="n133">
      <data key="v_name">530662</data>
      <data key="v_label">530662</data>
    </node>
    <node id="n134">
      <data key="v_name">464128</data>
      <data key="v_label">464128</data>
    </node>
    <node id="n135">
      <data key="v_name">550054</data>
      <data key="v_label">550054</data>
    </node>
    <node id="n136">
      <data key="v_name">486320</data>
      <data key="v_label">486320</data>
    </node>
    <node id="n137">
      <data key="v_name">550636</data>
      <data key="v_label">550636</data>
    </node>
    <node id="n138">
      <data key="v_name">148351</data>
      <data key="v_label">148351</data>
    </node>
    <node id="n139">
      <data key="v_name">517810</data>
      <data key="v_label">517810</data>
    </node>
    <node id="n140">
      <data key="v_name">560285</data>
      <data key="v_label">560285</data>
    </node>
    <node id="n141">
      <data key="v_name">521840</data>
      <data key="v_label">521840</data>
    </node>
    <node id="n142">
      <data key="v_name">509594</data>
      <data key="v_label">509594</data>
    </node>
    <node id="n143">
      <data key="v_name">553643</data>
      <data key="v_label">553643</data>
    </node>
    <node id="n144">
      <data key="v_name">517236</data>
      <data key="v_label">517236</data>
    </node>
    <node id="n145">
      <data key="v_name">402461</data>
      <data key="v_label">402461</data>
    </node>
    <node id="n146">
      <data key="v_name">551079</data>
      <data key="v_label">551079</data>
    </node>
    <node id="n147">
      <data key="v_name">409715</data>
      <data key="v_label">409715</data>
    </node>
    <node id="n148">
      <data key="v_name">309179</data>
      <data key="v_label">309179</data>
    </node>
    <node id="n149">
      <data key="v_name">298189</data>
      <data key="v_label">298189</data>
    </node>
    <node id="n150">
      <data key="v_name">234699</data>
      <data key="v_label">234699</data>
    </node>
    <node id="n151">
      <data key="v_name">157442</data>
      <data key="v_label">157442</data>
    </node>
    <node id="n152">
      <data key="v_name">157443</data>
      <data key="v_label">157443</data>
    </node>
    <node id="n153">
      <data key="v_name">550462</data>
    </node>
    <node id="n154">
      <data key="v_name">517054</data>
      <data key="v_label">517054</data>
    </node>
    <node id="n155">
      <data key="v_name">485936</data>
      <data key="v_label">485936</data>
    </node>
    <node id="n156">
      <data key="v_name">563645</data>
      <data key="v_label">563645</data>
    </node>
    <node id="n157">
      <data key="v_name">523352</data>
      <data key="v_label">523352</data>
    </node>
    <node id="n158">
      <data key="v_name">405032</data>
      <data key="v_label">405032</data>
    </node>
    <node id="n159">
      <data key="v_name">538871</data>
      <data key="v_label">538871</data>
    </node>
    <node id="n160">
      <data key="v_name">547852</data>
      <data key="v_label">547852</data>
    </node>
    <node id="n161">
      <data key="v_name">215008</data>
      <data key="v_label">215008</data>
    </node>
    <node id="n162">
      <data key="v_name">558957</data>
      <data key="v_label">558957</data>
    </node>
    <node id="n163">
      <data key="v_name">226023</data>
      <data key="v_label">226023</data>
    </node>
    <node id="n164">
      <data key="v_name">179862</data>
      <data key="v_label">179862</data>
    </node>
    <node id="n165">
      <data key="v_name">451739</data>
      <data key="v_label">451739</data>
    </node>
    <node id="n166">
      <data key="v_name">155536</data>
      <data key="v_label">155536</data>
    </node>
    <node id="n167">
      <data key="v_name">155537</data>
      <data key="v_label">155537</data>
    </node>
    <node id="n168">
      <data key="v_name">250003</data>
      <data key="v_label">250003</data>
    </node>
    <node id="n169">
      <data key="v_name">532055</data>
      <data key="v_label">532055</data>
    </node>
    <node id="n170">
      <data key="v_name">140587</data>
      <data key="v_label">140587</data>
    </node>
    <node id="n171">
      <data key="v_name">250004</data>
      <data key="v_label">250004</data>
    </node>
    <node id="n172">
      <data key="v_name">140589</data>
      <data key="v_label">140589</data>
    </node>
    <node id="n173">
      <data key="v_name">506686</data>
      <data key="v_label">506686</data>
    </node>
    <node id="n174">
      <data key="v_name">186645</data>
      <data key="v_label">186645</data>
    </node>
    <node id="n175">
      <data key="v_name">218583</data>
      <data key="v_label">218583</data>
    </node>
    <node id="n176">
      <data key="v_name">560018</data>
      <data key="v_label">560018</data>
    </node>
    <node id="n177">
      <data key="v_name">531018</data>
    </node>
    <node id="n178">
      <data key="v_name">404042</data>
      <data key="v_label">404042</data>
    </node>
    <node id="n179">
      <data key="v_name">303104</data>
      <data key="v_label">303104</data>
    </node>
    <node id="n180">
      <data key="v_name">298301</data>
      <data key="v_label">298301</data>
    </node>
    <node id="n181">
      <data key="v_name">152070</data>
      <data key="v_label">152070</data>
    </node>
    <node id="n182">
      <data key="v_name">446082</data>
      <data key="v_label">446082</data>
    </node>
    <node id="n183">
      <data key="v_name">556768</data>
      <data key="v_label">556768</data>
    </node>
    <node id="n184">
      <data key="v_name">451518</data>
      <data key="v_label">451518</data>
    </node>
    <node id="n185">
      <data key="v_name">508528</data>
      <data key="v_label">508528</data>
    </node>
    <node id="n186">
      <data key="v_name">462892</data>
      <data key="v_label">462892</data>
    </node>
    <node id="n187">
      <data key="v_name">140730</data>
      <data key="v_label">140730</data>
    </node>
    <node id="n188">
      <data key="v_name">556912</data>
      <data key="v_label">556912</data>
    </node>
    <node id="n189">
      <data key="v_name">554666</data>
      <data key="v_label">554666</data>
    </node>
    <node id="n190">
      <data key="v_name">151889</data>
      <data key="v_label">151889</data>
    </node>
    <node id="n191">
      <data key="v_name">289452</data>
      <data key="v_label">289452</data>
    </node>
    <node id="n192">
      <data key="v_name">561385</data>
      <data key="v_label">561385</data>
    </node>
    <node id="n193">
      <data key="v_name">40607</data>
      <data key="v_label">40607</data>
    </node>
    <node id="n194">
      <data key="v_name">562576</data>
      <data key="v_label">562576</data>
    </node>
    <node id="n195">
      <data key="v_name">141392</data>
      <data key="v_label">141392</data>
    </node>
    <node id="n196">
      <data key="v_name">531548</data>
      <data key="v_label">531548</data>
    </node>
    <node id="n197">
      <data key="v_name">289988</data>
      <data key="v_label">289988</data>
    </node>
    <node id="n198">
      <data key="v_name">293104</data>
      <data key="v_label">293104</data>
    </node>
    <node id="n199">
      <data key="v_name">485932</data>
      <data key="v_label">485932</data>
    </node>
    <node id="n200">
      <data key="v_name">172870</data>
      <data key="v_label">172870</data>
    </node>
    <node id="n201">
      <data key="v_name">310230</data>
      <data key="v_label">310230</data>
    </node>
    <node id="n202">
      <data key="v_name">310230</data>
    </node>
    <node id="n203">
      <data key="v_name">144657</data>
      <data key="v_label">144657</data>
    </node>
    <node id="n204">
      <data key="v_name">446708</data>
      <data key="v_label">446708</data>
    </node>
    <node id="n205">
      <data key="v_name">144659</data>
      <data key="v_label">144659</data>
    </node>
    <node id="n206">
      <data key="v_name">518680</data>
      <data key="v_label">518680</data>
    </node>
    <node id="n207">
      <data key="v_name">492597</data>
      <data key="v_label">492597</data>
    </node>
    <node id="n208">
      <data key="v_name">550709</data>
      <data key="v_label">550709</data>
    </node>
    <node id="n209">
      <data key="v_name">550342</data>
      <data key="v_label">550342</data>
    </node>
    <node id="n210">
      <data key="v_name">309127</data>
      <data key="v_label">309127</data>
    </node>
    <node id="n211">
      <data key="v_name">301560</data>
      <data key="v_label">301560</data>
    </node>
    <node id="n212">
      <data key="v_name">553633</data>
      <data key="v_label">553633</data>
    </node>
    <node id="n213">
      <data key="v_name">531411</data>
      <data key="v_label">531411</data>
    </node>
    <node id="n214">
      <data key="v_name">263931</data>
      <data key="v_label">263931</data>
    </node>
    <node id="n215">
      <data key="v_name">486250</data>
      <data key="v_label">486250</data>
    </node>
    <node id="n216">
      <data key="v_name">217882</data>
      <data key="v_label">217882</data>
    </node>
    <node id="n217">
      <data key="v_name">409593</data>
      <data key="v_label">409593</data>
    </node>
    <node id="n218">
      <data key="v_name">229551</data>
      <data key="v_label">229551</data>
    </node>
    <node id="n219">
      <data key="v_name">505248</data>
      <data key="v_label">505248</data>
    </node>
    <node id="n220">
      <data key="v_name">485659</data>
      <data key="v_label">485659</data>
    </node>
    <node id="n221">
      <data key="v_name">260890</data>
      <data key="v_label">260890</data>
    </node>
    <node id="n222">
      <data key="v_name">494757</data>
      <data key="v_label">494757</data>
    </node>
    <node id="n223">
      <data key="v_name">553786</data>
      <data key="v_label">553786</data>
    </node>
    <node id="n224">
      <data key="v_name">489959</data>
      <data key="v_label">489959</data>
    </node>
    <node id="n225">
      <data key="v_name">499432</data>
      <data key="v_label">499432</data>
    </node>
    <node id="n226">
      <data key="v_name">271025</data>
      <data key="v_label">271025</data>
    </node>
    <node id="n227">
      <data key="v_name">555090</data>
      <data key="v_label">555090</data>
    </node>
    <node id="n228">
      <data key="v_name">551642</data>
      <data key="v_label">551642</data>
    </node>
    <node id="n229">
      <data key="v_name">258712</data>
      <data key="v_label">258712</data>
    </node>
    <node id="n230">
      <data key="v_name">320975</data>
      <data key="v_label">320975</data>
    </node>
    <node id="n231">
      <data key="v_name">562752</data>
      <data key="v_label">562752</data>
    </node>
    <node id="n232">
      <data key="v_name">449854</data>
      <data key="v_label">449854</data>
    </node>
    <node id="n233">
      <data key="v_name">518515</data>
      <data key="v_label">518515</data>
    </node>
    <node id="n234">
      <data key="v_name">257577</data>
      <data key="v_label">257577</data>
    </node>
    <node id="n235">
      <data key="v_name">518513</data>
      <data key="v_label">518513</data>
    </node>
    <node id="n236">
      <data key="v_name">550895</data>
      <data key="v_label">550895</data>
    </node>
    <node id="n237">
      <data key="v_name">438626</data>
      <data key="v_label">438626</data>
    </node>
    <node id="n238">
      <data key="v_name">393827</data>
      <data key="v_label">393827</data>
    </node>
    <node id="n239">
      <data key="v_name">539704</data>
      <data key="v_label">539704</data>
    </node>
    <node id="n240">
      <data key="v_name">564051</data>
      <data key="v_label">564051</data>
    </node>
    <node id="n241">
      <data key="v_name">505885</data>
      <data key="v_label">505885</data>
    </node>
    <node id="n242">
      <data key="v_name">505886</data>
      <data key="v_label">505886</data>
    </node>
    <node id="n243">
      <data key="v_name">267562</data>
      <data key="v_label">267562</data>
    </node>
    <node id="n244">
      <data key="v_name">412303</data>
      <data key="v_label">412303</data>
    </node>
    <node id="n245">
      <data key="v_name">559216</data>
      <data key="v_label">559216</data>
    </node>
    <node id="n246">
      <data key="v_name">555023</data>
      <data key="v_label">555023</data>
    </node>
    <node id="n247">
      <data key="v_name">565251</data>
      <data key="v_label">565251</data>
    </node>
    <node id="n248">
      <data key="v_name">542495</data>
      <data key="v_label">542495</data>
    </node>
    <node id="n249">
      <data key="v_name">442245</data>
      <data key="v_label">442245</data>
    </node>
    <node id="n250">
      <data key="v_name">157704</data>
      <data key="v_label">157704</data>
    </node>
    <node id="n251">
      <data key="v_name">541869</data>
      <data key="v_label">541869</data>
    </node>
    <node id="n252">
      <data key="v_name">539738</data>
      <data key="v_label">539738</data>
    </node>
    <node id="n253">
      <data key="v_name">543603</data>
      <data key="v_label">543603</data>
    </node>
    <node id="n254">
      <data key="v_name">289878</data>
      <data key="v_label">289878</data>
    </node>
    <node id="n255">
      <data key="v_name">292881</data>
      <data key="v_label">292881</data>
    </node>
    <node id="n256">
      <data key="v_name">54008</data>
      <data key="v_label">54008</data>
    </node>
    <node id="n257">
      <data key="v_name">460483</data>
      <data key="v_label">460483</data>
    </node>
    <node id="n258">
      <data key="v_name">148168</data>
      <data key="v_label">148168</data>
    </node>
    <node id="n259">
      <data key="v_name">433610</data>
      <data key="v_label">433610</data>
    </node>
    <node id="n260">
      <data key="v_name">551444</data>
      <data key="v_label">551444</data>
    </node>
    <node id="n261">
      <data key="v_name">433032</data>
      <data key="v_label">433032</data>
    </node>
    <node id="n262">
      <data key="v_name">382541</data>
      <data key="v_label">382541</data>
    </node>
    <node id="n263">
      <data key="v_name">556730</data>
      <data key="v_label">556730</data>
    </node>
    <node id="n264">
      <data key="v_name">292960</data>
      <data key="v_label">292960</data>
    </node>
    <node id="n265">
      <data key="v_name">561580</data>
      <data key="v_label">561580</data>
    </node>
    <node id="n266">
      <data key="v_name">284472</data>
      <data key="v_label">284472</data>
    </node>
    <node id="n267">
      <data key="v_name">485861</data>
      <data key="v_label">485861</data>
    </node>
    <node id="n268">
      <data key="v_name">560200</data>
      <data key="v_label">560200</data>
    </node>
    <node id="n269">
      <data key="v_name">184167</data>
      <data key="v_label">184167</data>
    </node>
    <node id="n270">
      <data key="v_name">311348</data>
      <data key="v_label">311348</data>
    </node>
    <node id="n271">
      <data key="v_name">559849</data>
      <data key="v_label">559849</data>
    </node>
    <node id="n272">
      <data key="v_name">147217</data>
      <data key="v_label">147217</data>
    </node>
    <node id="n273">
      <data key="v_name">518277</data>
      <data key="v_label">518277</data>
    </node>
    <node id="n274">
      <data key="v_name">311350</data>
      <data key="v_label">311350</data>
    </node>
    <node id="n275">
      <data key="v_name">529182</data>
      <data key="v_label">529182</data>
    </node>
    <node id="n276">
      <data key="v_name">541989</data>
      <data key="v_label">541989</data>
    </node>
    <node id="n277">
      <data key="v_name">561267</data>
      <data key="v_label">561267</data>
    </node>
    <node id="n278">
      <data key="v_name">156063</data>
      <data key="v_label">156063</data>
    </node>
    <node id="n279">
      <data key="v_name">156064</data>
      <data key="v_label">156064</data>
    </node>
    <node id="n280">
      <data key="v_name">559524</data>
      <data key="v_label">559524</data>
    </node>
    <node id="n281">
      <data key="v_name">173930</data>
      <data key="v_label">173930</data>
    </node>
    <node id="n282">
      <data key="v_name">347935</data>
      <data key="v_label">347935</data>
    </node>
    <node id="n283">
      <data key="v_name">453727</data>
      <data key="v_label">453727</data>
    </node>
    <node id="n284">
      <data key="v_name">554329</data>
      <data key="v_label">554329</data>
    </node>
    <node id="n285">
      <data key="v_name">516912</data>
      <data key="v_label">516912</data>
    </node>
    <node id="n286">
      <data key="v_name">478371</data>
      <data key="v_label">478371</data>
    </node>
    <node id="n287">
      <data key="v_name">451003</data>
      <data key="v_label">451003</data>
    </node>
    <node id="n288">
      <data key="v_name">521186</data>
      <data key="v_label">521186</data>
    </node>
    <node id="n289">
      <data key="v_name">155714</data>
      <data key="v_label">155714</data>
    </node>
    <node id="n290">
      <data key="v_name">546850</data>
      <data key="v_label">546850</data>
    </node>
    <node id="n291">
      <data key="v_name">542095</data>
      <data key="v_label">542095</data>
    </node>
    <node id="n292">
      <data key="v_name">290021</data>
      <data key="v_label">290021</data>
    </node>
    <node id="n293">
      <data key="v_name">258685</data>
      <data key="v_label">258685</data>
    </node>
    <node id="n294">
      <data key="v_name">290022</data>
      <data key="v_label">290022</data>
    </node>
    <node id="n295">
      <data key="v_name">485314</data>
      <data key="v_label">485314</data>
    </node>
    <node id="n296">
      <data key="v_name">152439</data>
      <data key="v_label">152439</data>
    </node>
    <node id="n297">
      <data key="v_name">152440</data>
      <data key="v_label">152440</data>
    </node>
    <node id="n298">
      <data key="v_name">551061</data>
      <data key="v_label">551061</data>
    </node>
    <node id="n299">
      <data key="v_name">306203</data>
      <data key="v_label">306203</data>
    </node>
    <node id="n300">
      <data key="v_name">521573</data>
      <data key="v_label">521573</data>
    </node>
    <node id="n301">
      <data key="v_name">176134</data>
      <data key="v_label">176134</data>
    </node>
    <node id="n302">
      <data key="v_name">355581</data>
      <data key="v_label">355581</data>
    </node>
    <node id="n303">
      <data key="v_name">152128</data>
      <data key="v_label">152128</data>
    </node>
    <node id="n304">
      <data key="v_name">521544</data>
      <data key="v_label">521544</data>
    </node>
    <node id="n305">
      <data key="v_name">549800</data>
      <data key="v_label">549800</data>
    </node>
    <node id="n306">
      <data key="v_name">550986</data>
      <data key="v_label">550986</data>
    </node>
    <node id="n307">
      <data key="v_name">518376</data>
      <data key="v_label">518376</data>
    </node>
    <node id="n308">
      <data key="v_name">564558</data>
      <data key="v_label">564558</data>
    </node>
    <node id="n309">
      <data key="v_name">503906</data>
      <data key="v_label">503906</data>
    </node>
    <node id="n310">
      <data key="v_name">445172</data>
    </node>
    <node id="n311">
      <data key="v_name">445173</data>
    </node>
    <node id="n312">
      <data key="v_name">168075</data>
    </node>
    <node id="n313">
      <data key="v_name">313342</data>
      <data key="v_label">313342</data>
    </node>
    <node id="n314">
      <data key="v_name">162171</data>
      <data key="v_label">162171</data>
    </node>
    <node id="n315">
      <data key="v_name">522597</data>
      <data key="v_label">522597</data>
    </node>
    <node id="n316">
      <data key="v_name">328464</data>
      <data key="v_label">328464</data>
    </node>
    <node id="n317">
      <data key="v_name">169920</data>
      <data key="v_label">169920</data>
    </node>
    <node id="n318">
      <data key="v_name">427334</data>
      <data key="v_label">427334</data>
    </node>
    <node id="n319">
      <data key="v_name">550560</data>
      <data key="v_label">550560</data>
    </node>
    <node id="n320">
      <data key="v_name">458795</data>
      <data key="v_label">458795</data>
    </node>
    <node id="n321">
      <data key="v_name">165198</data>
      <data key="v_label">165198</data>
    </node>
    <node id="n322">
      <data key="v_name">551003</data>
      <data key="v_label">551003</data>
    </node>
    <node id="n323">
      <data key="v_name">538796</data>
      <data key="v_label">538796</data>
    </node>
    <node id="n324">
      <data key="v_name">410098</data>
      <data key="v_label">410098</data>
    </node>
    <node id="n325">
      <data key="v_name">561267</data>
    </node>
    <node id="n326">
      <data key="v_name">561267</data>
    </node>
    <node id="n327">
      <data key="v_name">157430</data>
      <data key="v_label">157430</data>
    </node>
    <node id="n328">
      <data key="v_name">490281</data>
      <data key="v_label">490281</data>
    </node>
    <node id="n329">
      <data key="v_name">550407</data>
      <data key="v_label">550407</data>
    </node>
    <node id="n330">
      <data key="v_name">269188</data>
      <data key="v_label">269188</data>
    </node>
    <node id="n331">
      <data key="v_name">167072</data>
      <data key="v_label">167072</data>
    </node>
    <node id="n332">
      <data key="v_name">501232</data>
      <data key="v_label">501232</data>
    </node>
    <node id="n333">
      <data key="v_name">407833</data>
      <data key="v_label">407833</data>
    </node>
    <node id="n334">
      <data key="v_name">151512</data>
      <data key="v_label">151512</data>
    </node>
    <node id="n335">
      <data key="v_name">525365</data>
      <data key="v_label">525365</data>
    </node>
    <node id="n336">
      <data key="v_name">445659</data>
      <data key="v_label">445659</data>
    </node>
    <node id="n337">
      <data key="v_name">515841</data>
      <data key="v_label">515841</data>
    </node>
    <node id="n338">
      <data key="v_name">515842</data>
      <data key="v_label">515842</data>
    </node>
    <node id="n339">
      <data key="v_name">152364</data>
      <data key="v_label">152364</data>
    </node>
    <node id="n340">
      <data key="v_name">528451</data>
      <data key="v_label">528451</data>
    </node>
    <node id="n341">
      <data key="v_name">208531</data>
    </node>
    <node id="n342">
      <data key="v_name">550462</data>
    </node>
    <node id="n343">
      <data key="v_name">212922</data>
      <data key="v_label">212922</data>
    </node>
    <node id="n344">
      <data key="v_name">168426</data>
      <data key="v_label">168426</data>
    </node>
    <node id="n345">
      <data key="v_name">168426</data>
    </node>
    <node id="n346">
      <data key="v_name">430757</data>
      <data key="v_label">430757</data>
    </node>
    <node id="n347">
      <data key="v_name">280544</data>
      <data key="v_label">280544</data>
    </node>
    <node id="n348">
      <data key="v_name">516549</data>
      <data key="v_label">516549</data>
    </node>
    <node id="n349">
      <data key="v_name">157558</data>
      <data key="v_label">157558</data>
    </node>
    <node id="n350">
      <data key="v_name">424075</data>
      <data key="v_label">424075</data>
    </node>
    <node id="n351">
      <data key="v_name">513727</data>
      <data key="v_label">513727</data>
    </node>
    <node id="n352">
      <data key="v_name">464191</data>
      <data key="v_label">464191</data>
    </node>
    <node id="n353">
      <data key="v_name">486115</data>
      <data key="v_label">486115</data>
    </node>
    <node id="n354">
      <data key="v_name">320878</data>
      <data key="v_label">320878</data>
    </node>
    <node id="n355">
      <data key="v_name">517969</data>
      <data key="v_label">517969</data>
    </node>
    <node id="n356">
      <data key="v_name">551029</data>
      <data key="v_label">551029</data>
    </node>
    <node id="n357">
      <data key="v_name">401471</data>
      <data key="v_label">401471</data>
    </node>
    <node id="n358">
      <data key="v_name">191820</data>
      <data key="v_label">191820</data>
    </node>
    <node id="n359">
      <data key="v_name">164101</data>
      <data key="v_label">164101</data>
    </node>
    <node id="n360">
      <data key="v_name">217964</data>
      <data key="v_label">217964</data>
    </node>
    <node id="n361">
      <data key="v_name">549399</data>
      <data key="v_label">549399</data>
    </node>
    <node id="n362">
      <data key="v_name">485574</data>
      <data key="v_label">485574</data>
    </node>
    <node id="n363">
      <data key="v_name">555889</data>
      <data key="v_label">555889</data>
    </node>
    <node id="n364">
      <data key="v_name">555889</data>
    </node>
    <node id="n365">
      <data key="v_name">460643</data>
      <data key="v_label">460643</data>
    </node>
    <node id="n366">
      <data key="v_name">542063</data>
      <data key="v_label">542063</data>
    </node>
    <node id="n367">
      <data key="v_name">226023</data>
    </node>
    <node id="n368">
      <data key="v_name">518003</data>
      <data key="v_label">518003</data>
    </node>
    <node id="n369">
      <data key="v_name">550275</data>
      <data key="v_label">550275</data>
    </node>
    <node id="n370">
      <data key="v_name">289744</data>
      <data key="v_label">289744</data>
    </node>
    <node id="n371">
      <data key="v_name">518569</data>
      <data key="v_label">518569</data>
    </node>
    <node id="n372">
      <data key="v_name">209897</data>
      <data key="v_label">209897</data>
    </node>
    <node id="n373">
      <data key="v_name">554394</data>
      <data key="v_label">554394</data>
    </node>
    <node id="n374">
      <data key="v_name">527984</data>
      <data key="v_label">527984</data>
    </node>
    <node id="n375">
      <data key="v_name">475422</data>
      <data key="v_label">475422</data>
    </node>
    <node id="n376">
      <data key="v_name">157621</data>
      <data key="v_label">157621</data>
    </node>
    <node id="n377">
      <data key="v_name">399214</data>
      <data key="v_label">399214</data>
    </node>
    <node id="n378">
      <data key="v_name">157623</data>
      <data key="v_label">157623</data>
    </node>
    <node id="n379">
      <data key="v_name">209513</data>
      <data key="v_label">209513</data>
    </node>
    <node id="n380">
      <data key="v_name">416071</data>
      <data key="v_label">416071</data>
    </node>
    <node id="n381">
      <data key="v_name">541908</data>
      <data key="v_label">541908</data>
    </node>
    <node id="n382">
      <data key="v_name">554666</data>
    </node>
    <node id="n383">
      <data key="v_name">565201</data>
      <data key="v_label">565201</data>
    </node>
    <node id="n384">
      <data key="v_name">161531</data>
      <data key="v_label">161531</data>
    </node>
    <node id="n385">
      <data key="v_name">451563</data>
      <data key="v_label">451563</data>
    </node>
    <node id="n386">
      <data key="v_name">152242</data>
      <data key="v_label">152242</data>
    </node>
    <node id="n387">
      <data key="v_name">558563</data>
      <data key="v_label">558563</data>
    </node>
    <node id="n388">
      <data key="v_name">156286</data>
      <data key="v_label">156286</data>
    </node>
    <node id="n389">
      <data key="v_name">156287</data>
      <data key="v_label">156287</data>
    </node>
    <node id="n390">
      <data key="v_name">549403</data>
      <data key="v_label">549403</data>
    </node>
    <node id="n391">
      <data key="v_name">346013</data>
      <data key="v_label">346013</data>
    </node>
    <node id="n392">
      <data key="v_name">361346</data>
      <data key="v_label">361346</data>
    </node>
    <node id="n393">
      <data key="v_name">130670</data>
      <data key="v_label">130670</data>
    </node>
    <node id="n394">
      <data key="v_name">549376</data>
      <data key="v_label">549376</data>
    </node>
    <node id="n395">
      <data key="v_name">173322</data>
      <data key="v_label">173322</data>
    </node>
    <node id="n396">
      <data key="v_name">551068</data>
      <data key="v_label">551068</data>
    </node>
    <node id="n397">
      <data key="v_name">524834</data>
      <data key="v_label">524834</data>
    </node>
    <node id="n398">
      <data key="v_name">525589</data>
    </node>
    <node id="n399">
      <data key="v_name">558475</data>
      <data key="v_label">558475</data>
    </node>
    <node id="n400">
      <data key="v_name">436421</data>
      <data key="v_label">436421</data>
    </node>
    <node id="n401">
      <data key="v_name">420958</data>
      <data key="v_label">420958</data>
    </node>
    <node id="n402">
      <data key="v_name">213293</data>
      <data key="v_label">213293</data>
    </node>
    <node id="n403">
      <data key="v_name">154031</data>
      <data key="v_label">154031</data>
    </node>
    <node id="n404">
      <data key="v_name">531145</data>
      <data key="v_label">531145</data>
    </node>
    <node id="n405">
      <data key="v_name">218164</data>
      <data key="v_label">218164</data>
    </node>
    <node id="n406">
      <data key="v_name">486136</data>
      <data key="v_label">486136</data>
    </node>
    <node id="n407">
      <data key="v_name">541915</data>
      <data key="v_label">541915</data>
    </node>
    <node id="n408">
      <data key="v_name">556819</data>
      <data key="v_label">556819</data>
    </node>
    <node id="n409">
      <data key="v_name">147205</data>
      <data key="v_label">147205</data>
    </node>
    <node id="n410">
      <data key="v_name">554456</data>
      <data key="v_label">554456</data>
    </node>
    <node id="n411">
      <data key="v_name">433344</data>
      <data key="v_label">433344</data>
    </node>
    <node id="n412">
      <data key="v_name">541946</data>
      <data key="v_label">541946</data>
    </node>
    <node id="n413">
      <data key="v_name">495365</data>
      <data key="v_label">495365</data>
    </node>
    <node id="n414">
      <data key="v_name">531861</data>
    </node>
    <node id="n415">
      <data key="v_name">450982</data>
      <data key="v_label">450982</data>
    </node>
    <node id="n416">
      <data key="v_name">382346</data>
      <data key="v_label">382346</data>
    </node>
    <node id="n417">
      <data key="v_name">549589</data>
      <data key="v_label">549589</data>
    </node>
    <node id="n418">
      <data key="v_name">534213</data>
      <data key="v_label">534213</data>
    </node>
    <node id="n419">
      <data key="v_name">381704</data>
      <data key="v_label">381704</data>
    </node>
    <node id="n420">
      <data key="v_name">297760</data>
      <data key="v_label">297760</data>
    </node>
    <node id="n421">
      <data key="v_name">550779</data>
      <data key="v_label">550779</data>
    </node>
    <node id="n422">
      <data key="v_name">156218</data>
      <data key="v_label">156218</data>
    </node>
    <node id="n423">
      <data key="v_name">377652</data>
      <data key="v_label">377652</data>
    </node>
    <node id="n424">
      <data key="v_name">323733</data>
      <data key="v_label">323733</data>
    </node>
    <node id="n425">
      <data key="v_name">168988</data>
      <data key="v_label">168988</data>
    </node>
    <node id="n426">
      <data key="v_name">486576</data>
      <data key="v_label">486576</data>
    </node>
    <node id="n427">
      <data key="v_name">333515</data>
      <data key="v_label">333515</data>
    </node>
    <node id="n428">
      <data key="v_name">134851</data>
      <data key="v_label">134851</data>
    </node>
    <node id="n429">
      <data key="v_name">446347</data>
      <data key="v_label">446347</data>
    </node>
    <node id="n430">
      <data key="v_name">134853</data>
      <data key="v_label">134853</data>
    </node>
    <node id="n431">
      <data key="v_name">254233</data>
      <data key="v_label">254233</data>
    </node>
    <node id="n432">
      <data key="v_name">464231</data>
      <data key="v_label">464231</data>
    </node>
    <node id="n433">
      <data key="v_name">451554</data>
      <data key="v_label">451554</data>
    </node>
    <node id="n434">
      <data key="v_name">518455</data>
      <data key="v_label">518455</data>
    </node>
    <node id="n435">
      <data key="v_name">521666</data>
      <data key="v_label">521666</data>
    </node>
    <node id="n436">
      <data key="v_name">152126</data>
      <data key="v_label">152126</data>
    </node>
    <node id="n437">
      <data key="v_name">433032</data>
    </node>
    <node id="n438">
      <data key="v_name">521231</data>
      <data key="v_label">521231</data>
    </node>
    <node id="n439">
      <data key="v_name">221311</data>
      <data key="v_label">221311</data>
    </node>
    <node id="n440">
      <data key="v_name">485433</data>
      <data key="v_label">485433</data>
    </node>
    <node id="n441">
      <data key="v_name">515840</data>
      <data key="v_label">515840</data>
    </node>
    <node id="n442">
      <data key="v_name">410022</data>
      <data key="v_label">410022</data>
    </node>
    <node id="n443">
      <data key="v_name">517951</data>
      <data key="v_label">517951</data>
    </node>
    <node id="n444">
      <data key="v_name">142498</data>
      <data key="v_label">142498</data>
    </node>
    <node id="n445">
      <data key="v_name">561985</data>
      <data key="v_label">561985</data>
    </node>
    <node id="n446">
      <data key="v_name">359059</data>
      <data key="v_label">359059</data>
    </node>
    <node id="n447">
      <data key="v_name">441000</data>
      <data key="v_label">441000</data>
    </node>
    <node id="n448">
      <data key="v_name">551130</data>
      <data key="v_label">551130</data>
    </node>
    <node id="n449">
      <data key="v_name">275165</data>
      <data key="v_label">275165</data>
    </node>
    <node id="n450">
      <data key="v_name">384946</data>
      <data key="v_label">384946</data>
    </node>
    <node id="n451">
      <data key="v_name">509541</data>
      <data key="v_label">509541</data>
    </node>
    <node id="n452">
      <data key="v_name">547042</data>
      <data key="v_label">547042</data>
    </node>
    <node id="n453">
      <data key="v_name">331030</data>
      <data key="v_label">331030</data>
    </node>
    <node id="n454">
      <data key="v_name">401676</data>
      <data key="v_label">401676</data>
    </node>
    <node id="n455">
      <data key="v_name">452845</data>
      <data key="v_label">452845</data>
    </node>
    <node id="n456">
      <data key="v_name">561046</data>
      <data key="v_label">561046</data>
    </node>
    <node id="n457">
      <data key="v_name">337768</data>
      <data key="v_label">337768</data>
    </node>
    <node id="n458">
      <data key="v_name">460548</data>
      <data key="v_label">460548</data>
    </node>
    <node id="n459">
      <data key="v_name">198121</data>
      <data key="v_label">198121</data>
    </node>
    <node id="n460">
      <data key="v_name">424849</data>
      <data key="v_label">424849</data>
    </node>
    <node id="n461">
      <data key="v_name">539128</data>
      <data key="v_label">539128</data>
    </node>
    <node id="n462">
      <data key="v_name">539129</data>
      <data key="v_label">539129</data>
    </node>
    <node id="n463">
      <data key="v_name">533341</data>
      <data key="v_label">533341</data>
    </node>
    <node id="n464">
      <data key="v_name">268760</data>
      <data key="v_label">268760</data>
    </node>
    <node id="n465">
      <data key="v_name">380954</data>
      <data key="v_label">380954</data>
    </node>
    <node id="n466">
      <data key="v_name">445522</data>
      <data key="v_label">445522</data>
    </node>
    <node id="n467">
      <data key="v_name">273428</data>
      <data key="v_label">273428</data>
    </node>
    <node id="n468">
      <data key="v_name">545414</data>
      <data key="v_label">545414</data>
    </node>
    <node id="n469">
      <data key="v_name">487381</data>
      <data key="v_label">487381</data>
    </node>
    <node id="n470">
      <data key="v_name">521989</data>
      <data key="v_label">521989</data>
    </node>
    <node id="n471">
      <data key="v_name">560562</data>
      <data key="v_label">560562</data>
    </node>
    <node id="n472">
      <data key="v_name">516972</data>
      <data key="v_label">516972</data>
    </node>
    <node id="n473">
      <data key="v_name">564024</data>
      <data key="v_label">564024</data>
    </node>
    <node id="n474">
      <data key="v_name">564025</data>
      <data key="v_label">564025</data>
    </node>
    <node id="n475">
      <data key="v_name">382002</data>
      <data key="v_label">382002</data>
    </node>
    <node id="n476">
      <data key="v_name">556616</data>
      <data key="v_label">556616</data>
    </node>
    <node id="n477">
      <data key="v_name">550407</data>
    </node>
    <node id="n478">
      <data key="v_name">540130</data>
      <data key="v_label">540130</data>
    </node>
    <node id="n479">
      <data key="v_name">531613</data>
      <data key="v_label">531613</data>
    </node>
    <node id="n480">
      <data key="v_name">439280</data>
      <data key="v_label">439280</data>
    </node>
    <node id="n481">
      <data key="v_name">542088</data>
      <data key="v_label">542088</data>
    </node>
    <node id="n482">
      <data key="v_name">416761</data>
      <data key="v_label">416761</data>
    </node>
    <node id="n483">
      <data key="v_name">499748</data>
    </node>
    <node id="n484">
      <data key="v_name">464438</data>
      <data key="v_label">464438</data>
    </node>
    <node id="n485">
      <data key="v_name">342952</data>
      <data key="v_label">342952</data>
    </node>
    <node id="n486">
      <data key="v_name">235460</data>
      <data key="v_label">235460</data>
    </node>
    <node id="n487">
      <data key="v_name">517936</data>
      <data key="v_label">517936</data>
    </node>
    <node id="n488">
      <data key="v_name">549589</data>
    </node>
    <node id="n489">
      <data key="v_name">438138</data>
      <data key="v_label">438138</data>
    </node>
    <node id="n490">
      <data key="v_name">13545</data>
      <data key="v_label">13545</data>
    </node>
    <node id="n491">
      <data key="v_name">533936</data>
      <data key="v_label">533936</data>
    </node>
    <node id="n492">
      <data key="v_name">485574</data>
    </node>
    <node id="n493">
      <data key="v_name">558020</data>
      <data key="v_label">558020</data>
    </node>
    <node id="n494">
      <data key="v_name">550445</data>
      <data key="v_label">550445</data>
    </node>
    <node id="n495">
      <data key="v_name">228091</data>
      <data key="v_label">228091</data>
    </node>
    <node id="n496">
      <data key="v_name">523294</data>
      <data key="v_label">523294</data>
    </node>
    <node id="n497">
      <data key="v_name">435190</data>
      <data key="v_label">435190</data>
    </node>
    <node id="n498">
      <data key="v_name">549488</data>
      <data key="v_label">549488</data>
    </node>
    <node id="n499">
      <data key="v_name">209728</data>
      <data key="v_label">209728</data>
    </node>
    <node id="n500">
      <data key="v_name">351676</data>
      <data key="v_label">351676</data>
    </node>
    <node id="n501">
      <data key="v_name">370683</data>
      <data key="v_label">370683</data>
    </node>
    <node id="n502">
      <data key="v_name">540130</data>
    </node>
    <node id="n503">
      <data key="v_name">377870</data>
      <data key="v_label">377870</data>
    </node>
    <node id="n504">
      <data key="v_name">182786</data>
      <data key="v_label">182786</data>
    </node>
    <node id="n505">
      <data key="v_name">531055</data>
      <data key="v_label">531055</data>
    </node>
    <node id="n506">
      <data key="v_name">553223</data>
      <data key="v_label">553223</data>
    </node>
    <node id="n507">
      <data key="v_name">520895</data>
      <data key="v_label">520895</data>
    </node>
    <node id="n508">
      <data key="v_name">439956</data>
      <data key="v_label">439956</data>
    </node>
    <node id="n509">
      <data key="v_name">156315</data>
      <data key="v_label">156315</data>
    </node>
    <node id="n510">
      <data key="v_name">462606</data>
      <data key="v_label">462606</data>
    </node>
    <node id="n511">
      <data key="v_name">369802</data>
      <data key="v_label">369802</data>
    </node>
    <node id="n512">
      <data key="v_name">391314</data>
      <data key="v_label">391314</data>
    </node>
    <node id="n513">
      <data key="v_name">162520</data>
      <data key="v_label">162520</data>
    </node>
    <node id="n514">
      <data key="v_name">446323</data>
      <data key="v_label">446323</data>
    </node>
    <node id="n515">
      <data key="v_name">529311</data>
      <data key="v_label">529311</data>
    </node>
    <node id="n516">
      <data key="v_name">156122</data>
      <data key="v_label">156122</data>
    </node>
    <node id="n517">
      <data key="v_name">494196</data>
      <data key="v_label">494196</data>
    </node>
    <node id="n518">
      <data key="v_name">359059</data>
    </node>
    <node id="n519">
      <data key="v_name">152226</data>
      <data key="v_label">152226</data>
    </node>
    <node id="n520">
      <data key="v_name">484710</data>
      <data key="v_label">484710</data>
    </node>
    <node id="n521">
      <data key="v_name">379286</data>
      <data key="v_label">379286</data>
    </node>
    <node id="n522">
      <data key="v_name">547420</data>
      <data key="v_label">547420</data>
    </node>
    <node id="n523">
      <data key="v_name">484709</data>
      <data key="v_label">484709</data>
    </node>
    <node id="n524">
      <data key="v_name">389021</data>
      <data key="v_label">389021</data>
    </node>
    <node id="n525">
      <data key="v_name">165339</data>
      <data key="v_label">165339</data>
    </node>
    <node id="n526">
      <data key="v_name">259346</data>
      <data key="v_label">259346</data>
    </node>
    <node id="n527">
      <data key="v_name">517717</data>
      <data key="v_label">517717</data>
    </node>
    <node id="n528">
      <data key="v_name">251275</data>
      <data key="v_label">251275</data>
    </node>
    <node id="n529">
      <data key="v_name">510840</data>
      <data key="v_label">510840</data>
    </node>
    <node id="n530">
      <data key="v_name">527631</data>
      <data key="v_label">527631</data>
    </node>
    <node id="n531">
      <data key="v_name">542016</data>
      <data key="v_label">542016</data>
    </node>
    <node id="n532">
      <data key="v_name">408968</data>
      <data key="v_label">408968</data>
    </node>
    <node id="n533">
      <data key="v_name">451312</data>
      <data key="v_label">451312</data>
    </node>
    <node id="n534">
      <data key="v_name">491467</data>
      <data key="v_label">491467</data>
    </node>
    <node id="n535">
      <data key="v_name">147262</data>
      <data key="v_label">147262</data>
    </node>
    <node id="n536">
      <data key="v_name">486378</data>
      <data key="v_label">486378</data>
    </node>
    <node id="n537">
      <data key="v_name">194011</data>
      <data key="v_label">194011</data>
    </node>
    <node id="n538">
      <data key="v_name">157608</data>
      <data key="v_label">157608</data>
    </node>
    <node id="n539">
      <data key="v_name">383899</data>
      <data key="v_label">383899</data>
    </node>
    <node id="n540">
      <data key="v_name">549542</data>
      <data key="v_label">549542</data>
    </node>
    <node id="n541">
      <data key="v_name">550373</data>
    </node>
    <node id="n542">
      <data key="v_name">508352</data>
      <data key="v_label">508352</data>
    </node>
    <node id="n543">
      <data key="v_name">461728</data>
      <data key="v_label">461728</data>
    </node>
    <node id="n544">
      <data key="v_name">202714</data>
      <data key="v_label">202714</data>
    </node>
    <node id="n545">
      <data key="v_name">166424</data>
      <data key="v_label">166424</data>
    </node>
    <node id="n546">
      <data key="v_name">225743</data>
      <data key="v_label">225743</data>
    </node>
    <node id="n547">
      <data key="v_name">152236</data>
      <data key="v_label">152236</data>
    </node>
    <node id="n548">
      <data key="v_name">152237</data>
      <data key="v_label">152237</data>
    </node>
    <node id="n549">
      <data key="v_name">277844</data>
      <data key="v_label">277844</data>
    </node>
    <node id="n550">
      <data key="v_name">539039</data>
      <data key="v_label">539039</data>
    </node>
    <node id="n551">
      <data key="v_name">394642</data>
      <data key="v_label">394642</data>
    </node>
    <node id="n552">
      <data key="v_name">430298</data>
      <data key="v_label">430298</data>
    </node>
    <node id="n553">
      <data key="v_name">169453</data>
      <data key="v_label">169453</data>
    </node>
    <node id="n554">
      <data key="v_name">517581</data>
      <data key="v_label">517581</data>
    </node>
    <node id="n555">
      <data key="v_name">334660</data>
      <data key="v_label">334660</data>
    </node>
    <node id="n556">
      <data key="v_name">379546</data>
      <data key="v_label">379546</data>
    </node>
    <node id="n557">
      <data key="v_name">148166</data>
      <data key="v_label">148166</data>
    </node>
    <node id="n558">
      <data key="v_name">161277</data>
      <data key="v_label">161277</data>
    </node>
    <node id="n559">
      <data key="v_name">319269</data>
      <data key="v_label">319269</data>
    </node>
    <node id="n560">
      <data key="v_name">550859</data>
      <data key="v_label">550859</data>
    </node>
    <node id="n561">
      <data key="v_name">164669</data>
      <data key="v_label">164669</data>
    </node>
    <node id="n562">
      <data key="v_name">483530</data>
      <data key="v_label">483530</data>
    </node>
    <node id="n563">
      <data key="v_name">349921</data>
      <data key="v_label">349921</data>
    </node>
    <node id="n564">
      <data key="v_name">264120</data>
      <data key="v_label">264120</data>
    </node>
    <node id="n565">
      <data key="v_name">157594</data>
      <data key="v_label">157594</data>
    </node>
    <node id="n566">
      <data key="v_name">519404</data>
      <data key="v_label">519404</data>
    </node>
    <node id="n567">
      <data key="v_name">455118</data>
      <data key="v_label">455118</data>
    </node>
    <node id="n568">
      <data key="v_name">486140</data>
      <data key="v_label">486140</data>
    </node>
    <node id="n569">
      <data key="v_name">485983</data>
      <data key="v_label">485983</data>
    </node>
    <node id="n570">
      <data key="v_name">562714</data>
      <data key="v_label">562714</data>
    </node>
    <node id="n571">
      <data key="v_name">173930</data>
    </node>
    <node id="n572">
      <data key="v_name">347935</data>
    </node>
    <node id="n573">
      <data key="v_name">147875</data>
      <data key="v_label">147875</data>
    </node>
    <node id="n574">
      <data key="v_name">494757</data>
    </node>
    <node id="n575">
      <data key="v_name">540130</data>
    </node>
    <node id="n576">
      <data key="v_name">516895</data>
      <data key="v_label">516895</data>
    </node>
    <node id="n577">
      <data key="v_name">213293</data>
    </node>
    <node id="n578">
      <data key="v_name">355022</data>
      <data key="v_label">355022</data>
    </node>
    <node id="n579">
      <data key="v_name">461693</data>
      <data key="v_label">461693</data>
    </node>
    <node id="n580">
      <data key="v_name">549429</data>
      <data key="v_label">549429</data>
    </node>
    <node id="n581">
      <data key="v_name">521544</data>
    </node>
    <node id="n582">
      <data key="v_name">508269</data>
      <data key="v_label">508269</data>
    </node>
    <node id="n583">
      <data key="v_name">530445</data>
      <data key="v_label">530445</data>
    </node>
    <node id="n584">
      <data key="v_name">229259</data>
      <data key="v_label">229259</data>
    </node>
    <node id="n585">
      <data key="v_name">152485</data>
      <data key="v_label">152485</data>
    </node>
    <node id="n586">
      <data key="v_name">152486</data>
      <data key="v_label">152486</data>
    </node>
    <node id="n587">
      <data key="v_name">152487</data>
      <data key="v_label">152487</data>
    </node>
    <node id="n588">
      <data key="v_name">518072</data>
      <data key="v_label">518072</data>
    </node>
    <node id="n589">
      <data key="v_name">180294</data>
      <data key="v_label">180294</data>
    </node>
    <node id="n590">
      <data key="v_name">264120</data>
    </node>
    <node id="n591">
      <data key="v_name">392589</data>
      <data key="v_label">392589</data>
    </node>
    <node id="n592">
      <data key="v_name">180282</data>
      <data key="v_label">180282</data>
    </node>
    <node id="n593">
      <data key="v_name">539039</data>
    </node>
    <node id="n594">
      <data key="v_name">394642</data>
    </node>
    <node id="n595">
      <data key="v_name">180288</data>
      <data key="v_label">180288</data>
    </node>
    <node id="n596">
      <data key="v_name">130705</data>
      <data key="v_label">130705</data>
    </node>
    <node id="n597">
      <data key="v_name">180277</data>
      <data key="v_label">180277</data>
    </node>
    <node id="n598">
      <data key="v_name">565135</data>
      <data key="v_label">565135</data>
    </node>
    <node id="n599">
      <data key="v_name">497441</data>
      <data key="v_label">497441</data>
    </node>
    <node id="n600">
      <data key="v_name">518179</data>
      <data key="v_label">518179</data>
    </node>
    <node id="n601">
      <data key="v_name">548310</data>
      <data key="v_label">548310</data>
    </node>
    <node id="n602">
      <data key="v_name">233003</data>
      <data key="v_label">233003</data>
    </node>
    <node id="n603">
      <data key="v_name">166110</data>
      <data key="v_label">166110</data>
    </node>
    <node id="n604">
      <data key="v_name">517079</data>
      <data key="v_label">517079</data>
    </node>
    <node id="n605">
      <data key="v_name">538796</data>
    </node>
    <node id="n606">
      <data key="v_name">410098</data>
    </node>
    <node id="n607">
      <data key="v_name">402395</data>
      <data key="v_label">402395</data>
    </node>
    <node id="n608">
      <data key="v_name">438784</data>
      <data key="v_label">438784</data>
    </node>
    <node id="n609">
      <data key="v_name">544688</data>
      <data key="v_label">544688</data>
    </node>
    <node id="n610">
      <data key="v_name">550599</data>
      <data key="v_label">550599</data>
    </node>
    <node id="n611">
      <data key="v_name">528421</data>
      <data key="v_label">528421</data>
    </node>
    <node id="n612">
      <data key="v_name">409637</data>
      <data key="v_label">409637</data>
    </node>
    <node id="n613">
      <data key="v_name">451563</data>
    </node>
    <node id="n614">
      <data key="v_name">69811</data>
      <data key="v_label">69811</data>
    </node>
    <node id="n615">
      <data key="v_name">531861</data>
    </node>
    <node id="n616">
      <data key="v_name">554823</data>
      <data key="v_label">554823</data>
    </node>
    <node id="n617">
      <data key="v_name">155980</data>
      <data key="v_label">155980</data>
    </node>
    <node id="n618">
      <data key="v_name">554822</data>
      <data key="v_label">554822</data>
    </node>
    <node id="n619">
      <data key="v_name">296135</data>
      <data key="v_label">296135</data>
    </node>
    <node id="n620">
      <data key="v_name">558957</data>
    </node>
    <node id="n621">
      <data key="v_name">172721</data>
      <data key="v_label">172721</data>
    </node>
    <node id="n622">
      <data key="v_name">402357</data>
      <data key="v_label">402357</data>
    </node>
    <node id="n623">
      <data key="v_name">490673</data>
      <data key="v_label">490673</data>
    </node>
    <node id="n624">
      <data key="v_name">343491</data>
      <data key="v_label">343491</data>
    </node>
    <node id="n625">
      <data key="v_name">562301</data>
      <data key="v_label">562301</data>
    </node>
    <node id="n626">
      <data key="v_name">483625</data>
      <data key="v_label">483625</data>
    </node>
    <node id="n627">
      <data key="v_name">320931</data>
      <data key="v_label">320931</data>
    </node>
    <node id="n628">
      <data key="v_name">401071</data>
      <data key="v_label">401071</data>
    </node>
    <node id="n629">
      <data key="v_name">513776</data>
      <data key="v_label">513776</data>
    </node>
    <node id="n630">
      <data key="v_name">531740</data>
      <data key="v_label">531740</data>
    </node>
    <node id="n631">
      <data key="v_name">347935</data>
    </node>
    <node id="n632">
      <data key="v_name">518535</data>
      <data key="v_label">518535</data>
    </node>
    <node id="n633">
      <data key="v_name">496669</data>
      <data key="v_label">496669</data>
    </node>
    <node id="n634">
      <data key="v_name">382994</data>
      <data key="v_label">382994</data>
    </node>
    <node id="n635">
      <data key="v_name">156002</data>
      <data key="v_label">156002</data>
    </node>
    <node id="n636">
      <data key="v_name">360639</data>
      <data key="v_label">360639</data>
    </node>
    <node id="n637">
      <data key="v_name">360640</data>
      <data key="v_label">360640</data>
    </node>
    <node id="n638">
      <data key="v_name">161478</data>
      <data key="v_label">161478</data>
    </node>
    <node id="n639">
      <data key="v_name">402469</data>
      <data key="v_label">402469</data>
    </node>
    <node id="n640">
      <data key="v_name">290022</data>
    </node>
    <node id="n641">
      <data key="v_name">438098</data>
    </node>
    <node id="n642">
      <data key="v_name">551341</data>
      <data key="v_label">551341</data>
    </node>
    <node id="n643">
      <data key="v_name">553537</data>
    </node>
    <node id="n644">
      <data key="v_name">485126</data>
      <data key="v_label">485126</data>
    </node>
    <node id="n645">
      <data key="v_name">553537</data>
    </node>
    <node id="n646">
      <data key="v_name">516897</data>
      <data key="v_label">516897</data>
    </node>
    <node id="n647">
      <data key="v_name">516898</data>
      <data key="v_label">516898</data>
    </node>
    <node id="n648">
      <data key="v_name">283481</data>
      <data key="v_label">283481</data>
    </node>
    <node id="n649">
      <data key="v_name">540775</data>
      <data key="v_label">540775</data>
    </node>
    <node id="n650">
      <data key="v_name">332138</data>
      <data key="v_label">332138</data>
    </node>
    <node id="n651">
      <data key="v_name">518384</data>
    </node>
    <node id="n652">
      <data key="v_name">540130</data>
    </node>
    <node id="n653">
      <data key="v_name">241298</data>
      <data key="v_label">241298</data>
    </node>
    <node id="n654">
      <data key="v_name">550173</data>
      <data key="v_label">550173</data>
    </node>
    <node id="n655">
      <data key="v_name">550785</data>
      <data key="v_label">550785</data>
    </node>
    <node id="n656">
      <data key="v_name">166259</data>
      <data key="v_label">166259</data>
    </node>
    <node id="n657">
      <data key="v_name">554456</data>
    </node>
    <node id="n658">
      <data key="v_name">557296</data>
      <data key="v_label">557296</data>
    </node>
    <node id="n659">
      <data key="v_name">384032</data>
      <data key="v_label">384032</data>
    </node>
    <node id="n660">
      <data key="v_name">528361</data>
      <data key="v_label">528361</data>
    </node>
    <node id="n661">
      <data key="v_name">539892</data>
    </node>
    <node id="n662">
      <data key="v_name">239244</data>
      <data key="v_label">239244</data>
    </node>
    <node id="n663">
      <data key="v_name">239245</data>
      <data key="v_label">239245</data>
    </node>
    <node id="n664">
      <data key="v_name">542015</data>
      <data key="v_label">542015</data>
    </node>
    <node id="n665">
      <data key="v_name">542016</data>
    </node>
    <node id="n666">
      <data key="v_name">550491</data>
      <data key="v_label">550491</data>
    </node>
    <node id="n667">
      <data key="v_name">527853</data>
      <data key="v_label">527853</data>
    </node>
    <node id="n668">
      <data key="v_name">485947</data>
      <data key="v_label">485947</data>
    </node>
    <node id="n669">
      <data key="v_name">364501</data>
      <data key="v_label">364501</data>
    </node>
    <node id="n670">
      <data key="v_name">543580</data>
      <data key="v_label">543580</data>
    </node>
    <node id="n671">
      <data key="v_name">334102</data>
      <data key="v_label">334102</data>
    </node>
    <node id="n672">
      <data key="v_name">564282</data>
      <data key="v_label">564282</data>
    </node>
    <node id="n673">
      <data key="v_name">543582</data>
      <data key="v_label">543582</data>
    </node>
    <node id="n674">
      <data key="v_name">562965</data>
      <data key="v_label">562965</data>
    </node>
    <node id="n675">
      <data key="v_name">550775</data>
      <data key="v_label">550775</data>
    </node>
    <node id="n676">
      <data key="v_name">438686</data>
      <data key="v_label">438686</data>
    </node>
    <node id="n677">
      <data key="v_name">554699</data>
      <data key="v_label">554699</data>
    </node>
    <node id="n678">
      <data key="v_name">387675</data>
      <data key="v_label">387675</data>
    </node>
    <node id="n679">
      <data key="v_name">553596</data>
    </node>
    <node id="n680">
      <data key="v_name">547675</data>
    </node>
    <node id="n681">
      <data key="v_name">550149</data>
    </node>
    <node id="n682">
      <data key="v_name">155175</data>
      <data key="v_label">155175</data>
    </node>
    <node id="n683">
      <data key="v_name">233332</data>
      <data key="v_label">233332</data>
    </node>
    <node id="n684">
      <data key="v_name">334091</data>
      <data key="v_label">334091</data>
    </node>
    <node id="n685">
      <data key="v_name">527253</data>
      <data key="v_label">527253</data>
    </node>
    <node id="n686">
      <data key="v_name">74434</data>
      <data key="v_label">74434</data>
    </node>
    <node id="n687">
      <data key="v_name">409472</data>
      <data key="v_label">409472</data>
    </node>
    <node id="n688">
      <data key="v_name">486616</data>
      <data key="v_label">486616</data>
    </node>
    <node id="n689">
      <data key="v_name">173901</data>
      <data key="v_label">173901</data>
    </node>
    <node id="n690">
      <data key="v_name">173902</data>
      <data key="v_label">173902</data>
    </node>
    <node id="n691">
      <data key="v_name">164944</data>
      <data key="v_label">164944</data>
    </node>
    <node id="n692">
      <data key="v_name">240400</data>
      <data key="v_label">240400</data>
    </node>
    <node id="n693">
      <data key="v_name">565242</data>
      <data key="v_label">565242</data>
    </node>
    <node id="n694">
      <data key="v_name">474337</data>
      <data key="v_label">474337</data>
    </node>
    <node id="n695">
      <data key="v_name">342042</data>
      <data key="v_label">342042</data>
    </node>
    <node id="n696">
      <data key="v_name">549998</data>
      <data key="v_label">549998</data>
    </node>
    <node id="n697">
      <data key="v_name">564588</data>
      <data key="v_label">564588</data>
    </node>
    <node id="n698">
      <data key="v_name">486281</data>
      <data key="v_label">486281</data>
    </node>
    <node id="n699">
      <data key="v_name">486458</data>
      <data key="v_label">486458</data>
    </node>
    <node id="n700">
      <data key="v_name">530999</data>
      <data key="v_label">530999</data>
    </node>
    <node id="n701">
      <data key="v_name">483390</data>
      <data key="v_label">483390</data>
    </node>
    <node id="n702">
      <data key="v_name">494196</data>
    </node>
    <node id="n703">
      <data key="v_name">154569</data>
      <data key="v_label">154569</data>
    </node>
    <node id="n704">
      <data key="v_name">359059</data>
    </node>
    <node id="n705">
      <data key="v_name">442408</data>
      <data key="v_label">442408</data>
    </node>
    <node id="n706">
      <data key="v_name">528208</data>
      <data key="v_label">528208</data>
    </node>
    <node id="n707">
      <data key="v_name">542246</data>
      <data key="v_label">542246</data>
    </node>
    <node id="n708">
      <data key="v_name">518277</data>
    </node>
    <node id="n709">
      <data key="v_name">154576</data>
      <data key="v_label">154576</data>
    </node>
    <node id="n710">
      <data key="v_name">52508</data>
      <data key="v_label">52508</data>
    </node>
    <node id="n711">
      <data key="v_name">560046</data>
      <data key="v_label">560046</data>
    </node>
    <node id="n712">
      <data key="v_name">213809</data>
      <data key="v_label">213809</data>
    </node>
    <node id="n713">
      <data key="v_name">467986</data>
      <data key="v_label">467986</data>
    </node>
    <node id="n714">
      <data key="v_name">495323</data>
      <data key="v_label">495323</data>
    </node>
    <node id="n715">
      <data key="v_name">264120</data>
    </node>
    <node id="n716">
      <data key="v_name">166463</data>
      <data key="v_label">166463</data>
    </node>
    <node id="n717">
      <data key="v_name">323356</data>
      <data key="v_label">323356</data>
    </node>
    <node id="n718">
      <data key="v_name">485670</data>
      <data key="v_label">485670</data>
    </node>
    <node id="n719">
      <data key="v_name">431529</data>
      <data key="v_label">431529</data>
    </node>
    <node id="n720">
      <data key="v_name">485671</data>
      <data key="v_label">485671</data>
    </node>
    <node id="n721">
      <data key="v_name">553248</data>
      <data key="v_label">553248</data>
    </node>
    <node id="n722">
      <data key="v_name">495278</data>
      <data key="v_label">495278</data>
    </node>
    <node id="n723">
      <data key="v_name">166921</data>
      <data key="v_label">166921</data>
    </node>
    <node id="n724">
      <data key="v_name">489873</data>
      <data key="v_label">489873</data>
    </node>
    <node id="n725">
      <data key="v_name">544518</data>
      <data key="v_label">544518</data>
    </node>
    <node id="n726">
      <data key="v_name">558957</data>
    </node>
    <node id="n727">
      <data key="v_name">434205</data>
      <data key="v_label">434205</data>
    </node>
    <node id="n728">
      <data key="v_name">385370</data>
      <data key="v_label">385370</data>
    </node>
    <node id="n729">
      <data key="v_name">548182</data>
      <data key="v_label">548182</data>
    </node>
    <node id="n730">
      <data key="v_name">286927</data>
      <data key="v_label">286927</data>
    </node>
    <node id="n731">
      <data key="v_name">541893</data>
      <data key="v_label">541893</data>
    </node>
    <node id="n732">
      <data key="v_name">434205</data>
    </node>
    <node id="n733">
      <data key="v_name">523728</data>
      <data key="v_label">523728</data>
    </node>
    <node id="n734">
      <data key="v_name">165715</data>
      <data key="v_label">165715</data>
    </node>
    <node id="n735">
      <data key="v_name">541869</data>
    </node>
    <node id="n736">
      <data key="v_name">165717</data>
      <data key="v_label">165717</data>
    </node>
    <node id="n737">
      <data key="v_name">550832</data>
      <data key="v_label">550832</data>
    </node>
    <node id="n738">
      <data key="v_name">409429</data>
      <data key="v_label">409429</data>
    </node>
    <node id="n739">
      <data key="v_name">401344</data>
      <data key="v_label">401344</data>
    </node>
    <node id="n740">
      <data key="v_name">560997</data>
      <data key="v_label">560997</data>
    </node>
    <node id="n741">
      <data key="v_name">513354</data>
      <data key="v_label">513354</data>
    </node>
    <node id="n742">
      <data key="v_name">292838</data>
      <data key="v_label">292838</data>
    </node>
    <node id="n743">
      <data key="v_name">553308</data>
      <data key="v_label">553308</data>
    </node>
    <node id="n744">
      <data key="v_name">147932</data>
      <data key="v_label">147932</data>
    </node>
    <node id="n745">
      <data key="v_name">438603</data>
      <data key="v_label">438603</data>
    </node>
    <node id="n746">
      <data key="v_name">450771</data>
      <data key="v_label">450771</data>
    </node>
    <node id="n747">
      <data key="v_name">440155</data>
      <data key="v_label">440155</data>
    </node>
    <node id="n748">
      <data key="v_name">471123</data>
      <data key="v_label">471123</data>
    </node>
    <node id="n749">
      <data key="v_name">369365</data>
      <data key="v_label">369365</data>
    </node>
    <node id="n750">
      <data key="v_name">183023</data>
      <data key="v_label">183023</data>
    </node>
    <node id="n751">
      <data key="v_name">183024</data>
      <data key="v_label">183024</data>
    </node>
    <node id="n752">
      <data key="v_name">401872</data>
      <data key="v_label">401872</data>
    </node>
    <node id="n753">
      <data key="v_name">410581</data>
      <data key="v_label">410581</data>
    </node>
    <node id="n754">
      <data key="v_name">559061</data>
      <data key="v_label">559061</data>
    </node>
    <node id="n755">
      <data key="v_name">551355</data>
      <data key="v_label">551355</data>
    </node>
    <node id="n756">
      <data key="v_name">560510</data>
      <data key="v_label">560510</data>
    </node>
    <node id="n757">
      <data key="v_name">553614</data>
      <data key="v_label">553614</data>
    </node>
    <node id="n758">
      <data key="v_name">553615</data>
      <data key="v_label">553615</data>
    </node>
    <node id="n759">
      <data key="v_name">280792</data>
      <data key="v_label">280792</data>
    </node>
    <node id="n760">
      <data key="v_name">151306</data>
      <data key="v_label">151306</data>
    </node>
    <node id="n761">
      <data key="v_name">151307</data>
      <data key="v_label">151307</data>
    </node>
    <node id="n762">
      <data key="v_name">424178</data>
      <data key="v_label">424178</data>
    </node>
    <node id="n763">
      <data key="v_name">311190</data>
      <data key="v_label">311190</data>
    </node>
    <node id="n764">
      <data key="v_name">491070</data>
      <data key="v_label">491070</data>
    </node>
    <node id="n765">
      <data key="v_name">226193</data>
      <data key="v_label">226193</data>
    </node>
    <node id="n766">
      <data key="v_name">162110</data>
      <data key="v_label">162110</data>
    </node>
    <node id="n767">
      <data key="v_name">400310</data>
      <data key="v_label">400310</data>
    </node>
    <node id="n768">
      <data key="v_name">508256</data>
      <data key="v_label">508256</data>
    </node>
    <node id="n769">
      <data key="v_name">349186</data>
      <data key="v_label">349186</data>
    </node>
    <node id="n770">
      <data key="v_name">550026</data>
    </node>
    <node id="n771">
      <data key="v_name">472308</data>
      <data key="v_label">472308</data>
    </node>
    <node id="n772">
      <data key="v_name">557607</data>
      <data key="v_label">557607</data>
    </node>
    <node id="n773">
      <data key="v_name">558517</data>
      <data key="v_label">558517</data>
    </node>
    <node id="n774">
      <data key="v_name">163496</data>
      <data key="v_label">163496</data>
    </node>
    <node id="n775">
      <data key="v_name">558518</data>
      <data key="v_label">558518</data>
    </node>
    <node id="n776">
      <data key="v_name">544556</data>
      <data key="v_label">544556</data>
    </node>
    <node id="n777">
      <data key="v_name">162978</data>
      <data key="v_label">162978</data>
    </node>
    <node id="n778">
      <data key="v_name">243624</data>
      <data key="v_label">243624</data>
    </node>
    <node id="n779">
      <data key="v_name">366168</data>
      <data key="v_label">366168</data>
    </node>
    <node id="n780">
      <data key="v_name">535303</data>
      <data key="v_label">535303</data>
    </node>
    <node id="n781">
      <data key="v_name">438661</data>
      <data key="v_label">438661</data>
    </node>
    <node id="n782">
      <data key="v_name">529481</data>
      <data key="v_label">529481</data>
    </node>
    <node id="n783">
      <data key="v_name">454014</data>
      <data key="v_label">454014</data>
    </node>
    <node id="n784">
      <data key="v_name">387218</data>
      <data key="v_label">387218</data>
    </node>
    <node id="n785">
      <data key="v_name">290509</data>
      <data key="v_label">290509</data>
    </node>
    <node id="n786">
      <data key="v_name">518643</data>
      <data key="v_label">518643</data>
    </node>
    <node id="n787">
      <data key="v_name">542069</data>
      <data key="v_label">542069</data>
    </node>
    <node id="n788">
      <data key="v_name">293079</data>
      <data key="v_label">293079</data>
    </node>
    <node id="n789">
      <data key="v_name">209826</data>
      <data key="v_label">209826</data>
    </node>
    <node id="n790">
      <data key="v_name">425870</data>
      <data key="v_label">425870</data>
    </node>
    <node id="n791">
      <data key="v_name">353752</data>
      <data key="v_label">353752</data>
    </node>
    <node id="n792">
      <data key="v_name">541641</data>
      <data key="v_label">541641</data>
    </node>
    <node id="n793">
      <data key="v_name">542299</data>
      <data key="v_label">542299</data>
    </node>
    <node id="n794">
      <data key="v_name">518026</data>
      <data key="v_label">518026</data>
    </node>
    <node id="n795">
      <data key="v_name">530199</data>
      <data key="v_label">530199</data>
    </node>
    <node id="n796">
      <data key="v_name">518545</data>
      <data key="v_label">518545</data>
    </node>
    <node id="n797">
      <data key="v_name">550604</data>
      <data key="v_label">550604</data>
    </node>
    <node id="n798">
      <data key="v_name">550604</data>
    </node>
    <node id="n799">
      <data key="v_name">564799</data>
      <data key="v_label">564799</data>
    </node>
    <node id="n800">
      <data key="v_name">564799</data>
    </node>
    <node id="n801">
      <data key="v_name">521384</data>
      <data key="v_label">521384</data>
    </node>
    <node id="n802">
      <data key="v_name">562295</data>
      <data key="v_label">562295</data>
    </node>
    <node id="n803">
      <data key="v_name">157507</data>
      <data key="v_label">157507</data>
    </node>
    <node id="n804">
      <data key="v_name">438665</data>
      <data key="v_label">438665</data>
    </node>
    <node id="n805">
      <data key="v_name">361939</data>
      <data key="v_label">361939</data>
    </node>
    <node id="n806">
      <data key="v_name">508165</data>
      <data key="v_label">508165</data>
    </node>
    <node id="n807">
      <data key="v_name">378572</data>
      <data key="v_label">378572</data>
    </node>
    <node id="n808">
      <data key="v_name">167434</data>
      <data key="v_label">167434</data>
    </node>
    <node id="n809">
      <data key="v_name">244065</data>
      <data key="v_label">244065</data>
    </node>
    <node id="n810">
      <data key="v_name">474073</data>
      <data key="v_label">474073</data>
    </node>
    <node id="n811">
      <data key="v_name">403560</data>
      <data key="v_label">403560</data>
    </node>
    <node id="n812">
      <data key="v_name">515533</data>
      <data key="v_label">515533</data>
    </node>
    <node id="n813">
      <data key="v_name">148204</data>
      <data key="v_label">148204</data>
    </node>
    <node id="n814">
      <data key="v_name">527566</data>
      <data key="v_label">527566</data>
    </node>
    <node id="n815">
      <data key="v_name">361939</data>
    </node>
    <node id="n816">
      <data key="v_name">563733</data>
      <data key="v_label">563733</data>
    </node>
    <node id="n817">
      <data key="v_name">333704</data>
    </node>
    <node id="n818">
      <data key="v_name">466602</data>
    </node>
    <node id="n819">
      <data key="v_name">472085</data>
      <data key="v_label">472085</data>
    </node>
    <node id="n820">
      <data key="v_name">557461</data>
      <data key="v_label">557461</data>
    </node>
    <node id="n821">
      <data key="v_name">402054</data>
      <data key="v_label">402054</data>
    </node>
    <node id="n822">
      <data key="v_name">553596</data>
    </node>
    <node id="n823">
      <data key="v_name">409925</data>
      <data key="v_label">409925</data>
    </node>
    <node id="n824">
      <data key="v_name">562061</data>
      <data key="v_label">562061</data>
    </node>
    <node id="n825">
      <data key="v_name">486062</data>
      <data key="v_label">486062</data>
    </node>
    <node id="n826">
      <data key="v_name">438360</data>
      <data key="v_label">438360</data>
    </node>
    <node id="n827">
      <data key="v_name">485498</data>
      <data key="v_label">485498</data>
    </node>
    <node id="n828">
      <data key="v_name">349823</data>
      <data key="v_label">349823</data>
    </node>
    <node id="n829">
      <data key="v_name">451118</data>
      <data key="v_label">451118</data>
    </node>
    <node id="n830">
      <data key="v_name">540138</data>
      <data key="v_label">540138</data>
    </node>
    <node id="n831">
      <data key="v_name">289631</data>
      <data key="v_label">289631</data>
    </node>
    <node id="n832">
      <data key="v_name">167070</data>
      <data key="v_label">167070</data>
    </node>
    <node id="n833">
      <data key="v_name">399151</data>
      <data key="v_label">399151</data>
    </node>
    <node id="n834">
      <data key="v_name">235820</data>
      <data key="v_label">235820</data>
    </node>
    <node id="n835">
      <data key="v_name">499664</data>
      <data key="v_label">499664</data>
    </node>
    <node id="n836">
      <data key="v_name">518026</data>
    </node>
    <node id="n837">
      <data key="v_name">531167</data>
      <data key="v_label">531167</data>
    </node>
    <node id="n838">
      <data key="v_name">530199</data>
    </node>
    <node id="n839">
      <data key="v_name">167086</data>
      <data key="v_label">167086</data>
    </node>
    <node id="n840">
      <data key="v_name">167087</data>
      <data key="v_label">167087</data>
    </node>
    <node id="n841">
      <data key="v_name">167095</data>
      <data key="v_label">167095</data>
    </node>
    <node id="n842">
      <data key="v_name">493332</data>
      <data key="v_label">493332</data>
    </node>
    <node id="n843">
      <data key="v_name">317581</data>
      <data key="v_label">317581</data>
    </node>
    <node id="n844">
      <data key="v_name">167098</data>
      <data key="v_label">167098</data>
    </node>
    <node id="n845">
      <data key="v_name">528796</data>
      <data key="v_label">528796</data>
    </node>
    <node id="n846">
      <data key="v_name">542246</data>
    </node>
    <node id="n847">
      <data key="v_name">521163</data>
      <data key="v_label">521163</data>
    </node>
    <node id="n848">
      <data key="v_name">508973</data>
      <data key="v_label">508973</data>
    </node>
    <node id="n849">
      <data key="v_name">361346</data>
    </node>
    <node id="n850">
      <data key="v_name">457136</data>
      <data key="v_label">457136</data>
    </node>
    <node id="n851">
      <data key="v_name">559496</data>
      <data key="v_label">559496</data>
    </node>
    <node id="n852">
      <data key="v_name">391834</data>
      <data key="v_label">391834</data>
    </node>
    <node id="n853">
      <data key="v_name">376530</data>
      <data key="v_label">376530</data>
    </node>
    <node id="n854">
      <data key="v_name">258283</data>
      <data key="v_label">258283</data>
    </node>
    <node id="n855">
      <data key="v_name">460466</data>
      <data key="v_label">460466</data>
    </node>
    <node id="n856">
      <data key="v_name">553686</data>
      <data key="v_label">553686</data>
    </node>
    <node id="n857">
      <data key="v_name">356146</data>
      <data key="v_label">356146</data>
    </node>
    <node id="n858">
      <data key="v_name">404642</data>
      <data key="v_label">404642</data>
    </node>
    <node id="n859">
      <data key="v_name">196484</data>
      <data key="v_label">196484</data>
    </node>
    <node id="n860">
      <data key="v_name">258835</data>
      <data key="v_label">258835</data>
    </node>
    <node id="n861">
      <data key="v_name">550377</data>
      <data key="v_label">550377</data>
    </node>
    <node id="n862">
      <data key="v_name">549740</data>
      <data key="v_label">549740</data>
    </node>
    <node id="n863">
      <data key="v_name">549739</data>
      <data key="v_label">549739</data>
    </node>
    <node id="n864">
      <data key="v_name">334102</data>
    </node>
    <node id="n865">
      <data key="v_name">514912</data>
      <data key="v_label">514912</data>
    </node>
    <node id="n866">
      <data key="v_name">461021</data>
      <data key="v_label">461021</data>
    </node>
    <node id="n867">
      <data key="v_name">380924</data>
      <data key="v_label">380924</data>
    </node>
    <node id="n868">
      <data key="v_name">259496</data>
      <data key="v_label">259496</data>
    </node>
    <node id="n869">
      <data key="v_name">315512</data>
      <data key="v_label">315512</data>
    </node>
    <node id="n870">
      <data key="v_name">486331</data>
      <data key="v_label">486331</data>
    </node>
    <node id="n871">
      <data key="v_name">229244</data>
      <data key="v_label">229244</data>
    </node>
    <node id="n872">
      <data key="v_name">558497</data>
      <data key="v_label">558497</data>
    </node>
    <node id="n873">
      <data key="v_name">531624</data>
      <data key="v_label">531624</data>
    </node>
    <node id="n874">
      <data key="v_name">518420</data>
      <data key="v_label">518420</data>
    </node>
    <node id="n875">
      <data key="v_name">451494</data>
      <data key="v_label">451494</data>
    </node>
    <node id="n876">
      <data key="v_name">493395</data>
      <data key="v_label">493395</data>
    </node>
    <node id="n877">
      <data key="v_name">508477</data>
    </node>
    <node id="n878">
      <data key="v_name">522280</data>
      <data key="v_label">522280</data>
    </node>
    <node id="n879">
      <data key="v_name">319643</data>
      <data key="v_label">319643</data>
    </node>
    <node id="n880">
      <data key="v_name">164495</data>
      <data key="v_label">164495</data>
    </node>
    <node id="n881">
      <data key="v_name">164496</data>
      <data key="v_label">164496</data>
    </node>
    <node id="n882">
      <data key="v_name">315258</data>
      <data key="v_label">315258</data>
    </node>
    <node id="n883">
      <data key="v_name">552897</data>
      <data key="v_label">552897</data>
    </node>
    <node id="n884">
      <data key="v_name">326088</data>
      <data key="v_label">326088</data>
    </node>
    <node id="n885">
      <data key="v_name">464484</data>
      <data key="v_label">464484</data>
    </node>
    <node id="n886">
      <data key="v_name">517075</data>
      <data key="v_label">517075</data>
    </node>
    <node id="n887">
      <data key="v_name">398834</data>
      <data key="v_label">398834</data>
    </node>
    <node id="n888">
      <data key="v_name">553633</data>
    </node>
    <node id="n889">
      <data key="v_name">505325</data>
      <data key="v_label">505325</data>
    </node>
    <node id="n890">
      <data key="v_name">173681</data>
      <data key="v_label">173681</data>
    </node>
    <node id="n891">
      <data key="v_name">353411</data>
      <data key="v_label">353411</data>
    </node>
    <node id="n892">
      <data key="v_name">409197</data>
      <data key="v_label">409197</data>
    </node>
    <node id="n893">
      <data key="v_name">316670</data>
      <data key="v_label">316670</data>
    </node>
    <node id="n894">
      <data key="v_name">559652</data>
      <data key="v_label">559652</data>
    </node>
    <node id="n895">
      <data key="v_name">533228</data>
      <data key="v_label">533228</data>
    </node>
    <node id="n896">
      <data key="v_name">292992</data>
      <data key="v_label">292992</data>
    </node>
    <node id="n897">
      <data key="v_name">334091</data>
    </node>
    <node id="n898">
      <data key="v_name">451003</data>
    </node>
    <node id="n899">
      <data key="v_name">257184</data>
      <data key="v_label">257184</data>
    </node>
    <node id="n900">
      <data key="v_name">464128</data>
    </node>
    <node id="n901">
      <data key="v_name">173334</data>
      <data key="v_label">173334</data>
    </node>
    <node id="n902">
      <data key="v_name">492724</data>
      <data key="v_label">492724</data>
    </node>
    <node id="n903">
      <data key="v_name">486470</data>
      <data key="v_label">486470</data>
    </node>
    <node id="n904">
      <data key="v_name">471780</data>
      <data key="v_label">471780</data>
    </node>
    <node id="n905">
      <data key="v_name">465435</data>
      <data key="v_label">465435</data>
    </node>
    <node id="n906">
      <data key="v_name">347888</data>
      <data key="v_label">347888</data>
    </node>
    <node id="n907">
      <data key="v_name">533380</data>
      <data key="v_label">533380</data>
    </node>
    <node id="n908">
      <data key="v_name">558488</data>
      <data key="v_label">558488</data>
    </node>
    <node id="n909">
      <data key="v_name">551063</data>
      <data key="v_label">551063</data>
    </node>
    <node id="n910">
      <data key="v_name">517379</data>
      <data key="v_label">517379</data>
    </node>
    <node id="n911">
      <data key="v_name">328586</data>
      <data key="v_label">328586</data>
    </node>
    <node id="n912">
      <data key="v_name">159328</data>
      <data key="v_label">159328</data>
    </node>
    <node id="n913">
      <data key="v_name">156364</data>
      <data key="v_label">156364</data>
    </node>
    <node id="n914">
      <data key="v_name">549589</data>
    </node>
    <node id="n915">
      <data key="v_name">533242</data>
      <data key="v_label">533242</data>
    </node>
    <node id="n916">
      <data key="v_name">558467</data>
      <data key="v_label">558467</data>
    </node>
    <node id="n917">
      <data key="v_name">165174</data>
      <data key="v_label">165174</data>
    </node>
    <node id="n918">
      <data key="v_name">553248</data>
    </node>
    <node id="n919">
      <data key="v_name">508378</data>
      <data key="v_label">508378</data>
    </node>
    <node id="n920">
      <data key="v_name">550619</data>
      <data key="v_label">550619</data>
    </node>
    <node id="n921">
      <data key="v_name">157976</data>
      <data key="v_label">157976</data>
    </node>
    <node id="n922">
      <data key="v_name">157977</data>
      <data key="v_label">157977</data>
    </node>
    <node id="n923">
      <data key="v_name">270075</data>
      <data key="v_label">270075</data>
    </node>
    <node id="n924">
      <data key="v_name">380971</data>
      <data key="v_label">380971</data>
    </node>
    <node id="n925">
      <data key="v_name">496841</data>
      <data key="v_label">496841</data>
    </node>
    <node id="n926">
      <data key="v_name">517378</data>
      <data key="v_label">517378</data>
    </node>
    <node id="n927">
      <data key="v_name">561154</data>
      <data key="v_label">561154</data>
    </node>
    <node id="n928">
      <data key="v_name">513354</data>
    </node>
    <node id="n929">
      <data key="v_name">290244</data>
      <data key="v_label">290244</data>
    </node>
    <node id="n930">
      <data key="v_name">416996</data>
      <data key="v_label">416996</data>
    </node>
    <node id="n931">
      <data key="v_name">550540</data>
      <data key="v_label">550540</data>
    </node>
    <node id="n932">
      <data key="v_name">541916</data>
      <data key="v_label">541916</data>
    </node>
    <node id="n933">
      <data key="v_name">284570</data>
      <data key="v_label">284570</data>
    </node>
    <node id="n934">
      <data key="v_name">461485</data>
      <data key="v_label">461485</data>
    </node>
    <node id="n935">
      <data key="v_name">548478</data>
      <data key="v_label">548478</data>
    </node>
    <node id="n936">
      <data key="v_name">284572</data>
      <data key="v_label">284572</data>
    </node>
    <node id="n937">
      <data key="v_name">482142</data>
      <data key="v_label">482142</data>
    </node>
    <node id="n938">
      <data key="v_name">173067</data>
      <data key="v_label">173067</data>
    </node>
    <node id="n939">
      <data key="v_name">485691</data>
      <data key="v_label">485691</data>
    </node>
    <node id="n940">
      <data key="v_name">65705</data>
      <data key="v_label">65705</data>
    </node>
    <node id="n941">
      <data key="v_name">539293</data>
      <data key="v_label">539293</data>
    </node>
    <node id="n942">
      <data key="v_name">308498</data>
      <data key="v_label">308498</data>
    </node>
    <node id="n943">
      <data key="v_name">308499</data>
      <data key="v_label">308499</data>
    </node>
    <node id="n944">
      <data key="v_name">187175</data>
      <data key="v_label">187175</data>
    </node>
    <node id="n945">
      <data key="v_name">495864</data>
      <data key="v_label">495864</data>
    </node>
    <node id="n946">
      <data key="v_name">560926</data>
      <data key="v_label">560926</data>
    </node>
    <node id="n947">
      <data key="v_name">558935</data>
      <data key="v_label">558935</data>
    </node>
    <node id="n948">
      <data key="v_name">560335</data>
      <data key="v_label">560335</data>
    </node>
    <node id="n949">
      <data key="v_name">485891</data>
      <data key="v_label">485891</data>
    </node>
    <node id="n950">
      <data key="v_name">496511</data>
      <data key="v_label">496511</data>
    </node>
    <node id="n951">
      <data key="v_name">502549</data>
      <data key="v_label">502549</data>
    </node>
    <node id="n952">
      <data key="v_name">562859</data>
      <data key="v_label">562859</data>
    </node>
    <node id="n953">
      <data key="v_name">226262</data>
      <data key="v_label">226262</data>
    </node>
    <node id="n954">
      <data key="v_name">409328</data>
      <data key="v_label">409328</data>
    </node>
    <node id="n955">
      <data key="v_name">554195</data>
      <data key="v_label">554195</data>
    </node>
    <node id="n956">
      <data key="v_name">564690</data>
      <data key="v_label">564690</data>
    </node>
    <node id="n957">
      <data key="v_name">230250</data>
      <data key="v_label">230250</data>
    </node>
    <node id="n958">
      <data key="v_name">292887</data>
      <data key="v_label">292887</data>
    </node>
    <node id="n959">
      <data key="v_name">517810</data>
    </node>
    <node id="n960">
      <data key="v_name">540194</data>
      <data key="v_label">540194</data>
    </node>
    <node id="n961">
      <data key="v_name">424178</data>
    </node>
    <node id="n962">
      <data key="v_name">339843</data>
      <data key="v_label">339843</data>
    </node>
    <node id="n963">
      <data key="v_name">339844</data>
      <data key="v_label">339844</data>
    </node>
    <node id="n964">
      <data key="v_name">485877</data>
      <data key="v_label">485877</data>
    </node>
    <node id="n965">
      <data key="v_name">297663</data>
      <data key="v_label">297663</data>
    </node>
    <node id="n966">
      <data key="v_name">525789</data>
      <data key="v_label">525789</data>
    </node>
    <node id="n967">
      <data key="v_name">164962</data>
      <data key="v_label">164962</data>
    </node>
    <node id="n968">
      <data key="v_name">506701</data>
      <data key="v_label">506701</data>
    </node>
    <node id="n969">
      <data key="v_name">362119</data>
      <data key="v_label">362119</data>
    </node>
    <node id="n970">
      <data key="v_name">475099</data>
      <data key="v_label">475099</data>
    </node>
    <node id="n971">
      <data key="v_name">530562</data>
      <data key="v_label">530562</data>
    </node>
    <node id="n972">
      <data key="v_name">553671</data>
      <data key="v_label">553671</data>
    </node>
    <node id="n973">
      <data key="v_name">515490</data>
      <data key="v_label">515490</data>
    </node>
    <node id="n974">
      <data key="v_name">539504</data>
      <data key="v_label">539504</data>
    </node>
    <node id="n975">
      <data key="v_name">492724</data>
    </node>
    <node id="n976">
      <data key="v_name">550512</data>
      <data key="v_label">550512</data>
    </node>
    <node id="n977">
      <data key="v_name">451494</data>
    </node>
    <node id="n978">
      <data key="v_name">493395</data>
    </node>
    <node id="n979">
      <data key="v_name">524966</data>
      <data key="v_label">524966</data>
    </node>
    <node id="n980">
      <data key="v_name">547801</data>
      <data key="v_label">547801</data>
    </node>
    <node id="n981">
      <data key="v_name">534213</data>
    </node>
    <node id="n982">
      <data key="v_name">532922</data>
      <data key="v_label">532922</data>
    </node>
    <node id="n983">
      <data key="v_name">487903</data>
      <data key="v_label">487903</data>
    </node>
    <node id="n984">
      <data key="v_name">535238</data>
    </node>
    <node id="n985">
      <data key="v_name">165391</data>
      <data key="v_label">165391</data>
    </node>
    <node id="n986">
      <data key="v_name">531543</data>
      <data key="v_label">531543</data>
    </node>
    <node id="n987">
      <data key="v_name">328368</data>
      <data key="v_label">328368</data>
    </node>
    <node id="n988">
      <data key="v_name">196349</data>
      <data key="v_label">196349</data>
    </node>
    <node id="n989">
      <data key="v_name">553600</data>
      <data key="v_label">553600</data>
    </node>
    <node id="n990">
      <data key="v_name">162387</data>
      <data key="v_label">162387</data>
    </node>
    <node id="n991">
      <data key="v_name">507639</data>
      <data key="v_label">507639</data>
    </node>
    <node id="n992">
      <data key="v_name">534923</data>
      <data key="v_label">534923</data>
    </node>
    <node id="n993">
      <data key="v_name">517806</data>
      <data key="v_label">517806</data>
    </node>
    <node id="n994">
      <data key="v_name">499391</data>
      <data key="v_label">499391</data>
    </node>
    <node id="n995">
      <data key="v_name">389215</data>
      <data key="v_label">389215</data>
    </node>
    <node id="n996">
      <data key="v_name">486559</data>
      <data key="v_label">486559</data>
    </node>
    <node id="n997">
      <data key="v_name">196484</data>
    </node>
    <node id="n998">
      <data key="v_name">495935</data>
      <data key="v_label">495935</data>
    </node>
    <node id="n999">
      <data key="v_name">231101</data>
      <data key="v_label">231101</data>
    </node>
    <node id="n1000">
      <data key="v_name">177383</data>
      <data key="v_label">177383</data>
    </node>
    <node id="n1001">
      <data key="v_name">529556</data>
      <data key="v_label">529556</data>
    </node>
    <node id="n1002">
      <data key="v_name">540130</data>
    </node>
    <node id="n1003">
      <data key="v_name">438490</data>
      <data key="v_label">438490</data>
    </node>
    <node id="n1004">
      <data key="v_name">492015</data>
      <data key="v_label">492015</data>
    </node>
    <node id="n1005">
      <data key="v_name">151489</data>
      <data key="v_label">151489</data>
    </node>
    <node id="n1006">
      <data key="v_name">151490</data>
      <data key="v_label">151490</data>
    </node>
    <node id="n1007">
      <data key="v_name">161446</data>
      <data key="v_label">161446</data>
    </node>
    <node id="n1008">
      <data key="v_name">161698</data>
      <data key="v_label">161698</data>
    </node>
    <node id="n1009">
      <data key="v_name">303787</data>
      <data key="v_label">303787</data>
    </node>
    <node id="n1010">
      <data key="v_name">161700</data>
      <data key="v_label">161700</data>
    </node>
    <node id="n1011">
      <data key="v_name">460500</data>
      <data key="v_label">460500</data>
    </node>
    <node id="n1012">
      <data key="v_name">526924</data>
      <data key="v_label">526924</data>
    </node>
    <node id="n1013">
      <data key="v_name">161696</data>
      <data key="v_label">161696</data>
    </node>
    <node id="n1014">
      <data key="v_name">430298</data>
    </node>
    <node id="n1015">
      <data key="v_name">495364</data>
      <data key="v_label">495364</data>
    </node>
    <node id="n1016">
      <data key="v_name">383979</data>
      <data key="v_label">383979</data>
    </node>
    <node id="n1017">
      <data key="v_name">383980</data>
      <data key="v_label">383980</data>
    </node>
    <node id="n1018">
      <data key="v_name">448955</data>
      <data key="v_label">448955</data>
    </node>
    <node id="n1019">
      <data key="v_name">507741</data>
      <data key="v_label">507741</data>
    </node>
    <node id="n1020">
      <data key="v_name">398545</data>
      <data key="v_label">398545</data>
    </node>
    <node id="n1021">
      <data key="v_name">531866</data>
      <data key="v_label">531866</data>
    </node>
    <node id="n1022">
      <data key="v_name">518256</data>
      <data key="v_label">518256</data>
    </node>
    <node id="n1023">
      <data key="v_name">492978</data>
      <data key="v_label">492978</data>
    </node>
    <node id="n1024">
      <data key="v_name">506987</data>
      <data key="v_label">506987</data>
    </node>
    <node id="n1025">
      <data key="v_name">517969</data>
    </node>
    <node id="n1026">
      <data key="v_name">485891</data>
    </node>
    <node id="n1027">
      <data key="v_name">258340</data>
      <data key="v_label">258340</data>
    </node>
    <node id="n1028">
      <data key="v_name">168133</data>
      <data key="v_label">168133</data>
    </node>
    <node id="n1029">
      <data key="v_name">561985</data>
    </node>
    <node id="n1030">
      <data key="v_name">548200</data>
      <data key="v_label">548200</data>
    </node>
    <node id="n1031">
      <data key="v_name">168137</data>
      <data key="v_label">168137</data>
    </node>
    <node id="n1032">
      <data key="v_name">168138</data>
      <data key="v_label">168138</data>
    </node>
    <node id="n1033">
      <data key="v_name">478239</data>
      <data key="v_label">478239</data>
    </node>
    <node id="n1034">
      <data key="v_name">538211</data>
      <data key="v_label">538211</data>
    </node>
    <node id="n1035">
      <data key="v_name">530692</data>
      <data key="v_label">530692</data>
    </node>
    <node id="n1036">
      <data key="v_name">559256</data>
      <data key="v_label">559256</data>
    </node>
    <node id="n1037">
      <data key="v_name">531440</data>
      <data key="v_label">531440</data>
    </node>
    <node id="n1038">
      <data key="v_name">554547</data>
      <data key="v_label">554547</data>
    </node>
    <node id="n1039">
      <data key="v_name">533355</data>
    </node>
    <node id="n1040">
      <data key="v_name">450650</data>
      <data key="v_label">450650</data>
    </node>
    <node id="n1041">
      <data key="v_name">424207</data>
      <data key="v_label">424207</data>
    </node>
    <node id="n1042">
      <data key="v_name">213809</data>
    </node>
    <node id="n1043">
      <data key="v_name">497082</data>
      <data key="v_label">497082</data>
    </node>
    <node id="n1044">
      <data key="v_name">526896</data>
      <data key="v_label">526896</data>
    </node>
    <node id="n1045">
      <data key="v_name">152774</data>
      <data key="v_label">152774</data>
    </node>
    <node id="n1046">
      <data key="v_name">540016</data>
      <data key="v_label">540016</data>
    </node>
    <node id="n1047">
      <data key="v_name">459571</data>
      <data key="v_label">459571</data>
    </node>
    <node id="n1048">
      <data key="v_name">483300</data>
      <data key="v_label">483300</data>
    </node>
    <node id="n1049">
      <data key="v_name">341681</data>
      <data key="v_label">341681</data>
    </node>
    <node id="n1050">
      <data key="v_name">562760</data>
      <data key="v_label">562760</data>
    </node>
    <node id="n1051">
      <data key="v_name">518376</data>
    </node>
    <node id="n1052">
      <data key="v_name">532090</data>
      <data key="v_label">532090</data>
    </node>
    <node id="n1053">
      <data key="v_name">259849</data>
      <data key="v_label">259849</data>
    </node>
    <node id="n1054">
      <data key="v_name">214520</data>
      <data key="v_label">214520</data>
    </node>
    <node id="n1055">
      <data key="v_name">475693</data>
      <data key="v_label">475693</data>
    </node>
    <node id="n1056">
      <data key="v_name">518814</data>
      <data key="v_label">518814</data>
    </node>
    <node id="n1057">
      <data key="v_name">542046</data>
      <data key="v_label">542046</data>
    </node>
    <node id="n1058">
      <data key="v_name">167042</data>
      <data key="v_label">167042</data>
    </node>
    <node id="n1059">
      <data key="v_name">292367</data>
      <data key="v_label">292367</data>
    </node>
    <node id="n1060">
      <data key="v_name">406100</data>
      <data key="v_label">406100</data>
    </node>
    <node id="n1061">
      <data key="v_name">264297</data>
      <data key="v_label">264297</data>
    </node>
    <node id="n1062">
      <data key="v_name">559934</data>
      <data key="v_label">559934</data>
    </node>
    <node id="n1063">
      <data key="v_name">538628</data>
      <data key="v_label">538628</data>
    </node>
    <node id="n1064">
      <data key="v_name">537233</data>
      <data key="v_label">537233</data>
    </node>
    <node id="n1065">
      <data key="v_name">69811</data>
    </node>
    <node id="n1066">
      <data key="v_name">538795</data>
      <data key="v_label">538795</data>
    </node>
    <node id="n1067">
      <data key="v_name">538795</data>
    </node>
    <node id="n1068">
      <data key="v_name">157565</data>
      <data key="v_label">157565</data>
    </node>
    <node id="n1069">
      <data key="v_name">450784</data>
      <data key="v_label">450784</data>
    </node>
    <node id="n1070">
      <data key="v_name">543444</data>
      <data key="v_label">543444</data>
    </node>
    <node id="n1071">
      <data key="v_name">543446</data>
      <data key="v_label">543446</data>
    </node>
    <node id="n1072">
      <data key="v_name">543447</data>
      <data key="v_label">543447</data>
    </node>
    <node id="n1073">
      <data key="v_name">498157</data>
      <data key="v_label">498157</data>
    </node>
    <node id="n1074">
      <data key="v_name">538000</data>
      <data key="v_label">538000</data>
    </node>
    <node id="n1075">
      <data key="v_name">173848</data>
      <data key="v_label">173848</data>
    </node>
    <node id="n1076">
      <data key="v_name">421169</data>
    </node>
    <node id="n1077">
      <data key="v_name">540138</data>
    </node>
    <node id="n1078">
      <data key="v_name">423456</data>
      <data key="v_label">423456</data>
    </node>
    <node id="n1079">
      <data key="v_name">326088</data>
    </node>
    <node id="n1080">
      <data key="v_name">165821</data>
      <data key="v_label">165821</data>
    </node>
    <node id="n1081">
      <data key="v_name">410040</data>
      <data key="v_label">410040</data>
    </node>
    <node id="n1082">
      <data key="v_name">402290</data>
      <data key="v_label">402290</data>
    </node>
    <node id="n1083">
      <data key="v_name">431683</data>
      <data key="v_label">431683</data>
    </node>
    <node id="n1084">
      <data key="v_name">550709</data>
    </node>
    <node id="n1085">
      <data key="v_name">550876</data>
      <data key="v_label">550876</data>
    </node>
    <node id="n1086">
      <data key="v_name">490760</data>
      <data key="v_label">490760</data>
    </node>
    <node id="n1087">
      <data key="v_name">462892</data>
    </node>
    <node id="n1088">
      <data key="v_name">558126</data>
      <data key="v_label">558126</data>
    </node>
    <node id="n1089">
      <data key="v_name">492224</data>
      <data key="v_label">492224</data>
    </node>
    <node id="n1090">
      <data key="v_name">560092</data>
      <data key="v_label">560092</data>
    </node>
    <node id="n1091">
      <data key="v_name">335579</data>
      <data key="v_label">335579</data>
    </node>
    <node id="n1092">
      <data key="v_name">161576</data>
      <data key="v_label">161576</data>
    </node>
    <node id="n1093">
      <data key="v_name">487172</data>
      <data key="v_label">487172</data>
    </node>
    <node id="n1094">
      <data key="v_name">161578</data>
      <data key="v_label">161578</data>
    </node>
    <node id="n1095">
      <data key="v_name">540138</data>
    </node>
    <node id="n1096">
      <data key="v_name">284791</data>
      <data key="v_label">284791</data>
    </node>
    <node id="n1097">
      <data key="v_name">564461</data>
      <data key="v_label">564461</data>
    </node>
    <node id="n1098">
      <data key="v_name">549951</data>
      <data key="v_label">549951</data>
    </node>
    <node id="n1099">
      <data key="v_name">550273</data>
      <data key="v_label">550273</data>
    </node>
    <node id="n1100">
      <data key="v_name">490801</data>
      <data key="v_label">490801</data>
    </node>
    <node id="n1101">
      <data key="v_name">490803</data>
      <data key="v_label">490803</data>
    </node>
    <node id="n1102">
      <data key="v_name">173391</data>
      <data key="v_label">173391</data>
    </node>
    <node id="n1103">
      <data key="v_name">514674</data>
      <data key="v_label">514674</data>
    </node>
    <node id="n1104">
      <data key="v_name">541876</data>
      <data key="v_label">541876</data>
    </node>
    <node id="n1105">
      <data key="v_name">548127</data>
      <data key="v_label">548127</data>
    </node>
    <node id="n1106">
      <data key="v_name">533380</data>
    </node>
    <node id="n1107">
      <data key="v_name">485665</data>
      <data key="v_label">485665</data>
    </node>
    <node id="n1108">
      <data key="v_name">492380</data>
      <data key="v_label">492380</data>
    </node>
    <node id="n1109">
      <data key="v_name">533504</data>
      <data key="v_label">533504</data>
    </node>
    <node id="n1110">
      <data key="v_name">159102</data>
      <data key="v_label">159102</data>
    </node>
    <node id="n1111">
      <data key="v_name">547799</data>
      <data key="v_label">547799</data>
    </node>
    <node id="n1112">
      <data key="v_name">448584</data>
      <data key="v_label">448584</data>
    </node>
    <node id="n1113">
      <data key="v_name">78952</data>
      <data key="v_label">78952</data>
    </node>
    <node id="n1114">
      <data key="v_name">417615</data>
      <data key="v_label">417615</data>
    </node>
    <node id="n1115">
      <data key="v_name">140111</data>
      <data key="v_label">140111</data>
    </node>
    <node id="n1116">
      <data key="v_name">140112</data>
      <data key="v_label">140112</data>
    </node>
    <node id="n1117">
      <data key="v_name">140113</data>
      <data key="v_label">140113</data>
    </node>
    <node id="n1118">
      <data key="v_name">152351</data>
      <data key="v_label">152351</data>
    </node>
    <node id="n1119">
      <data key="v_name">485900</data>
      <data key="v_label">485900</data>
    </node>
    <node id="n1120">
      <data key="v_name">528453</data>
      <data key="v_label">528453</data>
    </node>
    <node id="n1121">
      <data key="v_name">557830</data>
      <data key="v_label">557830</data>
    </node>
    <node id="n1122">
      <data key="v_name">485468</data>
      <data key="v_label">485468</data>
    </node>
    <node id="n1123">
      <data key="v_name">553614</data>
    </node>
    <node id="n1124">
      <data key="v_name">553615</data>
    </node>
    <node id="n1125">
      <data key="v_name">550445</data>
    </node>
    <node id="n1126">
      <data key="v_name">483285</data>
      <data key="v_label">483285</data>
    </node>
    <node id="n1127">
      <data key="v_name">562346</data>
      <data key="v_label">562346</data>
    </node>
    <node id="n1128">
      <data key="v_name">224941</data>
      <data key="v_label">224941</data>
    </node>
    <node id="n1129">
      <data key="v_name">515756</data>
      <data key="v_label">515756</data>
    </node>
    <node id="n1130">
      <data key="v_name">550872</data>
      <data key="v_label">550872</data>
    </node>
    <node id="n1131">
      <data key="v_name">157230</data>
      <data key="v_label">157230</data>
    </node>
    <node id="n1132">
      <data key="v_name">336016</data>
      <data key="v_label">336016</data>
    </node>
    <node id="n1133">
      <data key="v_name">563479</data>
      <data key="v_label">563479</data>
    </node>
    <node id="n1134">
      <data key="v_name">283428</data>
      <data key="v_label">283428</data>
    </node>
    <node id="n1135">
      <data key="v_name">496204</data>
      <data key="v_label">496204</data>
    </node>
    <node id="n1136">
      <data key="v_name">561892</data>
      <data key="v_label">561892</data>
    </node>
    <node id="n1137">
      <data key="v_name">516907</data>
      <data key="v_label">516907</data>
    </node>
    <node id="n1138">
      <data key="v_name">508240</data>
      <data key="v_label">508240</data>
    </node>
    <node id="n1139">
      <data key="v_name">490256</data>
      <data key="v_label">490256</data>
    </node>
    <node id="n1140">
      <data key="v_name">564049</data>
      <data key="v_label">564049</data>
    </node>
    <node id="n1141">
      <data key="v_name">182948</data>
      <data key="v_label">182948</data>
    </node>
    <node id="n1142">
      <data key="v_name">438098</data>
    </node>
    <node id="n1143">
      <data key="v_name">565135</data>
    </node>
    <node id="n1144">
      <data key="v_name">526244</data>
      <data key="v_label">526244</data>
    </node>
    <node id="n1145">
      <data key="v_name">532976</data>
      <data key="v_label">532976</data>
    </node>
    <node id="n1146">
      <data key="v_name">553286</data>
      <data key="v_label">553286</data>
    </node>
    <node id="n1147">
      <data key="v_name">456808</data>
      <data key="v_label">456808</data>
    </node>
    <node id="n1148">
      <data key="v_name">517945</data>
      <data key="v_label">517945</data>
    </node>
    <node id="n1149">
      <data key="v_name">548216</data>
      <data key="v_label">548216</data>
    </node>
    <node id="n1150">
      <data key="v_name">387279</data>
      <data key="v_label">387279</data>
    </node>
    <node id="n1151">
      <data key="v_name">491613</data>
      <data key="v_label">491613</data>
    </node>
    <node id="n1152">
      <data key="v_name">553703</data>
      <data key="v_label">553703</data>
    </node>
    <node id="n1153">
      <data key="v_name">553704</data>
      <data key="v_label">553704</data>
    </node>
    <node id="n1154">
      <data key="v_name">441922</data>
      <data key="v_label">441922</data>
    </node>
    <node id="n1155">
      <data key="v_name">451890</data>
    </node>
    <node id="n1156">
      <data key="v_name">521384</data>
    </node>
    <node id="n1157">
      <data key="v_name">459379</data>
      <data key="v_label">459379</data>
    </node>
    <node id="n1158">
      <data key="v_name">7594</data>
      <data key="v_label">7594</data>
    </node>
    <node id="n1159">
      <data key="v_name">173346</data>
      <data key="v_label">173346</data>
    </node>
    <node id="n1160">
      <data key="v_name">173143</data>
      <data key="v_label">173143</data>
    </node>
    <node id="n1161">
      <data key="v_name">158256</data>
      <data key="v_label">158256</data>
    </node>
    <node id="n1162">
      <data key="v_name">517945</data>
    </node>
    <node id="n1163">
      <data key="v_name">486183</data>
      <data key="v_label">486183</data>
    </node>
    <node id="n1164">
      <data key="v_name">515828</data>
      <data key="v_label">515828</data>
    </node>
    <node id="n1165">
      <data key="v_name">518376</data>
    </node>
    <node id="n1166">
      <data key="v_name">520937</data>
      <data key="v_label">520937</data>
    </node>
    <node id="n1167">
      <data key="v_name">167169</data>
      <data key="v_label">167169</data>
    </node>
    <node id="n1168">
      <data key="v_name">540286</data>
      <data key="v_label">540286</data>
    </node>
    <node id="n1169">
      <data key="v_name">551113</data>
      <data key="v_label">551113</data>
    </node>
    <node id="n1170">
      <data key="v_name">217983</data>
      <data key="v_label">217983</data>
    </node>
    <node id="n1171">
      <data key="v_name">167173</data>
      <data key="v_label">167173</data>
    </node>
    <node id="n1172">
      <data key="v_name">438603</data>
    </node>
    <node id="n1173">
      <data key="v_name">542095</data>
    </node>
    <node id="n1174">
      <data key="v_name">549589</data>
    </node>
    <node id="n1175">
      <data key="v_name">450771</data>
    </node>
    <node id="n1176">
      <data key="v_name">440155</data>
    </node>
    <node id="n1177">
      <data key="v_name">144300</data>
      <data key="v_label">144300</data>
    </node>
    <node id="n1178">
      <data key="v_name">543519</data>
      <data key="v_label">543519</data>
    </node>
    <node id="n1179">
      <data key="v_name">543520</data>
      <data key="v_label">543520</data>
    </node>
    <node id="n1180">
      <data key="v_name">448703</data>
      <data key="v_label">448703</data>
    </node>
    <node id="n1181">
      <data key="v_name">359767</data>
      <data key="v_label">359767</data>
    </node>
    <node id="n1182">
      <data key="v_name">277392</data>
      <data key="v_label">277392</data>
    </node>
    <node id="n1183">
      <data key="v_name">493345</data>
      <data key="v_label">493345</data>
    </node>
    <node id="n1184">
      <data key="v_name">414683</data>
      <data key="v_label">414683</data>
    </node>
    <node id="n1185">
      <data key="v_name">553335</data>
      <data key="v_label">553335</data>
    </node>
    <node id="n1186">
      <data key="v_name">556394</data>
      <data key="v_label">556394</data>
    </node>
    <node id="n1187">
      <data key="v_name">162505</data>
      <data key="v_label">162505</data>
    </node>
    <node id="n1188">
      <data key="v_name">201965</data>
      <data key="v_label">201965</data>
    </node>
    <node id="n1189">
      <data key="v_name">542602</data>
      <data key="v_label">542602</data>
    </node>
    <node id="n1190">
      <data key="v_name">167538</data>
      <data key="v_label">167538</data>
    </node>
    <node id="n1191">
      <data key="v_name">524677</data>
      <data key="v_label">524677</data>
    </node>
    <node id="n1192">
      <data key="v_name">370776</data>
      <data key="v_label">370776</data>
    </node>
    <node id="n1193">
      <data key="v_name">182123</data>
      <data key="v_label">182123</data>
    </node>
    <node id="n1194">
      <data key="v_name">231158</data>
      <data key="v_label">231158</data>
    </node>
    <node id="n1195">
      <data key="v_name">536741</data>
      <data key="v_label">536741</data>
    </node>
    <node id="n1196">
      <data key="v_name">561154</data>
    </node>
    <node id="n1197">
      <data key="v_name">496841</data>
    </node>
    <node id="n1198">
      <data key="v_name">485665</data>
    </node>
    <node id="n1199">
      <data key="v_name">542046</data>
    </node>
    <node id="n1200">
      <data key="v_name">409115</data>
      <data key="v_label">409115</data>
    </node>
    <node id="n1201">
      <data key="v_name">562752</data>
    </node>
    <node id="n1202">
      <data key="v_name">347935</data>
    </node>
    <node id="n1203">
      <data key="v_name">485126</data>
    </node>
    <node id="n1204">
      <data key="v_name">560261</data>
      <data key="v_label">560261</data>
    </node>
    <node id="n1205">
      <data key="v_name">483488</data>
      <data key="v_label">483488</data>
    </node>
    <node id="n1206">
      <data key="v_name">550380</data>
      <data key="v_label">550380</data>
    </node>
    <node id="n1207">
      <data key="v_name">340480</data>
      <data key="v_label">340480</data>
    </node>
    <node id="n1208">
      <data key="v_name">166002</data>
      <data key="v_label">166002</data>
    </node>
    <node id="n1209">
      <data key="v_name">485200</data>
      <data key="v_label">485200</data>
    </node>
    <node id="n1210">
      <data key="v_name">166004</data>
      <data key="v_label">166004</data>
    </node>
    <node id="n1211">
      <data key="v_name">507772</data>
      <data key="v_label">507772</data>
    </node>
    <node id="n1212">
      <data key="v_name">464680</data>
      <data key="v_label">464680</data>
    </node>
    <node id="n1213">
      <data key="v_name">494506</data>
      <data key="v_label">494506</data>
    </node>
    <node id="n1214">
      <data key="v_name">517717</data>
    </node>
    <node id="n1215">
      <data key="v_name">557489</data>
      <data key="v_label">557489</data>
    </node>
    <node id="n1216">
      <data key="v_name">560175</data>
      <data key="v_label">560175</data>
    </node>
    <node id="n1217">
      <data key="v_name">344399</data>
      <data key="v_label">344399</data>
    </node>
    <node id="n1218">
      <data key="v_name">560677</data>
      <data key="v_label">560677</data>
    </node>
    <node id="n1219">
      <data key="v_name">430756</data>
      <data key="v_label">430756</data>
    </node>
    <node id="n1220">
      <data key="v_name">541895</data>
      <data key="v_label">541895</data>
    </node>
    <node id="n1221">
      <data key="v_name">541893</data>
    </node>
    <node id="n1222">
      <data key="v_name">550389</data>
      <data key="v_label">550389</data>
    </node>
    <node id="n1223">
      <data key="v_name">550390</data>
      <data key="v_label">550390</data>
    </node>
    <node id="n1224">
      <data key="v_name">393581</data>
      <data key="v_label">393581</data>
    </node>
    <node id="n1225">
      <data key="v_name">425804</data>
      <data key="v_label">425804</data>
    </node>
    <node id="n1226">
      <data key="v_name">485742</data>
      <data key="v_label">485742</data>
    </node>
    <node id="n1227">
      <data key="v_name">486839</data>
      <data key="v_label">486839</data>
    </node>
    <node id="n1228">
      <data key="v_name">559843</data>
      <data key="v_label">559843</data>
    </node>
    <node id="n1229">
      <data key="v_name">438877</data>
      <data key="v_label">438877</data>
    </node>
    <node id="n1230">
      <data key="v_name">508418</data>
      <data key="v_label">508418</data>
    </node>
    <node id="n1231">
      <data key="v_name">526352</data>
      <data key="v_label">526352</data>
    </node>
    <node id="n1232">
      <data key="v_name">456490</data>
      <data key="v_label">456490</data>
    </node>
    <node id="n1233">
      <data key="v_name">364451</data>
      <data key="v_label">364451</data>
    </node>
    <node id="n1234">
      <data key="v_name">492042</data>
      <data key="v_label">492042</data>
    </node>
    <node id="n1235">
      <data key="v_name">551857</data>
      <data key="v_label">551857</data>
    </node>
    <node id="n1236">
      <data key="v_name">541976</data>
      <data key="v_label">541976</data>
    </node>
    <node id="n1237">
      <data key="v_name">510448</data>
      <data key="v_label">510448</data>
    </node>
    <node id="n1238">
      <data key="v_name">194049</data>
      <data key="v_label">194049</data>
    </node>
    <node id="n1239">
      <data key="v_name">152454</data>
      <data key="v_label">152454</data>
    </node>
    <node id="n1240">
      <data key="v_name">218209</data>
      <data key="v_label">218209</data>
    </node>
    <node id="n1241">
      <data key="v_name">455101</data>
      <data key="v_label">455101</data>
    </node>
    <node id="n1242">
      <data key="v_name">523483</data>
      <data key="v_label">523483</data>
    </node>
    <node id="n1243">
      <data key="v_name">467745</data>
      <data key="v_label">467745</data>
    </node>
    <node id="n1244">
      <data key="v_name">376063</data>
      <data key="v_label">376063</data>
    </node>
    <node id="n1245">
      <data key="v_name">426259</data>
      <data key="v_label">426259</data>
    </node>
    <node id="n1246">
      <data key="v_name">200680</data>
      <data key="v_label">200680</data>
    </node>
    <node id="n1247">
      <data key="v_name">427568</data>
      <data key="v_label">427568</data>
    </node>
    <node id="n1248">
      <data key="v_name">335768</data>
      <data key="v_label">335768</data>
    </node>
    <node id="n1249">
      <data key="v_name">166463</data>
    </node>
    <node id="n1250">
      <data key="v_name">431529</data>
    </node>
    <node id="n1251">
      <data key="v_name">410638</data>
      <data key="v_label">410638</data>
    </node>
    <node id="n1252">
      <data key="v_name">147425</data>
      <data key="v_label">147425</data>
    </node>
    <node id="n1253">
      <data key="v_name">293075</data>
      <data key="v_label">293075</data>
    </node>
    <node id="n1254">
      <data key="v_name">560392</data>
      <data key="v_label">560392</data>
    </node>
    <node id="n1255">
      <data key="v_name">542016</data>
    </node>
    <node id="n1256">
      <data key="v_name">248351</data>
      <data key="v_label">248351</data>
    </node>
    <node id="n1257">
      <data key="v_name">248351</data>
    </node>
    <node id="n1258">
      <data key="v_name">158093</data>
      <data key="v_label">158093</data>
    </node>
    <node id="n1259">
      <data key="v_name">158093</data>
    </node>
    <node id="n1260">
      <data key="v_name">329226</data>
      <data key="v_label">329226</data>
    </node>
    <node id="n1261">
      <data key="v_name">516924</data>
    </node>
    <node id="n1262">
      <data key="v_name">520933</data>
      <data key="v_label">520933</data>
    </node>
    <node id="n1263">
      <data key="v_name">517079</data>
    </node>
    <node id="n1264">
      <data key="v_name">560683</data>
      <data key="v_label">560683</data>
    </node>
    <node id="n1265">
      <data key="v_name">489873</data>
    </node>
    <node id="n1266">
      <data key="v_name">527785</data>
      <data key="v_label">527785</data>
    </node>
    <node id="n1267">
      <data key="v_name">157665</data>
      <data key="v_label">157665</data>
    </node>
    <node id="n1268">
      <data key="v_name">555419</data>
    </node>
    <node id="n1269">
      <data key="v_name">548220</data>
      <data key="v_label">548220</data>
    </node>
    <node id="n1270">
      <data key="v_name">140602</data>
      <data key="v_label">140602</data>
    </node>
    <node id="n1271">
      <data key="v_name">554071</data>
      <data key="v_label">554071</data>
    </node>
    <node id="n1272">
      <data key="v_name">198121</data>
    </node>
    <node id="n1273">
      <data key="v_name">560217</data>
      <data key="v_label">560217</data>
    </node>
    <node id="n1274">
      <data key="v_name">167381</data>
      <data key="v_label">167381</data>
    </node>
    <node id="n1275">
      <data key="v_name">320635</data>
      <data key="v_label">320635</data>
    </node>
    <node id="n1276">
      <data key="v_name">140300</data>
      <data key="v_label">140300</data>
    </node>
    <node id="n1277">
      <data key="v_name">560601</data>
      <data key="v_label">560601</data>
    </node>
    <node id="n1278">
      <data key="v_name">392322</data>
      <data key="v_label">392322</data>
    </node>
    <node id="n1279">
      <data key="v_name">171437</data>
      <data key="v_label">171437</data>
    </node>
    <node id="n1280">
      <data key="v_name">143459</data>
      <data key="v_label">143459</data>
    </node>
    <node id="n1281">
      <data key="v_name">237796</data>
      <data key="v_label">237796</data>
    </node>
    <node id="n1282">
      <data key="v_name">143461</data>
      <data key="v_label">143461</data>
    </node>
    <node id="n1283">
      <data key="v_name">155198</data>
      <data key="v_label">155198</data>
    </node>
    <node id="n1284">
      <data key="v_name">549718</data>
      <data key="v_label">549718</data>
    </node>
    <node id="n1285">
      <data key="v_name">338902</data>
      <data key="v_label">338902</data>
    </node>
    <node id="n1286">
      <data key="v_name">516887</data>
      <data key="v_label">516887</data>
    </node>
    <node id="n1287">
      <data key="v_name">506707</data>
      <data key="v_label">506707</data>
    </node>
    <node id="n1288">
      <data key="v_name">429111</data>
      <data key="v_label">429111</data>
    </node>
    <node id="n1289">
      <data key="v_name">214137</data>
      <data key="v_label">214137</data>
    </node>
    <node id="n1290">
      <data key="v_name">561267</data>
    </node>
    <node id="n1291">
      <data key="v_name">474594</data>
      <data key="v_label">474594</data>
    </node>
    <node id="n1292">
      <data key="v_name">456506</data>
      <data key="v_label">456506</data>
    </node>
    <node id="n1293">
      <data key="v_name">546362</data>
      <data key="v_label">546362</data>
    </node>
    <node id="n1294">
      <data key="v_name">473862</data>
      <data key="v_label">473862</data>
    </node>
    <node id="n1295">
      <data key="v_name">473863</data>
      <data key="v_label">473863</data>
    </node>
    <node id="n1296">
      <data key="v_name">559428</data>
      <data key="v_label">559428</data>
    </node>
    <node id="n1297">
      <data key="v_name">473316</data>
      <data key="v_label">473316</data>
    </node>
    <node id="n1298">
      <data key="v_name">167554</data>
      <data key="v_label">167554</data>
    </node>
    <node id="n1299">
      <data key="v_name">532936</data>
      <data key="v_label">532936</data>
    </node>
    <node id="n1300">
      <data key="v_name">548334</data>
      <data key="v_label">548334</data>
    </node>
    <node id="n1301">
      <data key="v_name">536675</data>
      <data key="v_label">536675</data>
    </node>
    <node id="n1302">
      <data key="v_name">511575</data>
      <data key="v_label">511575</data>
    </node>
    <node id="n1303">
      <data key="v_name">290799</data>
      <data key="v_label">290799</data>
    </node>
    <node id="n1304">
      <data key="v_name">283428</data>
    </node>
    <node id="n1305">
      <data key="v_name">285595</data>
      <data key="v_label">285595</data>
    </node>
    <node id="n1306">
      <data key="v_name">171437</data>
    </node>
    <node id="n1307">
      <data key="v_name">477210</data>
      <data key="v_label">477210</data>
    </node>
    <node id="n1308">
      <data key="v_name">528940</data>
      <data key="v_label">528940</data>
    </node>
    <node id="n1309">
      <data key="v_name">425207</data>
      <data key="v_label">425207</data>
    </node>
    <node id="n1310">
      <data key="v_name">168153</data>
      <data key="v_label">168153</data>
    </node>
    <node id="n1311">
      <data key="v_name">143788</data>
      <data key="v_label">143788</data>
    </node>
    <node id="n1312">
      <data key="v_name">280545</data>
      <data key="v_label">280545</data>
    </node>
    <node id="n1313">
      <data key="v_name">292992</data>
    </node>
    <node id="n1314">
      <data key="v_name">334068</data>
      <data key="v_label">334068</data>
    </node>
    <node id="n1315">
      <data key="v_name">531055</data>
    </node>
    <node id="n1316">
      <data key="v_name">457204</data>
      <data key="v_label">457204</data>
    </node>
    <node id="n1317">
      <data key="v_name">147254</data>
      <data key="v_label">147254</data>
    </node>
    <node id="n1318">
      <data key="v_name">226023</data>
    </node>
    <node id="n1319">
      <data key="v_name">518003</data>
    </node>
    <node id="n1320">
      <data key="v_name">564777</data>
    </node>
    <node id="n1321">
      <data key="v_name">547675</data>
    </node>
    <node id="n1322">
      <data key="v_name">550149</data>
    </node>
    <node id="n1323">
      <data key="v_name">163585</data>
    </node>
    <node id="n1324">
      <data key="v_name">505936</data>
      <data key="v_label">505936</data>
    </node>
    <node id="n1325">
      <data key="v_name">524497</data>
      <data key="v_label">524497</data>
    </node>
    <node id="n1326">
      <data key="v_name">543535</data>
      <data key="v_label">543535</data>
    </node>
    <node id="n1327">
      <data key="v_name">559716</data>
      <data key="v_label">559716</data>
    </node>
    <node id="n1328">
      <data key="v_name">553609</data>
      <data key="v_label">553609</data>
    </node>
    <node id="n1329">
      <data key="v_name">318770</data>
      <data key="v_label">318770</data>
    </node>
    <node id="n1330">
      <data key="v_name">418746</data>
      <data key="v_label">418746</data>
    </node>
    <node id="n1331">
      <data key="v_name">551992</data>
      <data key="v_label">551992</data>
    </node>
    <node id="n1332">
      <data key="v_name">558595</data>
      <data key="v_label">558595</data>
    </node>
    <node id="n1333">
      <data key="v_name">226709</data>
      <data key="v_label">226709</data>
    </node>
    <node id="n1334">
      <data key="v_name">540637</data>
      <data key="v_label">540637</data>
    </node>
    <node id="n1335">
      <data key="v_name">316739</data>
      <data key="v_label">316739</data>
    </node>
    <node id="n1336">
      <data key="v_name">224279</data>
      <data key="v_label">224279</data>
    </node>
    <node id="n1337">
      <data key="v_name">551035</data>
      <data key="v_label">551035</data>
    </node>
    <node id="n1338">
      <data key="v_name">486206</data>
      <data key="v_label">486206</data>
    </node>
    <node id="n1339">
      <data key="v_name">550008</data>
      <data key="v_label">550008</data>
    </node>
    <node id="n1340">
      <data key="v_name">542982</data>
      <data key="v_label">542982</data>
    </node>
    <node id="n1341">
      <data key="v_name">478385</data>
      <data key="v_label">478385</data>
    </node>
    <node id="n1342">
      <data key="v_name">340578</data>
      <data key="v_label">340578</data>
    </node>
    <node id="n1343">
      <data key="v_name">192425</data>
      <data key="v_label">192425</data>
    </node>
    <node id="n1344">
      <data key="v_name">500659</data>
      <data key="v_label">500659</data>
    </node>
    <node id="n1345">
      <data key="v_name">255197</data>
      <data key="v_label">255197</data>
    </node>
    <node id="n1346">
      <data key="v_name">553603</data>
      <data key="v_label">553603</data>
    </node>
    <node id="n1347">
      <data key="v_name">533937</data>
      <data key="v_label">533937</data>
    </node>
    <node id="n1348">
      <data key="v_name">550769</data>
      <data key="v_label">550769</data>
    </node>
    <node id="n1349">
      <data key="v_name">526817</data>
      <data key="v_label">526817</data>
    </node>
    <node id="n1350">
      <data key="v_name">161738</data>
      <data key="v_label">161738</data>
    </node>
    <node id="n1351">
      <data key="v_name">469100</data>
      <data key="v_label">469100</data>
    </node>
    <node id="n1352">
      <data key="v_name">563971</data>
      <data key="v_label">563971</data>
    </node>
    <node id="n1353">
      <data key="v_name">474722</data>
      <data key="v_label">474722</data>
    </node>
    <node id="n1354">
      <data key="v_name">482838</data>
      <data key="v_label">482838</data>
    </node>
    <node id="n1355">
      <data key="v_name">561141</data>
      <data key="v_label">561141</data>
    </node>
    <node id="n1356">
      <data key="v_name">435957</data>
      <data key="v_label">435957</data>
    </node>
    <node id="n1357">
      <data key="v_name">385617</data>
      <data key="v_label">385617</data>
    </node>
    <node id="n1358">
      <data key="v_name">165764</data>
      <data key="v_label">165764</data>
    </node>
    <node id="n1359">
      <data key="v_name">165765</data>
      <data key="v_label">165765</data>
    </node>
    <node id="n1360">
      <data key="v_name">506686</data>
    </node>
    <node id="n1361">
      <data key="v_name">353970</data>
      <data key="v_label">353970</data>
    </node>
    <node id="n1362">
      <data key="v_name">518483</data>
      <data key="v_label">518483</data>
    </node>
    <node id="n1363">
      <data key="v_name">157684</data>
      <data key="v_label">157684</data>
    </node>
    <node id="n1364">
      <data key="v_name">173131</data>
      <data key="v_label">173131</data>
    </node>
    <node id="n1365">
      <data key="v_name">327312</data>
      <data key="v_label">327312</data>
    </node>
    <node id="n1366">
      <data key="v_name">164113</data>
      <data key="v_label">164113</data>
    </node>
    <node id="n1367">
      <data key="v_name">164114</data>
      <data key="v_label">164114</data>
    </node>
    <node id="n1368">
      <data key="v_name">487976</data>
      <data key="v_label">487976</data>
    </node>
    <node id="n1369">
      <data key="v_name">548182</data>
    </node>
    <node id="n1370">
      <data key="v_name">231101</data>
    </node>
    <node id="n1371">
      <data key="v_name">153088</data>
      <data key="v_label">153088</data>
    </node>
    <node id="n1372">
      <data key="v_name">172543</data>
      <data key="v_label">172543</data>
    </node>
    <node id="n1373">
      <data key="v_name">540507</data>
      <data key="v_label">540507</data>
    </node>
    <node id="n1374">
      <data key="v_name">562709</data>
      <data key="v_label">562709</data>
    </node>
    <node id="n1375">
      <data key="v_name">162458</data>
      <data key="v_label">162458</data>
    </node>
    <node id="n1376">
      <data key="v_name">170917</data>
      <data key="v_label">170917</data>
    </node>
    <node id="n1377">
      <data key="v_name">456506</data>
    </node>
    <node id="n1378">
      <data key="v_name">546362</data>
    </node>
    <node id="n1379">
      <data key="v_name">473862</data>
    </node>
    <node id="n1380">
      <data key="v_name">162453</data>
      <data key="v_label">162453</data>
    </node>
    <node id="n1381">
      <data key="v_name">473863</data>
    </node>
    <node id="n1382">
      <data key="v_name">536698</data>
      <data key="v_label">536698</data>
    </node>
    <node id="n1383">
      <data key="v_name">518043</data>
      <data key="v_label">518043</data>
    </node>
    <node id="n1384">
      <data key="v_name">564316</data>
      <data key="v_label">564316</data>
    </node>
    <node id="n1385">
      <data key="v_name">526576</data>
      <data key="v_label">526576</data>
    </node>
    <node id="n1386">
      <data key="v_name">457024</data>
      <data key="v_label">457024</data>
    </node>
    <node id="n1387">
      <data key="v_name">162470</data>
      <data key="v_label">162470</data>
    </node>
    <node id="n1388">
      <data key="v_name">509628</data>
      <data key="v_label">509628</data>
    </node>
    <node id="n1389">
      <data key="v_name">385409</data>
      <data key="v_label">385409</data>
    </node>
    <node id="n1390">
      <data key="v_name">355756</data>
      <data key="v_label">355756</data>
    </node>
    <node id="n1391">
      <data key="v_name">557048</data>
      <data key="v_label">557048</data>
    </node>
    <node id="n1392">
      <data key="v_name">381337</data>
      <data key="v_label">381337</data>
    </node>
    <node id="n1393">
      <data key="v_name">517933</data>
      <data key="v_label">517933</data>
    </node>
    <node id="n1394">
      <data key="v_name">486458</data>
    </node>
    <node id="n1395">
      <data key="v_name">470080</data>
      <data key="v_label">470080</data>
    </node>
    <node id="n1396">
      <data key="v_name">425109</data>
      <data key="v_label">425109</data>
    </node>
    <node id="n1397">
      <data key="v_name">451830</data>
      <data key="v_label">451830</data>
    </node>
    <node id="n1398">
      <data key="v_name">539039</data>
    </node>
    <node id="n1399">
      <data key="v_name">394642</data>
    </node>
    <node id="n1400">
      <data key="v_name">180288</data>
    </node>
    <node id="n1401">
      <data key="v_name">530675</data>
      <data key="v_label">530675</data>
    </node>
    <node id="n1402">
      <data key="v_name">169075</data>
      <data key="v_label">169075</data>
    </node>
    <node id="n1403">
      <data key="v_name">423537</data>
      <data key="v_label">423537</data>
    </node>
    <node id="n1404">
      <data key="v_name">503756</data>
      <data key="v_label">503756</data>
    </node>
    <node id="n1405">
      <data key="v_name">148214</data>
      <data key="v_label">148214</data>
    </node>
    <node id="n1406">
      <data key="v_name">517378</data>
    </node>
    <node id="n1407">
      <data key="v_name">499748</data>
    </node>
    <node id="n1408">
      <data key="v_name">507476</data>
      <data key="v_label">507476</data>
    </node>
    <node id="n1409">
      <data key="v_name">403203</data>
      <data key="v_label">403203</data>
    </node>
    <node id="n1410">
      <data key="v_name">167135</data>
      <data key="v_label">167135</data>
    </node>
    <node id="n1411">
      <data key="v_name">550455</data>
      <data key="v_label">550455</data>
    </node>
    <node id="n1412">
      <data key="v_name">517826</data>
      <data key="v_label">517826</data>
    </node>
    <node id="n1413">
      <data key="v_name">451177</data>
      <data key="v_label">451177</data>
    </node>
    <node id="n1414">
      <data key="v_name">521840</data>
    </node>
    <node id="n1415">
      <data key="v_name">417502</data>
      <data key="v_label">417502</data>
    </node>
    <node id="n1416">
      <data key="v_name">564777</data>
    </node>
    <node id="n1417">
      <data key="v_name">553596</data>
    </node>
    <node id="n1418">
      <data key="v_name">547675</data>
    </node>
    <node id="n1419">
      <data key="v_name">550149</data>
    </node>
    <node id="n1420">
      <data key="v_name">338092</data>
      <data key="v_label">338092</data>
    </node>
    <node id="n1421">
      <data key="v_name">226214</data>
      <data key="v_label">226214</data>
    </node>
    <node id="n1422">
      <data key="v_name">493579</data>
      <data key="v_label">493579</data>
    </node>
    <node id="n1423">
      <data key="v_name">559268</data>
      <data key="v_label">559268</data>
    </node>
    <node id="n1424">
      <data key="v_name">550389</data>
    </node>
    <node id="n1425">
      <data key="v_name">548322</data>
      <data key="v_label">548322</data>
    </node>
    <node id="n1426">
      <data key="v_name">511983</data>
      <data key="v_label">511983</data>
    </node>
    <node id="n1427">
      <data key="v_name">498150</data>
      <data key="v_label">498150</data>
    </node>
    <node id="n1428">
      <data key="v_name">422017</data>
      <data key="v_label">422017</data>
    </node>
    <node id="n1429">
      <data key="v_name">166040</data>
      <data key="v_label">166040</data>
    </node>
    <node id="n1430">
      <data key="v_name">209618</data>
      <data key="v_label">209618</data>
    </node>
    <node id="n1431">
      <data key="v_name">309212</data>
      <data key="v_label">309212</data>
    </node>
    <node id="n1432">
      <data key="v_name">381203</data>
      <data key="v_label">381203</data>
    </node>
    <node id="n1433">
      <data key="v_name">381204</data>
      <data key="v_label">381204</data>
    </node>
    <node id="n1434">
      <data key="v_name">381205</data>
      <data key="v_label">381205</data>
    </node>
    <node id="n1435">
      <data key="v_name">507752</data>
      <data key="v_label">507752</data>
    </node>
    <node id="n1436">
      <data key="v_name">542015</data>
    </node>
    <node id="n1437">
      <data key="v_name">542016</data>
    </node>
    <node id="n1438">
      <data key="v_name">549542</data>
    </node>
    <node id="n1439">
      <data key="v_name">167668</data>
      <data key="v_label">167668</data>
    </node>
    <node id="n1440">
      <data key="v_name">207718</data>
      <data key="v_label">207718</data>
    </node>
    <node id="n1441">
      <data key="v_name">167670</data>
      <data key="v_label">167670</data>
    </node>
    <node id="n1442">
      <data key="v_name">167671</data>
      <data key="v_label">167671</data>
    </node>
    <node id="n1443">
      <data key="v_name">167672</data>
      <data key="v_label">167672</data>
    </node>
    <node id="n1444">
      <data key="v_name">167054</data>
      <data key="v_label">167054</data>
    </node>
    <node id="n1445">
      <data key="v_name">551122</data>
      <data key="v_label">551122</data>
    </node>
    <node id="n1446">
      <data key="v_name">167056</data>
      <data key="v_label">167056</data>
    </node>
    <node id="n1447">
      <data key="v_name">167057</data>
      <data key="v_label">167057</data>
    </node>
    <node id="n1448">
      <data key="v_name">545964</data>
      <data key="v_label">545964</data>
    </node>
    <node id="n1449">
      <data key="v_name">218583</data>
    </node>
    <node id="n1450">
      <data key="v_name">561926</data>
      <data key="v_label">561926</data>
    </node>
    <node id="n1451">
      <data key="v_name">147905</data>
      <data key="v_label">147905</data>
    </node>
    <node id="n1452">
      <data key="v_name">548220</data>
    </node>
    <node id="n1453">
      <data key="v_name">493225</data>
      <data key="v_label">493225</data>
    </node>
    <node id="n1454">
      <data key="v_name">511536</data>
      <data key="v_label">511536</data>
    </node>
    <node id="n1455">
      <data key="v_name">517079</data>
    </node>
    <node id="n1456">
      <data key="v_name">410040</data>
    </node>
    <node id="n1457">
      <data key="v_name">165632</data>
      <data key="v_label">165632</data>
    </node>
    <node id="n1458">
      <data key="v_name">524966</data>
    </node>
    <node id="n1459">
      <data key="v_name">547801</data>
    </node>
    <node id="n1460">
      <data key="v_name">549181</data>
      <data key="v_label">549181</data>
    </node>
    <node id="n1461">
      <data key="v_name">353592</data>
      <data key="v_label">353592</data>
    </node>
    <node id="n1462">
      <data key="v_name">522400</data>
      <data key="v_label">522400</data>
    </node>
    <node id="n1463">
      <data key="v_name">173300</data>
      <data key="v_label">173300</data>
    </node>
    <node id="n1464">
      <data key="v_name">496947</data>
      <data key="v_label">496947</data>
    </node>
    <node id="n1465">
      <data key="v_name">346629</data>
      <data key="v_label">346629</data>
    </node>
    <node id="n1466">
      <data key="v_name">473966</data>
      <data key="v_label">473966</data>
    </node>
    <node id="n1467">
      <data key="v_name">473967</data>
      <data key="v_label">473967</data>
    </node>
    <node id="n1468">
      <data key="v_name">208197</data>
      <data key="v_label">208197</data>
    </node>
    <node id="n1469">
      <data key="v_name">498314</data>
      <data key="v_label">498314</data>
    </node>
    <node id="n1470">
      <data key="v_name">549841</data>
      <data key="v_label">549841</data>
    </node>
    <node id="n1471">
      <data key="v_name">556819</data>
    </node>
    <node id="n1472">
      <data key="v_name">550390</data>
    </node>
    <node id="n1473">
      <data key="v_name">338092</data>
    </node>
    <node id="n1474">
      <data key="v_name">358793</data>
      <data key="v_label">358793</data>
    </node>
    <node id="n1475">
      <data key="v_name">558505</data>
      <data key="v_label">558505</data>
    </node>
    <node id="n1476">
      <data key="v_name">209728</data>
    </node>
    <node id="n1477">
      <data key="v_name">560798</data>
      <data key="v_label">560798</data>
    </node>
    <node id="n1478">
      <data key="v_name">309451</data>
      <data key="v_label">309451</data>
    </node>
    <node id="n1479">
      <data key="v_name">381805</data>
      <data key="v_label">381805</data>
    </node>
    <node id="n1480">
      <data key="v_name">329226</data>
    </node>
    <node id="n1481">
      <data key="v_name">416036</data>
      <data key="v_label">416036</data>
    </node>
    <node id="n1482">
      <data key="v_name">486425</data>
      <data key="v_label">486425</data>
    </node>
    <node id="n1483">
      <data key="v_name">421129</data>
      <data key="v_label">421129</data>
    </node>
    <node id="n1484">
      <data key="v_name">494757</data>
    </node>
    <node id="n1485">
      <data key="v_name">167269</data>
      <data key="v_label">167269</data>
    </node>
    <node id="n1486">
      <data key="v_name">416036</data>
    </node>
    <node id="n1487">
      <data key="v_name">353783</data>
      <data key="v_label">353783</data>
    </node>
    <node id="n1488">
      <data key="v_name">518945</data>
      <data key="v_label">518945</data>
    </node>
    <node id="n1489">
      <data key="v_name">519226</data>
      <data key="v_label">519226</data>
    </node>
    <node id="n1490">
      <data key="v_name">526576</data>
    </node>
    <node id="n1491">
      <data key="v_name">167277</data>
      <data key="v_label">167277</data>
    </node>
    <node id="n1492">
      <data key="v_name">506607</data>
      <data key="v_label">506607</data>
    </node>
    <node id="n1493">
      <data key="v_name">548283</data>
      <data key="v_label">548283</data>
    </node>
    <node id="n1494">
      <data key="v_name">380954</data>
    </node>
    <node id="n1495">
      <data key="v_name">379772</data>
      <data key="v_label">379772</data>
    </node>
    <node id="n1496">
      <data key="v_name">382816</data>
      <data key="v_label">382816</data>
    </node>
    <node id="n1497">
      <data key="v_name">382817</data>
      <data key="v_label">382817</data>
    </node>
    <node id="n1498">
      <data key="v_name">173354</data>
      <data key="v_label">173354</data>
    </node>
    <node id="n1499">
      <data key="v_name">173355</data>
      <data key="v_label">173355</data>
    </node>
    <node id="n1500">
      <data key="v_name">318078</data>
      <data key="v_label">318078</data>
    </node>
    <node id="n1501">
      <data key="v_name">173357</data>
      <data key="v_label">173357</data>
    </node>
    <node id="n1502">
      <data key="v_name">454577</data>
      <data key="v_label">454577</data>
    </node>
    <node id="n1503">
      <data key="v_name">507838</data>
      <data key="v_label">507838</data>
    </node>
    <node id="n1504">
      <data key="v_name">496864</data>
      <data key="v_label">496864</data>
    </node>
    <node id="n1505">
      <data key="v_name">315316</data>
      <data key="v_label">315316</data>
    </node>
    <node id="n1506">
      <data key="v_name">485591</data>
      <data key="v_label">485591</data>
    </node>
    <node id="n1507">
      <data key="v_name">541322</data>
      <data key="v_label">541322</data>
    </node>
    <node id="n1508">
      <data key="v_name">255078</data>
      <data key="v_label">255078</data>
    </node>
    <node id="n1509">
      <data key="v_name">167898</data>
      <data key="v_label">167898</data>
    </node>
    <node id="n1510">
      <data key="v_name">184119</data>
      <data key="v_label">184119</data>
    </node>
    <node id="n1511">
      <data key="v_name">553504</data>
      <data key="v_label">553504</data>
    </node>
    <node id="n1512">
      <data key="v_name">390875</data>
      <data key="v_label">390875</data>
    </node>
    <node id="n1513">
      <data key="v_name">560692</data>
      <data key="v_label">560692</data>
    </node>
    <node id="n1514">
      <data key="v_name">497072</data>
      <data key="v_label">497072</data>
    </node>
    <node id="n1515">
      <data key="v_name">341404</data>
      <data key="v_label">341404</data>
    </node>
    <node id="n1516">
      <data key="v_name">559445</data>
      <data key="v_label">559445</data>
    </node>
    <node id="n1517">
      <data key="v_name">531608</data>
      <data key="v_label">531608</data>
    </node>
    <node id="n1518">
      <data key="v_name">313372</data>
      <data key="v_label">313372</data>
    </node>
    <node id="n1519">
      <data key="v_name">462040</data>
      <data key="v_label">462040</data>
    </node>
    <node id="n1520">
      <data key="v_name">518029</data>
      <data key="v_label">518029</data>
    </node>
    <node id="n1521">
      <data key="v_name">173374</data>
      <data key="v_label">173374</data>
    </node>
    <node id="n1522">
      <data key="v_name">276726</data>
      <data key="v_label">276726</data>
    </node>
    <node id="n1523">
      <data key="v_name">533380</data>
    </node>
    <node id="n1524">
      <data key="v_name">556730</data>
    </node>
    <node id="n1525">
      <data key="v_name">161570</data>
      <data key="v_label">161570</data>
    </node>
    <node id="n1526">
      <data key="v_name">214166</data>
      <data key="v_label">214166</data>
    </node>
    <node id="n1527">
      <data key="v_name">161572</data>
      <data key="v_label">161572</data>
    </node>
    <node id="n1528">
      <data key="v_name">454789</data>
      <data key="v_label">454789</data>
    </node>
    <node id="n1529">
      <data key="v_name">560018</data>
    </node>
    <node id="n1530">
      <data key="v_name">401830</data>
      <data key="v_label">401830</data>
    </node>
    <node id="n1531">
      <data key="v_name">250071</data>
      <data key="v_label">250071</data>
    </node>
    <node id="n1532">
      <data key="v_name">518488</data>
      <data key="v_label">518488</data>
    </node>
    <node id="n1533">
      <data key="v_name">456863</data>
      <data key="v_label">456863</data>
    </node>
    <node id="n1534">
      <data key="v_name">531852</data>
      <data key="v_label">531852</data>
    </node>
    <node id="n1535">
      <data key="v_name">246359</data>
      <data key="v_label">246359</data>
    </node>
    <node id="n1536">
      <data key="v_name">557489</data>
    </node>
    <node id="n1537">
      <data key="v_name">429027</data>
      <data key="v_label">429027</data>
    </node>
    <node id="n1538">
      <data key="v_name">474957</data>
      <data key="v_label">474957</data>
    </node>
    <node id="n1539">
      <data key="v_name">558173</data>
      <data key="v_label">558173</data>
    </node>
    <node id="n1540">
      <data key="v_name">451820</data>
      <data key="v_label">451820</data>
    </node>
    <node id="n1541">
      <data key="v_name">559163</data>
      <data key="v_label">559163</data>
    </node>
    <node id="n1542">
      <data key="v_name">409563</data>
      <data key="v_label">409563</data>
    </node>
    <node id="n1543">
      <data key="v_name">250083</data>
      <data key="v_label">250083</data>
    </node>
    <node id="n1544">
      <data key="v_name">376954</data>
      <data key="v_label">376954</data>
    </node>
    <node id="n1545">
      <data key="v_name">543580</data>
    </node>
    <node id="n1546">
      <data key="v_name">529929</data>
      <data key="v_label">529929</data>
    </node>
    <node id="n1547">
      <data key="v_name">334101</data>
      <data key="v_label">334101</data>
    </node>
    <node id="n1548">
      <data key="v_name">543277</data>
      <data key="v_label">543277</data>
    </node>
    <node id="n1549">
      <data key="v_name">379546</data>
    </node>
    <node id="n1550">
      <data key="v_name">310658</data>
      <data key="v_label">310658</data>
    </node>
    <node id="n1551">
      <data key="v_name">559294</data>
      <data key="v_label">559294</data>
    </node>
    <node id="n1552">
      <data key="v_name">171549</data>
      <data key="v_label">171549</data>
    </node>
    <node id="n1553">
      <data key="v_name">235075</data>
      <data key="v_label">235075</data>
    </node>
    <node id="n1554">
      <data key="v_name">343329</data>
      <data key="v_label">343329</data>
    </node>
    <node id="n1555">
      <data key="v_name">427040</data>
      <data key="v_label">427040</data>
    </node>
    <node id="n1556">
      <data key="v_name">520677</data>
      <data key="v_label">520677</data>
    </node>
    <node id="n1557">
      <data key="v_name">529083</data>
      <data key="v_label">529083</data>
    </node>
    <node id="n1558">
      <data key="v_name">410638</data>
    </node>
    <node id="n1559">
      <data key="v_name">468846</data>
      <data key="v_label">468846</data>
    </node>
    <node id="n1560">
      <data key="v_name">539633</data>
    </node>
    <node id="n1561">
      <data key="v_name">560813</data>
      <data key="v_label">560813</data>
    </node>
    <node id="n1562">
      <data key="v_name">485219</data>
      <data key="v_label">485219</data>
    </node>
    <node id="n1563">
      <data key="v_name">297550</data>
      <data key="v_label">297550</data>
    </node>
    <node id="n1564">
      <data key="v_name">460117</data>
      <data key="v_label">460117</data>
    </node>
    <node id="n1565">
      <data key="v_name">450764</data>
      <data key="v_label">450764</data>
    </node>
    <node id="n1566">
      <data key="v_name">450784</data>
    </node>
    <node id="n1567">
      <data key="v_name">532591</data>
      <data key="v_label">532591</data>
    </node>
    <node id="n1568">
      <data key="v_name">167184</data>
      <data key="v_label">167184</data>
    </node>
    <node id="n1569">
      <data key="v_name">376155</data>
      <data key="v_label">376155</data>
    </node>
    <node id="n1570">
      <data key="v_name">161750</data>
      <data key="v_label">161750</data>
    </node>
    <node id="n1571">
      <data key="v_name">209868</data>
      <data key="v_label">209868</data>
    </node>
    <node id="n1572">
      <data key="v_name">496078</data>
      <data key="v_label">496078</data>
    </node>
    <node id="n1573">
      <data key="v_name">429867</data>
      <data key="v_label">429867</data>
    </node>
    <node id="n1574">
      <data key="v_name">413474</data>
      <data key="v_label">413474</data>
    </node>
    <node id="n1575">
      <data key="v_name">267325</data>
      <data key="v_label">267325</data>
    </node>
    <node id="n1576">
      <data key="v_name">520920</data>
      <data key="v_label">520920</data>
    </node>
    <node id="n1577">
      <data key="v_name">518455</data>
    </node>
    <node id="n1578">
      <data key="v_name">533271</data>
      <data key="v_label">533271</data>
    </node>
    <node id="n1579">
      <data key="v_name">549757</data>
      <data key="v_label">549757</data>
    </node>
    <node id="n1580">
      <data key="v_name">167498</data>
      <data key="v_label">167498</data>
    </node>
    <node id="n1581">
      <data key="v_name">532971</data>
      <data key="v_label">532971</data>
    </node>
    <node id="n1582">
      <data key="v_name">278821</data>
      <data key="v_label">278821</data>
    </node>
    <node id="n1583">
      <data key="v_name">521101</data>
      <data key="v_label">521101</data>
    </node>
    <node id="n1584">
      <data key="v_name">457516</data>
      <data key="v_label">457516</data>
    </node>
    <node id="n1585">
      <data key="v_name">371226</data>
      <data key="v_label">371226</data>
    </node>
    <node id="n1586">
      <data key="v_name">533348</data>
      <data key="v_label">533348</data>
    </node>
    <node id="n1587">
      <data key="v_name">371225</data>
      <data key="v_label">371225</data>
    </node>
    <node id="n1588">
      <data key="v_name">167495</data>
      <data key="v_label">167495</data>
    </node>
    <node id="n1589">
      <data key="v_name">558205</data>
      <data key="v_label">558205</data>
    </node>
    <node id="n1590">
      <data key="v_name">531167</data>
    </node>
    <node id="n1591">
      <data key="v_name">345777</data>
      <data key="v_label">345777</data>
    </node>
    <node id="n1592">
      <data key="v_name">516991</data>
      <data key="v_label">516991</data>
    </node>
    <node id="n1593">
      <data key="v_name">527902</data>
      <data key="v_label">527902</data>
    </node>
    <node id="n1594">
      <data key="v_name">338092</data>
    </node>
    <node id="n1595">
      <data key="v_name">409042</data>
      <data key="v_label">409042</data>
    </node>
    <node id="n1596">
      <data key="v_name">483791</data>
      <data key="v_label">483791</data>
    </node>
    <node id="n1597">
      <data key="v_name">438512</data>
      <data key="v_label">438512</data>
    </node>
    <node id="n1598">
      <data key="v_name">166377</data>
      <data key="v_label">166377</data>
    </node>
    <node id="n1599">
      <data key="v_name">166378</data>
      <data key="v_label">166378</data>
    </node>
    <node id="n1600">
      <data key="v_name">166379</data>
      <data key="v_label">166379</data>
    </node>
    <node id="n1601">
      <data key="v_name">450215</data>
      <data key="v_label">450215</data>
    </node>
    <node id="n1602">
      <data key="v_name">450216</data>
      <data key="v_label">450216</data>
    </node>
    <node id="n1603">
      <data key="v_name">166396</data>
      <data key="v_label">166396</data>
    </node>
    <node id="n1604">
      <data key="v_name">166397</data>
      <data key="v_label">166397</data>
    </node>
    <node id="n1605">
      <data key="v_name">166398</data>
      <data key="v_label">166398</data>
    </node>
    <node id="n1606">
      <data key="v_name">370419</data>
      <data key="v_label">370419</data>
    </node>
    <node id="n1607">
      <data key="v_name">521484</data>
      <data key="v_label">521484</data>
    </node>
    <node id="n1608">
      <data key="v_name">521367</data>
      <data key="v_label">521367</data>
    </node>
    <node id="n1609">
      <data key="v_name">553671</data>
    </node>
    <node id="n1610">
      <data key="v_name">283519</data>
      <data key="v_label">283519</data>
    </node>
    <node id="n1611">
      <data key="v_name">530883</data>
      <data key="v_label">530883</data>
    </node>
    <node id="n1612">
      <data key="v_name">501795</data>
      <data key="v_label">501795</data>
    </node>
    <node id="n1613">
      <data key="v_name">167522</data>
      <data key="v_label">167522</data>
    </node>
    <node id="n1614">
      <data key="v_name">167523</data>
      <data key="v_label">167523</data>
    </node>
    <node id="n1615">
      <data key="v_name">424015</data>
      <data key="v_label">424015</data>
    </node>
    <node id="n1616">
      <data key="v_name">563803</data>
      <data key="v_label">563803</data>
    </node>
    <node id="n1617">
      <data key="v_name">508256</data>
    </node>
    <node id="n1618">
      <data key="v_name">409620</data>
      <data key="v_label">409620</data>
    </node>
    <node id="n1619">
      <data key="v_name">506686</data>
    </node>
    <node id="n1620">
      <data key="v_name">167530</data>
      <data key="v_label">167530</data>
    </node>
    <node id="n1621">
      <data key="v_name">180543</data>
      <data key="v_label">180543</data>
    </node>
    <node id="n1622">
      <data key="v_name">253766</data>
      <data key="v_label">253766</data>
    </node>
    <node id="n1623">
      <data key="v_name">557162</data>
      <data key="v_label">557162</data>
    </node>
    <node id="n1624">
      <data key="v_name">375361</data>
      <data key="v_label">375361</data>
    </node>
    <node id="n1625">
      <data key="v_name">431099</data>
      <data key="v_label">431099</data>
    </node>
    <node id="n1626">
      <data key="v_name">565122</data>
      <data key="v_label">565122</data>
    </node>
    <node id="n1627">
      <data key="v_name">510611</data>
      <data key="v_label">510611</data>
    </node>
    <node id="n1628">
      <data key="v_name">358370</data>
      <data key="v_label">358370</data>
    </node>
    <node id="n1629">
      <data key="v_name">162482</data>
      <data key="v_label">162482</data>
    </node>
    <node id="n1630">
      <data key="v_name">156744</data>
      <data key="v_label">156744</data>
    </node>
    <node id="n1631">
      <data key="v_name">156745</data>
      <data key="v_label">156745</data>
    </node>
    <node id="n1632">
      <data key="v_name">156746</data>
      <data key="v_label">156746</data>
    </node>
    <node id="n1633">
      <data key="v_name">226214</data>
    </node>
    <node id="n1634">
      <data key="v_name">261524</data>
      <data key="v_label">261524</data>
    </node>
    <node id="n1635">
      <data key="v_name">342230</data>
      <data key="v_label">342230</data>
    </node>
    <node id="n1636">
      <data key="v_name">226217</data>
      <data key="v_label">226217</data>
    </node>
    <node id="n1637">
      <data key="v_name">166197</data>
      <data key="v_label">166197</data>
    </node>
    <node id="n1638">
      <data key="v_name">166198</data>
      <data key="v_label">166198</data>
    </node>
    <node id="n1639">
      <data key="v_name">536697</data>
      <data key="v_label">536697</data>
    </node>
    <node id="n1640">
      <data key="v_name">228331</data>
      <data key="v_label">228331</data>
    </node>
    <node id="n1641">
      <data key="v_name">161938</data>
      <data key="v_label">161938</data>
    </node>
    <node id="n1642">
      <data key="v_name">356253</data>
      <data key="v_label">356253</data>
    </node>
    <node id="n1643">
      <data key="v_name">214520</data>
    </node>
    <node id="n1644">
      <data key="v_name">533851</data>
      <data key="v_label">533851</data>
    </node>
    <node id="n1645">
      <data key="v_name">554329</data>
    </node>
    <node id="n1646">
      <data key="v_name">148078</data>
      <data key="v_label">148078</data>
    </node>
    <node id="n1647">
      <data key="v_name">438081</data>
      <data key="v_label">438081</data>
    </node>
    <node id="n1648">
      <data key="v_name">232487</data>
      <data key="v_label">232487</data>
    </node>
    <node id="n1649">
      <data key="v_name">429720</data>
      <data key="v_label">429720</data>
    </node>
    <node id="n1650">
      <data key="v_name">185113</data>
      <data key="v_label">185113</data>
    </node>
    <node id="n1651">
      <data key="v_name">502856</data>
      <data key="v_label">502856</data>
    </node>
    <node id="n1652">
      <data key="v_name">520062</data>
      <data key="v_label">520062</data>
    </node>
    <node id="n1653">
      <data key="v_name">544475</data>
      <data key="v_label">544475</data>
    </node>
    <node id="n1654">
      <data key="v_name">561682</data>
      <data key="v_label">561682</data>
    </node>
    <node id="n1655">
      <data key="v_name">558880</data>
      <data key="v_label">558880</data>
    </node>
    <node id="n1656">
      <data key="v_name">402689</data>
      <data key="v_label">402689</data>
    </node>
    <node id="n1657">
      <data key="v_name">450995</data>
      <data key="v_label">450995</data>
    </node>
    <node id="n1658">
      <data key="v_name">523725</data>
      <data key="v_label">523725</data>
    </node>
    <node id="n1659">
      <data key="v_name">409820</data>
      <data key="v_label">409820</data>
    </node>
    <node id="n1660">
      <data key="v_name">341681</data>
    </node>
    <node id="n1661">
      <data key="v_name">518376</data>
    </node>
    <node id="n1662">
      <data key="v_name">451080</data>
      <data key="v_label">451080</data>
    </node>
    <node id="n1663">
      <data key="v_name">236779</data>
      <data key="v_label">236779</data>
    </node>
    <node id="n1664">
      <data key="v_name">343207</data>
      <data key="v_label">343207</data>
    </node>
    <node id="n1665">
      <data key="v_name">562753</data>
      <data key="v_label">562753</data>
    </node>
    <node id="n1666">
      <data key="v_name">507792</data>
      <data key="v_label">507792</data>
    </node>
    <node id="n1667">
      <data key="v_name">386026</data>
      <data key="v_label">386026</data>
    </node>
    <node id="n1668">
      <data key="v_name">562754</data>
      <data key="v_label">562754</data>
    </node>
    <node id="n1669">
      <data key="v_name">451820</data>
    </node>
    <node id="n1670">
      <data key="v_name">152390</data>
      <data key="v_label">152390</data>
    </node>
    <node id="n1671">
      <data key="v_name">152391</data>
      <data key="v_label">152391</data>
    </node>
    <node id="n1672">
      <data key="v_name">445172</data>
    </node>
    <node id="n1673">
      <data key="v_name">445173</data>
    </node>
    <node id="n1674">
      <data key="v_name">445169</data>
      <data key="v_label">445169</data>
    </node>
    <node id="n1675">
      <data key="v_name">168075</data>
    </node>
    <node id="n1676">
      <data key="v_name">438772</data>
      <data key="v_label">438772</data>
    </node>
    <node id="n1677">
      <data key="v_name">482235</data>
      <data key="v_label">482235</data>
    </node>
    <node id="n1678">
      <data key="v_name">167115</data>
      <data key="v_label">167115</data>
    </node>
    <node id="n1679">
      <data key="v_name">465584</data>
      <data key="v_label">465584</data>
    </node>
    <node id="n1680">
      <data key="v_name">167117</data>
      <data key="v_label">167117</data>
    </node>
    <node id="n1681">
      <data key="v_name">521249</data>
      <data key="v_label">521249</data>
    </node>
    <node id="n1682">
      <data key="v_name">521182</data>
      <data key="v_label">521182</data>
    </node>
    <node id="n1683">
      <data key="v_name">257198</data>
      <data key="v_label">257198</data>
    </node>
    <node id="n1684">
      <data key="v_name">485455</data>
      <data key="v_label">485455</data>
    </node>
    <node id="n1685">
      <data key="v_name">450689</data>
      <data key="v_label">450689</data>
    </node>
    <node id="n1686">
      <data key="v_name">507180</data>
      <data key="v_label">507180</data>
    </node>
    <node id="n1687">
      <data key="v_name">527004</data>
      <data key="v_label">527004</data>
    </node>
    <node id="n1688">
      <data key="v_name">533504</data>
    </node>
    <node id="n1689">
      <data key="v_name">161474</data>
      <data key="v_label">161474</data>
    </node>
    <node id="n1690">
      <data key="v_name">493368</data>
      <data key="v_label">493368</data>
    </node>
    <node id="n1691">
      <data key="v_name">426532</data>
      <data key="v_label">426532</data>
    </node>
    <node id="n1692">
      <data key="v_name">176188</data>
      <data key="v_label">176188</data>
    </node>
    <node id="n1693">
      <data key="v_name">379546</data>
    </node>
    <node id="n1694">
      <data key="v_name">521488</data>
      <data key="v_label">521488</data>
    </node>
    <node id="n1695">
      <data key="v_name">382726</data>
      <data key="v_label">382726</data>
    </node>
    <node id="n1696">
      <data key="v_name">331779</data>
      <data key="v_label">331779</data>
    </node>
    <node id="n1697">
      <data key="v_name">229244</data>
    </node>
    <node id="n1698">
      <data key="v_name">457136</data>
    </node>
    <node id="n1699">
      <data key="v_name">258282</data>
      <data key="v_label">258282</data>
    </node>
    <node id="n1700">
      <data key="v_name">173973</data>
      <data key="v_label">173973</data>
    </node>
    <node id="n1701">
      <data key="v_name">547733</data>
      <data key="v_label">547733</data>
    </node>
    <node id="n1702">
      <data key="v_name">173975</data>
      <data key="v_label">173975</data>
    </node>
    <node id="n1703">
      <data key="v_name">342424</data>
      <data key="v_label">342424</data>
    </node>
    <node id="n1704">
      <data key="v_name">527078</data>
      <data key="v_label">527078</data>
    </node>
    <node id="n1705">
      <data key="v_name">558475</data>
    </node>
    <node id="n1706">
      <data key="v_name">358528</data>
      <data key="v_label">358528</data>
    </node>
    <node id="n1707">
      <data key="v_name">526900</data>
      <data key="v_label">526900</data>
    </node>
    <node id="n1708">
      <data key="v_name">282251</data>
      <data key="v_label">282251</data>
    </node>
    <node id="n1709">
      <data key="v_name">483588</data>
      <data key="v_label">483588</data>
    </node>
    <node id="n1710">
      <data key="v_name">158160</data>
      <data key="v_label">158160</data>
    </node>
    <node id="n1711">
      <data key="v_name">515242</data>
      <data key="v_label">515242</data>
    </node>
    <node id="n1712">
      <data key="v_name">523602</data>
      <data key="v_label">523602</data>
    </node>
    <node id="n1713">
      <data key="v_name">456506</data>
    </node>
    <node id="n1714">
      <data key="v_name">292526</data>
      <data key="v_label">292526</data>
    </node>
    <node id="n1715">
      <data key="v_name">525906</data>
      <data key="v_label">525906</data>
    </node>
    <node id="n1716">
      <data key="v_name">425870</data>
    </node>
    <node id="n1717">
      <data key="v_name">521813</data>
      <data key="v_label">521813</data>
    </node>
    <node id="n1718">
      <data key="v_name">474015</data>
      <data key="v_label">474015</data>
    </node>
    <node id="n1719">
      <data key="v_name">537156</data>
      <data key="v_label">537156</data>
    </node>
    <node id="n1720">
      <data key="v_name">555419</data>
    </node>
    <node id="n1721">
      <data key="v_name">531861</data>
    </node>
    <node id="n1722">
      <data key="v_name">522701</data>
      <data key="v_label">522701</data>
    </node>
    <node id="n1723">
      <data key="v_name">166355</data>
      <data key="v_label">166355</data>
    </node>
    <node id="n1724">
      <data key="v_name">166357</data>
      <data key="v_label">166357</data>
    </node>
    <node id="n1725">
      <data key="v_name">166358</data>
      <data key="v_label">166358</data>
    </node>
    <node id="n1726">
      <data key="v_name">166359</data>
      <data key="v_label">166359</data>
    </node>
    <node id="n1727">
      <data key="v_name">560676</data>
      <data key="v_label">560676</data>
    </node>
    <node id="n1728">
      <data key="v_name">366025</data>
      <data key="v_label">366025</data>
    </node>
    <node id="n1729">
      <data key="v_name">205700</data>
      <data key="v_label">205700</data>
    </node>
    <node id="n1730">
      <data key="v_name">229903</data>
      <data key="v_label">229903</data>
    </node>
    <node id="n1731">
      <data key="v_name">482588</data>
      <data key="v_label">482588</data>
    </node>
    <node id="n1732">
      <data key="v_name">293059</data>
      <data key="v_label">293059</data>
    </node>
    <node id="n1733">
      <data key="v_name">486565</data>
      <data key="v_label">486565</data>
    </node>
    <node id="n1734">
      <data key="v_name">410444</data>
      <data key="v_label">410444</data>
    </node>
    <node id="n1735">
      <data key="v_name">543582</data>
    </node>
    <node id="n1736">
      <data key="v_name">168075</data>
    </node>
    <node id="n1737">
      <data key="v_name">314965</data>
      <data key="v_label">314965</data>
    </node>
    <node id="n1738">
      <data key="v_name">168077</data>
      <data key="v_label">168077</data>
    </node>
    <node id="n1739">
      <data key="v_name">168078</data>
      <data key="v_label">168078</data>
    </node>
    <node id="n1740">
      <data key="v_name">448955</data>
    </node>
    <node id="n1741">
      <data key="v_name">226023</data>
    </node>
    <node id="n1742">
      <data key="v_name">518003</data>
    </node>
    <node id="n1743">
      <data key="v_name">153088</data>
    </node>
    <node id="n1744">
      <data key="v_name">402055</data>
      <data key="v_label">402055</data>
    </node>
    <node id="n1745">
      <data key="v_name">522541</data>
      <data key="v_label">522541</data>
    </node>
    <node id="n1746">
      <data key="v_name">538990</data>
      <data key="v_label">538990</data>
    </node>
    <node id="n1747">
      <data key="v_name">255973</data>
      <data key="v_label">255973</data>
    </node>
    <node id="n1748">
      <data key="v_name">228852</data>
      <data key="v_label">228852</data>
    </node>
    <node id="n1749">
      <data key="v_name">550606</data>
      <data key="v_label">550606</data>
    </node>
    <node id="n1750">
      <data key="v_name">176134</data>
    </node>
    <node id="n1751">
      <data key="v_name">560689</data>
      <data key="v_label">560689</data>
    </node>
    <node id="n1752">
      <data key="v_name">386433</data>
      <data key="v_label">386433</data>
    </node>
    <node id="n1753">
      <data key="v_name">355022</data>
    </node>
    <node id="n1754">
      <data key="v_name">461693</data>
    </node>
    <node id="n1755">
      <data key="v_name">507610</data>
      <data key="v_label">507610</data>
    </node>
    <node id="n1756">
      <data key="v_name">235820</data>
    </node>
    <node id="n1757">
      <data key="v_name">522499</data>
      <data key="v_label">522499</data>
    </node>
    <node id="n1758">
      <data key="v_name">517054</data>
    </node>
    <node id="n1759">
      <data key="v_name">507609</data>
      <data key="v_label">507609</data>
    </node>
    <node id="n1760">
      <data key="v_name">556730</data>
    </node>
    <node id="n1761">
      <data key="v_name">380937</data>
      <data key="v_label">380937</data>
    </node>
    <node id="n1762">
      <data key="v_name">563330</data>
      <data key="v_label">563330</data>
    </node>
    <node id="n1763">
      <data key="v_name">475422</data>
    </node>
    <node id="n1764">
      <data key="v_name">156449</data>
      <data key="v_label">156449</data>
    </node>
    <node id="n1765">
      <data key="v_name">398928</data>
      <data key="v_label">398928</data>
    </node>
    <node id="n1766">
      <data key="v_name">345620</data>
      <data key="v_label">345620</data>
    </node>
    <node id="n1767">
      <data key="v_name">497354</data>
      <data key="v_label">497354</data>
    </node>
    <node id="n1768">
      <data key="v_name">508076</data>
      <data key="v_label">508076</data>
    </node>
    <node id="n1769">
      <data key="v_name">165644</data>
      <data key="v_label">165644</data>
    </node>
    <node id="n1770">
      <data key="v_name">165645</data>
      <data key="v_label">165645</data>
    </node>
    <node id="n1771">
      <data key="v_name">535422</data>
      <data key="v_label">535422</data>
    </node>
    <node id="n1772">
      <data key="v_name">505885</data>
    </node>
    <node id="n1773">
      <data key="v_name">398928</data>
    </node>
    <node id="n1774">
      <data key="v_name">487472</data>
      <data key="v_label">487472</data>
    </node>
    <node id="n1775">
      <data key="v_name">214166</data>
    </node>
    <node id="n1776">
      <data key="v_name">486525</data>
      <data key="v_label">486525</data>
    </node>
    <node id="n1777">
      <data key="v_name">398928</data>
    </node>
    <node id="n1778">
      <data key="v_name">294776</data>
      <data key="v_label">294776</data>
    </node>
    <node id="n1779">
      <data key="v_name">550053</data>
      <data key="v_label">550053</data>
    </node>
    <node id="n1780">
      <data key="v_name">269393</data>
      <data key="v_label">269393</data>
    </node>
    <node id="n1781">
      <data key="v_name">527384</data>
      <data key="v_label">527384</data>
    </node>
    <node id="n1782">
      <data key="v_name">379219</data>
      <data key="v_label">379219</data>
    </node>
    <node id="n1783">
      <data key="v_name">214068</data>
      <data key="v_label">214068</data>
    </node>
    <node id="n1784">
      <data key="v_name">317018</data>
      <data key="v_label">317018</data>
    </node>
    <node id="n1785">
      <data key="v_name">398928</data>
    </node>
    <node id="n1786">
      <data key="v_name">499671</data>
      <data key="v_label">499671</data>
    </node>
    <node id="n1787">
      <data key="v_name">355022</data>
    </node>
    <node id="n1788">
      <data key="v_name">544491</data>
      <data key="v_label">544491</data>
    </node>
    <node id="n1789">
      <data key="v_name">554177</data>
      <data key="v_label">554177</data>
    </node>
    <node id="n1790">
      <data key="v_name">387239</data>
      <data key="v_label">387239</data>
    </node>
    <node id="n1791">
      <data key="v_name">210755</data>
      <data key="v_label">210755</data>
    </node>
    <node id="n1792">
      <data key="v_name">503343</data>
      <data key="v_label">503343</data>
    </node>
    <node id="n1793">
      <data key="v_name">563668</data>
      <data key="v_label">563668</data>
    </node>
    <edge source="n1" target="n2">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">CRCD: Wireless Multimedia Communications for Virtual Environments</data>
      <data key="e_abstract">0088071&lt;br/&gt;Dickerson, Julie A.&lt;br/&gt;Iowa State University&lt;br/&gt;&lt;br/&gt;CRCD: Wireless Multimedia Communications for Virtual Environments&lt;br/&gt;&lt;br/&gt;This project involves the transfer of research from several areas into an unusual curriculum that serves upper-level undergraduate and introductory-level graduate students at this university. The project integrates concepts from communications, radio frequency (RF) and very large scale integrated (VLSI) hardware and software design, with virtual environments into an interdisciplinary program that teaches students hardware/software co-design. The curriculum (that includes three new courses) brings together teams of students and faculty who create solutions to complex problems that combine human factors, real-time systems, compact wearable computers, and highly sophisticated graphics. The three new classes cover software engineering for real-time software, the design of practical wireless devices, and the design of virtual environments. The curriculum developed in this project continually incorporates advanced research topics in virtual reality, sensor design, and wireless multimedia. By providing a venue for interdisciplinary work in communications, VLSI circuit design, and immersive environments, the curriculum offers coordinated experiences in complex system co-design and significantly broadens student skills in this area.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88071</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88071</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1" target="n3">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">CRCD: Wireless Multimedia Communications for Virtual Environments</data>
      <data key="e_abstract">0088071&lt;br/&gt;Dickerson, Julie A.&lt;br/&gt;Iowa State University&lt;br/&gt;&lt;br/&gt;CRCD: Wireless Multimedia Communications for Virtual Environments&lt;br/&gt;&lt;br/&gt;This project involves the transfer of research from several areas into an unusual curriculum that serves upper-level undergraduate and introductory-level graduate students at this university. The project integrates concepts from communications, radio frequency (RF) and very large scale integrated (VLSI) hardware and software design, with virtual environments into an interdisciplinary program that teaches students hardware/software co-design. The curriculum (that includes three new courses) brings together teams of students and faculty who create solutions to complex problems that combine human factors, real-time systems, compact wearable computers, and highly sophisticated graphics. The three new classes cover software engineering for real-time software, the design of practical wireless devices, and the design of virtual environments. The curriculum developed in this project continually incorporates advanced research topics in virtual reality, sensor design, and wireless multimedia. By providing a venue for interdisciplinary work in communications, VLSI circuit design, and immersive environments, the curriculum offers coordinated experiences in complex system co-design and significantly broadens student skills in this area.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88071</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88071</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1" target="n4">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">CRCD: Wireless Multimedia Communications for Virtual Environments</data>
      <data key="e_abstract">0088071&lt;br/&gt;Dickerson, Julie A.&lt;br/&gt;Iowa State University&lt;br/&gt;&lt;br/&gt;CRCD: Wireless Multimedia Communications for Virtual Environments&lt;br/&gt;&lt;br/&gt;This project involves the transfer of research from several areas into an unusual curriculum that serves upper-level undergraduate and introductory-level graduate students at this university. The project integrates concepts from communications, radio frequency (RF) and very large scale integrated (VLSI) hardware and software design, with virtual environments into an interdisciplinary program that teaches students hardware/software co-design. The curriculum (that includes three new courses) brings together teams of students and faculty who create solutions to complex problems that combine human factors, real-time systems, compact wearable computers, and highly sophisticated graphics. The three new classes cover software engineering for real-time software, the design of practical wireless devices, and the design of virtual environments. The curriculum developed in this project continually incorporates advanced research topics in virtual reality, sensor design, and wireless multimedia. By providing a venue for interdisciplinary work in communications, VLSI circuit design, and immersive environments, the curriculum offers coordinated experiences in complex system co-design and significantly broadens student skills in this area.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88071</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88071</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1" target="n5">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">CRCD: Wireless Multimedia Communications for Virtual Environments</data>
      <data key="e_abstract">0088071&lt;br/&gt;Dickerson, Julie A.&lt;br/&gt;Iowa State University&lt;br/&gt;&lt;br/&gt;CRCD: Wireless Multimedia Communications for Virtual Environments&lt;br/&gt;&lt;br/&gt;This project involves the transfer of research from several areas into an unusual curriculum that serves upper-level undergraduate and introductory-level graduate students at this university. The project integrates concepts from communications, radio frequency (RF) and very large scale integrated (VLSI) hardware and software design, with virtual environments into an interdisciplinary program that teaches students hardware/software co-design. The curriculum (that includes three new courses) brings together teams of students and faculty who create solutions to complex problems that combine human factors, real-time systems, compact wearable computers, and highly sophisticated graphics. The three new classes cover software engineering for real-time software, the design of practical wireless devices, and the design of virtual environments. The curriculum developed in this project continually incorporates advanced research topics in virtual reality, sensor design, and wireless multimedia. By providing a venue for interdisciplinary work in communications, VLSI circuit design, and immersive environments, the curriculum offers coordinated experiences in complex system co-design and significantly broadens student skills in this area.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88071</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88071</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n2" target="n3">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">CRCD: Wireless Multimedia Communications for Virtual Environments</data>
      <data key="e_abstract">0088071&lt;br/&gt;Dickerson, Julie A.&lt;br/&gt;Iowa State University&lt;br/&gt;&lt;br/&gt;CRCD: Wireless Multimedia Communications for Virtual Environments&lt;br/&gt;&lt;br/&gt;This project involves the transfer of research from several areas into an unusual curriculum that serves upper-level undergraduate and introductory-level graduate students at this university. The project integrates concepts from communications, radio frequency (RF) and very large scale integrated (VLSI) hardware and software design, with virtual environments into an interdisciplinary program that teaches students hardware/software co-design. The curriculum (that includes three new courses) brings together teams of students and faculty who create solutions to complex problems that combine human factors, real-time systems, compact wearable computers, and highly sophisticated graphics. The three new classes cover software engineering for real-time software, the design of practical wireless devices, and the design of virtual environments. The curriculum developed in this project continually incorporates advanced research topics in virtual reality, sensor design, and wireless multimedia. By providing a venue for interdisciplinary work in communications, VLSI circuit design, and immersive environments, the curriculum offers coordinated experiences in complex system co-design and significantly broadens student skills in this area.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88071</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88071</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n2" target="n4">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">CRCD: Wireless Multimedia Communications for Virtual Environments</data>
      <data key="e_abstract">0088071&lt;br/&gt;Dickerson, Julie A.&lt;br/&gt;Iowa State University&lt;br/&gt;&lt;br/&gt;CRCD: Wireless Multimedia Communications for Virtual Environments&lt;br/&gt;&lt;br/&gt;This project involves the transfer of research from several areas into an unusual curriculum that serves upper-level undergraduate and introductory-level graduate students at this university. The project integrates concepts from communications, radio frequency (RF) and very large scale integrated (VLSI) hardware and software design, with virtual environments into an interdisciplinary program that teaches students hardware/software co-design. The curriculum (that includes three new courses) brings together teams of students and faculty who create solutions to complex problems that combine human factors, real-time systems, compact wearable computers, and highly sophisticated graphics. The three new classes cover software engineering for real-time software, the design of practical wireless devices, and the design of virtual environments. The curriculum developed in this project continually incorporates advanced research topics in virtual reality, sensor design, and wireless multimedia. By providing a venue for interdisciplinary work in communications, VLSI circuit design, and immersive environments, the curriculum offers coordinated experiences in complex system co-design and significantly broadens student skills in this area.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88071</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88071</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n2" target="n5">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">CRCD: Wireless Multimedia Communications for Virtual Environments</data>
      <data key="e_abstract">0088071&lt;br/&gt;Dickerson, Julie A.&lt;br/&gt;Iowa State University&lt;br/&gt;&lt;br/&gt;CRCD: Wireless Multimedia Communications for Virtual Environments&lt;br/&gt;&lt;br/&gt;This project involves the transfer of research from several areas into an unusual curriculum that serves upper-level undergraduate and introductory-level graduate students at this university. The project integrates concepts from communications, radio frequency (RF) and very large scale integrated (VLSI) hardware and software design, with virtual environments into an interdisciplinary program that teaches students hardware/software co-design. The curriculum (that includes three new courses) brings together teams of students and faculty who create solutions to complex problems that combine human factors, real-time systems, compact wearable computers, and highly sophisticated graphics. The three new classes cover software engineering for real-time software, the design of practical wireless devices, and the design of virtual environments. The curriculum developed in this project continually incorporates advanced research topics in virtual reality, sensor design, and wireless multimedia. By providing a venue for interdisciplinary work in communications, VLSI circuit design, and immersive environments, the curriculum offers coordinated experiences in complex system co-design and significantly broadens student skills in this area.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88071</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88071</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n3" target="n4">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">CRCD: Wireless Multimedia Communications for Virtual Environments</data>
      <data key="e_abstract">0088071&lt;br/&gt;Dickerson, Julie A.&lt;br/&gt;Iowa State University&lt;br/&gt;&lt;br/&gt;CRCD: Wireless Multimedia Communications for Virtual Environments&lt;br/&gt;&lt;br/&gt;This project involves the transfer of research from several areas into an unusual curriculum that serves upper-level undergraduate and introductory-level graduate students at this university. The project integrates concepts from communications, radio frequency (RF) and very large scale integrated (VLSI) hardware and software design, with virtual environments into an interdisciplinary program that teaches students hardware/software co-design. The curriculum (that includes three new courses) brings together teams of students and faculty who create solutions to complex problems that combine human factors, real-time systems, compact wearable computers, and highly sophisticated graphics. The three new classes cover software engineering for real-time software, the design of practical wireless devices, and the design of virtual environments. The curriculum developed in this project continually incorporates advanced research topics in virtual reality, sensor design, and wireless multimedia. By providing a venue for interdisciplinary work in communications, VLSI circuit design, and immersive environments, the curriculum offers coordinated experiences in complex system co-design and significantly broadens student skills in this area.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88071</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88071</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n3" target="n5">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">CRCD: Wireless Multimedia Communications for Virtual Environments</data>
      <data key="e_abstract">0088071&lt;br/&gt;Dickerson, Julie A.&lt;br/&gt;Iowa State University&lt;br/&gt;&lt;br/&gt;CRCD: Wireless Multimedia Communications for Virtual Environments&lt;br/&gt;&lt;br/&gt;This project involves the transfer of research from several areas into an unusual curriculum that serves upper-level undergraduate and introductory-level graduate students at this university. The project integrates concepts from communications, radio frequency (RF) and very large scale integrated (VLSI) hardware and software design, with virtual environments into an interdisciplinary program that teaches students hardware/software co-design. The curriculum (that includes three new courses) brings together teams of students and faculty who create solutions to complex problems that combine human factors, real-time systems, compact wearable computers, and highly sophisticated graphics. The three new classes cover software engineering for real-time software, the design of practical wireless devices, and the design of virtual environments. The curriculum developed in this project continually incorporates advanced research topics in virtual reality, sensor design, and wireless multimedia. By providing a venue for interdisciplinary work in communications, VLSI circuit design, and immersive environments, the curriculum offers coordinated experiences in complex system co-design and significantly broadens student skills in this area.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88071</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88071</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n4" target="n5">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">CRCD: Wireless Multimedia Communications for Virtual Environments</data>
      <data key="e_abstract">0088071&lt;br/&gt;Dickerson, Julie A.&lt;br/&gt;Iowa State University&lt;br/&gt;&lt;br/&gt;CRCD: Wireless Multimedia Communications for Virtual Environments&lt;br/&gt;&lt;br/&gt;This project involves the transfer of research from several areas into an unusual curriculum that serves upper-level undergraduate and introductory-level graduate students at this university. The project integrates concepts from communications, radio frequency (RF) and very large scale integrated (VLSI) hardware and software design, with virtual environments into an interdisciplinary program that teaches students hardware/software co-design. The curriculum (that includes three new courses) brings together teams of students and faculty who create solutions to complex problems that combine human factors, real-time systems, compact wearable computers, and highly sophisticated graphics. The three new classes cover software engineering for real-time software, the design of practical wireless devices, and the design of virtual environments. The curriculum developed in this project continually incorporates advanced research topics in virtual reality, sensor design, and wireless multimedia. By providing a venue for interdisciplinary work in communications, VLSI circuit design, and immersive environments, the curriculum offers coordinated experiences in complex system co-design and significantly broadens student skills in this area.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88071</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88071</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n6" target="n7">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">ITW: Out of the Loop: Why Are So Few Underrepresented Minority High School Students Learning Computer Science?</data>
      <data key="e_abstract">Institution: University of California-Los Angeles&lt;br/&gt;Proposal Number: EIA 0090043&lt;br/&gt;PI: Jane Margolis&lt;br/&gt;Title: Out of the Loop: Why Are So Few Underrepresented Minority High School Students Learning Computer Science&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to study why so few male and female African American and Hispanic students are studying computer science at the high school level. Specifically, the study will examine the decisions of college-bound African American and Hispanic students to take (or not take), persist (or not persist) in computer science courses beyond the introductory level. The research model considers the interplay between the high school educational environment and the psychological and cultural factors that affect male and female underrepresented minority students&apos; interest in learning computer science. The sites of the investigation are schools within the Los Angeles Unified School District. This project has the potential to provide valuable insights about how to attract underrepresented minorities to IT classes in high school and college.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">90043</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90043</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n8" target="n9">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">Performance Modeling and Analysis of Real-Time Distributed Systems</data>
      <data key="e_abstract">The objective of this research is to investigate formal modeling and &lt;br/&gt;analysis methods that are applicable to large-scale real-time distributed&lt;br/&gt;systems, such as networked virtual environments. The research focuses on&lt;br/&gt;the development of techniques and tools for performance analysis of&lt;br/&gt;network protocols and Quality-of-Service requirements analysis of the&lt;br/&gt;network connecting distributed real-time applications. &lt;br/&gt;It is proposed to work on the following tasks:&lt;br/&gt;1) Develop (extended fuzzy-timing Petri net) models for protocols in each&lt;br/&gt;layer of network architecture;&lt;br/&gt;2) Conduct network performance analysis and evaluate network effects on&lt;br/&gt;several distributed real-time applications;&lt;br/&gt;3) Focus on the development of new techniques to deal with the state &lt;br/&gt;explosion problem; and&lt;br/&gt;4) Maintain an open library of models for real-time distributed&lt;br/&gt;applications.&lt;br/&gt;The significance and impact of the research are: &lt;br/&gt;1) Provide an integrated framework to conduct performance analysis for&lt;br/&gt;large-scale real-time distributed systems with high efficiency; &lt;br/&gt;2) Help designers of distributed real-time systems in testing, planning, &lt;br/&gt;and improving their work; and&lt;br/&gt;3) Help students learn formal modeling and analysis methods for &lt;br/&gt;network-based software design via graphic interface and simulation.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">9.98833e+06</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98833e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n19" target="n20">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">High-Performance Switch Architectures for CC-NUMA Servers</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">4715</data>
      <data key="e_label">196102</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">196102</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n22" target="n23">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">Mobility Tolerant Adaptive Multicast Protocols for Ad Hoc Networks</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">4097</data>
      <data key="e_label">196156</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">196156</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n19" target="n20">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">Cache Coherence in Wormhole Networks</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">4715</data>
      <data key="e_label">196074</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">196074</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n19" target="n30">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">Cache Coherence in Wormhole Networks</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">4715</data>
      <data key="e_label">196074</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">196074</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n20" target="n30">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">Cache Coherence in Wormhole Networks</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">4715</data>
      <data key="e_label">196074</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">196074</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n31" target="n32">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">Replication of Heterogeneous Multimedia Data</data>
      <data key="e_abstract">This work develops a framework for the replication of heterogeneous multimedia data for achieving high availability while maintaining low access cost. The framework encompasses various aspects of replication in distributed systems: users and policy, networks, and servers. The research develops the technical concept of quality quorum systems to enable efficient and robust multimedia replication. The results of the research is brought together in a Heterogeneous Multimedia Object Replication (HUMOR) system. The work has a significant impact on distributed server architecture design, multimedia databases, digital libraries, and distant learning environments.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">9.9884e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.9884e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n33" target="n34">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">Educational Innovation: Integrating Intelligent Agent and Wireless Computing Research into the Undergraduate Curriculum</data>
      <data key="e_abstract">EIA- 0086260&lt;br/&gt;Cook, Diane J.&lt;br/&gt;University of Texas at Arlington&lt;br/&gt;&lt;br/&gt;CISE Educational Innovation: Integrating Intelligent Agent and Wireless Computing Research into the Undergraduate Curriculum&lt;br/&gt;&lt;br/&gt;The research-base for this project is Artificial Intelligence (AI), in particular, rational agent development. The project provides undergraduate students with access to a large-scale distributed mobile agent laboratory containing both real and simulated agents. The focus of the project is new curriculum material for AI, mobile computing, multimedia, and robotics courses that allow students to test their knowledge in a real and virtual environment as well as new courses in human-computer interaction and wireless-multimedia computing. The PIs have demonstrated successful research and development of AI simulators that provide students with environments for testing agent design ideas in decision making, multi-agent cooperation, and learning. The PIs expanded agent environment includes real-world tasks involving distributed decision-making, cooperation with both human and computer agents, and wireless communication. The project increases students&apos; interest and expertise in this area through hands-on experiences with physical and simulated collaborative environments. In particular, the project uses a wireless communication system, called Wireless Intelligent Simulator Environment (WISE), in this institution&apos;s Computer Science area that permits human, software, and robot agents to interact over a distributed environment. The WISE environment, in addition to supporting new features of this university&apos;s existing courses (in AI, Mobile Networking and Computing, Multimedia, Robotics, and Senior Capstone Design Courses), supports new courses in Human-Computer Interaction and Wireless Multimedia Computing.</data>
      <data key="e_pgm">4725</data>
      <data key="e_label">86260</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86260</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n33" target="n35">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">Educational Innovation: Integrating Intelligent Agent and Wireless Computing Research into the Undergraduate Curriculum</data>
      <data key="e_abstract">EIA- 0086260&lt;br/&gt;Cook, Diane J.&lt;br/&gt;University of Texas at Arlington&lt;br/&gt;&lt;br/&gt;CISE Educational Innovation: Integrating Intelligent Agent and Wireless Computing Research into the Undergraduate Curriculum&lt;br/&gt;&lt;br/&gt;The research-base for this project is Artificial Intelligence (AI), in particular, rational agent development. The project provides undergraduate students with access to a large-scale distributed mobile agent laboratory containing both real and simulated agents. The focus of the project is new curriculum material for AI, mobile computing, multimedia, and robotics courses that allow students to test their knowledge in a real and virtual environment as well as new courses in human-computer interaction and wireless-multimedia computing. The PIs have demonstrated successful research and development of AI simulators that provide students with environments for testing agent design ideas in decision making, multi-agent cooperation, and learning. The PIs expanded agent environment includes real-world tasks involving distributed decision-making, cooperation with both human and computer agents, and wireless communication. The project increases students&apos; interest and expertise in this area through hands-on experiences with physical and simulated collaborative environments. In particular, the project uses a wireless communication system, called Wireless Intelligent Simulator Environment (WISE), in this institution&apos;s Computer Science area that permits human, software, and robot agents to interact over a distributed environment. The WISE environment, in addition to supporting new features of this university&apos;s existing courses (in AI, Mobile Networking and Computing, Multimedia, Robotics, and Senior Capstone Design Courses), supports new courses in Human-Computer Interaction and Wireless Multimedia Computing.</data>
      <data key="e_pgm">4725</data>
      <data key="e_label">86260</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86260</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n33" target="n36">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">Educational Innovation: Integrating Intelligent Agent and Wireless Computing Research into the Undergraduate Curriculum</data>
      <data key="e_abstract">EIA- 0086260&lt;br/&gt;Cook, Diane J.&lt;br/&gt;University of Texas at Arlington&lt;br/&gt;&lt;br/&gt;CISE Educational Innovation: Integrating Intelligent Agent and Wireless Computing Research into the Undergraduate Curriculum&lt;br/&gt;&lt;br/&gt;The research-base for this project is Artificial Intelligence (AI), in particular, rational agent development. The project provides undergraduate students with access to a large-scale distributed mobile agent laboratory containing both real and simulated agents. The focus of the project is new curriculum material for AI, mobile computing, multimedia, and robotics courses that allow students to test their knowledge in a real and virtual environment as well as new courses in human-computer interaction and wireless-multimedia computing. The PIs have demonstrated successful research and development of AI simulators that provide students with environments for testing agent design ideas in decision making, multi-agent cooperation, and learning. The PIs expanded agent environment includes real-world tasks involving distributed decision-making, cooperation with both human and computer agents, and wireless communication. The project increases students&apos; interest and expertise in this area through hands-on experiences with physical and simulated collaborative environments. In particular, the project uses a wireless communication system, called Wireless Intelligent Simulator Environment (WISE), in this institution&apos;s Computer Science area that permits human, software, and robot agents to interact over a distributed environment. The WISE environment, in addition to supporting new features of this university&apos;s existing courses (in AI, Mobile Networking and Computing, Multimedia, Robotics, and Senior Capstone Design Courses), supports new courses in Human-Computer Interaction and Wireless Multimedia Computing.</data>
      <data key="e_pgm">4725</data>
      <data key="e_label">86260</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86260</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n33" target="n37">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">Educational Innovation: Integrating Intelligent Agent and Wireless Computing Research into the Undergraduate Curriculum</data>
      <data key="e_abstract">EIA- 0086260&lt;br/&gt;Cook, Diane J.&lt;br/&gt;University of Texas at Arlington&lt;br/&gt;&lt;br/&gt;CISE Educational Innovation: Integrating Intelligent Agent and Wireless Computing Research into the Undergraduate Curriculum&lt;br/&gt;&lt;br/&gt;The research-base for this project is Artificial Intelligence (AI), in particular, rational agent development. The project provides undergraduate students with access to a large-scale distributed mobile agent laboratory containing both real and simulated agents. The focus of the project is new curriculum material for AI, mobile computing, multimedia, and robotics courses that allow students to test their knowledge in a real and virtual environment as well as new courses in human-computer interaction and wireless-multimedia computing. The PIs have demonstrated successful research and development of AI simulators that provide students with environments for testing agent design ideas in decision making, multi-agent cooperation, and learning. The PIs expanded agent environment includes real-world tasks involving distributed decision-making, cooperation with both human and computer agents, and wireless communication. The project increases students&apos; interest and expertise in this area through hands-on experiences with physical and simulated collaborative environments. In particular, the project uses a wireless communication system, called Wireless Intelligent Simulator Environment (WISE), in this institution&apos;s Computer Science area that permits human, software, and robot agents to interact over a distributed environment. The WISE environment, in addition to supporting new features of this university&apos;s existing courses (in AI, Mobile Networking and Computing, Multimedia, Robotics, and Senior Capstone Design Courses), supports new courses in Human-Computer Interaction and Wireless Multimedia Computing.</data>
      <data key="e_pgm">4725</data>
      <data key="e_label">86260</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86260</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n34" target="n35">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">Educational Innovation: Integrating Intelligent Agent and Wireless Computing Research into the Undergraduate Curriculum</data>
      <data key="e_abstract">EIA- 0086260&lt;br/&gt;Cook, Diane J.&lt;br/&gt;University of Texas at Arlington&lt;br/&gt;&lt;br/&gt;CISE Educational Innovation: Integrating Intelligent Agent and Wireless Computing Research into the Undergraduate Curriculum&lt;br/&gt;&lt;br/&gt;The research-base for this project is Artificial Intelligence (AI), in particular, rational agent development. The project provides undergraduate students with access to a large-scale distributed mobile agent laboratory containing both real and simulated agents. The focus of the project is new curriculum material for AI, mobile computing, multimedia, and robotics courses that allow students to test their knowledge in a real and virtual environment as well as new courses in human-computer interaction and wireless-multimedia computing. The PIs have demonstrated successful research and development of AI simulators that provide students with environments for testing agent design ideas in decision making, multi-agent cooperation, and learning. The PIs expanded agent environment includes real-world tasks involving distributed decision-making, cooperation with both human and computer agents, and wireless communication. The project increases students&apos; interest and expertise in this area through hands-on experiences with physical and simulated collaborative environments. In particular, the project uses a wireless communication system, called Wireless Intelligent Simulator Environment (WISE), in this institution&apos;s Computer Science area that permits human, software, and robot agents to interact over a distributed environment. The WISE environment, in addition to supporting new features of this university&apos;s existing courses (in AI, Mobile Networking and Computing, Multimedia, Robotics, and Senior Capstone Design Courses), supports new courses in Human-Computer Interaction and Wireless Multimedia Computing.</data>
      <data key="e_pgm">4725</data>
      <data key="e_label">86260</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86260</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n34" target="n36">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">Educational Innovation: Integrating Intelligent Agent and Wireless Computing Research into the Undergraduate Curriculum</data>
      <data key="e_abstract">EIA- 0086260&lt;br/&gt;Cook, Diane J.&lt;br/&gt;University of Texas at Arlington&lt;br/&gt;&lt;br/&gt;CISE Educational Innovation: Integrating Intelligent Agent and Wireless Computing Research into the Undergraduate Curriculum&lt;br/&gt;&lt;br/&gt;The research-base for this project is Artificial Intelligence (AI), in particular, rational agent development. The project provides undergraduate students with access to a large-scale distributed mobile agent laboratory containing both real and simulated agents. The focus of the project is new curriculum material for AI, mobile computing, multimedia, and robotics courses that allow students to test their knowledge in a real and virtual environment as well as new courses in human-computer interaction and wireless-multimedia computing. The PIs have demonstrated successful research and development of AI simulators that provide students with environments for testing agent design ideas in decision making, multi-agent cooperation, and learning. The PIs expanded agent environment includes real-world tasks involving distributed decision-making, cooperation with both human and computer agents, and wireless communication. The project increases students&apos; interest and expertise in this area through hands-on experiences with physical and simulated collaborative environments. In particular, the project uses a wireless communication system, called Wireless Intelligent Simulator Environment (WISE), in this institution&apos;s Computer Science area that permits human, software, and robot agents to interact over a distributed environment. The WISE environment, in addition to supporting new features of this university&apos;s existing courses (in AI, Mobile Networking and Computing, Multimedia, Robotics, and Senior Capstone Design Courses), supports new courses in Human-Computer Interaction and Wireless Multimedia Computing.</data>
      <data key="e_pgm">4725</data>
      <data key="e_label">86260</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86260</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n34" target="n37">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">Educational Innovation: Integrating Intelligent Agent and Wireless Computing Research into the Undergraduate Curriculum</data>
      <data key="e_abstract">EIA- 0086260&lt;br/&gt;Cook, Diane J.&lt;br/&gt;University of Texas at Arlington&lt;br/&gt;&lt;br/&gt;CISE Educational Innovation: Integrating Intelligent Agent and Wireless Computing Research into the Undergraduate Curriculum&lt;br/&gt;&lt;br/&gt;The research-base for this project is Artificial Intelligence (AI), in particular, rational agent development. The project provides undergraduate students with access to a large-scale distributed mobile agent laboratory containing both real and simulated agents. The focus of the project is new curriculum material for AI, mobile computing, multimedia, and robotics courses that allow students to test their knowledge in a real and virtual environment as well as new courses in human-computer interaction and wireless-multimedia computing. The PIs have demonstrated successful research and development of AI simulators that provide students with environments for testing agent design ideas in decision making, multi-agent cooperation, and learning. The PIs expanded agent environment includes real-world tasks involving distributed decision-making, cooperation with both human and computer agents, and wireless communication. The project increases students&apos; interest and expertise in this area through hands-on experiences with physical and simulated collaborative environments. In particular, the project uses a wireless communication system, called Wireless Intelligent Simulator Environment (WISE), in this institution&apos;s Computer Science area that permits human, software, and robot agents to interact over a distributed environment. The WISE environment, in addition to supporting new features of this university&apos;s existing courses (in AI, Mobile Networking and Computing, Multimedia, Robotics, and Senior Capstone Design Courses), supports new courses in Human-Computer Interaction and Wireless Multimedia Computing.</data>
      <data key="e_pgm">4725</data>
      <data key="e_label">86260</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86260</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n35" target="n36">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">Educational Innovation: Integrating Intelligent Agent and Wireless Computing Research into the Undergraduate Curriculum</data>
      <data key="e_abstract">EIA- 0086260&lt;br/&gt;Cook, Diane J.&lt;br/&gt;University of Texas at Arlington&lt;br/&gt;&lt;br/&gt;CISE Educational Innovation: Integrating Intelligent Agent and Wireless Computing Research into the Undergraduate Curriculum&lt;br/&gt;&lt;br/&gt;The research-base for this project is Artificial Intelligence (AI), in particular, rational agent development. The project provides undergraduate students with access to a large-scale distributed mobile agent laboratory containing both real and simulated agents. The focus of the project is new curriculum material for AI, mobile computing, multimedia, and robotics courses that allow students to test their knowledge in a real and virtual environment as well as new courses in human-computer interaction and wireless-multimedia computing. The PIs have demonstrated successful research and development of AI simulators that provide students with environments for testing agent design ideas in decision making, multi-agent cooperation, and learning. The PIs expanded agent environment includes real-world tasks involving distributed decision-making, cooperation with both human and computer agents, and wireless communication. The project increases students&apos; interest and expertise in this area through hands-on experiences with physical and simulated collaborative environments. In particular, the project uses a wireless communication system, called Wireless Intelligent Simulator Environment (WISE), in this institution&apos;s Computer Science area that permits human, software, and robot agents to interact over a distributed environment. The WISE environment, in addition to supporting new features of this university&apos;s existing courses (in AI, Mobile Networking and Computing, Multimedia, Robotics, and Senior Capstone Design Courses), supports new courses in Human-Computer Interaction and Wireless Multimedia Computing.</data>
      <data key="e_pgm">4725</data>
      <data key="e_label">86260</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86260</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n35" target="n37">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">Educational Innovation: Integrating Intelligent Agent and Wireless Computing Research into the Undergraduate Curriculum</data>
      <data key="e_abstract">EIA- 0086260&lt;br/&gt;Cook, Diane J.&lt;br/&gt;University of Texas at Arlington&lt;br/&gt;&lt;br/&gt;CISE Educational Innovation: Integrating Intelligent Agent and Wireless Computing Research into the Undergraduate Curriculum&lt;br/&gt;&lt;br/&gt;The research-base for this project is Artificial Intelligence (AI), in particular, rational agent development. The project provides undergraduate students with access to a large-scale distributed mobile agent laboratory containing both real and simulated agents. The focus of the project is new curriculum material for AI, mobile computing, multimedia, and robotics courses that allow students to test their knowledge in a real and virtual environment as well as new courses in human-computer interaction and wireless-multimedia computing. The PIs have demonstrated successful research and development of AI simulators that provide students with environments for testing agent design ideas in decision making, multi-agent cooperation, and learning. The PIs expanded agent environment includes real-world tasks involving distributed decision-making, cooperation with both human and computer agents, and wireless communication. The project increases students&apos; interest and expertise in this area through hands-on experiences with physical and simulated collaborative environments. In particular, the project uses a wireless communication system, called Wireless Intelligent Simulator Environment (WISE), in this institution&apos;s Computer Science area that permits human, software, and robot agents to interact over a distributed environment. The WISE environment, in addition to supporting new features of this university&apos;s existing courses (in AI, Mobile Networking and Computing, Multimedia, Robotics, and Senior Capstone Design Courses), supports new courses in Human-Computer Interaction and Wireless Multimedia Computing.</data>
      <data key="e_pgm">4725</data>
      <data key="e_label">86260</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86260</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n36" target="n37">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">Educational Innovation: Integrating Intelligent Agent and Wireless Computing Research into the Undergraduate Curriculum</data>
      <data key="e_abstract">EIA- 0086260&lt;br/&gt;Cook, Diane J.&lt;br/&gt;University of Texas at Arlington&lt;br/&gt;&lt;br/&gt;CISE Educational Innovation: Integrating Intelligent Agent and Wireless Computing Research into the Undergraduate Curriculum&lt;br/&gt;&lt;br/&gt;The research-base for this project is Artificial Intelligence (AI), in particular, rational agent development. The project provides undergraduate students with access to a large-scale distributed mobile agent laboratory containing both real and simulated agents. The focus of the project is new curriculum material for AI, mobile computing, multimedia, and robotics courses that allow students to test their knowledge in a real and virtual environment as well as new courses in human-computer interaction and wireless-multimedia computing. The PIs have demonstrated successful research and development of AI simulators that provide students with environments for testing agent design ideas in decision making, multi-agent cooperation, and learning. The PIs expanded agent environment includes real-world tasks involving distributed decision-making, cooperation with both human and computer agents, and wireless communication. The project increases students&apos; interest and expertise in this area through hands-on experiences with physical and simulated collaborative environments. In particular, the project uses a wireless communication system, called Wireless Intelligent Simulator Environment (WISE), in this institution&apos;s Computer Science area that permits human, software, and robot agents to interact over a distributed environment. The WISE environment, in addition to supporting new features of this university&apos;s existing courses (in AI, Mobile Networking and Computing, Multimedia, Robotics, and Senior Capstone Design Courses), supports new courses in Human-Computer Interaction and Wireless Multimedia Computing.</data>
      <data key="e_pgm">4725</data>
      <data key="e_label">86260</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86260</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n38" target="n39">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">WORKSHOP: Human-Computer Interaction Doctoral Research Consortium</data>
      <data key="e_abstract">This is funding in support of a doctoral research consortium (workshop) of promising graduate students and distinguished research faculty. The consortium will be held in conjunction with the ACM 2001 Conference on Human Factors in Computing Systems (CHI 2001), sponsored by the Association for Computing Machinery&apos;s Special Interest Group on Human Computer Interaction (SIGCHI). The goals of the workshop include building a cohort group of new researchers who will then have a network of colleagues spread out across the world, guiding the work of new researchers by having experts in the research field give them advice, and making it possible for promising new entrants to the field to attend their research conference. Student participants will make formal presentations of their work during the workshop, and will receive feedback from the faculty panel. The feedback is geared to helping students understand and articulate how their work is positioned relative to other human-computer interaction research, whether their topics are adequately focused for thesis research projects, whether their methods are correctly chosen and applied, and whether their results are appropriately analyzed and presented. Student participants will also present their work during the technical program of the CHI 2001 conference. Extended abstracts of the students&apos; work will be disseminated via publication in the CHI 2001 Extended Abstracts, which has wide print and electronic distribution. Evaluation of the consortium will be conducted by ACM SIGCHI&apos;s conference management committee, and results of the evaluation will be available to the organizers of future consortia.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">101295</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">101295</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n38" target="n40">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">WORKSHOP: Human-Computer Interaction Doctoral Research Consortium</data>
      <data key="e_abstract">This is funding in support of a doctoral research consortium (workshop) of promising graduate students and distinguished research faculty. The consortium will be held in conjunction with the ACM 2001 Conference on Human Factors in Computing Systems (CHI 2001), sponsored by the Association for Computing Machinery&apos;s Special Interest Group on Human Computer Interaction (SIGCHI). The goals of the workshop include building a cohort group of new researchers who will then have a network of colleagues spread out across the world, guiding the work of new researchers by having experts in the research field give them advice, and making it possible for promising new entrants to the field to attend their research conference. Student participants will make formal presentations of their work during the workshop, and will receive feedback from the faculty panel. The feedback is geared to helping students understand and articulate how their work is positioned relative to other human-computer interaction research, whether their topics are adequately focused for thesis research projects, whether their methods are correctly chosen and applied, and whether their results are appropriately analyzed and presented. Student participants will also present their work during the technical program of the CHI 2001 conference. Extended abstracts of the students&apos; work will be disseminated via publication in the CHI 2001 Extended Abstracts, which has wide print and electronic distribution. Evaluation of the consortium will be conducted by ACM SIGCHI&apos;s conference management committee, and results of the evaluation will be available to the organizers of future consortia.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">101295</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">101295</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n39" target="n40">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">WORKSHOP: Human-Computer Interaction Doctoral Research Consortium</data>
      <data key="e_abstract">This is funding in support of a doctoral research consortium (workshop) of promising graduate students and distinguished research faculty. The consortium will be held in conjunction with the ACM 2001 Conference on Human Factors in Computing Systems (CHI 2001), sponsored by the Association for Computing Machinery&apos;s Special Interest Group on Human Computer Interaction (SIGCHI). The goals of the workshop include building a cohort group of new researchers who will then have a network of colleagues spread out across the world, guiding the work of new researchers by having experts in the research field give them advice, and making it possible for promising new entrants to the field to attend their research conference. Student participants will make formal presentations of their work during the workshop, and will receive feedback from the faculty panel. The feedback is geared to helping students understand and articulate how their work is positioned relative to other human-computer interaction research, whether their topics are adequately focused for thesis research projects, whether their methods are correctly chosen and applied, and whether their results are appropriately analyzed and presented. Student participants will also present their work during the technical program of the CHI 2001 conference. Extended abstracts of the students&apos; work will be disseminated via publication in the CHI 2001 Extended Abstracts, which has wide print and electronic distribution. Evaluation of the consortium will be conducted by ACM SIGCHI&apos;s conference management committee, and results of the evaluation will be available to the organizers of future consortia.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">101295</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">101295</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n42" target="n43">
      <data key="e_effectiveDate">2001-01-01</data>
      <data key="e_title">Differentiated Reliability (DiR) in Multi-layer Optical Networks</data>
      <data key="e_abstract">Current optical networks typically offer two degrees of service reliability: full (100%) protection (e.g., in presence of a single fault in the network) and no (0%) protection. This reflects the historical duality that has its roots in the once divided telephone and data environments, in which the circuit oriented service required protection, i.e., provisioning readily available spare resources to replace working resources in case of fault, while the datagram oriented service relied upon restoration, i.e., on dynamic search for and reallocation of affected resources via such actions as routing table updates. &lt;br/&gt; The current trend, however, is gradually driving the design of networks towards a unified solution that will support, together with the traditional voice and data services, a variety of novel multimedia applications. Evidence of this trend over the last decade is the growing importance of concepts, such Quality of Service (QoS) and Differentiated Services to provide varying levels of service performance in the same network. According to the fact that today&apos;s competitive networks can no longer provide only pure voice and datagram services, the historical duality between fully protected and unprotected (100% and 0% reliability in case of a single fault) is rapidly becoming obsolete. Modern networks can no longer limit the options of reliability only to these two extreme degrees. On the other hand, while much work is being done on QoS and Differentiated Services, surprisingly little has been discussed about and proposed for developing differentiated network reliability to accommodate this change in the way networks are designed.&lt;br/&gt; In this proposal, the researchers proposed to address the problem of designing cost effective multi-layer network architectures that are capable of providing various reliability degrees (as opposed to 0% and 100% only) as required by the applications. In the proposal, the concept of Differentiated Reliability (DiR) is for the first time formally introduced and applied to provide multiple reliability degrees (classes) in the same layer using a common protection mechanism, e.g., line switching or path switching. According to the DiR concept, each connection in the layer under consideration is assigned a minimum reliability degree, defined as the probability that the connection is available at any given time. The overall reliability degree chosen for a given connection is determined by the application requirements. In a multi-layer network, the lower layer can thus provide the above layers with the desired reliability degree, transparently from the actual network topology, constraints, device technology, etc. The cost of the connection depends on the chosen reliability degree, with a variety of options offered by DiR.&lt;br/&gt; The proposed research explores the multifaceted aspects of DiR-based design of multi-layer optical networks, with specific emphasis on the IP/WDM architecture. Optimally designing a DiR network is in general extremely complex and will require special techniques tailored to handle it with acceptable computational time. Therefore, along with research on the architecture and modeling of DiR-based networks the researchers propose a powerful novel discrete optimization paradigm to efficiently handle the difficult tasks. The optimization approach is based on adopting and adjusting the Fourier Transform technique for binary domains. This unique technique makes it possible to realize an efficient &quot;filtering&quot; of the complex design/optimization problem, such that the solution becomes computationally feasible, while still preserving sufficient accuracy.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">82085</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82085</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n50" target="n51">
      <data key="e_effectiveDate">2001-02-15</data>
      <data key="e_title">Distance Education Delivery for Isolated Rural Communities: A Contingency Approach</data>
      <data key="e_abstract">0090474 &lt;br/&gt;Scott&lt;br/&gt;&lt;br/&gt;This award is to Ilisagvik College to support the activity described below for 36 months. The proposal was submitted in response to the Partnerships for Innovation Program Solicitation (NSF 0082).&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Partners&lt;br/&gt;The partners for this award include California Virtual Campus, Bay Area Region 1; Lewis-Clark State College; Northwest Indian College; Phillips Community College; Arctic Slope Regional Corporation; North Slope Borough Office of the Mayor; Alaska Growth Capital; Arctic Development Council; Central Council Tlingit Haida Indian Tribes of Alaska; Interior Athabascan Tribal College; Tanana Chiefs Conference. &lt;br/&gt;&lt;br/&gt;Proposed Activities&lt;br/&gt;The activities for this award include delivery of post-secondary education to Alaska Natives living in intensely rural villages, typically far off the road system, with unemployment rates 3-5 times the national average; develop, test, and refine a situational model for distance education that takes into account variables for indigenous communities; combine distance learning with technical assistance for entrepreneurs and job placement services to develop Alaska&apos;s rural economy; assess occupational demand in the region; determine the skill levels of the workers; design and deliver two rounds of distance learning; provide small business training and technical assistance; assess the emerging model and disseminate the learnings.&lt;br/&gt;&lt;br/&gt;Proposed Innovation&lt;br/&gt;The innovation goals for the award include providing distance training to prepare workers for job demands in remote Alaska; providing small business training and technical assistance; research and development of models for remote education in rural indigenous populations; enablement of increased business and job opportunities in rural Alaska.&lt;br/&gt;&lt;br/&gt;Potential Economic Impact&lt;br/&gt;The economic outcomes for this award include increased skills and technical assistance for small business in rural Alaska, as well as increased jobs and business training and opportunity.&lt;br/&gt;&lt;br/&gt;Potential Societal Impact&lt;br/&gt;The potential benefits to society from this award include involvement of indigenous tribal population in a rural environment in job training and opportunity, plus creation of increased wealth in remote Alaska.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">90474</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">90474</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n50" target="n52">
      <data key="e_effectiveDate">2001-02-15</data>
      <data key="e_title">Distance Education Delivery for Isolated Rural Communities: A Contingency Approach</data>
      <data key="e_abstract">0090474 &lt;br/&gt;Scott&lt;br/&gt;&lt;br/&gt;This award is to Ilisagvik College to support the activity described below for 36 months. The proposal was submitted in response to the Partnerships for Innovation Program Solicitation (NSF 0082).&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Partners&lt;br/&gt;The partners for this award include California Virtual Campus, Bay Area Region 1; Lewis-Clark State College; Northwest Indian College; Phillips Community College; Arctic Slope Regional Corporation; North Slope Borough Office of the Mayor; Alaska Growth Capital; Arctic Development Council; Central Council Tlingit Haida Indian Tribes of Alaska; Interior Athabascan Tribal College; Tanana Chiefs Conference. &lt;br/&gt;&lt;br/&gt;Proposed Activities&lt;br/&gt;The activities for this award include delivery of post-secondary education to Alaska Natives living in intensely rural villages, typically far off the road system, with unemployment rates 3-5 times the national average; develop, test, and refine a situational model for distance education that takes into account variables for indigenous communities; combine distance learning with technical assistance for entrepreneurs and job placement services to develop Alaska&apos;s rural economy; assess occupational demand in the region; determine the skill levels of the workers; design and deliver two rounds of distance learning; provide small business training and technical assistance; assess the emerging model and disseminate the learnings.&lt;br/&gt;&lt;br/&gt;Proposed Innovation&lt;br/&gt;The innovation goals for the award include providing distance training to prepare workers for job demands in remote Alaska; providing small business training and technical assistance; research and development of models for remote education in rural indigenous populations; enablement of increased business and job opportunities in rural Alaska.&lt;br/&gt;&lt;br/&gt;Potential Economic Impact&lt;br/&gt;The economic outcomes for this award include increased skills and technical assistance for small business in rural Alaska, as well as increased jobs and business training and opportunity.&lt;br/&gt;&lt;br/&gt;Potential Societal Impact&lt;br/&gt;The potential benefits to society from this award include involvement of indigenous tribal population in a rural environment in job training and opportunity, plus creation of increased wealth in remote Alaska.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">90474</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">90474</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n50" target="n53">
      <data key="e_effectiveDate">2001-02-15</data>
      <data key="e_title">Distance Education Delivery for Isolated Rural Communities: A Contingency Approach</data>
      <data key="e_abstract">0090474 &lt;br/&gt;Scott&lt;br/&gt;&lt;br/&gt;This award is to Ilisagvik College to support the activity described below for 36 months. The proposal was submitted in response to the Partnerships for Innovation Program Solicitation (NSF 0082).&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Partners&lt;br/&gt;The partners for this award include California Virtual Campus, Bay Area Region 1; Lewis-Clark State College; Northwest Indian College; Phillips Community College; Arctic Slope Regional Corporation; North Slope Borough Office of the Mayor; Alaska Growth Capital; Arctic Development Council; Central Council Tlingit Haida Indian Tribes of Alaska; Interior Athabascan Tribal College; Tanana Chiefs Conference. &lt;br/&gt;&lt;br/&gt;Proposed Activities&lt;br/&gt;The activities for this award include delivery of post-secondary education to Alaska Natives living in intensely rural villages, typically far off the road system, with unemployment rates 3-5 times the national average; develop, test, and refine a situational model for distance education that takes into account variables for indigenous communities; combine distance learning with technical assistance for entrepreneurs and job placement services to develop Alaska&apos;s rural economy; assess occupational demand in the region; determine the skill levels of the workers; design and deliver two rounds of distance learning; provide small business training and technical assistance; assess the emerging model and disseminate the learnings.&lt;br/&gt;&lt;br/&gt;Proposed Innovation&lt;br/&gt;The innovation goals for the award include providing distance training to prepare workers for job demands in remote Alaska; providing small business training and technical assistance; research and development of models for remote education in rural indigenous populations; enablement of increased business and job opportunities in rural Alaska.&lt;br/&gt;&lt;br/&gt;Potential Economic Impact&lt;br/&gt;The economic outcomes for this award include increased skills and technical assistance for small business in rural Alaska, as well as increased jobs and business training and opportunity.&lt;br/&gt;&lt;br/&gt;Potential Societal Impact&lt;br/&gt;The potential benefits to society from this award include involvement of indigenous tribal population in a rural environment in job training and opportunity, plus creation of increased wealth in remote Alaska.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">90474</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">90474</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n50" target="n54">
      <data key="e_effectiveDate">2001-02-15</data>
      <data key="e_title">Distance Education Delivery for Isolated Rural Communities: A Contingency Approach</data>
      <data key="e_abstract">0090474 &lt;br/&gt;Scott&lt;br/&gt;&lt;br/&gt;This award is to Ilisagvik College to support the activity described below for 36 months. The proposal was submitted in response to the Partnerships for Innovation Program Solicitation (NSF 0082).&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Partners&lt;br/&gt;The partners for this award include California Virtual Campus, Bay Area Region 1; Lewis-Clark State College; Northwest Indian College; Phillips Community College; Arctic Slope Regional Corporation; North Slope Borough Office of the Mayor; Alaska Growth Capital; Arctic Development Council; Central Council Tlingit Haida Indian Tribes of Alaska; Interior Athabascan Tribal College; Tanana Chiefs Conference. &lt;br/&gt;&lt;br/&gt;Proposed Activities&lt;br/&gt;The activities for this award include delivery of post-secondary education to Alaska Natives living in intensely rural villages, typically far off the road system, with unemployment rates 3-5 times the national average; develop, test, and refine a situational model for distance education that takes into account variables for indigenous communities; combine distance learning with technical assistance for entrepreneurs and job placement services to develop Alaska&apos;s rural economy; assess occupational demand in the region; determine the skill levels of the workers; design and deliver two rounds of distance learning; provide small business training and technical assistance; assess the emerging model and disseminate the learnings.&lt;br/&gt;&lt;br/&gt;Proposed Innovation&lt;br/&gt;The innovation goals for the award include providing distance training to prepare workers for job demands in remote Alaska; providing small business training and technical assistance; research and development of models for remote education in rural indigenous populations; enablement of increased business and job opportunities in rural Alaska.&lt;br/&gt;&lt;br/&gt;Potential Economic Impact&lt;br/&gt;The economic outcomes for this award include increased skills and technical assistance for small business in rural Alaska, as well as increased jobs and business training and opportunity.&lt;br/&gt;&lt;br/&gt;Potential Societal Impact&lt;br/&gt;The potential benefits to society from this award include involvement of indigenous tribal population in a rural environment in job training and opportunity, plus creation of increased wealth in remote Alaska.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">90474</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">90474</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n51" target="n52">
      <data key="e_effectiveDate">2001-02-15</data>
      <data key="e_title">Distance Education Delivery for Isolated Rural Communities: A Contingency Approach</data>
      <data key="e_abstract">0090474 &lt;br/&gt;Scott&lt;br/&gt;&lt;br/&gt;This award is to Ilisagvik College to support the activity described below for 36 months. The proposal was submitted in response to the Partnerships for Innovation Program Solicitation (NSF 0082).&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Partners&lt;br/&gt;The partners for this award include California Virtual Campus, Bay Area Region 1; Lewis-Clark State College; Northwest Indian College; Phillips Community College; Arctic Slope Regional Corporation; North Slope Borough Office of the Mayor; Alaska Growth Capital; Arctic Development Council; Central Council Tlingit Haida Indian Tribes of Alaska; Interior Athabascan Tribal College; Tanana Chiefs Conference. &lt;br/&gt;&lt;br/&gt;Proposed Activities&lt;br/&gt;The activities for this award include delivery of post-secondary education to Alaska Natives living in intensely rural villages, typically far off the road system, with unemployment rates 3-5 times the national average; develop, test, and refine a situational model for distance education that takes into account variables for indigenous communities; combine distance learning with technical assistance for entrepreneurs and job placement services to develop Alaska&apos;s rural economy; assess occupational demand in the region; determine the skill levels of the workers; design and deliver two rounds of distance learning; provide small business training and technical assistance; assess the emerging model and disseminate the learnings.&lt;br/&gt;&lt;br/&gt;Proposed Innovation&lt;br/&gt;The innovation goals for the award include providing distance training to prepare workers for job demands in remote Alaska; providing small business training and technical assistance; research and development of models for remote education in rural indigenous populations; enablement of increased business and job opportunities in rural Alaska.&lt;br/&gt;&lt;br/&gt;Potential Economic Impact&lt;br/&gt;The economic outcomes for this award include increased skills and technical assistance for small business in rural Alaska, as well as increased jobs and business training and opportunity.&lt;br/&gt;&lt;br/&gt;Potential Societal Impact&lt;br/&gt;The potential benefits to society from this award include involvement of indigenous tribal population in a rural environment in job training and opportunity, plus creation of increased wealth in remote Alaska.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">90474</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">90474</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n51" target="n53">
      <data key="e_effectiveDate">2001-02-15</data>
      <data key="e_title">Distance Education Delivery for Isolated Rural Communities: A Contingency Approach</data>
      <data key="e_abstract">0090474 &lt;br/&gt;Scott&lt;br/&gt;&lt;br/&gt;This award is to Ilisagvik College to support the activity described below for 36 months. The proposal was submitted in response to the Partnerships for Innovation Program Solicitation (NSF 0082).&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Partners&lt;br/&gt;The partners for this award include California Virtual Campus, Bay Area Region 1; Lewis-Clark State College; Northwest Indian College; Phillips Community College; Arctic Slope Regional Corporation; North Slope Borough Office of the Mayor; Alaska Growth Capital; Arctic Development Council; Central Council Tlingit Haida Indian Tribes of Alaska; Interior Athabascan Tribal College; Tanana Chiefs Conference. &lt;br/&gt;&lt;br/&gt;Proposed Activities&lt;br/&gt;The activities for this award include delivery of post-secondary education to Alaska Natives living in intensely rural villages, typically far off the road system, with unemployment rates 3-5 times the national average; develop, test, and refine a situational model for distance education that takes into account variables for indigenous communities; combine distance learning with technical assistance for entrepreneurs and job placement services to develop Alaska&apos;s rural economy; assess occupational demand in the region; determine the skill levels of the workers; design and deliver two rounds of distance learning; provide small business training and technical assistance; assess the emerging model and disseminate the learnings.&lt;br/&gt;&lt;br/&gt;Proposed Innovation&lt;br/&gt;The innovation goals for the award include providing distance training to prepare workers for job demands in remote Alaska; providing small business training and technical assistance; research and development of models for remote education in rural indigenous populations; enablement of increased business and job opportunities in rural Alaska.&lt;br/&gt;&lt;br/&gt;Potential Economic Impact&lt;br/&gt;The economic outcomes for this award include increased skills and technical assistance for small business in rural Alaska, as well as increased jobs and business training and opportunity.&lt;br/&gt;&lt;br/&gt;Potential Societal Impact&lt;br/&gt;The potential benefits to society from this award include involvement of indigenous tribal population in a rural environment in job training and opportunity, plus creation of increased wealth in remote Alaska.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">90474</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">90474</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n51" target="n54">
      <data key="e_effectiveDate">2001-02-15</data>
      <data key="e_title">Distance Education Delivery for Isolated Rural Communities: A Contingency Approach</data>
      <data key="e_abstract">0090474 &lt;br/&gt;Scott&lt;br/&gt;&lt;br/&gt;This award is to Ilisagvik College to support the activity described below for 36 months. The proposal was submitted in response to the Partnerships for Innovation Program Solicitation (NSF 0082).&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Partners&lt;br/&gt;The partners for this award include California Virtual Campus, Bay Area Region 1; Lewis-Clark State College; Northwest Indian College; Phillips Community College; Arctic Slope Regional Corporation; North Slope Borough Office of the Mayor; Alaska Growth Capital; Arctic Development Council; Central Council Tlingit Haida Indian Tribes of Alaska; Interior Athabascan Tribal College; Tanana Chiefs Conference. &lt;br/&gt;&lt;br/&gt;Proposed Activities&lt;br/&gt;The activities for this award include delivery of post-secondary education to Alaska Natives living in intensely rural villages, typically far off the road system, with unemployment rates 3-5 times the national average; develop, test, and refine a situational model for distance education that takes into account variables for indigenous communities; combine distance learning with technical assistance for entrepreneurs and job placement services to develop Alaska&apos;s rural economy; assess occupational demand in the region; determine the skill levels of the workers; design and deliver two rounds of distance learning; provide small business training and technical assistance; assess the emerging model and disseminate the learnings.&lt;br/&gt;&lt;br/&gt;Proposed Innovation&lt;br/&gt;The innovation goals for the award include providing distance training to prepare workers for job demands in remote Alaska; providing small business training and technical assistance; research and development of models for remote education in rural indigenous populations; enablement of increased business and job opportunities in rural Alaska.&lt;br/&gt;&lt;br/&gt;Potential Economic Impact&lt;br/&gt;The economic outcomes for this award include increased skills and technical assistance for small business in rural Alaska, as well as increased jobs and business training and opportunity.&lt;br/&gt;&lt;br/&gt;Potential Societal Impact&lt;br/&gt;The potential benefits to society from this award include involvement of indigenous tribal population in a rural environment in job training and opportunity, plus creation of increased wealth in remote Alaska.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">90474</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">90474</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n52" target="n53">
      <data key="e_effectiveDate">2001-02-15</data>
      <data key="e_title">Distance Education Delivery for Isolated Rural Communities: A Contingency Approach</data>
      <data key="e_abstract">0090474 &lt;br/&gt;Scott&lt;br/&gt;&lt;br/&gt;This award is to Ilisagvik College to support the activity described below for 36 months. The proposal was submitted in response to the Partnerships for Innovation Program Solicitation (NSF 0082).&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Partners&lt;br/&gt;The partners for this award include California Virtual Campus, Bay Area Region 1; Lewis-Clark State College; Northwest Indian College; Phillips Community College; Arctic Slope Regional Corporation; North Slope Borough Office of the Mayor; Alaska Growth Capital; Arctic Development Council; Central Council Tlingit Haida Indian Tribes of Alaska; Interior Athabascan Tribal College; Tanana Chiefs Conference. &lt;br/&gt;&lt;br/&gt;Proposed Activities&lt;br/&gt;The activities for this award include delivery of post-secondary education to Alaska Natives living in intensely rural villages, typically far off the road system, with unemployment rates 3-5 times the national average; develop, test, and refine a situational model for distance education that takes into account variables for indigenous communities; combine distance learning with technical assistance for entrepreneurs and job placement services to develop Alaska&apos;s rural economy; assess occupational demand in the region; determine the skill levels of the workers; design and deliver two rounds of distance learning; provide small business training and technical assistance; assess the emerging model and disseminate the learnings.&lt;br/&gt;&lt;br/&gt;Proposed Innovation&lt;br/&gt;The innovation goals for the award include providing distance training to prepare workers for job demands in remote Alaska; providing small business training and technical assistance; research and development of models for remote education in rural indigenous populations; enablement of increased business and job opportunities in rural Alaska.&lt;br/&gt;&lt;br/&gt;Potential Economic Impact&lt;br/&gt;The economic outcomes for this award include increased skills and technical assistance for small business in rural Alaska, as well as increased jobs and business training and opportunity.&lt;br/&gt;&lt;br/&gt;Potential Societal Impact&lt;br/&gt;The potential benefits to society from this award include involvement of indigenous tribal population in a rural environment in job training and opportunity, plus creation of increased wealth in remote Alaska.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">90474</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">90474</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n52" target="n54">
      <data key="e_effectiveDate">2001-02-15</data>
      <data key="e_title">Distance Education Delivery for Isolated Rural Communities: A Contingency Approach</data>
      <data key="e_abstract">0090474 &lt;br/&gt;Scott&lt;br/&gt;&lt;br/&gt;This award is to Ilisagvik College to support the activity described below for 36 months. The proposal was submitted in response to the Partnerships for Innovation Program Solicitation (NSF 0082).&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Partners&lt;br/&gt;The partners for this award include California Virtual Campus, Bay Area Region 1; Lewis-Clark State College; Northwest Indian College; Phillips Community College; Arctic Slope Regional Corporation; North Slope Borough Office of the Mayor; Alaska Growth Capital; Arctic Development Council; Central Council Tlingit Haida Indian Tribes of Alaska; Interior Athabascan Tribal College; Tanana Chiefs Conference. &lt;br/&gt;&lt;br/&gt;Proposed Activities&lt;br/&gt;The activities for this award include delivery of post-secondary education to Alaska Natives living in intensely rural villages, typically far off the road system, with unemployment rates 3-5 times the national average; develop, test, and refine a situational model for distance education that takes into account variables for indigenous communities; combine distance learning with technical assistance for entrepreneurs and job placement services to develop Alaska&apos;s rural economy; assess occupational demand in the region; determine the skill levels of the workers; design and deliver two rounds of distance learning; provide small business training and technical assistance; assess the emerging model and disseminate the learnings.&lt;br/&gt;&lt;br/&gt;Proposed Innovation&lt;br/&gt;The innovation goals for the award include providing distance training to prepare workers for job demands in remote Alaska; providing small business training and technical assistance; research and development of models for remote education in rural indigenous populations; enablement of increased business and job opportunities in rural Alaska.&lt;br/&gt;&lt;br/&gt;Potential Economic Impact&lt;br/&gt;The economic outcomes for this award include increased skills and technical assistance for small business in rural Alaska, as well as increased jobs and business training and opportunity.&lt;br/&gt;&lt;br/&gt;Potential Societal Impact&lt;br/&gt;The potential benefits to society from this award include involvement of indigenous tribal population in a rural environment in job training and opportunity, plus creation of increased wealth in remote Alaska.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">90474</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">90474</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n53" target="n54">
      <data key="e_effectiveDate">2001-02-15</data>
      <data key="e_title">Distance Education Delivery for Isolated Rural Communities: A Contingency Approach</data>
      <data key="e_abstract">0090474 &lt;br/&gt;Scott&lt;br/&gt;&lt;br/&gt;This award is to Ilisagvik College to support the activity described below for 36 months. The proposal was submitted in response to the Partnerships for Innovation Program Solicitation (NSF 0082).&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Partners&lt;br/&gt;The partners for this award include California Virtual Campus, Bay Area Region 1; Lewis-Clark State College; Northwest Indian College; Phillips Community College; Arctic Slope Regional Corporation; North Slope Borough Office of the Mayor; Alaska Growth Capital; Arctic Development Council; Central Council Tlingit Haida Indian Tribes of Alaska; Interior Athabascan Tribal College; Tanana Chiefs Conference. &lt;br/&gt;&lt;br/&gt;Proposed Activities&lt;br/&gt;The activities for this award include delivery of post-secondary education to Alaska Natives living in intensely rural villages, typically far off the road system, with unemployment rates 3-5 times the national average; develop, test, and refine a situational model for distance education that takes into account variables for indigenous communities; combine distance learning with technical assistance for entrepreneurs and job placement services to develop Alaska&apos;s rural economy; assess occupational demand in the region; determine the skill levels of the workers; design and deliver two rounds of distance learning; provide small business training and technical assistance; assess the emerging model and disseminate the learnings.&lt;br/&gt;&lt;br/&gt;Proposed Innovation&lt;br/&gt;The innovation goals for the award include providing distance training to prepare workers for job demands in remote Alaska; providing small business training and technical assistance; research and development of models for remote education in rural indigenous populations; enablement of increased business and job opportunities in rural Alaska.&lt;br/&gt;&lt;br/&gt;Potential Economic Impact&lt;br/&gt;The economic outcomes for this award include increased skills and technical assistance for small business in rural Alaska, as well as increased jobs and business training and opportunity.&lt;br/&gt;&lt;br/&gt;Potential Societal Impact&lt;br/&gt;The potential benefits to society from this award include involvement of indigenous tribal population in a rural environment in job training and opportunity, plus creation of increased wealth in remote Alaska.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">90474</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">90474</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n56" target="n57">
      <data key="e_effectiveDate">2001-02-01</data>
      <data key="e_title">ITW: Understanding Female and Minority Retention and Success in the IT Workplace: Total Rewards and Social Networks Perspectives</data>
      <data key="e_abstract">Institution: University of Maryland College Park&lt;br/&gt;Proposal Number: EIA 0089941&lt;br/&gt;PI: Kathryn M. Bartol&lt;br/&gt;Title: Understanding Female and Minority Retention and Success in the IT Workplace: Total Rewards and Social Networks Perspectives &lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds for a research program aimed at addressing fundamental issues underlying the effective utilization, retention, and upward mobility of women and minorities in the IT workplace. The project takes a multidisciplinary approach to the design and implementation of three studies aimed at better understanding these issues. The first study adopts a total rewards perspective in analyzing the IT workplace success and retention of women and minorities. The second study uses social network analysis to study interaction patterns among members of an IT organization and their impact on women and minorities. The third study longitudinally tracks students graduating from undergraduate programs in IT to learn more about how early perceptions and subsequent experiences influence career success and retention for women and minorities. This project has the potential to provide valuable insights about the retention and advancement of women and underrepresented minorities in IT careers.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">89941</data>
      <data key="e_expirationDate">2006-01-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">89941</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n56" target="n58">
      <data key="e_effectiveDate">2001-02-01</data>
      <data key="e_title">ITW: Understanding Female and Minority Retention and Success in the IT Workplace: Total Rewards and Social Networks Perspectives</data>
      <data key="e_abstract">Institution: University of Maryland College Park&lt;br/&gt;Proposal Number: EIA 0089941&lt;br/&gt;PI: Kathryn M. Bartol&lt;br/&gt;Title: Understanding Female and Minority Retention and Success in the IT Workplace: Total Rewards and Social Networks Perspectives &lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds for a research program aimed at addressing fundamental issues underlying the effective utilization, retention, and upward mobility of women and minorities in the IT workplace. The project takes a multidisciplinary approach to the design and implementation of three studies aimed at better understanding these issues. The first study adopts a total rewards perspective in analyzing the IT workplace success and retention of women and minorities. The second study uses social network analysis to study interaction patterns among members of an IT organization and their impact on women and minorities. The third study longitudinally tracks students graduating from undergraduate programs in IT to learn more about how early perceptions and subsequent experiences influence career success and retention for women and minorities. This project has the potential to provide valuable insights about the retention and advancement of women and underrepresented minorities in IT careers.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">89941</data>
      <data key="e_expirationDate">2006-01-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">89941</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n57" target="n58">
      <data key="e_effectiveDate">2001-02-01</data>
      <data key="e_title">ITW: Understanding Female and Minority Retention and Success in the IT Workplace: Total Rewards and Social Networks Perspectives</data>
      <data key="e_abstract">Institution: University of Maryland College Park&lt;br/&gt;Proposal Number: EIA 0089941&lt;br/&gt;PI: Kathryn M. Bartol&lt;br/&gt;Title: Understanding Female and Minority Retention and Success in the IT Workplace: Total Rewards and Social Networks Perspectives &lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds for a research program aimed at addressing fundamental issues underlying the effective utilization, retention, and upward mobility of women and minorities in the IT workplace. The project takes a multidisciplinary approach to the design and implementation of three studies aimed at better understanding these issues. The first study adopts a total rewards perspective in analyzing the IT workplace success and retention of women and minorities. The second study uses social network analysis to study interaction patterns among members of an IT organization and their impact on women and minorities. The third study longitudinally tracks students graduating from undergraduate programs in IT to learn more about how early perceptions and subsequent experiences influence career success and retention for women and minorities. This project has the potential to provide valuable insights about the retention and advancement of women and underrepresented minorities in IT careers.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">89941</data>
      <data key="e_expirationDate">2006-01-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">89941</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n59" target="n60">
      <data key="e_effectiveDate">2001-02-01</data>
      <data key="e_title">Synergistic Electronic Commerce (SynreCom) Partnership for Innovation</data>
      <data key="e_abstract">0090959 &lt;br/&gt;Sera &lt;br/&gt;&lt;br/&gt;This award is to Texas A&amp;M University to support the activity described below for 24 months. The proposal was submitted in response to the Partnerships for Innovation Program Solicitation (NSF 0082).&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Partners&lt;br/&gt;The partners for this award include Texas A&amp;M Engineering Extension Service; Edgewood ISD, San Antonio School District; Our Lady of the Lake University; TEKSA Innovations Corporation; San Antonio Day Care Providers.&lt;br/&gt;&lt;br/&gt;Proposed Activities&lt;br/&gt;The award supports the following activities: formation of minority-lead entrepreneurial teams to form and assist small businesses; provide e-commerce capability for small business; job training (computer training) for at-risk students; provide business management tools, including incubation support, to small minority-owned small businesses.&lt;br/&gt;&lt;br/&gt;Proposed Innovation&lt;br/&gt;The innovation goals of this award include formation of minority-led entrepreneurial teams to assist formation and growth of small business in the San Antonio area; enablement of economic growth by providing e-commerce capability of small businesses, especially minority owned businesses; providing incubator services where needed; providing workforce training for at-risk under-represented groups and providing job opportunities for them in small companies.&lt;br/&gt;&lt;br/&gt;Potential Economic Impact&lt;br/&gt;The potential economic benefits from this award includes a wide range of outcomes, such as job training and job placement for under-represented, at-risk of poverty groups by creation of new business and new jobs.&lt;br/&gt;&lt;br/&gt;Potential Societal Impact&lt;br/&gt;The potential benefits to society include education and training for at-risk under-represented minorities; creation of business opportunities for minority-owned small businesses; creation of new employment opportunities for minorities.</data>
      <data key="e_pgm">2860</data>
      <data key="e_label">90959</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">90959</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n61" target="n62">
      <data key="e_effectiveDate">2001-02-15</data>
      <data key="e_title">Special Projects: Graduate Students in Computing Travel Support for Career Workshops</data>
      <data key="e_abstract">EIA-0101335&lt;br/&gt; Aspray,William&lt;br/&gt; Computing Research Association&lt;br/&gt;&lt;br/&gt;Special Projects: Graduate Students in Computing Travel Support for Career Workshops&lt;br/&gt; &lt;br/&gt;This award to the Computing Research Association (CRA) provides funds to support advanced graduate students to attend the CRA career workshops in 2001, 2002, and 2003. The workshops are held in Washington, DC and are open to all recent doctorates and advanced graduate students in computing disciplines. Topics incorporated in the workshops include development of effective teaching practices, research programs, and developing a successful academic career overall. Senior computer scientists and engineers conduct the workshops and program managers from NSF and DARPA discussing how their research funding programs are organized are also part of the workshops.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101335</data>
      <data key="e_expirationDate">2006-01-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101335</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n64" target="n65">
      <data key="e_effectiveDate">2001-02-15</data>
      <data key="e_title">Giga-Scale System-On-A-Chip Design</data>
      <data key="e_abstract">This project is to establish an international center for research on giga-scale system-on-a-chip (SOC) designs. It involves researchers from U.S., Taiwan, and China. The center is being supported by the National Science Council (NSC) in Taiwan and the Chinese National Science Foundation (CNSF) to support the activities by researchers from Taiwan and China respectively.&lt;br/&gt;Focus of the project is on innovative design methodologies, tools, and algorithms that enable efficient giga-scale SOC integration in nanometer technologies. Research activities include investigation and development of efficient SOC synthesis tools and methodologies, SOC verification, test, and diagnostic technologies, and an SOC design driver that motivates and validates various synthesis, verification, and test techniques developed during the course of this research project. The design driver is a SOC design of a network processor, which includes embedded CPUs, DPSs, FPGAs, and various kinds of memory components. The research on SOC synthesis tools and methodologies includes design specification, design partitioning, synthesis and optimization for embedded DPSs and FPGAs, physical synthesis for full-chip assembly, and synthesis techniques for design re-use. The research on verification and test focuses functional verification, self-test using on-chip programmable logic, analog and mixed-signal self-test, and test for embedded memories.</data>
      <data key="e_pgm">5978</data>
      <data key="e_label">96383</data>
      <data key="e_expirationDate">2008-01-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">96383</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n87" target="n88">
      <data key="e_effectiveDate">2001-02-15</data>
      <data key="e_title">Middle High German Interlinked: A Full-Text Archive and Medieval German Dictionaries Collaboratory</data>
      <data key="e_abstract">The project will explore and demonstrate the possibilities and advantages of international cooperation for the creation and delivery of complex literary and linguistic documents as encoded XML files. The University of Trier brings strong computer science, lexicographical, and philological expertise to the project. The University of Virginia Library and other departments have a long and successful record of applying SGML and XML encoding to humanities and other textual material, along with considerable capabilities for online delivery for tens of thousands of text. This particular project will bring these strengths to bear on Middle High German Interlinked, a full-text archive and Medieval German Dictionaries collaboratory. Prior efforts at Virginia to interlink the Oxford English Dictionary with texts of Early American fiction have been of value in examining the influence of pre-1850 American fiction on the development of the modern English language and usage. Part of the project will be focussed on evaluation and implementation of web interfaces for multiple user communities.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">3214</data>
      <data key="e_expirationDate">2004-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">3214</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n34" target="n97">
      <data key="e_effectiveDate">2001-03-01</data>
      <data key="e_title">2001 NSF Information and Data Management Program Workshop</data>
      <data key="e_abstract">Recent advances in computer and network technologies and storage, the explosion of publishing, and the vast increase in data availability from sources such as the Internet and satellites, that have enabled the emergence of an unprecedented number of new computer applications, present a new challenge to the ways data and information are used and managed. This challenge will shape both the research agenda as well as the technology to be developed for the 21st century. The objective of this workshop is to bring together the PIs and Co-PIs currently funded by the Information and Data Management Program (IDM) of the National Science Foundation to: (1) cooperatively analyze and focus on the research and development issues of problems that are fundamental in making progress towards this new challenge; (2) share, provide demonstrations and interact with each other on the objectives, contributions and challenges of major research activities funded by the IDM and explore fruitful collaboration and synergism; and (3) provide an opportunity to NSF program officers, other foundations and funding agencies, and industry representatives to learn more about the current research efforts and successes of projects funded by IDM, and for such officers to share their program highlights and concerns. &lt;br/&gt;&lt;br/&gt;The workshop will generate a proceedings on all projects currently funded by IDM in both hard copy and electronic form. The workshop will also generate a report detailing future directions of research and will suggest promising modalities of research with an aim to foster innovation and technology transfer. The proceedings will provide project information searchable by different criteria and provide connections between discoveries and their use to society. The workshop website (http://itlab.uta.edu/idm01) will also include links to other relevant material.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">112914</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112914</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n100" target="n101">
      <data key="e_effectiveDate">2001-03-01</data>
      <data key="e_title">Robust Knowledge Discovery from Parallel Speech and Text Sources</data>
      <data key="e_abstract">The goal of this project is to enable information retrieval systems to handle speech-based information by considerably enhancing speech recognition technology. The PI&apos;s approach is to align collections of stories from text and speech sources in multiple languages, and to develop methods which exploit the resulting parallelism. The PI will improve acoustic and language models, and will develop iterative recognition methods to facilitate capture of cross-lingual information resident in text and speech. Voice-of-America broadcasts in English, Greek, and a third language to be determined, will provide the data source for this research.</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">9.98233e+06</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">9.98233e+06</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n100" target="n102">
      <data key="e_effectiveDate">2001-03-01</data>
      <data key="e_title">Robust Knowledge Discovery from Parallel Speech and Text Sources</data>
      <data key="e_abstract">The goal of this project is to enable information retrieval systems to handle speech-based information by considerably enhancing speech recognition technology. The PI&apos;s approach is to align collections of stories from text and speech sources in multiple languages, and to develop methods which exploit the resulting parallelism. The PI will improve acoustic and language models, and will develop iterative recognition methods to facilitate capture of cross-lingual information resident in text and speech. Voice-of-America broadcasts in English, Greek, and a third language to be determined, will provide the data source for this research.</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">9.98233e+06</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">9.98233e+06</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n101" target="n102">
      <data key="e_effectiveDate">2001-03-01</data>
      <data key="e_title">Robust Knowledge Discovery from Parallel Speech and Text Sources</data>
      <data key="e_abstract">The goal of this project is to enable information retrieval systems to handle speech-based information by considerably enhancing speech recognition technology. The PI&apos;s approach is to align collections of stories from text and speech sources in multiple languages, and to develop methods which exploit the resulting parallelism. The PI will improve acoustic and language models, and will develop iterative recognition methods to facilitate capture of cross-lingual information resident in text and speech. Voice-of-America broadcasts in English, Greek, and a third language to be determined, will provide the data source for this research.</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">9.98233e+06</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">9.98233e+06</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n112" target="n113">
      <data key="e_effectiveDate">2001-03-01</data>
      <data key="e_title">Open Archives: Distributed Services for Physicists and Graduate Students (OAD)</data>
      <data key="e_abstract">The project will build on established activities exploring new modes and mechanism for scholarly communication. In particular, the project will draw on existing efforts of the Networked Digital &lt;br/&gt;Library of Theses and Dissertations (NDLTD) and the Open Archives Initiative (OAI). The NDLTD promotes student production and submission of their own electronic theses and dissertations - in many &lt;br/&gt;different natural languages. This project will pursue metadata standards and tool building for searching the distributed, multilingual, large-scale collection. The collaboration with the University of Oldenburg will focus on PhysNet, an existing service which collects physics papers and scholarly materials from the Web</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">86227</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86227</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n115" target="n116">
      <data key="e_effectiveDate">2001-03-15</data>
      <data key="e_title">Association of Research Libraries and Internet Archive Workshop: &quot;Research in the &apos;Born-Digital&apos; Domain&quot; on March 12-13, 2001 in San Francisco, California</data>
      <data key="e_abstract">This proposal is to fund travel to a workshop in San Francisco, March 12-13, 2001, on the subject of digital preservation and digital archiving. The workshop will identify and stimulate pilot projects in the collectin&lt;br/&gt;and use of born-digital information, build agreement on guidelines for servicing and use of born-digital material, and write a white paper on the role of born-digital resources in research libraries, including an outline of next steps and identification of the individuals and organizations that could take them. It will improve our understanding of how to use digital information for long-term societal benefit.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">109524</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">109524</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n115" target="n117">
      <data key="e_effectiveDate">2001-03-15</data>
      <data key="e_title">Association of Research Libraries and Internet Archive Workshop: &quot;Research in the &apos;Born-Digital&apos; Domain&quot; on March 12-13, 2001 in San Francisco, California</data>
      <data key="e_abstract">This proposal is to fund travel to a workshop in San Francisco, March 12-13, 2001, on the subject of digital preservation and digital archiving. The workshop will identify and stimulate pilot projects in the collectin&lt;br/&gt;and use of born-digital information, build agreement on guidelines for servicing and use of born-digital material, and write a white paper on the role of born-digital resources in research libraries, including an outline of next steps and identification of the individuals and organizations that could take them. It will improve our understanding of how to use digital information for long-term societal benefit.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">109524</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">109524</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n115" target="n118">
      <data key="e_effectiveDate">2001-03-15</data>
      <data key="e_title">Association of Research Libraries and Internet Archive Workshop: &quot;Research in the &apos;Born-Digital&apos; Domain&quot; on March 12-13, 2001 in San Francisco, California</data>
      <data key="e_abstract">This proposal is to fund travel to a workshop in San Francisco, March 12-13, 2001, on the subject of digital preservation and digital archiving. The workshop will identify and stimulate pilot projects in the collectin&lt;br/&gt;and use of born-digital information, build agreement on guidelines for servicing and use of born-digital material, and write a white paper on the role of born-digital resources in research libraries, including an outline of next steps and identification of the individuals and organizations that could take them. It will improve our understanding of how to use digital information for long-term societal benefit.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">109524</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">109524</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n116" target="n117">
      <data key="e_effectiveDate">2001-03-15</data>
      <data key="e_title">Association of Research Libraries and Internet Archive Workshop: &quot;Research in the &apos;Born-Digital&apos; Domain&quot; on March 12-13, 2001 in San Francisco, California</data>
      <data key="e_abstract">This proposal is to fund travel to a workshop in San Francisco, March 12-13, 2001, on the subject of digital preservation and digital archiving. The workshop will identify and stimulate pilot projects in the collectin&lt;br/&gt;and use of born-digital information, build agreement on guidelines for servicing and use of born-digital material, and write a white paper on the role of born-digital resources in research libraries, including an outline of next steps and identification of the individuals and organizations that could take them. It will improve our understanding of how to use digital information for long-term societal benefit.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">109524</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">109524</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n116" target="n118">
      <data key="e_effectiveDate">2001-03-15</data>
      <data key="e_title">Association of Research Libraries and Internet Archive Workshop: &quot;Research in the &apos;Born-Digital&apos; Domain&quot; on March 12-13, 2001 in San Francisco, California</data>
      <data key="e_abstract">This proposal is to fund travel to a workshop in San Francisco, March 12-13, 2001, on the subject of digital preservation and digital archiving. The workshop will identify and stimulate pilot projects in the collectin&lt;br/&gt;and use of born-digital information, build agreement on guidelines for servicing and use of born-digital material, and write a white paper on the role of born-digital resources in research libraries, including an outline of next steps and identification of the individuals and organizations that could take them. It will improve our understanding of how to use digital information for long-term societal benefit.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">109524</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">109524</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n117" target="n118">
      <data key="e_effectiveDate">2001-03-15</data>
      <data key="e_title">Association of Research Libraries and Internet Archive Workshop: &quot;Research in the &apos;Born-Digital&apos; Domain&quot; on March 12-13, 2001 in San Francisco, California</data>
      <data key="e_abstract">This proposal is to fund travel to a workshop in San Francisco, March 12-13, 2001, on the subject of digital preservation and digital archiving. The workshop will identify and stimulate pilot projects in the collectin&lt;br/&gt;and use of born-digital information, build agreement on guidelines for servicing and use of born-digital material, and write a white paper on the role of born-digital resources in research libraries, including an outline of next steps and identification of the individuals and organizations that could take them. It will improve our understanding of how to use digital information for long-term societal benefit.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">109524</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">109524</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n124" target="n125">
      <data key="e_effectiveDate">2001-03-15</data>
      <data key="e_title">ITW: Attracting Women into the Information Technology Workforce through Technology Immersion</data>
      <data key="e_abstract">EIA-0090000&lt;br/&gt;Andrea L. Houston&lt;br/&gt;Louisiana State University &lt;br/&gt;&lt;br/&gt;Attracting Women into the Information Technology Workforce through Technology Immersion&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to investigate IT immersion on the college major and career choices of young women. The IT immersion program is modeled on the total immersion method of learning a foreign language. In the IT immersion program at Saint Joseph&apos;s Academy, an all-girls Catholic high school, entering freshmen receive a laptop computer the summer before they enter the Academy. They are required to take two weeks of summer school to learn to use their laptop and software that will be used in the classroom. Information technology has been totally integrated into the curriculum, and teachers are evaluated on the degree of information technology integration in their classes. This research project will use a longitudinal case study approach including observation, interviews, and biannual surveys focussing on the current 9-11th grade young women at the Academy (using the IT immersion curriculum) and comparing them to the current 12th grade and alumni from the Academy (using a similar curriculum without IT immersion). The student using the IT immersion curriculum will also be compared to another private high school which does not use the IT immersion curriculum. This project has the potential to provide valuable insights about factors that both motivate and discourage young women from choosing IT majors and careers.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">90000</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90000</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n124" target="n126">
      <data key="e_effectiveDate">2001-03-15</data>
      <data key="e_title">ITW: Attracting Women into the Information Technology Workforce through Technology Immersion</data>
      <data key="e_abstract">EIA-0090000&lt;br/&gt;Andrea L. Houston&lt;br/&gt;Louisiana State University &lt;br/&gt;&lt;br/&gt;Attracting Women into the Information Technology Workforce through Technology Immersion&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to investigate IT immersion on the college major and career choices of young women. The IT immersion program is modeled on the total immersion method of learning a foreign language. In the IT immersion program at Saint Joseph&apos;s Academy, an all-girls Catholic high school, entering freshmen receive a laptop computer the summer before they enter the Academy. They are required to take two weeks of summer school to learn to use their laptop and software that will be used in the classroom. Information technology has been totally integrated into the curriculum, and teachers are evaluated on the degree of information technology integration in their classes. This research project will use a longitudinal case study approach including observation, interviews, and biannual surveys focussing on the current 9-11th grade young women at the Academy (using the IT immersion curriculum) and comparing them to the current 12th grade and alumni from the Academy (using a similar curriculum without IT immersion). The student using the IT immersion curriculum will also be compared to another private high school which does not use the IT immersion curriculum. This project has the potential to provide valuable insights about factors that both motivate and discourage young women from choosing IT majors and careers.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">90000</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90000</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n125" target="n126">
      <data key="e_effectiveDate">2001-03-15</data>
      <data key="e_title">ITW: Attracting Women into the Information Technology Workforce through Technology Immersion</data>
      <data key="e_abstract">EIA-0090000&lt;br/&gt;Andrea L. Houston&lt;br/&gt;Louisiana State University &lt;br/&gt;&lt;br/&gt;Attracting Women into the Information Technology Workforce through Technology Immersion&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to investigate IT immersion on the college major and career choices of young women. The IT immersion program is modeled on the total immersion method of learning a foreign language. In the IT immersion program at Saint Joseph&apos;s Academy, an all-girls Catholic high school, entering freshmen receive a laptop computer the summer before they enter the Academy. They are required to take two weeks of summer school to learn to use their laptop and software that will be used in the classroom. Information technology has been totally integrated into the curriculum, and teachers are evaluated on the degree of information technology integration in their classes. This research project will use a longitudinal case study approach including observation, interviews, and biannual surveys focussing on the current 9-11th grade young women at the Academy (using the IT immersion curriculum) and comparing them to the current 12th grade and alumni from the Academy (using a similar curriculum without IT immersion). The student using the IT immersion curriculum will also be compared to another private high school which does not use the IT immersion curriculum. This project has the potential to provide valuable insights about factors that both motivate and discourage young women from choosing IT majors and careers.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">90000</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90000</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n133" target="n134">
      <data key="e_effectiveDate">2001-03-15</data>
      <data key="e_title">SGER: Exploratory Research: Interactive Visualization and Control of Mobile Network Simulations</data>
      <data key="e_abstract">The objective of this research is to explore information visualization schemes to interactively control, drive and analyze simulations of adaptive resource management protocols in mobile networks. The considerable number of system variables and metrics that characterize adaptive resource management protocols and the need to monitor them in real-time results in a data explosion, necessitating data visualization schemes for efficient analysis and optimization. Two ideas will be explored:&lt;br/&gt;&lt;br/&gt;1. Interactively steer simulations to optimize multi-layer adaptive protocols, via visualization interactions of appropriate simulation variables (system monitoring metrics, metric paramaters and simulation system parameters).&lt;br/&gt;2. Characterize and segment simulations in terms of their critical features (e.g., congestion, failure) under a variety of dynamic network conditions.&lt;br/&gt;&lt;br/&gt;Objective 1 is aimed at making the user central to the simulation environment, allowing simulations to be conrolled (stopped, backed up, variables/protocols changed) and directed towards certain predefined objectives. Dynamic visualizations of system variables and interactions among them are fundamental to this interactive exploration, and more important, the user&apos;s intuition and domain expertise helps explore (reduce) a large search space of these variables. Objective 2 is targeted at characterizing simulations by focusing on critical features of a simulaltion, such as normal, congestion, transient and steady state failure, and recovery conditions. A robust scheme to detect and track these features will permit large amounts of simulation data to be compactly represented, stored and searched. The goal is to evaluate the application of this approach to automating adaptive protocol design and optimization.&lt;br/&gt;&lt;br/&gt;The two proposed ideas will be evaluated for a specific problem instance that is being addressed by an ongoing NSF project on survivable wireless access networks. The problem is to develop a scaleable resource management scheme that integrates the use of adaptive admission control and adaptive multiple access control within hierarchical cellsite architectures of mobile networks. This multi-layer analysis and optimization problem presents a hugh search space of simulation scenarios, posing significant challenges to any type of comparative or systematic study. Earlier work has largely been restricted to individual components of such problems, at best. The approach is to cast this as a multi-variate data analysis problem, and explore several interactive multi-variate visualization methods. This should help in understanding the complex spatial and temporal relationships among the search space variables. Interactive steering is an efficient means to exploring this space and lead to desired objectives, while abstracting simulations by extracting and parameterizing critical features serves to facilitate automated evaluation of batch siumlation runs or network monitoring.&lt;br/&gt;&lt;br/&gt;Support by an SGER is being requested for two reasons. (1) The proposed application of information visualization to the mobile networking domain has not previously been attempted. The proposed work shows great promise to dramatically change/improve current approaches to development and simulation-based analysis of adaptive protocols. However, this work requires experimentation with new ideas in emerging research areas (interactive steering and feature characterization), and specific outcomes are not guaranteed. (2) Immediate support is needed since the proposed work is to be conducted in conjucntion with a newly-funded NSF project. It is expected that this will greatly enhance the progress achieved within both projects.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">101866</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101866</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n141" target="n142">
      <data key="e_effectiveDate">2001-03-01</data>
      <data key="e_title">Robust Real-Time System Resource Management Using Feedback</data>
      <data key="e_abstract">Real-time computing is an enabling technology for many current and future application areas. Many future generations real-time systems are expected to be highly dynamic and operate in fault-prone on-deterministic environments under strict timing constraints. Therefore, these systems need to be robust while delivering high performance. This motivates the need for robust resource management techniques that dynamically address real-time requirements and provide graceful degradation in the presence of uncertainty. Despite the significant body of results in resource management in real-time systems, most of them are based on&lt;br/&gt;``open-loop&apos;&apos; strategies which are effective when the workload can be accurately modeled. These schemes are inadequate for many real world problems wherein the workload cannot be accurately modeled. Thus, there is a need for efficient architectures for resource management where predictable performance guarantees&lt;br/&gt;can be obtained in the presence of uncertainty.&lt;br/&gt;&lt;br/&gt;Feedback control theory has been central to modeling systems operating in uncertain environments. In the past few decades, this theory has made impressive strides in this direction. Correct adaptation as illustrated by feedback control theory will yield significant dividends with respect to robustness. This research focuses on developing a robust resource management framework for real-time systems employing feedback control&lt;br/&gt;strategies.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">98354</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">98354</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n141" target="n143">
      <data key="e_effectiveDate">2001-03-01</data>
      <data key="e_title">Robust Real-Time System Resource Management Using Feedback</data>
      <data key="e_abstract">Real-time computing is an enabling technology for many current and future application areas. Many future generations real-time systems are expected to be highly dynamic and operate in fault-prone on-deterministic environments under strict timing constraints. Therefore, these systems need to be robust while delivering high performance. This motivates the need for robust resource management techniques that dynamically address real-time requirements and provide graceful degradation in the presence of uncertainty. Despite the significant body of results in resource management in real-time systems, most of them are based on&lt;br/&gt;``open-loop&apos;&apos; strategies which are effective when the workload can be accurately modeled. These schemes are inadequate for many real world problems wherein the workload cannot be accurately modeled. Thus, there is a need for efficient architectures for resource management where predictable performance guarantees&lt;br/&gt;can be obtained in the presence of uncertainty.&lt;br/&gt;&lt;br/&gt;Feedback control theory has been central to modeling systems operating in uncertain environments. In the past few decades, this theory has made impressive strides in this direction. Correct adaptation as illustrated by feedback control theory will yield significant dividends with respect to robustness. This research focuses on developing a robust resource management framework for real-time systems employing feedback control&lt;br/&gt;strategies.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">98354</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">98354</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n142" target="n143">
      <data key="e_effectiveDate">2001-03-01</data>
      <data key="e_title">Robust Real-Time System Resource Management Using Feedback</data>
      <data key="e_abstract">Real-time computing is an enabling technology for many current and future application areas. Many future generations real-time systems are expected to be highly dynamic and operate in fault-prone on-deterministic environments under strict timing constraints. Therefore, these systems need to be robust while delivering high performance. This motivates the need for robust resource management techniques that dynamically address real-time requirements and provide graceful degradation in the presence of uncertainty. Despite the significant body of results in resource management in real-time systems, most of them are based on&lt;br/&gt;``open-loop&apos;&apos; strategies which are effective when the workload can be accurately modeled. These schemes are inadequate for many real world problems wherein the workload cannot be accurately modeled. Thus, there is a need for efficient architectures for resource management where predictable performance guarantees&lt;br/&gt;can be obtained in the presence of uncertainty.&lt;br/&gt;&lt;br/&gt;Feedback control theory has been central to modeling systems operating in uncertain environments. In the past few decades, this theory has made impressive strides in this direction. Correct adaptation as illustrated by feedback control theory will yield significant dividends with respect to robustness. This research focuses on developing a robust resource management framework for real-time systems employing feedback control&lt;br/&gt;strategies.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">98354</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">98354</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n146" target="n147">
      <data key="e_effectiveDate">2001-03-15</data>
      <data key="e_title">Conference: The Internet as a Large-Scale Complex System; Santa Fe, NM; March 2001</data>
      <data key="e_abstract">The program coordinators propose to organize a workshop titled &quot;The Internet as a Large-Scale Complex System&quot; to be held at the Santa Fe Institute in March 2001. The main purpose of the workshop is to provide a forum for disseminating relevant accomplishments and discussing future challenges associated with trying to gain a solid understanding of the dynamics and evolution of the global Internet from a broad and still unconventional perspective-complex systems dynamics. The coordinators plan to bring together network researchers with a group of distinguished researchers with expertise ranging from statistical physics, dynamical systems theory, and control theory to biological, physical and social systems. Select contributions from invited speakers will be published as part of the SFI book series by Oxford University Press.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">102129</data>
      <data key="e_expirationDate">2002-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">102129</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n149" target="n150">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">Special Projects: Support for the Core Research Activities of the Computer Science and Telecommunications Board</data>
      <data key="e_abstract">EIA 0091076&lt;br/&gt;National Academy of Sciences&lt;br/&gt;Marjorie S. Blumenthal&lt;br/&gt;&lt;br/&gt;Title: Special Projects: Support for the Core Research Activities of the Computer Science and Telecommunications Board&lt;br/&gt;&lt;br/&gt;This projects requests support for the core activities of the Computer Science and Telecommunications Board. The Board conducts a variety of activities such as: monitoring and promoting the health of computer science including research, human resources, and infrastructure; initiating studies involving computer science and national economic strength; responding to requests from government for expert input; fostering interactions between computer science and other areas of science and technology; and providing a forum for the exchange of information related to computer science, computer technology, and telecommunications.</data>
      <data key="e_pgm">1714</data>
      <data key="e_label">91076</data>
      <data key="e_expirationDate">2004-03-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">91076</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n151" target="n152">
      <data key="e_effectiveDate">2001-04-15</data>
      <data key="e_title">Special Projects: Group Travel to 7th IFIP World Conference on Computers and Education (WCCE&apos;2001)</data>
      <data key="e_abstract">EIA 0105186&lt;br/&gt; Turner, Albert J. &lt;br/&gt; Parrish, Edward A. &lt;br/&gt;Association for Computing Machinery&lt;br/&gt;&lt;br/&gt;Special Projects: Group Travel to 7th IFIP World Conference on Computers and Education (WCCE2001) &lt;br/&gt;&lt;br/&gt;This proposal requests support to bring US participants to a major international conference on the use of information and communication technologies in education and on the teaching of computer science. The travel program will be administered jointly by the Association for Computing Machinery in association with the IEEE Computer Society.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">105186</data>
      <data key="e_expirationDate">2002-03-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">105186</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n154" target="n155">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">Source and Channel Coding for Multidimensional Channels</data>
      <data key="e_abstract">This project studies the theory of multidimensional channels and investigates effective coding techniques for such channels. The coding techniques are of three types: error-correction channel coding, constrained coding, and joint source-channel coding. Emphasis is placed on applications to two-dimensional magnetic and optical recording as well as three-dimensional holographic storage. These are the storage devices of the future. An important application of this research within the next few years is the storage of massive amounts of&lt;br/&gt;still imagery and video on two-dimensional media. This will likely extend to three-dimensional and four-dimensional (the fourth dimension is wavelength) devices over the next 5 to 10 years. Such multidimensional devices will require a shift in paradigm, since most of the existing theory for&lt;br/&gt;error-correcting codes, constrained codes, and source-channel codes was developed in the context of one-dimensional applications. There is much to be gained by coding for multidimensional channels, but the problems associated with such channels are considerably more challenging than their one-dimensional&lt;br/&gt;counterparts.&lt;br/&gt;&lt;br/&gt;Interesting technical problems arise due to the spatially dependent nature of errors in multidimensional storage media. New error-correcting codes and interleaving techniques are needed to effectively protect data stored on such media. The physical properties of optical and holographic recording channels call for a new theory of constrained coding in multiple dimensions. New joint source-channel coding techniques and theory are needed to maximize the recovered source fidelity for images and video stored on multidimensional&lt;br/&gt;devices while keeping the storage density as high as possible. Inparticular, the storage capacity of multidimensional devices can be greatly increased at the expense of a less reliable recovery of the stored imagery/video than is current practice for the storage of data. This project studies the tradeoff&lt;br/&gt;between increased storage capacity and quantitative loss in fidelity of the reproduced source signal. The main topics being investigated for multidimensional channels are: (1) Error-correcting codes, (2)&lt;br/&gt;Interleaving techniques, (3) Soft-decision decoding, (4) Capacity computation for constrained channels, (5)~Encoders and decoders for specific constraints, (6) Joint source-channel coder design.</data>
      <data key="e_pgm">4096</data>
      <data key="e_label">73489</data>
      <data key="e_expirationDate">2005-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">73489</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n156" target="n157">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">GK-12 Environment, Materials Science, and Information Technology Themes in Eighth, Ninth and Tenth Grades</data>
      <data key="e_abstract">We propose a collaboration between Harvard and the Cambridge Public Schools to help public school students learn science and technology. Cambridge teachers follow a curriculum which meets standards of the Massachusetts Comprehensive Assessment System (MCAS) and prepares students for state-wide examinations. Our plan is based on teams, each consisting of one faculty member, three GK-12 Fellows, and three Cambridge teachers. Each team will choose a topic from one of three areas: the environment, materials science, and information technology. During the first half year the team will meet weekly to introduce Cambridge teachers to research at Harvard and to discuss how to involve students in investigations related to the Cambridge curriculum and MCAS standards. In the second half year, GK- 12 Fellows will move to the Cambridge Public Schools to help teachers and students in discussions and student projects. A workshop will be held at the end of the year in which Cambridge students present their results to an audience of students and parents. We will start with eighth grade students and move to the ninth and tenth grades in following years. These activities will help Cambridge students learn science and technology and help GK- 12 Fellows become more involved in public education.&lt;br/&gt;&lt;br/&gt;This award is co-supported by the Office of Multidisciplinary Activities in the Directorate for Mathematical and Physical Sciences.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">86387</data>
      <data key="e_expirationDate">2007-01-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">86387</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n156" target="n158">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">GK-12 Environment, Materials Science, and Information Technology Themes in Eighth, Ninth and Tenth Grades</data>
      <data key="e_abstract">We propose a collaboration between Harvard and the Cambridge Public Schools to help public school students learn science and technology. Cambridge teachers follow a curriculum which meets standards of the Massachusetts Comprehensive Assessment System (MCAS) and prepares students for state-wide examinations. Our plan is based on teams, each consisting of one faculty member, three GK-12 Fellows, and three Cambridge teachers. Each team will choose a topic from one of three areas: the environment, materials science, and information technology. During the first half year the team will meet weekly to introduce Cambridge teachers to research at Harvard and to discuss how to involve students in investigations related to the Cambridge curriculum and MCAS standards. In the second half year, GK- 12 Fellows will move to the Cambridge Public Schools to help teachers and students in discussions and student projects. A workshop will be held at the end of the year in which Cambridge students present their results to an audience of students and parents. We will start with eighth grade students and move to the ninth and tenth grades in following years. These activities will help Cambridge students learn science and technology and help GK- 12 Fellows become more involved in public education.&lt;br/&gt;&lt;br/&gt;This award is co-supported by the Office of Multidisciplinary Activities in the Directorate for Mathematical and Physical Sciences.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">86387</data>
      <data key="e_expirationDate">2007-01-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">86387</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n156" target="n159">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">GK-12 Environment, Materials Science, and Information Technology Themes in Eighth, Ninth and Tenth Grades</data>
      <data key="e_abstract">We propose a collaboration between Harvard and the Cambridge Public Schools to help public school students learn science and technology. Cambridge teachers follow a curriculum which meets standards of the Massachusetts Comprehensive Assessment System (MCAS) and prepares students for state-wide examinations. Our plan is based on teams, each consisting of one faculty member, three GK-12 Fellows, and three Cambridge teachers. Each team will choose a topic from one of three areas: the environment, materials science, and information technology. During the first half year the team will meet weekly to introduce Cambridge teachers to research at Harvard and to discuss how to involve students in investigations related to the Cambridge curriculum and MCAS standards. In the second half year, GK- 12 Fellows will move to the Cambridge Public Schools to help teachers and students in discussions and student projects. A workshop will be held at the end of the year in which Cambridge students present their results to an audience of students and parents. We will start with eighth grade students and move to the ninth and tenth grades in following years. These activities will help Cambridge students learn science and technology and help GK- 12 Fellows become more involved in public education.&lt;br/&gt;&lt;br/&gt;This award is co-supported by the Office of Multidisciplinary Activities in the Directorate for Mathematical and Physical Sciences.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">86387</data>
      <data key="e_expirationDate">2007-01-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">86387</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n156" target="n160">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">GK-12 Environment, Materials Science, and Information Technology Themes in Eighth, Ninth and Tenth Grades</data>
      <data key="e_abstract">We propose a collaboration between Harvard and the Cambridge Public Schools to help public school students learn science and technology. Cambridge teachers follow a curriculum which meets standards of the Massachusetts Comprehensive Assessment System (MCAS) and prepares students for state-wide examinations. Our plan is based on teams, each consisting of one faculty member, three GK-12 Fellows, and three Cambridge teachers. Each team will choose a topic from one of three areas: the environment, materials science, and information technology. During the first half year the team will meet weekly to introduce Cambridge teachers to research at Harvard and to discuss how to involve students in investigations related to the Cambridge curriculum and MCAS standards. In the second half year, GK- 12 Fellows will move to the Cambridge Public Schools to help teachers and students in discussions and student projects. A workshop will be held at the end of the year in which Cambridge students present their results to an audience of students and parents. We will start with eighth grade students and move to the ninth and tenth grades in following years. These activities will help Cambridge students learn science and technology and help GK- 12 Fellows become more involved in public education.&lt;br/&gt;&lt;br/&gt;This award is co-supported by the Office of Multidisciplinary Activities in the Directorate for Mathematical and Physical Sciences.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">86387</data>
      <data key="e_expirationDate">2007-01-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">86387</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n157" target="n158">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">GK-12 Environment, Materials Science, and Information Technology Themes in Eighth, Ninth and Tenth Grades</data>
      <data key="e_abstract">We propose a collaboration between Harvard and the Cambridge Public Schools to help public school students learn science and technology. Cambridge teachers follow a curriculum which meets standards of the Massachusetts Comprehensive Assessment System (MCAS) and prepares students for state-wide examinations. Our plan is based on teams, each consisting of one faculty member, three GK-12 Fellows, and three Cambridge teachers. Each team will choose a topic from one of three areas: the environment, materials science, and information technology. During the first half year the team will meet weekly to introduce Cambridge teachers to research at Harvard and to discuss how to involve students in investigations related to the Cambridge curriculum and MCAS standards. In the second half year, GK- 12 Fellows will move to the Cambridge Public Schools to help teachers and students in discussions and student projects. A workshop will be held at the end of the year in which Cambridge students present their results to an audience of students and parents. We will start with eighth grade students and move to the ninth and tenth grades in following years. These activities will help Cambridge students learn science and technology and help GK- 12 Fellows become more involved in public education.&lt;br/&gt;&lt;br/&gt;This award is co-supported by the Office of Multidisciplinary Activities in the Directorate for Mathematical and Physical Sciences.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">86387</data>
      <data key="e_expirationDate">2007-01-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">86387</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n157" target="n159">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">GK-12 Environment, Materials Science, and Information Technology Themes in Eighth, Ninth and Tenth Grades</data>
      <data key="e_abstract">We propose a collaboration between Harvard and the Cambridge Public Schools to help public school students learn science and technology. Cambridge teachers follow a curriculum which meets standards of the Massachusetts Comprehensive Assessment System (MCAS) and prepares students for state-wide examinations. Our plan is based on teams, each consisting of one faculty member, three GK-12 Fellows, and three Cambridge teachers. Each team will choose a topic from one of three areas: the environment, materials science, and information technology. During the first half year the team will meet weekly to introduce Cambridge teachers to research at Harvard and to discuss how to involve students in investigations related to the Cambridge curriculum and MCAS standards. In the second half year, GK- 12 Fellows will move to the Cambridge Public Schools to help teachers and students in discussions and student projects. A workshop will be held at the end of the year in which Cambridge students present their results to an audience of students and parents. We will start with eighth grade students and move to the ninth and tenth grades in following years. These activities will help Cambridge students learn science and technology and help GK- 12 Fellows become more involved in public education.&lt;br/&gt;&lt;br/&gt;This award is co-supported by the Office of Multidisciplinary Activities in the Directorate for Mathematical and Physical Sciences.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">86387</data>
      <data key="e_expirationDate">2007-01-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">86387</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n157" target="n160">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">GK-12 Environment, Materials Science, and Information Technology Themes in Eighth, Ninth and Tenth Grades</data>
      <data key="e_abstract">We propose a collaboration between Harvard and the Cambridge Public Schools to help public school students learn science and technology. Cambridge teachers follow a curriculum which meets standards of the Massachusetts Comprehensive Assessment System (MCAS) and prepares students for state-wide examinations. Our plan is based on teams, each consisting of one faculty member, three GK-12 Fellows, and three Cambridge teachers. Each team will choose a topic from one of three areas: the environment, materials science, and information technology. During the first half year the team will meet weekly to introduce Cambridge teachers to research at Harvard and to discuss how to involve students in investigations related to the Cambridge curriculum and MCAS standards. In the second half year, GK- 12 Fellows will move to the Cambridge Public Schools to help teachers and students in discussions and student projects. A workshop will be held at the end of the year in which Cambridge students present their results to an audience of students and parents. We will start with eighth grade students and move to the ninth and tenth grades in following years. These activities will help Cambridge students learn science and technology and help GK- 12 Fellows become more involved in public education.&lt;br/&gt;&lt;br/&gt;This award is co-supported by the Office of Multidisciplinary Activities in the Directorate for Mathematical and Physical Sciences.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">86387</data>
      <data key="e_expirationDate">2007-01-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">86387</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n158" target="n159">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">GK-12 Environment, Materials Science, and Information Technology Themes in Eighth, Ninth and Tenth Grades</data>
      <data key="e_abstract">We propose a collaboration between Harvard and the Cambridge Public Schools to help public school students learn science and technology. Cambridge teachers follow a curriculum which meets standards of the Massachusetts Comprehensive Assessment System (MCAS) and prepares students for state-wide examinations. Our plan is based on teams, each consisting of one faculty member, three GK-12 Fellows, and three Cambridge teachers. Each team will choose a topic from one of three areas: the environment, materials science, and information technology. During the first half year the team will meet weekly to introduce Cambridge teachers to research at Harvard and to discuss how to involve students in investigations related to the Cambridge curriculum and MCAS standards. In the second half year, GK- 12 Fellows will move to the Cambridge Public Schools to help teachers and students in discussions and student projects. A workshop will be held at the end of the year in which Cambridge students present their results to an audience of students and parents. We will start with eighth grade students and move to the ninth and tenth grades in following years. These activities will help Cambridge students learn science and technology and help GK- 12 Fellows become more involved in public education.&lt;br/&gt;&lt;br/&gt;This award is co-supported by the Office of Multidisciplinary Activities in the Directorate for Mathematical and Physical Sciences.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">86387</data>
      <data key="e_expirationDate">2007-01-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">86387</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n158" target="n160">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">GK-12 Environment, Materials Science, and Information Technology Themes in Eighth, Ninth and Tenth Grades</data>
      <data key="e_abstract">We propose a collaboration between Harvard and the Cambridge Public Schools to help public school students learn science and technology. Cambridge teachers follow a curriculum which meets standards of the Massachusetts Comprehensive Assessment System (MCAS) and prepares students for state-wide examinations. Our plan is based on teams, each consisting of one faculty member, three GK-12 Fellows, and three Cambridge teachers. Each team will choose a topic from one of three areas: the environment, materials science, and information technology. During the first half year the team will meet weekly to introduce Cambridge teachers to research at Harvard and to discuss how to involve students in investigations related to the Cambridge curriculum and MCAS standards. In the second half year, GK- 12 Fellows will move to the Cambridge Public Schools to help teachers and students in discussions and student projects. A workshop will be held at the end of the year in which Cambridge students present their results to an audience of students and parents. We will start with eighth grade students and move to the ninth and tenth grades in following years. These activities will help Cambridge students learn science and technology and help GK- 12 Fellows become more involved in public education.&lt;br/&gt;&lt;br/&gt;This award is co-supported by the Office of Multidisciplinary Activities in the Directorate for Mathematical and Physical Sciences.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">86387</data>
      <data key="e_expirationDate">2007-01-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">86387</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n159" target="n160">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">GK-12 Environment, Materials Science, and Information Technology Themes in Eighth, Ninth and Tenth Grades</data>
      <data key="e_abstract">We propose a collaboration between Harvard and the Cambridge Public Schools to help public school students learn science and technology. Cambridge teachers follow a curriculum which meets standards of the Massachusetts Comprehensive Assessment System (MCAS) and prepares students for state-wide examinations. Our plan is based on teams, each consisting of one faculty member, three GK-12 Fellows, and three Cambridge teachers. Each team will choose a topic from one of three areas: the environment, materials science, and information technology. During the first half year the team will meet weekly to introduce Cambridge teachers to research at Harvard and to discuss how to involve students in investigations related to the Cambridge curriculum and MCAS standards. In the second half year, GK- 12 Fellows will move to the Cambridge Public Schools to help teachers and students in discussions and student projects. A workshop will be held at the end of the year in which Cambridge students present their results to an audience of students and parents. We will start with eighth grade students and move to the ninth and tenth grades in following years. These activities will help Cambridge students learn science and technology and help GK- 12 Fellows become more involved in public education.&lt;br/&gt;&lt;br/&gt;This award is co-supported by the Office of Multidisciplinary Activities in the Directorate for Mathematical and Physical Sciences.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">86387</data>
      <data key="e_expirationDate">2007-01-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">86387</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n164" target="n165">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">Digital Government: Social Processes and Content in Intelink Online Chat Data</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">2885</data>
      <data key="e_label">196374</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">196374</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n166" target="n167">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">Special Projects: Impact of Information Technology on the Future of the Research University</data>
      <data key="e_abstract">EIA 0102264 &lt;br/&gt;National Academy of Sciences &lt;br/&gt;T.H. Moss &lt;br/&gt;&lt;br/&gt;Impact of Information Technology on the Future of the Research University &lt;br/&gt; &lt;br/&gt;&lt;br/&gt;This is a workshop proposal to consider research university responses to new challenges and opportunities presented by information technology with a focus on the need for future policies to ensure the evolving research enterprise will be capable of addressing national needs. The results of the workshop will be broadly disseminated. Participants include members of diverse constituencies: early faculty adopters in innovative uses of information technology, intellectual and administrative university leaders, and representatives of government and the private sector.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">102264</data>
      <data key="e_expirationDate">2002-03-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">102264</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n168" target="n169">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">Target-Based Document-Independent Information Extraction</data>
      <data key="e_abstract">Target-Based Document-Independent Information Extraction&lt;br/&gt;&lt;br/&gt; With ever-growing volumes of data in widely varying formats, there is a need to sift and funnel information to users to meet their own specific requirements. This project addresses the challenge of finding, extracting, and delivering appropriate data by developing a versatile framework that is target-based (i.e., based on a user&apos;s description of the desired information) and document-independent (i.e., robust, not failing whenever documents change or when new documents of interest are encountered). A combination of document-related clues regarding textual content as well as geometrical and organizational layout enables processing across various document formats. Developers and users specify areas of interest via descriptive ontologies (i.e., declarations of information types and concept relationships). These ontologies facilitate reformulating, matching, and merging retrieved information. The result of these efforts will be a comprehensive infrastructure to extract expertly, organize automatically, and summarize succinctly critical information in a queriable personalized view. An online repository will contain research results, downloadable software (including source code), and a Web interface enabling user access to the various tools and engines developed. Potentially, this technology can be embedded in personal agents; leveraged in customized search, filtering, and extraction tools; and used to provide tailored views of data via integration, organization, and summarization. http://www.deg.byu.edu</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">83127</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83127</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n168" target="n170">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">Target-Based Document-Independent Information Extraction</data>
      <data key="e_abstract">Target-Based Document-Independent Information Extraction&lt;br/&gt;&lt;br/&gt; With ever-growing volumes of data in widely varying formats, there is a need to sift and funnel information to users to meet their own specific requirements. This project addresses the challenge of finding, extracting, and delivering appropriate data by developing a versatile framework that is target-based (i.e., based on a user&apos;s description of the desired information) and document-independent (i.e., robust, not failing whenever documents change or when new documents of interest are encountered). A combination of document-related clues regarding textual content as well as geometrical and organizational layout enables processing across various document formats. Developers and users specify areas of interest via descriptive ontologies (i.e., declarations of information types and concept relationships). These ontologies facilitate reformulating, matching, and merging retrieved information. The result of these efforts will be a comprehensive infrastructure to extract expertly, organize automatically, and summarize succinctly critical information in a queriable personalized view. An online repository will contain research results, downloadable software (including source code), and a Web interface enabling user access to the various tools and engines developed. Potentially, this technology can be embedded in personal agents; leveraged in customized search, filtering, and extraction tools; and used to provide tailored views of data via integration, organization, and summarization. http://www.deg.byu.edu</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">83127</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83127</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n168" target="n171">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">Target-Based Document-Independent Information Extraction</data>
      <data key="e_abstract">Target-Based Document-Independent Information Extraction&lt;br/&gt;&lt;br/&gt; With ever-growing volumes of data in widely varying formats, there is a need to sift and funnel information to users to meet their own specific requirements. This project addresses the challenge of finding, extracting, and delivering appropriate data by developing a versatile framework that is target-based (i.e., based on a user&apos;s description of the desired information) and document-independent (i.e., robust, not failing whenever documents change or when new documents of interest are encountered). A combination of document-related clues regarding textual content as well as geometrical and organizational layout enables processing across various document formats. Developers and users specify areas of interest via descriptive ontologies (i.e., declarations of information types and concept relationships). These ontologies facilitate reformulating, matching, and merging retrieved information. The result of these efforts will be a comprehensive infrastructure to extract expertly, organize automatically, and summarize succinctly critical information in a queriable personalized view. An online repository will contain research results, downloadable software (including source code), and a Web interface enabling user access to the various tools and engines developed. Potentially, this technology can be embedded in personal agents; leveraged in customized search, filtering, and extraction tools; and used to provide tailored views of data via integration, organization, and summarization. http://www.deg.byu.edu</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">83127</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83127</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n168" target="n172">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">Target-Based Document-Independent Information Extraction</data>
      <data key="e_abstract">Target-Based Document-Independent Information Extraction&lt;br/&gt;&lt;br/&gt; With ever-growing volumes of data in widely varying formats, there is a need to sift and funnel information to users to meet their own specific requirements. This project addresses the challenge of finding, extracting, and delivering appropriate data by developing a versatile framework that is target-based (i.e., based on a user&apos;s description of the desired information) and document-independent (i.e., robust, not failing whenever documents change or when new documents of interest are encountered). A combination of document-related clues regarding textual content as well as geometrical and organizational layout enables processing across various document formats. Developers and users specify areas of interest via descriptive ontologies (i.e., declarations of information types and concept relationships). These ontologies facilitate reformulating, matching, and merging retrieved information. The result of these efforts will be a comprehensive infrastructure to extract expertly, organize automatically, and summarize succinctly critical information in a queriable personalized view. An online repository will contain research results, downloadable software (including source code), and a Web interface enabling user access to the various tools and engines developed. Potentially, this technology can be embedded in personal agents; leveraged in customized search, filtering, and extraction tools; and used to provide tailored views of data via integration, organization, and summarization. http://www.deg.byu.edu</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">83127</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83127</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n169" target="n170">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">Target-Based Document-Independent Information Extraction</data>
      <data key="e_abstract">Target-Based Document-Independent Information Extraction&lt;br/&gt;&lt;br/&gt; With ever-growing volumes of data in widely varying formats, there is a need to sift and funnel information to users to meet their own specific requirements. This project addresses the challenge of finding, extracting, and delivering appropriate data by developing a versatile framework that is target-based (i.e., based on a user&apos;s description of the desired information) and document-independent (i.e., robust, not failing whenever documents change or when new documents of interest are encountered). A combination of document-related clues regarding textual content as well as geometrical and organizational layout enables processing across various document formats. Developers and users specify areas of interest via descriptive ontologies (i.e., declarations of information types and concept relationships). These ontologies facilitate reformulating, matching, and merging retrieved information. The result of these efforts will be a comprehensive infrastructure to extract expertly, organize automatically, and summarize succinctly critical information in a queriable personalized view. An online repository will contain research results, downloadable software (including source code), and a Web interface enabling user access to the various tools and engines developed. Potentially, this technology can be embedded in personal agents; leveraged in customized search, filtering, and extraction tools; and used to provide tailored views of data via integration, organization, and summarization. http://www.deg.byu.edu</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">83127</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83127</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n169" target="n171">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">Target-Based Document-Independent Information Extraction</data>
      <data key="e_abstract">Target-Based Document-Independent Information Extraction&lt;br/&gt;&lt;br/&gt; With ever-growing volumes of data in widely varying formats, there is a need to sift and funnel information to users to meet their own specific requirements. This project addresses the challenge of finding, extracting, and delivering appropriate data by developing a versatile framework that is target-based (i.e., based on a user&apos;s description of the desired information) and document-independent (i.e., robust, not failing whenever documents change or when new documents of interest are encountered). A combination of document-related clues regarding textual content as well as geometrical and organizational layout enables processing across various document formats. Developers and users specify areas of interest via descriptive ontologies (i.e., declarations of information types and concept relationships). These ontologies facilitate reformulating, matching, and merging retrieved information. The result of these efforts will be a comprehensive infrastructure to extract expertly, organize automatically, and summarize succinctly critical information in a queriable personalized view. An online repository will contain research results, downloadable software (including source code), and a Web interface enabling user access to the various tools and engines developed. Potentially, this technology can be embedded in personal agents; leveraged in customized search, filtering, and extraction tools; and used to provide tailored views of data via integration, organization, and summarization. http://www.deg.byu.edu</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">83127</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83127</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n169" target="n172">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">Target-Based Document-Independent Information Extraction</data>
      <data key="e_abstract">Target-Based Document-Independent Information Extraction&lt;br/&gt;&lt;br/&gt; With ever-growing volumes of data in widely varying formats, there is a need to sift and funnel information to users to meet their own specific requirements. This project addresses the challenge of finding, extracting, and delivering appropriate data by developing a versatile framework that is target-based (i.e., based on a user&apos;s description of the desired information) and document-independent (i.e., robust, not failing whenever documents change or when new documents of interest are encountered). A combination of document-related clues regarding textual content as well as geometrical and organizational layout enables processing across various document formats. Developers and users specify areas of interest via descriptive ontologies (i.e., declarations of information types and concept relationships). These ontologies facilitate reformulating, matching, and merging retrieved information. The result of these efforts will be a comprehensive infrastructure to extract expertly, organize automatically, and summarize succinctly critical information in a queriable personalized view. An online repository will contain research results, downloadable software (including source code), and a Web interface enabling user access to the various tools and engines developed. Potentially, this technology can be embedded in personal agents; leveraged in customized search, filtering, and extraction tools; and used to provide tailored views of data via integration, organization, and summarization. http://www.deg.byu.edu</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">83127</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83127</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n170" target="n171">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">Target-Based Document-Independent Information Extraction</data>
      <data key="e_abstract">Target-Based Document-Independent Information Extraction&lt;br/&gt;&lt;br/&gt; With ever-growing volumes of data in widely varying formats, there is a need to sift and funnel information to users to meet their own specific requirements. This project addresses the challenge of finding, extracting, and delivering appropriate data by developing a versatile framework that is target-based (i.e., based on a user&apos;s description of the desired information) and document-independent (i.e., robust, not failing whenever documents change or when new documents of interest are encountered). A combination of document-related clues regarding textual content as well as geometrical and organizational layout enables processing across various document formats. Developers and users specify areas of interest via descriptive ontologies (i.e., declarations of information types and concept relationships). These ontologies facilitate reformulating, matching, and merging retrieved information. The result of these efforts will be a comprehensive infrastructure to extract expertly, organize automatically, and summarize succinctly critical information in a queriable personalized view. An online repository will contain research results, downloadable software (including source code), and a Web interface enabling user access to the various tools and engines developed. Potentially, this technology can be embedded in personal agents; leveraged in customized search, filtering, and extraction tools; and used to provide tailored views of data via integration, organization, and summarization. http://www.deg.byu.edu</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">83127</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83127</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n170" target="n172">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">Target-Based Document-Independent Information Extraction</data>
      <data key="e_abstract">Target-Based Document-Independent Information Extraction&lt;br/&gt;&lt;br/&gt; With ever-growing volumes of data in widely varying formats, there is a need to sift and funnel information to users to meet their own specific requirements. This project addresses the challenge of finding, extracting, and delivering appropriate data by developing a versatile framework that is target-based (i.e., based on a user&apos;s description of the desired information) and document-independent (i.e., robust, not failing whenever documents change or when new documents of interest are encountered). A combination of document-related clues regarding textual content as well as geometrical and organizational layout enables processing across various document formats. Developers and users specify areas of interest via descriptive ontologies (i.e., declarations of information types and concept relationships). These ontologies facilitate reformulating, matching, and merging retrieved information. The result of these efforts will be a comprehensive infrastructure to extract expertly, organize automatically, and summarize succinctly critical information in a queriable personalized view. An online repository will contain research results, downloadable software (including source code), and a Web interface enabling user access to the various tools and engines developed. Potentially, this technology can be embedded in personal agents; leveraged in customized search, filtering, and extraction tools; and used to provide tailored views of data via integration, organization, and summarization. http://www.deg.byu.edu</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">83127</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83127</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n171" target="n172">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">Target-Based Document-Independent Information Extraction</data>
      <data key="e_abstract">Target-Based Document-Independent Information Extraction&lt;br/&gt;&lt;br/&gt; With ever-growing volumes of data in widely varying formats, there is a need to sift and funnel information to users to meet their own specific requirements. This project addresses the challenge of finding, extracting, and delivering appropriate data by developing a versatile framework that is target-based (i.e., based on a user&apos;s description of the desired information) and document-independent (i.e., robust, not failing whenever documents change or when new documents of interest are encountered). A combination of document-related clues regarding textual content as well as geometrical and organizational layout enables processing across various document formats. Developers and users specify areas of interest via descriptive ontologies (i.e., declarations of information types and concept relationships). These ontologies facilitate reformulating, matching, and merging retrieved information. The result of these efforts will be a comprehensive infrastructure to extract expertly, organize automatically, and summarize succinctly critical information in a queriable personalized view. An online repository will contain research results, downloadable software (including source code), and a Web interface enabling user access to the various tools and engines developed. Potentially, this technology can be embedded in personal agents; leveraged in customized search, filtering, and extraction tools; and used to provide tailored views of data via integration, organization, and summarization. http://www.deg.byu.edu</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">83127</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83127</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n178" target="n179">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">REU Site: Physics Research Experience for Undergraduates at the University of California, San Diego</data>
      <data key="e_abstract">The Department of Physics at the University of California, San Diego proposes to offer summer research training to fifteen undergraduate students per year for the next 5 years. Applications will be considered from students across the nation. Students will participate in an individual research project supervised by a Physics Department faculty member. Student projects will include experimental, theoretical, and computational research in high energy physics, condensed matter physics, biophysics, plasma physics, astrophysics and nonlinear physics. The research projects will be designed to guide students from a dependent research status to one that will allow each student to become as independent as their research skills and abilities permit. On arriving, students will work with their faculty advisor to prepare and present a proposed project. At the end of the summer, they will write a final research report and give a presentation at a campus wide Undergraduate Research Conference run by the UCSD Academic Enrichment Office. The REU program includes physics seminars by faculty, regular meetings with one of the program directors, seminars/workshops on computer usage, laboratory safety and library research, and joint scientific seminars and meetings with groups from other departments. Students will have the opportunity to attend workshops on graduate school admission and fellowship procedures, how to write a research paper, public speaking, instructional technology and GRE training.</data>
      <data key="e_pgm">9134</data>
      <data key="e_label">97854</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">97854</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n178" target="n180">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">REU Site: Physics Research Experience for Undergraduates at the University of California, San Diego</data>
      <data key="e_abstract">The Department of Physics at the University of California, San Diego proposes to offer summer research training to fifteen undergraduate students per year for the next 5 years. Applications will be considered from students across the nation. Students will participate in an individual research project supervised by a Physics Department faculty member. Student projects will include experimental, theoretical, and computational research in high energy physics, condensed matter physics, biophysics, plasma physics, astrophysics and nonlinear physics. The research projects will be designed to guide students from a dependent research status to one that will allow each student to become as independent as their research skills and abilities permit. On arriving, students will work with their faculty advisor to prepare and present a proposed project. At the end of the summer, they will write a final research report and give a presentation at a campus wide Undergraduate Research Conference run by the UCSD Academic Enrichment Office. The REU program includes physics seminars by faculty, regular meetings with one of the program directors, seminars/workshops on computer usage, laboratory safety and library research, and joint scientific seminars and meetings with groups from other departments. Students will have the opportunity to attend workshops on graduate school admission and fellowship procedures, how to write a research paper, public speaking, instructional technology and GRE training.</data>
      <data key="e_pgm">9134</data>
      <data key="e_label">97854</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">97854</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n178" target="n181">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">REU Site: Physics Research Experience for Undergraduates at the University of California, San Diego</data>
      <data key="e_abstract">The Department of Physics at the University of California, San Diego proposes to offer summer research training to fifteen undergraduate students per year for the next 5 years. Applications will be considered from students across the nation. Students will participate in an individual research project supervised by a Physics Department faculty member. Student projects will include experimental, theoretical, and computational research in high energy physics, condensed matter physics, biophysics, plasma physics, astrophysics and nonlinear physics. The research projects will be designed to guide students from a dependent research status to one that will allow each student to become as independent as their research skills and abilities permit. On arriving, students will work with their faculty advisor to prepare and present a proposed project. At the end of the summer, they will write a final research report and give a presentation at a campus wide Undergraduate Research Conference run by the UCSD Academic Enrichment Office. The REU program includes physics seminars by faculty, regular meetings with one of the program directors, seminars/workshops on computer usage, laboratory safety and library research, and joint scientific seminars and meetings with groups from other departments. Students will have the opportunity to attend workshops on graduate school admission and fellowship procedures, how to write a research paper, public speaking, instructional technology and GRE training.</data>
      <data key="e_pgm">9134</data>
      <data key="e_label">97854</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">97854</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n178" target="n182">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">REU Site: Physics Research Experience for Undergraduates at the University of California, San Diego</data>
      <data key="e_abstract">The Department of Physics at the University of California, San Diego proposes to offer summer research training to fifteen undergraduate students per year for the next 5 years. Applications will be considered from students across the nation. Students will participate in an individual research project supervised by a Physics Department faculty member. Student projects will include experimental, theoretical, and computational research in high energy physics, condensed matter physics, biophysics, plasma physics, astrophysics and nonlinear physics. The research projects will be designed to guide students from a dependent research status to one that will allow each student to become as independent as their research skills and abilities permit. On arriving, students will work with their faculty advisor to prepare and present a proposed project. At the end of the summer, they will write a final research report and give a presentation at a campus wide Undergraduate Research Conference run by the UCSD Academic Enrichment Office. The REU program includes physics seminars by faculty, regular meetings with one of the program directors, seminars/workshops on computer usage, laboratory safety and library research, and joint scientific seminars and meetings with groups from other departments. Students will have the opportunity to attend workshops on graduate school admission and fellowship procedures, how to write a research paper, public speaking, instructional technology and GRE training.</data>
      <data key="e_pgm">9134</data>
      <data key="e_label">97854</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">97854</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n178" target="n183">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">REU Site: Physics Research Experience for Undergraduates at the University of California, San Diego</data>
      <data key="e_abstract">The Department of Physics at the University of California, San Diego proposes to offer summer research training to fifteen undergraduate students per year for the next 5 years. Applications will be considered from students across the nation. Students will participate in an individual research project supervised by a Physics Department faculty member. Student projects will include experimental, theoretical, and computational research in high energy physics, condensed matter physics, biophysics, plasma physics, astrophysics and nonlinear physics. The research projects will be designed to guide students from a dependent research status to one that will allow each student to become as independent as their research skills and abilities permit. On arriving, students will work with their faculty advisor to prepare and present a proposed project. At the end of the summer, they will write a final research report and give a presentation at a campus wide Undergraduate Research Conference run by the UCSD Academic Enrichment Office. The REU program includes physics seminars by faculty, regular meetings with one of the program directors, seminars/workshops on computer usage, laboratory safety and library research, and joint scientific seminars and meetings with groups from other departments. Students will have the opportunity to attend workshops on graduate school admission and fellowship procedures, how to write a research paper, public speaking, instructional technology and GRE training.</data>
      <data key="e_pgm">9134</data>
      <data key="e_label">97854</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">97854</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n179" target="n180">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">REU Site: Physics Research Experience for Undergraduates at the University of California, San Diego</data>
      <data key="e_abstract">The Department of Physics at the University of California, San Diego proposes to offer summer research training to fifteen undergraduate students per year for the next 5 years. Applications will be considered from students across the nation. Students will participate in an individual research project supervised by a Physics Department faculty member. Student projects will include experimental, theoretical, and computational research in high energy physics, condensed matter physics, biophysics, plasma physics, astrophysics and nonlinear physics. The research projects will be designed to guide students from a dependent research status to one that will allow each student to become as independent as their research skills and abilities permit. On arriving, students will work with their faculty advisor to prepare and present a proposed project. At the end of the summer, they will write a final research report and give a presentation at a campus wide Undergraduate Research Conference run by the UCSD Academic Enrichment Office. The REU program includes physics seminars by faculty, regular meetings with one of the program directors, seminars/workshops on computer usage, laboratory safety and library research, and joint scientific seminars and meetings with groups from other departments. Students will have the opportunity to attend workshops on graduate school admission and fellowship procedures, how to write a research paper, public speaking, instructional technology and GRE training.</data>
      <data key="e_pgm">9134</data>
      <data key="e_label">97854</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">97854</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n179" target="n181">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">REU Site: Physics Research Experience for Undergraduates at the University of California, San Diego</data>
      <data key="e_abstract">The Department of Physics at the University of California, San Diego proposes to offer summer research training to fifteen undergraduate students per year for the next 5 years. Applications will be considered from students across the nation. Students will participate in an individual research project supervised by a Physics Department faculty member. Student projects will include experimental, theoretical, and computational research in high energy physics, condensed matter physics, biophysics, plasma physics, astrophysics and nonlinear physics. The research projects will be designed to guide students from a dependent research status to one that will allow each student to become as independent as their research skills and abilities permit. On arriving, students will work with their faculty advisor to prepare and present a proposed project. At the end of the summer, they will write a final research report and give a presentation at a campus wide Undergraduate Research Conference run by the UCSD Academic Enrichment Office. The REU program includes physics seminars by faculty, regular meetings with one of the program directors, seminars/workshops on computer usage, laboratory safety and library research, and joint scientific seminars and meetings with groups from other departments. Students will have the opportunity to attend workshops on graduate school admission and fellowship procedures, how to write a research paper, public speaking, instructional technology and GRE training.</data>
      <data key="e_pgm">9134</data>
      <data key="e_label">97854</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">97854</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n179" target="n182">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">REU Site: Physics Research Experience for Undergraduates at the University of California, San Diego</data>
      <data key="e_abstract">The Department of Physics at the University of California, San Diego proposes to offer summer research training to fifteen undergraduate students per year for the next 5 years. Applications will be considered from students across the nation. Students will participate in an individual research project supervised by a Physics Department faculty member. Student projects will include experimental, theoretical, and computational research in high energy physics, condensed matter physics, biophysics, plasma physics, astrophysics and nonlinear physics. The research projects will be designed to guide students from a dependent research status to one that will allow each student to become as independent as their research skills and abilities permit. On arriving, students will work with their faculty advisor to prepare and present a proposed project. At the end of the summer, they will write a final research report and give a presentation at a campus wide Undergraduate Research Conference run by the UCSD Academic Enrichment Office. The REU program includes physics seminars by faculty, regular meetings with one of the program directors, seminars/workshops on computer usage, laboratory safety and library research, and joint scientific seminars and meetings with groups from other departments. Students will have the opportunity to attend workshops on graduate school admission and fellowship procedures, how to write a research paper, public speaking, instructional technology and GRE training.</data>
      <data key="e_pgm">9134</data>
      <data key="e_label">97854</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">97854</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n179" target="n183">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">REU Site: Physics Research Experience for Undergraduates at the University of California, San Diego</data>
      <data key="e_abstract">The Department of Physics at the University of California, San Diego proposes to offer summer research training to fifteen undergraduate students per year for the next 5 years. Applications will be considered from students across the nation. Students will participate in an individual research project supervised by a Physics Department faculty member. Student projects will include experimental, theoretical, and computational research in high energy physics, condensed matter physics, biophysics, plasma physics, astrophysics and nonlinear physics. The research projects will be designed to guide students from a dependent research status to one that will allow each student to become as independent as their research skills and abilities permit. On arriving, students will work with their faculty advisor to prepare and present a proposed project. At the end of the summer, they will write a final research report and give a presentation at a campus wide Undergraduate Research Conference run by the UCSD Academic Enrichment Office. The REU program includes physics seminars by faculty, regular meetings with one of the program directors, seminars/workshops on computer usage, laboratory safety and library research, and joint scientific seminars and meetings with groups from other departments. Students will have the opportunity to attend workshops on graduate school admission and fellowship procedures, how to write a research paper, public speaking, instructional technology and GRE training.</data>
      <data key="e_pgm">9134</data>
      <data key="e_label">97854</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">97854</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n180" target="n181">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">REU Site: Physics Research Experience for Undergraduates at the University of California, San Diego</data>
      <data key="e_abstract">The Department of Physics at the University of California, San Diego proposes to offer summer research training to fifteen undergraduate students per year for the next 5 years. Applications will be considered from students across the nation. Students will participate in an individual research project supervised by a Physics Department faculty member. Student projects will include experimental, theoretical, and computational research in high energy physics, condensed matter physics, biophysics, plasma physics, astrophysics and nonlinear physics. The research projects will be designed to guide students from a dependent research status to one that will allow each student to become as independent as their research skills and abilities permit. On arriving, students will work with their faculty advisor to prepare and present a proposed project. At the end of the summer, they will write a final research report and give a presentation at a campus wide Undergraduate Research Conference run by the UCSD Academic Enrichment Office. The REU program includes physics seminars by faculty, regular meetings with one of the program directors, seminars/workshops on computer usage, laboratory safety and library research, and joint scientific seminars and meetings with groups from other departments. Students will have the opportunity to attend workshops on graduate school admission and fellowship procedures, how to write a research paper, public speaking, instructional technology and GRE training.</data>
      <data key="e_pgm">9134</data>
      <data key="e_label">97854</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">97854</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n180" target="n182">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">REU Site: Physics Research Experience for Undergraduates at the University of California, San Diego</data>
      <data key="e_abstract">The Department of Physics at the University of California, San Diego proposes to offer summer research training to fifteen undergraduate students per year for the next 5 years. Applications will be considered from students across the nation. Students will participate in an individual research project supervised by a Physics Department faculty member. Student projects will include experimental, theoretical, and computational research in high energy physics, condensed matter physics, biophysics, plasma physics, astrophysics and nonlinear physics. The research projects will be designed to guide students from a dependent research status to one that will allow each student to become as independent as their research skills and abilities permit. On arriving, students will work with their faculty advisor to prepare and present a proposed project. At the end of the summer, they will write a final research report and give a presentation at a campus wide Undergraduate Research Conference run by the UCSD Academic Enrichment Office. The REU program includes physics seminars by faculty, regular meetings with one of the program directors, seminars/workshops on computer usage, laboratory safety and library research, and joint scientific seminars and meetings with groups from other departments. Students will have the opportunity to attend workshops on graduate school admission and fellowship procedures, how to write a research paper, public speaking, instructional technology and GRE training.</data>
      <data key="e_pgm">9134</data>
      <data key="e_label">97854</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">97854</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n180" target="n183">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">REU Site: Physics Research Experience for Undergraduates at the University of California, San Diego</data>
      <data key="e_abstract">The Department of Physics at the University of California, San Diego proposes to offer summer research training to fifteen undergraduate students per year for the next 5 years. Applications will be considered from students across the nation. Students will participate in an individual research project supervised by a Physics Department faculty member. Student projects will include experimental, theoretical, and computational research in high energy physics, condensed matter physics, biophysics, plasma physics, astrophysics and nonlinear physics. The research projects will be designed to guide students from a dependent research status to one that will allow each student to become as independent as their research skills and abilities permit. On arriving, students will work with their faculty advisor to prepare and present a proposed project. At the end of the summer, they will write a final research report and give a presentation at a campus wide Undergraduate Research Conference run by the UCSD Academic Enrichment Office. The REU program includes physics seminars by faculty, regular meetings with one of the program directors, seminars/workshops on computer usage, laboratory safety and library research, and joint scientific seminars and meetings with groups from other departments. Students will have the opportunity to attend workshops on graduate school admission and fellowship procedures, how to write a research paper, public speaking, instructional technology and GRE training.</data>
      <data key="e_pgm">9134</data>
      <data key="e_label">97854</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">97854</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n181" target="n182">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">REU Site: Physics Research Experience for Undergraduates at the University of California, San Diego</data>
      <data key="e_abstract">The Department of Physics at the University of California, San Diego proposes to offer summer research training to fifteen undergraduate students per year for the next 5 years. Applications will be considered from students across the nation. Students will participate in an individual research project supervised by a Physics Department faculty member. Student projects will include experimental, theoretical, and computational research in high energy physics, condensed matter physics, biophysics, plasma physics, astrophysics and nonlinear physics. The research projects will be designed to guide students from a dependent research status to one that will allow each student to become as independent as their research skills and abilities permit. On arriving, students will work with their faculty advisor to prepare and present a proposed project. At the end of the summer, they will write a final research report and give a presentation at a campus wide Undergraduate Research Conference run by the UCSD Academic Enrichment Office. The REU program includes physics seminars by faculty, regular meetings with one of the program directors, seminars/workshops on computer usage, laboratory safety and library research, and joint scientific seminars and meetings with groups from other departments. Students will have the opportunity to attend workshops on graduate school admission and fellowship procedures, how to write a research paper, public speaking, instructional technology and GRE training.</data>
      <data key="e_pgm">9134</data>
      <data key="e_label">97854</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">97854</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n181" target="n183">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">REU Site: Physics Research Experience for Undergraduates at the University of California, San Diego</data>
      <data key="e_abstract">The Department of Physics at the University of California, San Diego proposes to offer summer research training to fifteen undergraduate students per year for the next 5 years. Applications will be considered from students across the nation. Students will participate in an individual research project supervised by a Physics Department faculty member. Student projects will include experimental, theoretical, and computational research in high energy physics, condensed matter physics, biophysics, plasma physics, astrophysics and nonlinear physics. The research projects will be designed to guide students from a dependent research status to one that will allow each student to become as independent as their research skills and abilities permit. On arriving, students will work with their faculty advisor to prepare and present a proposed project. At the end of the summer, they will write a final research report and give a presentation at a campus wide Undergraduate Research Conference run by the UCSD Academic Enrichment Office. The REU program includes physics seminars by faculty, regular meetings with one of the program directors, seminars/workshops on computer usage, laboratory safety and library research, and joint scientific seminars and meetings with groups from other departments. Students will have the opportunity to attend workshops on graduate school admission and fellowship procedures, how to write a research paper, public speaking, instructional technology and GRE training.</data>
      <data key="e_pgm">9134</data>
      <data key="e_label">97854</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">97854</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n182" target="n183">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">REU Site: Physics Research Experience for Undergraduates at the University of California, San Diego</data>
      <data key="e_abstract">The Department of Physics at the University of California, San Diego proposes to offer summer research training to fifteen undergraduate students per year for the next 5 years. Applications will be considered from students across the nation. Students will participate in an individual research project supervised by a Physics Department faculty member. Student projects will include experimental, theoretical, and computational research in high energy physics, condensed matter physics, biophysics, plasma physics, astrophysics and nonlinear physics. The research projects will be designed to guide students from a dependent research status to one that will allow each student to become as independent as their research skills and abilities permit. On arriving, students will work with their faculty advisor to prepare and present a proposed project. At the end of the summer, they will write a final research report and give a presentation at a campus wide Undergraduate Research Conference run by the UCSD Academic Enrichment Office. The REU program includes physics seminars by faculty, regular meetings with one of the program directors, seminars/workshops on computer usage, laboratory safety and library research, and joint scientific seminars and meetings with groups from other departments. Students will have the opportunity to attend workshops on graduate school admission and fellowship procedures, how to write a research paper, public speaking, instructional technology and GRE training.</data>
      <data key="e_pgm">9134</data>
      <data key="e_label">97854</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">97854</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n189" target="n190">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">Algorithms and Implementations for OFDM Based Wireless LAN</data>
      <data key="e_abstract">Wireless local area networks (WLANs) have the advantages of inexpensive network reconfiguration&lt;br/&gt;and user mobility over wired local area networks. However, WLANs are so far limited to niche applications due to their low data rate and data rate uncertainty. There is a tremendous user demand for WLANs with higher data rates and international availability. This research addresses the challenges of meeting the user demand. This research combines theoretical algorithm development with real-time implementation in hardware. It fosters multidisciplinary research and development among researchers at the University&lt;br/&gt;of Florida and the local industry as well as the technology transfer to the local industry.&lt;br/&gt;&lt;br/&gt;The main objective of the research is the development and application of efficient and robust parameter estimation and symbol detection algorithms and their real time implementations for high data rate WLANs. The investigators devise and evaluate efficient and robust parameter estimation and symbol detection algorithms for Orthogonal Frequency Division Multiplexing (OFDM) based WLANs with one or more transmit&lt;br/&gt;and receive antennas for both stationary and rapidly time-varying channels. The most effective algorithms are implemented efficiently in hardware in real time. The investigators study the tradeoffs of effectiveness, simplicity, computational complexity, and practical hardware implementation in real time.</data>
      <data key="e_pgm">4096</data>
      <data key="e_label">97114</data>
      <data key="e_expirationDate">2005-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">97114</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n192" target="n193">
      <data key="e_effectiveDate">2001-04-01</data>
      <data key="e_title">Travel Support for the 2001 IEEE International Symposium on Information Theory</data>
      <data key="e_abstract">The IEEE International Symposium on Information Theory (ISIT) is held yearly at sites alternating between the U.S. and abroad. The 2001 Symposium will be held at the Omni Shoreham Hotel in Washington, DC, USA on June 24-29, 2001.&lt;br/&gt;&lt;br/&gt;This proposal requests support for the travel expenses of ISIT participants from the U.S., Russia and Eastern Europe. These participants would be individuals whose papers have been selected for presentation at the Symposium, but who lack funds and would not be able to attend without travel support. It is proposed that the co-chairs of the symposium, Thomas Fuja and Prakash Narayan make the awards. The total amount requested is $40,000, split equally between U.S. and non-U.S. Participants; this is based on 40 U.S. awards (averaging $500 apiece) and 15 foreign awards (averaging $1333 apiece).</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">101993</data>
      <data key="e_expirationDate">2003-03-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">101993</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n194" target="n195">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">U.S.-France Cooperative Research (INRIA): Implementing a Cluster Version of Java with the PM2 Distributed and Multithreaded Run-Time System</data>
      <data key="e_abstract">0084330&lt;br/&gt;Hatcher&lt;br/&gt;&lt;br/&gt;This three-year award for U.S.-France cooperative research involves Philip J. Hatcher and Robert D. Russell of the University of New Hampshire and Luc Bouge&apos;s research group at the French National Institute for Research in Informatics and Applied Mathematics&apos;(INRIA) Laboratoire de l&apos;Informatique du Parallelisme in Lyon, France. The proposal addresses implementation of the Java programming language for parallel programs on cluster computers using the multithreading package PM2 developed by the French researchers. The work will be performed in the context of the Hyperion environment, developed at the University of New Hampshire and comprised of a Java-bytecode-to-C translator and a run-time library for the distributed execution of Java threads. The proposal will advance understanding of Java language as a tool for programming distributed memory parallel computers. &lt;br/&gt;&lt;br/&gt;This award represents the US side of a joint proposal to NSF and the French National Institute for Research in Informatics and Applied Mathematics (INRIA). NSF will cover travel funds and living expenses for US Investigator. INRIA will support visits of French researchers to the US.</data>
      <data key="e_pgm">5980</data>
      <data key="e_label">84330</data>
      <data key="e_expirationDate">2004-10-31</data>
      <data key="e_div">0109</data>
      <data key="e_awardID">84330</data>
      <data key="e_dir">01</data>
    </edge>
    <edge source="n196" target="n197">
      <data key="e_effectiveDate">2001-05-15</data>
      <data key="e_title">Agile Views for Video Browsing: Advanced Surrogates, Control Mechanisms, and Usability</data>
      <data key="e_abstract">This is the first year funding of a three year continuing award. This project will afford people agile views - effortless control over different representations - for digital video objects, by providing a multiplicity of indexes that are attuned to people&apos;s experiences, leverage the features of the content, and are easily and rapidly used and changed. The PI will specify such an interface, create prototype instantiations, and develop and apply procedures for testing the usability of the prototypes and specifications. Most research on digital video systems focuses on the retrieval of specific objects and on one or a few indexing attributes. This project is unique in three ways. First, it aims to address tasks at the collection level as well as at the item level; thus, in addition to the retrieval task, it aims to help people understand a video collection&apos;s structure, what is and is not available, and what attributes might be useful for retrieval purposes. Second, it aims to provide people with a range of surrogates and to integrate these into an effective and efficient interface; it aims to create an environment that provides multiple surrogates at the collection level as well as at the individual item level, and to provide novel control mechanisms to manage these alternatives. Third, it aims to assess user performance on these different tasks using these surrogates; evaluating browsing behavior is a challenge in any medium, and this work will build upon tasks and metrics developed in previous studies to go beyond traditional metrics to address time-benefit tradeoff measures. For this project, the video objects will be drawn from the Open Video Repository. Surrogates such as key-frames (in slide shows, storyboards, and skims), audio extracts, and keywords will be used as the basic representa-tions for specific video segments, and will be user-manipulable through a variety of interaction mechanisms. Additional surrogates (such as two-dimensional layouts of metadata and coordinated metadata lists) for collections of videos will also be developed, and additional views, i.e., histories (reviews), peripheral views, and shared views, will also be investigated within the agile views environment. The usability of the agile views environment (its individual components and the completely-integrated interface) will be evaluated iteratively, concluding with assessments of user performance (object and action recognition, and video comprehension), tradeoffs associated with viewing compaction rates, and user satisfaction with the interface. These evaluation techniques will themselves contribute to research and development by providing metrics and techniques for assessing interactive browsing. The ultimate goal of this work is to provide an information-rich and interactive environment that enables people to go beyond their innate visual and audio abilities to browse video content and process large volumes of video information.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">99538</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">99538</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n201" target="n201">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">CRCD: A Curriculum in Networked and Distributed Systems</data>
      <data key="e_abstract">EIA-0088081&lt;br/&gt;Walsh, Gregory C.&lt;br/&gt;University of Maryland&lt;br/&gt;&lt;br/&gt;CRCD: A Curriculum in Networked and Distributed Systems&lt;br/&gt;&lt;br/&gt;This project develops an innovative senior/masters-level curriculum designed to: (a) bring the important new technologies in Wireless and Networked Distributed Systems into the classroom, (b) make use of novel teaching and evaluation methods to enhance faculty productivity in laboratory and project courses, and (c) improve the educational value of students&apos; experiences in laboratory and project courses. The laboratory facilities, which are part of this work, also play a key role in enabling multi-disciplinary research in networks, communications, embedded systems, and controls. The project crosses several disciplinary boundaries and has clearly defined deliverables that expose students to bodies of knowledge in great demand in the workforce. Two leading companies in this emerging area, United Technologies and General Electric, mentor this project.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">88081</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88081</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n201" target="n203">
      <data key="e_effectiveDate">None</data>
      <data key="e_title">None</data>
      <data key="e_abstract">None</data>
      <data key="e_pgm">None</data>
      <data key="e_expirationDate">None</data>
      <data key="e_div">None</data>
      <data key="e_dir">None</data>
    </edge>
    <edge source="n201" target="n204">
      <data key="e_effectiveDate">None</data>
      <data key="e_title">None</data>
      <data key="e_abstract">None</data>
      <data key="e_pgm">None</data>
      <data key="e_expirationDate">None</data>
      <data key="e_div">None</data>
      <data key="e_dir">None</data>
    </edge>
    <edge source="n201" target="n205">
      <data key="e_effectiveDate">None</data>
      <data key="e_title">None</data>
      <data key="e_abstract">None</data>
      <data key="e_pgm">None</data>
      <data key="e_expirationDate">None</data>
      <data key="e_div">None</data>
      <data key="e_dir">None</data>
    </edge>
    <edge source="n201" target="n203">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">CRCD: A Curriculum in Networked and Distributed Systems</data>
      <data key="e_abstract">EIA-0088081&lt;br/&gt;Walsh, Gregory C.&lt;br/&gt;University of Maryland&lt;br/&gt;&lt;br/&gt;CRCD: A Curriculum in Networked and Distributed Systems&lt;br/&gt;&lt;br/&gt;This project develops an innovative senior/masters-level curriculum designed to: (a) bring the important new technologies in Wireless and Networked Distributed Systems into the classroom, (b) make use of novel teaching and evaluation methods to enhance faculty productivity in laboratory and project courses, and (c) improve the educational value of students&apos; experiences in laboratory and project courses. The laboratory facilities, which are part of this work, also play a key role in enabling multi-disciplinary research in networks, communications, embedded systems, and controls. The project crosses several disciplinary boundaries and has clearly defined deliverables that expose students to bodies of knowledge in great demand in the workforce. Two leading companies in this emerging area, United Technologies and General Electric, mentor this project.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">88081</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88081</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n201" target="n204">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">CRCD: A Curriculum in Networked and Distributed Systems</data>
      <data key="e_abstract">EIA-0088081&lt;br/&gt;Walsh, Gregory C.&lt;br/&gt;University of Maryland&lt;br/&gt;&lt;br/&gt;CRCD: A Curriculum in Networked and Distributed Systems&lt;br/&gt;&lt;br/&gt;This project develops an innovative senior/masters-level curriculum designed to: (a) bring the important new technologies in Wireless and Networked Distributed Systems into the classroom, (b) make use of novel teaching and evaluation methods to enhance faculty productivity in laboratory and project courses, and (c) improve the educational value of students&apos; experiences in laboratory and project courses. The laboratory facilities, which are part of this work, also play a key role in enabling multi-disciplinary research in networks, communications, embedded systems, and controls. The project crosses several disciplinary boundaries and has clearly defined deliverables that expose students to bodies of knowledge in great demand in the workforce. Two leading companies in this emerging area, United Technologies and General Electric, mentor this project.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">88081</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88081</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n201" target="n205">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">CRCD: A Curriculum in Networked and Distributed Systems</data>
      <data key="e_abstract">EIA-0088081&lt;br/&gt;Walsh, Gregory C.&lt;br/&gt;University of Maryland&lt;br/&gt;&lt;br/&gt;CRCD: A Curriculum in Networked and Distributed Systems&lt;br/&gt;&lt;br/&gt;This project develops an innovative senior/masters-level curriculum designed to: (a) bring the important new technologies in Wireless and Networked Distributed Systems into the classroom, (b) make use of novel teaching and evaluation methods to enhance faculty productivity in laboratory and project courses, and (c) improve the educational value of students&apos; experiences in laboratory and project courses. The laboratory facilities, which are part of this work, also play a key role in enabling multi-disciplinary research in networks, communications, embedded systems, and controls. The project crosses several disciplinary boundaries and has clearly defined deliverables that expose students to bodies of knowledge in great demand in the workforce. Two leading companies in this emerging area, United Technologies and General Electric, mentor this project.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">88081</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88081</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n203" target="n204">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">CRCD: A Curriculum in Networked and Distributed Systems</data>
      <data key="e_abstract">EIA-0088081&lt;br/&gt;Walsh, Gregory C.&lt;br/&gt;University of Maryland&lt;br/&gt;&lt;br/&gt;CRCD: A Curriculum in Networked and Distributed Systems&lt;br/&gt;&lt;br/&gt;This project develops an innovative senior/masters-level curriculum designed to: (a) bring the important new technologies in Wireless and Networked Distributed Systems into the classroom, (b) make use of novel teaching and evaluation methods to enhance faculty productivity in laboratory and project courses, and (c) improve the educational value of students&apos; experiences in laboratory and project courses. The laboratory facilities, which are part of this work, also play a key role in enabling multi-disciplinary research in networks, communications, embedded systems, and controls. The project crosses several disciplinary boundaries and has clearly defined deliverables that expose students to bodies of knowledge in great demand in the workforce. Two leading companies in this emerging area, United Technologies and General Electric, mentor this project.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">88081</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88081</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n203" target="n205">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">CRCD: A Curriculum in Networked and Distributed Systems</data>
      <data key="e_abstract">EIA-0088081&lt;br/&gt;Walsh, Gregory C.&lt;br/&gt;University of Maryland&lt;br/&gt;&lt;br/&gt;CRCD: A Curriculum in Networked and Distributed Systems&lt;br/&gt;&lt;br/&gt;This project develops an innovative senior/masters-level curriculum designed to: (a) bring the important new technologies in Wireless and Networked Distributed Systems into the classroom, (b) make use of novel teaching and evaluation methods to enhance faculty productivity in laboratory and project courses, and (c) improve the educational value of students&apos; experiences in laboratory and project courses. The laboratory facilities, which are part of this work, also play a key role in enabling multi-disciplinary research in networks, communications, embedded systems, and controls. The project crosses several disciplinary boundaries and has clearly defined deliverables that expose students to bodies of knowledge in great demand in the workforce. Two leading companies in this emerging area, United Technologies and General Electric, mentor this project.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">88081</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88081</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n204" target="n205">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">CRCD: A Curriculum in Networked and Distributed Systems</data>
      <data key="e_abstract">EIA-0088081&lt;br/&gt;Walsh, Gregory C.&lt;br/&gt;University of Maryland&lt;br/&gt;&lt;br/&gt;CRCD: A Curriculum in Networked and Distributed Systems&lt;br/&gt;&lt;br/&gt;This project develops an innovative senior/masters-level curriculum designed to: (a) bring the important new technologies in Wireless and Networked Distributed Systems into the classroom, (b) make use of novel teaching and evaluation methods to enhance faculty productivity in laboratory and project courses, and (c) improve the educational value of students&apos; experiences in laboratory and project courses. The laboratory facilities, which are part of this work, also play a key role in enabling multi-disciplinary research in networks, communications, embedded systems, and controls. The project crosses several disciplinary boundaries and has clearly defined deliverables that expose students to bodies of knowledge in great demand in the workforce. Two leading companies in this emerging area, United Technologies and General Electric, mentor this project.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">88081</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88081</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n209" target="n210">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">A Control-Theoretical Approach to Performance Guarantees in Performance-Critical Systems</data>
      <data key="e_abstract">Many important applications must provide guaranteed real-time &lt;br/&gt;performance in spite of uncertain workloads, highly varying &lt;br/&gt;computation times for tasks, and with a large number of &lt;br/&gt;interacting sites. Examples of such applications include &lt;br/&gt;smart spaces, financial markets on the Internet, collections &lt;br/&gt;of factories supporting agile manufacturing, and high-tech &lt;br/&gt;battlefield coordination. The objective of this research is &lt;br/&gt;to develop a software-oriented theory and practice of feedback &lt;br/&gt;control that will provide aggregate performance guarantees for &lt;br/&gt;these types of systems. Methods to embed such controllers in &lt;br/&gt;the operating systems of these types of applications is &lt;br/&gt;investigated. The ultimate vision of this research is that &lt;br/&gt;software designers will be able to model parts of software &lt;br/&gt;systems and use those models to develop software control &lt;br/&gt;algorithms based on a theory of feedback control.&lt;br/&gt;This work establishes a scientific basis upon which to design &lt;br/&gt;and analyze the aggregate behavior of large systems that operate &lt;br/&gt;under a great deal of uncertainty. The solutions can be embedded &lt;br/&gt;in operating systems to meet the performance specs in transient &lt;br/&gt;and steady states, such as deadline miss ratio, stability, &lt;br/&gt;overshoot, settling time, and sensitivity requirements.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">98269</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">98269</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n209" target="n211">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">A Control-Theoretical Approach to Performance Guarantees in Performance-Critical Systems</data>
      <data key="e_abstract">Many important applications must provide guaranteed real-time &lt;br/&gt;performance in spite of uncertain workloads, highly varying &lt;br/&gt;computation times for tasks, and with a large number of &lt;br/&gt;interacting sites. Examples of such applications include &lt;br/&gt;smart spaces, financial markets on the Internet, collections &lt;br/&gt;of factories supporting agile manufacturing, and high-tech &lt;br/&gt;battlefield coordination. The objective of this research is &lt;br/&gt;to develop a software-oriented theory and practice of feedback &lt;br/&gt;control that will provide aggregate performance guarantees for &lt;br/&gt;these types of systems. Methods to embed such controllers in &lt;br/&gt;the operating systems of these types of applications is &lt;br/&gt;investigated. The ultimate vision of this research is that &lt;br/&gt;software designers will be able to model parts of software &lt;br/&gt;systems and use those models to develop software control &lt;br/&gt;algorithms based on a theory of feedback control.&lt;br/&gt;This work establishes a scientific basis upon which to design &lt;br/&gt;and analyze the aggregate behavior of large systems that operate &lt;br/&gt;under a great deal of uncertainty. The solutions can be embedded &lt;br/&gt;in operating systems to meet the performance specs in transient &lt;br/&gt;and steady states, such as deadline miss ratio, stability, &lt;br/&gt;overshoot, settling time, and sensitivity requirements.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">98269</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">98269</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n209" target="n212">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">A Control-Theoretical Approach to Performance Guarantees in Performance-Critical Systems</data>
      <data key="e_abstract">Many important applications must provide guaranteed real-time &lt;br/&gt;performance in spite of uncertain workloads, highly varying &lt;br/&gt;computation times for tasks, and with a large number of &lt;br/&gt;interacting sites. Examples of such applications include &lt;br/&gt;smart spaces, financial markets on the Internet, collections &lt;br/&gt;of factories supporting agile manufacturing, and high-tech &lt;br/&gt;battlefield coordination. The objective of this research is &lt;br/&gt;to develop a software-oriented theory and practice of feedback &lt;br/&gt;control that will provide aggregate performance guarantees for &lt;br/&gt;these types of systems. Methods to embed such controllers in &lt;br/&gt;the operating systems of these types of applications is &lt;br/&gt;investigated. The ultimate vision of this research is that &lt;br/&gt;software designers will be able to model parts of software &lt;br/&gt;systems and use those models to develop software control &lt;br/&gt;algorithms based on a theory of feedback control.&lt;br/&gt;This work establishes a scientific basis upon which to design &lt;br/&gt;and analyze the aggregate behavior of large systems that operate &lt;br/&gt;under a great deal of uncertainty. The solutions can be embedded &lt;br/&gt;in operating systems to meet the performance specs in transient &lt;br/&gt;and steady states, such as deadline miss ratio, stability, &lt;br/&gt;overshoot, settling time, and sensitivity requirements.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">98269</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">98269</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n210" target="n211">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">A Control-Theoretical Approach to Performance Guarantees in Performance-Critical Systems</data>
      <data key="e_abstract">Many important applications must provide guaranteed real-time &lt;br/&gt;performance in spite of uncertain workloads, highly varying &lt;br/&gt;computation times for tasks, and with a large number of &lt;br/&gt;interacting sites. Examples of such applications include &lt;br/&gt;smart spaces, financial markets on the Internet, collections &lt;br/&gt;of factories supporting agile manufacturing, and high-tech &lt;br/&gt;battlefield coordination. The objective of this research is &lt;br/&gt;to develop a software-oriented theory and practice of feedback &lt;br/&gt;control that will provide aggregate performance guarantees for &lt;br/&gt;these types of systems. Methods to embed such controllers in &lt;br/&gt;the operating systems of these types of applications is &lt;br/&gt;investigated. The ultimate vision of this research is that &lt;br/&gt;software designers will be able to model parts of software &lt;br/&gt;systems and use those models to develop software control &lt;br/&gt;algorithms based on a theory of feedback control.&lt;br/&gt;This work establishes a scientific basis upon which to design &lt;br/&gt;and analyze the aggregate behavior of large systems that operate &lt;br/&gt;under a great deal of uncertainty. The solutions can be embedded &lt;br/&gt;in operating systems to meet the performance specs in transient &lt;br/&gt;and steady states, such as deadline miss ratio, stability, &lt;br/&gt;overshoot, settling time, and sensitivity requirements.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">98269</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">98269</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n210" target="n212">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">A Control-Theoretical Approach to Performance Guarantees in Performance-Critical Systems</data>
      <data key="e_abstract">Many important applications must provide guaranteed real-time &lt;br/&gt;performance in spite of uncertain workloads, highly varying &lt;br/&gt;computation times for tasks, and with a large number of &lt;br/&gt;interacting sites. Examples of such applications include &lt;br/&gt;smart spaces, financial markets on the Internet, collections &lt;br/&gt;of factories supporting agile manufacturing, and high-tech &lt;br/&gt;battlefield coordination. The objective of this research is &lt;br/&gt;to develop a software-oriented theory and practice of feedback &lt;br/&gt;control that will provide aggregate performance guarantees for &lt;br/&gt;these types of systems. Methods to embed such controllers in &lt;br/&gt;the operating systems of these types of applications is &lt;br/&gt;investigated. The ultimate vision of this research is that &lt;br/&gt;software designers will be able to model parts of software &lt;br/&gt;systems and use those models to develop software control &lt;br/&gt;algorithms based on a theory of feedback control.&lt;br/&gt;This work establishes a scientific basis upon which to design &lt;br/&gt;and analyze the aggregate behavior of large systems that operate &lt;br/&gt;under a great deal of uncertainty. The solutions can be embedded &lt;br/&gt;in operating systems to meet the performance specs in transient &lt;br/&gt;and steady states, such as deadline miss ratio, stability, &lt;br/&gt;overshoot, settling time, and sensitivity requirements.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">98269</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">98269</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n211" target="n212">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">A Control-Theoretical Approach to Performance Guarantees in Performance-Critical Systems</data>
      <data key="e_abstract">Many important applications must provide guaranteed real-time &lt;br/&gt;performance in spite of uncertain workloads, highly varying &lt;br/&gt;computation times for tasks, and with a large number of &lt;br/&gt;interacting sites. Examples of such applications include &lt;br/&gt;smart spaces, financial markets on the Internet, collections &lt;br/&gt;of factories supporting agile manufacturing, and high-tech &lt;br/&gt;battlefield coordination. The objective of this research is &lt;br/&gt;to develop a software-oriented theory and practice of feedback &lt;br/&gt;control that will provide aggregate performance guarantees for &lt;br/&gt;these types of systems. Methods to embed such controllers in &lt;br/&gt;the operating systems of these types of applications is &lt;br/&gt;investigated. The ultimate vision of this research is that &lt;br/&gt;software designers will be able to model parts of software &lt;br/&gt;systems and use those models to develop software control &lt;br/&gt;algorithms based on a theory of feedback control.&lt;br/&gt;This work establishes a scientific basis upon which to design &lt;br/&gt;and analyze the aggregate behavior of large systems that operate &lt;br/&gt;under a great deal of uncertainty. The solutions can be embedded &lt;br/&gt;in operating systems to meet the performance specs in transient &lt;br/&gt;and steady states, such as deadline miss ratio, stability, &lt;br/&gt;overshoot, settling time, and sensitivity requirements.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">98269</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">98269</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n215" target="n216">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">Mining Human Brain Data: Analysis, Classification and Visualization</data>
      <data key="e_abstract">This multidisciplinary research effort develops methods for content based retrieval, analysis and visualization of probabilistic 3D spatial maps, in particular, the statistical parametric maps obtained from the fMRI (functional Magnetic Resonance Imaging) analysis of the brain. The approach is based on &quot;activation signatures&quot; which represent fMRI brain activations by size, shape, number (of foci), location, density, orientation, and other parameters. These signatures are used in providing automatic classification of activations into classes. Since fMRI brain imaging is a non-invasive technique it has attracted enormous interest in recent years because of its potential to solve fundamental problems explaining cognitive processes and to find relations between brain structure and function. The environment developed in this project can fully utilize fMRI&apos;s potential by enabling inter- and intra- studies and support large-scale mining of fMRI brain activations. The techniques can extract associations at multiple levels: among different activations for the same or different (groups of) individuals, between activations and tasks, between activations and ancillary data, and between tasks and ancillary data. Automatic characterization tools open the field to numerous new research opportunities, as they can offer common signature formats to do cross-modality brain data searches and correlation over multiple studies. In turn, the mined results can speed up discovery in the neuroscience field. The methods will be demonstrated on a prototype robust database of brain activations that includes scan data as well as biographical, clinical, historical and other subject-centered ancillary data. This database supports retrieval requests and subsequent analysis incorporating spatial statistics, classification, and visualization techniques and provides an evaluation of the system by the professional communities. The results of this work will be applicable to the analysis of other brain data such as brain lesions, Positron Emission Tomography (PET) scans, and MRI scans; thus helping to create tools for search across brain data formats. In addition, objects similar to brain activations appear in many other different domains, including cellular biology, tumor analysis, fluid dynamics, or financial records, therefore, the resulting techniques are expected to have broad impact.&lt;br/&gt;http://devlab.dartmouth.edu/</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">83423</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83423</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n215" target="n217">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">Mining Human Brain Data: Analysis, Classification and Visualization</data>
      <data key="e_abstract">This multidisciplinary research effort develops methods for content based retrieval, analysis and visualization of probabilistic 3D spatial maps, in particular, the statistical parametric maps obtained from the fMRI (functional Magnetic Resonance Imaging) analysis of the brain. The approach is based on &quot;activation signatures&quot; which represent fMRI brain activations by size, shape, number (of foci), location, density, orientation, and other parameters. These signatures are used in providing automatic classification of activations into classes. Since fMRI brain imaging is a non-invasive technique it has attracted enormous interest in recent years because of its potential to solve fundamental problems explaining cognitive processes and to find relations between brain structure and function. The environment developed in this project can fully utilize fMRI&apos;s potential by enabling inter- and intra- studies and support large-scale mining of fMRI brain activations. The techniques can extract associations at multiple levels: among different activations for the same or different (groups of) individuals, between activations and tasks, between activations and ancillary data, and between tasks and ancillary data. Automatic characterization tools open the field to numerous new research opportunities, as they can offer common signature formats to do cross-modality brain data searches and correlation over multiple studies. In turn, the mined results can speed up discovery in the neuroscience field. The methods will be demonstrated on a prototype robust database of brain activations that includes scan data as well as biographical, clinical, historical and other subject-centered ancillary data. This database supports retrieval requests and subsequent analysis incorporating spatial statistics, classification, and visualization techniques and provides an evaluation of the system by the professional communities. The results of this work will be applicable to the analysis of other brain data such as brain lesions, Positron Emission Tomography (PET) scans, and MRI scans; thus helping to create tools for search across brain data formats. In addition, objects similar to brain activations appear in many other different domains, including cellular biology, tumor analysis, fluid dynamics, or financial records, therefore, the resulting techniques are expected to have broad impact.&lt;br/&gt;http://devlab.dartmouth.edu/</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">83423</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83423</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n215" target="n218">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">Mining Human Brain Data: Analysis, Classification and Visualization</data>
      <data key="e_abstract">This multidisciplinary research effort develops methods for content based retrieval, analysis and visualization of probabilistic 3D spatial maps, in particular, the statistical parametric maps obtained from the fMRI (functional Magnetic Resonance Imaging) analysis of the brain. The approach is based on &quot;activation signatures&quot; which represent fMRI brain activations by size, shape, number (of foci), location, density, orientation, and other parameters. These signatures are used in providing automatic classification of activations into classes. Since fMRI brain imaging is a non-invasive technique it has attracted enormous interest in recent years because of its potential to solve fundamental problems explaining cognitive processes and to find relations between brain structure and function. The environment developed in this project can fully utilize fMRI&apos;s potential by enabling inter- and intra- studies and support large-scale mining of fMRI brain activations. The techniques can extract associations at multiple levels: among different activations for the same or different (groups of) individuals, between activations and tasks, between activations and ancillary data, and between tasks and ancillary data. Automatic characterization tools open the field to numerous new research opportunities, as they can offer common signature formats to do cross-modality brain data searches and correlation over multiple studies. In turn, the mined results can speed up discovery in the neuroscience field. The methods will be demonstrated on a prototype robust database of brain activations that includes scan data as well as biographical, clinical, historical and other subject-centered ancillary data. This database supports retrieval requests and subsequent analysis incorporating spatial statistics, classification, and visualization techniques and provides an evaluation of the system by the professional communities. The results of this work will be applicable to the analysis of other brain data such as brain lesions, Positron Emission Tomography (PET) scans, and MRI scans; thus helping to create tools for search across brain data formats. In addition, objects similar to brain activations appear in many other different domains, including cellular biology, tumor analysis, fluid dynamics, or financial records, therefore, the resulting techniques are expected to have broad impact.&lt;br/&gt;http://devlab.dartmouth.edu/</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">83423</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83423</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n216" target="n217">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">Mining Human Brain Data: Analysis, Classification and Visualization</data>
      <data key="e_abstract">This multidisciplinary research effort develops methods for content based retrieval, analysis and visualization of probabilistic 3D spatial maps, in particular, the statistical parametric maps obtained from the fMRI (functional Magnetic Resonance Imaging) analysis of the brain. The approach is based on &quot;activation signatures&quot; which represent fMRI brain activations by size, shape, number (of foci), location, density, orientation, and other parameters. These signatures are used in providing automatic classification of activations into classes. Since fMRI brain imaging is a non-invasive technique it has attracted enormous interest in recent years because of its potential to solve fundamental problems explaining cognitive processes and to find relations between brain structure and function. The environment developed in this project can fully utilize fMRI&apos;s potential by enabling inter- and intra- studies and support large-scale mining of fMRI brain activations. The techniques can extract associations at multiple levels: among different activations for the same or different (groups of) individuals, between activations and tasks, between activations and ancillary data, and between tasks and ancillary data. Automatic characterization tools open the field to numerous new research opportunities, as they can offer common signature formats to do cross-modality brain data searches and correlation over multiple studies. In turn, the mined results can speed up discovery in the neuroscience field. The methods will be demonstrated on a prototype robust database of brain activations that includes scan data as well as biographical, clinical, historical and other subject-centered ancillary data. This database supports retrieval requests and subsequent analysis incorporating spatial statistics, classification, and visualization techniques and provides an evaluation of the system by the professional communities. The results of this work will be applicable to the analysis of other brain data such as brain lesions, Positron Emission Tomography (PET) scans, and MRI scans; thus helping to create tools for search across brain data formats. In addition, objects similar to brain activations appear in many other different domains, including cellular biology, tumor analysis, fluid dynamics, or financial records, therefore, the resulting techniques are expected to have broad impact.&lt;br/&gt;http://devlab.dartmouth.edu/</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">83423</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83423</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n216" target="n218">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">Mining Human Brain Data: Analysis, Classification and Visualization</data>
      <data key="e_abstract">This multidisciplinary research effort develops methods for content based retrieval, analysis and visualization of probabilistic 3D spatial maps, in particular, the statistical parametric maps obtained from the fMRI (functional Magnetic Resonance Imaging) analysis of the brain. The approach is based on &quot;activation signatures&quot; which represent fMRI brain activations by size, shape, number (of foci), location, density, orientation, and other parameters. These signatures are used in providing automatic classification of activations into classes. Since fMRI brain imaging is a non-invasive technique it has attracted enormous interest in recent years because of its potential to solve fundamental problems explaining cognitive processes and to find relations between brain structure and function. The environment developed in this project can fully utilize fMRI&apos;s potential by enabling inter- and intra- studies and support large-scale mining of fMRI brain activations. The techniques can extract associations at multiple levels: among different activations for the same or different (groups of) individuals, between activations and tasks, between activations and ancillary data, and between tasks and ancillary data. Automatic characterization tools open the field to numerous new research opportunities, as they can offer common signature formats to do cross-modality brain data searches and correlation over multiple studies. In turn, the mined results can speed up discovery in the neuroscience field. The methods will be demonstrated on a prototype robust database of brain activations that includes scan data as well as biographical, clinical, historical and other subject-centered ancillary data. This database supports retrieval requests and subsequent analysis incorporating spatial statistics, classification, and visualization techniques and provides an evaluation of the system by the professional communities. The results of this work will be applicable to the analysis of other brain data such as brain lesions, Positron Emission Tomography (PET) scans, and MRI scans; thus helping to create tools for search across brain data formats. In addition, objects similar to brain activations appear in many other different domains, including cellular biology, tumor analysis, fluid dynamics, or financial records, therefore, the resulting techniques are expected to have broad impact.&lt;br/&gt;http://devlab.dartmouth.edu/</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">83423</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83423</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n217" target="n218">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">Mining Human Brain Data: Analysis, Classification and Visualization</data>
      <data key="e_abstract">This multidisciplinary research effort develops methods for content based retrieval, analysis and visualization of probabilistic 3D spatial maps, in particular, the statistical parametric maps obtained from the fMRI (functional Magnetic Resonance Imaging) analysis of the brain. The approach is based on &quot;activation signatures&quot; which represent fMRI brain activations by size, shape, number (of foci), location, density, orientation, and other parameters. These signatures are used in providing automatic classification of activations into classes. Since fMRI brain imaging is a non-invasive technique it has attracted enormous interest in recent years because of its potential to solve fundamental problems explaining cognitive processes and to find relations between brain structure and function. The environment developed in this project can fully utilize fMRI&apos;s potential by enabling inter- and intra- studies and support large-scale mining of fMRI brain activations. The techniques can extract associations at multiple levels: among different activations for the same or different (groups of) individuals, between activations and tasks, between activations and ancillary data, and between tasks and ancillary data. Automatic characterization tools open the field to numerous new research opportunities, as they can offer common signature formats to do cross-modality brain data searches and correlation over multiple studies. In turn, the mined results can speed up discovery in the neuroscience field. The methods will be demonstrated on a prototype robust database of brain activations that includes scan data as well as biographical, clinical, historical and other subject-centered ancillary data. This database supports retrieval requests and subsequent analysis incorporating spatial statistics, classification, and visualization techniques and provides an evaluation of the system by the professional communities. The results of this work will be applicable to the analysis of other brain data such as brain lesions, Positron Emission Tomography (PET) scans, and MRI scans; thus helping to create tools for search across brain data formats. In addition, objects similar to brain activations appear in many other different domains, including cellular biology, tumor analysis, fluid dynamics, or financial records, therefore, the resulting techniques are expected to have broad impact.&lt;br/&gt;http://devlab.dartmouth.edu/</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">83423</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83423</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n220" target="n221">
      <data key="e_effectiveDate">2001-05-15</data>
      <data key="e_title">Digital Spectral Analysis for Mixed-Signal System-on-a-Chip Testing and Verification</data>
      <data key="e_abstract">We investigate a new application of the theory of signals and systems to test and verification of mixed-signal systems-on-a-chip. Both digital and analog input signals are considered as a set of time-varying waveforms, characterized by a correlation matrix. The matrix elements are the auto-correlation and cross-correlation coefficients. Auto-correlation indicates how much a signal resembles its prior values in time. Cross-correlation indicates how similar or dissimilar two signals are. The matrix is determined from circuit inputs having &quot;good&quot; fault detection properties. Auto-correlation was used on tests for a sequential digital circuit. Random vectors were generated, and only those detecting faults were retained during vector compaction. The correlation matrix (generated from the vectors) was used to create additional test vectors. The results were spectacular -- more faults were detected, using significantly fewer vectors, and with less computation, than by any other known method. The shorter vector length significantly reduces testing costs, which are typically one third of integrated circuit costs.&lt;br/&gt;&lt;br/&gt;We apply this idea to analog and mixed-signal circuits. We compose the Hadamard matrix (which describes the time history of prior vectors) for a digital circuit with an analog circuit transfer function. The matrix expresses successful digital test waveforms in terms of their digital spectrum. Analog circuits are tested using spectral analysis, but this is the first time that digital circuits have been tested spectrally. In a mixed digital/analog system, we arrange digital test waveforms to configure the digital part as a &quot;programmable tone generator&quot; to test the analog part. Likewise, the analog part is configured to provide the appropriate signal spectrum to test the digital part. The possible benefits would be: (1) Removal of isolation test hardware between digital and analog circuits, which are now tested together; (2) Elimination of delays and distortion due to test hardware; (3) Lower cost; and (4) Simpler test generation algorithms. We also are applying spectral testing ideas to formal hardware verification, which determines whether a circuit, as implemented, is consistent with its specification.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">98304</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98304</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n220" target="n222">
      <data key="e_effectiveDate">2001-05-15</data>
      <data key="e_title">Digital Spectral Analysis for Mixed-Signal System-on-a-Chip Testing and Verification</data>
      <data key="e_abstract">We investigate a new application of the theory of signals and systems to test and verification of mixed-signal systems-on-a-chip. Both digital and analog input signals are considered as a set of time-varying waveforms, characterized by a correlation matrix. The matrix elements are the auto-correlation and cross-correlation coefficients. Auto-correlation indicates how much a signal resembles its prior values in time. Cross-correlation indicates how similar or dissimilar two signals are. The matrix is determined from circuit inputs having &quot;good&quot; fault detection properties. Auto-correlation was used on tests for a sequential digital circuit. Random vectors were generated, and only those detecting faults were retained during vector compaction. The correlation matrix (generated from the vectors) was used to create additional test vectors. The results were spectacular -- more faults were detected, using significantly fewer vectors, and with less computation, than by any other known method. The shorter vector length significantly reduces testing costs, which are typically one third of integrated circuit costs.&lt;br/&gt;&lt;br/&gt;We apply this idea to analog and mixed-signal circuits. We compose the Hadamard matrix (which describes the time history of prior vectors) for a digital circuit with an analog circuit transfer function. The matrix expresses successful digital test waveforms in terms of their digital spectrum. Analog circuits are tested using spectral analysis, but this is the first time that digital circuits have been tested spectrally. In a mixed digital/analog system, we arrange digital test waveforms to configure the digital part as a &quot;programmable tone generator&quot; to test the analog part. Likewise, the analog part is configured to provide the appropriate signal spectrum to test the digital part. The possible benefits would be: (1) Removal of isolation test hardware between digital and analog circuits, which are now tested together; (2) Elimination of delays and distortion due to test hardware; (3) Lower cost; and (4) Simpler test generation algorithms. We also are applying spectral testing ideas to formal hardware verification, which determines whether a circuit, as implemented, is consistent with its specification.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">98304</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98304</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n221" target="n222">
      <data key="e_effectiveDate">2001-05-15</data>
      <data key="e_title">Digital Spectral Analysis for Mixed-Signal System-on-a-Chip Testing and Verification</data>
      <data key="e_abstract">We investigate a new application of the theory of signals and systems to test and verification of mixed-signal systems-on-a-chip. Both digital and analog input signals are considered as a set of time-varying waveforms, characterized by a correlation matrix. The matrix elements are the auto-correlation and cross-correlation coefficients. Auto-correlation indicates how much a signal resembles its prior values in time. Cross-correlation indicates how similar or dissimilar two signals are. The matrix is determined from circuit inputs having &quot;good&quot; fault detection properties. Auto-correlation was used on tests for a sequential digital circuit. Random vectors were generated, and only those detecting faults were retained during vector compaction. The correlation matrix (generated from the vectors) was used to create additional test vectors. The results were spectacular -- more faults were detected, using significantly fewer vectors, and with less computation, than by any other known method. The shorter vector length significantly reduces testing costs, which are typically one third of integrated circuit costs.&lt;br/&gt;&lt;br/&gt;We apply this idea to analog and mixed-signal circuits. We compose the Hadamard matrix (which describes the time history of prior vectors) for a digital circuit with an analog circuit transfer function. The matrix expresses successful digital test waveforms in terms of their digital spectrum. Analog circuits are tested using spectral analysis, but this is the first time that digital circuits have been tested spectrally. In a mixed digital/analog system, we arrange digital test waveforms to configure the digital part as a &quot;programmable tone generator&quot; to test the analog part. Likewise, the analog part is configured to provide the appropriate signal spectrum to test the digital part. The possible benefits would be: (1) Removal of isolation test hardware between digital and analog circuits, which are now tested together; (2) Elimination of delays and distortion due to test hardware; (3) Lower cost; and (4) Simpler test generation algorithms. We also are applying spectral testing ideas to formal hardware verification, which determines whether a circuit, as implemented, is consistent with its specification.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">98304</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98304</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n226" target="n227">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">REU SITES: Virtual Environments REU Site</data>
      <data key="e_abstract">EIA-0097688&lt;br/&gt;Van Scoy, Van L&lt;br/&gt;Baker, David V&lt;br/&gt;West Virginia University Research Corporation&lt;br/&gt;&lt;br/&gt;REU Sites: Virtual Environments REU Site&lt;br/&gt;&lt;br/&gt; The focus of this NSF-REU program to be hosted at West Virginia University is &quot;virtual environments.&quot; The objectives of the program are (1) to give undergraduate students practical skills in advanced visualization techniques where &quot;visualization&quot; has the extended meaning of &quot;presentation of information to the ears, or the fingers, or even the nose&quot;; (2) to give undergraduate students practical experience in multidisciplinary research applying virtual environments (VE) technology to problems in physical science, health science, and social science; (3) to help undergraduates discover the excitement of a career in research; and (4) to provide a cross cultural experience, by bringing together students primarily from the Appalachian region and from Puerto Rico and by giving some students the opportunity to work in an academic research lab in Japan.&lt;br/&gt;&lt;br/&gt;Our recruiting goal is to have 3 students from the University of Puerto Rico-Mayaguez, 6 students from four year colleges in West Virginia and the Appalachian region, and 3 students from WVU each summer. These students are expected to come from a variety of majors, including biology, computer science, geography, and physics and will be divided into teams to work with faculty and graduate student researchers on problems in computer science, geographic information systems, materials science, and pharmacy. Each week except for the first and last week of the summer program, students will work on Mondays, Wednesday, and Fridays with researchers on specific disciplinary research projects. On Tuesdays and Thursdays they will attend half day training sessions on specific VE technologies and tools, work in the VE Lab, and participate in a weekly research seminar involving all participants, undergraduate students, graduate students, and faculty. &lt;br/&gt;&lt;br/&gt;After each summer program in 2001, 2002, and 2003, three students will be selected from that year&apos;s participants to work in the Virtual Systems Laboratory at Gifu University in Japan during the following summer. The program will provide Japanese language and culture training for these selected participants during spring before their summer experience in Japan.</data>
      <data key="e_pgm">1139</data>
      <data key="e_label">97688</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">97688</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n229" target="n230">
      <data key="e_effectiveDate">2001-05-01</data>
      <data key="e_title">Unwrapping Phase Images: Theory and Applications Using Probabilistic Inference Techniques</data>
      <data key="e_abstract">Koetter Abstract&lt;br/&gt;&lt;br/&gt;Phase unwrapping in 2-dimensional topologies is a signal-processing problem that has been extensively studied over the past 20 years and has important applications, such as medical imaging and synthetic aperture radar. However, despite its importance in science and engineering, to date, phase unwrapping in 2-dimensional grids has remained an essentially unsolved problem. This research takes a fresh approach to the problem using methods from probabilistic inference. The work not only holds the promise of resulting in powerful phase unwrapping schemes based on the sum-product algorithm and structured variational methods, but also has the potential to provide deep theoretical insight into the ill-posed nature and solvability of the phase unwrapping problem. Such an insight is extremely important for guiding the development of practical algorithms.&lt;br/&gt;&lt;br/&gt;The main objective of this research is to develop and refine algorithms for phase unwrapping that are versatile, efficient and that significantly improve upon earlier approaches. One of the guiding ideas in this context is the use of probability inference as a nonlinear preprocessing step in a phase unwrapping scheme. Initial experiments have confirmed that the performance of traditional techniques can indeed be significantly boosted with such an approach. Success of this research can have a profound practical impact. For example, in SAR interferometry, phase unwrapping is an essential step in generating terrain elevation maps, and this work can significantly enhance the accuracy of existing algorithms based on deterministic phase models. Similarly, the proposed work will make routine phase imaging using Magnetic Resonance Imaging (MRI) signals feasible, which will significantly extend the clinical utility of MRI.</data>
      <data key="e_pgm">4720</data>
      <data key="e_label">105719</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105719</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n241" target="n242">
      <data key="e_effectiveDate">2001-06-01</data>
      <data key="e_title">REU Sites - Research Experience for Undergraduates in Computer Vision</data>
      <data key="e_abstract">EIA -0097738&lt;br/&gt;Shah, Mubarak&lt;br/&gt;Niels J da Vitoria Lobo&lt;br/&gt;&lt;br/&gt;REU Sites: Research Experience for Undergraduates in Computer Vision&lt;br/&gt;&lt;br/&gt; This project represents a continuation of a Research Experience for Undergraduates site&lt;br/&gt;which has operated successfully for the past {\bf thirteen} years. Approximately {\bf one hundred thirty} undergraduate students from several institutions in Florida and outside Florida have participated in this&lt;br/&gt;program. The key distinctive elements of our approach are (1) to have a full calendar year experience planned for the participants, so that they have time to follow a substantial project through to completion, (2) to present each participant with several possible project topics, so that they can feel they have chosen a project which is most interesting to them, (3) to immerse the participants in the general research environment&lt;br/&gt;essentially as if they were graduate students, by having them meet with their faculty advisor once each week&lt;br/&gt;to discuss their project, participate in the weekly research group meetings, attend research presentations and meet with visiting researchers, and (4) to follow through over the year by working with the students&lt;br/&gt;to write a technical report on their project, to prepare for the GREs and to apply to graduate programs.&lt;br/&gt;In past years, a large fraction of our REU participants have been able to prepare a paper for submission to a conference, have the paper accepted and then attend the conference to present the paper. Several past participants have even accomplished substantial enough research to also result in journal publications.&lt;br/&gt;Many of our past participants are now pursuing graduate studies at various institutions, and some are now occupying faculty positions.</data>
      <data key="e_pgm">1139</data>
      <data key="e_label">97738</data>
      <data key="e_expirationDate">2007-05-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">97738</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n243" target="n244">
      <data key="e_effectiveDate">2001-06-01</data>
      <data key="e_title">Conference &quot;First International Symposium on Computational Cell Biology&quot; held March 4-6, 2001 in Lenox, MA.</data>
      <data key="e_abstract">This grant provides partial support for the First International Symposium on Computational Cell&lt;br/&gt;Biology to be held March 4-6, 2001 in Lenox, MA. Computational Cell Biology is an emerging interdisciplinary field that responds to the need for computational methods to analyze and organize the abundance of experimental data on the structure and function of the cell. Whereas other meetings have been held that focus on bioinformatics and molecular and structural biology, this will be the first meeting geared to the use of computational modeling applications in cell biology, and will be primarily targeted to cell&lt;br/&gt;biologists. The major focus of the meeting will be on areas of cell biology for which modeling approaches are currently being developed, or that are ripe for computational modeling approaches. A key goal is to bring together cell biologists whose research addresses quantitative aspects of cellular mechanisms with computer scientists and mathematicians who can provide the computational tools. The symposium is being organized by The National Resource for Cell Analysis and Modeling (NRCAM), located in the Center for Biomedical Imaging Technology (CBIT) at the University of Connecticut Health Center. The organizing committee consists of&lt;br/&gt;Drs. Leslie M. Loew, Director of NRCAM and CBIT, John Carson, co-Director of NRCAM and Vladimir Rodionov, associate faculty of CBIT, and is chaired by Ann Cowan, Deputy Director of CBIT and head of dissemination and training for NRCAM. The members of the organizing committee feel that the time is ripe to bring together cell biologists, mathematicians, and computer scientists to develop a true community of scientists sharing the common goal of developing computational tools for cell biological modeling and simulation. Topics at the meetings will encompass a range of cellular mechanisms including regulation of the cytoskeleton and molecular motors, membrane and protein trafficking, regulation of calcium dynamics, signal transduction pathways, and cell cycle control. In each of these areas key researchers who utilize highly quantitative experimental approaches and/or are applying mathematical modeling approaches will be invited to speak. The meeting will be an intensive mix of platform sessions, poster sessions and small workshops and tutorials. The venue for the meeting is a resort in the Berkshire mountains that is limited to a maximum of 125 participants. This setting and meeting size will be ideal for encouraging individual interactions between this diverse group of scientists. It will also serve as an excellent educational activity for graduate and&lt;br/&gt;postdoctoral students working in this young and expanding field.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">96625</data>
      <data key="e_expirationDate">2002-05-31</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">96625</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n243" target="n245">
      <data key="e_effectiveDate">2001-06-01</data>
      <data key="e_title">Conference &quot;First International Symposium on Computational Cell Biology&quot; held March 4-6, 2001 in Lenox, MA.</data>
      <data key="e_abstract">This grant provides partial support for the First International Symposium on Computational Cell&lt;br/&gt;Biology to be held March 4-6, 2001 in Lenox, MA. Computational Cell Biology is an emerging interdisciplinary field that responds to the need for computational methods to analyze and organize the abundance of experimental data on the structure and function of the cell. Whereas other meetings have been held that focus on bioinformatics and molecular and structural biology, this will be the first meeting geared to the use of computational modeling applications in cell biology, and will be primarily targeted to cell&lt;br/&gt;biologists. The major focus of the meeting will be on areas of cell biology for which modeling approaches are currently being developed, or that are ripe for computational modeling approaches. A key goal is to bring together cell biologists whose research addresses quantitative aspects of cellular mechanisms with computer scientists and mathematicians who can provide the computational tools. The symposium is being organized by The National Resource for Cell Analysis and Modeling (NRCAM), located in the Center for Biomedical Imaging Technology (CBIT) at the University of Connecticut Health Center. The organizing committee consists of&lt;br/&gt;Drs. Leslie M. Loew, Director of NRCAM and CBIT, John Carson, co-Director of NRCAM and Vladimir Rodionov, associate faculty of CBIT, and is chaired by Ann Cowan, Deputy Director of CBIT and head of dissemination and training for NRCAM. The members of the organizing committee feel that the time is ripe to bring together cell biologists, mathematicians, and computer scientists to develop a true community of scientists sharing the common goal of developing computational tools for cell biological modeling and simulation. Topics at the meetings will encompass a range of cellular mechanisms including regulation of the cytoskeleton and molecular motors, membrane and protein trafficking, regulation of calcium dynamics, signal transduction pathways, and cell cycle control. In each of these areas key researchers who utilize highly quantitative experimental approaches and/or are applying mathematical modeling approaches will be invited to speak. The meeting will be an intensive mix of platform sessions, poster sessions and small workshops and tutorials. The venue for the meeting is a resort in the Berkshire mountains that is limited to a maximum of 125 participants. This setting and meeting size will be ideal for encouraging individual interactions between this diverse group of scientists. It will also serve as an excellent educational activity for graduate and&lt;br/&gt;postdoctoral students working in this young and expanding field.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">96625</data>
      <data key="e_expirationDate">2002-05-31</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">96625</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n244" target="n245">
      <data key="e_effectiveDate">2001-06-01</data>
      <data key="e_title">Conference &quot;First International Symposium on Computational Cell Biology&quot; held March 4-6, 2001 in Lenox, MA.</data>
      <data key="e_abstract">This grant provides partial support for the First International Symposium on Computational Cell&lt;br/&gt;Biology to be held March 4-6, 2001 in Lenox, MA. Computational Cell Biology is an emerging interdisciplinary field that responds to the need for computational methods to analyze and organize the abundance of experimental data on the structure and function of the cell. Whereas other meetings have been held that focus on bioinformatics and molecular and structural biology, this will be the first meeting geared to the use of computational modeling applications in cell biology, and will be primarily targeted to cell&lt;br/&gt;biologists. The major focus of the meeting will be on areas of cell biology for which modeling approaches are currently being developed, or that are ripe for computational modeling approaches. A key goal is to bring together cell biologists whose research addresses quantitative aspects of cellular mechanisms with computer scientists and mathematicians who can provide the computational tools. The symposium is being organized by The National Resource for Cell Analysis and Modeling (NRCAM), located in the Center for Biomedical Imaging Technology (CBIT) at the University of Connecticut Health Center. The organizing committee consists of&lt;br/&gt;Drs. Leslie M. Loew, Director of NRCAM and CBIT, John Carson, co-Director of NRCAM and Vladimir Rodionov, associate faculty of CBIT, and is chaired by Ann Cowan, Deputy Director of CBIT and head of dissemination and training for NRCAM. The members of the organizing committee feel that the time is ripe to bring together cell biologists, mathematicians, and computer scientists to develop a true community of scientists sharing the common goal of developing computational tools for cell biological modeling and simulation. Topics at the meetings will encompass a range of cellular mechanisms including regulation of the cytoskeleton and molecular motors, membrane and protein trafficking, regulation of calcium dynamics, signal transduction pathways, and cell cycle control. In each of these areas key researchers who utilize highly quantitative experimental approaches and/or are applying mathematical modeling approaches will be invited to speak. The meeting will be an intensive mix of platform sessions, poster sessions and small workshops and tutorials. The venue for the meeting is a resort in the Berkshire mountains that is limited to a maximum of 125 participants. This setting and meeting size will be ideal for encouraging individual interactions between this diverse group of scientists. It will also serve as an excellent educational activity for graduate and&lt;br/&gt;postdoctoral students working in this young and expanding field.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">96625</data>
      <data key="e_expirationDate">2002-05-31</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">96625</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n259" target="n260">
      <data key="e_effectiveDate">2001-06-15</data>
      <data key="e_title">Digital Government: Developing an Information Technology and Organizational Design Research Agenda for the Evaluation and Management of Research Proposals</data>
      <data key="e_abstract">EIA-0109049&lt;br/&gt;Sharon Dawes&lt;br/&gt;SUNY @ Albany&lt;br/&gt;&lt;br/&gt;Digital Government: Proposal to investigate e-business transformation at the National Science Foundation&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This workshop will examine today&apos;s information technology resarch status and products, and the variety of ways research granting institutions currently accomplish their missions. That examination will support an extrapolation of technology and organizational practices 5-7 years into the future, to consider how those coming technologies and practices might be employed in a granting institution&apos;s mission. Of interest also will be how the organization of such an institution might be constructed to best take advantage of those technologies and to ensure the appropriate injection of new technologies in achieving high effectiveness in the furtherance of its mission.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">109049</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">109049</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n259" target="n261">
      <data key="e_effectiveDate">2001-06-15</data>
      <data key="e_title">Digital Government: Developing an Information Technology and Organizational Design Research Agenda for the Evaluation and Management of Research Proposals</data>
      <data key="e_abstract">EIA-0109049&lt;br/&gt;Sharon Dawes&lt;br/&gt;SUNY @ Albany&lt;br/&gt;&lt;br/&gt;Digital Government: Proposal to investigate e-business transformation at the National Science Foundation&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This workshop will examine today&apos;s information technology resarch status and products, and the variety of ways research granting institutions currently accomplish their missions. That examination will support an extrapolation of technology and organizational practices 5-7 years into the future, to consider how those coming technologies and practices might be employed in a granting institution&apos;s mission. Of interest also will be how the organization of such an institution might be constructed to best take advantage of those technologies and to ensure the appropriate injection of new technologies in achieving high effectiveness in the furtherance of its mission.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">109049</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">109049</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n260" target="n261">
      <data key="e_effectiveDate">2001-06-15</data>
      <data key="e_title">Digital Government: Developing an Information Technology and Organizational Design Research Agenda for the Evaluation and Management of Research Proposals</data>
      <data key="e_abstract">EIA-0109049&lt;br/&gt;Sharon Dawes&lt;br/&gt;SUNY @ Albany&lt;br/&gt;&lt;br/&gt;Digital Government: Proposal to investigate e-business transformation at the National Science Foundation&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This workshop will examine today&apos;s information technology resarch status and products, and the variety of ways research granting institutions currently accomplish their missions. That examination will support an extrapolation of technology and organizational practices 5-7 years into the future, to consider how those coming technologies and practices might be employed in a granting institution&apos;s mission. Of interest also will be how the organization of such an institution might be constructed to best take advantage of those technologies and to ensure the appropriate injection of new technologies in achieving high effectiveness in the furtherance of its mission.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">109049</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">109049</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n263" target="n264">
      <data key="e_effectiveDate">2001-06-01</data>
      <data key="e_title">Synthesis of Block-recursive Codes for Deep Memory Hierarchies</data>
      <data key="e_abstract">Modern computers have multi-level memory hierarchies in which the cost of data access may increase by an order of magnitude or more from one level to the next. On these machines, a program that touches a large amount of data runs well only if it exhibits locality - that is, if most of its references are satisfied by the highest level of the memory hierarchy. Unfortunately, many programs do not exhibit locality.&lt;br/&gt;&lt;br/&gt;For 2-level memory hierarchies, the numerical linear algebra community has addressed the problem by implementing libraries of blocked codes such as LAPACK. For multi-level memory hierarchies, they are developing libraries of block-recursive algorithms. &lt;br/&gt;&lt;br/&gt;The problem with any library is that it is not general-purpose - for example, the BLAS and LAPACK libraries cannot be used for obtaining good performance on finite-difference codes. This research will develop a general-purpose restructuring compiler for synthesizing block-recursive codes from standard iterative ones. The compiler is general-purpose in the sense that in principle, it can restructure any program in which dense arrays are accessed by affine array references. The technology proposed has already been used successfully to restructure iterative codes into LAPACK-style blocked codes.&lt;br/&gt;&lt;br/&gt;The approach to be taken consists of:&lt;br/&gt;&lt;br/&gt;--- converting the problem of generating code that touches data in a block-recursive order into the problem of generating code that walks over a certain iteration space called the product space in a block-recursive or space-filling order. This enables standard techniques from dependence analysis to be used to synthesize the appropriate restructuring transformations.&lt;br/&gt;&lt;br/&gt;- switching to an iterative code once the problem size becomes small enough. The threshold problem size at which this transition should happen may be difficult to determine analytically. In our system, the compiler will estimate this size using a simple abstraction of the underlying machine architecture. For frequently used codes however, the system will use a new approach called empirical optimization - whenever there are free cycles on the machine, our system will experiment with different threshold values, remember the best threshold value for each input size, and use that value when that code is run again.&lt;br/&gt;&lt;br/&gt;- developing a symbolic analysis technique called fractal symbolic analysis for verifying the legality of transformations on codes like LU with pivoting (since it is well-known that dependence analysis is inadequate for restructuring codes such as LU factorization with pivoting). However, it is not yet known how to synthesize the right locality-enhancing transformations from information provided by fractal symbolic analysis. Such techniques will be developed and integrated with dependence-based techniques.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">90217</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">90217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n266" target="n267">
      <data key="e_effectiveDate">2001-06-01</data>
      <data key="e_title">Perspectives on Software</data>
      <data key="e_abstract">Our ability to build complex software systems relies on composition techniques that enable complete programs to be constructed from smaller pieces. A particular decomposition imposes a structure that makes it possible to manage the complexity of a large system. But the same decomposition can also make it harder to add or change features that cut across the structure. The thesis of this project is that the design, evolution, and comprehension of software can be better supported by tools that allow programmers to examine and work with multiple perspectives of a system. In particular, such tools empower programmers to think of programs, not as linear texts, but as equivalence classes of concrete views, each of which might be appropriate for different tasks.&lt;br/&gt;&lt;br/&gt;The project develops both foundational results and practical tools for representing and manipulating software. Firm semantic foundations are provided by a new algebraic framework for reasoning about the construction of complex software systems. Practical experience is obtained by developing and experimenting with an interactive browser and editor based on the same algebra. This tool will allow programmers to explore and modify a software system from multiple perspectives, some user-defined, and some automatically generated.</data>
      <data key="e_pgm">2880</data>
      <data key="e_label">98323</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98323</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n270" target="n271">
      <data key="e_effectiveDate">2001-06-15</data>
      <data key="e_title">Digital Goverment: Digitalization of Coastal Management and Decision Making Supported by Multi-dimensional Geospatial Information and Analysis</data>
      <data key="e_abstract">EIA-0091494&lt;br/&gt;Ohio State University&lt;br/&gt;Rongxing Li&lt;br/&gt;&lt;br/&gt;TITLE: Digital Government: Digitalization of Coastal Management and Decision-Making Supported by Multi-dimensional Geospatial Information and Analysis&lt;br/&gt;&lt;br/&gt;With the continuing trend of increasing population and economic activities in the coastal zone, a sustainable coastal environment is crucial to the well being of human, wildlife, and properties in the zone. Recently, the coastal zone has been directly or indirectly affected by a series of problems such as global warming, climate change, seal level raising, coastal erosion, environmental contamination, and overpopulation. Coastal management and decision-making involve extremely comprehensive and critical operations across multiple governmental organizations, where the above problems and impacts have to be dealt with. Geospatial information and analysis can be used to coordinate and support multi-agency operations include a) federal government: coastal erosion monitoring, coastal resource management, coastal forecasting, coastal development permission, and residential erosion awareness and watching system. The goal of this research is to investigate and develop technologies that will greatly enhance operation capabilities of federal, state, and local government agencies for coastal management and decision-making using multiple spaceborne, airborne, and in situ remotely-sensed measurements, spatio-temporal databases, and geospatial analysis tools.&lt;br/&gt;&lt;br/&gt;The objectives of this research project are:&lt;br/&gt;a) develop a strategy and technologies for utilizing geospatial information of all level of government agencies in coastal management and decision-making.&lt;br/&gt;b) develop and integrate technologies of space- and airborne and insitu spatial data acquisition, spatio-temporal data modeling and analysis, hydrological modeling and forecasting, and web-based information systems for digital government operations,&lt;br/&gt;c) develop and test an integrated coastal management and decision-making system that incorporates the spatial analysis and hydrological modeling technologies that will examine historical coastal environment and future changes based on desired or proposed scenarios,&lt;br/&gt;d) test and implement the above strategy and the system to demonstrate the significantly improved interagency operations of coastal decision-making through a pilot project in the primary site in Lake Erie area and another test site in the Tampa Bay area, and&lt;br/&gt;e) disseminate coastal geospatial data and coastal erosion information to end users including coastal residents in erosion areas and to assist them in making their decisions such as property protection measures, property purchasing/selling, and small community planning activities.&lt;br/&gt;&lt;br/&gt;If successfully implemented, this project will significantly enhance our capability of handling spatio-temporal coastal databases, build a fundamental basis of coastal geospatial information for inter-governmental agency operations, and provide innovative tools for all levels of governmental agencies to improve efficiency and reduce operational costs.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91494</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91494</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n270" target="n272">
      <data key="e_effectiveDate">2001-06-15</data>
      <data key="e_title">Digital Goverment: Digitalization of Coastal Management and Decision Making Supported by Multi-dimensional Geospatial Information and Analysis</data>
      <data key="e_abstract">EIA-0091494&lt;br/&gt;Ohio State University&lt;br/&gt;Rongxing Li&lt;br/&gt;&lt;br/&gt;TITLE: Digital Government: Digitalization of Coastal Management and Decision-Making Supported by Multi-dimensional Geospatial Information and Analysis&lt;br/&gt;&lt;br/&gt;With the continuing trend of increasing population and economic activities in the coastal zone, a sustainable coastal environment is crucial to the well being of human, wildlife, and properties in the zone. Recently, the coastal zone has been directly or indirectly affected by a series of problems such as global warming, climate change, seal level raising, coastal erosion, environmental contamination, and overpopulation. Coastal management and decision-making involve extremely comprehensive and critical operations across multiple governmental organizations, where the above problems and impacts have to be dealt with. Geospatial information and analysis can be used to coordinate and support multi-agency operations include a) federal government: coastal erosion monitoring, coastal resource management, coastal forecasting, coastal development permission, and residential erosion awareness and watching system. The goal of this research is to investigate and develop technologies that will greatly enhance operation capabilities of federal, state, and local government agencies for coastal management and decision-making using multiple spaceborne, airborne, and in situ remotely-sensed measurements, spatio-temporal databases, and geospatial analysis tools.&lt;br/&gt;&lt;br/&gt;The objectives of this research project are:&lt;br/&gt;a) develop a strategy and technologies for utilizing geospatial information of all level of government agencies in coastal management and decision-making.&lt;br/&gt;b) develop and integrate technologies of space- and airborne and insitu spatial data acquisition, spatio-temporal data modeling and analysis, hydrological modeling and forecasting, and web-based information systems for digital government operations,&lt;br/&gt;c) develop and test an integrated coastal management and decision-making system that incorporates the spatial analysis and hydrological modeling technologies that will examine historical coastal environment and future changes based on desired or proposed scenarios,&lt;br/&gt;d) test and implement the above strategy and the system to demonstrate the significantly improved interagency operations of coastal decision-making through a pilot project in the primary site in Lake Erie area and another test site in the Tampa Bay area, and&lt;br/&gt;e) disseminate coastal geospatial data and coastal erosion information to end users including coastal residents in erosion areas and to assist them in making their decisions such as property protection measures, property purchasing/selling, and small community planning activities.&lt;br/&gt;&lt;br/&gt;If successfully implemented, this project will significantly enhance our capability of handling spatio-temporal coastal databases, build a fundamental basis of coastal geospatial information for inter-governmental agency operations, and provide innovative tools for all levels of governmental agencies to improve efficiency and reduce operational costs.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91494</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91494</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n270" target="n273">
      <data key="e_effectiveDate">2001-06-15</data>
      <data key="e_title">Digital Goverment: Digitalization of Coastal Management and Decision Making Supported by Multi-dimensional Geospatial Information and Analysis</data>
      <data key="e_abstract">EIA-0091494&lt;br/&gt;Ohio State University&lt;br/&gt;Rongxing Li&lt;br/&gt;&lt;br/&gt;TITLE: Digital Government: Digitalization of Coastal Management and Decision-Making Supported by Multi-dimensional Geospatial Information and Analysis&lt;br/&gt;&lt;br/&gt;With the continuing trend of increasing population and economic activities in the coastal zone, a sustainable coastal environment is crucial to the well being of human, wildlife, and properties in the zone. Recently, the coastal zone has been directly or indirectly affected by a series of problems such as global warming, climate change, seal level raising, coastal erosion, environmental contamination, and overpopulation. Coastal management and decision-making involve extremely comprehensive and critical operations across multiple governmental organizations, where the above problems and impacts have to be dealt with. Geospatial information and analysis can be used to coordinate and support multi-agency operations include a) federal government: coastal erosion monitoring, coastal resource management, coastal forecasting, coastal development permission, and residential erosion awareness and watching system. The goal of this research is to investigate and develop technologies that will greatly enhance operation capabilities of federal, state, and local government agencies for coastal management and decision-making using multiple spaceborne, airborne, and in situ remotely-sensed measurements, spatio-temporal databases, and geospatial analysis tools.&lt;br/&gt;&lt;br/&gt;The objectives of this research project are:&lt;br/&gt;a) develop a strategy and technologies for utilizing geospatial information of all level of government agencies in coastal management and decision-making.&lt;br/&gt;b) develop and integrate technologies of space- and airborne and insitu spatial data acquisition, spatio-temporal data modeling and analysis, hydrological modeling and forecasting, and web-based information systems for digital government operations,&lt;br/&gt;c) develop and test an integrated coastal management and decision-making system that incorporates the spatial analysis and hydrological modeling technologies that will examine historical coastal environment and future changes based on desired or proposed scenarios,&lt;br/&gt;d) test and implement the above strategy and the system to demonstrate the significantly improved interagency operations of coastal decision-making through a pilot project in the primary site in Lake Erie area and another test site in the Tampa Bay area, and&lt;br/&gt;e) disseminate coastal geospatial data and coastal erosion information to end users including coastal residents in erosion areas and to assist them in making their decisions such as property protection measures, property purchasing/selling, and small community planning activities.&lt;br/&gt;&lt;br/&gt;If successfully implemented, this project will significantly enhance our capability of handling spatio-temporal coastal databases, build a fundamental basis of coastal geospatial information for inter-governmental agency operations, and provide innovative tools for all levels of governmental agencies to improve efficiency and reduce operational costs.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91494</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91494</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n270" target="n274">
      <data key="e_effectiveDate">2001-06-15</data>
      <data key="e_title">Digital Goverment: Digitalization of Coastal Management and Decision Making Supported by Multi-dimensional Geospatial Information and Analysis</data>
      <data key="e_abstract">EIA-0091494&lt;br/&gt;Ohio State University&lt;br/&gt;Rongxing Li&lt;br/&gt;&lt;br/&gt;TITLE: Digital Government: Digitalization of Coastal Management and Decision-Making Supported by Multi-dimensional Geospatial Information and Analysis&lt;br/&gt;&lt;br/&gt;With the continuing trend of increasing population and economic activities in the coastal zone, a sustainable coastal environment is crucial to the well being of human, wildlife, and properties in the zone. Recently, the coastal zone has been directly or indirectly affected by a series of problems such as global warming, climate change, seal level raising, coastal erosion, environmental contamination, and overpopulation. Coastal management and decision-making involve extremely comprehensive and critical operations across multiple governmental organizations, where the above problems and impacts have to be dealt with. Geospatial information and analysis can be used to coordinate and support multi-agency operations include a) federal government: coastal erosion monitoring, coastal resource management, coastal forecasting, coastal development permission, and residential erosion awareness and watching system. The goal of this research is to investigate and develop technologies that will greatly enhance operation capabilities of federal, state, and local government agencies for coastal management and decision-making using multiple spaceborne, airborne, and in situ remotely-sensed measurements, spatio-temporal databases, and geospatial analysis tools.&lt;br/&gt;&lt;br/&gt;The objectives of this research project are:&lt;br/&gt;a) develop a strategy and technologies for utilizing geospatial information of all level of government agencies in coastal management and decision-making.&lt;br/&gt;b) develop and integrate technologies of space- and airborne and insitu spatial data acquisition, spatio-temporal data modeling and analysis, hydrological modeling and forecasting, and web-based information systems for digital government operations,&lt;br/&gt;c) develop and test an integrated coastal management and decision-making system that incorporates the spatial analysis and hydrological modeling technologies that will examine historical coastal environment and future changes based on desired or proposed scenarios,&lt;br/&gt;d) test and implement the above strategy and the system to demonstrate the significantly improved interagency operations of coastal decision-making through a pilot project in the primary site in Lake Erie area and another test site in the Tampa Bay area, and&lt;br/&gt;e) disseminate coastal geospatial data and coastal erosion information to end users including coastal residents in erosion areas and to assist them in making their decisions such as property protection measures, property purchasing/selling, and small community planning activities.&lt;br/&gt;&lt;br/&gt;If successfully implemented, this project will significantly enhance our capability of handling spatio-temporal coastal databases, build a fundamental basis of coastal geospatial information for inter-governmental agency operations, and provide innovative tools for all levels of governmental agencies to improve efficiency and reduce operational costs.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91494</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91494</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n271" target="n272">
      <data key="e_effectiveDate">2001-06-15</data>
      <data key="e_title">Digital Goverment: Digitalization of Coastal Management and Decision Making Supported by Multi-dimensional Geospatial Information and Analysis</data>
      <data key="e_abstract">EIA-0091494&lt;br/&gt;Ohio State University&lt;br/&gt;Rongxing Li&lt;br/&gt;&lt;br/&gt;TITLE: Digital Government: Digitalization of Coastal Management and Decision-Making Supported by Multi-dimensional Geospatial Information and Analysis&lt;br/&gt;&lt;br/&gt;With the continuing trend of increasing population and economic activities in the coastal zone, a sustainable coastal environment is crucial to the well being of human, wildlife, and properties in the zone. Recently, the coastal zone has been directly or indirectly affected by a series of problems such as global warming, climate change, seal level raising, coastal erosion, environmental contamination, and overpopulation. Coastal management and decision-making involve extremely comprehensive and critical operations across multiple governmental organizations, where the above problems and impacts have to be dealt with. Geospatial information and analysis can be used to coordinate and support multi-agency operations include a) federal government: coastal erosion monitoring, coastal resource management, coastal forecasting, coastal development permission, and residential erosion awareness and watching system. The goal of this research is to investigate and develop technologies that will greatly enhance operation capabilities of federal, state, and local government agencies for coastal management and decision-making using multiple spaceborne, airborne, and in situ remotely-sensed measurements, spatio-temporal databases, and geospatial analysis tools.&lt;br/&gt;&lt;br/&gt;The objectives of this research project are:&lt;br/&gt;a) develop a strategy and technologies for utilizing geospatial information of all level of government agencies in coastal management and decision-making.&lt;br/&gt;b) develop and integrate technologies of space- and airborne and insitu spatial data acquisition, spatio-temporal data modeling and analysis, hydrological modeling and forecasting, and web-based information systems for digital government operations,&lt;br/&gt;c) develop and test an integrated coastal management and decision-making system that incorporates the spatial analysis and hydrological modeling technologies that will examine historical coastal environment and future changes based on desired or proposed scenarios,&lt;br/&gt;d) test and implement the above strategy and the system to demonstrate the significantly improved interagency operations of coastal decision-making through a pilot project in the primary site in Lake Erie area and another test site in the Tampa Bay area, and&lt;br/&gt;e) disseminate coastal geospatial data and coastal erosion information to end users including coastal residents in erosion areas and to assist them in making their decisions such as property protection measures, property purchasing/selling, and small community planning activities.&lt;br/&gt;&lt;br/&gt;If successfully implemented, this project will significantly enhance our capability of handling spatio-temporal coastal databases, build a fundamental basis of coastal geospatial information for inter-governmental agency operations, and provide innovative tools for all levels of governmental agencies to improve efficiency and reduce operational costs.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91494</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91494</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n271" target="n273">
      <data key="e_effectiveDate">2001-06-15</data>
      <data key="e_title">Digital Goverment: Digitalization of Coastal Management and Decision Making Supported by Multi-dimensional Geospatial Information and Analysis</data>
      <data key="e_abstract">EIA-0091494&lt;br/&gt;Ohio State University&lt;br/&gt;Rongxing Li&lt;br/&gt;&lt;br/&gt;TITLE: Digital Government: Digitalization of Coastal Management and Decision-Making Supported by Multi-dimensional Geospatial Information and Analysis&lt;br/&gt;&lt;br/&gt;With the continuing trend of increasing population and economic activities in the coastal zone, a sustainable coastal environment is crucial to the well being of human, wildlife, and properties in the zone. Recently, the coastal zone has been directly or indirectly affected by a series of problems such as global warming, climate change, seal level raising, coastal erosion, environmental contamination, and overpopulation. Coastal management and decision-making involve extremely comprehensive and critical operations across multiple governmental organizations, where the above problems and impacts have to be dealt with. Geospatial information and analysis can be used to coordinate and support multi-agency operations include a) federal government: coastal erosion monitoring, coastal resource management, coastal forecasting, coastal development permission, and residential erosion awareness and watching system. The goal of this research is to investigate and develop technologies that will greatly enhance operation capabilities of federal, state, and local government agencies for coastal management and decision-making using multiple spaceborne, airborne, and in situ remotely-sensed measurements, spatio-temporal databases, and geospatial analysis tools.&lt;br/&gt;&lt;br/&gt;The objectives of this research project are:&lt;br/&gt;a) develop a strategy and technologies for utilizing geospatial information of all level of government agencies in coastal management and decision-making.&lt;br/&gt;b) develop and integrate technologies of space- and airborne and insitu spatial data acquisition, spatio-temporal data modeling and analysis, hydrological modeling and forecasting, and web-based information systems for digital government operations,&lt;br/&gt;c) develop and test an integrated coastal management and decision-making system that incorporates the spatial analysis and hydrological modeling technologies that will examine historical coastal environment and future changes based on desired or proposed scenarios,&lt;br/&gt;d) test and implement the above strategy and the system to demonstrate the significantly improved interagency operations of coastal decision-making through a pilot project in the primary site in Lake Erie area and another test site in the Tampa Bay area, and&lt;br/&gt;e) disseminate coastal geospatial data and coastal erosion information to end users including coastal residents in erosion areas and to assist them in making their decisions such as property protection measures, property purchasing/selling, and small community planning activities.&lt;br/&gt;&lt;br/&gt;If successfully implemented, this project will significantly enhance our capability of handling spatio-temporal coastal databases, build a fundamental basis of coastal geospatial information for inter-governmental agency operations, and provide innovative tools for all levels of governmental agencies to improve efficiency and reduce operational costs.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91494</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91494</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n271" target="n274">
      <data key="e_effectiveDate">2001-06-15</data>
      <data key="e_title">Digital Goverment: Digitalization of Coastal Management and Decision Making Supported by Multi-dimensional Geospatial Information and Analysis</data>
      <data key="e_abstract">EIA-0091494&lt;br/&gt;Ohio State University&lt;br/&gt;Rongxing Li&lt;br/&gt;&lt;br/&gt;TITLE: Digital Government: Digitalization of Coastal Management and Decision-Making Supported by Multi-dimensional Geospatial Information and Analysis&lt;br/&gt;&lt;br/&gt;With the continuing trend of increasing population and economic activities in the coastal zone, a sustainable coastal environment is crucial to the well being of human, wildlife, and properties in the zone. Recently, the coastal zone has been directly or indirectly affected by a series of problems such as global warming, climate change, seal level raising, coastal erosion, environmental contamination, and overpopulation. Coastal management and decision-making involve extremely comprehensive and critical operations across multiple governmental organizations, where the above problems and impacts have to be dealt with. Geospatial information and analysis can be used to coordinate and support multi-agency operations include a) federal government: coastal erosion monitoring, coastal resource management, coastal forecasting, coastal development permission, and residential erosion awareness and watching system. The goal of this research is to investigate and develop technologies that will greatly enhance operation capabilities of federal, state, and local government agencies for coastal management and decision-making using multiple spaceborne, airborne, and in situ remotely-sensed measurements, spatio-temporal databases, and geospatial analysis tools.&lt;br/&gt;&lt;br/&gt;The objectives of this research project are:&lt;br/&gt;a) develop a strategy and technologies for utilizing geospatial information of all level of government agencies in coastal management and decision-making.&lt;br/&gt;b) develop and integrate technologies of space- and airborne and insitu spatial data acquisition, spatio-temporal data modeling and analysis, hydrological modeling and forecasting, and web-based information systems for digital government operations,&lt;br/&gt;c) develop and test an integrated coastal management and decision-making system that incorporates the spatial analysis and hydrological modeling technologies that will examine historical coastal environment and future changes based on desired or proposed scenarios,&lt;br/&gt;d) test and implement the above strategy and the system to demonstrate the significantly improved interagency operations of coastal decision-making through a pilot project in the primary site in Lake Erie area and another test site in the Tampa Bay area, and&lt;br/&gt;e) disseminate coastal geospatial data and coastal erosion information to end users including coastal residents in erosion areas and to assist them in making their decisions such as property protection measures, property purchasing/selling, and small community planning activities.&lt;br/&gt;&lt;br/&gt;If successfully implemented, this project will significantly enhance our capability of handling spatio-temporal coastal databases, build a fundamental basis of coastal geospatial information for inter-governmental agency operations, and provide innovative tools for all levels of governmental agencies to improve efficiency and reduce operational costs.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91494</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91494</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n272" target="n273">
      <data key="e_effectiveDate">2001-06-15</data>
      <data key="e_title">Digital Goverment: Digitalization of Coastal Management and Decision Making Supported by Multi-dimensional Geospatial Information and Analysis</data>
      <data key="e_abstract">EIA-0091494&lt;br/&gt;Ohio State University&lt;br/&gt;Rongxing Li&lt;br/&gt;&lt;br/&gt;TITLE: Digital Government: Digitalization of Coastal Management and Decision-Making Supported by Multi-dimensional Geospatial Information and Analysis&lt;br/&gt;&lt;br/&gt;With the continuing trend of increasing population and economic activities in the coastal zone, a sustainable coastal environment is crucial to the well being of human, wildlife, and properties in the zone. Recently, the coastal zone has been directly or indirectly affected by a series of problems such as global warming, climate change, seal level raising, coastal erosion, environmental contamination, and overpopulation. Coastal management and decision-making involve extremely comprehensive and critical operations across multiple governmental organizations, where the above problems and impacts have to be dealt with. Geospatial information and analysis can be used to coordinate and support multi-agency operations include a) federal government: coastal erosion monitoring, coastal resource management, coastal forecasting, coastal development permission, and residential erosion awareness and watching system. The goal of this research is to investigate and develop technologies that will greatly enhance operation capabilities of federal, state, and local government agencies for coastal management and decision-making using multiple spaceborne, airborne, and in situ remotely-sensed measurements, spatio-temporal databases, and geospatial analysis tools.&lt;br/&gt;&lt;br/&gt;The objectives of this research project are:&lt;br/&gt;a) develop a strategy and technologies for utilizing geospatial information of all level of government agencies in coastal management and decision-making.&lt;br/&gt;b) develop and integrate technologies of space- and airborne and insitu spatial data acquisition, spatio-temporal data modeling and analysis, hydrological modeling and forecasting, and web-based information systems for digital government operations,&lt;br/&gt;c) develop and test an integrated coastal management and decision-making system that incorporates the spatial analysis and hydrological modeling technologies that will examine historical coastal environment and future changes based on desired or proposed scenarios,&lt;br/&gt;d) test and implement the above strategy and the system to demonstrate the significantly improved interagency operations of coastal decision-making through a pilot project in the primary site in Lake Erie area and another test site in the Tampa Bay area, and&lt;br/&gt;e) disseminate coastal geospatial data and coastal erosion information to end users including coastal residents in erosion areas and to assist them in making their decisions such as property protection measures, property purchasing/selling, and small community planning activities.&lt;br/&gt;&lt;br/&gt;If successfully implemented, this project will significantly enhance our capability of handling spatio-temporal coastal databases, build a fundamental basis of coastal geospatial information for inter-governmental agency operations, and provide innovative tools for all levels of governmental agencies to improve efficiency and reduce operational costs.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91494</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91494</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n272" target="n274">
      <data key="e_effectiveDate">2001-06-15</data>
      <data key="e_title">Digital Goverment: Digitalization of Coastal Management and Decision Making Supported by Multi-dimensional Geospatial Information and Analysis</data>
      <data key="e_abstract">EIA-0091494&lt;br/&gt;Ohio State University&lt;br/&gt;Rongxing Li&lt;br/&gt;&lt;br/&gt;TITLE: Digital Government: Digitalization of Coastal Management and Decision-Making Supported by Multi-dimensional Geospatial Information and Analysis&lt;br/&gt;&lt;br/&gt;With the continuing trend of increasing population and economic activities in the coastal zone, a sustainable coastal environment is crucial to the well being of human, wildlife, and properties in the zone. Recently, the coastal zone has been directly or indirectly affected by a series of problems such as global warming, climate change, seal level raising, coastal erosion, environmental contamination, and overpopulation. Coastal management and decision-making involve extremely comprehensive and critical operations across multiple governmental organizations, where the above problems and impacts have to be dealt with. Geospatial information and analysis can be used to coordinate and support multi-agency operations include a) federal government: coastal erosion monitoring, coastal resource management, coastal forecasting, coastal development permission, and residential erosion awareness and watching system. The goal of this research is to investigate and develop technologies that will greatly enhance operation capabilities of federal, state, and local government agencies for coastal management and decision-making using multiple spaceborne, airborne, and in situ remotely-sensed measurements, spatio-temporal databases, and geospatial analysis tools.&lt;br/&gt;&lt;br/&gt;The objectives of this research project are:&lt;br/&gt;a) develop a strategy and technologies for utilizing geospatial information of all level of government agencies in coastal management and decision-making.&lt;br/&gt;b) develop and integrate technologies of space- and airborne and insitu spatial data acquisition, spatio-temporal data modeling and analysis, hydrological modeling and forecasting, and web-based information systems for digital government operations,&lt;br/&gt;c) develop and test an integrated coastal management and decision-making system that incorporates the spatial analysis and hydrological modeling technologies that will examine historical coastal environment and future changes based on desired or proposed scenarios,&lt;br/&gt;d) test and implement the above strategy and the system to demonstrate the significantly improved interagency operations of coastal decision-making through a pilot project in the primary site in Lake Erie area and another test site in the Tampa Bay area, and&lt;br/&gt;e) disseminate coastal geospatial data and coastal erosion information to end users including coastal residents in erosion areas and to assist them in making their decisions such as property protection measures, property purchasing/selling, and small community planning activities.&lt;br/&gt;&lt;br/&gt;If successfully implemented, this project will significantly enhance our capability of handling spatio-temporal coastal databases, build a fundamental basis of coastal geospatial information for inter-governmental agency operations, and provide innovative tools for all levels of governmental agencies to improve efficiency and reduce operational costs.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91494</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91494</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n273" target="n274">
      <data key="e_effectiveDate">2001-06-15</data>
      <data key="e_title">Digital Goverment: Digitalization of Coastal Management and Decision Making Supported by Multi-dimensional Geospatial Information and Analysis</data>
      <data key="e_abstract">EIA-0091494&lt;br/&gt;Ohio State University&lt;br/&gt;Rongxing Li&lt;br/&gt;&lt;br/&gt;TITLE: Digital Government: Digitalization of Coastal Management and Decision-Making Supported by Multi-dimensional Geospatial Information and Analysis&lt;br/&gt;&lt;br/&gt;With the continuing trend of increasing population and economic activities in the coastal zone, a sustainable coastal environment is crucial to the well being of human, wildlife, and properties in the zone. Recently, the coastal zone has been directly or indirectly affected by a series of problems such as global warming, climate change, seal level raising, coastal erosion, environmental contamination, and overpopulation. Coastal management and decision-making involve extremely comprehensive and critical operations across multiple governmental organizations, where the above problems and impacts have to be dealt with. Geospatial information and analysis can be used to coordinate and support multi-agency operations include a) federal government: coastal erosion monitoring, coastal resource management, coastal forecasting, coastal development permission, and residential erosion awareness and watching system. The goal of this research is to investigate and develop technologies that will greatly enhance operation capabilities of federal, state, and local government agencies for coastal management and decision-making using multiple spaceborne, airborne, and in situ remotely-sensed measurements, spatio-temporal databases, and geospatial analysis tools.&lt;br/&gt;&lt;br/&gt;The objectives of this research project are:&lt;br/&gt;a) develop a strategy and technologies for utilizing geospatial information of all level of government agencies in coastal management and decision-making.&lt;br/&gt;b) develop and integrate technologies of space- and airborne and insitu spatial data acquisition, spatio-temporal data modeling and analysis, hydrological modeling and forecasting, and web-based information systems for digital government operations,&lt;br/&gt;c) develop and test an integrated coastal management and decision-making system that incorporates the spatial analysis and hydrological modeling technologies that will examine historical coastal environment and future changes based on desired or proposed scenarios,&lt;br/&gt;d) test and implement the above strategy and the system to demonstrate the significantly improved interagency operations of coastal decision-making through a pilot project in the primary site in Lake Erie area and another test site in the Tampa Bay area, and&lt;br/&gt;e) disseminate coastal geospatial data and coastal erosion information to end users including coastal residents in erosion areas and to assist them in making their decisions such as property protection measures, property purchasing/selling, and small community planning activities.&lt;br/&gt;&lt;br/&gt;If successfully implemented, this project will significantly enhance our capability of handling spatio-temporal coastal databases, build a fundamental basis of coastal geospatial information for inter-governmental agency operations, and provide innovative tools for all levels of governmental agencies to improve efficiency and reduce operational costs.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91494</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91494</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n278" target="n279">
      <data key="e_effectiveDate">2001-06-01</data>
      <data key="e_title">NER: Dynamic Behavior of Ligand-Receptor Interactions in Living Cells on the Nanoscale</data>
      <data key="e_abstract">This research project, supported by the Division of Integrative Biology and Neuroscience, will be carried out by Dr. Rong Wang, Dr. Nickolas Menhart and their students at the Illinois Institute of Technology. The target of the research is to develop a novel approach that allows a real-time study of the dynamic behavior of individual ligand-receptor pairs (typical size of several nanometers) in the living cell environment. The essence of this approach is to guide the tip of the atomic force microscope (AFM) to desired receptor proteins regardless of the roughness and complexity on the cell membrane surface. Besides imaging of biomolecules at the submolecular level under physiological conditions, dynamic and kinetic processes of the biorecognition events can be clarified on the nanoscale. &lt;br/&gt;The research will involve novel experiments aimed at fundamental studies of single ligand-receptor interaction in the natural environment. This will provide the molecular basis for biological activities and molecular communications within cells. One of the promising applications is to elucidate a vaccine or drug target at the particular cell-surface protein in a diseased cell or an activator target in a growth cell. This revolutionary approach shows strong promise to elevate the development of the fundamental understanding of molecular functions in bioscience to an entirely new level, and will stimulate progress in the study of biological and biologically inspired systems in which nanostructures play an important role.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">103080</data>
      <data key="e_expirationDate">2003-05-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103080</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n281" target="n282">
      <data key="e_effectiveDate">2001-06-01</data>
      <data key="e_title">Multiple Description Coding with Correlating Transforms for Multiple Antenna Wireless Systems</data>
      <data key="e_abstract">An effective way of providing error resilience for multimedia transmission in a&lt;br/&gt;communication system with a relatively small reduction in efficiency is multiple&lt;br/&gt;description coding (MDC), which assumes the existence of multiple independent&lt;br/&gt;channels between the transmitter and receiver, each of which can be temporarily&lt;br/&gt;down or can experience burst errors. With MDC several coded streams, called&lt;br/&gt;descriptions, are generated and transmitted over different channels. At the&lt;br/&gt;destination, if all of the streams are received error free, than the signal can&lt;br/&gt;be reconstructed at its highest level of fidelity. However, if only one or a few&lt;br/&gt;descriptions are received in a usable form, the receiver can still reconstruct&lt;br/&gt;an acceptable signal. All multiple description coding methods to date assume an&lt;br/&gt;on-off channel model between the transmitter and the receiver; each link is&lt;br/&gt;either broken, in which case the transmitted symbols, or packets, are lost&lt;br/&gt;completely, or it functions properly, in which case the packets are received&lt;br/&gt;free of errors. This model is appropriate for Internet transmission, but it is&lt;br/&gt;not appropriate for wireless channels.&lt;br/&gt;&lt;br/&gt;This study replaces the parallel independent on-off channel model with a&lt;br/&gt;wireless channel model, such as a Rayleigh fading model. Communication is&lt;br/&gt;performed using multiple transmit and receive antennas over the channel. With&lt;br/&gt;these models the signal at any of the receive antennas is the superposition of&lt;br/&gt;the transmitted signals from each transmit antenna independently faded.&lt;br/&gt;Therefore, even if the descriptions at the receiver side are completely&lt;br/&gt;independent, the received signal at each antenna will include some information&lt;br/&gt;from each description. This research involves finding the best multiple&lt;br/&gt;description coding strategy for these channels, the theoretical limits of such a&lt;br/&gt;scheme, and the efficiency with which it can be implemented.</data>
      <data key="e_pgm">4720</data>
      <data key="e_label">105654</data>
      <data key="e_expirationDate">2002-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105654</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n283" target="n284">
      <data key="e_effectiveDate">2001-06-01</data>
      <data key="e_title">Proposal for Supporting Travel Expenses to Attend the 7th IEEE Real-Time Technology and Application Symposium (RTAS) in Taipei, Taiwan</data>
      <data key="e_abstract">This proposal seeks to sponsor travel expenses for graduate students and junior faculty members&lt;br/&gt;to attend the 7th IEEE Real-Time Technology and Application Symposium (RTAS) to be held&lt;br/&gt;in Taipei, Taiwan and sponsored by the IEEE Computer Society Technical Committee on Real-&lt;br/&gt;Time Systems. RTAS provides a forum for presenting the latest advances in real-time systems&lt;br/&gt;research and for discussing the practical challenges encountered and the solutions adopted. A&lt;br/&gt;special mission of RTAS 2001 is to foster discussion and technology exchange in the real-time&lt;br/&gt;computing and communication communities between Asia and North America. As such, we have&lt;br/&gt;invited Dr. C. L. Liu (who developed the rate-monotonic scheduling theory in the 70&apos;s and is&lt;br/&gt;currently the president of National Ching-Hua University in Hsin-Chu, Taiwan) to serve as the&lt;br/&gt;Honorary Chair, and top-notch researchers in both Asia, Americas, and Europe to serve on the&lt;br/&gt;Technical Program Committee. In addition to single-track technical sessions, we will also organize&lt;br/&gt;tutorials and workshops that address practical, temporal QoS issues in application domains or over&lt;br/&gt;the Internet.&lt;br/&gt;&lt;br/&gt;The symposium, as many other academic activities, relies on the experience of senior members&lt;br/&gt;of the community but requires fresh ideas and new people to join the community. As such, and&lt;br/&gt;to encourage graduate students and young faculty members to participate in the exciting technical&lt;br/&gt;program, we are requesting support to defray the travel expenses for graduate students and junior&lt;br/&gt;faculty attendees. The total amount requested is $12,000 on the basis of an average travel grant&lt;br/&gt;of $1,200 for each selected junior faculty participant and $800 for each selected graduate student.&lt;br/&gt;We believe attendance at this Symposium will be very beneficial to young scientists who otherwise&lt;br/&gt;would be unable to participlate in the gathering. Contacts for future (international) collaborations&lt;br/&gt;will be made and idea exchange will certainly occur. Travel support for student and junior faculty&lt;br/&gt;attendance will be advertised nationwide and women, minorities and disabled students will be&lt;br/&gt;especially encouraged to apply for sponsorship.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">116796</data>
      <data key="e_expirationDate">2002-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">116796</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n289" target="n290">
      <data key="e_effectiveDate">2001-06-01</data>
      <data key="e_title">Augmented Reality of the Natural World and its Psychological Effects: A Value-Sensitive Design Approach</data>
      <data key="e_abstract">Research shows that direct experiences with nature have beneficial effects on people&apos;s physical, cognitive, and emotional well-being. Yet what happens psychologically when advanced technologies augment human/nature interactions? Grounded in Value-Sensitive Design, this project represents an early, cohesive, and rigorous effort to answer this question. At the heart of this project lies five empirical studies. Study 1 involves a room with an augmented &quot;window&quot; view (a video plasma display of a real-time local nature scene). Study 2 involves whether it matters psychologically that an augmentation of a natural scene occurs in real-time. Study 3 involves robot &quot;pets&quot; in the lives of children and adolescents. Study 4 involves robot &quot;pets&quot; as possible companions for the elderly. Study 5 involves a &quot;telegarden&quot; (a telerobotic installation that allows Web users to garden by controlling a remote industrial robot arm). A diverse range of physiological, behavioral, and social-cognitive data are collected over short-term and longer-term conditions. In the coming years, augmented reality will become part of our everyday lives, and pervasive across diverse fields, such as architecture, education, gerontology, recreation, and ecommerce. This project contributes proactively to the design and use - from an ethical stance - of this emerging technology.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">102558</data>
      <data key="e_expirationDate">2005-05-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">102558</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n289" target="n291">
      <data key="e_effectiveDate">2001-06-01</data>
      <data key="e_title">Augmented Reality of the Natural World and its Psychological Effects: A Value-Sensitive Design Approach</data>
      <data key="e_abstract">Research shows that direct experiences with nature have beneficial effects on people&apos;s physical, cognitive, and emotional well-being. Yet what happens psychologically when advanced technologies augment human/nature interactions? Grounded in Value-Sensitive Design, this project represents an early, cohesive, and rigorous effort to answer this question. At the heart of this project lies five empirical studies. Study 1 involves a room with an augmented &quot;window&quot; view (a video plasma display of a real-time local nature scene). Study 2 involves whether it matters psychologically that an augmentation of a natural scene occurs in real-time. Study 3 involves robot &quot;pets&quot; in the lives of children and adolescents. Study 4 involves robot &quot;pets&quot; as possible companions for the elderly. Study 5 involves a &quot;telegarden&quot; (a telerobotic installation that allows Web users to garden by controlling a remote industrial robot arm). A diverse range of physiological, behavioral, and social-cognitive data are collected over short-term and longer-term conditions. In the coming years, augmented reality will become part of our everyday lives, and pervasive across diverse fields, such as architecture, education, gerontology, recreation, and ecommerce. This project contributes proactively to the design and use - from an ethical stance - of this emerging technology.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">102558</data>
      <data key="e_expirationDate">2005-05-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">102558</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n290" target="n291">
      <data key="e_effectiveDate">2001-06-01</data>
      <data key="e_title">Augmented Reality of the Natural World and its Psychological Effects: A Value-Sensitive Design Approach</data>
      <data key="e_abstract">Research shows that direct experiences with nature have beneficial effects on people&apos;s physical, cognitive, and emotional well-being. Yet what happens psychologically when advanced technologies augment human/nature interactions? Grounded in Value-Sensitive Design, this project represents an early, cohesive, and rigorous effort to answer this question. At the heart of this project lies five empirical studies. Study 1 involves a room with an augmented &quot;window&quot; view (a video plasma display of a real-time local nature scene). Study 2 involves whether it matters psychologically that an augmentation of a natural scene occurs in real-time. Study 3 involves robot &quot;pets&quot; in the lives of children and adolescents. Study 4 involves robot &quot;pets&quot; as possible companions for the elderly. Study 5 involves a &quot;telegarden&quot; (a telerobotic installation that allows Web users to garden by controlling a remote industrial robot arm). A diverse range of physiological, behavioral, and social-cognitive data are collected over short-term and longer-term conditions. In the coming years, augmented reality will become part of our everyday lives, and pervasive across diverse fields, such as architecture, education, gerontology, recreation, and ecommerce. This project contributes proactively to the design and use - from an ethical stance - of this emerging technology.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">102558</data>
      <data key="e_expirationDate">2005-05-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">102558</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n292" target="n293">
      <data key="e_effectiveDate">2001-06-01</data>
      <data key="e_title">SGER: Digital Aerial Video Mosaics for EcoSystem Carbon Cycle Modeling</data>
      <data key="e_abstract">EIA-0105272&lt;br/&gt;Edward Riseman&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;SGER: Digital Aerial Video Mosaics for Ecosystem Carbon Cycle Modeling&lt;br/&gt;&lt;br/&gt;This grant will support preliminary explorations for applying digital video aerial image analysis to long-term issues of global warming, in particular carbon cycle modeling, in the government arena. Many government agencies are already using GIS heavily, but in a less dynamic way, relying on single images in time, without fine-grain time references. This project will explore government uses of real-time imaging, at resolutions beyond what can be achieved with commercial satellite images, allowing the digital film to be geo-referenced, integrated with data streams from other sensors, and viewed in 3D through polarized glasses.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">105272</data>
      <data key="e_expirationDate">2002-05-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">105272</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n292" target="n294">
      <data key="e_effectiveDate">2001-06-01</data>
      <data key="e_title">SGER: Digital Aerial Video Mosaics for EcoSystem Carbon Cycle Modeling</data>
      <data key="e_abstract">EIA-0105272&lt;br/&gt;Edward Riseman&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;SGER: Digital Aerial Video Mosaics for Ecosystem Carbon Cycle Modeling&lt;br/&gt;&lt;br/&gt;This grant will support preliminary explorations for applying digital video aerial image analysis to long-term issues of global warming, in particular carbon cycle modeling, in the government arena. Many government agencies are already using GIS heavily, but in a less dynamic way, relying on single images in time, without fine-grain time references. This project will explore government uses of real-time imaging, at resolutions beyond what can be achieved with commercial satellite images, allowing the digital film to be geo-referenced, integrated with data streams from other sensors, and viewed in 3D through polarized glasses.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">105272</data>
      <data key="e_expirationDate">2002-05-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">105272</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n293" target="n294">
      <data key="e_effectiveDate">2001-06-01</data>
      <data key="e_title">SGER: Digital Aerial Video Mosaics for EcoSystem Carbon Cycle Modeling</data>
      <data key="e_abstract">EIA-0105272&lt;br/&gt;Edward Riseman&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;SGER: Digital Aerial Video Mosaics for Ecosystem Carbon Cycle Modeling&lt;br/&gt;&lt;br/&gt;This grant will support preliminary explorations for applying digital video aerial image analysis to long-term issues of global warming, in particular carbon cycle modeling, in the government arena. Many government agencies are already using GIS heavily, but in a less dynamic way, relying on single images in time, without fine-grain time references. This project will explore government uses of real-time imaging, at resolutions beyond what can be achieved with commercial satellite images, allowing the digital film to be geo-referenced, integrated with data streams from other sensors, and viewed in 3D through polarized glasses.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">105272</data>
      <data key="e_expirationDate">2002-05-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">105272</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n296" target="n297">
      <data key="e_effectiveDate">2001-06-01</data>
      <data key="e_title">Code Modulated Self-Encoded Multiple Access Communications</data>
      <data key="e_abstract">This project investigates novel self-encoded multiple access (SEMA) communications for the transmission of digital information in multiuser wireless channels. The approach is based on the unconventional self-encoded spread spectrum technique that has been developed at the University of Nebraska for the modulation and detection of spread spectrum signals. The application of self-encoded spread spectrum to multiple access communications entails a number of considerations in system and coding designs. They arise from the fact that self-encoding does not guarantee pair-wise code isolations between the spreading sequences. Multiuser channel coding can mitigate code collisions and improve the system performance. The research will characterize the system performance under various wireless channel disturbances such as noise, interference and fading. The results from this project demonstrate the feasibility of SEMA communications that will have a developmental impact on mobile communications technology.&lt;br/&gt;&lt;br/&gt;The focus of this project is the analysis and applications of code-modulated SEMA communications. An analytical model of the system is developed that incorporates multiuser channel coding to control statistical code collisions between the self-encoding sequences. Cooperative channel coding among the users is exploited to determine the balanced trade-offs between signal despreading, capacity (code rate) and error correction capability. The bit-error-rate and capacity of the system are analyzed under various channel conditions. The research includes investigating the use of training sequences and preambles for synchronization acquisition and tracking of self-encoded signals. The development of SEMA multiuser detectors is of particular interest given the fact that the spreading sequences are random, time varying, and not known a priori. SEMA turbo receiver that employs iterative, joint multiuser detector and channel decoding is also studied. The goal of the research is to develop the theoretical foundation for self-encoded multiple access communications, with the potential applications spanning from terrestrial wireless to satellite and optical fiber communications systems.</data>
      <data key="e_pgm">4096</data>
      <data key="e_label">98273</data>
      <data key="e_expirationDate">2005-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98273</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n303" target="n304">
      <data key="e_effectiveDate">2001-06-01</data>
      <data key="e_title">Formal Methods for Extensible Object-Oriented Software</data>
      <data key="e_abstract">This project advances the theory and practice of extensible object-oriented (OO) software by investigating how to support careful design, specification, and reasoning. Enhancing theoretical understanding for specifying and verifying extensible OO software forms the first subproblem. The project investigates several avenues: the soundness of proof techniques for behavioral subtyping among abstract data types with mutable objects, whose operations may have nondeterministic specifications; the extent to which the use of multimethods affects one&apos;s ability to prove behavioral subtyping; and the soundness of a specification and verification technique that allows the implementation of a subclass from a superclass&apos;s specification, without seeing the superclass&apos;s code.&lt;br/&gt;&lt;br/&gt;The second subproblem comprises enhancing Java and a specification language for Java to better support extensible OO frameworks and libraries. The project implements and refines MultiJava, an extension to Java that supports both open classes and multimethods. The former allow one to extend existing classes with new methods, while the latter allow one to easily extend both a set of data types and the methods that work on them. The project also extends and refines JML, which is a behavioral interface specification language tailored to Java.</data>
      <data key="e_pgm">2880</data>
      <data key="e_label">97907</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">97907</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n100" target="n101">
      <data key="e_effectiveDate">2001-06-15</data>
      <data key="e_title">2001 Language Engineering Workshop for Students and Professionals: Integrating Research and Education (WS01)</data>
      <data key="e_abstract">This is a standard award. Automated systems that interact with human users in spoken and written communication will greatly enhance productivity and program usability. These systems will allow friendly access to information services and are essential for people with disabilities, or for accessing databases while performing intricate tasks. Unfortunately, current technology is inadequate for the tasks at hand, and there is a need to make progress. The number of available personnel trained in the field must be vastly increased and solutions to long standing problems must be found. At this time, relatively few universities educate students capable of performing the required tasks. Leading professionals are scattered in various industrial, academic, and governmental institutions, often duplicating each other&apos;s work. This award will allow a summer workshop on language engineering to be conducted where mixed teams of leading professionals and students can cooperate to advance the state of the art. The professionals will normally be university professors and industrial and government researchers working in widely dispersed locations. Graduate and undergraduate students will join these teams. The participation of undergraduate students is intended not only as an educational opportunity, but also to broaden the appeal of the language engineering field amongst students considering graduate studies.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">97467</data>
      <data key="e_expirationDate">2003-05-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">97467</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n100" target="n102">
      <data key="e_effectiveDate">2001-06-15</data>
      <data key="e_title">2001 Language Engineering Workshop for Students and Professionals: Integrating Research and Education (WS01)</data>
      <data key="e_abstract">This is a standard award. Automated systems that interact with human users in spoken and written communication will greatly enhance productivity and program usability. These systems will allow friendly access to information services and are essential for people with disabilities, or for accessing databases while performing intricate tasks. Unfortunately, current technology is inadequate for the tasks at hand, and there is a need to make progress. The number of available personnel trained in the field must be vastly increased and solutions to long standing problems must be found. At this time, relatively few universities educate students capable of performing the required tasks. Leading professionals are scattered in various industrial, academic, and governmental institutions, often duplicating each other&apos;s work. This award will allow a summer workshop on language engineering to be conducted where mixed teams of leading professionals and students can cooperate to advance the state of the art. The professionals will normally be university professors and industrial and government researchers working in widely dispersed locations. Graduate and undergraduate students will join these teams. The participation of undergraduate students is intended not only as an educational opportunity, but also to broaden the appeal of the language engineering field amongst students considering graduate studies.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">97467</data>
      <data key="e_expirationDate">2003-05-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">97467</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n101" target="n102">
      <data key="e_effectiveDate">2001-06-15</data>
      <data key="e_title">2001 Language Engineering Workshop for Students and Professionals: Integrating Research and Education (WS01)</data>
      <data key="e_abstract">This is a standard award. Automated systems that interact with human users in spoken and written communication will greatly enhance productivity and program usability. These systems will allow friendly access to information services and are essential for people with disabilities, or for accessing databases while performing intricate tasks. Unfortunately, current technology is inadequate for the tasks at hand, and there is a need to make progress. The number of available personnel trained in the field must be vastly increased and solutions to long standing problems must be found. At this time, relatively few universities educate students capable of performing the required tasks. Leading professionals are scattered in various industrial, academic, and governmental institutions, often duplicating each other&apos;s work. This award will allow a summer workshop on language engineering to be conducted where mixed teams of leading professionals and students can cooperate to advance the state of the art. The professionals will normally be university professors and industrial and government researchers working in widely dispersed locations. Graduate and undergraduate students will join these teams. The participation of undergraduate students is intended not only as an educational opportunity, but also to broaden the appeal of the language engineering field amongst students considering graduate studies.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">97467</data>
      <data key="e_expirationDate">2003-05-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">97467</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n314" target="n315">
      <data key="e_effectiveDate">2001-06-01</data>
      <data key="e_title">REU Site: Developing Undergraduate Research in Computer Network-Based Intrusion Detection and Information System Protection</data>
      <data key="e_abstract">EIA-0097858&lt;br/&gt;Frincke, Deborah&lt;br/&gt;University of Idaho&lt;br/&gt;&lt;br/&gt;REU SITES: Developing Undergraduate Research in Computer Network-Based Intruision Detection and Information System Protection&lt;br/&gt;&lt;br/&gt;The need for technologically literate workers has reached a critical level. The US Department of commerce predicts that by 2006, the United States will need more than 1.3 million new technology workers. Further, there is a growing consensus that the lack of trained information security an information assurance personnel has reached or soon will reach crisis proportions. This REU is intended to address the shortage of information assurance researchers by providing opportunities for undergraduates to participate in ongoing research projects under the auspices of the CSDS. The Center for Secure and Dependable Software (CSDS) at the University of Idaho (UI) was formed in 1998 as an Idaho Center to better organize and promote information security education and research. The CSDS/University of Idaho was named as one of the first seven Centers of Excellence in Information Assurance Education, and is therefore uniquely suited to lead this initiative.&lt;br/&gt;&lt;br/&gt; Our specific objectives are to include 14 undergraduate REU participants yearly (42 total) in a program, which is intended:&lt;br/&gt;&lt;br/&gt;1. To address the national need for researchers and &quot;thinkers&quot; with competencies in critical infrastructure protection by involving undergraduates in this area of research.&lt;br/&gt;2. To provide an &quot;learn by doing&quot; environment in which motivated undergraduates learn how to take charge of their research in a disciplined and creative manner, and to lead others in such endeavors.&lt;br/&gt;3. To offer a unique research experiences to people who would not otherwise have such opportunities.&lt;br/&gt;4. To stimulate undergraduates to pursue graduate education or research careers in information assurance.&lt;br/&gt;5. To attract a diverse student population to this field of study.&lt;br/&gt;6. To provoke discussion and consideration of the vast array of ethical issues involved in computer security and privacy research and practice, both for the REU participants and later at her REU participants&apos; home site.&lt;br/&gt;&lt;br/&gt;Key components of our program include: a faculty with considerable experience in working with undergraduates in a research environment; immediate integration of students in existing research programs in information assurance, drawn from the PI and senior investigator&apos;s currently funded research: an emphasis on critical thinking, leadership, presentation, and publication skills; the Expanding Horizons seminars, which bring students into direct contact with highly regarded members of the information assurance field, with practitioners, and with those involved in setting national policy in this area; also an optional ethics component in the form of an integrated sequence of lectures and discussion sections to be led by senior investigators with considerable expertise in this area. A service aspect is also included, where students lead a project of their choice to solve specific real problems in information assurance.&lt;br/&gt;&lt;br/&gt;The program is geared towards students who would not normally have an opportunity to work in research or in this area. Recruitment will emphasize diversity, both to enrich the student experience and to bring underrepresented groups in the national security/privacy/information assurance policy and technology debates. Ongoing assessment is emphasized in terms both of examining how student capabilities change through their participation in this program as well as in tracking student career choices after the program ends.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">97858</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">97858</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n322" target="n323">
      <data key="e_effectiveDate">2001-06-15</data>
      <data key="e_title">Digital Government: Scalable Data Collection Infrastructure for Digital Government Applications</data>
      <data key="e_abstract">EIA 0091474&lt;br/&gt;Hanan Samet&lt;br/&gt;University of Maryland&lt;br/&gt;&lt;br/&gt;Digital Government: Scalable Data Collection Infrastructure for Digital Government Applications&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project in collaboration with the Internal Revenue Service, the Department of Justice, and the US Census Bureau will explore the collection of data over wide-area networks. Of particular interest are situations with high submission volume from a variety of sites to one agency (hot spots); e.g. the submission of electronic tax forms to IRS in the April 14-14 timeframe. Within that context, research issues will include scalability, and privacy/integrity. Other possible government applications include submissions of proposals at deadlines, such as the NSF Fastlane system, and voting in digital democracy. The technology to be developed will work at the application layer of TCP/IP, implementing a small set of primitive services.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91474</data>
      <data key="e_expirationDate">2007-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91474</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n322" target="n324">
      <data key="e_effectiveDate">2001-06-15</data>
      <data key="e_title">Digital Government: Scalable Data Collection Infrastructure for Digital Government Applications</data>
      <data key="e_abstract">EIA 0091474&lt;br/&gt;Hanan Samet&lt;br/&gt;University of Maryland&lt;br/&gt;&lt;br/&gt;Digital Government: Scalable Data Collection Infrastructure for Digital Government Applications&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project in collaboration with the Internal Revenue Service, the Department of Justice, and the US Census Bureau will explore the collection of data over wide-area networks. Of particular interest are situations with high submission volume from a variety of sites to one agency (hot spots); e.g. the submission of electronic tax forms to IRS in the April 14-14 timeframe. Within that context, research issues will include scalability, and privacy/integrity. Other possible government applications include submissions of proposals at deadlines, such as the NSF Fastlane system, and voting in digital democracy. The technology to be developed will work at the application layer of TCP/IP, implementing a small set of primitive services.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91474</data>
      <data key="e_expirationDate">2007-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91474</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n323" target="n324">
      <data key="e_effectiveDate">2001-06-15</data>
      <data key="e_title">Digital Government: Scalable Data Collection Infrastructure for Digital Government Applications</data>
      <data key="e_abstract">EIA 0091474&lt;br/&gt;Hanan Samet&lt;br/&gt;University of Maryland&lt;br/&gt;&lt;br/&gt;Digital Government: Scalable Data Collection Infrastructure for Digital Government Applications&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project in collaboration with the Internal Revenue Service, the Department of Justice, and the US Census Bureau will explore the collection of data over wide-area networks. Of particular interest are situations with high submission volume from a variety of sites to one agency (hot spots); e.g. the submission of electronic tax forms to IRS in the April 14-14 timeframe. Within that context, research issues will include scalability, and privacy/integrity. Other possible government applications include submissions of proposals at deadlines, such as the NSF Fastlane system, and voting in digital democracy. The technology to be developed will work at the application layer of TCP/IP, implementing a small set of primitive services.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91474</data>
      <data key="e_expirationDate">2007-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91474</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n328" target="n329">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">NER: Fabrication of Fullerene Based Novel Molecular Electronic Devices Using Quantum Mechanical Simulations</data>
      <data key="e_abstract">This proposal was received in response to NSE, NSF-0019. A novel approach for the fabrication of atomistic electronic devices is proposed which, if realized, will have important industrial applications. The theoretical methods involve simulations using quantum tight-binding molecular dynamics scheme that can be used to accurately treat interactions in carbon systems at the nanoscale level. Large scale simulations will be performed using novel parallel computer algorithms using a synergistic interdisciplinary collaboration. Simulation results can be used as a guide in the experimental investigations.&lt;br/&gt;&lt;br/&gt;Although the present electronic technology is dominated by silicon, it is becoming clear that Si based electronic devices cannot be relied on to sustain the current pace of miniaturization. It is&lt;br/&gt;becoming clear that a new class of molecularly perfect materials are needed to make these new devices. Single-wall carbon nanotubes are one such material that are expected to execute a ``quantum&apos;&apos; leap i the area of nanoscale electronics, computers, and materials. A focussed effort to lay the foundation for fullerene and nanotube based molecular electronics which will revolutionize the electronics and computer industries is proposed. The emphasis is on the modeling and simulations which will be used to guide experimental efforts to realize these devices. The theoretical method for the treatment of these systems contains many state-of-the-art features, making it ideally suited for studying these systems.</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">102345</data>
      <data key="e_expirationDate">2003-06-30</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">102345</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n332" target="n333">
      <data key="e_effectiveDate">2001-07-15</data>
      <data key="e_title">Nonmonotonic Reasoning and Computational Knowledge Representation</data>
      <data key="e_abstract">IIS-0097278&lt;br/&gt;Miroslaw Truszczynski, Raphael A. Finkel and Victor Marek&lt;br/&gt;University of Kentucky&lt;br/&gt;$146,167 - 12 mos&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Nonmonotonic Reasoning and Computational Knowledge Representation&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This is the first year funding of a three year continuing award. The PI will study and implement computational knowledge-representation systems based on the paradigm of answer-set programming (ASP) with nonmonotonic logic that he recently identified, in order to demonstrate the practicality and effectiveness of the approach. Logic is most commonly used in knowledge representation as follows. To solve a problem we represent its constraints and the relevant background knowledge as a theory in the language of first-order logic (or its fragment). We formulate the goal (the statement of the problem) as a formula of the logic. We then use proof techniques to decide whether this formula follows from the theory. A proof of the formula, variable substitutions or both determine a solution. Taking a different approach, the PI will study and develop computational knowledge representation tools based on nonmonotonic logics rather than on the first-order logic. In addition, he departs from the single-intended model approach dominant in logic programming. Under the ASP paradigm, a theory in a nonmonotonic formalism is regarded as a specification of a family of sets - a collection of its intended models. Each model is viewed as a representation of a different single solution. The PI will investigate syntactic and semantic issues of ASP formalisms based on nonmonotonic logics, study methods for fast computing with these formalisms, develop practical implementations, and demonstrate effectiveness of answer-set programming engines and their applicability in knowledge representation. If successful, the work will establish answer-set programming as a viable approach to declarative programming, which in turn will provide AI researchers and practitioners with a new generation of computational tools for knowledge representation.</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">97278</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">97278</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n332" target="n334">
      <data key="e_effectiveDate">2001-07-15</data>
      <data key="e_title">Nonmonotonic Reasoning and Computational Knowledge Representation</data>
      <data key="e_abstract">IIS-0097278&lt;br/&gt;Miroslaw Truszczynski, Raphael A. Finkel and Victor Marek&lt;br/&gt;University of Kentucky&lt;br/&gt;$146,167 - 12 mos&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Nonmonotonic Reasoning and Computational Knowledge Representation&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This is the first year funding of a three year continuing award. The PI will study and implement computational knowledge-representation systems based on the paradigm of answer-set programming (ASP) with nonmonotonic logic that he recently identified, in order to demonstrate the practicality and effectiveness of the approach. Logic is most commonly used in knowledge representation as follows. To solve a problem we represent its constraints and the relevant background knowledge as a theory in the language of first-order logic (or its fragment). We formulate the goal (the statement of the problem) as a formula of the logic. We then use proof techniques to decide whether this formula follows from the theory. A proof of the formula, variable substitutions or both determine a solution. Taking a different approach, the PI will study and develop computational knowledge representation tools based on nonmonotonic logics rather than on the first-order logic. In addition, he departs from the single-intended model approach dominant in logic programming. Under the ASP paradigm, a theory in a nonmonotonic formalism is regarded as a specification of a family of sets - a collection of its intended models. Each model is viewed as a representation of a different single solution. The PI will investigate syntactic and semantic issues of ASP formalisms based on nonmonotonic logics, study methods for fast computing with these formalisms, develop practical implementations, and demonstrate effectiveness of answer-set programming engines and their applicability in knowledge representation. If successful, the work will establish answer-set programming as a viable approach to declarative programming, which in turn will provide AI researchers and practitioners with a new generation of computational tools for knowledge representation.</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">97278</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">97278</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n333" target="n334">
      <data key="e_effectiveDate">2001-07-15</data>
      <data key="e_title">Nonmonotonic Reasoning and Computational Knowledge Representation</data>
      <data key="e_abstract">IIS-0097278&lt;br/&gt;Miroslaw Truszczynski, Raphael A. Finkel and Victor Marek&lt;br/&gt;University of Kentucky&lt;br/&gt;$146,167 - 12 mos&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Nonmonotonic Reasoning and Computational Knowledge Representation&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This is the first year funding of a three year continuing award. The PI will study and implement computational knowledge-representation systems based on the paradigm of answer-set programming (ASP) with nonmonotonic logic that he recently identified, in order to demonstrate the practicality and effectiveness of the approach. Logic is most commonly used in knowledge representation as follows. To solve a problem we represent its constraints and the relevant background knowledge as a theory in the language of first-order logic (or its fragment). We formulate the goal (the statement of the problem) as a formula of the logic. We then use proof techniques to decide whether this formula follows from the theory. A proof of the formula, variable substitutions or both determine a solution. Taking a different approach, the PI will study and develop computational knowledge representation tools based on nonmonotonic logics rather than on the first-order logic. In addition, he departs from the single-intended model approach dominant in logic programming. Under the ASP paradigm, a theory in a nonmonotonic formalism is regarded as a specification of a family of sets - a collection of its intended models. Each model is viewed as a representation of a different single solution. The PI will investigate syntactic and semantic issues of ASP formalisms based on nonmonotonic logics, study methods for fast computing with these formalisms, develop practical implementations, and demonstrate effectiveness of answer-set programming engines and their applicability in knowledge representation. If successful, the work will establish answer-set programming as a viable approach to declarative programming, which in turn will provide AI researchers and practitioners with a new generation of computational tools for knowledge representation.</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">97278</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">97278</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n335" target="n336">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">NER: Quantum Nanosensors Based on Controllable Electron-Phonon Coupling</data>
      <data key="e_abstract">This proposal was received in response to NSE, NSF-0019. Sensitivity at the level of individual quanta is required for such diverse applications of nanosensors as characterization of macromolecules and biological objects, monitoring of molecular binding, and control of dephasing processes in quantum dots. In nanoscale structures the phonon exchange is too fast, and strong thermal (phonon) coupling between the sensor and its surroundings puts strict limitations on the sensitivity of ordinary bolometric sensors. In the hot-electron sensor, the incoming quanta overheat only electron states, which relax to equilibrium due to electron-phonon coupling. The sensitivity of hot-electron sensors can be improved by weakening the effective coupling between electrons and phonons. In nanoconductors, the electron-phonon interaction is substantially modified in comparison with the interaction in bulk materials. Due to the interference between electron-phonon and electron-boundary scattering, the electron relaxation/dephasing rate depends drastically on vibrations of boundaries. It may vary over a wide range, spanning several orders of magnitude, and may be controlled by selection of a substrate material. The proposed research includes complex investigations of the interference between electron scattering mechanisms in superconducting nanostructures and experimental demonstration of the electron energy relaxation controlled by elastic electron scattering from boundaries and defects as well as the design of a new hot-electron sensor with a record value of the noise equivalent power, NEP=10-20W/Hz1/2, and the energy resolution of 5 10-24J, which will be able to count individual low-energy quanta (photons or phonons).</data>
      <data key="e_pgm">2878</data>
      <data key="e_label">103072</data>
      <data key="e_expirationDate">2003-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">103072</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n337" target="n338">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Quantifying Intractability and the Complexity of Heuristics</data>
      <data key="e_abstract">Search and optimization problems are central to all areas of computer science&lt;br/&gt;and engineering. Finding the optimal layout for a VLSI circuit or the lowest &lt;br/&gt;energy configuration of a crystal are both examples of optimization problems.&lt;br/&gt;While such problems are believed to be intractable, requiring exponential time &lt;br/&gt;to solve the worst-case instances, many heuristic methods have been observed &lt;br/&gt;to be relatively successful on instances that arise in different applications.&lt;br/&gt;This project addresses questions concerning the quantitative measures of the &lt;br/&gt;intractability of search and optimization problems, as opposed to qualitative notions&lt;br/&gt;such as NP-completeness. The following are some of the questions addressed in this project:&lt;br/&gt;&lt;br/&gt;1. Which instances of optimization problems are the most intractable ones?&lt;br/&gt;&lt;br/&gt;2. Exactly how difficult are these problems? &lt;br/&gt;&lt;br/&gt;3. What are good heuristic methods for solving optimization problems ? &lt;br/&gt;When and how well do they work? &lt;br/&gt;&lt;br/&gt;4. Are specific non-complete problems such as factoring also intractable? &lt;br/&gt;&lt;br/&gt;5. How much does randomness help in solving problems? &lt;br/&gt;&lt;br/&gt;6. Are hard problems suitable for cryptographic applications?&lt;br/&gt;If so, what levels of security do they provide these applications? &lt;br/&gt;&lt;br/&gt;Unconditional answers to these questions first require solving the P=NP problem. &lt;br/&gt;However, this project will use two approaches to find the most likely answers to &lt;br/&gt;these questions. The first approach is to provide proofs resolving these issues under&lt;br/&gt;plausible complexity assumptions. The second approach is to examine restricted but &lt;br/&gt;powerful classes of algorithms that include the most successful heuristics for the problems&lt;br/&gt;under study. This approach will include attempts to both explain the success of &lt;br/&gt;such heuristics and to show limitations that can be used as a guide for the likely inherent&lt;br/&gt;complexity of the problems.</data>
      <data key="e_pgm">2860</data>
      <data key="e_label">98197</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98197</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n343" target="n344">
      <data key="e_effectiveDate">None</data>
      <data key="e_title">None</data>
      <data key="e_abstract">None</data>
      <data key="e_pgm">None</data>
      <data key="e_expirationDate">None</data>
      <data key="e_div">None</data>
      <data key="e_dir">None</data>
    </edge>
    <edge source="n343" target="n344">
      <data key="e_effectiveDate">2001-07-15</data>
      <data key="e_title">Digital Government: Connected Kids: Designing Database Software for Web Based Info Dissemination to Multiple Audiences</data>
      <data key="e_abstract">EIA-0091505&lt;br/&gt;Teresa Harrison&lt;br/&gt;Rensselaer Polytech Institute&lt;br/&gt;&lt;br/&gt;Digital Government: Connected Kids: Designing Database Software for Web Based Info Dissemination to Multiple Audiences&lt;br/&gt;&lt;br/&gt;In this project, computer and social scientist will collaborate with the city of Troy, NY. The focus will be on youth service educational and non-profit organizations to develop community information systems relevant to the topic area. Many sectors (youth, education, government, non-profit) will be involved in the project.&lt;br/&gt;&lt;br/&gt;A software package named CIRCLE will be developed to enable design, authoring and maintenance of multimedia applications by multiple and non-technical content developers and user groups. Various focus groups will be used to test CIRCLE&apos;s abilities and potential. In addition to computer science, an important research element will be to study the social processes associated with the construction and use of information systems for community audiences.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91505</data>
      <data key="e_expirationDate">2008-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91505</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n343" target="n346">
      <data key="e_effectiveDate">2001-07-15</data>
      <data key="e_title">Digital Government: Connected Kids: Designing Database Software for Web Based Info Dissemination to Multiple Audiences</data>
      <data key="e_abstract">EIA-0091505&lt;br/&gt;Teresa Harrison&lt;br/&gt;Rensselaer Polytech Institute&lt;br/&gt;&lt;br/&gt;Digital Government: Connected Kids: Designing Database Software for Web Based Info Dissemination to Multiple Audiences&lt;br/&gt;&lt;br/&gt;In this project, computer and social scientist will collaborate with the city of Troy, NY. The focus will be on youth service educational and non-profit organizations to develop community information systems relevant to the topic area. Many sectors (youth, education, government, non-profit) will be involved in the project.&lt;br/&gt;&lt;br/&gt;A software package named CIRCLE will be developed to enable design, authoring and maintenance of multimedia applications by multiple and non-technical content developers and user groups. Various focus groups will be used to test CIRCLE&apos;s abilities and potential. In addition to computer science, an important research element will be to study the social processes associated with the construction and use of information systems for community audiences.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91505</data>
      <data key="e_expirationDate">2008-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91505</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n344" target="n344">
      <data key="e_effectiveDate">2001-07-15</data>
      <data key="e_title">Digital Government: Connected Kids: Designing Database Software for Web Based Info Dissemination to Multiple Audiences</data>
      <data key="e_abstract">EIA-0091505&lt;br/&gt;Teresa Harrison&lt;br/&gt;Rensselaer Polytech Institute&lt;br/&gt;&lt;br/&gt;Digital Government: Connected Kids: Designing Database Software for Web Based Info Dissemination to Multiple Audiences&lt;br/&gt;&lt;br/&gt;In this project, computer and social scientist will collaborate with the city of Troy, NY. The focus will be on youth service educational and non-profit organizations to develop community information systems relevant to the topic area. Many sectors (youth, education, government, non-profit) will be involved in the project.&lt;br/&gt;&lt;br/&gt;A software package named CIRCLE will be developed to enable design, authoring and maintenance of multimedia applications by multiple and non-technical content developers and user groups. Various focus groups will be used to test CIRCLE&apos;s abilities and potential. In addition to computer science, an important research element will be to study the social processes associated with the construction and use of information systems for community audiences.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91505</data>
      <data key="e_expirationDate">2008-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91505</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n344" target="n346">
      <data key="e_effectiveDate">None</data>
      <data key="e_title">None</data>
      <data key="e_abstract">None</data>
      <data key="e_pgm">None</data>
      <data key="e_expirationDate">None</data>
      <data key="e_div">None</data>
      <data key="e_dir">None</data>
    </edge>
    <edge source="n344" target="n346">
      <data key="e_effectiveDate">2001-07-15</data>
      <data key="e_title">Digital Government: Connected Kids: Designing Database Software for Web Based Info Dissemination to Multiple Audiences</data>
      <data key="e_abstract">EIA-0091505&lt;br/&gt;Teresa Harrison&lt;br/&gt;Rensselaer Polytech Institute&lt;br/&gt;&lt;br/&gt;Digital Government: Connected Kids: Designing Database Software for Web Based Info Dissemination to Multiple Audiences&lt;br/&gt;&lt;br/&gt;In this project, computer and social scientist will collaborate with the city of Troy, NY. The focus will be on youth service educational and non-profit organizations to develop community information systems relevant to the topic area. Many sectors (youth, education, government, non-profit) will be involved in the project.&lt;br/&gt;&lt;br/&gt;A software package named CIRCLE will be developed to enable design, authoring and maintenance of multimedia applications by multiple and non-technical content developers and user groups. Various focus groups will be used to test CIRCLE&apos;s abilities and potential. In addition to computer science, an important research element will be to study the social processes associated with the construction and use of information systems for community audiences.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91505</data>
      <data key="e_expirationDate">2008-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91505</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n348" target="n349">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Experimental Analysis and Modeling of Digital Video Quality</data>
      <data key="e_abstract">Abstract &lt;br/&gt;Experimental Analysis and Modeling of Digital Video Quality&lt;br/&gt;&lt;br/&gt;The cost of the production and transmission of digital video imagery is large and roughly proportional to the information in the images. Consequently, information is routinely deleted from video imagery to reduce cost. This deletion reduces the quality of the imagery as perceived by human viewers. The ultimate objective of this project is to quantify and minimize this loss of quality. As a step toward this goal, an understanding of how humans evaluate the quality of video images needs to be developed. To gain this understanding a series of experiments are being performed in which human observers describe and evaluate defects that they detect in video images of the kind that are typical in digital video applications. These images are processed so that they incorporate defects similar to those that occur in digital video.&lt;br/&gt;&lt;br/&gt;The results of these experiments are then used to develop an understanding of how humans process and evaluate video imagery. Image processing methods are being used to create sets of video defects that are quite similar to typical video defects, but are under much greater control. These defects occur at random times and places within normal video clips. They are treated as visual signals presented in a context of normal video imagery. Established methods of visual pattern detection and visual pattern appearance analysis are applied to the study of these defects. These include the following tasks: detecting the presence of a defect, rating the annoyance caused by a defect, and analyzing the perceived features of a defect. The results of these experiments will be used to develop an increasingly detailed model of video quality evaluation by human observers.</data>
      <data key="e_pgm">4720</data>
      <data key="e_label">105404</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105404</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n350" target="n351">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">ITW: Women in Information Technology Workplaces: A Study of Women Computer Science Degree Recipients in the Software Industry</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">1713</data>
      <data key="e_label">196431</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">196431</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n350" target="n352">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">ITW: Women in Information Technology Workplaces: A Study of Women Computer Science Degree Recipients in the Software Industry</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">1713</data>
      <data key="e_label">196431</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">196431</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n351" target="n352">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">ITW: Women in Information Technology Workplaces: A Study of Women Computer Science Degree Recipients in the Software Industry</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">1713</data>
      <data key="e_label">196431</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">196431</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n354" target="n355">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Computer Science Approaches to Finance Problems: Computational Complexity and Efficient Algorithms</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">2860</data>
      <data key="e_label">296040</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">296040</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n356" target="n357">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">MRI: Acquisition of a Parallel Computing Facility for Computational Engineering</data>
      <data key="e_abstract">EIA-0116289&lt;br/&gt;Tayfun E. Tezduyar&lt;br/&gt;Rice University &lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Parallel Computing Facility for Computational Engineering&lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a range of computational engineering activities including computational mechanics, computational heat transfer, computational materials science, computational bioengineering, computational environmental engineering, computational mathematics, and computer science, with an emphasis on interdisciplinary projects. For example, work on shared memory in a network of processors has led to techniques which can be tested in a production environment in applications in large-scale turbulence control analyses and in molecular simulations of nanostructures.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116289</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116289</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n356" target="n358">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">MRI: Acquisition of a Parallel Computing Facility for Computational Engineering</data>
      <data key="e_abstract">EIA-0116289&lt;br/&gt;Tayfun E. Tezduyar&lt;br/&gt;Rice University &lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Parallel Computing Facility for Computational Engineering&lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a range of computational engineering activities including computational mechanics, computational heat transfer, computational materials science, computational bioengineering, computational environmental engineering, computational mathematics, and computer science, with an emphasis on interdisciplinary projects. For example, work on shared memory in a network of processors has led to techniques which can be tested in a production environment in applications in large-scale turbulence control analyses and in molecular simulations of nanostructures.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116289</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116289</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n356" target="n359">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">MRI: Acquisition of a Parallel Computing Facility for Computational Engineering</data>
      <data key="e_abstract">EIA-0116289&lt;br/&gt;Tayfun E. Tezduyar&lt;br/&gt;Rice University &lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Parallel Computing Facility for Computational Engineering&lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a range of computational engineering activities including computational mechanics, computational heat transfer, computational materials science, computational bioengineering, computational environmental engineering, computational mathematics, and computer science, with an emphasis on interdisciplinary projects. For example, work on shared memory in a network of processors has led to techniques which can be tested in a production environment in applications in large-scale turbulence control analyses and in molecular simulations of nanostructures.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116289</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116289</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n356" target="n360">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">MRI: Acquisition of a Parallel Computing Facility for Computational Engineering</data>
      <data key="e_abstract">EIA-0116289&lt;br/&gt;Tayfun E. Tezduyar&lt;br/&gt;Rice University &lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Parallel Computing Facility for Computational Engineering&lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a range of computational engineering activities including computational mechanics, computational heat transfer, computational materials science, computational bioengineering, computational environmental engineering, computational mathematics, and computer science, with an emphasis on interdisciplinary projects. For example, work on shared memory in a network of processors has led to techniques which can be tested in a production environment in applications in large-scale turbulence control analyses and in molecular simulations of nanostructures.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116289</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116289</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n357" target="n358">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">MRI: Acquisition of a Parallel Computing Facility for Computational Engineering</data>
      <data key="e_abstract">EIA-0116289&lt;br/&gt;Tayfun E. Tezduyar&lt;br/&gt;Rice University &lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Parallel Computing Facility for Computational Engineering&lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a range of computational engineering activities including computational mechanics, computational heat transfer, computational materials science, computational bioengineering, computational environmental engineering, computational mathematics, and computer science, with an emphasis on interdisciplinary projects. For example, work on shared memory in a network of processors has led to techniques which can be tested in a production environment in applications in large-scale turbulence control analyses and in molecular simulations of nanostructures.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116289</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116289</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n357" target="n359">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">MRI: Acquisition of a Parallel Computing Facility for Computational Engineering</data>
      <data key="e_abstract">EIA-0116289&lt;br/&gt;Tayfun E. Tezduyar&lt;br/&gt;Rice University &lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Parallel Computing Facility for Computational Engineering&lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a range of computational engineering activities including computational mechanics, computational heat transfer, computational materials science, computational bioengineering, computational environmental engineering, computational mathematics, and computer science, with an emphasis on interdisciplinary projects. For example, work on shared memory in a network of processors has led to techniques which can be tested in a production environment in applications in large-scale turbulence control analyses and in molecular simulations of nanostructures.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116289</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116289</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n357" target="n360">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">MRI: Acquisition of a Parallel Computing Facility for Computational Engineering</data>
      <data key="e_abstract">EIA-0116289&lt;br/&gt;Tayfun E. Tezduyar&lt;br/&gt;Rice University &lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Parallel Computing Facility for Computational Engineering&lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a range of computational engineering activities including computational mechanics, computational heat transfer, computational materials science, computational bioengineering, computational environmental engineering, computational mathematics, and computer science, with an emphasis on interdisciplinary projects. For example, work on shared memory in a network of processors has led to techniques which can be tested in a production environment in applications in large-scale turbulence control analyses and in molecular simulations of nanostructures.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116289</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116289</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n358" target="n359">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">MRI: Acquisition of a Parallel Computing Facility for Computational Engineering</data>
      <data key="e_abstract">EIA-0116289&lt;br/&gt;Tayfun E. Tezduyar&lt;br/&gt;Rice University &lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Parallel Computing Facility for Computational Engineering&lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a range of computational engineering activities including computational mechanics, computational heat transfer, computational materials science, computational bioengineering, computational environmental engineering, computational mathematics, and computer science, with an emphasis on interdisciplinary projects. For example, work on shared memory in a network of processors has led to techniques which can be tested in a production environment in applications in large-scale turbulence control analyses and in molecular simulations of nanostructures.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116289</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116289</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n358" target="n360">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">MRI: Acquisition of a Parallel Computing Facility for Computational Engineering</data>
      <data key="e_abstract">EIA-0116289&lt;br/&gt;Tayfun E. Tezduyar&lt;br/&gt;Rice University &lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Parallel Computing Facility for Computational Engineering&lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a range of computational engineering activities including computational mechanics, computational heat transfer, computational materials science, computational bioengineering, computational environmental engineering, computational mathematics, and computer science, with an emphasis on interdisciplinary projects. For example, work on shared memory in a network of processors has led to techniques which can be tested in a production environment in applications in large-scale turbulence control analyses and in molecular simulations of nanostructures.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116289</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116289</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n359" target="n360">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">MRI: Acquisition of a Parallel Computing Facility for Computational Engineering</data>
      <data key="e_abstract">EIA-0116289&lt;br/&gt;Tayfun E. Tezduyar&lt;br/&gt;Rice University &lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Parallel Computing Facility for Computational Engineering&lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a range of computational engineering activities including computational mechanics, computational heat transfer, computational materials science, computational bioengineering, computational environmental engineering, computational mathematics, and computer science, with an emphasis on interdisciplinary projects. For example, work on shared memory in a network of processors has led to techniques which can be tested in a production environment in applications in large-scale turbulence control analyses and in molecular simulations of nanostructures.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116289</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116289</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n361" target="n362">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Flexible and Robust Coding of Images and Video</data>
      <data key="e_abstract">Proposal # 0104800&lt;br/&gt;PI: Eve Riskin&lt;br/&gt;U of Washington&lt;br/&gt;&lt;br/&gt;In the not-too-distant future, downloading images and video to a handheld device will be as commonplace as viewing an image on a Web page is today. To enable this, high quality, flexible, and robust image and video compression algorithms will be required. Recently, Group Testing for Wavelets (GTW), a new type of wavelet coder based on group testing was developed. It offers competitive performance to the best compression coders available today and with further development, GTW could outperform them significantly.&lt;br/&gt;&lt;br/&gt;This research involves developing new algorithms for robust and flexible coding of images and video. Whereas&lt;br/&gt;GTW is used as a motivating example, the research is applicable to many compression algorithms and in many scenarios. First, a set of problems related to GTW will be explored. This includes applying GTW to wavelet packet decompositions and reduced complexity transforms; extending it to video and the new area of progressive geometry compression; and developing a theory for its strong performance.&lt;br/&gt;Next, methods for applying forward error correction to compressed data in a progressive manner will be investigated. Finally, unequal frame expansions will be used to recover from packet loss. The research will have practical use in many arenas including the Internet and wireless communications and will be included&lt;br/&gt;in a new undergraduate course on data compression, to be taught jointly by the two principal investigators.</data>
      <data key="e_pgm">4720</data>
      <data key="e_label">104800</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">104800</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n363" target="n363">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Understanding and Improving On-Line Planning Methods</data>
      <data key="e_abstract">This is the first year funding of a three year continuing award. A variety of on-line planning methods are used in artificial intelligence including, for example, real-time search methods such as LRTA*, reinforcement-learning methods such as Q-learning, and robot-navigation methods such as D*. The PIs intend to improve the performance of these and other on-line planning methods substantially so that, for example, future robot-navigation methods will be able to map unknown terrain significantly faster than is now possible, yet have the same advantageous properties as existing on-line planning methods. Many on-line planning methods, either always or most of the time, execute actions that move the agent in the perceived direction of the goal, that is, move the agent so that it reduces the estimates of the goal distances the most. However, the PIs preliminary theoretical results show that executing actions that move the agent in the perceived direction of the goal is usually not a good idea. For example, D* does not reach a goal location in unknown terrain with a minimal travel distance in the worst case. The key to improving the performance of these on-line planning methods then is to exploit the distance estimates that they maintain (or can maintain) in a way that is more directly related to the planning or learning objective. The PIs will study the properties of on-line planning methods both theoretically and experimentally, and will develop improved on-line planning methods that have the same interface as the existing methods, which allows users of these methods to easily substitute the new methods for the ones they are currently using. Side benefits of the proposed research include developing a test-bed for the experimental evaluation of robot navigation methods in unknown terrain, and creating a solid theoretical foundation for understanding robot-navigation methods in unknown terrain, including D*.</data>
      <data key="e_pgm">6856</data>
      <data key="e_label">98807</data>
      <data key="e_expirationDate">2006-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">98807</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n363" target="n365">
      <data key="e_effectiveDate">None</data>
      <data key="e_title">None</data>
      <data key="e_abstract">None</data>
      <data key="e_pgm">None</data>
      <data key="e_expirationDate">None</data>
      <data key="e_div">None</data>
      <data key="e_dir">None</data>
    </edge>
    <edge source="n363" target="n365">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Understanding and Improving On-Line Planning Methods</data>
      <data key="e_abstract">This is the first year funding of a three year continuing award. A variety of on-line planning methods are used in artificial intelligence including, for example, real-time search methods such as LRTA*, reinforcement-learning methods such as Q-learning, and robot-navigation methods such as D*. The PIs intend to improve the performance of these and other on-line planning methods substantially so that, for example, future robot-navigation methods will be able to map unknown terrain significantly faster than is now possible, yet have the same advantageous properties as existing on-line planning methods. Many on-line planning methods, either always or most of the time, execute actions that move the agent in the perceived direction of the goal, that is, move the agent so that it reduces the estimates of the goal distances the most. However, the PIs preliminary theoretical results show that executing actions that move the agent in the perceived direction of the goal is usually not a good idea. For example, D* does not reach a goal location in unknown terrain with a minimal travel distance in the worst case. The key to improving the performance of these on-line planning methods then is to exploit the distance estimates that they maintain (or can maintain) in a way that is more directly related to the planning or learning objective. The PIs will study the properties of on-line planning methods both theoretically and experimentally, and will develop improved on-line planning methods that have the same interface as the existing methods, which allows users of these methods to easily substitute the new methods for the ones they are currently using. Side benefits of the proposed research include developing a test-bed for the experimental evaluation of robot navigation methods in unknown terrain, and creating a solid theoretical foundation for understanding robot-navigation methods in unknown terrain, including D*.</data>
      <data key="e_pgm">6856</data>
      <data key="e_label">98807</data>
      <data key="e_expirationDate">2006-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">98807</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n163" target="n368">
      <data key="e_effectiveDate">2001-07-15</data>
      <data key="e_title">Computation for Structural Biology: Tools to Enable Dynamic 3-D Reconstruction of Time-varying Viral Structures</data>
      <data key="e_abstract">Proposal #0098156&lt;br/&gt;Purdue Research Foundation&lt;br/&gt;Doerschuk, Peter C&lt;br/&gt;&lt;br/&gt;A key challenge in computational structural biology is the determination of the 3-D structure of a virus, and&lt;br/&gt;especially dynamical changes in 3-D structure which are central to understanding the function of the virus. This information is central to rational design of drugs to combat viral infections and to the use of viruses for other purposes, e.g., as vehicles for the targeted delivery of drugs to specific organs. Solving these problems involves the development, analysis, implementation and use of new algorithms for two numerical computation problems, global optimization and multidimensional quadrature.&lt;br/&gt;&lt;br/&gt;The investigators compute a 3-D structure by locating the global minimum of a cost which is a function of experimental data and of a predictor, and which quantitates the difference between the data and the prediction of the data. The predictor, whose evaluation requires multidimensional quadrature, is a function of parameters describing the 3-D structure of the virus and any unknown aspects of the data collection process and the minimization is with respect to these parameters. Performance of the approach is limited by&lt;br/&gt;the global optimization and quadrature tools and therefore these tools are the foci of this research. Key global&lt;br/&gt;optimization issues are exploiting the multi-scale structure of the data and parameters in the cost due to the presence of Fourier transforms and the tradeoff between accuracy and computational expense in the evaluation of the cost due to embedded quadratures. Key multidimensional quadrature issues are the unusual integrands and regions of integration, e.g., to integrate a function of three variables with icosahedral symmetry over the three Euler angles that define a 3-D rotation.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">98156</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98156</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n374" target="n375">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Increasing Parallel Program Performance with the LC Memory Consistency Model</data>
      <data key="e_abstract">This research takes a significant step towards enabling and evaluating the application of compiler&lt;br/&gt;optimizations for uniprocessor performance within explicitly parallel programs, with a flexible view&lt;br/&gt;of memory consistency. Two important unanswered questions are targeted: &lt;br/&gt;(1) What kind of performance can be gained with the Location Consistency (LC) memory model in comparison&lt;br/&gt;to the sequential consistency (SC)-derived models for shared memory parallel programs,&lt;br/&gt;amidst the new developments of compiler analysis and optimization for SC-derived models?&lt;br/&gt;(2) As a compromise between the two divergent approaches, can both the SC-derived models and the LC model be supported within the same program, by developing a programmer-controlled memory consistency strategy&lt;br/&gt;supported by compiler technology?&lt;br/&gt;The results&lt;br/&gt;will include:&lt;br/&gt;(1) specification and implemenation of a programming model that assumes an end-to-end view of the memory system based on the LC model, and a study of its programmability,&lt;br/&gt;(2) compilation analyses to uphold programmer-controlled memory consistency so programmers can choose between the memory models in different parts of the same application,&lt;br/&gt;(3) development and refinement of cache consistency protocols based on the LC model in a software caching context,&lt;br/&gt;(4) experimental evaluation of compiler optimization and program performance under&lt;br/&gt;different memory consistency models and cache protocols.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">105540</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105540</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n376" target="n377">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">On the Analysis, Optimization, and Efficient Scalarization of Monolithic-Level Array Programs</data>
      <data key="e_abstract">The research focuses on techniques, algorithms, and methodologies for the analysis and transformation of monolithic programs, which use operations on entire arrays. High-level monolithic analysis drives the mechanical optimization and efficient scalarization of such programs. Whereas optimization of monolithic code has previously focused primarily on expressions, this project investigates optimization over larger units of program granularity.&lt;br/&gt;&lt;br/&gt;The optimum elimination of unnecessary array partial results, with a particular focus on partial results assigned to a program variable may studied. A given array value assigned to a program variable may contain a permutation of the elements in some other array variable, and hence a compiler may be able to avoid materializing the given value. In contrast to minimizing materializations, there are situations where compiler introduced materializations, such as data rearrangement, or partial materializations, can significantly improve the efficiency of memory access at various levels of the memory hierarchy. Optimization techniques are studied both for avoiding materializations and for utilizing compiler introduced materializations.&lt;br/&gt;&lt;br/&gt;An intrinsic aspect of compiling monolithic code is scalarization. The use of monolithic analysis to obtain information that guides or drives scalarization, hopefully directly yielding optimized scalarized code.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">105536</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105536</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n376" target="n378">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">On the Analysis, Optimization, and Efficient Scalarization of Monolithic-Level Array Programs</data>
      <data key="e_abstract">The research focuses on techniques, algorithms, and methodologies for the analysis and transformation of monolithic programs, which use operations on entire arrays. High-level monolithic analysis drives the mechanical optimization and efficient scalarization of such programs. Whereas optimization of monolithic code has previously focused primarily on expressions, this project investigates optimization over larger units of program granularity.&lt;br/&gt;&lt;br/&gt;The optimum elimination of unnecessary array partial results, with a particular focus on partial results assigned to a program variable may studied. A given array value assigned to a program variable may contain a permutation of the elements in some other array variable, and hence a compiler may be able to avoid materializing the given value. In contrast to minimizing materializations, there are situations where compiler introduced materializations, such as data rearrangement, or partial materializations, can significantly improve the efficiency of memory access at various levels of the memory hierarchy. Optimization techniques are studied both for avoiding materializations and for utilizing compiler introduced materializations.&lt;br/&gt;&lt;br/&gt;An intrinsic aspect of compiling monolithic code is scalarization. The use of monolithic analysis to obtain information that guides or drives scalarization, hopefully directly yielding optimized scalarized code.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">105536</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105536</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n377" target="n378">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">On the Analysis, Optimization, and Efficient Scalarization of Monolithic-Level Array Programs</data>
      <data key="e_abstract">The research focuses on techniques, algorithms, and methodologies for the analysis and transformation of monolithic programs, which use operations on entire arrays. High-level monolithic analysis drives the mechanical optimization and efficient scalarization of such programs. Whereas optimization of monolithic code has previously focused primarily on expressions, this project investigates optimization over larger units of program granularity.&lt;br/&gt;&lt;br/&gt;The optimum elimination of unnecessary array partial results, with a particular focus on partial results assigned to a program variable may studied. A given array value assigned to a program variable may contain a permutation of the elements in some other array variable, and hence a compiler may be able to avoid materializing the given value. In contrast to minimizing materializations, there are situations where compiler introduced materializations, such as data rearrangement, or partial materializations, can significantly improve the efficiency of memory access at various levels of the memory hierarchy. Optimization techniques are studied both for avoiding materializations and for utilizing compiler introduced materializations.&lt;br/&gt;&lt;br/&gt;An intrinsic aspect of compiling monolithic code is scalarization. The use of monolithic analysis to obtain information that guides or drives scalarization, hopefully directly yielding optimized scalarized code.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">105536</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105536</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n380" target="n381">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">A Haptic-based Interface and Sculpting System for Virtual Environments</data>
      <data key="e_abstract">This is the first year funding of a three-year continuing award. Computer-integrated engineering design consists of a variety of complex and challenging processes, ranging from conceptual design, geometric modeling, evaluation, prototyping, manufacturing, assembly, to production. To ameliorate CAD/CAM processes, as well as to revolutionize human-computer interaction technology, the PIs will develop an interactive and tangible virtual environment to advance the current state-of-the-art through the novel integration of dynamic modeling and real-time haptic sculpting. To these ends, they will focus on critical fundamental issues relating to the rapid and accurate synchronization of multiple heterogeneous representations of geometric primitives and physical properties of virtual material in a software environment, including optimal algorithms and their time/space analysis, and numerical characteristics such as stability, robustness, and error bounds. They will investigate haptic interaction techniques towards the next-generation design technology in a systematic way through the development of novel haptic sculpting toolkits and the evaluation of both toolkit utilities and human factors. Finally, they will investigate the effective integration of haptic principles with mature geometric design techniques and develop an experimental virtual environment with haptic interface and real-time haptic sculpting capabilities. The PIs will disseminate the novel haptic technology and its software to the U.S. design industry and computer enterprises. To further broaden the accessibility of the new haptic technology in engineering, sciences, and medicine, the PIs will make extra efforts (through extensive collaborations) towards generalizing their prototype software system to other haptics-relevant applications such as surgical simulation and training, and haptic visualization of large scientific data sets, as time allows.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">97646</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">97646</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n383" target="n384">
      <data key="e_effectiveDate">2001-07-15</data>
      <data key="e_title">ITR/ASC: Collaborative Research - Linbox: A Generic Library for Seminumeric Black Box Linear Algebra</data>
      <data key="e_abstract">The LinBox group of twelve researchers in three countries (USA, France, Canada) proposes research in the design of efficient algorithms for linear algebra, in their implementation in a software library, and in how to interface the library to widely-used scientific computing software. Algorithms will be implemented, and new algorithms designed, for the black box representation of matrices---hence the name LinBox---over entry domains that are either symbolic, that is, exact, or floating point, that is, inexact. The library is generically programmed as C++ template classes with abstract underlying arithmetics; they can be compiled with a variety of fast libraries for the basic field, floating point, and polynomial operations. A server/client interface seamlessly attaches the library to the common general purpose symbolic systems Maple and Mathematica and to the numeric system MatLab. Parallel execution of the implemented algorithms is facilitated. &lt;br/&gt;&lt;br/&gt;Black box matrices are stored as functions (as linear operators in effect): the matrix is a procedure that takes an arbitrary vector as input and efficiently computes the matrix-times-vector product. Black box linear algebra generalizes sparsity. The LinBox library will contain algorithms for solving singular and non-singular systems of linear equations whose coefficient matrix is given in black box representation. Furthermore, it is proposed to develop fast methods for the rank and the minimal and characteristic polynomial of a black box matrix. Finally, LinBox will contain methods for linear Diophantine problems with black box matrices, such as computing an integral solution to a linear system with integer entries and computing the Smith normal form of an integer matrix.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112807</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">112807</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n383" target="n385">
      <data key="e_effectiveDate">2001-07-15</data>
      <data key="e_title">ITR/ASC: Collaborative Research - Linbox: A Generic Library for Seminumeric Black Box Linear Algebra</data>
      <data key="e_abstract">The LinBox group of twelve researchers in three countries (USA, France, Canada) proposes research in the design of efficient algorithms for linear algebra, in their implementation in a software library, and in how to interface the library to widely-used scientific computing software. Algorithms will be implemented, and new algorithms designed, for the black box representation of matrices---hence the name LinBox---over entry domains that are either symbolic, that is, exact, or floating point, that is, inexact. The library is generically programmed as C++ template classes with abstract underlying arithmetics; they can be compiled with a variety of fast libraries for the basic field, floating point, and polynomial operations. A server/client interface seamlessly attaches the library to the common general purpose symbolic systems Maple and Mathematica and to the numeric system MatLab. Parallel execution of the implemented algorithms is facilitated. &lt;br/&gt;&lt;br/&gt;Black box matrices are stored as functions (as linear operators in effect): the matrix is a procedure that takes an arbitrary vector as input and efficiently computes the matrix-times-vector product. Black box linear algebra generalizes sparsity. The LinBox library will contain algorithms for solving singular and non-singular systems of linear equations whose coefficient matrix is given in black box representation. Furthermore, it is proposed to develop fast methods for the rank and the minimal and characteristic polynomial of a black box matrix. Finally, LinBox will contain methods for linear Diophantine problems with black box matrices, such as computing an integral solution to a linear system with integer entries and computing the Smith normal form of an integer matrix.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112807</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">112807</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n384" target="n385">
      <data key="e_effectiveDate">2001-07-15</data>
      <data key="e_title">ITR/ASC: Collaborative Research - Linbox: A Generic Library for Seminumeric Black Box Linear Algebra</data>
      <data key="e_abstract">The LinBox group of twelve researchers in three countries (USA, France, Canada) proposes research in the design of efficient algorithms for linear algebra, in their implementation in a software library, and in how to interface the library to widely-used scientific computing software. Algorithms will be implemented, and new algorithms designed, for the black box representation of matrices---hence the name LinBox---over entry domains that are either symbolic, that is, exact, or floating point, that is, inexact. The library is generically programmed as C++ template classes with abstract underlying arithmetics; they can be compiled with a variety of fast libraries for the basic field, floating point, and polynomial operations. A server/client interface seamlessly attaches the library to the common general purpose symbolic systems Maple and Mathematica and to the numeric system MatLab. Parallel execution of the implemented algorithms is facilitated. &lt;br/&gt;&lt;br/&gt;Black box matrices are stored as functions (as linear operators in effect): the matrix is a procedure that takes an arbitrary vector as input and efficiently computes the matrix-times-vector product. Black box linear algebra generalizes sparsity. The LinBox library will contain algorithms for solving singular and non-singular systems of linear equations whose coefficient matrix is given in black box representation. Furthermore, it is proposed to develop fast methods for the rank and the minimal and characteristic polynomial of a black box matrix. Finally, LinBox will contain methods for linear Diophantine problems with black box matrices, such as computing an integral solution to a linear system with integer entries and computing the Smith normal form of an integer matrix.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112807</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">112807</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n386" target="n387">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Proxy-based Dissemination of Dynamic Web Data</data>
      <data key="e_abstract">Emerging web applications are increasingly likely to use dynamic web&lt;br/&gt;data. Traditionally, requests for dynamic data---both time-varying and&lt;br/&gt;dynamically-generated---have been handled directly by servers and&lt;br/&gt;intermediate proxies haven&apos;t been allowed to process requests for such&lt;br/&gt;objects (web proxies have been primarily employed to disseminate data&lt;br/&gt;that is mostly static). A pure server-based approach for managing&lt;br/&gt;dynamic data is likely to limit the scalability of emerging web&lt;br/&gt;applications and increase their vulnerability to server failures. To&lt;br/&gt;overcome this drawback, novel proxy-based techniques will be developed&lt;br/&gt;to manage and disseminate dynamic web data. The proposed&lt;br/&gt;research will address two key issues: (i) data dissemination, which&lt;br/&gt;addresses the issue of disseminating time-varying web data using&lt;br/&gt;proxy-based push and pull techniques, and (ii) computation&lt;br/&gt;dissemination, which addresses the issue of moving computations from&lt;br/&gt;servers to proxies so as to dynamically generate web objects at a&lt;br/&gt;proxy. Both techniques have the potential of radically changing the&lt;br/&gt;way web proxies are designed and used. These techniques will need to&lt;br/&gt;satisfy three key requirements: user-cognizance (i.e., awareness&lt;br/&gt;of user and application requirements), intelligence (i.e., the ability&lt;br/&gt;to dynamically choose the most efficient set of mechanisms to service&lt;br/&gt;each application), and adaptivity (i.e., the ability to react to&lt;br/&gt;changing load characteristics). The proposed techniques will be&lt;br/&gt;evaluated using an eclectic mix of simulation, analysis and prototype&lt;br/&gt;implementation.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">98060</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">98060</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n388" target="n389">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">NER: Biological Nanomachines: Assembly and Function of Protein-DNA Nanostructures at the Single-Molecule Level</data>
      <data key="e_abstract">Through evolution, biology has produced a remarkably diverse and efficient collection of proteins. These proteins assemble into a seemingly infinite variety of nanobiostructures, and they promote an extensive repertoire of chemical processes; hence, they comprise an interesting collection of biomolecular nanomachines. The proteins that are responsible for the maintenance and manipulation of DNA are one important subset of this collection. Their capacity to assemble with, and to alter the structure of, DNA is essential for all biological function. These proteins function in the packaging of DNA, the high-fidelity copying of genetic information, the reading of the genetic code and its conversion into RNA, the generation of genetic diversity, and the preservation of genetic and structural integrity of the genome. This project describes a new experimental approach to study the assembly and function of several of the nanomachines that function in these biological processes. The method involves the direct visualization by fluorescence microscopy, in real-time, of the assembly, disassembly, and movement of these nanobiostructures on single, optically-trapped DNA molecules using a novel, multi-port, laminar-flow, micro flow cell. This instrument will allows one to readily introduce an individual, optically-trapped DNA molecule sequentially into a series of reaction conditions, and to visualize the changes in structure/assembly of the molecules in real-time using multi-wavelength fluorescence microscopy. The successful development of this instrument will reveal important information about the structure and function of DNA-protein interactions that cannot be obtained using large ensembles of DNA molecules (where such information is often lost by averaging, or obscured by competing intermolecular interactions). This research will provide revolutionary new information about how proteins function to alter the structure of DNA. Furthermore, the experimental techniques can be applied to the study of many other nanoscale biostructures.</data>
      <data key="e_pgm">1248</data>
      <data key="e_label">103556</data>
      <data key="e_expirationDate">2002-06-30</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">103556</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n399" target="n400">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Automatic Performance Tuning of Numerical Kernels</data>
      <data key="e_abstract">Large scale simulations in computational engineering and science often spend a great deal of their time in a few computational methods kernels, such as dense or sparse matrix-vector products, relaxation on a structured or unstructured mesh, or the computation of forces between pairs of attracting or repelling particles. There has been a great deal of work in generating high performance libraries for these applications, including dense and sparse linear algebra, multigrid methods, and n-body techniques.&lt;br/&gt;&lt;br/&gt;One idea established in these application-level libraries is to organize the computations around a set of numerical kernels, with the assumption that these kernels will be highly optimized on each of the hardware platforms of interest. The best known example of this approach is the BLAS (the Basic Linear Algebra routines), which are used in building LAPACK, ScaLAPACK, and other libraries; the BLAS are implemented by hardware vendors and are highly tuned to the memory hierarchy of each machine.&lt;br/&gt;&lt;br/&gt;However, this approach is limited by the growing number of kernels, the large number of machines, the increasing depth of memory hierarchies and complexity of processors, and by the difficulty of performance tuning each kernel on each machine. The great majority of these kernels are susceptible to large speedups when machine-specific tuning is performed. However, the hand tuning takes weeks or months of a skilled engineer&apos;s time, and this work must be repeated for each micro-architecture, or operating system change. &lt;br/&gt;&lt;br/&gt;This research will work to automate the process of architecture-dependent tuning of numerical kernels, replacing the current hand-tuning process with a semi-automated search procedure. Prototypes of this approach exist for dense matrix-multiplication (Atlas and PHiPAC), FFTs (FFTW), and sparse matrix-vector multiplication (Sparsity). These results show that we can frequently do as well as or even better than hand-tuned vendor code on the kernels attempted. These systems use a hand-written &quot;search directed code generator (SDCG)&quot; to produce many different implementations of a single kernel, which are all run on each architecture, with the fastest one being selected. This approach will be extended to a much wider range of numerical kernels by combing compiler technology with algorithm-specific transformation rules to automate the production of these SDCGs.&lt;br/&gt;&lt;br/&gt;Ultimately, the technology is expected to be useful in conventional compilers, provided that appropriate abstract data types or annotations are used to side-step very difficult or &quot;impossible&quot; dependency-analysis needed to justify the desired code transformations. This work should also stimulate research into new high level numerical methods and architectures, both of which are limited by the lack of highly tuned kernels.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">90127</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">90127</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n407" target="n408">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Exploiting the Critical Path in the Design and Performance Analysis of Modern Processors</data>
      <data key="e_abstract">An undesired consequence of the growing parallelism of modern processors is that it is dramatically more difficult to separate the events that limit execution speed from the &gt;events whose latencies are tolerated. A method for focusing design effort is critical-path analysis. This research proposes to apply critical-path &lt;br/&gt;analysis at the micro-architectural level, with the goal of detecting and eliminating execution bottlenecks.&lt;br/&gt;This research will explore the potential of the critical path in four interrelated efforts: (1) Modeling the micro-architectural critical path. The main task is to define a model of the critical path, i.e., the set of events and dependences in a micro-execution that will be exposed in the dependence graph on which the critical path&lt;br/&gt;will be computed. (2) Efficient tracing of the critical path. This effort will develop an on-line algorithm that will use a last-arrival edge at each node to calculate the critical path in one pass in a simulator. (3) Hardware critical-path predictors. This research will explore the use of approximation methods in avoiding the analysis of the entire dependence graph. (4) Criticality-aware processor policies. This research will&lt;br/&gt;use hardware critical-path predictors to focus hardware policies on events likely to be on the critical path.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">105721</data>
      <data key="e_expirationDate">2006-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105721</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n409" target="n410">
      <data key="e_effectiveDate">2001-07-15</data>
      <data key="e_title">Digital Government: Integrating Metadata Development, XML, and DBMS Search and Query Techniques in a State of Wisconsin Land Information System</data>
      <data key="e_abstract">EIA-0091489&lt;br/&gt;Nancy Wiegand&lt;br/&gt;University of Wisconsin&lt;br/&gt;&lt;br/&gt;Digital Government: Integrating Metadata Development XML, and DBMS Search and Query Techniques in a State of Wisconsin Land Information System&lt;br/&gt;&lt;br/&gt;This project will involve several state, local and national agencies in developing a query able database of heterogeneous spatial and textual data. The data will continue to be held in various local and other databases, but will appear to be integrated though the system to be developed. Automated development of Metada and semantic conversions will be required. Three technical thrusts are i) to locate and search data sources, and to support user manipulation of the data. XML will be an important technical tool in the project in expressing Metadata.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91489</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91489</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n409" target="n411">
      <data key="e_effectiveDate">2001-07-15</data>
      <data key="e_title">Digital Government: Integrating Metadata Development, XML, and DBMS Search and Query Techniques in a State of Wisconsin Land Information System</data>
      <data key="e_abstract">EIA-0091489&lt;br/&gt;Nancy Wiegand&lt;br/&gt;University of Wisconsin&lt;br/&gt;&lt;br/&gt;Digital Government: Integrating Metadata Development XML, and DBMS Search and Query Techniques in a State of Wisconsin Land Information System&lt;br/&gt;&lt;br/&gt;This project will involve several state, local and national agencies in developing a query able database of heterogeneous spatial and textual data. The data will continue to be held in various local and other databases, but will appear to be integrated though the system to be developed. Automated development of Metada and semantic conversions will be required. Three technical thrusts are i) to locate and search data sources, and to support user manipulation of the data. XML will be an important technical tool in the project in expressing Metadata.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91489</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91489</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n410" target="n411">
      <data key="e_effectiveDate">2001-07-15</data>
      <data key="e_title">Digital Government: Integrating Metadata Development, XML, and DBMS Search and Query Techniques in a State of Wisconsin Land Information System</data>
      <data key="e_abstract">EIA-0091489&lt;br/&gt;Nancy Wiegand&lt;br/&gt;University of Wisconsin&lt;br/&gt;&lt;br/&gt;Digital Government: Integrating Metadata Development XML, and DBMS Search and Query Techniques in a State of Wisconsin Land Information System&lt;br/&gt;&lt;br/&gt;This project will involve several state, local and national agencies in developing a query able database of heterogeneous spatial and textual data. The data will continue to be held in various local and other databases, but will appear to be integrated though the system to be developed. Automated development of Metada and semantic conversions will be required. Three technical thrusts are i) to locate and search data sources, and to support user manipulation of the data. XML will be an important technical tool in the project in expressing Metadata.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91489</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91489</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n412" target="n413">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">ITR/SY(CISE): Putting Multi Stage Annotations to Work</data>
      <data key="e_abstract">Techniques such as program generation, partial evaluation, just-in-time compilation, and run-time code generation respond to the need for general purpose programs which do not pay unnecessary&lt;br/&gt;run-time overheads. The thesis of this project is that a uniform, principled, high-level, and practical view of these diverse techniques is possible through multi-stage programming, a novel paradigm for the development of maintainable, high-performance software. The key idea in multi-stage programming is the use of simple, high-level annotations to allow the programmer to break down the cost of a computation into distinct stages.&lt;br/&gt;&lt;br/&gt;The goal of this proposal is to demonstrate that the theoretical machinery that has been developed for multi-stage programming can be put to work. This project will involve the development of compilers&lt;br/&gt;of multi-stage programming languages, addressing both practical and theoretical problems that arise in the development of such systems, and using these compilers in interesting applications ranging from dynamic programming algorithms and rewriting systems to implementations of domain specific programming languages.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113569</data>
      <data key="e_expirationDate">2003-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113569</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n415" target="n416">
      <data key="e_effectiveDate">2001-07-15</data>
      <data key="e_title">Massive Data Streams: Algorithms and Complexity</data>
      <data key="e_abstract">Title: &quot;Massive Data Streams: Algorithms and Complexity&quot;&lt;br/&gt;&lt;br/&gt;Investigators: Joan Feigenbaum and Sampath Kannan&lt;br/&gt;&lt;br/&gt;Abstract:&lt;br/&gt; Massive data sets are increasingly important in many applications,&lt;br/&gt;including observational sciences, product marketing, and monitoring and&lt;br/&gt;operations of large systems. In network operations, raw data typically arrive&lt;br/&gt;in streams, and decisions must be made by algorithms that make one pass&lt;br/&gt;over each stream, throw much of the raw data away, and produce ``synopses&apos;&apos;&lt;br/&gt;or ``sketches&apos;&apos; for further processing. Moreover, network-generated massive&lt;br/&gt;data sets are often distributed: Several different, physically separated&lt;br/&gt;network elements may receive or generate data streams that, together, comprise&lt;br/&gt;one logical data set. The enormous scale, distributed nature, and one-pass &lt;br/&gt;processing requirement on the data sets of interest must be addressed with &lt;br/&gt;new algorithmic techniques.&lt;br/&gt; Two programming paradigms for massive data sets are &quot;sampling&quot; and&lt;br/&gt;&quot;streaming.&quot; Rather than take time even to read a massive data&lt;br/&gt;set, a sampling algorithm extracts a small random sample and computes&lt;br/&gt;on it. By contrast, a streaming algorithm takes time to read all the input, &lt;br/&gt;but little more time and little total space. Input to a streaming algorithm &lt;br/&gt;is a sequence of items; the streaming algorithm is given the items in order, &lt;br/&gt;lacks space to record more than a small amount of the input, and is required&lt;br/&gt;to perform its per-item processing quickly in order to keep up with&lt;br/&gt;the unbuffered input. The investigators continue the study of &lt;br/&gt;fundamental algorithms for massive data streams. Specific problems of&lt;br/&gt;interest include but are not limited to the complexity of proving properties &lt;br/&gt;of data streams, the construction of one-pass testers of properties of &lt;br/&gt;massive graphs, and the streaming space complexity of clustering.</data>
      <data key="e_pgm">2860</data>
      <data key="e_label">105337</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105337</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n422" target="n423">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">NER: Magnetic &quot;Smart-wires&quot; for Spintronic Nanostructured Devices</data>
      <data key="e_abstract">This proposal was received in response to NSE, NSF-0019.&lt;br/&gt;&lt;br/&gt;The project involves blending of molecular biology, polymer chemistry, and materials physics, in an attempt to create a new class of spin-tronic functional nanostructures. A new architecture is proposed to create &lt;br/&gt;spin-tronic magnetic devices, using a biologically modified, metal coated, polymer nanotube process. The advantages of spin-tronics include higher speed, greater storage density, low power dissipation, and non-volatility. The proposed architecture uses a form of molecular self-assembly based on biological models. The magnetic nano-wires are formed from a tubular polymer backbone, that can be coated with various metallic layers. By functionalizing the ends of the tubes with special molecules, the nano-tubes become what we call &quot;Smart Wires&quot;. Smart-Wires are at the center of our proposed new architecture for nanoscale device fabrication, since the instructions for &quot;wiring&quot; the circuits are built into the molecular nanostructures &lt;br/&gt;themselves, rather than having to be imposed afterwards using a patterning process. The &quot;Smart-Wires&quot; use molecular recognition, modeled after the biological antibody-antigen bonding reaction, to attach the wire ends to the appropriate substrate structures. Under this project metal coated nano-tube will be fabricated and the work will be extended to include the &quot;Smart Wire&quot; functionalization and bonding, and also metallic &lt;br/&gt;multilayer coatings. A full range of sensitive magnetic and electronic probes will be used to measure the nano-tube properties, using methods perfected in a previous NSF project to study epitaxial spin-valve structures. The research team plans to develop a prototype of a magnetic nano-tube assembly with a switchable magnetic state.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">103430</data>
      <data key="e_expirationDate">2003-06-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103430</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n424" target="n425">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Children&apos;s Programming Odyssey</data>
      <data key="e_abstract">IIS-0123712&lt;br/&gt;Allen L. Ambler and Jennifer L. Leopold, University of Kansas&lt;br/&gt;$26,250 - 12 mos&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Children&apos;s Programming Odyssey&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This is a standard award. The 2001 IEEE Symposia on Human-Centric Computing Languages and Environments (HCC&apos;01), to be held in Stresa, Italy on September 5-7, will include a special event - the Children&apos;s Programming Odyssey - featuring some of the world&apos;s premier researchers in children&apos;s programming languages, including Alan Kay (Squeak), Ken Kahn (Toontalk), Allan Cypher (Stagecast), and Alex Repenning (AgentSheets). This is funding to support participation in the Odyssey of up to ten graduate students from the United States, who will each give brief presentations of their thesis work followed by a positive critique from the panel of experts. Student participants will be invited based on materials submitted to a selection committee, to consist of the PI (who is also organizer of the Odyssey), Margaret Burnett from Oregon State University, and another member yet to be named. The primary criterion to be used by the selection committee is relevance of the student&apos;s dissertation research to children&apos;s programming, computer-based construction environments for learning, or end-user programming. In addition, the student must have done sufficient work so that s/he can talk from some experience. The selected students will be expected to submit a 5-page written paper describing their current work, to attend HCC&apos;01, and to make a 10-minute presentation in the Odyssey. The students will benefit immensely from this unique opportunity to expose their ideas to and to interact with the leading researchers in this area; this nurturing effort is an inexpensive yet effective means of encouraging young and upcoming scientists.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">123712</data>
      <data key="e_expirationDate">2002-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">123712</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n427" target="n428">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Mathematical Analysis of the Compositional Structure of Images</data>
      <data key="e_abstract">The investigator and his colleagues study mathematical, statistical, and computational questions motivated by the problem in computer vision of defining and computing &quot;structural scene description,&quot; descriptions of the objects in a scene and their relations to each other. The first goal is to obtain a theoretical understanding of the problem of inferring objects and their compositional structure, both in standard optical images and in laser range imagery and motion images. The second goal is to test this understanding by developing effective algorithmsfor the statistical inference of this structure, algorithms that work with real data at reasonable speed on current computers. The general approach is to formulate mathematical representations of patterns, typically compositional with a hierarchy of structures, to encode the variability of these structures in stochastic models, and to use the models to infer information about the image. Standard stochastic models are rarely adequate, prompting the development of new classes of probability measures or completely new directions for traditional models. Additionally, in the context of this application the team aims to develop mathematical, statistical, and computational ideas that are of broader use. For example, compositional issues in vision are similar to ones in the grammars of language. One aspect of the project studies this connection.&lt;br/&gt;&lt;br/&gt;The investigator and his five colleagues continue their mathematical, statistical, and computational investigations of a range of problems motivated by image processing and computer vision. The underlying question is simple enough: Here&apos;s an image, what is it an image of? Despite its simplicity, this is a hard question to answer. The approach taken here is to decompose the image into different components in some hierarchical structure and to use statistical analysis to infer information about the image from the relationships of the components within the structure. There are similarities with the grammatical structure of language, which the project explores. Recognition of objects in an image is a fundamental problem for both computer systems and biological systems. Advances are important for engineers developing computer vision applications, computer scientists seeking efficient algorithms in problems related to intelligent behavior of machines, and cognitive scientists studying human vision and language skills. Results of the project are published on CD-ROM, allowing demonstration of the dynamic behavior of algorithms in ways impossible through traditional publication modes and offering new ways to exploit multi-media communications for both education and research purposes.</data>
      <data key="e_pgm">1265</data>
      <data key="e_label">74276</data>
      <data key="e_expirationDate">2006-06-30</data>
      <data key="e_div">0304</data>
      <data key="e_awardID">74276</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n427" target="n429">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Mathematical Analysis of the Compositional Structure of Images</data>
      <data key="e_abstract">The investigator and his colleagues study mathematical, statistical, and computational questions motivated by the problem in computer vision of defining and computing &quot;structural scene description,&quot; descriptions of the objects in a scene and their relations to each other. The first goal is to obtain a theoretical understanding of the problem of inferring objects and their compositional structure, both in standard optical images and in laser range imagery and motion images. The second goal is to test this understanding by developing effective algorithmsfor the statistical inference of this structure, algorithms that work with real data at reasonable speed on current computers. The general approach is to formulate mathematical representations of patterns, typically compositional with a hierarchy of structures, to encode the variability of these structures in stochastic models, and to use the models to infer information about the image. Standard stochastic models are rarely adequate, prompting the development of new classes of probability measures or completely new directions for traditional models. Additionally, in the context of this application the team aims to develop mathematical, statistical, and computational ideas that are of broader use. For example, compositional issues in vision are similar to ones in the grammars of language. One aspect of the project studies this connection.&lt;br/&gt;&lt;br/&gt;The investigator and his five colleagues continue their mathematical, statistical, and computational investigations of a range of problems motivated by image processing and computer vision. The underlying question is simple enough: Here&apos;s an image, what is it an image of? Despite its simplicity, this is a hard question to answer. The approach taken here is to decompose the image into different components in some hierarchical structure and to use statistical analysis to infer information about the image from the relationships of the components within the structure. There are similarities with the grammatical structure of language, which the project explores. Recognition of objects in an image is a fundamental problem for both computer systems and biological systems. Advances are important for engineers developing computer vision applications, computer scientists seeking efficient algorithms in problems related to intelligent behavior of machines, and cognitive scientists studying human vision and language skills. Results of the project are published on CD-ROM, allowing demonstration of the dynamic behavior of algorithms in ways impossible through traditional publication modes and offering new ways to exploit multi-media communications for both education and research purposes.</data>
      <data key="e_pgm">1265</data>
      <data key="e_label">74276</data>
      <data key="e_expirationDate">2006-06-30</data>
      <data key="e_div">0304</data>
      <data key="e_awardID">74276</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n427" target="n430">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Mathematical Analysis of the Compositional Structure of Images</data>
      <data key="e_abstract">The investigator and his colleagues study mathematical, statistical, and computational questions motivated by the problem in computer vision of defining and computing &quot;structural scene description,&quot; descriptions of the objects in a scene and their relations to each other. The first goal is to obtain a theoretical understanding of the problem of inferring objects and their compositional structure, both in standard optical images and in laser range imagery and motion images. The second goal is to test this understanding by developing effective algorithmsfor the statistical inference of this structure, algorithms that work with real data at reasonable speed on current computers. The general approach is to formulate mathematical representations of patterns, typically compositional with a hierarchy of structures, to encode the variability of these structures in stochastic models, and to use the models to infer information about the image. Standard stochastic models are rarely adequate, prompting the development of new classes of probability measures or completely new directions for traditional models. Additionally, in the context of this application the team aims to develop mathematical, statistical, and computational ideas that are of broader use. For example, compositional issues in vision are similar to ones in the grammars of language. One aspect of the project studies this connection.&lt;br/&gt;&lt;br/&gt;The investigator and his five colleagues continue their mathematical, statistical, and computational investigations of a range of problems motivated by image processing and computer vision. The underlying question is simple enough: Here&apos;s an image, what is it an image of? Despite its simplicity, this is a hard question to answer. The approach taken here is to decompose the image into different components in some hierarchical structure and to use statistical analysis to infer information about the image from the relationships of the components within the structure. There are similarities with the grammatical structure of language, which the project explores. Recognition of objects in an image is a fundamental problem for both computer systems and biological systems. Advances are important for engineers developing computer vision applications, computer scientists seeking efficient algorithms in problems related to intelligent behavior of machines, and cognitive scientists studying human vision and language skills. Results of the project are published on CD-ROM, allowing demonstration of the dynamic behavior of algorithms in ways impossible through traditional publication modes and offering new ways to exploit multi-media communications for both education and research purposes.</data>
      <data key="e_pgm">1265</data>
      <data key="e_label">74276</data>
      <data key="e_expirationDate">2006-06-30</data>
      <data key="e_div">0304</data>
      <data key="e_awardID">74276</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n427" target="n431">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Mathematical Analysis of the Compositional Structure of Images</data>
      <data key="e_abstract">The investigator and his colleagues study mathematical, statistical, and computational questions motivated by the problem in computer vision of defining and computing &quot;structural scene description,&quot; descriptions of the objects in a scene and their relations to each other. The first goal is to obtain a theoretical understanding of the problem of inferring objects and their compositional structure, both in standard optical images and in laser range imagery and motion images. The second goal is to test this understanding by developing effective algorithmsfor the statistical inference of this structure, algorithms that work with real data at reasonable speed on current computers. The general approach is to formulate mathematical representations of patterns, typically compositional with a hierarchy of structures, to encode the variability of these structures in stochastic models, and to use the models to infer information about the image. Standard stochastic models are rarely adequate, prompting the development of new classes of probability measures or completely new directions for traditional models. Additionally, in the context of this application the team aims to develop mathematical, statistical, and computational ideas that are of broader use. For example, compositional issues in vision are similar to ones in the grammars of language. One aspect of the project studies this connection.&lt;br/&gt;&lt;br/&gt;The investigator and his five colleagues continue their mathematical, statistical, and computational investigations of a range of problems motivated by image processing and computer vision. The underlying question is simple enough: Here&apos;s an image, what is it an image of? Despite its simplicity, this is a hard question to answer. The approach taken here is to decompose the image into different components in some hierarchical structure and to use statistical analysis to infer information about the image from the relationships of the components within the structure. There are similarities with the grammatical structure of language, which the project explores. Recognition of objects in an image is a fundamental problem for both computer systems and biological systems. Advances are important for engineers developing computer vision applications, computer scientists seeking efficient algorithms in problems related to intelligent behavior of machines, and cognitive scientists studying human vision and language skills. Results of the project are published on CD-ROM, allowing demonstration of the dynamic behavior of algorithms in ways impossible through traditional publication modes and offering new ways to exploit multi-media communications for both education and research purposes.</data>
      <data key="e_pgm">1265</data>
      <data key="e_label">74276</data>
      <data key="e_expirationDate">2006-06-30</data>
      <data key="e_div">0304</data>
      <data key="e_awardID">74276</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n428" target="n429">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Mathematical Analysis of the Compositional Structure of Images</data>
      <data key="e_abstract">The investigator and his colleagues study mathematical, statistical, and computational questions motivated by the problem in computer vision of defining and computing &quot;structural scene description,&quot; descriptions of the objects in a scene and their relations to each other. The first goal is to obtain a theoretical understanding of the problem of inferring objects and their compositional structure, both in standard optical images and in laser range imagery and motion images. The second goal is to test this understanding by developing effective algorithmsfor the statistical inference of this structure, algorithms that work with real data at reasonable speed on current computers. The general approach is to formulate mathematical representations of patterns, typically compositional with a hierarchy of structures, to encode the variability of these structures in stochastic models, and to use the models to infer information about the image. Standard stochastic models are rarely adequate, prompting the development of new classes of probability measures or completely new directions for traditional models. Additionally, in the context of this application the team aims to develop mathematical, statistical, and computational ideas that are of broader use. For example, compositional issues in vision are similar to ones in the grammars of language. One aspect of the project studies this connection.&lt;br/&gt;&lt;br/&gt;The investigator and his five colleagues continue their mathematical, statistical, and computational investigations of a range of problems motivated by image processing and computer vision. The underlying question is simple enough: Here&apos;s an image, what is it an image of? Despite its simplicity, this is a hard question to answer. The approach taken here is to decompose the image into different components in some hierarchical structure and to use statistical analysis to infer information about the image from the relationships of the components within the structure. There are similarities with the grammatical structure of language, which the project explores. Recognition of objects in an image is a fundamental problem for both computer systems and biological systems. Advances are important for engineers developing computer vision applications, computer scientists seeking efficient algorithms in problems related to intelligent behavior of machines, and cognitive scientists studying human vision and language skills. Results of the project are published on CD-ROM, allowing demonstration of the dynamic behavior of algorithms in ways impossible through traditional publication modes and offering new ways to exploit multi-media communications for both education and research purposes.</data>
      <data key="e_pgm">1265</data>
      <data key="e_label">74276</data>
      <data key="e_expirationDate">2006-06-30</data>
      <data key="e_div">0304</data>
      <data key="e_awardID">74276</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n428" target="n430">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Mathematical Analysis of the Compositional Structure of Images</data>
      <data key="e_abstract">The investigator and his colleagues study mathematical, statistical, and computational questions motivated by the problem in computer vision of defining and computing &quot;structural scene description,&quot; descriptions of the objects in a scene and their relations to each other. The first goal is to obtain a theoretical understanding of the problem of inferring objects and their compositional structure, both in standard optical images and in laser range imagery and motion images. The second goal is to test this understanding by developing effective algorithmsfor the statistical inference of this structure, algorithms that work with real data at reasonable speed on current computers. The general approach is to formulate mathematical representations of patterns, typically compositional with a hierarchy of structures, to encode the variability of these structures in stochastic models, and to use the models to infer information about the image. Standard stochastic models are rarely adequate, prompting the development of new classes of probability measures or completely new directions for traditional models. Additionally, in the context of this application the team aims to develop mathematical, statistical, and computational ideas that are of broader use. For example, compositional issues in vision are similar to ones in the grammars of language. One aspect of the project studies this connection.&lt;br/&gt;&lt;br/&gt;The investigator and his five colleagues continue their mathematical, statistical, and computational investigations of a range of problems motivated by image processing and computer vision. The underlying question is simple enough: Here&apos;s an image, what is it an image of? Despite its simplicity, this is a hard question to answer. The approach taken here is to decompose the image into different components in some hierarchical structure and to use statistical analysis to infer information about the image from the relationships of the components within the structure. There are similarities with the grammatical structure of language, which the project explores. Recognition of objects in an image is a fundamental problem for both computer systems and biological systems. Advances are important for engineers developing computer vision applications, computer scientists seeking efficient algorithms in problems related to intelligent behavior of machines, and cognitive scientists studying human vision and language skills. Results of the project are published on CD-ROM, allowing demonstration of the dynamic behavior of algorithms in ways impossible through traditional publication modes and offering new ways to exploit multi-media communications for both education and research purposes.</data>
      <data key="e_pgm">1265</data>
      <data key="e_label">74276</data>
      <data key="e_expirationDate">2006-06-30</data>
      <data key="e_div">0304</data>
      <data key="e_awardID">74276</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n428" target="n431">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Mathematical Analysis of the Compositional Structure of Images</data>
      <data key="e_abstract">The investigator and his colleagues study mathematical, statistical, and computational questions motivated by the problem in computer vision of defining and computing &quot;structural scene description,&quot; descriptions of the objects in a scene and their relations to each other. The first goal is to obtain a theoretical understanding of the problem of inferring objects and their compositional structure, both in standard optical images and in laser range imagery and motion images. The second goal is to test this understanding by developing effective algorithmsfor the statistical inference of this structure, algorithms that work with real data at reasonable speed on current computers. The general approach is to formulate mathematical representations of patterns, typically compositional with a hierarchy of structures, to encode the variability of these structures in stochastic models, and to use the models to infer information about the image. Standard stochastic models are rarely adequate, prompting the development of new classes of probability measures or completely new directions for traditional models. Additionally, in the context of this application the team aims to develop mathematical, statistical, and computational ideas that are of broader use. For example, compositional issues in vision are similar to ones in the grammars of language. One aspect of the project studies this connection.&lt;br/&gt;&lt;br/&gt;The investigator and his five colleagues continue their mathematical, statistical, and computational investigations of a range of problems motivated by image processing and computer vision. The underlying question is simple enough: Here&apos;s an image, what is it an image of? Despite its simplicity, this is a hard question to answer. The approach taken here is to decompose the image into different components in some hierarchical structure and to use statistical analysis to infer information about the image from the relationships of the components within the structure. There are similarities with the grammatical structure of language, which the project explores. Recognition of objects in an image is a fundamental problem for both computer systems and biological systems. Advances are important for engineers developing computer vision applications, computer scientists seeking efficient algorithms in problems related to intelligent behavior of machines, and cognitive scientists studying human vision and language skills. Results of the project are published on CD-ROM, allowing demonstration of the dynamic behavior of algorithms in ways impossible through traditional publication modes and offering new ways to exploit multi-media communications for both education and research purposes.</data>
      <data key="e_pgm">1265</data>
      <data key="e_label">74276</data>
      <data key="e_expirationDate">2006-06-30</data>
      <data key="e_div">0304</data>
      <data key="e_awardID">74276</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n429" target="n430">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Mathematical Analysis of the Compositional Structure of Images</data>
      <data key="e_abstract">The investigator and his colleagues study mathematical, statistical, and computational questions motivated by the problem in computer vision of defining and computing &quot;structural scene description,&quot; descriptions of the objects in a scene and their relations to each other. The first goal is to obtain a theoretical understanding of the problem of inferring objects and their compositional structure, both in standard optical images and in laser range imagery and motion images. The second goal is to test this understanding by developing effective algorithmsfor the statistical inference of this structure, algorithms that work with real data at reasonable speed on current computers. The general approach is to formulate mathematical representations of patterns, typically compositional with a hierarchy of structures, to encode the variability of these structures in stochastic models, and to use the models to infer information about the image. Standard stochastic models are rarely adequate, prompting the development of new classes of probability measures or completely new directions for traditional models. Additionally, in the context of this application the team aims to develop mathematical, statistical, and computational ideas that are of broader use. For example, compositional issues in vision are similar to ones in the grammars of language. One aspect of the project studies this connection.&lt;br/&gt;&lt;br/&gt;The investigator and his five colleagues continue their mathematical, statistical, and computational investigations of a range of problems motivated by image processing and computer vision. The underlying question is simple enough: Here&apos;s an image, what is it an image of? Despite its simplicity, this is a hard question to answer. The approach taken here is to decompose the image into different components in some hierarchical structure and to use statistical analysis to infer information about the image from the relationships of the components within the structure. There are similarities with the grammatical structure of language, which the project explores. Recognition of objects in an image is a fundamental problem for both computer systems and biological systems. Advances are important for engineers developing computer vision applications, computer scientists seeking efficient algorithms in problems related to intelligent behavior of machines, and cognitive scientists studying human vision and language skills. Results of the project are published on CD-ROM, allowing demonstration of the dynamic behavior of algorithms in ways impossible through traditional publication modes and offering new ways to exploit multi-media communications for both education and research purposes.</data>
      <data key="e_pgm">1265</data>
      <data key="e_label">74276</data>
      <data key="e_expirationDate">2006-06-30</data>
      <data key="e_div">0304</data>
      <data key="e_awardID">74276</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n429" target="n431">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Mathematical Analysis of the Compositional Structure of Images</data>
      <data key="e_abstract">The investigator and his colleagues study mathematical, statistical, and computational questions motivated by the problem in computer vision of defining and computing &quot;structural scene description,&quot; descriptions of the objects in a scene and their relations to each other. The first goal is to obtain a theoretical understanding of the problem of inferring objects and their compositional structure, both in standard optical images and in laser range imagery and motion images. The second goal is to test this understanding by developing effective algorithmsfor the statistical inference of this structure, algorithms that work with real data at reasonable speed on current computers. The general approach is to formulate mathematical representations of patterns, typically compositional with a hierarchy of structures, to encode the variability of these structures in stochastic models, and to use the models to infer information about the image. Standard stochastic models are rarely adequate, prompting the development of new classes of probability measures or completely new directions for traditional models. Additionally, in the context of this application the team aims to develop mathematical, statistical, and computational ideas that are of broader use. For example, compositional issues in vision are similar to ones in the grammars of language. One aspect of the project studies this connection.&lt;br/&gt;&lt;br/&gt;The investigator and his five colleagues continue their mathematical, statistical, and computational investigations of a range of problems motivated by image processing and computer vision. The underlying question is simple enough: Here&apos;s an image, what is it an image of? Despite its simplicity, this is a hard question to answer. The approach taken here is to decompose the image into different components in some hierarchical structure and to use statistical analysis to infer information about the image from the relationships of the components within the structure. There are similarities with the grammatical structure of language, which the project explores. Recognition of objects in an image is a fundamental problem for both computer systems and biological systems. Advances are important for engineers developing computer vision applications, computer scientists seeking efficient algorithms in problems related to intelligent behavior of machines, and cognitive scientists studying human vision and language skills. Results of the project are published on CD-ROM, allowing demonstration of the dynamic behavior of algorithms in ways impossible through traditional publication modes and offering new ways to exploit multi-media communications for both education and research purposes.</data>
      <data key="e_pgm">1265</data>
      <data key="e_label">74276</data>
      <data key="e_expirationDate">2006-06-30</data>
      <data key="e_div">0304</data>
      <data key="e_awardID">74276</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n430" target="n431">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Mathematical Analysis of the Compositional Structure of Images</data>
      <data key="e_abstract">The investigator and his colleagues study mathematical, statistical, and computational questions motivated by the problem in computer vision of defining and computing &quot;structural scene description,&quot; descriptions of the objects in a scene and their relations to each other. The first goal is to obtain a theoretical understanding of the problem of inferring objects and their compositional structure, both in standard optical images and in laser range imagery and motion images. The second goal is to test this understanding by developing effective algorithmsfor the statistical inference of this structure, algorithms that work with real data at reasonable speed on current computers. The general approach is to formulate mathematical representations of patterns, typically compositional with a hierarchy of structures, to encode the variability of these structures in stochastic models, and to use the models to infer information about the image. Standard stochastic models are rarely adequate, prompting the development of new classes of probability measures or completely new directions for traditional models. Additionally, in the context of this application the team aims to develop mathematical, statistical, and computational ideas that are of broader use. For example, compositional issues in vision are similar to ones in the grammars of language. One aspect of the project studies this connection.&lt;br/&gt;&lt;br/&gt;The investigator and his five colleagues continue their mathematical, statistical, and computational investigations of a range of problems motivated by image processing and computer vision. The underlying question is simple enough: Here&apos;s an image, what is it an image of? Despite its simplicity, this is a hard question to answer. The approach taken here is to decompose the image into different components in some hierarchical structure and to use statistical analysis to infer information about the image from the relationships of the components within the structure. There are similarities with the grammatical structure of language, which the project explores. Recognition of objects in an image is a fundamental problem for both computer systems and biological systems. Advances are important for engineers developing computer vision applications, computer scientists seeking efficient algorithms in problems related to intelligent behavior of machines, and cognitive scientists studying human vision and language skills. Results of the project are published on CD-ROM, allowing demonstration of the dynamic behavior of algorithms in ways impossible through traditional publication modes and offering new ways to exploit multi-media communications for both education and research purposes.</data>
      <data key="e_pgm">1265</data>
      <data key="e_label">74276</data>
      <data key="e_expirationDate">2006-06-30</data>
      <data key="e_div">0304</data>
      <data key="e_awardID">74276</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n433" target="n434">
      <data key="e_effectiveDate">2001-07-15</data>
      <data key="e_title">Effective Techniques and Tools for Resource Management in Clustered Web Servers</data>
      <data key="e_abstract">The explosive growth of the World Wide Web, the variety of information hosted by a given site, the rapid fluctuations in user demand, and the complexity of the preferred architectural trend are among the reasons that make increasingly challenging for system administrators to effectively mange today&apos;s complex web server systems. This proposal addresses this problem by seeking to develop smart algorithms that help performing difficult tasks such as web server cluster configuration, capacity planning, fault management and load balancing, by continuously monitoring and adapting to changes in the workload and system. Workload mointoring will provide input to analytic models whose solution will in turn be used by both off-line and on-line algorithms that can propose improved system configurations. To this end, the proposed research objectives will provide: (a) methods to derive detailed statistical workload characterizations of web servers, (b) new and efficient solutions for analytic models of systems that serve tasks drawn from heavy tail probability distributions, (c) a software tool particularly targeted to load balancing in clustered web servers, to be used by non expert modelers, including system administrators, and (d) workload-aware and system-aware algorithms that significantly improve performance and ease of web system management.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">98278</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">98278</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n439" target="n440">
      <data key="e_effectiveDate">2001-07-15</data>
      <data key="e_title">ITR/ACS: Collaborative Research LinBox: A Generic Library for Seminumeric Black Box Linear Algebra</data>
      <data key="e_abstract">The LinBox group of twelve researchers in three countries (USA, France, Canada) proposes research in the design of efficient algorithms for linear algebra, in their implementation in a software library, and in how to interface the library to widely-used scientific computing software. Algorithms will be implemented, and new algorithms designed, for the black box representation of matrices---hence the name LinBox---over entry domains that are either symbolic, that is, exact, or floating point, that is, inexact. The library is generically programmed as C++ template classes with abstract underlying arithmetics; they can be compiled with a variety of fast libraries for the basic field, floating point, and polynomial operations. A server/client interface seamlessly attaches the library to the common general purpose symbolic systems Maple and Mathematica and to the numeric system MatLab. Parallel execution of the implemented algorithms is facilitated. &lt;br/&gt;&lt;br/&gt;Black box matrices are stored as functions (as linear operators in effect): the matrix is a procedure that takes an arbitrary vector as input and efficiently computes the matrix-times-vector product. Black box linear algebra generalizes sparsity. The LinBox library will contain algorithms for solving singular and non-singular systems of linear equations whose coefficient matrix is given in black box representation. Furthermore, it is proposed to develop fast methods for the rank and the minimal and characteristic polynomial of a black box matrix. Finally, LinBox will contain methods for linear Diophantine problems with black box matrices, such as computing an integral solution to a linear system with integer entries and computing the Smith normal form of an integer matrix.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113121</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113121</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n442" target="n443">
      <data key="e_effectiveDate">2001-07-15</data>
      <data key="e_title">High Performance Parallel I/O</data>
      <data key="e_abstract">This project addresses the challenges of successfully deploying large-scale parallel I/O servers to meet the demands of modern data-intensive applications multimedia retrieval, We and database servers, visualization and graphics, and spatial and temporal databases. The goals are to develop scheduling and resource management algorithms for parallel I/O systems, and increase our understanding of the complex underlying resource tradeoffs. These include basic scheduling issues dealing with parallel I/O, including those related to prefetching and caching, on -line scheduling, fair servicing of multiple users and deadline-constrained real-time parallel I/O. Secondly the algorithms designed in this research will be directly applied to areas like multimedia systems, including variable-bit rate (VBR) video retrieval, and Web, database and application servers dealing with large numbers of concurrent, interacting I/Os.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">105565</data>
      <data key="e_expirationDate">2007-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105565</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n449" target="n450">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">NER: Investigation of Electronic Transport in Carbon Nanotubes Using an Ultrafast Photoconduction Technique</data>
      <data key="e_abstract">This proposal was received in response to NSE, NSF-0019. Carbon nanotubes are of interest as electronic conductors because of their unique properties, notably their behavior as 1-D conductors and their ballistic nature over several microns at room temperature. A general picture of electronic conduction in nanotubes is being developed by researchers in the field. Although the transient response of current flow is expected to be quite rapid (picosecond range), present experimental data for conduction properties of nanotubes is primarily obtained at low frequencies or in static measurements. In order to shed light on the fundamental current transport processes in nanotubes, an experimental and theoretical effort is proposed to develop characterization techniques which would allow the measurement of the transient current responses through nanotube conductors. This work will address two issues: i) the integration of nanotube conductors with appropriate high speed photoconductor materials and ii) the development of characterization techniques for nanoscale conductors based on pump/probe measurements using high speed pulsed laser systems. It is expected that the proposed work will shed light on the conduction processes of these interesting electronic materials and provide important capabilities for integration of these materials with other electronic materials, including semiconductor device structures.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">103227</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103227</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n449" target="n451">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">NER: Investigation of Electronic Transport in Carbon Nanotubes Using an Ultrafast Photoconduction Technique</data>
      <data key="e_abstract">This proposal was received in response to NSE, NSF-0019. Carbon nanotubes are of interest as electronic conductors because of their unique properties, notably their behavior as 1-D conductors and their ballistic nature over several microns at room temperature. A general picture of electronic conduction in nanotubes is being developed by researchers in the field. Although the transient response of current flow is expected to be quite rapid (picosecond range), present experimental data for conduction properties of nanotubes is primarily obtained at low frequencies or in static measurements. In order to shed light on the fundamental current transport processes in nanotubes, an experimental and theoretical effort is proposed to develop characterization techniques which would allow the measurement of the transient current responses through nanotube conductors. This work will address two issues: i) the integration of nanotube conductors with appropriate high speed photoconductor materials and ii) the development of characterization techniques for nanoscale conductors based on pump/probe measurements using high speed pulsed laser systems. It is expected that the proposed work will shed light on the conduction processes of these interesting electronic materials and provide important capabilities for integration of these materials with other electronic materials, including semiconductor device structures.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">103227</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103227</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n449" target="n452">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">NER: Investigation of Electronic Transport in Carbon Nanotubes Using an Ultrafast Photoconduction Technique</data>
      <data key="e_abstract">This proposal was received in response to NSE, NSF-0019. Carbon nanotubes are of interest as electronic conductors because of their unique properties, notably their behavior as 1-D conductors and their ballistic nature over several microns at room temperature. A general picture of electronic conduction in nanotubes is being developed by researchers in the field. Although the transient response of current flow is expected to be quite rapid (picosecond range), present experimental data for conduction properties of nanotubes is primarily obtained at low frequencies or in static measurements. In order to shed light on the fundamental current transport processes in nanotubes, an experimental and theoretical effort is proposed to develop characterization techniques which would allow the measurement of the transient current responses through nanotube conductors. This work will address two issues: i) the integration of nanotube conductors with appropriate high speed photoconductor materials and ii) the development of characterization techniques for nanoscale conductors based on pump/probe measurements using high speed pulsed laser systems. It is expected that the proposed work will shed light on the conduction processes of these interesting electronic materials and provide important capabilities for integration of these materials with other electronic materials, including semiconductor device structures.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">103227</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103227</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n450" target="n451">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">NER: Investigation of Electronic Transport in Carbon Nanotubes Using an Ultrafast Photoconduction Technique</data>
      <data key="e_abstract">This proposal was received in response to NSE, NSF-0019. Carbon nanotubes are of interest as electronic conductors because of their unique properties, notably their behavior as 1-D conductors and their ballistic nature over several microns at room temperature. A general picture of electronic conduction in nanotubes is being developed by researchers in the field. Although the transient response of current flow is expected to be quite rapid (picosecond range), present experimental data for conduction properties of nanotubes is primarily obtained at low frequencies or in static measurements. In order to shed light on the fundamental current transport processes in nanotubes, an experimental and theoretical effort is proposed to develop characterization techniques which would allow the measurement of the transient current responses through nanotube conductors. This work will address two issues: i) the integration of nanotube conductors with appropriate high speed photoconductor materials and ii) the development of characterization techniques for nanoscale conductors based on pump/probe measurements using high speed pulsed laser systems. It is expected that the proposed work will shed light on the conduction processes of these interesting electronic materials and provide important capabilities for integration of these materials with other electronic materials, including semiconductor device structures.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">103227</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103227</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n450" target="n452">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">NER: Investigation of Electronic Transport in Carbon Nanotubes Using an Ultrafast Photoconduction Technique</data>
      <data key="e_abstract">This proposal was received in response to NSE, NSF-0019. Carbon nanotubes are of interest as electronic conductors because of their unique properties, notably their behavior as 1-D conductors and their ballistic nature over several microns at room temperature. A general picture of electronic conduction in nanotubes is being developed by researchers in the field. Although the transient response of current flow is expected to be quite rapid (picosecond range), present experimental data for conduction properties of nanotubes is primarily obtained at low frequencies or in static measurements. In order to shed light on the fundamental current transport processes in nanotubes, an experimental and theoretical effort is proposed to develop characterization techniques which would allow the measurement of the transient current responses through nanotube conductors. This work will address two issues: i) the integration of nanotube conductors with appropriate high speed photoconductor materials and ii) the development of characterization techniques for nanoscale conductors based on pump/probe measurements using high speed pulsed laser systems. It is expected that the proposed work will shed light on the conduction processes of these interesting electronic materials and provide important capabilities for integration of these materials with other electronic materials, including semiconductor device structures.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">103227</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103227</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n451" target="n452">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">NER: Investigation of Electronic Transport in Carbon Nanotubes Using an Ultrafast Photoconduction Technique</data>
      <data key="e_abstract">This proposal was received in response to NSE, NSF-0019. Carbon nanotubes are of interest as electronic conductors because of their unique properties, notably their behavior as 1-D conductors and their ballistic nature over several microns at room temperature. A general picture of electronic conduction in nanotubes is being developed by researchers in the field. Although the transient response of current flow is expected to be quite rapid (picosecond range), present experimental data for conduction properties of nanotubes is primarily obtained at low frequencies or in static measurements. In order to shed light on the fundamental current transport processes in nanotubes, an experimental and theoretical effort is proposed to develop characterization techniques which would allow the measurement of the transient current responses through nanotube conductors. This work will address two issues: i) the integration of nanotube conductors with appropriate high speed photoconductor materials and ii) the development of characterization techniques for nanoscale conductors based on pump/probe measurements using high speed pulsed laser systems. It is expected that the proposed work will shed light on the conduction processes of these interesting electronic materials and provide important capabilities for integration of these materials with other electronic materials, including semiconductor device structures.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">103227</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103227</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n456" target="n457">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Adaptive Synchronization Framework Supporting Device-independent Mobile Computing</data>
      <data key="e_abstract">Recent technological advances in wireless networks and portable information appliances have engendered the new paradigm of mobile computing, enabling users carrying portable devices to access and update data regardless of their physical location or movement behavior. However, the use of this technology will remain limited unless we develop the necessary concepts, theory and infrastructure that can seamlessly integrate mobility and disconnection into everyday networked computing. Extensive research is needed in order for mobile computing to become pervasive. &lt;br/&gt;Current research efforts do not support sufficient automation and management of mobile data access and update. Assumptions are usually made regarding the application, the source of mobile data, or the particularities of the mobile device. Ubiquitous data access is lacking due to the tight coupling between the mobile device and the mobile data. Users are therefore not allowed to switch mobile devices without spending a lot of effort on copying and re-hoarding. Additionally, current solutions do not support mobile access and hoarding from heterogeneous sources of data such as file systems, database servers, web servers, etc. Current research is also limited to basic synchronization schemes that do not take into consideration the variable and individual needs of consistency and up-to-dateness of mobile data. Finally, current mobile transaction models are not efficient in terms of successful commit rate, especially under prolonged periods of disconnection. &lt;br/&gt;This proposal is based on ongoing research on mobile computing, operating systems and data warehousing. The overall goal is to make mobile computing available to a broader range of users and applications by automating the hoarding of a wide variety of data from multiple heterogeneous sources into mobile devices, and to facilitate sophisticated synchronization between the mobile devices and the fixed networks where the sources reside. The specific goals are:&lt;br/&gt;Develop and evaluate a three-tiered architecture based on the Coda file system that provides independence between the mobile data and the mobile devices via a data warehouse for storing the user&apos;s working set. &lt;br/&gt;Develop algorithms for automatically and incrementally hoarding data on mobile devices and for synchronizing the contents of the mobile device with the working set in the warehouse and the original sources.&lt;br/&gt;Develop new synchronization techniques for maintaining the user&apos;s working set, based on programmable and conditional consistency specifications of mobile data items. Different data items may have different consistency requirements, and must therefore be synchronized differently.&lt;br/&gt;Develop a new model of mobile transactions that can guarantee that transactional updates performed during disconnection are highly likely to be committed upon reconnection.&lt;br/&gt;Our goal is to build upon existing results to develop the architecture and algorithms to make smart hoarding and synchronization in mobile environments a reality. We see the realization of such a framework as an important and necessary step towards making mobile computing a viable practice for a broad audience; a step that some day may lead us to the realization of ubiquitous computing. In our vision, users should be allowed to switch to any mobile device to connect to the fixed network and carry the necessary data with them, without having to worry about hoarding, synchronization, and other low-level burdens. Given our prior experience and research results in the areas of mobile computing, distributed systems, and data warehousing, we believe that we can successfully apply data warehousing technology to manage important user data as well as support the incremental maintenance of this data in light of frequent updates.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">100770</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">100770</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n461" target="n462">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Integer and Combinatorial Optimization: Polyhedral and Graph Theoretic Methods</data>
      <data key="e_abstract">This project addresses basic theoretical and computational aspects of integer programming and combinatorial optimization, using tools of linear algebra and graph theory. In the nineties the principal investigators have developed a computationally successful approach to mixed 0-1 programming known as lift-and-project. A central theme of the research is to develop this approach in new directions that seem computationally even more promising. One of these directions uses a one to one correspondence recently established by the investigators&apos; team between basic solutions to the higher dimensional linear program used to generate lift-and-project cuts, and certain basic solutions of the LP relaxation of the mixed integer program itself. This correspondence can be used to generate &quot;deepest cuts&quot; in the lift-and-project sense without explicitly generating the higher dimensional linear program. Another important topic is the creation of bridges from integer programming to the branch of computer science known as constraint programming. A recently discovered linear characterization of cardinality rules and similar logical constructs, along with the linear time separability of the inequalities involved, makes it possible to develop symbolic constraints usable in an integer programming context that may significantly enhance the power of algorithms dealing with problems involving logical conditions. A third line of research pursued under this project investigates properties of a 0-1 matrix that make the set packing problem or the set covering problem (or both) defined by it have only integer basic solutions. Structural properties of balanced matrices were obtained under previous NSF grants.; ideal and perfect matrices are currently under investigation.&lt;br/&gt;&lt;br/&gt;Decision makers often face problems that have a combinatorial aspect: choose one among a very large number of possible decisions. A standard approach is to formulate such problems as integer programs and to use a &quot;solver&quot; to find the best solution. Although integer programming solvers have improved significantly over the last decade, they are still unable to solve many large scale problems to optimality. This project lays the theoretical and analytical foundation for a new generation of solvers for integer programs. The lift-and-project approach developed by the principal investigators has proved well suited to solve hard integer programming problems. Speed remains an issue however. This research project addresses the speed issue by investigating new, faster ways of computing the cuts, lift-and-project as well as other. The potential benefits of the project are significant since cut generators are already being implemented in commercial integer programming solvers and, obviously, the performance of these solvers would be improved by better cut generators. Based on the recent increase in the use of solvers by managers in most fields of business, solvers with improved performance have the potential to increase productivity in the industries where these solvers are used (manufacturing, airline, financial and other industries).</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">98427</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98427</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n463" target="n464">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Nanoscale Single-electron Switching Arrays for Self-evolving Neuromorphic Networks</data>
      <data key="e_abstract">The goal of this project is to carry out a detailed multi-disciplinary study of single-electron&lt;br/&gt;latching switches and of possible use of 2D arrays of such switches for hardware&lt;br/&gt;implementation of self-organizing (plastic) neuromorphic networks. Preliminary estimates&lt;br/&gt;show that such networks may provide unparalleled possibilities for complex information&lt;br/&gt;processing. By these estimates, the networks may also have remarkable scaling properties:&lt;br/&gt;if implemented using a 10-nm technology, they may have density about 10 8 neurons per&lt;br/&gt;cm 2 at manageable power dissipation below 100 W/cm 2 , and feature full learning cycle time&lt;br/&gt;of the order of a few seconds. This scaling gives every hope that the networks will be able,&lt;br/&gt;after initial (largely unsupervised) learning, not only provide complex information processing&lt;br/&gt;including complex image recognition, but possibly reproduce biological evolution of the&lt;br/&gt;cerebral cortex at a time scale some 6 orders of magnitude shorter.&lt;br/&gt;The objective of the proposed project is to carry out a preliminary study of this&lt;br/&gt;remarkable opportunity, addressing all its basic aspects at several structural levels. In&lt;br/&gt;particular, research will include the following components:&lt;br/&gt;A. Single-electron switch node design (D. Averin, K. Likharev, J. Wells).&lt;br/&gt;Detailed theoretical analysis and modeling (on two basic levels of single-electron transport&lt;br/&gt;theory) of statics, dynamics, and statistics of the proposed single-electron latching switches.&lt;br/&gt;B. Low temperature prototyping (J. Lukens). Fabrication and experimental&lt;br/&gt;study of Al/AlOx/Al prototypes of single-electron latching switches, with the goal to scale&lt;br/&gt;single-electron islands down to 100 nm and tunnel junctions to 10 nm, respectively, which&lt;br/&gt;would bring the reliable operation temperature up to about 10 K.&lt;br/&gt;C. Molecular single-electron device development (B. Brunschwig, J. Lukens,&lt;br/&gt;A. Mayr). Exploration of the opportunity to implement the basic component of the switches,&lt;br/&gt;the single-electron transistor, by chemical self-assembly of molecular components. The&lt;br/&gt;molecular components will be deposited in solution on the prefabricated metallic wire&lt;br/&gt;structures, and then characterized using a set of electrical, electrochemical, and time-resolved&lt;br/&gt;laser-spectrometry methods.&lt;br/&gt;D. Top level modeling and analysis (J. Barhen, M. Bender, K. Likharev).&lt;br/&gt;Large-scale computer simulation and a partial analytical study of the growth, dynamics, and&lt;br/&gt;self-adaptation of neuromorphic networks based on these switches.&lt;br/&gt;Hopefully, the project will achieve enough progress to justify a large-scale R&amp;D effort&lt;br/&gt;in this exciting direction. In particular, a reliable evidence of self-organization of adaptive&lt;br/&gt;neuromorphic networks during largely unsupervised learning would certainly be followed by&lt;br/&gt;the first hardware implementations of sizable networks (possibly, after an initial stage of&lt;br/&gt;purely-CMOS-based prototyping using commercially available FPGA technology).&lt;br/&gt;The project will have a substantial educational component. Specifically (besides&lt;br/&gt;participating in general educational Stony Brook initiatives), at least 4 FTE graduate&lt;br/&gt;students will be involved in the project each year, and some 20 undergraduate and&lt;br/&gt;graduate students will take part in the project during its full 4-year period. At least one&lt;br/&gt;student will work in BNL and one in ORNL most of the time. Working in a multi-disciplinary&lt;br/&gt;team will allow these students to overcome inter-departmental barriers in their education.&lt;br/&gt;As another specific educational initiative, we plan to organize a Web-based undergraduate&lt;br/&gt;course on massively parallel supercomputing and neural networks, using the IBM SP3&lt;br/&gt;computer at Oak Ridge.&lt;br/&gt;Work on the inter-related aspects of this multi-disciplinary project will be constantly&lt;br/&gt;coordinated by its P.I. (K. Likharev). In particular, regular meetings of all Stony Brook and&lt;br/&gt;Brookhaven participants of the team working on the project (including postdoctoral&lt;br/&gt;associates and students), and annual meetings with Oak Ridge collaborators, are planned.</data>
      <data key="e_pgm">2878</data>
      <data key="e_label">103059</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">103059</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n463" target="n465">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Nanoscale Single-electron Switching Arrays for Self-evolving Neuromorphic Networks</data>
      <data key="e_abstract">The goal of this project is to carry out a detailed multi-disciplinary study of single-electron&lt;br/&gt;latching switches and of possible use of 2D arrays of such switches for hardware&lt;br/&gt;implementation of self-organizing (plastic) neuromorphic networks. Preliminary estimates&lt;br/&gt;show that such networks may provide unparalleled possibilities for complex information&lt;br/&gt;processing. By these estimates, the networks may also have remarkable scaling properties:&lt;br/&gt;if implemented using a 10-nm technology, they may have density about 10 8 neurons per&lt;br/&gt;cm 2 at manageable power dissipation below 100 W/cm 2 , and feature full learning cycle time&lt;br/&gt;of the order of a few seconds. This scaling gives every hope that the networks will be able,&lt;br/&gt;after initial (largely unsupervised) learning, not only provide complex information processing&lt;br/&gt;including complex image recognition, but possibly reproduce biological evolution of the&lt;br/&gt;cerebral cortex at a time scale some 6 orders of magnitude shorter.&lt;br/&gt;The objective of the proposed project is to carry out a preliminary study of this&lt;br/&gt;remarkable opportunity, addressing all its basic aspects at several structural levels. In&lt;br/&gt;particular, research will include the following components:&lt;br/&gt;A. Single-electron switch node design (D. Averin, K. Likharev, J. Wells).&lt;br/&gt;Detailed theoretical analysis and modeling (on two basic levels of single-electron transport&lt;br/&gt;theory) of statics, dynamics, and statistics of the proposed single-electron latching switches.&lt;br/&gt;B. Low temperature prototyping (J. Lukens). Fabrication and experimental&lt;br/&gt;study of Al/AlOx/Al prototypes of single-electron latching switches, with the goal to scale&lt;br/&gt;single-electron islands down to 100 nm and tunnel junctions to 10 nm, respectively, which&lt;br/&gt;would bring the reliable operation temperature up to about 10 K.&lt;br/&gt;C. Molecular single-electron device development (B. Brunschwig, J. Lukens,&lt;br/&gt;A. Mayr). Exploration of the opportunity to implement the basic component of the switches,&lt;br/&gt;the single-electron transistor, by chemical self-assembly of molecular components. The&lt;br/&gt;molecular components will be deposited in solution on the prefabricated metallic wire&lt;br/&gt;structures, and then characterized using a set of electrical, electrochemical, and time-resolved&lt;br/&gt;laser-spectrometry methods.&lt;br/&gt;D. Top level modeling and analysis (J. Barhen, M. Bender, K. Likharev).&lt;br/&gt;Large-scale computer simulation and a partial analytical study of the growth, dynamics, and&lt;br/&gt;self-adaptation of neuromorphic networks based on these switches.&lt;br/&gt;Hopefully, the project will achieve enough progress to justify a large-scale R&amp;D effort&lt;br/&gt;in this exciting direction. In particular, a reliable evidence of self-organization of adaptive&lt;br/&gt;neuromorphic networks during largely unsupervised learning would certainly be followed by&lt;br/&gt;the first hardware implementations of sizable networks (possibly, after an initial stage of&lt;br/&gt;purely-CMOS-based prototyping using commercially available FPGA technology).&lt;br/&gt;The project will have a substantial educational component. Specifically (besides&lt;br/&gt;participating in general educational Stony Brook initiatives), at least 4 FTE graduate&lt;br/&gt;students will be involved in the project each year, and some 20 undergraduate and&lt;br/&gt;graduate students will take part in the project during its full 4-year period. At least one&lt;br/&gt;student will work in BNL and one in ORNL most of the time. Working in a multi-disciplinary&lt;br/&gt;team will allow these students to overcome inter-departmental barriers in their education.&lt;br/&gt;As another specific educational initiative, we plan to organize a Web-based undergraduate&lt;br/&gt;course on massively parallel supercomputing and neural networks, using the IBM SP3&lt;br/&gt;computer at Oak Ridge.&lt;br/&gt;Work on the inter-related aspects of this multi-disciplinary project will be constantly&lt;br/&gt;coordinated by its P.I. (K. Likharev). In particular, regular meetings of all Stony Brook and&lt;br/&gt;Brookhaven participants of the team working on the project (including postdoctoral&lt;br/&gt;associates and students), and annual meetings with Oak Ridge collaborators, are planned.</data>
      <data key="e_pgm">2878</data>
      <data key="e_label">103059</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">103059</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n464" target="n465">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Nanoscale Single-electron Switching Arrays for Self-evolving Neuromorphic Networks</data>
      <data key="e_abstract">The goal of this project is to carry out a detailed multi-disciplinary study of single-electron&lt;br/&gt;latching switches and of possible use of 2D arrays of such switches for hardware&lt;br/&gt;implementation of self-organizing (plastic) neuromorphic networks. Preliminary estimates&lt;br/&gt;show that such networks may provide unparalleled possibilities for complex information&lt;br/&gt;processing. By these estimates, the networks may also have remarkable scaling properties:&lt;br/&gt;if implemented using a 10-nm technology, they may have density about 10 8 neurons per&lt;br/&gt;cm 2 at manageable power dissipation below 100 W/cm 2 , and feature full learning cycle time&lt;br/&gt;of the order of a few seconds. This scaling gives every hope that the networks will be able,&lt;br/&gt;after initial (largely unsupervised) learning, not only provide complex information processing&lt;br/&gt;including complex image recognition, but possibly reproduce biological evolution of the&lt;br/&gt;cerebral cortex at a time scale some 6 orders of magnitude shorter.&lt;br/&gt;The objective of the proposed project is to carry out a preliminary study of this&lt;br/&gt;remarkable opportunity, addressing all its basic aspects at several structural levels. In&lt;br/&gt;particular, research will include the following components:&lt;br/&gt;A. Single-electron switch node design (D. Averin, K. Likharev, J. Wells).&lt;br/&gt;Detailed theoretical analysis and modeling (on two basic levels of single-electron transport&lt;br/&gt;theory) of statics, dynamics, and statistics of the proposed single-electron latching switches.&lt;br/&gt;B. Low temperature prototyping (J. Lukens). Fabrication and experimental&lt;br/&gt;study of Al/AlOx/Al prototypes of single-electron latching switches, with the goal to scale&lt;br/&gt;single-electron islands down to 100 nm and tunnel junctions to 10 nm, respectively, which&lt;br/&gt;would bring the reliable operation temperature up to about 10 K.&lt;br/&gt;C. Molecular single-electron device development (B. Brunschwig, J. Lukens,&lt;br/&gt;A. Mayr). Exploration of the opportunity to implement the basic component of the switches,&lt;br/&gt;the single-electron transistor, by chemical self-assembly of molecular components. The&lt;br/&gt;molecular components will be deposited in solution on the prefabricated metallic wire&lt;br/&gt;structures, and then characterized using a set of electrical, electrochemical, and time-resolved&lt;br/&gt;laser-spectrometry methods.&lt;br/&gt;D. Top level modeling and analysis (J. Barhen, M. Bender, K. Likharev).&lt;br/&gt;Large-scale computer simulation and a partial analytical study of the growth, dynamics, and&lt;br/&gt;self-adaptation of neuromorphic networks based on these switches.&lt;br/&gt;Hopefully, the project will achieve enough progress to justify a large-scale R&amp;D effort&lt;br/&gt;in this exciting direction. In particular, a reliable evidence of self-organization of adaptive&lt;br/&gt;neuromorphic networks during largely unsupervised learning would certainly be followed by&lt;br/&gt;the first hardware implementations of sizable networks (possibly, after an initial stage of&lt;br/&gt;purely-CMOS-based prototyping using commercially available FPGA technology).&lt;br/&gt;The project will have a substantial educational component. Specifically (besides&lt;br/&gt;participating in general educational Stony Brook initiatives), at least 4 FTE graduate&lt;br/&gt;students will be involved in the project each year, and some 20 undergraduate and&lt;br/&gt;graduate students will take part in the project during its full 4-year period. At least one&lt;br/&gt;student will work in BNL and one in ORNL most of the time. Working in a multi-disciplinary&lt;br/&gt;team will allow these students to overcome inter-departmental barriers in their education.&lt;br/&gt;As another specific educational initiative, we plan to organize a Web-based undergraduate&lt;br/&gt;course on massively parallel supercomputing and neural networks, using the IBM SP3&lt;br/&gt;computer at Oak Ridge.&lt;br/&gt;Work on the inter-related aspects of this multi-disciplinary project will be constantly&lt;br/&gt;coordinated by its P.I. (K. Likharev). In particular, regular meetings of all Stony Brook and&lt;br/&gt;Brookhaven participants of the team working on the project (including postdoctoral&lt;br/&gt;associates and students), and annual meetings with Oak Ridge collaborators, are planned.</data>
      <data key="e_pgm">2878</data>
      <data key="e_label">103059</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">103059</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n466" target="n467">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">NER: Direct Electron Beam Writing for Fabrication of Functional Nano-Scale Architectures</data>
      <data key="e_abstract">This proposal was recieved in response to NSE, NSF-0019. Focused electron beam decomposition of molecules adsorbed on surfaces will be investigated as a means for fabricating electrical contacts to individual or small arrays of nanostructures. This direct write technique offers a convenient, flexible and practical method for bridging the gap between mesoscopic lithography and the nanoscale. Of primary importance is the purity and resistivity of the deposited film. Resistivity will be correlated with process parameters by depositing between predefined metallic contact pads. Contamination of deposited metallic features will be avoided by using inorganic precursor molecules such as TaF5, TiCl4 and WF6 . Film purity will be determined in situ ,using standard surface science techniques. Preliminary estimates of write speeds achievable in environmental electron microscopes indicate that 1:1 aspect ratio,nm-scale wires can be written at rates of 0.1 um /sec. Additionally, use of environmental electron microscopy will allow simultaneous identification and contacting features of interest. The flexibility of this technique will allow tailoring the deposited structures for different applications. Examples are nanowires for electrical contacts, metal nanodot arrays for attachment of functionalized organic molecules or specially shaped metal gates deposited on semiconductor surfaces to allow charge confinement and manipulation in nanoscale regions. Thus,success of this technique will enable rapid prototyping of diverse concepts cutting across several nanoscience and technology subfields. Longer term, the capability for e-beam writing of entire nanodevices is envisioned. Electron-beam decomposition of inorganic species leading to growth of semiconductors and insulators will also be investigated. Novel precursor chemistries will be developed for e-beam growth of insulating and semiconducting phases compatible with Si-based nanoelectronics. Essentially,this is an athermal method for depositing nanoscale features at temperatures below that for which the features &apos;melt&apos; via surface diffusion of deposited atoms.</data>
      <data key="e_pgm">2878</data>
      <data key="e_label">103061</data>
      <data key="e_expirationDate">2003-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">103061</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n466" target="n468">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">NER: Direct Electron Beam Writing for Fabrication of Functional Nano-Scale Architectures</data>
      <data key="e_abstract">This proposal was recieved in response to NSE, NSF-0019. Focused electron beam decomposition of molecules adsorbed on surfaces will be investigated as a means for fabricating electrical contacts to individual or small arrays of nanostructures. This direct write technique offers a convenient, flexible and practical method for bridging the gap between mesoscopic lithography and the nanoscale. Of primary importance is the purity and resistivity of the deposited film. Resistivity will be correlated with process parameters by depositing between predefined metallic contact pads. Contamination of deposited metallic features will be avoided by using inorganic precursor molecules such as TaF5, TiCl4 and WF6 . Film purity will be determined in situ ,using standard surface science techniques. Preliminary estimates of write speeds achievable in environmental electron microscopes indicate that 1:1 aspect ratio,nm-scale wires can be written at rates of 0.1 um /sec. Additionally, use of environmental electron microscopy will allow simultaneous identification and contacting features of interest. The flexibility of this technique will allow tailoring the deposited structures for different applications. Examples are nanowires for electrical contacts, metal nanodot arrays for attachment of functionalized organic molecules or specially shaped metal gates deposited on semiconductor surfaces to allow charge confinement and manipulation in nanoscale regions. Thus,success of this technique will enable rapid prototyping of diverse concepts cutting across several nanoscience and technology subfields. Longer term, the capability for e-beam writing of entire nanodevices is envisioned. Electron-beam decomposition of inorganic species leading to growth of semiconductors and insulators will also be investigated. Novel precursor chemistries will be developed for e-beam growth of insulating and semiconducting phases compatible with Si-based nanoelectronics. Essentially,this is an athermal method for depositing nanoscale features at temperatures below that for which the features &apos;melt&apos; via surface diffusion of deposited atoms.</data>
      <data key="e_pgm">2878</data>
      <data key="e_label">103061</data>
      <data key="e_expirationDate">2003-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">103061</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n467" target="n468">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">NER: Direct Electron Beam Writing for Fabrication of Functional Nano-Scale Architectures</data>
      <data key="e_abstract">This proposal was recieved in response to NSE, NSF-0019. Focused electron beam decomposition of molecules adsorbed on surfaces will be investigated as a means for fabricating electrical contacts to individual or small arrays of nanostructures. This direct write technique offers a convenient, flexible and practical method for bridging the gap between mesoscopic lithography and the nanoscale. Of primary importance is the purity and resistivity of the deposited film. Resistivity will be correlated with process parameters by depositing between predefined metallic contact pads. Contamination of deposited metallic features will be avoided by using inorganic precursor molecules such as TaF5, TiCl4 and WF6 . Film purity will be determined in situ ,using standard surface science techniques. Preliminary estimates of write speeds achievable in environmental electron microscopes indicate that 1:1 aspect ratio,nm-scale wires can be written at rates of 0.1 um /sec. Additionally, use of environmental electron microscopy will allow simultaneous identification and contacting features of interest. The flexibility of this technique will allow tailoring the deposited structures for different applications. Examples are nanowires for electrical contacts, metal nanodot arrays for attachment of functionalized organic molecules or specially shaped metal gates deposited on semiconductor surfaces to allow charge confinement and manipulation in nanoscale regions. Thus,success of this technique will enable rapid prototyping of diverse concepts cutting across several nanoscience and technology subfields. Longer term, the capability for e-beam writing of entire nanodevices is envisioned. Electron-beam decomposition of inorganic species leading to growth of semiconductors and insulators will also be investigated. Novel precursor chemistries will be developed for e-beam growth of insulating and semiconducting phases compatible with Si-based nanoelectronics. Essentially,this is an athermal method for depositing nanoscale features at temperatures below that for which the features &apos;melt&apos; via surface diffusion of deposited atoms.</data>
      <data key="e_pgm">2878</data>
      <data key="e_label">103061</data>
      <data key="e_expirationDate">2003-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">103061</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n471" target="n472">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">Learning and the Design of the Internet</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">4095</data>
      <data key="e_label">196514</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">196514</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n473" target="n474">
      <data key="e_effectiveDate">2001-07-01</data>
      <data key="e_title">FIU AMPATH Workshop to Identify Areas of Scientific Collaboration between the US and the AMPATH Service Area; August 2001</data>
      <data key="e_abstract">A workshop will be conducted by Florida International University (FIU) to discuss the current and future needs and activities of researchers needing a high performance network for their research projects associated with activities and projects in South and Central America, Mexico and the Caribbean. These projects will focus on the interests and benefits to US science. The participants in the works hop will be scientist from the United States and scientists from the region countries. The expected output from the workshop will be :&lt;br/&gt;&lt;br/&gt; A report of the factual situation on the present and foreseeable future needs of scientific research projects in the region&lt;br/&gt;A statement of recommendation for action concerning these needs.</data>
      <data key="e_pgm">4090</data>
      <data key="e_label">123388</data>
      <data key="e_expirationDate">2001-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">123388</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n480" target="n481">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Haptic Texture Perception and Rendering for Personal Robotics</data>
      <data key="e_abstract">The proposed work focuses on human-robot interaction, namely on the robot physically sensing the human hand. Specifically, the PIs will study the microstructure (texture) of the contact surfaces between a robot and a human hand, to infer the perceptual dimensionality of haptic texture sensing (perceptual model), and establish the mapping of relevant spaces. Methods for producing intuitive and efficient synthetic textures will be investigated. Rendering algorithms will be developed for synthesizing textures with desired perceptual qualities. The work is expected to contribute to various areas of haptic perception, texture studies, and multimodal rendering of information.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">98443</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">98443</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n480" target="n482">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Haptic Texture Perception and Rendering for Personal Robotics</data>
      <data key="e_abstract">The proposed work focuses on human-robot interaction, namely on the robot physically sensing the human hand. Specifically, the PIs will study the microstructure (texture) of the contact surfaces between a robot and a human hand, to infer the perceptual dimensionality of haptic texture sensing (perceptual model), and establish the mapping of relevant spaces. Methods for producing intuitive and efficient synthetic textures will be investigated. Rendering algorithms will be developed for synthesizing textures with desired perceptual qualities. The work is expected to contribute to various areas of haptic perception, texture studies, and multimodal rendering of information.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">98443</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">98443</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n16" target="n480">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Haptic Texture Perception and Rendering for Personal Robotics</data>
      <data key="e_abstract">The proposed work focuses on human-robot interaction, namely on the robot physically sensing the human hand. Specifically, the PIs will study the microstructure (texture) of the contact surfaces between a robot and a human hand, to infer the perceptual dimensionality of haptic texture sensing (perceptual model), and establish the mapping of relevant spaces. Methods for producing intuitive and efficient synthetic textures will be investigated. Rendering algorithms will be developed for synthesizing textures with desired perceptual qualities. The work is expected to contribute to various areas of haptic perception, texture studies, and multimodal rendering of information.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">98443</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">98443</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n480" target="n484">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Haptic Texture Perception and Rendering for Personal Robotics</data>
      <data key="e_abstract">The proposed work focuses on human-robot interaction, namely on the robot physically sensing the human hand. Specifically, the PIs will study the microstructure (texture) of the contact surfaces between a robot and a human hand, to infer the perceptual dimensionality of haptic texture sensing (perceptual model), and establish the mapping of relevant spaces. Methods for producing intuitive and efficient synthetic textures will be investigated. Rendering algorithms will be developed for synthesizing textures with desired perceptual qualities. The work is expected to contribute to various areas of haptic perception, texture studies, and multimodal rendering of information.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">98443</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">98443</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n481" target="n482">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Haptic Texture Perception and Rendering for Personal Robotics</data>
      <data key="e_abstract">The proposed work focuses on human-robot interaction, namely on the robot physically sensing the human hand. Specifically, the PIs will study the microstructure (texture) of the contact surfaces between a robot and a human hand, to infer the perceptual dimensionality of haptic texture sensing (perceptual model), and establish the mapping of relevant spaces. Methods for producing intuitive and efficient synthetic textures will be investigated. Rendering algorithms will be developed for synthesizing textures with desired perceptual qualities. The work is expected to contribute to various areas of haptic perception, texture studies, and multimodal rendering of information.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">98443</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">98443</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n16" target="n481">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Haptic Texture Perception and Rendering for Personal Robotics</data>
      <data key="e_abstract">The proposed work focuses on human-robot interaction, namely on the robot physically sensing the human hand. Specifically, the PIs will study the microstructure (texture) of the contact surfaces between a robot and a human hand, to infer the perceptual dimensionality of haptic texture sensing (perceptual model), and establish the mapping of relevant spaces. Methods for producing intuitive and efficient synthetic textures will be investigated. Rendering algorithms will be developed for synthesizing textures with desired perceptual qualities. The work is expected to contribute to various areas of haptic perception, texture studies, and multimodal rendering of information.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">98443</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">98443</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n481" target="n484">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Haptic Texture Perception and Rendering for Personal Robotics</data>
      <data key="e_abstract">The proposed work focuses on human-robot interaction, namely on the robot physically sensing the human hand. Specifically, the PIs will study the microstructure (texture) of the contact surfaces between a robot and a human hand, to infer the perceptual dimensionality of haptic texture sensing (perceptual model), and establish the mapping of relevant spaces. Methods for producing intuitive and efficient synthetic textures will be investigated. Rendering algorithms will be developed for synthesizing textures with desired perceptual qualities. The work is expected to contribute to various areas of haptic perception, texture studies, and multimodal rendering of information.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">98443</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">98443</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n16" target="n482">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Haptic Texture Perception and Rendering for Personal Robotics</data>
      <data key="e_abstract">The proposed work focuses on human-robot interaction, namely on the robot physically sensing the human hand. Specifically, the PIs will study the microstructure (texture) of the contact surfaces between a robot and a human hand, to infer the perceptual dimensionality of haptic texture sensing (perceptual model), and establish the mapping of relevant spaces. Methods for producing intuitive and efficient synthetic textures will be investigated. Rendering algorithms will be developed for synthesizing textures with desired perceptual qualities. The work is expected to contribute to various areas of haptic perception, texture studies, and multimodal rendering of information.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">98443</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">98443</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n482" target="n484">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Haptic Texture Perception and Rendering for Personal Robotics</data>
      <data key="e_abstract">The proposed work focuses on human-robot interaction, namely on the robot physically sensing the human hand. Specifically, the PIs will study the microstructure (texture) of the contact surfaces between a robot and a human hand, to infer the perceptual dimensionality of haptic texture sensing (perceptual model), and establish the mapping of relevant spaces. Methods for producing intuitive and efficient synthetic textures will be investigated. Rendering algorithms will be developed for synthesizing textures with desired perceptual qualities. The work is expected to contribute to various areas of haptic perception, texture studies, and multimodal rendering of information.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">98443</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">98443</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n16" target="n484">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Haptic Texture Perception and Rendering for Personal Robotics</data>
      <data key="e_abstract">The proposed work focuses on human-robot interaction, namely on the robot physically sensing the human hand. Specifically, the PIs will study the microstructure (texture) of the contact surfaces between a robot and a human hand, to infer the perceptual dimensionality of haptic texture sensing (perceptual model), and establish the mapping of relevant spaces. Methods for producing intuitive and efficient synthetic textures will be investigated. Rendering algorithms will be developed for synthesizing textures with desired perceptual qualities. The work is expected to contribute to various areas of haptic perception, texture studies, and multimodal rendering of information.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">98443</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">98443</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n486" target="n487">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">ITR/PE: Aligning Societal Values, Privacy Policy, and IT Requirements</data>
      <data key="e_abstract">NSF PROPOSAL #0113792&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;ITR: Aligning Societal Values, Privacy Policy, and IT Requirements&lt;br/&gt;&lt;br/&gt;Colin Potts, Georgia Institute of Technology&lt;br/&gt;Annie I. Anton, North Carolina State University&lt;br/&gt;&lt;br/&gt;The guarantee and assurance of privacy must be included in the design of information technologies from the onset. This research focuses on how society uses, values, and protects citizens&apos; personal information. From the perspective of system design, software engineers need methods and tools to enable them to design systems that reflect those values and protect personal information, accordingly. This research examines how privacy considerations and value systems influence the design, deployment and consequences of IT. Investigations will include study of the motivations and barriers to the use of IT when use of these technologies requires the user to provide Personally Identifiable Information (PII). In essence, this work focuses on: societal values, web site policies, and the operational functioning of web-based e-commerce systems, which are often misaligned. The goal is to develop concepts, tools and techniques that help IT professionals and policy makers bring policies and system requirements into better alignment. An action-oriented set of conceptual tools, including guidelines and privacy-relevant policy templates will be constructed and validated. The tools will be fully documented and illustrated on a web site developed for the purpose of conducting the proposed project and disseminating results and recommendations.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113792</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113792</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n417" target="n489">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Surface and Motion Capture for High Fidelity Synthesis of Digital Humans</data>
      <data key="e_abstract">Proposal #0098005&lt;br/&gt;Popovic, Zoran&lt;br/&gt;U of Washington&lt;br/&gt;&lt;br/&gt;We propose a multi-layered approach to capturing and synthesizing realistic human shapes and motions. To capture the static shape of real humans, we will employ 3D scanning techniques including hierarchical light striping, simultaneous multi-striping, and photometric stereo. A feature-tracked motion capture system as well as 3D scanning techniques will generate motion data. The investigators will acquire this motion data at varying resolutions in order to drive the analysis of skeletal motion, body part deformation such as bulging due to flexing a muscle, and secondary motion such as leg vibrations that occur when stomping on the ground.&lt;br/&gt;&lt;br/&gt;This wealth of human data will then drive an analysis, modeling, and synthesis stage. The static scan data will be analyzed to construct the space of possible human shapes. This human shape model together with the body part motion capture and full-body motion capture will be used to construct a detailed kinematic&lt;br/&gt;model of the human body. Modeling human shape movement at such different levels of detail will allow control of the human motion on the coarse skeletal level while preserving the fine details such as muscle bulging. Furthermore, this multi-layered approach will enable selective replacement of different layers in the human model structure. For example, it will be possible to map the animated movement onto a different body scan and observe a different surface shape movement and creasing. The detailed kinematic human model will be further extended with a model of human dynamics by taking into account a number of physical properties of the human body such as muscle usage and mass distribution. This additional dynamic information provides a way to preserve the realism of motion even when the structure of motion is significantly modified. In addition, the investigators will extend the skeletal dynamic model with secondary motion simulations constructed to replicate the loose skin and tissue vibrations that occur in high-energy movements.&lt;br/&gt;&lt;br/&gt;The investigators will incorporate their work into new curriculum both at their university and in courses being offered to the professional community. This work will be folded into CDROM&apos;s that reach a wide audience, including the general public and a broad spectrum of high school students who may be considering careers in information technology. The results of the research will include complex databases of human shape and motion to be distributed to the general research community in order to encourage further research in this area.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">98005</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98005</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n490" target="n491">
      <data key="e_effectiveDate">2001-08-23</data>
      <data key="e_title">REU: Performance Measurement and Evaluation of Multithreaded Systems</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">2876</data>
      <data key="e_label">296115</data>
      <data key="e_expirationDate">2003-05-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">296115</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n496" target="n497">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">ITR/SY: A Neuromorphic Vision System for Every-Citizen Interfaces</data>
      <data key="e_abstract">This project aims to extend an existing simple saliency-based visual attention system to animated color video sequences so as to enable it to cue the object recognition module towards interesting locations in live video streams, and simultaneously to extend an existing model for object recognition to on-line adaptability through top-down signals and task- and object-dependent learning of features. The PIs will then integrate these attention and recognition models, by developing feedforward and feedback interactions between localization of regions of interest and object recognition in those regions. This will require substantial elaboration of both models, as well as specific work on their integration. The result will be a complete model of object localization and recognition in primates, with direct applicability to computer vision challenges. The PIs will next implement and deploy the combined model on a cluster of CPUs linked by very fast interconnect (just installed at USC) to allow for real-time processing, and will demonstrate its utility in a prototype video-conferencing application in which the on-line adaptive attentional component of the integrated system will quickly locate regions in the monitored environment where something interesting is happening (e.g., a user raising her hand in a conference room). The recognition part of the system will then be trained and refined on-line to recognize relatively simple hand signs (e.g., a finger pointing up, meaning that the user wishes to become the center of interest in a video-conference). This work will demonstrate two points: that a biologically-inspired approach to traditionally hard computer vision problems can yield unusually robust and versatile vision systems (which work with color video streams and quickly adapt to various environmental conditions, users, and tasks); and that computational neuroscience models of vision can be extended to yield real, useful and widely applicable computer vision systems, and are not restricted to testing neuroscience hypotheses under simple laboratory stimuli.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112991</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112991</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n496" target="n498">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">ITR/SY: A Neuromorphic Vision System for Every-Citizen Interfaces</data>
      <data key="e_abstract">This project aims to extend an existing simple saliency-based visual attention system to animated color video sequences so as to enable it to cue the object recognition module towards interesting locations in live video streams, and simultaneously to extend an existing model for object recognition to on-line adaptability through top-down signals and task- and object-dependent learning of features. The PIs will then integrate these attention and recognition models, by developing feedforward and feedback interactions between localization of regions of interest and object recognition in those regions. This will require substantial elaboration of both models, as well as specific work on their integration. The result will be a complete model of object localization and recognition in primates, with direct applicability to computer vision challenges. The PIs will next implement and deploy the combined model on a cluster of CPUs linked by very fast interconnect (just installed at USC) to allow for real-time processing, and will demonstrate its utility in a prototype video-conferencing application in which the on-line adaptive attentional component of the integrated system will quickly locate regions in the monitored environment where something interesting is happening (e.g., a user raising her hand in a conference room). The recognition part of the system will then be trained and refined on-line to recognize relatively simple hand signs (e.g., a finger pointing up, meaning that the user wishes to become the center of interest in a video-conference). This work will demonstrate two points: that a biologically-inspired approach to traditionally hard computer vision problems can yield unusually robust and versatile vision systems (which work with color video streams and quickly adapt to various environmental conditions, users, and tasks); and that computational neuroscience models of vision can be extended to yield real, useful and widely applicable computer vision systems, and are not restricted to testing neuroscience hypotheses under simple laboratory stimuli.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112991</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112991</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n497" target="n498">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">ITR/SY: A Neuromorphic Vision System for Every-Citizen Interfaces</data>
      <data key="e_abstract">This project aims to extend an existing simple saliency-based visual attention system to animated color video sequences so as to enable it to cue the object recognition module towards interesting locations in live video streams, and simultaneously to extend an existing model for object recognition to on-line adaptability through top-down signals and task- and object-dependent learning of features. The PIs will then integrate these attention and recognition models, by developing feedforward and feedback interactions between localization of regions of interest and object recognition in those regions. This will require substantial elaboration of both models, as well as specific work on their integration. The result will be a complete model of object localization and recognition in primates, with direct applicability to computer vision challenges. The PIs will next implement and deploy the combined model on a cluster of CPUs linked by very fast interconnect (just installed at USC) to allow for real-time processing, and will demonstrate its utility in a prototype video-conferencing application in which the on-line adaptive attentional component of the integrated system will quickly locate regions in the monitored environment where something interesting is happening (e.g., a user raising her hand in a conference room). The recognition part of the system will then be trained and refined on-line to recognize relatively simple hand signs (e.g., a finger pointing up, meaning that the user wishes to become the center of interest in a video-conference). This work will demonstrate two points: that a biologically-inspired approach to traditionally hard computer vision problems can yield unusually robust and versatile vision systems (which work with color video streams and quickly adapt to various environmental conditions, users, and tasks); and that computational neuroscience models of vision can be extended to yield real, useful and widely applicable computer vision systems, and are not restricted to testing neuroscience hypotheses under simple laboratory stimuli.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112991</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112991</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n503" target="n504">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Research for Mixed Signal Electronic Technologies: A Joint Initiative Between NSF and SRC: Fast Methods of Coupled EM. Circuit and Logic Simulation for Giga-Scale</data>
      <data key="e_abstract">This project aims at developing fast algorithms for rapid simulation of giga-scale systems-on-chip, where coupled circuit, logic, and electromagnetic (EM) effects (including RLC parasitics, coupling, cross talk, skin and thermal effects---both quasi-static and full-wave), are becoming increasingly important. For coupled EM and circuit simulation, we propose to investigate a novel partial-element equivalent electric circuit (PEEC) approach. For coupled EM and digital simulation, we propose to investigate variant time stepping in EM analysis and to use conservative synchronization with digital events. More specifically, our approach consists of the following key elements:&lt;br/&gt;&lt;br/&gt;Developing fast integral-equation-based solution methods that will make feasible EM simulation in these settings, and will permit hierarchical or multilevel EM analyses at varying degrees of model complexity.&lt;br/&gt;&lt;br/&gt;Implementation of fast and hierarchical schemes through a variation of the partial equivalent electric circuit method for triangular mesh tessellations.&lt;br/&gt;&lt;br/&gt;Coupling of EM simulation schemes directly to digital logic simulation in order to analyze switching and ground bounce in power-ground plane situations.&lt;br/&gt;&lt;br/&gt;Exploitation of structure and redundancies present in EM, circuit-level as well as coupled-system matrices in order to drastically reduce memory and computation time to a degree such that rigorous and complete coupled simulation will be feasible for complex-systems-on-chip. Sensitivity analysis, reduced-order modeling, and multi-physics simulations are targeted in addition.&lt;br/&gt;&lt;br/&gt;Development of optimized compilation techniques for generating ordinary differential equation (ODE) code for circuit simulation, and for exploiting the circuit structural regularity for fast hierarchical circuit/EM simulation. The application drivers for testing proposed coupled EM/circuit and EM/digital simulation will include (1) Giga-hertz CMOS transceivers on chip, and (2) Power/ground network of giga-hertz digital circuits including gate switching activity modeling.&lt;br/&gt;&lt;br/&gt;It is intended that the development of fast and rigorous EM/circuit and EM/logic simulation will enable accurate yet efficient sign-off simulation of next generation mixed-signal circuits and systems, enable simulation in the loop design optimization and architecture tradeoff, and finally enable design for testability where EM effects are becoming hard to characterize, predict, and measure.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">120371</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">120371</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n505" target="n506">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Scale-Invariant Skill Augmentation for Cooperative Human-Machine Micromanipulation Systems</data>
      <data key="e_abstract">The proposed work addresses development of novel methods for enhancing the ability of humans to perform complex and delicate manipulation tasks at a microscopic level. The proposed approach is based on a combination of off-line programming and on-line adaptation of task-specific human-computer manipulation idioms. The work builds upon this group&apos;s experience in the development of steady-hand robotic augmentation. Specific tasks from microsurgery and micro-assembly will be used for testing the ideas and evaluating the results obtained.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">99770</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">99770</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n505" target="n507">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Scale-Invariant Skill Augmentation for Cooperative Human-Machine Micromanipulation Systems</data>
      <data key="e_abstract">The proposed work addresses development of novel methods for enhancing the ability of humans to perform complex and delicate manipulation tasks at a microscopic level. The proposed approach is based on a combination of off-line programming and on-line adaptation of task-specific human-computer manipulation idioms. The work builds upon this group&apos;s experience in the development of steady-hand robotic augmentation. Specific tasks from microsurgery and micro-assembly will be used for testing the ideas and evaluating the results obtained.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">99770</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">99770</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n506" target="n507">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Scale-Invariant Skill Augmentation for Cooperative Human-Machine Micromanipulation Systems</data>
      <data key="e_abstract">The proposed work addresses development of novel methods for enhancing the ability of humans to perform complex and delicate manipulation tasks at a microscopic level. The proposed approach is based on a combination of off-line programming and on-line adaptation of task-specific human-computer manipulation idioms. The work builds upon this group&apos;s experience in the development of steady-hand robotic augmentation. Specific tasks from microsurgery and micro-assembly will be used for testing the ideas and evaluating the results obtained.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">99770</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">99770</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n509" target="n510">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Self-Assembly of Magnetic Nanostructures and Related Enabling Technologies</data>
      <data key="e_abstract">This multi-disciplinary proposal will unite researchers from physics, chemistry, and biology to work synergistically on a coherent project that involves one basic concept, the development and synthesis of novel materials from self-assembled magnetic nanostructures whose configuration and/or functions can be tuned and controlled by external fields. We will demonstrate that, by understanding physical mechanism of self-assembly and field-controlled phenomena and by bringing together two frontiers of the new century---the nanoscale science and the field of soft matter, we will be able to develop functional materials that enable new technologies ranging from memory devices, drug delivery agents, field-controllable nanomachines, magnetically actuatable polymers, and many other liquid, gel, or solid devices.&lt;br/&gt;We propose to perform experimental and theoretical research to study the conditions for self-assembly phenomena in surfactant-stabilized magnetic fluids. Experimental observations using scattering techniques such as neutron and light scattering, imaging (Atomic Force Microscopy (AFM)/Magnetic Force Microscopy (MFM)), and thermodynamic measurements (e.g. heat capacity) will be supported and evaluated by using computer simulations. We propose to use realistic quaternion molecular dynamics simulations in viscous media to study the dynamics of isomer transitions under varying conditions and to present a correct interpretation of the experimental results. Based on the configuration of self-assembled structures in zero field and its response to external fields, novel structures can be synthesized that have important applications.&lt;br/&gt;The proposed research will provide the basis for the design of new, smart materials and externally controlled systems that can respond to an external environment through the unique combination of theory, computer simulations, and experimental investigations. This work will have an important impact in applied physics, chemistry, material sciences, biology and medicine, and device industries. Our approach aims to facilitate the education of tomorrow&apos;s scientists in nanoscience and technology through the involvement of students in every aspect of proposed research.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">103587</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103587</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n509" target="n511">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Self-Assembly of Magnetic Nanostructures and Related Enabling Technologies</data>
      <data key="e_abstract">This multi-disciplinary proposal will unite researchers from physics, chemistry, and biology to work synergistically on a coherent project that involves one basic concept, the development and synthesis of novel materials from self-assembled magnetic nanostructures whose configuration and/or functions can be tuned and controlled by external fields. We will demonstrate that, by understanding physical mechanism of self-assembly and field-controlled phenomena and by bringing together two frontiers of the new century---the nanoscale science and the field of soft matter, we will be able to develop functional materials that enable new technologies ranging from memory devices, drug delivery agents, field-controllable nanomachines, magnetically actuatable polymers, and many other liquid, gel, or solid devices.&lt;br/&gt;We propose to perform experimental and theoretical research to study the conditions for self-assembly phenomena in surfactant-stabilized magnetic fluids. Experimental observations using scattering techniques such as neutron and light scattering, imaging (Atomic Force Microscopy (AFM)/Magnetic Force Microscopy (MFM)), and thermodynamic measurements (e.g. heat capacity) will be supported and evaluated by using computer simulations. We propose to use realistic quaternion molecular dynamics simulations in viscous media to study the dynamics of isomer transitions under varying conditions and to present a correct interpretation of the experimental results. Based on the configuration of self-assembled structures in zero field and its response to external fields, novel structures can be synthesized that have important applications.&lt;br/&gt;The proposed research will provide the basis for the design of new, smart materials and externally controlled systems that can respond to an external environment through the unique combination of theory, computer simulations, and experimental investigations. This work will have an important impact in applied physics, chemistry, material sciences, biology and medicine, and device industries. Our approach aims to facilitate the education of tomorrow&apos;s scientists in nanoscience and technology through the involvement of students in every aspect of proposed research.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">103587</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103587</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n510" target="n511">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Self-Assembly of Magnetic Nanostructures and Related Enabling Technologies</data>
      <data key="e_abstract">This multi-disciplinary proposal will unite researchers from physics, chemistry, and biology to work synergistically on a coherent project that involves one basic concept, the development and synthesis of novel materials from self-assembled magnetic nanostructures whose configuration and/or functions can be tuned and controlled by external fields. We will demonstrate that, by understanding physical mechanism of self-assembly and field-controlled phenomena and by bringing together two frontiers of the new century---the nanoscale science and the field of soft matter, we will be able to develop functional materials that enable new technologies ranging from memory devices, drug delivery agents, field-controllable nanomachines, magnetically actuatable polymers, and many other liquid, gel, or solid devices.&lt;br/&gt;We propose to perform experimental and theoretical research to study the conditions for self-assembly phenomena in surfactant-stabilized magnetic fluids. Experimental observations using scattering techniques such as neutron and light scattering, imaging (Atomic Force Microscopy (AFM)/Magnetic Force Microscopy (MFM)), and thermodynamic measurements (e.g. heat capacity) will be supported and evaluated by using computer simulations. We propose to use realistic quaternion molecular dynamics simulations in viscous media to study the dynamics of isomer transitions under varying conditions and to present a correct interpretation of the experimental results. Based on the configuration of self-assembled structures in zero field and its response to external fields, novel structures can be synthesized that have important applications.&lt;br/&gt;The proposed research will provide the basis for the design of new, smart materials and externally controlled systems that can respond to an external environment through the unique combination of theory, computer simulations, and experimental investigations. This work will have an important impact in applied physics, chemistry, material sciences, biology and medicine, and device industries. Our approach aims to facilitate the education of tomorrow&apos;s scientists in nanoscience and technology through the involvement of students in every aspect of proposed research.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">103587</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103587</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n512" target="n513">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">NIRT: Nanoscale Molecular Opto-Electronics</data>
      <data key="e_abstract">0103175&lt;br/&gt;Lindsay&lt;br/&gt;&lt;br/&gt;This proposal was submitted in response to the solicitation &quot;Nanoscale Science and Engineering&quot; (NSF 00-119). It brings together experts in organic photochemistry, experimental and theoretical physicists and engineers in a University (Arizona State)-Industry (Motorola)collaboration aimed at developing nanoscale molecular optoelectronic devices based on paradigms from photosynthetic electron transfer.&lt;br/&gt;&lt;br/&gt;The first phase of the project builds on the PIs&apos; current work on the basic building blocks of molecular electronic devices. They will use bifunctionalized molecules covalently bonded at one end to a gold-coated conducting AFM tip and at the other end to a gold substrate. In this way they will measure the electrical properties of simple molecular insulators (n-alkanes) and molecular wires (carotenoids) at the single molecule level. These measurements will be compared to first-principles simulations, with the goal of developing both theory and experiment until they have a reasonably accurate description of transport in both the molecules and their contacts to the metal electrodes. Armed with this information, they will insert the molecules into nano-scale gaps in gold electrodes on oxidized silicon wafers.These devices will be made at Motorola. Final gap fabrication uses active-feedback control of electrochemical deposition, a technique developed by a consultant to the group.The goals of this step are to (1)make two electrode devices on wafers that can be characterized in terms of the single-molecule AFM data and (2) explore the current-voltage characteristics of these devices with greater flexibility than possible in the AFM (for example, making temperature-dependent measurements).&lt;br/&gt;&lt;br/&gt;The second phase will focus on the electronic properties of optically excited molecules, and molecules in high-energy charge-separated states. The use of light to provide additional inputs to molecular-scale electronic devices offers several advantages, and may lead the way to the design and fabrication of technologically useful constructs. They will use much the same approach as outlined above, but with the addition of controlled optical excitation of chromophores. They will start with the carotenoids, as the simplest system, but will go on to study molecules containing porphyrins and fullerenes that are built to make transitions into long-lived triplet states, or into long-live charge-separated states. These systems present theoretical as well as experimental challenges, and they propose computational approaches for dealing with nuclear-relaxation on excitation or charging and for dealing with highly correlated molecular electronic states.&lt;br/&gt;&lt;br/&gt;They propose a single-molecule opto-electronic switch as a candidate device on which to focus the long-range efforts of the group. The device might prove useful as an optoelectronic molecular-scale building block. But developing the science and technology that would go into building the device and understanding it are the main motivation for this project.&lt;br/&gt;&lt;br/&gt;This group provides an extraordinary opportunity for training minority students in multidisciplinary approaches to nanoscience in both academic and industrial research environments.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">103175</data>
      <data key="e_expirationDate">2006-07-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103175</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n512" target="n514">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">NIRT: Nanoscale Molecular Opto-Electronics</data>
      <data key="e_abstract">0103175&lt;br/&gt;Lindsay&lt;br/&gt;&lt;br/&gt;This proposal was submitted in response to the solicitation &quot;Nanoscale Science and Engineering&quot; (NSF 00-119). It brings together experts in organic photochemistry, experimental and theoretical physicists and engineers in a University (Arizona State)-Industry (Motorola)collaboration aimed at developing nanoscale molecular optoelectronic devices based on paradigms from photosynthetic electron transfer.&lt;br/&gt;&lt;br/&gt;The first phase of the project builds on the PIs&apos; current work on the basic building blocks of molecular electronic devices. They will use bifunctionalized molecules covalently bonded at one end to a gold-coated conducting AFM tip and at the other end to a gold substrate. In this way they will measure the electrical properties of simple molecular insulators (n-alkanes) and molecular wires (carotenoids) at the single molecule level. These measurements will be compared to first-principles simulations, with the goal of developing both theory and experiment until they have a reasonably accurate description of transport in both the molecules and their contacts to the metal electrodes. Armed with this information, they will insert the molecules into nano-scale gaps in gold electrodes on oxidized silicon wafers.These devices will be made at Motorola. Final gap fabrication uses active-feedback control of electrochemical deposition, a technique developed by a consultant to the group.The goals of this step are to (1)make two electrode devices on wafers that can be characterized in terms of the single-molecule AFM data and (2) explore the current-voltage characteristics of these devices with greater flexibility than possible in the AFM (for example, making temperature-dependent measurements).&lt;br/&gt;&lt;br/&gt;The second phase will focus on the electronic properties of optically excited molecules, and molecules in high-energy charge-separated states. The use of light to provide additional inputs to molecular-scale electronic devices offers several advantages, and may lead the way to the design and fabrication of technologically useful constructs. They will use much the same approach as outlined above, but with the addition of controlled optical excitation of chromophores. They will start with the carotenoids, as the simplest system, but will go on to study molecules containing porphyrins and fullerenes that are built to make transitions into long-lived triplet states, or into long-live charge-separated states. These systems present theoretical as well as experimental challenges, and they propose computational approaches for dealing with nuclear-relaxation on excitation or charging and for dealing with highly correlated molecular electronic states.&lt;br/&gt;&lt;br/&gt;They propose a single-molecule opto-electronic switch as a candidate device on which to focus the long-range efforts of the group. The device might prove useful as an optoelectronic molecular-scale building block. But developing the science and technology that would go into building the device and understanding it are the main motivation for this project.&lt;br/&gt;&lt;br/&gt;This group provides an extraordinary opportunity for training minority students in multidisciplinary approaches to nanoscience in both academic and industrial research environments.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">103175</data>
      <data key="e_expirationDate">2006-07-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103175</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n512" target="n515">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">NIRT: Nanoscale Molecular Opto-Electronics</data>
      <data key="e_abstract">0103175&lt;br/&gt;Lindsay&lt;br/&gt;&lt;br/&gt;This proposal was submitted in response to the solicitation &quot;Nanoscale Science and Engineering&quot; (NSF 00-119). It brings together experts in organic photochemistry, experimental and theoretical physicists and engineers in a University (Arizona State)-Industry (Motorola)collaboration aimed at developing nanoscale molecular optoelectronic devices based on paradigms from photosynthetic electron transfer.&lt;br/&gt;&lt;br/&gt;The first phase of the project builds on the PIs&apos; current work on the basic building blocks of molecular electronic devices. They will use bifunctionalized molecules covalently bonded at one end to a gold-coated conducting AFM tip and at the other end to a gold substrate. In this way they will measure the electrical properties of simple molecular insulators (n-alkanes) and molecular wires (carotenoids) at the single molecule level. These measurements will be compared to first-principles simulations, with the goal of developing both theory and experiment until they have a reasonably accurate description of transport in both the molecules and their contacts to the metal electrodes. Armed with this information, they will insert the molecules into nano-scale gaps in gold electrodes on oxidized silicon wafers.These devices will be made at Motorola. Final gap fabrication uses active-feedback control of electrochemical deposition, a technique developed by a consultant to the group.The goals of this step are to (1)make two electrode devices on wafers that can be characterized in terms of the single-molecule AFM data and (2) explore the current-voltage characteristics of these devices with greater flexibility than possible in the AFM (for example, making temperature-dependent measurements).&lt;br/&gt;&lt;br/&gt;The second phase will focus on the electronic properties of optically excited molecules, and molecules in high-energy charge-separated states. The use of light to provide additional inputs to molecular-scale electronic devices offers several advantages, and may lead the way to the design and fabrication of technologically useful constructs. They will use much the same approach as outlined above, but with the addition of controlled optical excitation of chromophores. They will start with the carotenoids, as the simplest system, but will go on to study molecules containing porphyrins and fullerenes that are built to make transitions into long-lived triplet states, or into long-live charge-separated states. These systems present theoretical as well as experimental challenges, and they propose computational approaches for dealing with nuclear-relaxation on excitation or charging and for dealing with highly correlated molecular electronic states.&lt;br/&gt;&lt;br/&gt;They propose a single-molecule opto-electronic switch as a candidate device on which to focus the long-range efforts of the group. The device might prove useful as an optoelectronic molecular-scale building block. But developing the science and technology that would go into building the device and understanding it are the main motivation for this project.&lt;br/&gt;&lt;br/&gt;This group provides an extraordinary opportunity for training minority students in multidisciplinary approaches to nanoscience in both academic and industrial research environments.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">103175</data>
      <data key="e_expirationDate">2006-07-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103175</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n512" target="n516">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">NIRT: Nanoscale Molecular Opto-Electronics</data>
      <data key="e_abstract">0103175&lt;br/&gt;Lindsay&lt;br/&gt;&lt;br/&gt;This proposal was submitted in response to the solicitation &quot;Nanoscale Science and Engineering&quot; (NSF 00-119). It brings together experts in organic photochemistry, experimental and theoretical physicists and engineers in a University (Arizona State)-Industry (Motorola)collaboration aimed at developing nanoscale molecular optoelectronic devices based on paradigms from photosynthetic electron transfer.&lt;br/&gt;&lt;br/&gt;The first phase of the project builds on the PIs&apos; current work on the basic building blocks of molecular electronic devices. They will use bifunctionalized molecules covalently bonded at one end to a gold-coated conducting AFM tip and at the other end to a gold substrate. In this way they will measure the electrical properties of simple molecular insulators (n-alkanes) and molecular wires (carotenoids) at the single molecule level. These measurements will be compared to first-principles simulations, with the goal of developing both theory and experiment until they have a reasonably accurate description of transport in both the molecules and their contacts to the metal electrodes. Armed with this information, they will insert the molecules into nano-scale gaps in gold electrodes on oxidized silicon wafers.These devices will be made at Motorola. Final gap fabrication uses active-feedback control of electrochemical deposition, a technique developed by a consultant to the group.The goals of this step are to (1)make two electrode devices on wafers that can be characterized in terms of the single-molecule AFM data and (2) explore the current-voltage characteristics of these devices with greater flexibility than possible in the AFM (for example, making temperature-dependent measurements).&lt;br/&gt;&lt;br/&gt;The second phase will focus on the electronic properties of optically excited molecules, and molecules in high-energy charge-separated states. The use of light to provide additional inputs to molecular-scale electronic devices offers several advantages, and may lead the way to the design and fabrication of technologically useful constructs. They will use much the same approach as outlined above, but with the addition of controlled optical excitation of chromophores. They will start with the carotenoids, as the simplest system, but will go on to study molecules containing porphyrins and fullerenes that are built to make transitions into long-lived triplet states, or into long-live charge-separated states. These systems present theoretical as well as experimental challenges, and they propose computational approaches for dealing with nuclear-relaxation on excitation or charging and for dealing with highly correlated molecular electronic states.&lt;br/&gt;&lt;br/&gt;They propose a single-molecule opto-electronic switch as a candidate device on which to focus the long-range efforts of the group. The device might prove useful as an optoelectronic molecular-scale building block. But developing the science and technology that would go into building the device and understanding it are the main motivation for this project.&lt;br/&gt;&lt;br/&gt;This group provides an extraordinary opportunity for training minority students in multidisciplinary approaches to nanoscience in both academic and industrial research environments.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">103175</data>
      <data key="e_expirationDate">2006-07-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103175</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n513" target="n514">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">NIRT: Nanoscale Molecular Opto-Electronics</data>
      <data key="e_abstract">0103175&lt;br/&gt;Lindsay&lt;br/&gt;&lt;br/&gt;This proposal was submitted in response to the solicitation &quot;Nanoscale Science and Engineering&quot; (NSF 00-119). It brings together experts in organic photochemistry, experimental and theoretical physicists and engineers in a University (Arizona State)-Industry (Motorola)collaboration aimed at developing nanoscale molecular optoelectronic devices based on paradigms from photosynthetic electron transfer.&lt;br/&gt;&lt;br/&gt;The first phase of the project builds on the PIs&apos; current work on the basic building blocks of molecular electronic devices. They will use bifunctionalized molecules covalently bonded at one end to a gold-coated conducting AFM tip and at the other end to a gold substrate. In this way they will measure the electrical properties of simple molecular insulators (n-alkanes) and molecular wires (carotenoids) at the single molecule level. These measurements will be compared to first-principles simulations, with the goal of developing both theory and experiment until they have a reasonably accurate description of transport in both the molecules and their contacts to the metal electrodes. Armed with this information, they will insert the molecules into nano-scale gaps in gold electrodes on oxidized silicon wafers.These devices will be made at Motorola. Final gap fabrication uses active-feedback control of electrochemical deposition, a technique developed by a consultant to the group.The goals of this step are to (1)make two electrode devices on wafers that can be characterized in terms of the single-molecule AFM data and (2) explore the current-voltage characteristics of these devices with greater flexibility than possible in the AFM (for example, making temperature-dependent measurements).&lt;br/&gt;&lt;br/&gt;The second phase will focus on the electronic properties of optically excited molecules, and molecules in high-energy charge-separated states. The use of light to provide additional inputs to molecular-scale electronic devices offers several advantages, and may lead the way to the design and fabrication of technologically useful constructs. They will use much the same approach as outlined above, but with the addition of controlled optical excitation of chromophores. They will start with the carotenoids, as the simplest system, but will go on to study molecules containing porphyrins and fullerenes that are built to make transitions into long-lived triplet states, or into long-live charge-separated states. These systems present theoretical as well as experimental challenges, and they propose computational approaches for dealing with nuclear-relaxation on excitation or charging and for dealing with highly correlated molecular electronic states.&lt;br/&gt;&lt;br/&gt;They propose a single-molecule opto-electronic switch as a candidate device on which to focus the long-range efforts of the group. The device might prove useful as an optoelectronic molecular-scale building block. But developing the science and technology that would go into building the device and understanding it are the main motivation for this project.&lt;br/&gt;&lt;br/&gt;This group provides an extraordinary opportunity for training minority students in multidisciplinary approaches to nanoscience in both academic and industrial research environments.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">103175</data>
      <data key="e_expirationDate">2006-07-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103175</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n513" target="n515">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">NIRT: Nanoscale Molecular Opto-Electronics</data>
      <data key="e_abstract">0103175&lt;br/&gt;Lindsay&lt;br/&gt;&lt;br/&gt;This proposal was submitted in response to the solicitation &quot;Nanoscale Science and Engineering&quot; (NSF 00-119). It brings together experts in organic photochemistry, experimental and theoretical physicists and engineers in a University (Arizona State)-Industry (Motorola)collaboration aimed at developing nanoscale molecular optoelectronic devices based on paradigms from photosynthetic electron transfer.&lt;br/&gt;&lt;br/&gt;The first phase of the project builds on the PIs&apos; current work on the basic building blocks of molecular electronic devices. They will use bifunctionalized molecules covalently bonded at one end to a gold-coated conducting AFM tip and at the other end to a gold substrate. In this way they will measure the electrical properties of simple molecular insulators (n-alkanes) and molecular wires (carotenoids) at the single molecule level. These measurements will be compared to first-principles simulations, with the goal of developing both theory and experiment until they have a reasonably accurate description of transport in both the molecules and their contacts to the metal electrodes. Armed with this information, they will insert the molecules into nano-scale gaps in gold electrodes on oxidized silicon wafers.These devices will be made at Motorola. Final gap fabrication uses active-feedback control of electrochemical deposition, a technique developed by a consultant to the group.The goals of this step are to (1)make two electrode devices on wafers that can be characterized in terms of the single-molecule AFM data and (2) explore the current-voltage characteristics of these devices with greater flexibility than possible in the AFM (for example, making temperature-dependent measurements).&lt;br/&gt;&lt;br/&gt;The second phase will focus on the electronic properties of optically excited molecules, and molecules in high-energy charge-separated states. The use of light to provide additional inputs to molecular-scale electronic devices offers several advantages, and may lead the way to the design and fabrication of technologically useful constructs. They will use much the same approach as outlined above, but with the addition of controlled optical excitation of chromophores. They will start with the carotenoids, as the simplest system, but will go on to study molecules containing porphyrins and fullerenes that are built to make transitions into long-lived triplet states, or into long-live charge-separated states. These systems present theoretical as well as experimental challenges, and they propose computational approaches for dealing with nuclear-relaxation on excitation or charging and for dealing with highly correlated molecular electronic states.&lt;br/&gt;&lt;br/&gt;They propose a single-molecule opto-electronic switch as a candidate device on which to focus the long-range efforts of the group. The device might prove useful as an optoelectronic molecular-scale building block. But developing the science and technology that would go into building the device and understanding it are the main motivation for this project.&lt;br/&gt;&lt;br/&gt;This group provides an extraordinary opportunity for training minority students in multidisciplinary approaches to nanoscience in both academic and industrial research environments.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">103175</data>
      <data key="e_expirationDate">2006-07-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103175</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n513" target="n516">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">NIRT: Nanoscale Molecular Opto-Electronics</data>
      <data key="e_abstract">0103175&lt;br/&gt;Lindsay&lt;br/&gt;&lt;br/&gt;This proposal was submitted in response to the solicitation &quot;Nanoscale Science and Engineering&quot; (NSF 00-119). It brings together experts in organic photochemistry, experimental and theoretical physicists and engineers in a University (Arizona State)-Industry (Motorola)collaboration aimed at developing nanoscale molecular optoelectronic devices based on paradigms from photosynthetic electron transfer.&lt;br/&gt;&lt;br/&gt;The first phase of the project builds on the PIs&apos; current work on the basic building blocks of molecular electronic devices. They will use bifunctionalized molecules covalently bonded at one end to a gold-coated conducting AFM tip and at the other end to a gold substrate. In this way they will measure the electrical properties of simple molecular insulators (n-alkanes) and molecular wires (carotenoids) at the single molecule level. These measurements will be compared to first-principles simulations, with the goal of developing both theory and experiment until they have a reasonably accurate description of transport in both the molecules and their contacts to the metal electrodes. Armed with this information, they will insert the molecules into nano-scale gaps in gold electrodes on oxidized silicon wafers.These devices will be made at Motorola. Final gap fabrication uses active-feedback control of electrochemical deposition, a technique developed by a consultant to the group.The goals of this step are to (1)make two electrode devices on wafers that can be characterized in terms of the single-molecule AFM data and (2) explore the current-voltage characteristics of these devices with greater flexibility than possible in the AFM (for example, making temperature-dependent measurements).&lt;br/&gt;&lt;br/&gt;The second phase will focus on the electronic properties of optically excited molecules, and molecules in high-energy charge-separated states. The use of light to provide additional inputs to molecular-scale electronic devices offers several advantages, and may lead the way to the design and fabrication of technologically useful constructs. They will use much the same approach as outlined above, but with the addition of controlled optical excitation of chromophores. They will start with the carotenoids, as the simplest system, but will go on to study molecules containing porphyrins and fullerenes that are built to make transitions into long-lived triplet states, or into long-live charge-separated states. These systems present theoretical as well as experimental challenges, and they propose computational approaches for dealing with nuclear-relaxation on excitation or charging and for dealing with highly correlated molecular electronic states.&lt;br/&gt;&lt;br/&gt;They propose a single-molecule opto-electronic switch as a candidate device on which to focus the long-range efforts of the group. The device might prove useful as an optoelectronic molecular-scale building block. But developing the science and technology that would go into building the device and understanding it are the main motivation for this project.&lt;br/&gt;&lt;br/&gt;This group provides an extraordinary opportunity for training minority students in multidisciplinary approaches to nanoscience in both academic and industrial research environments.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">103175</data>
      <data key="e_expirationDate">2006-07-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103175</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n514" target="n515">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">NIRT: Nanoscale Molecular Opto-Electronics</data>
      <data key="e_abstract">0103175&lt;br/&gt;Lindsay&lt;br/&gt;&lt;br/&gt;This proposal was submitted in response to the solicitation &quot;Nanoscale Science and Engineering&quot; (NSF 00-119). It brings together experts in organic photochemistry, experimental and theoretical physicists and engineers in a University (Arizona State)-Industry (Motorola)collaboration aimed at developing nanoscale molecular optoelectronic devices based on paradigms from photosynthetic electron transfer.&lt;br/&gt;&lt;br/&gt;The first phase of the project builds on the PIs&apos; current work on the basic building blocks of molecular electronic devices. They will use bifunctionalized molecules covalently bonded at one end to a gold-coated conducting AFM tip and at the other end to a gold substrate. In this way they will measure the electrical properties of simple molecular insulators (n-alkanes) and molecular wires (carotenoids) at the single molecule level. These measurements will be compared to first-principles simulations, with the goal of developing both theory and experiment until they have a reasonably accurate description of transport in both the molecules and their contacts to the metal electrodes. Armed with this information, they will insert the molecules into nano-scale gaps in gold electrodes on oxidized silicon wafers.These devices will be made at Motorola. Final gap fabrication uses active-feedback control of electrochemical deposition, a technique developed by a consultant to the group.The goals of this step are to (1)make two electrode devices on wafers that can be characterized in terms of the single-molecule AFM data and (2) explore the current-voltage characteristics of these devices with greater flexibility than possible in the AFM (for example, making temperature-dependent measurements).&lt;br/&gt;&lt;br/&gt;The second phase will focus on the electronic properties of optically excited molecules, and molecules in high-energy charge-separated states. The use of light to provide additional inputs to molecular-scale electronic devices offers several advantages, and may lead the way to the design and fabrication of technologically useful constructs. They will use much the same approach as outlined above, but with the addition of controlled optical excitation of chromophores. They will start with the carotenoids, as the simplest system, but will go on to study molecules containing porphyrins and fullerenes that are built to make transitions into long-lived triplet states, or into long-live charge-separated states. These systems present theoretical as well as experimental challenges, and they propose computational approaches for dealing with nuclear-relaxation on excitation or charging and for dealing with highly correlated molecular electronic states.&lt;br/&gt;&lt;br/&gt;They propose a single-molecule opto-electronic switch as a candidate device on which to focus the long-range efforts of the group. The device might prove useful as an optoelectronic molecular-scale building block. But developing the science and technology that would go into building the device and understanding it are the main motivation for this project.&lt;br/&gt;&lt;br/&gt;This group provides an extraordinary opportunity for training minority students in multidisciplinary approaches to nanoscience in both academic and industrial research environments.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">103175</data>
      <data key="e_expirationDate">2006-07-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103175</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n514" target="n516">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">NIRT: Nanoscale Molecular Opto-Electronics</data>
      <data key="e_abstract">0103175&lt;br/&gt;Lindsay&lt;br/&gt;&lt;br/&gt;This proposal was submitted in response to the solicitation &quot;Nanoscale Science and Engineering&quot; (NSF 00-119). It brings together experts in organic photochemistry, experimental and theoretical physicists and engineers in a University (Arizona State)-Industry (Motorola)collaboration aimed at developing nanoscale molecular optoelectronic devices based on paradigms from photosynthetic electron transfer.&lt;br/&gt;&lt;br/&gt;The first phase of the project builds on the PIs&apos; current work on the basic building blocks of molecular electronic devices. They will use bifunctionalized molecules covalently bonded at one end to a gold-coated conducting AFM tip and at the other end to a gold substrate. In this way they will measure the electrical properties of simple molecular insulators (n-alkanes) and molecular wires (carotenoids) at the single molecule level. These measurements will be compared to first-principles simulations, with the goal of developing both theory and experiment until they have a reasonably accurate description of transport in both the molecules and their contacts to the metal electrodes. Armed with this information, they will insert the molecules into nano-scale gaps in gold electrodes on oxidized silicon wafers.These devices will be made at Motorola. Final gap fabrication uses active-feedback control of electrochemical deposition, a technique developed by a consultant to the group.The goals of this step are to (1)make two electrode devices on wafers that can be characterized in terms of the single-molecule AFM data and (2) explore the current-voltage characteristics of these devices with greater flexibility than possible in the AFM (for example, making temperature-dependent measurements).&lt;br/&gt;&lt;br/&gt;The second phase will focus on the electronic properties of optically excited molecules, and molecules in high-energy charge-separated states. The use of light to provide additional inputs to molecular-scale electronic devices offers several advantages, and may lead the way to the design and fabrication of technologically useful constructs. They will use much the same approach as outlined above, but with the addition of controlled optical excitation of chromophores. They will start with the carotenoids, as the simplest system, but will go on to study molecules containing porphyrins and fullerenes that are built to make transitions into long-lived triplet states, or into long-live charge-separated states. These systems present theoretical as well as experimental challenges, and they propose computational approaches for dealing with nuclear-relaxation on excitation or charging and for dealing with highly correlated molecular electronic states.&lt;br/&gt;&lt;br/&gt;They propose a single-molecule opto-electronic switch as a candidate device on which to focus the long-range efforts of the group. The device might prove useful as an optoelectronic molecular-scale building block. But developing the science and technology that would go into building the device and understanding it are the main motivation for this project.&lt;br/&gt;&lt;br/&gt;This group provides an extraordinary opportunity for training minority students in multidisciplinary approaches to nanoscience in both academic and industrial research environments.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">103175</data>
      <data key="e_expirationDate">2006-07-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103175</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n515" target="n516">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">NIRT: Nanoscale Molecular Opto-Electronics</data>
      <data key="e_abstract">0103175&lt;br/&gt;Lindsay&lt;br/&gt;&lt;br/&gt;This proposal was submitted in response to the solicitation &quot;Nanoscale Science and Engineering&quot; (NSF 00-119). It brings together experts in organic photochemistry, experimental and theoretical physicists and engineers in a University (Arizona State)-Industry (Motorola)collaboration aimed at developing nanoscale molecular optoelectronic devices based on paradigms from photosynthetic electron transfer.&lt;br/&gt;&lt;br/&gt;The first phase of the project builds on the PIs&apos; current work on the basic building blocks of molecular electronic devices. They will use bifunctionalized molecules covalently bonded at one end to a gold-coated conducting AFM tip and at the other end to a gold substrate. In this way they will measure the electrical properties of simple molecular insulators (n-alkanes) and molecular wires (carotenoids) at the single molecule level. These measurements will be compared to first-principles simulations, with the goal of developing both theory and experiment until they have a reasonably accurate description of transport in both the molecules and their contacts to the metal electrodes. Armed with this information, they will insert the molecules into nano-scale gaps in gold electrodes on oxidized silicon wafers.These devices will be made at Motorola. Final gap fabrication uses active-feedback control of electrochemical deposition, a technique developed by a consultant to the group.The goals of this step are to (1)make two electrode devices on wafers that can be characterized in terms of the single-molecule AFM data and (2) explore the current-voltage characteristics of these devices with greater flexibility than possible in the AFM (for example, making temperature-dependent measurements).&lt;br/&gt;&lt;br/&gt;The second phase will focus on the electronic properties of optically excited molecules, and molecules in high-energy charge-separated states. The use of light to provide additional inputs to molecular-scale electronic devices offers several advantages, and may lead the way to the design and fabrication of technologically useful constructs. They will use much the same approach as outlined above, but with the addition of controlled optical excitation of chromophores. They will start with the carotenoids, as the simplest system, but will go on to study molecules containing porphyrins and fullerenes that are built to make transitions into long-lived triplet states, or into long-live charge-separated states. These systems present theoretical as well as experimental challenges, and they propose computational approaches for dealing with nuclear-relaxation on excitation or charging and for dealing with highly correlated molecular electronic states.&lt;br/&gt;&lt;br/&gt;They propose a single-molecule opto-electronic switch as a candidate device on which to focus the long-range efforts of the group. The device might prove useful as an optoelectronic molecular-scale building block. But developing the science and technology that would go into building the device and understanding it are the main motivation for this project.&lt;br/&gt;&lt;br/&gt;This group provides an extraordinary opportunity for training minority students in multidisciplinary approaches to nanoscience in both academic and industrial research environments.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">103175</data>
      <data key="e_expirationDate">2006-07-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103175</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n446" target="n517">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">ITR/SY+IM(CISE): Self-Calibrating, Scalable Displays for Digital Library Collections</data>
      <data key="e_abstract">The primary goal of this project is to design, implement and test a new display system for digital libraries.&lt;br/&gt;It is motivated by the lack of display capabilities in most systems today; we need more screen resolution and screen size. This project will combine commodity video projectors and computing equipment with software to provide automatic calibration and to provide better interfaces to visualization techniques. This will be of value in virtual reality environments and scientific data analysis environments as well as in digital library implementations. The project will be done in partnership with a library at the University of Kentucky and a computing facility at the University of Puerto Rico.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">113325</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">113325</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n520" target="n521">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NIRT: The Evolution and Self-Assembly of Quantum Dots</data>
      <data key="e_abstract">This proposal was submitted in response to the solicitation &quot;Nanoscale Science and Engineering&quot; (NSF-00-119). The resulting grant is co-funded by the Divisions of Materials Research and Mathematical Sciences. &lt;br/&gt;&lt;br/&gt;Nanostructures such as quantum dots and quantum wires can be employed to yield devices with novel electronic properties. One particularly promising route to quantum dot formation is via the spontaneous self-assembly process that occurs during heteroepitaxy. To control the quantum dot formation and self-assembly process to the extent that these novel electronic devices become a reality, this team of researchers will investigate the mechanisms of dot formation, and develop predictive models of the dot formation and self-organization process.&lt;br/&gt;&lt;br/&gt;Achieving this goal requires an integrated interdisciplinary effort that can address the quantum dot formation and self-assembly process from the atomistic to the continuum or nanometer length scales. The work of this group will thus involve, for example, first-principle calculations of surface energies and surface diffusion coefficients, calculations of the evolution of quantum dot shape and composition during deposition, and the nonlinear dynamics of pattern formation or self-assembly of quantum dots. Each effort will feed into the other, as the information at the smaller length scales will be employed in the larger scale calculations, enabling us to bridge length scales that range from the fraction of a nanometer to thousands of nanometers. The ultimate goal of the project is to develop an understanding of the important materials issues governing formation and self-assembly of quantum dots, and to develop predictive models that enable the first-principles design of quantum dot nanostructures. The models can then be used to design and create specific quantum dot structures, providing for possible breakthroughs in the fabrication of new quantum dot electronic devices.&lt;br/&gt;%%%&lt;br/&gt;***</data>
      <data key="e_pgm">1765</data>
      <data key="e_label">102794</data>
      <data key="e_expirationDate">2006-11-30</data>
      <data key="e_div">0307</data>
      <data key="e_awardID">102794</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n520" target="n522">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NIRT: The Evolution and Self-Assembly of Quantum Dots</data>
      <data key="e_abstract">This proposal was submitted in response to the solicitation &quot;Nanoscale Science and Engineering&quot; (NSF-00-119). The resulting grant is co-funded by the Divisions of Materials Research and Mathematical Sciences. &lt;br/&gt;&lt;br/&gt;Nanostructures such as quantum dots and quantum wires can be employed to yield devices with novel electronic properties. One particularly promising route to quantum dot formation is via the spontaneous self-assembly process that occurs during heteroepitaxy. To control the quantum dot formation and self-assembly process to the extent that these novel electronic devices become a reality, this team of researchers will investigate the mechanisms of dot formation, and develop predictive models of the dot formation and self-organization process.&lt;br/&gt;&lt;br/&gt;Achieving this goal requires an integrated interdisciplinary effort that can address the quantum dot formation and self-assembly process from the atomistic to the continuum or nanometer length scales. The work of this group will thus involve, for example, first-principle calculations of surface energies and surface diffusion coefficients, calculations of the evolution of quantum dot shape and composition during deposition, and the nonlinear dynamics of pattern formation or self-assembly of quantum dots. Each effort will feed into the other, as the information at the smaller length scales will be employed in the larger scale calculations, enabling us to bridge length scales that range from the fraction of a nanometer to thousands of nanometers. The ultimate goal of the project is to develop an understanding of the important materials issues governing formation and self-assembly of quantum dots, and to develop predictive models that enable the first-principles design of quantum dot nanostructures. The models can then be used to design and create specific quantum dot structures, providing for possible breakthroughs in the fabrication of new quantum dot electronic devices.&lt;br/&gt;%%%&lt;br/&gt;***</data>
      <data key="e_pgm">1765</data>
      <data key="e_label">102794</data>
      <data key="e_expirationDate">2006-11-30</data>
      <data key="e_div">0307</data>
      <data key="e_awardID">102794</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n520" target="n523">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NIRT: The Evolution and Self-Assembly of Quantum Dots</data>
      <data key="e_abstract">This proposal was submitted in response to the solicitation &quot;Nanoscale Science and Engineering&quot; (NSF-00-119). The resulting grant is co-funded by the Divisions of Materials Research and Mathematical Sciences. &lt;br/&gt;&lt;br/&gt;Nanostructures such as quantum dots and quantum wires can be employed to yield devices with novel electronic properties. One particularly promising route to quantum dot formation is via the spontaneous self-assembly process that occurs during heteroepitaxy. To control the quantum dot formation and self-assembly process to the extent that these novel electronic devices become a reality, this team of researchers will investigate the mechanisms of dot formation, and develop predictive models of the dot formation and self-organization process.&lt;br/&gt;&lt;br/&gt;Achieving this goal requires an integrated interdisciplinary effort that can address the quantum dot formation and self-assembly process from the atomistic to the continuum or nanometer length scales. The work of this group will thus involve, for example, first-principle calculations of surface energies and surface diffusion coefficients, calculations of the evolution of quantum dot shape and composition during deposition, and the nonlinear dynamics of pattern formation or self-assembly of quantum dots. Each effort will feed into the other, as the information at the smaller length scales will be employed in the larger scale calculations, enabling us to bridge length scales that range from the fraction of a nanometer to thousands of nanometers. The ultimate goal of the project is to develop an understanding of the important materials issues governing formation and self-assembly of quantum dots, and to develop predictive models that enable the first-principles design of quantum dot nanostructures. The models can then be used to design and create specific quantum dot structures, providing for possible breakthroughs in the fabrication of new quantum dot electronic devices.&lt;br/&gt;%%%&lt;br/&gt;***</data>
      <data key="e_pgm">1765</data>
      <data key="e_label">102794</data>
      <data key="e_expirationDate">2006-11-30</data>
      <data key="e_div">0307</data>
      <data key="e_awardID">102794</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n520" target="n524">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NIRT: The Evolution and Self-Assembly of Quantum Dots</data>
      <data key="e_abstract">This proposal was submitted in response to the solicitation &quot;Nanoscale Science and Engineering&quot; (NSF-00-119). The resulting grant is co-funded by the Divisions of Materials Research and Mathematical Sciences. &lt;br/&gt;&lt;br/&gt;Nanostructures such as quantum dots and quantum wires can be employed to yield devices with novel electronic properties. One particularly promising route to quantum dot formation is via the spontaneous self-assembly process that occurs during heteroepitaxy. To control the quantum dot formation and self-assembly process to the extent that these novel electronic devices become a reality, this team of researchers will investigate the mechanisms of dot formation, and develop predictive models of the dot formation and self-organization process.&lt;br/&gt;&lt;br/&gt;Achieving this goal requires an integrated interdisciplinary effort that can address the quantum dot formation and self-assembly process from the atomistic to the continuum or nanometer length scales. The work of this group will thus involve, for example, first-principle calculations of surface energies and surface diffusion coefficients, calculations of the evolution of quantum dot shape and composition during deposition, and the nonlinear dynamics of pattern formation or self-assembly of quantum dots. Each effort will feed into the other, as the information at the smaller length scales will be employed in the larger scale calculations, enabling us to bridge length scales that range from the fraction of a nanometer to thousands of nanometers. The ultimate goal of the project is to develop an understanding of the important materials issues governing formation and self-assembly of quantum dots, and to develop predictive models that enable the first-principles design of quantum dot nanostructures. The models can then be used to design and create specific quantum dot structures, providing for possible breakthroughs in the fabrication of new quantum dot electronic devices.&lt;br/&gt;%%%&lt;br/&gt;***</data>
      <data key="e_pgm">1765</data>
      <data key="e_label">102794</data>
      <data key="e_expirationDate">2006-11-30</data>
      <data key="e_div">0307</data>
      <data key="e_awardID">102794</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n521" target="n522">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NIRT: The Evolution and Self-Assembly of Quantum Dots</data>
      <data key="e_abstract">This proposal was submitted in response to the solicitation &quot;Nanoscale Science and Engineering&quot; (NSF-00-119). The resulting grant is co-funded by the Divisions of Materials Research and Mathematical Sciences. &lt;br/&gt;&lt;br/&gt;Nanostructures such as quantum dots and quantum wires can be employed to yield devices with novel electronic properties. One particularly promising route to quantum dot formation is via the spontaneous self-assembly process that occurs during heteroepitaxy. To control the quantum dot formation and self-assembly process to the extent that these novel electronic devices become a reality, this team of researchers will investigate the mechanisms of dot formation, and develop predictive models of the dot formation and self-organization process.&lt;br/&gt;&lt;br/&gt;Achieving this goal requires an integrated interdisciplinary effort that can address the quantum dot formation and self-assembly process from the atomistic to the continuum or nanometer length scales. The work of this group will thus involve, for example, first-principle calculations of surface energies and surface diffusion coefficients, calculations of the evolution of quantum dot shape and composition during deposition, and the nonlinear dynamics of pattern formation or self-assembly of quantum dots. Each effort will feed into the other, as the information at the smaller length scales will be employed in the larger scale calculations, enabling us to bridge length scales that range from the fraction of a nanometer to thousands of nanometers. The ultimate goal of the project is to develop an understanding of the important materials issues governing formation and self-assembly of quantum dots, and to develop predictive models that enable the first-principles design of quantum dot nanostructures. The models can then be used to design and create specific quantum dot structures, providing for possible breakthroughs in the fabrication of new quantum dot electronic devices.&lt;br/&gt;%%%&lt;br/&gt;***</data>
      <data key="e_pgm">1765</data>
      <data key="e_label">102794</data>
      <data key="e_expirationDate">2006-11-30</data>
      <data key="e_div">0307</data>
      <data key="e_awardID">102794</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n521" target="n523">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NIRT: The Evolution and Self-Assembly of Quantum Dots</data>
      <data key="e_abstract">This proposal was submitted in response to the solicitation &quot;Nanoscale Science and Engineering&quot; (NSF-00-119). The resulting grant is co-funded by the Divisions of Materials Research and Mathematical Sciences. &lt;br/&gt;&lt;br/&gt;Nanostructures such as quantum dots and quantum wires can be employed to yield devices with novel electronic properties. One particularly promising route to quantum dot formation is via the spontaneous self-assembly process that occurs during heteroepitaxy. To control the quantum dot formation and self-assembly process to the extent that these novel electronic devices become a reality, this team of researchers will investigate the mechanisms of dot formation, and develop predictive models of the dot formation and self-organization process.&lt;br/&gt;&lt;br/&gt;Achieving this goal requires an integrated interdisciplinary effort that can address the quantum dot formation and self-assembly process from the atomistic to the continuum or nanometer length scales. The work of this group will thus involve, for example, first-principle calculations of surface energies and surface diffusion coefficients, calculations of the evolution of quantum dot shape and composition during deposition, and the nonlinear dynamics of pattern formation or self-assembly of quantum dots. Each effort will feed into the other, as the information at the smaller length scales will be employed in the larger scale calculations, enabling us to bridge length scales that range from the fraction of a nanometer to thousands of nanometers. The ultimate goal of the project is to develop an understanding of the important materials issues governing formation and self-assembly of quantum dots, and to develop predictive models that enable the first-principles design of quantum dot nanostructures. The models can then be used to design and create specific quantum dot structures, providing for possible breakthroughs in the fabrication of new quantum dot electronic devices.&lt;br/&gt;%%%&lt;br/&gt;***</data>
      <data key="e_pgm">1765</data>
      <data key="e_label">102794</data>
      <data key="e_expirationDate">2006-11-30</data>
      <data key="e_div">0307</data>
      <data key="e_awardID">102794</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n521" target="n524">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NIRT: The Evolution and Self-Assembly of Quantum Dots</data>
      <data key="e_abstract">This proposal was submitted in response to the solicitation &quot;Nanoscale Science and Engineering&quot; (NSF-00-119). The resulting grant is co-funded by the Divisions of Materials Research and Mathematical Sciences. &lt;br/&gt;&lt;br/&gt;Nanostructures such as quantum dots and quantum wires can be employed to yield devices with novel electronic properties. One particularly promising route to quantum dot formation is via the spontaneous self-assembly process that occurs during heteroepitaxy. To control the quantum dot formation and self-assembly process to the extent that these novel electronic devices become a reality, this team of researchers will investigate the mechanisms of dot formation, and develop predictive models of the dot formation and self-organization process.&lt;br/&gt;&lt;br/&gt;Achieving this goal requires an integrated interdisciplinary effort that can address the quantum dot formation and self-assembly process from the atomistic to the continuum or nanometer length scales. The work of this group will thus involve, for example, first-principle calculations of surface energies and surface diffusion coefficients, calculations of the evolution of quantum dot shape and composition during deposition, and the nonlinear dynamics of pattern formation or self-assembly of quantum dots. Each effort will feed into the other, as the information at the smaller length scales will be employed in the larger scale calculations, enabling us to bridge length scales that range from the fraction of a nanometer to thousands of nanometers. The ultimate goal of the project is to develop an understanding of the important materials issues governing formation and self-assembly of quantum dots, and to develop predictive models that enable the first-principles design of quantum dot nanostructures. The models can then be used to design and create specific quantum dot structures, providing for possible breakthroughs in the fabrication of new quantum dot electronic devices.&lt;br/&gt;%%%&lt;br/&gt;***</data>
      <data key="e_pgm">1765</data>
      <data key="e_label">102794</data>
      <data key="e_expirationDate">2006-11-30</data>
      <data key="e_div">0307</data>
      <data key="e_awardID">102794</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n522" target="n523">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NIRT: The Evolution and Self-Assembly of Quantum Dots</data>
      <data key="e_abstract">This proposal was submitted in response to the solicitation &quot;Nanoscale Science and Engineering&quot; (NSF-00-119). The resulting grant is co-funded by the Divisions of Materials Research and Mathematical Sciences. &lt;br/&gt;&lt;br/&gt;Nanostructures such as quantum dots and quantum wires can be employed to yield devices with novel electronic properties. One particularly promising route to quantum dot formation is via the spontaneous self-assembly process that occurs during heteroepitaxy. To control the quantum dot formation and self-assembly process to the extent that these novel electronic devices become a reality, this team of researchers will investigate the mechanisms of dot formation, and develop predictive models of the dot formation and self-organization process.&lt;br/&gt;&lt;br/&gt;Achieving this goal requires an integrated interdisciplinary effort that can address the quantum dot formation and self-assembly process from the atomistic to the continuum or nanometer length scales. The work of this group will thus involve, for example, first-principle calculations of surface energies and surface diffusion coefficients, calculations of the evolution of quantum dot shape and composition during deposition, and the nonlinear dynamics of pattern formation or self-assembly of quantum dots. Each effort will feed into the other, as the information at the smaller length scales will be employed in the larger scale calculations, enabling us to bridge length scales that range from the fraction of a nanometer to thousands of nanometers. The ultimate goal of the project is to develop an understanding of the important materials issues governing formation and self-assembly of quantum dots, and to develop predictive models that enable the first-principles design of quantum dot nanostructures. The models can then be used to design and create specific quantum dot structures, providing for possible breakthroughs in the fabrication of new quantum dot electronic devices.&lt;br/&gt;%%%&lt;br/&gt;***</data>
      <data key="e_pgm">1765</data>
      <data key="e_label">102794</data>
      <data key="e_expirationDate">2006-11-30</data>
      <data key="e_div">0307</data>
      <data key="e_awardID">102794</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n522" target="n524">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NIRT: The Evolution and Self-Assembly of Quantum Dots</data>
      <data key="e_abstract">This proposal was submitted in response to the solicitation &quot;Nanoscale Science and Engineering&quot; (NSF-00-119). The resulting grant is co-funded by the Divisions of Materials Research and Mathematical Sciences. &lt;br/&gt;&lt;br/&gt;Nanostructures such as quantum dots and quantum wires can be employed to yield devices with novel electronic properties. One particularly promising route to quantum dot formation is via the spontaneous self-assembly process that occurs during heteroepitaxy. To control the quantum dot formation and self-assembly process to the extent that these novel electronic devices become a reality, this team of researchers will investigate the mechanisms of dot formation, and develop predictive models of the dot formation and self-organization process.&lt;br/&gt;&lt;br/&gt;Achieving this goal requires an integrated interdisciplinary effort that can address the quantum dot formation and self-assembly process from the atomistic to the continuum or nanometer length scales. The work of this group will thus involve, for example, first-principle calculations of surface energies and surface diffusion coefficients, calculations of the evolution of quantum dot shape and composition during deposition, and the nonlinear dynamics of pattern formation or self-assembly of quantum dots. Each effort will feed into the other, as the information at the smaller length scales will be employed in the larger scale calculations, enabling us to bridge length scales that range from the fraction of a nanometer to thousands of nanometers. The ultimate goal of the project is to develop an understanding of the important materials issues governing formation and self-assembly of quantum dots, and to develop predictive models that enable the first-principles design of quantum dot nanostructures. The models can then be used to design and create specific quantum dot structures, providing for possible breakthroughs in the fabrication of new quantum dot electronic devices.&lt;br/&gt;%%%&lt;br/&gt;***</data>
      <data key="e_pgm">1765</data>
      <data key="e_label">102794</data>
      <data key="e_expirationDate">2006-11-30</data>
      <data key="e_div">0307</data>
      <data key="e_awardID">102794</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n523" target="n524">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NIRT: The Evolution and Self-Assembly of Quantum Dots</data>
      <data key="e_abstract">This proposal was submitted in response to the solicitation &quot;Nanoscale Science and Engineering&quot; (NSF-00-119). The resulting grant is co-funded by the Divisions of Materials Research and Mathematical Sciences. &lt;br/&gt;&lt;br/&gt;Nanostructures such as quantum dots and quantum wires can be employed to yield devices with novel electronic properties. One particularly promising route to quantum dot formation is via the spontaneous self-assembly process that occurs during heteroepitaxy. To control the quantum dot formation and self-assembly process to the extent that these novel electronic devices become a reality, this team of researchers will investigate the mechanisms of dot formation, and develop predictive models of the dot formation and self-organization process.&lt;br/&gt;&lt;br/&gt;Achieving this goal requires an integrated interdisciplinary effort that can address the quantum dot formation and self-assembly process from the atomistic to the continuum or nanometer length scales. The work of this group will thus involve, for example, first-principle calculations of surface energies and surface diffusion coefficients, calculations of the evolution of quantum dot shape and composition during deposition, and the nonlinear dynamics of pattern formation or self-assembly of quantum dots. Each effort will feed into the other, as the information at the smaller length scales will be employed in the larger scale calculations, enabling us to bridge length scales that range from the fraction of a nanometer to thousands of nanometers. The ultimate goal of the project is to develop an understanding of the important materials issues governing formation and self-assembly of quantum dots, and to develop predictive models that enable the first-principles design of quantum dot nanostructures. The models can then be used to design and create specific quantum dot structures, providing for possible breakthroughs in the fabrication of new quantum dot electronic devices.&lt;br/&gt;%%%&lt;br/&gt;***</data>
      <data key="e_pgm">1765</data>
      <data key="e_label">102794</data>
      <data key="e_expirationDate">2006-11-30</data>
      <data key="e_div">0307</data>
      <data key="e_awardID">102794</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n526" target="n527">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">EI: Enhancing the CS Undergraduate Curriculum to include data mining and information retrieval</data>
      <data key="e_abstract">EIA- 0119469&lt;br/&gt;Ophir Frieder&lt;br/&gt;Illinois Institute of Technology&lt;br/&gt;&lt;br/&gt;Educational Innovation: Enhancing the CS Undergraduate Curriculum to Include Data Mining and Information Retrieval&lt;br/&gt;&lt;br/&gt;This project involves the development (and introduction into the undergraduate computer science curriculum) of two new courses: Data Mining and Information Retrieval. In this sequence, students build systems that implement key data mining and information retrieval algorithms and learn how to apply these algorithms to solve real-world problems. At the end of this sequence, students understand and use the fundamental algorithms and existing state-of-the-art in web search engines, intranets, data mining, and customer relationship management. Two significant group projects provide students with experience as participants of a software development team. These projects enable larger implementation achievements that further the understanding of algorithms, implementation trade-offs and software project management. In addition, such a group project is a critical experience that future employers and graduate schools look for.</data>
      <data key="e_pgm">T471</data>
      <data key="e_label">119469</data>
      <data key="e_expirationDate">2008-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">119469</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n526" target="n528">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">EI: Enhancing the CS Undergraduate Curriculum to include data mining and information retrieval</data>
      <data key="e_abstract">EIA- 0119469&lt;br/&gt;Ophir Frieder&lt;br/&gt;Illinois Institute of Technology&lt;br/&gt;&lt;br/&gt;Educational Innovation: Enhancing the CS Undergraduate Curriculum to Include Data Mining and Information Retrieval&lt;br/&gt;&lt;br/&gt;This project involves the development (and introduction into the undergraduate computer science curriculum) of two new courses: Data Mining and Information Retrieval. In this sequence, students build systems that implement key data mining and information retrieval algorithms and learn how to apply these algorithms to solve real-world problems. At the end of this sequence, students understand and use the fundamental algorithms and existing state-of-the-art in web search engines, intranets, data mining, and customer relationship management. Two significant group projects provide students with experience as participants of a software development team. These projects enable larger implementation achievements that further the understanding of algorithms, implementation trade-offs and software project management. In addition, such a group project is a critical experience that future employers and graduate schools look for.</data>
      <data key="e_pgm">T471</data>
      <data key="e_label">119469</data>
      <data key="e_expirationDate">2008-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">119469</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n526" target="n529">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">EI: Enhancing the CS Undergraduate Curriculum to include data mining and information retrieval</data>
      <data key="e_abstract">EIA- 0119469&lt;br/&gt;Ophir Frieder&lt;br/&gt;Illinois Institute of Technology&lt;br/&gt;&lt;br/&gt;Educational Innovation: Enhancing the CS Undergraduate Curriculum to Include Data Mining and Information Retrieval&lt;br/&gt;&lt;br/&gt;This project involves the development (and introduction into the undergraduate computer science curriculum) of two new courses: Data Mining and Information Retrieval. In this sequence, students build systems that implement key data mining and information retrieval algorithms and learn how to apply these algorithms to solve real-world problems. At the end of this sequence, students understand and use the fundamental algorithms and existing state-of-the-art in web search engines, intranets, data mining, and customer relationship management. Two significant group projects provide students with experience as participants of a software development team. These projects enable larger implementation achievements that further the understanding of algorithms, implementation trade-offs and software project management. In addition, such a group project is a critical experience that future employers and graduate schools look for.</data>
      <data key="e_pgm">T471</data>
      <data key="e_label">119469</data>
      <data key="e_expirationDate">2008-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">119469</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n527" target="n528">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">EI: Enhancing the CS Undergraduate Curriculum to include data mining and information retrieval</data>
      <data key="e_abstract">EIA- 0119469&lt;br/&gt;Ophir Frieder&lt;br/&gt;Illinois Institute of Technology&lt;br/&gt;&lt;br/&gt;Educational Innovation: Enhancing the CS Undergraduate Curriculum to Include Data Mining and Information Retrieval&lt;br/&gt;&lt;br/&gt;This project involves the development (and introduction into the undergraduate computer science curriculum) of two new courses: Data Mining and Information Retrieval. In this sequence, students build systems that implement key data mining and information retrieval algorithms and learn how to apply these algorithms to solve real-world problems. At the end of this sequence, students understand and use the fundamental algorithms and existing state-of-the-art in web search engines, intranets, data mining, and customer relationship management. Two significant group projects provide students with experience as participants of a software development team. These projects enable larger implementation achievements that further the understanding of algorithms, implementation trade-offs and software project management. In addition, such a group project is a critical experience that future employers and graduate schools look for.</data>
      <data key="e_pgm">T471</data>
      <data key="e_label">119469</data>
      <data key="e_expirationDate">2008-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">119469</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n527" target="n529">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">EI: Enhancing the CS Undergraduate Curriculum to include data mining and information retrieval</data>
      <data key="e_abstract">EIA- 0119469&lt;br/&gt;Ophir Frieder&lt;br/&gt;Illinois Institute of Technology&lt;br/&gt;&lt;br/&gt;Educational Innovation: Enhancing the CS Undergraduate Curriculum to Include Data Mining and Information Retrieval&lt;br/&gt;&lt;br/&gt;This project involves the development (and introduction into the undergraduate computer science curriculum) of two new courses: Data Mining and Information Retrieval. In this sequence, students build systems that implement key data mining and information retrieval algorithms and learn how to apply these algorithms to solve real-world problems. At the end of this sequence, students understand and use the fundamental algorithms and existing state-of-the-art in web search engines, intranets, data mining, and customer relationship management. Two significant group projects provide students with experience as participants of a software development team. These projects enable larger implementation achievements that further the understanding of algorithms, implementation trade-offs and software project management. In addition, such a group project is a critical experience that future employers and graduate schools look for.</data>
      <data key="e_pgm">T471</data>
      <data key="e_label">119469</data>
      <data key="e_expirationDate">2008-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">119469</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n528" target="n529">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">EI: Enhancing the CS Undergraduate Curriculum to include data mining and information retrieval</data>
      <data key="e_abstract">EIA- 0119469&lt;br/&gt;Ophir Frieder&lt;br/&gt;Illinois Institute of Technology&lt;br/&gt;&lt;br/&gt;Educational Innovation: Enhancing the CS Undergraduate Curriculum to Include Data Mining and Information Retrieval&lt;br/&gt;&lt;br/&gt;This project involves the development (and introduction into the undergraduate computer science curriculum) of two new courses: Data Mining and Information Retrieval. In this sequence, students build systems that implement key data mining and information retrieval algorithms and learn how to apply these algorithms to solve real-world problems. At the end of this sequence, students understand and use the fundamental algorithms and existing state-of-the-art in web search engines, intranets, data mining, and customer relationship management. Two significant group projects provide students with experience as participants of a software development team. These projects enable larger implementation achievements that further the understanding of algorithms, implementation trade-offs and software project management. In addition, such a group project is a critical experience that future employers and graduate schools look for.</data>
      <data key="e_pgm">T471</data>
      <data key="e_label">119469</data>
      <data key="e_expirationDate">2008-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">119469</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n533" target="n534">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">DIGITAL GOVERNMENT: A Geospatial Decision Support System for Drought</data>
      <data key="e_abstract">EIA-0091530&lt;br/&gt;Stephen E. Reichenbach&lt;br/&gt;University of Nebraska-Lincoln&lt;br/&gt;&lt;br/&gt;Digital Government: A Geospatial Decision Support System for Drought Risk Management&lt;br/&gt;&lt;br/&gt;This project will develop and integrate new information technologies for improved government services in the U.S. Department of Agriculture (USDA) Risk Management Agency (RMA). The mission of the RMA is to strengthen the safety net for agricultural procedures (farmers) through sound risk management programs and education. Rick management in agriculture is critically important to producers, their communities, and the nation&apos;s economy, but it involves many complex problems.&lt;br/&gt;&lt;br/&gt;Our objective is to improve, through research and advanced development, the RMA&apos;s risk assessment services in three important ways:&lt;br/&gt;&lt;br/&gt;To speed risk assessment with automation&lt;br/&gt;To enhance risk assessment with increased spatial and temporal resolution and additional input variables To extend risk assessment to forecasts and economic analyses</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91530</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91530</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n533" target="n535">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">DIGITAL GOVERNMENT: A Geospatial Decision Support System for Drought</data>
      <data key="e_abstract">EIA-0091530&lt;br/&gt;Stephen E. Reichenbach&lt;br/&gt;University of Nebraska-Lincoln&lt;br/&gt;&lt;br/&gt;Digital Government: A Geospatial Decision Support System for Drought Risk Management&lt;br/&gt;&lt;br/&gt;This project will develop and integrate new information technologies for improved government services in the U.S. Department of Agriculture (USDA) Risk Management Agency (RMA). The mission of the RMA is to strengthen the safety net for agricultural procedures (farmers) through sound risk management programs and education. Rick management in agriculture is critically important to producers, their communities, and the nation&apos;s economy, but it involves many complex problems.&lt;br/&gt;&lt;br/&gt;Our objective is to improve, through research and advanced development, the RMA&apos;s risk assessment services in three important ways:&lt;br/&gt;&lt;br/&gt;To speed risk assessment with automation&lt;br/&gt;To enhance risk assessment with increased spatial and temporal resolution and additional input variables To extend risk assessment to forecasts and economic analyses</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91530</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91530</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n533" target="n536">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">DIGITAL GOVERNMENT: A Geospatial Decision Support System for Drought</data>
      <data key="e_abstract">EIA-0091530&lt;br/&gt;Stephen E. Reichenbach&lt;br/&gt;University of Nebraska-Lincoln&lt;br/&gt;&lt;br/&gt;Digital Government: A Geospatial Decision Support System for Drought Risk Management&lt;br/&gt;&lt;br/&gt;This project will develop and integrate new information technologies for improved government services in the U.S. Department of Agriculture (USDA) Risk Management Agency (RMA). The mission of the RMA is to strengthen the safety net for agricultural procedures (farmers) through sound risk management programs and education. Rick management in agriculture is critically important to producers, their communities, and the nation&apos;s economy, but it involves many complex problems.&lt;br/&gt;&lt;br/&gt;Our objective is to improve, through research and advanced development, the RMA&apos;s risk assessment services in three important ways:&lt;br/&gt;&lt;br/&gt;To speed risk assessment with automation&lt;br/&gt;To enhance risk assessment with increased spatial and temporal resolution and additional input variables To extend risk assessment to forecasts and economic analyses</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91530</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91530</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n533" target="n537">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">DIGITAL GOVERNMENT: A Geospatial Decision Support System for Drought</data>
      <data key="e_abstract">EIA-0091530&lt;br/&gt;Stephen E. Reichenbach&lt;br/&gt;University of Nebraska-Lincoln&lt;br/&gt;&lt;br/&gt;Digital Government: A Geospatial Decision Support System for Drought Risk Management&lt;br/&gt;&lt;br/&gt;This project will develop and integrate new information technologies for improved government services in the U.S. Department of Agriculture (USDA) Risk Management Agency (RMA). The mission of the RMA is to strengthen the safety net for agricultural procedures (farmers) through sound risk management programs and education. Rick management in agriculture is critically important to producers, their communities, and the nation&apos;s economy, but it involves many complex problems.&lt;br/&gt;&lt;br/&gt;Our objective is to improve, through research and advanced development, the RMA&apos;s risk assessment services in three important ways:&lt;br/&gt;&lt;br/&gt;To speed risk assessment with automation&lt;br/&gt;To enhance risk assessment with increased spatial and temporal resolution and additional input variables To extend risk assessment to forecasts and economic analyses</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91530</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91530</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n534" target="n535">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">DIGITAL GOVERNMENT: A Geospatial Decision Support System for Drought</data>
      <data key="e_abstract">EIA-0091530&lt;br/&gt;Stephen E. Reichenbach&lt;br/&gt;University of Nebraska-Lincoln&lt;br/&gt;&lt;br/&gt;Digital Government: A Geospatial Decision Support System for Drought Risk Management&lt;br/&gt;&lt;br/&gt;This project will develop and integrate new information technologies for improved government services in the U.S. Department of Agriculture (USDA) Risk Management Agency (RMA). The mission of the RMA is to strengthen the safety net for agricultural procedures (farmers) through sound risk management programs and education. Rick management in agriculture is critically important to producers, their communities, and the nation&apos;s economy, but it involves many complex problems.&lt;br/&gt;&lt;br/&gt;Our objective is to improve, through research and advanced development, the RMA&apos;s risk assessment services in three important ways:&lt;br/&gt;&lt;br/&gt;To speed risk assessment with automation&lt;br/&gt;To enhance risk assessment with increased spatial and temporal resolution and additional input variables To extend risk assessment to forecasts and economic analyses</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91530</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91530</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n534" target="n536">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">DIGITAL GOVERNMENT: A Geospatial Decision Support System for Drought</data>
      <data key="e_abstract">EIA-0091530&lt;br/&gt;Stephen E. Reichenbach&lt;br/&gt;University of Nebraska-Lincoln&lt;br/&gt;&lt;br/&gt;Digital Government: A Geospatial Decision Support System for Drought Risk Management&lt;br/&gt;&lt;br/&gt;This project will develop and integrate new information technologies for improved government services in the U.S. Department of Agriculture (USDA) Risk Management Agency (RMA). The mission of the RMA is to strengthen the safety net for agricultural procedures (farmers) through sound risk management programs and education. Rick management in agriculture is critically important to producers, their communities, and the nation&apos;s economy, but it involves many complex problems.&lt;br/&gt;&lt;br/&gt;Our objective is to improve, through research and advanced development, the RMA&apos;s risk assessment services in three important ways:&lt;br/&gt;&lt;br/&gt;To speed risk assessment with automation&lt;br/&gt;To enhance risk assessment with increased spatial and temporal resolution and additional input variables To extend risk assessment to forecasts and economic analyses</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91530</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91530</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n534" target="n537">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">DIGITAL GOVERNMENT: A Geospatial Decision Support System for Drought</data>
      <data key="e_abstract">EIA-0091530&lt;br/&gt;Stephen E. Reichenbach&lt;br/&gt;University of Nebraska-Lincoln&lt;br/&gt;&lt;br/&gt;Digital Government: A Geospatial Decision Support System for Drought Risk Management&lt;br/&gt;&lt;br/&gt;This project will develop and integrate new information technologies for improved government services in the U.S. Department of Agriculture (USDA) Risk Management Agency (RMA). The mission of the RMA is to strengthen the safety net for agricultural procedures (farmers) through sound risk management programs and education. Rick management in agriculture is critically important to producers, their communities, and the nation&apos;s economy, but it involves many complex problems.&lt;br/&gt;&lt;br/&gt;Our objective is to improve, through research and advanced development, the RMA&apos;s risk assessment services in three important ways:&lt;br/&gt;&lt;br/&gt;To speed risk assessment with automation&lt;br/&gt;To enhance risk assessment with increased spatial and temporal resolution and additional input variables To extend risk assessment to forecasts and economic analyses</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91530</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91530</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n535" target="n536">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">DIGITAL GOVERNMENT: A Geospatial Decision Support System for Drought</data>
      <data key="e_abstract">EIA-0091530&lt;br/&gt;Stephen E. Reichenbach&lt;br/&gt;University of Nebraska-Lincoln&lt;br/&gt;&lt;br/&gt;Digital Government: A Geospatial Decision Support System for Drought Risk Management&lt;br/&gt;&lt;br/&gt;This project will develop and integrate new information technologies for improved government services in the U.S. Department of Agriculture (USDA) Risk Management Agency (RMA). The mission of the RMA is to strengthen the safety net for agricultural procedures (farmers) through sound risk management programs and education. Rick management in agriculture is critically important to producers, their communities, and the nation&apos;s economy, but it involves many complex problems.&lt;br/&gt;&lt;br/&gt;Our objective is to improve, through research and advanced development, the RMA&apos;s risk assessment services in three important ways:&lt;br/&gt;&lt;br/&gt;To speed risk assessment with automation&lt;br/&gt;To enhance risk assessment with increased spatial and temporal resolution and additional input variables To extend risk assessment to forecasts and economic analyses</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91530</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91530</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n535" target="n537">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">DIGITAL GOVERNMENT: A Geospatial Decision Support System for Drought</data>
      <data key="e_abstract">EIA-0091530&lt;br/&gt;Stephen E. Reichenbach&lt;br/&gt;University of Nebraska-Lincoln&lt;br/&gt;&lt;br/&gt;Digital Government: A Geospatial Decision Support System for Drought Risk Management&lt;br/&gt;&lt;br/&gt;This project will develop and integrate new information technologies for improved government services in the U.S. Department of Agriculture (USDA) Risk Management Agency (RMA). The mission of the RMA is to strengthen the safety net for agricultural procedures (farmers) through sound risk management programs and education. Rick management in agriculture is critically important to producers, their communities, and the nation&apos;s economy, but it involves many complex problems.&lt;br/&gt;&lt;br/&gt;Our objective is to improve, through research and advanced development, the RMA&apos;s risk assessment services in three important ways:&lt;br/&gt;&lt;br/&gt;To speed risk assessment with automation&lt;br/&gt;To enhance risk assessment with increased spatial and temporal resolution and additional input variables To extend risk assessment to forecasts and economic analyses</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91530</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91530</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n536" target="n537">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">DIGITAL GOVERNMENT: A Geospatial Decision Support System for Drought</data>
      <data key="e_abstract">EIA-0091530&lt;br/&gt;Stephen E. Reichenbach&lt;br/&gt;University of Nebraska-Lincoln&lt;br/&gt;&lt;br/&gt;Digital Government: A Geospatial Decision Support System for Drought Risk Management&lt;br/&gt;&lt;br/&gt;This project will develop and integrate new information technologies for improved government services in the U.S. Department of Agriculture (USDA) Risk Management Agency (RMA). The mission of the RMA is to strengthen the safety net for agricultural procedures (farmers) through sound risk management programs and education. Rick management in agriculture is critically important to producers, their communities, and the nation&apos;s economy, but it involves many complex problems.&lt;br/&gt;&lt;br/&gt;Our objective is to improve, through research and advanced development, the RMA&apos;s risk assessment services in three important ways:&lt;br/&gt;&lt;br/&gt;To speed risk assessment with automation&lt;br/&gt;To enhance risk assessment with increased spatial and temporal resolution and additional input variables To extend risk assessment to forecasts and economic analyses</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91530</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91530</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n547" target="n548">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">Research on Reliability Enhancement of Mixed-Signal/Analog CMOS Integrated Circuits</data>
      <data key="e_abstract">Our research goal is to improve quality level of CMOS ICs without performing the high-cost burn-in process. High-voltage screening schemes have been successfully developed and implemented to eliminate early-life failures due to oxide defects in digital CMOS circuits. However, the success is not extended to its analog counterparts due to their working conditions and circuit topological structures. This project proposes to develop efficient yet effective high-voltage stress test process for analog circuits. The research objective is to develop the framework of an automatic stress test system for analog/mixed-signal circuits, where the system integrate three major components: stressability analysis, stressability design methodologies, and stress test process. The component of stressability design methodologies include a stress vector generation&lt;br/&gt;process and a stressability enhancement process. The stress test process generates the test programs with the defined stress conditions for the circuits under test.&lt;br/&gt; &lt;br/&gt; The success of this development will enable the analog circuit to be stressed properly using high-voltage screening to eliminate early-life failures due to oxide defects and to enhance reliability and quality of CMOS&lt;br/&gt;ICs without performing high-cost burn-in screening.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">98053</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98053</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n550" target="n551">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Workshop: Developing a Common Wireless Networking Infrastructure</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">2876</data>
      <data key="e_label">296188</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">296188</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n552" target="n553">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Collaborative Research: Field Studies of Organizational Memory and Information Reuse</data>
      <data key="e_abstract">The goal of this project is to investigate the retrieval and reuse of information in organizations. Research has shown that many organizations waste valuable information because these processes are complicated and difficult. Detailed systematic insights are necessary to understand how organizations can more effectively reuse information and engage in productive &quot;knowledge management&quot; practices. Based on their previous work, the PIs will examine key theoretical issues through micro-level distributed cognition analyses in two technical support organizations to better understand these processes and organizational practices. Such a project is necessary to move beyond the hype often associated with the discourse about knowledge management in the scholarly and management literatures.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">124878</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">124878</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n568" target="n569">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Improving System Functionality using Monitoring Processors</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;Proposal #0113418&lt;br/&gt;U of Cal Davis&lt;br/&gt;Farrens, Matthew&lt;br/&gt;&lt;br/&gt;Microprocessors now rival supercomputers in raw processing power, thanks to increases in transistor densities and architectural advances such as the exploitation of parallelism. However, the bandwidth and latency of memory systems is so limited that increasing performance in the microprocessor often leads to little overall system improvement.&lt;br/&gt;&lt;br/&gt;At the same time that this is occurring, software costs are burgeoning. This research explores using some of the increasing silicon real estate to provide extra functionality. The approach is to dedicate a portion of these new transistors to provide programmable monitoring hardware to enhance software development, make debugging more efficient, increase reliability and provide run-time security. Additional applications may be found in monitoring run-time guarantees and invariants for embedded systems.&lt;br/&gt;&lt;br/&gt;Taking a specific example, this approach can address pointer-related defects occurring in software which render systems unreliable and vulnerable to hackers. A simple, auxiliary co-processor monitors address references from a compute processor via a loose coupling (e.g. via the L1 cache coherence bus). This loose coupling reduces design complexity and avoids the need for any core CPU redesign and allows this approach to be readily added to existing designs. Furthermore, the approach is complementary to static compiler analysis techniques and the research extends conventional analysis to exploit efficient run-time monitoring capabilities.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113418</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">113418</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n568" target="n570">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Improving System Functionality using Monitoring Processors</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;Proposal #0113418&lt;br/&gt;U of Cal Davis&lt;br/&gt;Farrens, Matthew&lt;br/&gt;&lt;br/&gt;Microprocessors now rival supercomputers in raw processing power, thanks to increases in transistor densities and architectural advances such as the exploitation of parallelism. However, the bandwidth and latency of memory systems is so limited that increasing performance in the microprocessor often leads to little overall system improvement.&lt;br/&gt;&lt;br/&gt;At the same time that this is occurring, software costs are burgeoning. This research explores using some of the increasing silicon real estate to provide extra functionality. The approach is to dedicate a portion of these new transistors to provide programmable monitoring hardware to enhance software development, make debugging more efficient, increase reliability and provide run-time security. Additional applications may be found in monitoring run-time guarantees and invariants for embedded systems.&lt;br/&gt;&lt;br/&gt;Taking a specific example, this approach can address pointer-related defects occurring in software which render systems unreliable and vulnerable to hackers. A simple, auxiliary co-processor monitors address references from a compute processor via a loose coupling (e.g. via the L1 cache coherence bus). This loose coupling reduces design complexity and avoids the need for any core CPU redesign and allows this approach to be readily added to existing designs. Furthermore, the approach is complementary to static compiler analysis techniques and the research extends conventional analysis to exploit efficient run-time monitoring capabilities.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113418</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">113418</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n569" target="n570">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Improving System Functionality using Monitoring Processors</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;Proposal #0113418&lt;br/&gt;U of Cal Davis&lt;br/&gt;Farrens, Matthew&lt;br/&gt;&lt;br/&gt;Microprocessors now rival supercomputers in raw processing power, thanks to increases in transistor densities and architectural advances such as the exploitation of parallelism. However, the bandwidth and latency of memory systems is so limited that increasing performance in the microprocessor often leads to little overall system improvement.&lt;br/&gt;&lt;br/&gt;At the same time that this is occurring, software costs are burgeoning. This research explores using some of the increasing silicon real estate to provide extra functionality. The approach is to dedicate a portion of these new transistors to provide programmable monitoring hardware to enhance software development, make debugging more efficient, increase reliability and provide run-time security. Additional applications may be found in monitoring run-time guarantees and invariants for embedded systems.&lt;br/&gt;&lt;br/&gt;Taking a specific example, this approach can address pointer-related defects occurring in software which render systems unreliable and vulnerable to hackers. A simple, auxiliary co-processor monitors address references from a compute processor via a loose coupling (e.g. via the L1 cache coherence bus). This loose coupling reduces design complexity and avoids the need for any core CPU redesign and allows this approach to be readily added to existing designs. Furthermore, the approach is complementary to static compiler analysis techniques and the research extends conventional analysis to exploit efficient run-time monitoring capabilities.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113418</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">113418</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n281" target="n282">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">SPS: Multiple Description Coding with Correlating Transforms for Multiple Antenna Wireless Systems</data>
      <data key="e_abstract">Proposal #131855&lt;br/&gt;Altunbasak, Yucel&lt;br/&gt;GA Tech Res. Corp - GIT&lt;br/&gt;&lt;br/&gt;An effective way of providing error resilience for multimedia transmission in a communication system with a relatively small reduction in efficiency is multiple description coding (MDC), which assumes the existence of multiple independent channels between the transmitter and receiver, each of which can be temporarily down or can experience burst errors. With MDC several coded streams, called descriptions, are generated and transmitted over different channels. At the destination, if all of the streams are received error free, then the signal can be reconstructed at its highest level of fidelity. However, if only one or a few descriptions are received in a usable form, the receiver can still reconstruct an acceptable signal. All multiple description coding methods to date assume an on-off channel mode between the transmitter and the receiver; each link is either broken, in which case the transmitted symbols, or packets, are lost completely, or it functions properly, in which case the packets are received free of errors. This model is appropriate for Internet transmission, but it is not appropriate for wireless channels.&lt;br/&gt;&lt;br/&gt;This study replaces the parallel in independent on-off channel model with a wireless channel model, such a Rayleigh fading model. Communication is performed using multiple transmit and received antennas over the channel. With these models the signal at any of the receive antennas is the superposition of the transmitted signals from each transmit antenna independently faded. Therefore, even if the descriptions at the receiver side are completely independent, the received signal at each antenna will include some information from each description. This research involves finding the best multiple description coding strategy for these channels, the theoretical limits of such a scheme, and the efficiency</data>
      <data key="e_pgm">4720</data>
      <data key="e_label">131855</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">131855</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n578" target="n579">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">ITR/IM: The Million Book Project</data>
      <data key="e_abstract">The Million Book project is a multinational initiative to create a digital online archive of at least a&lt;br/&gt;million books and manuscripts freely available to anyone at any time. It will enhance research,&lt;br/&gt;learning and teaching by making a critical mass of scholarly information freely available to&lt;br/&gt;read online. It will support the needs of citizens for practical information and recreational reading&lt;br/&gt;as well as supporting scholarship and education. India, and possibly China, will supply the manpower&lt;br/&gt;for scanning centers while the U. S. provides equipment and software. This pilot project supplies&lt;br/&gt;the startup money for the first few scanning centers.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113878</data>
      <data key="e_expirationDate">2002-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">113878</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n304" target="n582">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">ITR/SY: Modular Interface Violation Checking Using Formally-Specified Contracts</data>
      <data key="e_abstract">Abstract&lt;br/&gt;Proposal #0113181&lt;br/&gt;Sitaraman&lt;br/&gt;Clemson University&lt;br/&gt;&lt;br/&gt;A fundamental goal of software engineering is to enable predictable and modular construction of software systems by assembling components. Any component-based approach works on the basic premise that participating components respect each other&apos;s contracts. If this premise is violated, the consequences can be both dangerous and expensive, because the problems may not surface until integration time. Even worse, a system may behave properly on test cases, though internal interface contracts are violated. Undetected failures from internal violations may be revealed ultimately only as accidents to component-based and embedded systems after deployment.&lt;br/&gt;&lt;br/&gt;This project offers a modular approach for detecting and isolating internal contractual violations. The approach allows checking at suitable levels of abstraction using formal specifications. It permits checking to be turned &quot;on&quot; or &quot;off&quot; selectively to facilitate effective regression testing, and it addresses violations of performance contracts in addition to functionality for parameterized and object-oriented components. To minimize errors in the violation checking process, the project will use and experimentally evaluate alternative combinations of automation, formal verification, model checking, and testing techniques.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">113181</data>
      <data key="e_expirationDate">2007-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">113181</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n304" target="n583">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">ITR/SY: Modular Interface Violation Checking Using Formally-Specified Contracts</data>
      <data key="e_abstract">Abstract&lt;br/&gt;Proposal #0113181&lt;br/&gt;Sitaraman&lt;br/&gt;Clemson University&lt;br/&gt;&lt;br/&gt;A fundamental goal of software engineering is to enable predictable and modular construction of software systems by assembling components. Any component-based approach works on the basic premise that participating components respect each other&apos;s contracts. If this premise is violated, the consequences can be both dangerous and expensive, because the problems may not surface until integration time. Even worse, a system may behave properly on test cases, though internal interface contracts are violated. Undetected failures from internal violations may be revealed ultimately only as accidents to component-based and embedded systems after deployment.&lt;br/&gt;&lt;br/&gt;This project offers a modular approach for detecting and isolating internal contractual violations. The approach allows checking at suitable levels of abstraction using formal specifications. It permits checking to be turned &quot;on&quot; or &quot;off&quot; selectively to facilitate effective regression testing, and it addresses violations of performance contracts in addition to functionality for parameterized and object-oriented components. To minimize errors in the violation checking process, the project will use and experimentally evaluate alternative combinations of automation, formal verification, model checking, and testing techniques.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">113181</data>
      <data key="e_expirationDate">2007-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">113181</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n582" target="n583">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">ITR/SY: Modular Interface Violation Checking Using Formally-Specified Contracts</data>
      <data key="e_abstract">Abstract&lt;br/&gt;Proposal #0113181&lt;br/&gt;Sitaraman&lt;br/&gt;Clemson University&lt;br/&gt;&lt;br/&gt;A fundamental goal of software engineering is to enable predictable and modular construction of software systems by assembling components. Any component-based approach works on the basic premise that participating components respect each other&apos;s contracts. If this premise is violated, the consequences can be both dangerous and expensive, because the problems may not surface until integration time. Even worse, a system may behave properly on test cases, though internal interface contracts are violated. Undetected failures from internal violations may be revealed ultimately only as accidents to component-based and embedded systems after deployment.&lt;br/&gt;&lt;br/&gt;This project offers a modular approach for detecting and isolating internal contractual violations. The approach allows checking at suitable levels of abstraction using formal specifications. It permits checking to be turned &quot;on&quot; or &quot;off&quot; selectively to facilitate effective regression testing, and it addresses violations of performance contracts in addition to functionality for parameterized and object-oriented components. To minimize errors in the violation checking process, the project will use and experimentally evaluate alternative combinations of automation, formal verification, model checking, and testing techniques.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">113181</data>
      <data key="e_expirationDate">2007-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">113181</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n585" target="n586">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">Spline-Wavelet Frames in Computer Graphics and other Applications</data>
      <data key="e_abstract">Proposal 0098331 &lt;br/&gt;Charles K Chui, Wenjie He, and Joachim Stoeckler&lt;br/&gt;U of Missouri, Saint Louis&lt;br/&gt;&lt;br/&gt;Abstract: Tight frames with scaling factor 2, generated by the standard affine operations of dilation and translation of two compactly supported cardinal splines, called frame generators, can be easily constructed for any spline order m (or degree m-1), by applying matrix extension techniques. However, regardless of the number of (spline) frame generators being used, at least one of them has only one vanishing moment, when the matrix extension approach is followed.&lt;br/&gt;&lt;br/&gt;In our recent work, we introduced the notion of &quot;vanishing-moment recovery&quot; Laurent polynomial factors S(z) is introduced to show that the maximum number m of vanishing moments can be achieved by both compactly supported tight frame generators, for any order m. Furthermore, the Laurent polynomials S(z) can be formulated explicitly when tight frames are relaxed to be sibling frames; that is, both frame generators, together with their corresponding duals, are compactly supported cardinal splines of the same order m. These additional vanishing moments are essential for effective use of the wavelet coefficients for feature extraction, noise removal, etc.&lt;br/&gt;&lt;br/&gt;Cardinal splines are spline functions with an equally spaced knot sequence extending from. However, in most practical applications, the intervals of interest are bounded and data samples may not be uniformly distributed. Hence, mth order splines with arbitrary knots, or at least with m stacked knots at one or both end-points of the interval of interest, are needed. This new research project is concerned with formulation of the matrix equivalent Sk of the Laurent polynomials S(z), construction of Sk and the corresponding tight (and more generally sibling) frame generators of mth order compactly supported splines with arbitrary knots and with m vanishing moments, achievement of such important features as inter-orthogonality for sibling frames, development and integration of the associated frame algorithms with the existing spline tools, investigation of spline-wavelet frame tools for adding sparsification and editing fearures for applications in computer graphics, and development of a portable software library.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">98331</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98331</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n585" target="n587">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">Spline-Wavelet Frames in Computer Graphics and other Applications</data>
      <data key="e_abstract">Proposal 0098331 &lt;br/&gt;Charles K Chui, Wenjie He, and Joachim Stoeckler&lt;br/&gt;U of Missouri, Saint Louis&lt;br/&gt;&lt;br/&gt;Abstract: Tight frames with scaling factor 2, generated by the standard affine operations of dilation and translation of two compactly supported cardinal splines, called frame generators, can be easily constructed for any spline order m (or degree m-1), by applying matrix extension techniques. However, regardless of the number of (spline) frame generators being used, at least one of them has only one vanishing moment, when the matrix extension approach is followed.&lt;br/&gt;&lt;br/&gt;In our recent work, we introduced the notion of &quot;vanishing-moment recovery&quot; Laurent polynomial factors S(z) is introduced to show that the maximum number m of vanishing moments can be achieved by both compactly supported tight frame generators, for any order m. Furthermore, the Laurent polynomials S(z) can be formulated explicitly when tight frames are relaxed to be sibling frames; that is, both frame generators, together with their corresponding duals, are compactly supported cardinal splines of the same order m. These additional vanishing moments are essential for effective use of the wavelet coefficients for feature extraction, noise removal, etc.&lt;br/&gt;&lt;br/&gt;Cardinal splines are spline functions with an equally spaced knot sequence extending from. However, in most practical applications, the intervals of interest are bounded and data samples may not be uniformly distributed. Hence, mth order splines with arbitrary knots, or at least with m stacked knots at one or both end-points of the interval of interest, are needed. This new research project is concerned with formulation of the matrix equivalent Sk of the Laurent polynomials S(z), construction of Sk and the corresponding tight (and more generally sibling) frame generators of mth order compactly supported splines with arbitrary knots and with m vanishing moments, achievement of such important features as inter-orthogonality for sibling frames, development and integration of the associated frame algorithms with the existing spline tools, investigation of spline-wavelet frame tools for adding sparsification and editing fearures for applications in computer graphics, and development of a portable software library.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">98331</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98331</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n586" target="n587">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">Spline-Wavelet Frames in Computer Graphics and other Applications</data>
      <data key="e_abstract">Proposal 0098331 &lt;br/&gt;Charles K Chui, Wenjie He, and Joachim Stoeckler&lt;br/&gt;U of Missouri, Saint Louis&lt;br/&gt;&lt;br/&gt;Abstract: Tight frames with scaling factor 2, generated by the standard affine operations of dilation and translation of two compactly supported cardinal splines, called frame generators, can be easily constructed for any spline order m (or degree m-1), by applying matrix extension techniques. However, regardless of the number of (spline) frame generators being used, at least one of them has only one vanishing moment, when the matrix extension approach is followed.&lt;br/&gt;&lt;br/&gt;In our recent work, we introduced the notion of &quot;vanishing-moment recovery&quot; Laurent polynomial factors S(z) is introduced to show that the maximum number m of vanishing moments can be achieved by both compactly supported tight frame generators, for any order m. Furthermore, the Laurent polynomials S(z) can be formulated explicitly when tight frames are relaxed to be sibling frames; that is, both frame generators, together with their corresponding duals, are compactly supported cardinal splines of the same order m. These additional vanishing moments are essential for effective use of the wavelet coefficients for feature extraction, noise removal, etc.&lt;br/&gt;&lt;br/&gt;Cardinal splines are spline functions with an equally spaced knot sequence extending from. However, in most practical applications, the intervals of interest are bounded and data samples may not be uniformly distributed. Hence, mth order splines with arbitrary knots, or at least with m stacked knots at one or both end-points of the interval of interest, are needed. This new research project is concerned with formulation of the matrix equivalent Sk of the Laurent polynomials S(z), construction of Sk and the corresponding tight (and more generally sibling) frame generators of mth order compactly supported splines with arbitrary knots and with m vanishing moments, achievement of such important features as inter-orthogonality for sibling frames, development and integration of the associated frame algorithms with the existing spline tools, investigation of spline-wavelet frame tools for adding sparsification and editing fearures for applications in computer graphics, and development of a portable software library.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">98331</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98331</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n591" target="n592">
      <data key="e_effectiveDate">2001-08-27</data>
      <data key="e_title">ITW:Mediating Careers: The Role of Labor Market Intermediaries in Facilitating the Entry, Retention, and Advancement of Women and Minorities in the Information Technology Workforce</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">2885</data>
      <data key="e_label">196555</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">196555</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n550" target="n551">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Middleware Components to Support Mobile Users in Heterogeneous Environments</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">2876</data>
      <data key="e_label">196557</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">196557</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n550" target="n595">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Middleware Components to Support Mobile Users in Heterogeneous Environments</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">2876</data>
      <data key="e_label">196557</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">196557</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n551" target="n595">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Middleware Components to Support Mobile Users in Heterogeneous Environments</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">2876</data>
      <data key="e_label">196557</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">196557</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n599" target="n600">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">INCITE: A Framework and Methodology for Edge-Based Traffic Processing and Service Inference</data>
      <data key="e_abstract">The explosive growth of computer networks, combined with rapid and unpredictable developments in ap-plications and workloads, has rendered network service inference and performance prediction increasingly de-manding and intractable tasks. Nonetheless, end-systems must have knowledge of internal network traffic con-ditions and servicing in order to validate, predict, or enhance performance capabilities required by demanding applications. Network service providers also have a great need for gauging the performance of their own sub-systems without recourse to global strategies. Without special-purpose network support, the only alternative is to indirectly infer dynamic network characteristics using edge-based network traffic processing.&lt;br/&gt; The INCITE (InterNet Control and Inference Tools at the Edge) Project focuses experts from the fields of&lt;br/&gt;networking, digital signal processing, and applied mathematics towards the goal of characterizing network ser-vice based solely on edge-based measurement at hosts and/or edge routers. This project blends recent work in multifractal traffic modeling, quality of service (QoS) measurement, and network tomography to develop a unique and innovative framework for network service inference. The INCITE Project will develop new al-gorithms and implementations using the latest in DSP-driven network processor technology, providing a vital step towards better managing and understanding of Internet performance. Our effort consists of three closely inter-related research thrusts:&lt;br/&gt;1. Multifractal Traffic and Path Modeling: We will develop new, highly accurate tools for analyzing,&lt;br/&gt;modeling, and measuring the dynamics of network connections and end-to-end paths from the edge. Our&lt;br/&gt;approach to inferring the competing cross-traffic load on a path utilizes an innovative exponentially spaced&lt;br/&gt;probing sequence that is inspired by the theory of multifractal random processes. These probing packet&lt;br/&gt;chirps balance the trade-off between overwhelming the network with probes and obtaining statistics rich&lt;br/&gt;enough for accurate estimates.&lt;br/&gt;2. Multiclass Service Inference: We will develop a framework for clients to assess a network&apos;s core QoS&lt;br/&gt;functionalities based on external and passive observations. Using the theory of traffic envelopes, maximum&lt;br/&gt;likelihood estimation, and hypothesis testing, clients will be able to assess a broad set of the network&apos;s&lt;br/&gt;multi-class control mechanisms such as the service disciplines, link sharing rules and parameters, and&lt;br/&gt;policing parameters.&lt;br/&gt;3. Unicast Network Tomography: We will develop a novel methodology for network tomography that pro-vides&lt;br/&gt;link-level performance characterization of networks of arbitrary topologies based on unicast traffic&lt;br/&gt;measurements at a the network edge. A new network modeling framework based on factor graphs will&lt;br/&gt;enable the statistical inference of link-level service parameters (e.g., losses, delays, and service strate-gies). A key strength of our envisioned methodology is that it will enable scalable, real-time tomography algorithms deployable on hosts and/or edge routers.&lt;br/&gt; The INCITE Project will develop the theoretical underpinnings of network multifractal traffic processing,&lt;br/&gt;service inference, and link-level characterization for complex, large-scale networks, and lead to computation-ally efficient and scalable service inference algorithms based only on traffic measurement at the network edge. Moreover, in collaboration with a leading provider of broadband Internet bandwidth (Enron) and an innovator in networked signal processing hardware (Texas Instruments), we will build a complete prototype implementation of the proposed algorithms, including modules for multifractal traffic and path modeling, service inference, and network tomography. This reference implementation will provide a first-of-its-kind platform for obtaining a deep understanding of large networks and enable principled designs of future network architectures, algorithms, and models.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">99148</data>
      <data key="e_expirationDate">2006-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">99148</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n599" target="n601">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">INCITE: A Framework and Methodology for Edge-Based Traffic Processing and Service Inference</data>
      <data key="e_abstract">The explosive growth of computer networks, combined with rapid and unpredictable developments in ap-plications and workloads, has rendered network service inference and performance prediction increasingly de-manding and intractable tasks. Nonetheless, end-systems must have knowledge of internal network traffic con-ditions and servicing in order to validate, predict, or enhance performance capabilities required by demanding applications. Network service providers also have a great need for gauging the performance of their own sub-systems without recourse to global strategies. Without special-purpose network support, the only alternative is to indirectly infer dynamic network characteristics using edge-based network traffic processing.&lt;br/&gt; The INCITE (InterNet Control and Inference Tools at the Edge) Project focuses experts from the fields of&lt;br/&gt;networking, digital signal processing, and applied mathematics towards the goal of characterizing network ser-vice based solely on edge-based measurement at hosts and/or edge routers. This project blends recent work in multifractal traffic modeling, quality of service (QoS) measurement, and network tomography to develop a unique and innovative framework for network service inference. The INCITE Project will develop new al-gorithms and implementations using the latest in DSP-driven network processor technology, providing a vital step towards better managing and understanding of Internet performance. Our effort consists of three closely inter-related research thrusts:&lt;br/&gt;1. Multifractal Traffic and Path Modeling: We will develop new, highly accurate tools for analyzing,&lt;br/&gt;modeling, and measuring the dynamics of network connections and end-to-end paths from the edge. Our&lt;br/&gt;approach to inferring the competing cross-traffic load on a path utilizes an innovative exponentially spaced&lt;br/&gt;probing sequence that is inspired by the theory of multifractal random processes. These probing packet&lt;br/&gt;chirps balance the trade-off between overwhelming the network with probes and obtaining statistics rich&lt;br/&gt;enough for accurate estimates.&lt;br/&gt;2. Multiclass Service Inference: We will develop a framework for clients to assess a network&apos;s core QoS&lt;br/&gt;functionalities based on external and passive observations. Using the theory of traffic envelopes, maximum&lt;br/&gt;likelihood estimation, and hypothesis testing, clients will be able to assess a broad set of the network&apos;s&lt;br/&gt;multi-class control mechanisms such as the service disciplines, link sharing rules and parameters, and&lt;br/&gt;policing parameters.&lt;br/&gt;3. Unicast Network Tomography: We will develop a novel methodology for network tomography that pro-vides&lt;br/&gt;link-level performance characterization of networks of arbitrary topologies based on unicast traffic&lt;br/&gt;measurements at a the network edge. A new network modeling framework based on factor graphs will&lt;br/&gt;enable the statistical inference of link-level service parameters (e.g., losses, delays, and service strate-gies). A key strength of our envisioned methodology is that it will enable scalable, real-time tomography algorithms deployable on hosts and/or edge routers.&lt;br/&gt; The INCITE Project will develop the theoretical underpinnings of network multifractal traffic processing,&lt;br/&gt;service inference, and link-level characterization for complex, large-scale networks, and lead to computation-ally efficient and scalable service inference algorithms based only on traffic measurement at the network edge. Moreover, in collaboration with a leading provider of broadband Internet bandwidth (Enron) and an innovator in networked signal processing hardware (Texas Instruments), we will build a complete prototype implementation of the proposed algorithms, including modules for multifractal traffic and path modeling, service inference, and network tomography. This reference implementation will provide a first-of-its-kind platform for obtaining a deep understanding of large networks and enable principled designs of future network architectures, algorithms, and models.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">99148</data>
      <data key="e_expirationDate">2006-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">99148</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n599" target="n602">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">INCITE: A Framework and Methodology for Edge-Based Traffic Processing and Service Inference</data>
      <data key="e_abstract">The explosive growth of computer networks, combined with rapid and unpredictable developments in ap-plications and workloads, has rendered network service inference and performance prediction increasingly de-manding and intractable tasks. Nonetheless, end-systems must have knowledge of internal network traffic con-ditions and servicing in order to validate, predict, or enhance performance capabilities required by demanding applications. Network service providers also have a great need for gauging the performance of their own sub-systems without recourse to global strategies. Without special-purpose network support, the only alternative is to indirectly infer dynamic network characteristics using edge-based network traffic processing.&lt;br/&gt; The INCITE (InterNet Control and Inference Tools at the Edge) Project focuses experts from the fields of&lt;br/&gt;networking, digital signal processing, and applied mathematics towards the goal of characterizing network ser-vice based solely on edge-based measurement at hosts and/or edge routers. This project blends recent work in multifractal traffic modeling, quality of service (QoS) measurement, and network tomography to develop a unique and innovative framework for network service inference. The INCITE Project will develop new al-gorithms and implementations using the latest in DSP-driven network processor technology, providing a vital step towards better managing and understanding of Internet performance. Our effort consists of three closely inter-related research thrusts:&lt;br/&gt;1. Multifractal Traffic and Path Modeling: We will develop new, highly accurate tools for analyzing,&lt;br/&gt;modeling, and measuring the dynamics of network connections and end-to-end paths from the edge. Our&lt;br/&gt;approach to inferring the competing cross-traffic load on a path utilizes an innovative exponentially spaced&lt;br/&gt;probing sequence that is inspired by the theory of multifractal random processes. These probing packet&lt;br/&gt;chirps balance the trade-off between overwhelming the network with probes and obtaining statistics rich&lt;br/&gt;enough for accurate estimates.&lt;br/&gt;2. Multiclass Service Inference: We will develop a framework for clients to assess a network&apos;s core QoS&lt;br/&gt;functionalities based on external and passive observations. Using the theory of traffic envelopes, maximum&lt;br/&gt;likelihood estimation, and hypothesis testing, clients will be able to assess a broad set of the network&apos;s&lt;br/&gt;multi-class control mechanisms such as the service disciplines, link sharing rules and parameters, and&lt;br/&gt;policing parameters.&lt;br/&gt;3. Unicast Network Tomography: We will develop a novel methodology for network tomography that pro-vides&lt;br/&gt;link-level performance characterization of networks of arbitrary topologies based on unicast traffic&lt;br/&gt;measurements at a the network edge. A new network modeling framework based on factor graphs will&lt;br/&gt;enable the statistical inference of link-level service parameters (e.g., losses, delays, and service strate-gies). A key strength of our envisioned methodology is that it will enable scalable, real-time tomography algorithms deployable on hosts and/or edge routers.&lt;br/&gt; The INCITE Project will develop the theoretical underpinnings of network multifractal traffic processing,&lt;br/&gt;service inference, and link-level characterization for complex, large-scale networks, and lead to computation-ally efficient and scalable service inference algorithms based only on traffic measurement at the network edge. Moreover, in collaboration with a leading provider of broadband Internet bandwidth (Enron) and an innovator in networked signal processing hardware (Texas Instruments), we will build a complete prototype implementation of the proposed algorithms, including modules for multifractal traffic and path modeling, service inference, and network tomography. This reference implementation will provide a first-of-its-kind platform for obtaining a deep understanding of large networks and enable principled designs of future network architectures, algorithms, and models.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">99148</data>
      <data key="e_expirationDate">2006-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">99148</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n600" target="n601">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">INCITE: A Framework and Methodology for Edge-Based Traffic Processing and Service Inference</data>
      <data key="e_abstract">The explosive growth of computer networks, combined with rapid and unpredictable developments in ap-plications and workloads, has rendered network service inference and performance prediction increasingly de-manding and intractable tasks. Nonetheless, end-systems must have knowledge of internal network traffic con-ditions and servicing in order to validate, predict, or enhance performance capabilities required by demanding applications. Network service providers also have a great need for gauging the performance of their own sub-systems without recourse to global strategies. Without special-purpose network support, the only alternative is to indirectly infer dynamic network characteristics using edge-based network traffic processing.&lt;br/&gt; The INCITE (InterNet Control and Inference Tools at the Edge) Project focuses experts from the fields of&lt;br/&gt;networking, digital signal processing, and applied mathematics towards the goal of characterizing network ser-vice based solely on edge-based measurement at hosts and/or edge routers. This project blends recent work in multifractal traffic modeling, quality of service (QoS) measurement, and network tomography to develop a unique and innovative framework for network service inference. The INCITE Project will develop new al-gorithms and implementations using the latest in DSP-driven network processor technology, providing a vital step towards better managing and understanding of Internet performance. Our effort consists of three closely inter-related research thrusts:&lt;br/&gt;1. Multifractal Traffic and Path Modeling: We will develop new, highly accurate tools for analyzing,&lt;br/&gt;modeling, and measuring the dynamics of network connections and end-to-end paths from the edge. Our&lt;br/&gt;approach to inferring the competing cross-traffic load on a path utilizes an innovative exponentially spaced&lt;br/&gt;probing sequence that is inspired by the theory of multifractal random processes. These probing packet&lt;br/&gt;chirps balance the trade-off between overwhelming the network with probes and obtaining statistics rich&lt;br/&gt;enough for accurate estimates.&lt;br/&gt;2. Multiclass Service Inference: We will develop a framework for clients to assess a network&apos;s core QoS&lt;br/&gt;functionalities based on external and passive observations. Using the theory of traffic envelopes, maximum&lt;br/&gt;likelihood estimation, and hypothesis testing, clients will be able to assess a broad set of the network&apos;s&lt;br/&gt;multi-class control mechanisms such as the service disciplines, link sharing rules and parameters, and&lt;br/&gt;policing parameters.&lt;br/&gt;3. Unicast Network Tomography: We will develop a novel methodology for network tomography that pro-vides&lt;br/&gt;link-level performance characterization of networks of arbitrary topologies based on unicast traffic&lt;br/&gt;measurements at a the network edge. A new network modeling framework based on factor graphs will&lt;br/&gt;enable the statistical inference of link-level service parameters (e.g., losses, delays, and service strate-gies). A key strength of our envisioned methodology is that it will enable scalable, real-time tomography algorithms deployable on hosts and/or edge routers.&lt;br/&gt; The INCITE Project will develop the theoretical underpinnings of network multifractal traffic processing,&lt;br/&gt;service inference, and link-level characterization for complex, large-scale networks, and lead to computation-ally efficient and scalable service inference algorithms based only on traffic measurement at the network edge. Moreover, in collaboration with a leading provider of broadband Internet bandwidth (Enron) and an innovator in networked signal processing hardware (Texas Instruments), we will build a complete prototype implementation of the proposed algorithms, including modules for multifractal traffic and path modeling, service inference, and network tomography. This reference implementation will provide a first-of-its-kind platform for obtaining a deep understanding of large networks and enable principled designs of future network architectures, algorithms, and models.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">99148</data>
      <data key="e_expirationDate">2006-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">99148</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n600" target="n602">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">INCITE: A Framework and Methodology for Edge-Based Traffic Processing and Service Inference</data>
      <data key="e_abstract">The explosive growth of computer networks, combined with rapid and unpredictable developments in ap-plications and workloads, has rendered network service inference and performance prediction increasingly de-manding and intractable tasks. Nonetheless, end-systems must have knowledge of internal network traffic con-ditions and servicing in order to validate, predict, or enhance performance capabilities required by demanding applications. Network service providers also have a great need for gauging the performance of their own sub-systems without recourse to global strategies. Without special-purpose network support, the only alternative is to indirectly infer dynamic network characteristics using edge-based network traffic processing.&lt;br/&gt; The INCITE (InterNet Control and Inference Tools at the Edge) Project focuses experts from the fields of&lt;br/&gt;networking, digital signal processing, and applied mathematics towards the goal of characterizing network ser-vice based solely on edge-based measurement at hosts and/or edge routers. This project blends recent work in multifractal traffic modeling, quality of service (QoS) measurement, and network tomography to develop a unique and innovative framework for network service inference. The INCITE Project will develop new al-gorithms and implementations using the latest in DSP-driven network processor technology, providing a vital step towards better managing and understanding of Internet performance. Our effort consists of three closely inter-related research thrusts:&lt;br/&gt;1. Multifractal Traffic and Path Modeling: We will develop new, highly accurate tools for analyzing,&lt;br/&gt;modeling, and measuring the dynamics of network connections and end-to-end paths from the edge. Our&lt;br/&gt;approach to inferring the competing cross-traffic load on a path utilizes an innovative exponentially spaced&lt;br/&gt;probing sequence that is inspired by the theory of multifractal random processes. These probing packet&lt;br/&gt;chirps balance the trade-off between overwhelming the network with probes and obtaining statistics rich&lt;br/&gt;enough for accurate estimates.&lt;br/&gt;2. Multiclass Service Inference: We will develop a framework for clients to assess a network&apos;s core QoS&lt;br/&gt;functionalities based on external and passive observations. Using the theory of traffic envelopes, maximum&lt;br/&gt;likelihood estimation, and hypothesis testing, clients will be able to assess a broad set of the network&apos;s&lt;br/&gt;multi-class control mechanisms such as the service disciplines, link sharing rules and parameters, and&lt;br/&gt;policing parameters.&lt;br/&gt;3. Unicast Network Tomography: We will develop a novel methodology for network tomography that pro-vides&lt;br/&gt;link-level performance characterization of networks of arbitrary topologies based on unicast traffic&lt;br/&gt;measurements at a the network edge. A new network modeling framework based on factor graphs will&lt;br/&gt;enable the statistical inference of link-level service parameters (e.g., losses, delays, and service strate-gies). A key strength of our envisioned methodology is that it will enable scalable, real-time tomography algorithms deployable on hosts and/or edge routers.&lt;br/&gt; The INCITE Project will develop the theoretical underpinnings of network multifractal traffic processing,&lt;br/&gt;service inference, and link-level characterization for complex, large-scale networks, and lead to computation-ally efficient and scalable service inference algorithms based only on traffic measurement at the network edge. Moreover, in collaboration with a leading provider of broadband Internet bandwidth (Enron) and an innovator in networked signal processing hardware (Texas Instruments), we will build a complete prototype implementation of the proposed algorithms, including modules for multifractal traffic and path modeling, service inference, and network tomography. This reference implementation will provide a first-of-its-kind platform for obtaining a deep understanding of large networks and enable principled designs of future network architectures, algorithms, and models.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">99148</data>
      <data key="e_expirationDate">2006-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">99148</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n601" target="n602">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">INCITE: A Framework and Methodology for Edge-Based Traffic Processing and Service Inference</data>
      <data key="e_abstract">The explosive growth of computer networks, combined with rapid and unpredictable developments in ap-plications and workloads, has rendered network service inference and performance prediction increasingly de-manding and intractable tasks. Nonetheless, end-systems must have knowledge of internal network traffic con-ditions and servicing in order to validate, predict, or enhance performance capabilities required by demanding applications. Network service providers also have a great need for gauging the performance of their own sub-systems without recourse to global strategies. Without special-purpose network support, the only alternative is to indirectly infer dynamic network characteristics using edge-based network traffic processing.&lt;br/&gt; The INCITE (InterNet Control and Inference Tools at the Edge) Project focuses experts from the fields of&lt;br/&gt;networking, digital signal processing, and applied mathematics towards the goal of characterizing network ser-vice based solely on edge-based measurement at hosts and/or edge routers. This project blends recent work in multifractal traffic modeling, quality of service (QoS) measurement, and network tomography to develop a unique and innovative framework for network service inference. The INCITE Project will develop new al-gorithms and implementations using the latest in DSP-driven network processor technology, providing a vital step towards better managing and understanding of Internet performance. Our effort consists of three closely inter-related research thrusts:&lt;br/&gt;1. Multifractal Traffic and Path Modeling: We will develop new, highly accurate tools for analyzing,&lt;br/&gt;modeling, and measuring the dynamics of network connections and end-to-end paths from the edge. Our&lt;br/&gt;approach to inferring the competing cross-traffic load on a path utilizes an innovative exponentially spaced&lt;br/&gt;probing sequence that is inspired by the theory of multifractal random processes. These probing packet&lt;br/&gt;chirps balance the trade-off between overwhelming the network with probes and obtaining statistics rich&lt;br/&gt;enough for accurate estimates.&lt;br/&gt;2. Multiclass Service Inference: We will develop a framework for clients to assess a network&apos;s core QoS&lt;br/&gt;functionalities based on external and passive observations. Using the theory of traffic envelopes, maximum&lt;br/&gt;likelihood estimation, and hypothesis testing, clients will be able to assess a broad set of the network&apos;s&lt;br/&gt;multi-class control mechanisms such as the service disciplines, link sharing rules and parameters, and&lt;br/&gt;policing parameters.&lt;br/&gt;3. Unicast Network Tomography: We will develop a novel methodology for network tomography that pro-vides&lt;br/&gt;link-level performance characterization of networks of arbitrary topologies based on unicast traffic&lt;br/&gt;measurements at a the network edge. A new network modeling framework based on factor graphs will&lt;br/&gt;enable the statistical inference of link-level service parameters (e.g., losses, delays, and service strate-gies). A key strength of our envisioned methodology is that it will enable scalable, real-time tomography algorithms deployable on hosts and/or edge routers.&lt;br/&gt; The INCITE Project will develop the theoretical underpinnings of network multifractal traffic processing,&lt;br/&gt;service inference, and link-level characterization for complex, large-scale networks, and lead to computation-ally efficient and scalable service inference algorithms based only on traffic measurement at the network edge. Moreover, in collaboration with a leading provider of broadband Internet bandwidth (Enron) and an innovator in networked signal processing hardware (Texas Instruments), we will build a complete prototype implementation of the proposed algorithms, including modules for multifractal traffic and path modeling, service inference, and network tomography. This reference implementation will provide a first-of-its-kind platform for obtaining a deep understanding of large networks and enable principled designs of future network architectures, algorithms, and models.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">99148</data>
      <data key="e_expirationDate">2006-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">99148</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n323" target="n324">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">ITR/SY: Algorithms for Data Storage and Movement</data>
      <data key="e_abstract">The central focus of this proposal is the development of efficient algorithms for the storage and movement of data. Specifically, we are interested in algorithms that impact the performance of large multimedia data storage systems. In algorithmic terms, some of the principal challenges that arise in the context of multimedia data storage are: (a) deciding how many copies of each data item need to be stored, (b) determining the exact layout of data on a set of servers, (c) dealing with changing workloads and dynamic data access patterns. These related challenges require the development of efficient algorithms for optimizing data layout to maximize client satisfaction, monitoring the performance of data storage systems and scheduling the movement of large amounts of data.&lt;br/&gt;&lt;br/&gt;Futhermore, what makes the issues that we consider even more significant is the fact that data storage and movement issues also arise within publicly share networks such as the Internet where the bandwidth can be dynamic and highly variable, and can result in a poor choice of paths chosen to transfer data in the network. One way to address this issue is to route data through specific holding points. By doing this we are able to increase throughput and decrease completion times by an order of magnitude to transfer data from several sources to a single destination. Algorithms related to this problem have been developed by us and are being tested with the Bistro framework, which is a framework for providing a data upload service such as one required by IRS for tax submission purposes. Our data movement algorithms are being used to schedule the transfer of data from many different locations to a final destination server.&lt;br/&gt;&lt;br/&gt;While some specific instances of the individual problems have been considered earlier, there is no work dealing comprehensively with the range of issues that we focus on.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113192</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113192</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n611" target="n612">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">Theory Revision and Related Problems in Learning Theory</data>
      <data key="e_abstract">Theory revision is the correcting of a given, roughly correct rule,&lt;br/&gt;also known as a concept or theory. This problem arises frequently in&lt;br/&gt;machine learning, for instance, when the initial output of an &lt;br/&gt;expert system is not correct, and when the machine learning&lt;br/&gt;problem is too large or too complex to solve from scratch, and an&lt;br/&gt;approximately correct rule is needed to jump-start the learning&lt;br/&gt;process. There has been considerable ad hoc building of theory &lt;br/&gt;revision systems, but the theory is poorly understood. This research&lt;br/&gt;investigates fundamental mathematical possibilities and limitations&lt;br/&gt;of efficient theory revision. It is hoped that as a result of&lt;br/&gt;this research, theory revision in computational learning theory will&lt;br/&gt;emerge as a general framework for the study of learning situations&lt;br/&gt;where a large amount of initial information is available or&lt;br/&gt;necessary. &lt;br/&gt;&lt;br/&gt;In particular, the PIs investigate the following areas: extensions&lt;br/&gt;of their previous work on propositional logic theory revision&lt;br/&gt;with queries, relations to certificate complexity and&lt;br/&gt;attribute-efficient learning, revision problems for predicate logic&lt;br/&gt;representations, and both the learning and revising of categorial &lt;br/&gt;grammars.</data>
      <data key="e_pgm">2860</data>
      <data key="e_label">100336</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">100336</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n616" target="n617">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NIRT/GOALI: DNA-Based Nanomechanical Devices</data>
      <data key="e_abstract">Abstract&lt;br/&gt;CTS-0103002&lt;br/&gt;N. Seeman, et al., New York University&lt;br/&gt;&lt;br/&gt;This proposal was received in response to Nanoscale Science and Engineering (NSE) solicitation, NSF-00119, in the category Nanoscale Interdisciplinary Research Teams (NIRT). This is a collaborative activity between New York University, California Institute of Technology and Dow Chemical Co. using GOALI model. The goal is to synthesize and demonstrate operational nanoscale machines or devices. The level of control offered by DNA systems can be exploited to make intricate DNA nanostructures, including self-assembling DNA that forms two-dimensional and three-dimensional arrays. Modeling and simulation is a critical part of this project, in order construct and test the DNA nanostructures. &lt;br/&gt;&lt;br/&gt;It is proposed to combine the activities of New York University, California Institute of Technology and Dow Chemical laboratories to achieve a demonstration of DNA based nanomechanical devices useful for performing fast calculations, for sensors that detect specific molecules in the environment, or to improve the properties or performance of a material. Practical design and manufacture of nanoscale machines and devices requires overcoming numerous challenges in synthesis, processing, characterization, design, optimization, and fabrication. The approach will be first to prototype the designs computationally, optimizing the particular base-pair sequences, making sure that the particular lengths and spacings will lead to proper clearances, and testing the operation of the device, including the dynamics. The project will focus on nanomechanical devices of three types. &lt;br/&gt;o The B-Z based nanomotor. A DNA based nanomotor predicated on the B to Z DNA&lt;br/&gt;transitions under different salt conditions. &lt;br/&gt;o A DNA sequence-specific mechanical device &lt;br/&gt;o A DNA based switch based on principles similar to the DNA sequence-&lt;br/&gt;specific mechanical device.</data>
      <data key="e_pgm">2878</data>
      <data key="e_label">103002</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">103002</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n616" target="n618">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NIRT/GOALI: DNA-Based Nanomechanical Devices</data>
      <data key="e_abstract">Abstract&lt;br/&gt;CTS-0103002&lt;br/&gt;N. Seeman, et al., New York University&lt;br/&gt;&lt;br/&gt;This proposal was received in response to Nanoscale Science and Engineering (NSE) solicitation, NSF-00119, in the category Nanoscale Interdisciplinary Research Teams (NIRT). This is a collaborative activity between New York University, California Institute of Technology and Dow Chemical Co. using GOALI model. The goal is to synthesize and demonstrate operational nanoscale machines or devices. The level of control offered by DNA systems can be exploited to make intricate DNA nanostructures, including self-assembling DNA that forms two-dimensional and three-dimensional arrays. Modeling and simulation is a critical part of this project, in order construct and test the DNA nanostructures. &lt;br/&gt;&lt;br/&gt;It is proposed to combine the activities of New York University, California Institute of Technology and Dow Chemical laboratories to achieve a demonstration of DNA based nanomechanical devices useful for performing fast calculations, for sensors that detect specific molecules in the environment, or to improve the properties or performance of a material. Practical design and manufacture of nanoscale machines and devices requires overcoming numerous challenges in synthesis, processing, characterization, design, optimization, and fabrication. The approach will be first to prototype the designs computationally, optimizing the particular base-pair sequences, making sure that the particular lengths and spacings will lead to proper clearances, and testing the operation of the device, including the dynamics. The project will focus on nanomechanical devices of three types. &lt;br/&gt;o The B-Z based nanomotor. A DNA based nanomotor predicated on the B to Z DNA&lt;br/&gt;transitions under different salt conditions. &lt;br/&gt;o A DNA sequence-specific mechanical device &lt;br/&gt;o A DNA based switch based on principles similar to the DNA sequence-&lt;br/&gt;specific mechanical device.</data>
      <data key="e_pgm">2878</data>
      <data key="e_label">103002</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">103002</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n616" target="n619">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NIRT/GOALI: DNA-Based Nanomechanical Devices</data>
      <data key="e_abstract">Abstract&lt;br/&gt;CTS-0103002&lt;br/&gt;N. Seeman, et al., New York University&lt;br/&gt;&lt;br/&gt;This proposal was received in response to Nanoscale Science and Engineering (NSE) solicitation, NSF-00119, in the category Nanoscale Interdisciplinary Research Teams (NIRT). This is a collaborative activity between New York University, California Institute of Technology and Dow Chemical Co. using GOALI model. The goal is to synthesize and demonstrate operational nanoscale machines or devices. The level of control offered by DNA systems can be exploited to make intricate DNA nanostructures, including self-assembling DNA that forms two-dimensional and three-dimensional arrays. Modeling and simulation is a critical part of this project, in order construct and test the DNA nanostructures. &lt;br/&gt;&lt;br/&gt;It is proposed to combine the activities of New York University, California Institute of Technology and Dow Chemical laboratories to achieve a demonstration of DNA based nanomechanical devices useful for performing fast calculations, for sensors that detect specific molecules in the environment, or to improve the properties or performance of a material. Practical design and manufacture of nanoscale machines and devices requires overcoming numerous challenges in synthesis, processing, characterization, design, optimization, and fabrication. The approach will be first to prototype the designs computationally, optimizing the particular base-pair sequences, making sure that the particular lengths and spacings will lead to proper clearances, and testing the operation of the device, including the dynamics. The project will focus on nanomechanical devices of three types. &lt;br/&gt;o The B-Z based nanomotor. A DNA based nanomotor predicated on the B to Z DNA&lt;br/&gt;transitions under different salt conditions. &lt;br/&gt;o A DNA sequence-specific mechanical device &lt;br/&gt;o A DNA based switch based on principles similar to the DNA sequence-&lt;br/&gt;specific mechanical device.</data>
      <data key="e_pgm">2878</data>
      <data key="e_label">103002</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">103002</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n162" target="n616">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NIRT/GOALI: DNA-Based Nanomechanical Devices</data>
      <data key="e_abstract">Abstract&lt;br/&gt;CTS-0103002&lt;br/&gt;N. Seeman, et al., New York University&lt;br/&gt;&lt;br/&gt;This proposal was received in response to Nanoscale Science and Engineering (NSE) solicitation, NSF-00119, in the category Nanoscale Interdisciplinary Research Teams (NIRT). This is a collaborative activity between New York University, California Institute of Technology and Dow Chemical Co. using GOALI model. The goal is to synthesize and demonstrate operational nanoscale machines or devices. The level of control offered by DNA systems can be exploited to make intricate DNA nanostructures, including self-assembling DNA that forms two-dimensional and three-dimensional arrays. Modeling and simulation is a critical part of this project, in order construct and test the DNA nanostructures. &lt;br/&gt;&lt;br/&gt;It is proposed to combine the activities of New York University, California Institute of Technology and Dow Chemical laboratories to achieve a demonstration of DNA based nanomechanical devices useful for performing fast calculations, for sensors that detect specific molecules in the environment, or to improve the properties or performance of a material. Practical design and manufacture of nanoscale machines and devices requires overcoming numerous challenges in synthesis, processing, characterization, design, optimization, and fabrication. The approach will be first to prototype the designs computationally, optimizing the particular base-pair sequences, making sure that the particular lengths and spacings will lead to proper clearances, and testing the operation of the device, including the dynamics. The project will focus on nanomechanical devices of three types. &lt;br/&gt;o The B-Z based nanomotor. A DNA based nanomotor predicated on the B to Z DNA&lt;br/&gt;transitions under different salt conditions. &lt;br/&gt;o A DNA sequence-specific mechanical device &lt;br/&gt;o A DNA based switch based on principles similar to the DNA sequence-&lt;br/&gt;specific mechanical device.</data>
      <data key="e_pgm">2878</data>
      <data key="e_label">103002</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">103002</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n617" target="n618">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NIRT/GOALI: DNA-Based Nanomechanical Devices</data>
      <data key="e_abstract">Abstract&lt;br/&gt;CTS-0103002&lt;br/&gt;N. Seeman, et al., New York University&lt;br/&gt;&lt;br/&gt;This proposal was received in response to Nanoscale Science and Engineering (NSE) solicitation, NSF-00119, in the category Nanoscale Interdisciplinary Research Teams (NIRT). This is a collaborative activity between New York University, California Institute of Technology and Dow Chemical Co. using GOALI model. The goal is to synthesize and demonstrate operational nanoscale machines or devices. The level of control offered by DNA systems can be exploited to make intricate DNA nanostructures, including self-assembling DNA that forms two-dimensional and three-dimensional arrays. Modeling and simulation is a critical part of this project, in order construct and test the DNA nanostructures. &lt;br/&gt;&lt;br/&gt;It is proposed to combine the activities of New York University, California Institute of Technology and Dow Chemical laboratories to achieve a demonstration of DNA based nanomechanical devices useful for performing fast calculations, for sensors that detect specific molecules in the environment, or to improve the properties or performance of a material. Practical design and manufacture of nanoscale machines and devices requires overcoming numerous challenges in synthesis, processing, characterization, design, optimization, and fabrication. The approach will be first to prototype the designs computationally, optimizing the particular base-pair sequences, making sure that the particular lengths and spacings will lead to proper clearances, and testing the operation of the device, including the dynamics. The project will focus on nanomechanical devices of three types. &lt;br/&gt;o The B-Z based nanomotor. A DNA based nanomotor predicated on the B to Z DNA&lt;br/&gt;transitions under different salt conditions. &lt;br/&gt;o A DNA sequence-specific mechanical device &lt;br/&gt;o A DNA based switch based on principles similar to the DNA sequence-&lt;br/&gt;specific mechanical device.</data>
      <data key="e_pgm">2878</data>
      <data key="e_label">103002</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">103002</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n617" target="n619">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NIRT/GOALI: DNA-Based Nanomechanical Devices</data>
      <data key="e_abstract">Abstract&lt;br/&gt;CTS-0103002&lt;br/&gt;N. Seeman, et al., New York University&lt;br/&gt;&lt;br/&gt;This proposal was received in response to Nanoscale Science and Engineering (NSE) solicitation, NSF-00119, in the category Nanoscale Interdisciplinary Research Teams (NIRT). This is a collaborative activity between New York University, California Institute of Technology and Dow Chemical Co. using GOALI model. The goal is to synthesize and demonstrate operational nanoscale machines or devices. The level of control offered by DNA systems can be exploited to make intricate DNA nanostructures, including self-assembling DNA that forms two-dimensional and three-dimensional arrays. Modeling and simulation is a critical part of this project, in order construct and test the DNA nanostructures. &lt;br/&gt;&lt;br/&gt;It is proposed to combine the activities of New York University, California Institute of Technology and Dow Chemical laboratories to achieve a demonstration of DNA based nanomechanical devices useful for performing fast calculations, for sensors that detect specific molecules in the environment, or to improve the properties or performance of a material. Practical design and manufacture of nanoscale machines and devices requires overcoming numerous challenges in synthesis, processing, characterization, design, optimization, and fabrication. The approach will be first to prototype the designs computationally, optimizing the particular base-pair sequences, making sure that the particular lengths and spacings will lead to proper clearances, and testing the operation of the device, including the dynamics. The project will focus on nanomechanical devices of three types. &lt;br/&gt;o The B-Z based nanomotor. A DNA based nanomotor predicated on the B to Z DNA&lt;br/&gt;transitions under different salt conditions. &lt;br/&gt;o A DNA sequence-specific mechanical device &lt;br/&gt;o A DNA based switch based on principles similar to the DNA sequence-&lt;br/&gt;specific mechanical device.</data>
      <data key="e_pgm">2878</data>
      <data key="e_label">103002</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">103002</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n162" target="n617">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NIRT/GOALI: DNA-Based Nanomechanical Devices</data>
      <data key="e_abstract">Abstract&lt;br/&gt;CTS-0103002&lt;br/&gt;N. Seeman, et al., New York University&lt;br/&gt;&lt;br/&gt;This proposal was received in response to Nanoscale Science and Engineering (NSE) solicitation, NSF-00119, in the category Nanoscale Interdisciplinary Research Teams (NIRT). This is a collaborative activity between New York University, California Institute of Technology and Dow Chemical Co. using GOALI model. The goal is to synthesize and demonstrate operational nanoscale machines or devices. The level of control offered by DNA systems can be exploited to make intricate DNA nanostructures, including self-assembling DNA that forms two-dimensional and three-dimensional arrays. Modeling and simulation is a critical part of this project, in order construct and test the DNA nanostructures. &lt;br/&gt;&lt;br/&gt;It is proposed to combine the activities of New York University, California Institute of Technology and Dow Chemical laboratories to achieve a demonstration of DNA based nanomechanical devices useful for performing fast calculations, for sensors that detect specific molecules in the environment, or to improve the properties or performance of a material. Practical design and manufacture of nanoscale machines and devices requires overcoming numerous challenges in synthesis, processing, characterization, design, optimization, and fabrication. The approach will be first to prototype the designs computationally, optimizing the particular base-pair sequences, making sure that the particular lengths and spacings will lead to proper clearances, and testing the operation of the device, including the dynamics. The project will focus on nanomechanical devices of three types. &lt;br/&gt;o The B-Z based nanomotor. A DNA based nanomotor predicated on the B to Z DNA&lt;br/&gt;transitions under different salt conditions. &lt;br/&gt;o A DNA sequence-specific mechanical device &lt;br/&gt;o A DNA based switch based on principles similar to the DNA sequence-&lt;br/&gt;specific mechanical device.</data>
      <data key="e_pgm">2878</data>
      <data key="e_label">103002</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">103002</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n618" target="n619">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NIRT/GOALI: DNA-Based Nanomechanical Devices</data>
      <data key="e_abstract">Abstract&lt;br/&gt;CTS-0103002&lt;br/&gt;N. Seeman, et al., New York University&lt;br/&gt;&lt;br/&gt;This proposal was received in response to Nanoscale Science and Engineering (NSE) solicitation, NSF-00119, in the category Nanoscale Interdisciplinary Research Teams (NIRT). This is a collaborative activity between New York University, California Institute of Technology and Dow Chemical Co. using GOALI model. The goal is to synthesize and demonstrate operational nanoscale machines or devices. The level of control offered by DNA systems can be exploited to make intricate DNA nanostructures, including self-assembling DNA that forms two-dimensional and three-dimensional arrays. Modeling and simulation is a critical part of this project, in order construct and test the DNA nanostructures. &lt;br/&gt;&lt;br/&gt;It is proposed to combine the activities of New York University, California Institute of Technology and Dow Chemical laboratories to achieve a demonstration of DNA based nanomechanical devices useful for performing fast calculations, for sensors that detect specific molecules in the environment, or to improve the properties or performance of a material. Practical design and manufacture of nanoscale machines and devices requires overcoming numerous challenges in synthesis, processing, characterization, design, optimization, and fabrication. The approach will be first to prototype the designs computationally, optimizing the particular base-pair sequences, making sure that the particular lengths and spacings will lead to proper clearances, and testing the operation of the device, including the dynamics. The project will focus on nanomechanical devices of three types. &lt;br/&gt;o The B-Z based nanomotor. A DNA based nanomotor predicated on the B to Z DNA&lt;br/&gt;transitions under different salt conditions. &lt;br/&gt;o A DNA sequence-specific mechanical device &lt;br/&gt;o A DNA based switch based on principles similar to the DNA sequence-&lt;br/&gt;specific mechanical device.</data>
      <data key="e_pgm">2878</data>
      <data key="e_label">103002</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">103002</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n162" target="n618">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NIRT/GOALI: DNA-Based Nanomechanical Devices</data>
      <data key="e_abstract">Abstract&lt;br/&gt;CTS-0103002&lt;br/&gt;N. Seeman, et al., New York University&lt;br/&gt;&lt;br/&gt;This proposal was received in response to Nanoscale Science and Engineering (NSE) solicitation, NSF-00119, in the category Nanoscale Interdisciplinary Research Teams (NIRT). This is a collaborative activity between New York University, California Institute of Technology and Dow Chemical Co. using GOALI model. The goal is to synthesize and demonstrate operational nanoscale machines or devices. The level of control offered by DNA systems can be exploited to make intricate DNA nanostructures, including self-assembling DNA that forms two-dimensional and three-dimensional arrays. Modeling and simulation is a critical part of this project, in order construct and test the DNA nanostructures. &lt;br/&gt;&lt;br/&gt;It is proposed to combine the activities of New York University, California Institute of Technology and Dow Chemical laboratories to achieve a demonstration of DNA based nanomechanical devices useful for performing fast calculations, for sensors that detect specific molecules in the environment, or to improve the properties or performance of a material. Practical design and manufacture of nanoscale machines and devices requires overcoming numerous challenges in synthesis, processing, characterization, design, optimization, and fabrication. The approach will be first to prototype the designs computationally, optimizing the particular base-pair sequences, making sure that the particular lengths and spacings will lead to proper clearances, and testing the operation of the device, including the dynamics. The project will focus on nanomechanical devices of three types. &lt;br/&gt;o The B-Z based nanomotor. A DNA based nanomotor predicated on the B to Z DNA&lt;br/&gt;transitions under different salt conditions. &lt;br/&gt;o A DNA sequence-specific mechanical device &lt;br/&gt;o A DNA based switch based on principles similar to the DNA sequence-&lt;br/&gt;specific mechanical device.</data>
      <data key="e_pgm">2878</data>
      <data key="e_label">103002</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">103002</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n162" target="n619">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NIRT/GOALI: DNA-Based Nanomechanical Devices</data>
      <data key="e_abstract">Abstract&lt;br/&gt;CTS-0103002&lt;br/&gt;N. Seeman, et al., New York University&lt;br/&gt;&lt;br/&gt;This proposal was received in response to Nanoscale Science and Engineering (NSE) solicitation, NSF-00119, in the category Nanoscale Interdisciplinary Research Teams (NIRT). This is a collaborative activity between New York University, California Institute of Technology and Dow Chemical Co. using GOALI model. The goal is to synthesize and demonstrate operational nanoscale machines or devices. The level of control offered by DNA systems can be exploited to make intricate DNA nanostructures, including self-assembling DNA that forms two-dimensional and three-dimensional arrays. Modeling and simulation is a critical part of this project, in order construct and test the DNA nanostructures. &lt;br/&gt;&lt;br/&gt;It is proposed to combine the activities of New York University, California Institute of Technology and Dow Chemical laboratories to achieve a demonstration of DNA based nanomechanical devices useful for performing fast calculations, for sensors that detect specific molecules in the environment, or to improve the properties or performance of a material. Practical design and manufacture of nanoscale machines and devices requires overcoming numerous challenges in synthesis, processing, characterization, design, optimization, and fabrication. The approach will be first to prototype the designs computationally, optimizing the particular base-pair sequences, making sure that the particular lengths and spacings will lead to proper clearances, and testing the operation of the device, including the dynamics. The project will focus on nanomechanical devices of three types. &lt;br/&gt;o The B-Z based nanomotor. A DNA based nanomotor predicated on the B to Z DNA&lt;br/&gt;transitions under different salt conditions. &lt;br/&gt;o A DNA sequence-specific mechanical device &lt;br/&gt;o A DNA based switch based on principles similar to the DNA sequence-&lt;br/&gt;specific mechanical device.</data>
      <data key="e_pgm">2878</data>
      <data key="e_label">103002</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">103002</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n623" target="n624">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">ITR/SI: Design and Implementation of a Graphics Supercomputer from Commodity Components</data>
      <data key="e_abstract">This project will design and implement a distributed rendering platform that consists of off-the-shelf commodity components, yet delivers real-time photorealism on a large scale. This parallel rendering system was inspired by a similar system recently built at Princeton University. Proposed extensions to that work include a significant expansion in scale, new task-scheduling algorithms, a new integrated network design that links graphics card drivers to network interface card drivers in distributed Linux systems, and an extension of the Keller technique, called instant radiosity, to deliver real-time, global illumination. Design and implementation of the platform should provide graduate students in computer science with valuable hands-on experience in designing large-scale systems for distributed processing. The resulting platform should serve as a valuable research tool for several groups including the NSF ERC group, which will use visualization to explore product and process design for new fibers and films. The enabling code extensions, to widely-available open software systems that this project builds, will be freely available over the Internet.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">113139</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113139</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n625" target="n626">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">Workshop Series on Mobile Computing</data>
      <data key="e_abstract">This is the first workshop of a planned workshop series. Its objective is to stimulate and focus interest in the emerging area of data management for context-aware, mobile computing environments. As wireless bandwidth becomes more widely available, the communication infrastructure will be heavily wireless oriented. It will provide adaptive connectivity among immobile systems and portable devices such as cell phones, laptops, and other such future devices. Under this platform the computing will begin to migrate away from the desktop toward these devices consequently it will become necessary to managed data more carefully. Industry and academia recognized wireless devices as an important area for development, however, the problems of data management have not been addressed in a coordinated manner. This workshop aims to fill this gap by providing a forum for leading researchers from both industry and academia in the areas of infomation management, wireless networking, communications and signal processing systems to set the research agenda for the future. This workshop will lead to a more cohesive research community and will result in a quicker development and adoption of technologies for this new area. It will facilitate the development of new standards and will provide the synergy necessary for the US technical community to become a dominant player in this field. The results of the workshop will be made available online through web site (http://www.cstp.umkc.edu/nsfmobile/wshop.html/ or http://www.cs.brown.edu/nsfmobile/wshop.html/) and will be reported in periodicals (e.g., ACM and IEEE) and relevant conferences, workshops or meetings.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">99128</data>
      <data key="e_expirationDate">2002-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">99128</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n629" target="n630">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Research for Mixed Signal Electronic Technologies: A Joint Initiative Between NSF and SRC: Novel RF Front-Ends For Future Mobile Communication Systems</data>
      <data key="e_abstract">Future mobile communications technologies are expected to provide a wide variety of services, high-quality audio, high-definition video and quick response control data, through wide-band access channels. Future systems will acquire multimedia capability and will facilitate the use of high-speed wireless local area networks (W-LAN) and Home Broadcasting networks in place of traditional architectures. These systems will be designed to address user demands for broadband wireless communications and will drive the development of new microwave and mm-wave devices and circuits. Possible in-home application scenarios for both analog and digital transmission will require very compact, low-cost and high efficiency receive/transmit devices that can provide asymmetric data transmission from a home server to various appliances for quality&lt;br/&gt;operation and control. In response to the above technology needs, this program proposes to develop a novel RF front-end receiver architecture that consumes very little power, is highly compact, very low-cost and high-performance.&lt;br/&gt;&lt;br/&gt;The proposed architecture is based on a CMOS on SOI implementation for both the RF and digital parts of the circuit and allows for an intimate integration of the circuit with the RF filters and antenna structure. The new receiver architecture will be based on the use of metamaterial substrates for the development of highly integrated filter banks, will rely on vertical integration for the development of highly compact three-dimensional wireless front ends, will have a novel antenna structure intimately integrated with the highly selective multi-frequency substrate and have a novel mixed signal digital IF circuitry. The design of the receiver will be accomplished through a holistic mixed circuit approach that accurately takes into account high frequency effects including dispersion, radiation and electromagnetic coupling. The modeling and simulation problem encountered in the implementation of the above vision is typical to mixed-signal RFICs. The proposed technical approach represents a solution to the broader problem and has the potential to alleviate the design bottleneck at the analog/RF/ package interface.&lt;br/&gt;&lt;br/&gt;The University of Michigan will use its own fabrication facilities together with the IBM Blue Logic Cu-11 CMOS on SOI 0.11m facilities to develop the proposed receiver. North Carolina State University will provide modeling and simulations necessary for the design of the receiver. A prototype 4-channel 5-10GHz receiver will be simulated, designed, fabricated, packaged and tested during the course of this work. In addition, 16-channel very high-Q electromagnetic bandgap filter bank will be designed, fabricated and tested. Moreover, topics such as manufacturing tolerance, temperature effects, filter tuning and frequency scaling will be studied.&lt;br/&gt;&lt;br/&gt;The proposed work will be performed in close collaboration with IBM High performance logic development and RF Technology development. Graduate and undergraduate student research investigators in both Universities will execute the research tasks. A number of these students may spend part of their summer at IBM on internships to facilitate the collaboration and take the opportunity to interact with IBM scientists. Furthermore, the research outcomes of this effort, in terms of the design of the mixed-signal circuit architectures, will become the basis of a new senior/graduate-level course in high frequency circuits. Special effort will be placed in attracting&lt;br/&gt;underrepresented students through the University of Michigan UROP (Undergraduate Research Opportunities) and Maria Sara Parker Programs specifically designed to provide research experiences to undergraduate and graduate female and minority students. The results of these efforts will be disseminated broadly via publications in scientific journal and/or presentations in technical conferences and the development of an interactive web site.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">120319</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">120319</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n633" target="n634">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NER: Microarray Devices of Aligned Carbon Nanotubes for Biological/Biomedical Research</data>
      <data key="e_abstract">This proposal was received in response to NSE, NSF-0019. The goal of the research is to explore the potential application of carbon nanotubes for greatly enhancing the efficiency of existing microarray devices in biological/biomedical research. The proposed work will synthesize arrays of well-aligned carbon nanotubes that can be used to transfer biomolecules from the tips of the nanotubes for creating microarrays at a level of packing density not achievable by current techniques. Carbon nanotubes are uniquely suited for this application owing to their extremely high elastic modulus and strength as well as exceptional capability in sustaining large nonlinear elastic deformation.&lt;br/&gt;&lt;br/&gt;This is a preliminary feasibility study of a novel idea in nanoscience and engineering with focuses on nanoscale devices and systems architecture, and modeling and simulation at the nanoscale. The effort is likely to catalyze rapid advances in biological/biomedical research.&lt;br/&gt;&lt;br/&gt;The objectives of this exploratory research are to (a) better understand the growth of aligned carbon nanotubes for achieving optimal control of their length, diameter, and packing density &lt;br/&gt;in microarrays, (b) better understand the structures and performance of individual nanotubes &lt;br/&gt;as well as microarrays through microscopic and mechanical characterizations, (c) initiate computational mechanics effort at both the continuum and atomistic levels for modeling the &lt;br/&gt;nano-structure, chirality, and mechanical behavior of nanotubes, (d) develop the tools and criteria for assessing the performance of this novel microarray device for biological/biomedical research, and (e) evaluate the feasibility of the nanotube-based microarray devices.&lt;br/&gt;&lt;br/&gt;It is expected that this project will advance microarray technology to enable more rapid study by biological and biomedical investigators in areas of genetic research. Ultimately,this technology may result in advancements in the study and treatment of human disease. Fundamentalinvestigations on synthesis, characterization, and property modeling of carbon nanotubes will benefit the technology of nanotube synthesis and engineering. Better understanding of nanotube growth mechanisms will enable improved synthesis and control of the nanotube structure. Development of structure/property models for mechanical behavior of carbon nanotubes, along with synthesis/structure relationships, will ultimately provide a tool for engineering their mechanical properties.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">103012</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103012</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n633" target="n635">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NER: Microarray Devices of Aligned Carbon Nanotubes for Biological/Biomedical Research</data>
      <data key="e_abstract">This proposal was received in response to NSE, NSF-0019. The goal of the research is to explore the potential application of carbon nanotubes for greatly enhancing the efficiency of existing microarray devices in biological/biomedical research. The proposed work will synthesize arrays of well-aligned carbon nanotubes that can be used to transfer biomolecules from the tips of the nanotubes for creating microarrays at a level of packing density not achievable by current techniques. Carbon nanotubes are uniquely suited for this application owing to their extremely high elastic modulus and strength as well as exceptional capability in sustaining large nonlinear elastic deformation.&lt;br/&gt;&lt;br/&gt;This is a preliminary feasibility study of a novel idea in nanoscience and engineering with focuses on nanoscale devices and systems architecture, and modeling and simulation at the nanoscale. The effort is likely to catalyze rapid advances in biological/biomedical research.&lt;br/&gt;&lt;br/&gt;The objectives of this exploratory research are to (a) better understand the growth of aligned carbon nanotubes for achieving optimal control of their length, diameter, and packing density &lt;br/&gt;in microarrays, (b) better understand the structures and performance of individual nanotubes &lt;br/&gt;as well as microarrays through microscopic and mechanical characterizations, (c) initiate computational mechanics effort at both the continuum and atomistic levels for modeling the &lt;br/&gt;nano-structure, chirality, and mechanical behavior of nanotubes, (d) develop the tools and criteria for assessing the performance of this novel microarray device for biological/biomedical research, and (e) evaluate the feasibility of the nanotube-based microarray devices.&lt;br/&gt;&lt;br/&gt;It is expected that this project will advance microarray technology to enable more rapid study by biological and biomedical investigators in areas of genetic research. Ultimately,this technology may result in advancements in the study and treatment of human disease. Fundamentalinvestigations on synthesis, characterization, and property modeling of carbon nanotubes will benefit the technology of nanotube synthesis and engineering. Better understanding of nanotube growth mechanisms will enable improved synthesis and control of the nanotube structure. Development of structure/property models for mechanical behavior of carbon nanotubes, along with synthesis/structure relationships, will ultimately provide a tool for engineering their mechanical properties.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">103012</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103012</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n634" target="n635">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">NER: Microarray Devices of Aligned Carbon Nanotubes for Biological/Biomedical Research</data>
      <data key="e_abstract">This proposal was received in response to NSE, NSF-0019. The goal of the research is to explore the potential application of carbon nanotubes for greatly enhancing the efficiency of existing microarray devices in biological/biomedical research. The proposed work will synthesize arrays of well-aligned carbon nanotubes that can be used to transfer biomolecules from the tips of the nanotubes for creating microarrays at a level of packing density not achievable by current techniques. Carbon nanotubes are uniquely suited for this application owing to their extremely high elastic modulus and strength as well as exceptional capability in sustaining large nonlinear elastic deformation.&lt;br/&gt;&lt;br/&gt;This is a preliminary feasibility study of a novel idea in nanoscience and engineering with focuses on nanoscale devices and systems architecture, and modeling and simulation at the nanoscale. The effort is likely to catalyze rapid advances in biological/biomedical research.&lt;br/&gt;&lt;br/&gt;The objectives of this exploratory research are to (a) better understand the growth of aligned carbon nanotubes for achieving optimal control of their length, diameter, and packing density &lt;br/&gt;in microarrays, (b) better understand the structures and performance of individual nanotubes &lt;br/&gt;as well as microarrays through microscopic and mechanical characterizations, (c) initiate computational mechanics effort at both the continuum and atomistic levels for modeling the &lt;br/&gt;nano-structure, chirality, and mechanical behavior of nanotubes, (d) develop the tools and criteria for assessing the performance of this novel microarray device for biological/biomedical research, and (e) evaluate the feasibility of the nanotube-based microarray devices.&lt;br/&gt;&lt;br/&gt;It is expected that this project will advance microarray technology to enable more rapid study by biological and biomedical investigators in areas of genetic research. Ultimately,this technology may result in advancements in the study and treatment of human disease. Fundamentalinvestigations on synthesis, characterization, and property modeling of carbon nanotubes will benefit the technology of nanotube synthesis and engineering. Better understanding of nanotube growth mechanisms will enable improved synthesis and control of the nanotube structure. Development of structure/property models for mechanical behavior of carbon nanotubes, along with synthesis/structure relationships, will ultimately provide a tool for engineering their mechanical properties.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">103012</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">103012</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n636" target="n637">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">ITR/SI: Musical Tele-presence</data>
      <data key="e_abstract">This project involves research to enable musical tele-presence employing high performance communication networks and audio-video immersive environments. The goal is to create the impression of &quot;being in the same room&quot; for geographically separated users offering complete and immediate two-way visual and auditory contact to enable two or more musicians at different locations to freely interact musically. Three linked &quot;tele-presence&quot; studios being developed at the University of Rochester will be employed to understand the sources and effects of latency in musical interactions arising from acoustic, electronic and network factors under two-way real-time links. The controlled links between the studios will allow for gradated stages between the controlled situation of a dedicated local network to the more congested state of the Internet2 and the wider Internet. Building upon existing research, the limits of tolerance for latency in musical situations will be explored for various available network protocols, important Quality of Service issues and tradeoffs found in each will be investigated, and fault tolerant data transmission applications that employ data interleaving will be developed. Resources of the University of Rochester&apos;s Eastman School of Music will help guide this development from the end-user perspective. Ultimately the development of musical tele-presence will create an infrastructure through which music lessons and master classes by the best instructors can be simulcast in an interactive way. Performers could rehearse, perform and record together over the Internet, and chamber music groups could form unencumbered by geographical distances.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">112689</data>
      <data key="e_expirationDate">2006-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112689</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n636" target="n638">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">ITR/SI: Musical Tele-presence</data>
      <data key="e_abstract">This project involves research to enable musical tele-presence employing high performance communication networks and audio-video immersive environments. The goal is to create the impression of &quot;being in the same room&quot; for geographically separated users offering complete and immediate two-way visual and auditory contact to enable two or more musicians at different locations to freely interact musically. Three linked &quot;tele-presence&quot; studios being developed at the University of Rochester will be employed to understand the sources and effects of latency in musical interactions arising from acoustic, electronic and network factors under two-way real-time links. The controlled links between the studios will allow for gradated stages between the controlled situation of a dedicated local network to the more congested state of the Internet2 and the wider Internet. Building upon existing research, the limits of tolerance for latency in musical situations will be explored for various available network protocols, important Quality of Service issues and tradeoffs found in each will be investigated, and fault tolerant data transmission applications that employ data interleaving will be developed. Resources of the University of Rochester&apos;s Eastman School of Music will help guide this development from the end-user perspective. Ultimately the development of musical tele-presence will create an infrastructure through which music lessons and master classes by the best instructors can be simulcast in an interactive way. Performers could rehearse, perform and record together over the Internet, and chamber music groups could form unencumbered by geographical distances.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">112689</data>
      <data key="e_expirationDate">2006-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112689</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n637" target="n638">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">ITR/SI: Musical Tele-presence</data>
      <data key="e_abstract">This project involves research to enable musical tele-presence employing high performance communication networks and audio-video immersive environments. The goal is to create the impression of &quot;being in the same room&quot; for geographically separated users offering complete and immediate two-way visual and auditory contact to enable two or more musicians at different locations to freely interact musically. Three linked &quot;tele-presence&quot; studios being developed at the University of Rochester will be employed to understand the sources and effects of latency in musical interactions arising from acoustic, electronic and network factors under two-way real-time links. The controlled links between the studios will allow for gradated stages between the controlled situation of a dedicated local network to the more congested state of the Internet2 and the wider Internet. Building upon existing research, the limits of tolerance for latency in musical situations will be explored for various available network protocols, important Quality of Service issues and tradeoffs found in each will be investigated, and fault tolerant data transmission applications that employ data interleaving will be developed. Resources of the University of Rochester&apos;s Eastman School of Music will help guide this development from the end-user perspective. Ultimately the development of musical tele-presence will create an infrastructure through which music lessons and master classes by the best instructors can be simulcast in an interactive way. Performers could rehearse, perform and record together over the Internet, and chamber music groups could form unencumbered by geographical distances.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">112689</data>
      <data key="e_expirationDate">2006-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112689</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n646" target="n647">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">Theory and Applications of Information-Based Complexity</data>
      <data key="e_abstract">Proposal #0097348&lt;br/&gt;Traub, Joseph F.&lt;br/&gt;Columbia University&lt;br/&gt;&lt;br/&gt;There is huge interest in solving high dimensional problems. Many applications involve functions of hundreds, thousands or even an infinite number of variables. Examples occur in physics, chemistry,&lt;br/&gt;mathematical finance, and economics. &lt;br/&gt;&lt;br/&gt;It is the rare high dimensional problem that can be solved analytically. Generally one must settle for an approximate numerical solution to within an error e. The computational complexity is the minimal computational resource need to solve a problem to within e. Time is the resource considered&lt;br/&gt;and is measured by the number of information operations, arithmetic operations and comparisons. An example of an information operation is the computation of a function value.&lt;br/&gt;&lt;br/&gt;If a worst case deterministic assurance of an e-approximation is desired, then often the computational complexity depends exponentially on the number of variables d; the problem suffers the &quot;curse of&lt;br/&gt;dimensionality&quot;. Examples include integration, approximation, globaloptimization, integral and partial differential equations over typical isotropic classical spaces of r-times continuously differentiable&lt;br/&gt;functions. If the computational complexity is exponential in either 1/e or d the problem is said to be intractable. If the complexity is polynomial in 1/e and d, it is tractable. If, in addition, the minimal number of information operations, arithmetic operations and comparisons is independent of d the problem is strongly tractable. &lt;br/&gt;&lt;br/&gt;Intractability may sometimes be broken by settling for a stochastic assurance of error; examples are randomization (for instance, Monte Carlo) or the average case. A second way in which intractability might&lt;br/&gt;be broken is additional domain knowledge about the problem. An example of the domain knowledge is that the integrands in certain mathematical finance problems are non-isotropic. Additional domain knowledge can sometimes be used to make the problem strongly tractable even in the worst case deterministic setting!&lt;br/&gt;&lt;br/&gt;Continuation of research on achieving tractability and strong tractability is proposed. In particular, one proposed area of research is under what conditions is a double-win achievable for high dimensional integration:&lt;br/&gt; * convergence faster than Monte Carlo,&lt;br/&gt; * with a worst case deterministic assurance.&lt;br/&gt;The theoretical results will be used to improve the FinDer software system.&lt;br/&gt;&lt;br/&gt; More generally, research is proposed on the following topics:&lt;br/&gt; * Theory and Computer Experiments for Mathematical Finance,&lt;br/&gt; * Tractability of Quasi-Monte Carlo and Monte Carlo Algorithms,&lt;br/&gt; * Variable Smoothness, &lt;br/&gt; * Generalized Tractability.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">97348</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">97348</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n114" target="n650">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">New Prediction Paradigms for Parallel and Distributed Systems</data>
      <data key="e_abstract">This research aims to expand and redefine the role of prediction-based techniques for parallel and distributed systems. First, we reduce barrier synchronization overhead by predicting the final producer of a&lt;br/&gt;value before the barrier. This producer identification allows the consumer to speculatively proceed past the barrier, only waiting on the actual production as needed. Second, we introduce the slipstream&lt;br/&gt;paradigm to multiprocessor systems. A redundant version of each parallel thread runs concurrently, its execution reduced by speculatively removing long-latency events, such as shared memory writes. The reduced thread dynamically detects sharing patterns, which are used by the original thread to optimize its coherence and synchronization actions, improving overall performance. Finally, we investigate the use of producer-validated message prediction to reduce traffic in a message-passing environment. Both the producer and the consumer of a message predict its contents, using redundant prediction histories. Since the producer knows the results of the consumer&apos;s prediction, it need only send those data that were not correctly&lt;br/&gt;predicted. This traffic reduction is significant in environments in which communication is much more costly than computation, such as networked embedded systems. These three avenues of research represent an excursion into new frontiers of prediction-based technology, resulting in parallel systems that scale to new levels of availability and performance.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">105628</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105628</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n653" target="n654">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">ITR/SY (CISE): Software Improvement Through Binary Rewriting</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;Proposal #0113633&lt;br/&gt;U of Arizona&lt;br/&gt;Andrews, Gregory&lt;br/&gt;&lt;br/&gt;A binary rewriting system is a software system that transforms a binary (executable) program into a different but functionally equivalent binary program. This project is developing binary rewriting techniques for flexible link-time and run-time code optimizations. The aim is to develop a unified binary rewriting infrastructure that is able to handle a wide variety of applications---sequential, parallel, distributed,&lt;br/&gt;and mobile---hardware architectures---from RISC to CISC---and optimization criteria---including execution time, power consumption, and communication bandwidth.&lt;br/&gt;&lt;br/&gt;Existing techniques for compile-time code optimization suffer from several limitations: they are unable to cross the dividing line between application code and libraries; they cannot take advantage of commonly encountered values along the critical path if such values cannot be guaranteed to be compile-time constants;&lt;br/&gt;and they typically focus only on improving execution time. To overcome these limitations, this project is investigating the following topics: (1) low-level cost models that can be used for cost-benefit analyses of different optimization metrics; (2) efficient computation of value profiles and their use for&lt;br/&gt;low-level code specialization; and (3) techniques that reduce the overheads associated with communication libraries used by parallel and mobile applications.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">113633</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">113633</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n662" target="n663">
      <data key="e_effectiveDate">2001-08-15</data>
      <data key="e_title">ITR: Bugscope: An IT Test Bed for Sustaining Educational Outreach</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">1640</data>
      <data key="e_label">296013</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">296013</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n531" target="n664">
      <data key="e_effectiveDate">2001-08-01</data>
      <data key="e_title">Systems Support for High Performance I/O on Shared Storage Clusters</data>
      <data key="e_abstract">Conventional solutions for I/O have attempted to provide hardware&lt;br/&gt;and software parallelism via RAIDs or parallel machines/supercomputers.&lt;br/&gt;However, the problems associated with cost, scalability, &lt;br/&gt;and/or accessibility of these environments make them unattractive &lt;br/&gt;for widespread usage. This research addresses this important&lt;br/&gt;deficiency in high-performance I/O support, by proposing a shared storage&lt;br/&gt;system using an off-the-shelf cluster of workstations, disks, and&lt;br/&gt;networks. The proposed research goes beyond current state-of-the-art in I/O &lt;br/&gt;support for clusters and examines a broad spectrum of&lt;br/&gt;issues related to I/O software on clusters, that include&lt;br/&gt;application-directed, compiler-directed, and runtime system-directed &lt;br/&gt;optimizations. These optimizations are crucial to reduce/hide the &lt;br/&gt;latencies to different levels of the I/O hierarchy which will help &lt;br/&gt;accelerate the deployment of clusters for I/O-intensive applications.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">97998</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">97998</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n669" target="n670">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Infrastructure to Develop a Large Scale Experiment Testbed of Multi-modal Resources</data>
      <data key="e_abstract">EIA-0130422 &lt;br/&gt;Louiqa Raschid&lt;br/&gt;University of Maryland College Park&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Infrastructure to Develop a Large Scale Experiment Testbed of Multi-model Resources&lt;br/&gt;&lt;br/&gt;The use of the widely distributed collections of structured and unstructured information expressed in multiple languages or modalities provided by the Internet, requires production of scalable, robust algorithms for the discovery of replicated content, determination of delay or access latency of sources, and the confrontation of the inherently dynamic nature of the Internet.&lt;br/&gt;&lt;br/&gt;This project&apos;s objective is to establish a laboratory testbed providing a controlled environment that captures structural, content, and latency characteristics of the (publicly accessible) Web. This will stimulate collaboration between researchers whose interests range over natural language applications, language independent processing of scanned documents, analysis of video information sources, information retrieval, and wide area applications and resource discovery across heterogeneous servers. &lt;br/&gt;&lt;br/&gt;The testbed will support the development and testing of: (1) tools for broad-scale, cross-linguistic analysis and discovery of relevant information across languages and modalities, (2) cost models and access cost catalogs for wide area environments, reflecting the temporal variability in access latency, (3) distributed content based indexing and association of media clips for resource discovery, (4) transcoding and scheduling of multimedia resources for delivery any time and anywhere to disparate clients; from mobile wireless to high speed optical links.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130422</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130422</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n669" target="n671">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Infrastructure to Develop a Large Scale Experiment Testbed of Multi-modal Resources</data>
      <data key="e_abstract">EIA-0130422 &lt;br/&gt;Louiqa Raschid&lt;br/&gt;University of Maryland College Park&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Infrastructure to Develop a Large Scale Experiment Testbed of Multi-model Resources&lt;br/&gt;&lt;br/&gt;The use of the widely distributed collections of structured and unstructured information expressed in multiple languages or modalities provided by the Internet, requires production of scalable, robust algorithms for the discovery of replicated content, determination of delay or access latency of sources, and the confrontation of the inherently dynamic nature of the Internet.&lt;br/&gt;&lt;br/&gt;This project&apos;s objective is to establish a laboratory testbed providing a controlled environment that captures structural, content, and latency characteristics of the (publicly accessible) Web. This will stimulate collaboration between researchers whose interests range over natural language applications, language independent processing of scanned documents, analysis of video information sources, information retrieval, and wide area applications and resource discovery across heterogeneous servers. &lt;br/&gt;&lt;br/&gt;The testbed will support the development and testing of: (1) tools for broad-scale, cross-linguistic analysis and discovery of relevant information across languages and modalities, (2) cost models and access cost catalogs for wide area environments, reflecting the temporal variability in access latency, (3) distributed content based indexing and association of media clips for resource discovery, (4) transcoding and scheduling of multimedia resources for delivery any time and anywhere to disparate clients; from mobile wireless to high speed optical links.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130422</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130422</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n669" target="n672">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Infrastructure to Develop a Large Scale Experiment Testbed of Multi-modal Resources</data>
      <data key="e_abstract">EIA-0130422 &lt;br/&gt;Louiqa Raschid&lt;br/&gt;University of Maryland College Park&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Infrastructure to Develop a Large Scale Experiment Testbed of Multi-model Resources&lt;br/&gt;&lt;br/&gt;The use of the widely distributed collections of structured and unstructured information expressed in multiple languages or modalities provided by the Internet, requires production of scalable, robust algorithms for the discovery of replicated content, determination of delay or access latency of sources, and the confrontation of the inherently dynamic nature of the Internet.&lt;br/&gt;&lt;br/&gt;This project&apos;s objective is to establish a laboratory testbed providing a controlled environment that captures structural, content, and latency characteristics of the (publicly accessible) Web. This will stimulate collaboration between researchers whose interests range over natural language applications, language independent processing of scanned documents, analysis of video information sources, information retrieval, and wide area applications and resource discovery across heterogeneous servers. &lt;br/&gt;&lt;br/&gt;The testbed will support the development and testing of: (1) tools for broad-scale, cross-linguistic analysis and discovery of relevant information across languages and modalities, (2) cost models and access cost catalogs for wide area environments, reflecting the temporal variability in access latency, (3) distributed content based indexing and association of media clips for resource discovery, (4) transcoding and scheduling of multimedia resources for delivery any time and anywhere to disparate clients; from mobile wireless to high speed optical links.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130422</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130422</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n669" target="n673">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Infrastructure to Develop a Large Scale Experiment Testbed of Multi-modal Resources</data>
      <data key="e_abstract">EIA-0130422 &lt;br/&gt;Louiqa Raschid&lt;br/&gt;University of Maryland College Park&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Infrastructure to Develop a Large Scale Experiment Testbed of Multi-model Resources&lt;br/&gt;&lt;br/&gt;The use of the widely distributed collections of structured and unstructured information expressed in multiple languages or modalities provided by the Internet, requires production of scalable, robust algorithms for the discovery of replicated content, determination of delay or access latency of sources, and the confrontation of the inherently dynamic nature of the Internet.&lt;br/&gt;&lt;br/&gt;This project&apos;s objective is to establish a laboratory testbed providing a controlled environment that captures structural, content, and latency characteristics of the (publicly accessible) Web. This will stimulate collaboration between researchers whose interests range over natural language applications, language independent processing of scanned documents, analysis of video information sources, information retrieval, and wide area applications and resource discovery across heterogeneous servers. &lt;br/&gt;&lt;br/&gt;The testbed will support the development and testing of: (1) tools for broad-scale, cross-linguistic analysis and discovery of relevant information across languages and modalities, (2) cost models and access cost catalogs for wide area environments, reflecting the temporal variability in access latency, (3) distributed content based indexing and association of media clips for resource discovery, (4) transcoding and scheduling of multimedia resources for delivery any time and anywhere to disparate clients; from mobile wireless to high speed optical links.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130422</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130422</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n670" target="n671">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Infrastructure to Develop a Large Scale Experiment Testbed of Multi-modal Resources</data>
      <data key="e_abstract">EIA-0130422 &lt;br/&gt;Louiqa Raschid&lt;br/&gt;University of Maryland College Park&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Infrastructure to Develop a Large Scale Experiment Testbed of Multi-model Resources&lt;br/&gt;&lt;br/&gt;The use of the widely distributed collections of structured and unstructured information expressed in multiple languages or modalities provided by the Internet, requires production of scalable, robust algorithms for the discovery of replicated content, determination of delay or access latency of sources, and the confrontation of the inherently dynamic nature of the Internet.&lt;br/&gt;&lt;br/&gt;This project&apos;s objective is to establish a laboratory testbed providing a controlled environment that captures structural, content, and latency characteristics of the (publicly accessible) Web. This will stimulate collaboration between researchers whose interests range over natural language applications, language independent processing of scanned documents, analysis of video information sources, information retrieval, and wide area applications and resource discovery across heterogeneous servers. &lt;br/&gt;&lt;br/&gt;The testbed will support the development and testing of: (1) tools for broad-scale, cross-linguistic analysis and discovery of relevant information across languages and modalities, (2) cost models and access cost catalogs for wide area environments, reflecting the temporal variability in access latency, (3) distributed content based indexing and association of media clips for resource discovery, (4) transcoding and scheduling of multimedia resources for delivery any time and anywhere to disparate clients; from mobile wireless to high speed optical links.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130422</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130422</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n670" target="n672">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Infrastructure to Develop a Large Scale Experiment Testbed of Multi-modal Resources</data>
      <data key="e_abstract">EIA-0130422 &lt;br/&gt;Louiqa Raschid&lt;br/&gt;University of Maryland College Park&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Infrastructure to Develop a Large Scale Experiment Testbed of Multi-model Resources&lt;br/&gt;&lt;br/&gt;The use of the widely distributed collections of structured and unstructured information expressed in multiple languages or modalities provided by the Internet, requires production of scalable, robust algorithms for the discovery of replicated content, determination of delay or access latency of sources, and the confrontation of the inherently dynamic nature of the Internet.&lt;br/&gt;&lt;br/&gt;This project&apos;s objective is to establish a laboratory testbed providing a controlled environment that captures structural, content, and latency characteristics of the (publicly accessible) Web. This will stimulate collaboration between researchers whose interests range over natural language applications, language independent processing of scanned documents, analysis of video information sources, information retrieval, and wide area applications and resource discovery across heterogeneous servers. &lt;br/&gt;&lt;br/&gt;The testbed will support the development and testing of: (1) tools for broad-scale, cross-linguistic analysis and discovery of relevant information across languages and modalities, (2) cost models and access cost catalogs for wide area environments, reflecting the temporal variability in access latency, (3) distributed content based indexing and association of media clips for resource discovery, (4) transcoding and scheduling of multimedia resources for delivery any time and anywhere to disparate clients; from mobile wireless to high speed optical links.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130422</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130422</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n670" target="n673">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Infrastructure to Develop a Large Scale Experiment Testbed of Multi-modal Resources</data>
      <data key="e_abstract">EIA-0130422 &lt;br/&gt;Louiqa Raschid&lt;br/&gt;University of Maryland College Park&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Infrastructure to Develop a Large Scale Experiment Testbed of Multi-model Resources&lt;br/&gt;&lt;br/&gt;The use of the widely distributed collections of structured and unstructured information expressed in multiple languages or modalities provided by the Internet, requires production of scalable, robust algorithms for the discovery of replicated content, determination of delay or access latency of sources, and the confrontation of the inherently dynamic nature of the Internet.&lt;br/&gt;&lt;br/&gt;This project&apos;s objective is to establish a laboratory testbed providing a controlled environment that captures structural, content, and latency characteristics of the (publicly accessible) Web. This will stimulate collaboration between researchers whose interests range over natural language applications, language independent processing of scanned documents, analysis of video information sources, information retrieval, and wide area applications and resource discovery across heterogeneous servers. &lt;br/&gt;&lt;br/&gt;The testbed will support the development and testing of: (1) tools for broad-scale, cross-linguistic analysis and discovery of relevant information across languages and modalities, (2) cost models and access cost catalogs for wide area environments, reflecting the temporal variability in access latency, (3) distributed content based indexing and association of media clips for resource discovery, (4) transcoding and scheduling of multimedia resources for delivery any time and anywhere to disparate clients; from mobile wireless to high speed optical links.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130422</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130422</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n671" target="n672">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Infrastructure to Develop a Large Scale Experiment Testbed of Multi-modal Resources</data>
      <data key="e_abstract">EIA-0130422 &lt;br/&gt;Louiqa Raschid&lt;br/&gt;University of Maryland College Park&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Infrastructure to Develop a Large Scale Experiment Testbed of Multi-model Resources&lt;br/&gt;&lt;br/&gt;The use of the widely distributed collections of structured and unstructured information expressed in multiple languages or modalities provided by the Internet, requires production of scalable, robust algorithms for the discovery of replicated content, determination of delay or access latency of sources, and the confrontation of the inherently dynamic nature of the Internet.&lt;br/&gt;&lt;br/&gt;This project&apos;s objective is to establish a laboratory testbed providing a controlled environment that captures structural, content, and latency characteristics of the (publicly accessible) Web. This will stimulate collaboration between researchers whose interests range over natural language applications, language independent processing of scanned documents, analysis of video information sources, information retrieval, and wide area applications and resource discovery across heterogeneous servers. &lt;br/&gt;&lt;br/&gt;The testbed will support the development and testing of: (1) tools for broad-scale, cross-linguistic analysis and discovery of relevant information across languages and modalities, (2) cost models and access cost catalogs for wide area environments, reflecting the temporal variability in access latency, (3) distributed content based indexing and association of media clips for resource discovery, (4) transcoding and scheduling of multimedia resources for delivery any time and anywhere to disparate clients; from mobile wireless to high speed optical links.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130422</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130422</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n671" target="n673">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Infrastructure to Develop a Large Scale Experiment Testbed of Multi-modal Resources</data>
      <data key="e_abstract">EIA-0130422 &lt;br/&gt;Louiqa Raschid&lt;br/&gt;University of Maryland College Park&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Infrastructure to Develop a Large Scale Experiment Testbed of Multi-model Resources&lt;br/&gt;&lt;br/&gt;The use of the widely distributed collections of structured and unstructured information expressed in multiple languages or modalities provided by the Internet, requires production of scalable, robust algorithms for the discovery of replicated content, determination of delay or access latency of sources, and the confrontation of the inherently dynamic nature of the Internet.&lt;br/&gt;&lt;br/&gt;This project&apos;s objective is to establish a laboratory testbed providing a controlled environment that captures structural, content, and latency characteristics of the (publicly accessible) Web. This will stimulate collaboration between researchers whose interests range over natural language applications, language independent processing of scanned documents, analysis of video information sources, information retrieval, and wide area applications and resource discovery across heterogeneous servers. &lt;br/&gt;&lt;br/&gt;The testbed will support the development and testing of: (1) tools for broad-scale, cross-linguistic analysis and discovery of relevant information across languages and modalities, (2) cost models and access cost catalogs for wide area environments, reflecting the temporal variability in access latency, (3) distributed content based indexing and association of media clips for resource discovery, (4) transcoding and scheduling of multimedia resources for delivery any time and anywhere to disparate clients; from mobile wireless to high speed optical links.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130422</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130422</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n672" target="n673">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Infrastructure to Develop a Large Scale Experiment Testbed of Multi-modal Resources</data>
      <data key="e_abstract">EIA-0130422 &lt;br/&gt;Louiqa Raschid&lt;br/&gt;University of Maryland College Park&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Infrastructure to Develop a Large Scale Experiment Testbed of Multi-model Resources&lt;br/&gt;&lt;br/&gt;The use of the widely distributed collections of structured and unstructured information expressed in multiple languages or modalities provided by the Internet, requires production of scalable, robust algorithms for the discovery of replicated content, determination of delay or access latency of sources, and the confrontation of the inherently dynamic nature of the Internet.&lt;br/&gt;&lt;br/&gt;This project&apos;s objective is to establish a laboratory testbed providing a controlled environment that captures structural, content, and latency characteristics of the (publicly accessible) Web. This will stimulate collaboration between researchers whose interests range over natural language applications, language independent processing of scanned documents, analysis of video information sources, information retrieval, and wide area applications and resource discovery across heterogeneous servers. &lt;br/&gt;&lt;br/&gt;The testbed will support the development and testing of: (1) tools for broad-scale, cross-linguistic analysis and discovery of relevant information across languages and modalities, (2) cost models and access cost catalogs for wide area environments, reflecting the temporal variability in access latency, (3) distributed content based indexing and association of media clips for resource discovery, (4) transcoding and scheduling of multimedia resources for delivery any time and anywhere to disparate clients; from mobile wireless to high speed optical links.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130422</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130422</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n674" target="n675">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+SY:Mapping Meetings: Language Technology to make Sense of Human Interaction</data>
      <data key="e_abstract">Meetings are essential and ongoing processes in almost every enterprise. To record meetings is to provide a history of human interactions. However, two central challenges remain: (1) how to make sense of the group dynamics in those meetings and (2) how to search through a history of those interactions to find the information one may want. This research aims to develop automatic information processing systems based on the metaphor of a &quot;meeting map&quot;, a structured representation that supports the presentation of multiple views of a meeting at different scales. The project will focus on two broad map categories: content maps, portraying topics discussed and decisions made; and interaction maps, identifying the roles and relationships of the participants and the level of concurrence. Building content and interaction maps will involve automatic classification of information from topic changes and salience to disagreement/consensus. These maps will be used for generating simple indicative summaries, and off-the-shelf visualization tools will be used for map presentation. The project will build on analyses of 100 hours of meetings. Evaluations will use objective recognition accuracies and expert assessments of automatic summaries. Meeting maps respect the diversity of information present in meeting scenarios, and provide effective support for human-to-human interactions.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121396</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121396</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n674" target="n676">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+SY:Mapping Meetings: Language Technology to make Sense of Human Interaction</data>
      <data key="e_abstract">Meetings are essential and ongoing processes in almost every enterprise. To record meetings is to provide a history of human interactions. However, two central challenges remain: (1) how to make sense of the group dynamics in those meetings and (2) how to search through a history of those interactions to find the information one may want. This research aims to develop automatic information processing systems based on the metaphor of a &quot;meeting map&quot;, a structured representation that supports the presentation of multiple views of a meeting at different scales. The project will focus on two broad map categories: content maps, portraying topics discussed and decisions made; and interaction maps, identifying the roles and relationships of the participants and the level of concurrence. Building content and interaction maps will involve automatic classification of information from topic changes and salience to disagreement/consensus. These maps will be used for generating simple indicative summaries, and off-the-shelf visualization tools will be used for map presentation. The project will build on analyses of 100 hours of meetings. Evaluations will use objective recognition accuracies and expert assessments of automatic summaries. Meeting maps respect the diversity of information present in meeting scenarios, and provide effective support for human-to-human interactions.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121396</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121396</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n674" target="n677">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+SY:Mapping Meetings: Language Technology to make Sense of Human Interaction</data>
      <data key="e_abstract">Meetings are essential and ongoing processes in almost every enterprise. To record meetings is to provide a history of human interactions. However, two central challenges remain: (1) how to make sense of the group dynamics in those meetings and (2) how to search through a history of those interactions to find the information one may want. This research aims to develop automatic information processing systems based on the metaphor of a &quot;meeting map&quot;, a structured representation that supports the presentation of multiple views of a meeting at different scales. The project will focus on two broad map categories: content maps, portraying topics discussed and decisions made; and interaction maps, identifying the roles and relationships of the participants and the level of concurrence. Building content and interaction maps will involve automatic classification of information from topic changes and salience to disagreement/consensus. These maps will be used for generating simple indicative summaries, and off-the-shelf visualization tools will be used for map presentation. The project will build on analyses of 100 hours of meetings. Evaluations will use objective recognition accuracies and expert assessments of automatic summaries. Meeting maps respect the diversity of information present in meeting scenarios, and provide effective support for human-to-human interactions.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121396</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121396</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n674" target="n678">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+SY:Mapping Meetings: Language Technology to make Sense of Human Interaction</data>
      <data key="e_abstract">Meetings are essential and ongoing processes in almost every enterprise. To record meetings is to provide a history of human interactions. However, two central challenges remain: (1) how to make sense of the group dynamics in those meetings and (2) how to search through a history of those interactions to find the information one may want. This research aims to develop automatic information processing systems based on the metaphor of a &quot;meeting map&quot;, a structured representation that supports the presentation of multiple views of a meeting at different scales. The project will focus on two broad map categories: content maps, portraying topics discussed and decisions made; and interaction maps, identifying the roles and relationships of the participants and the level of concurrence. Building content and interaction maps will involve automatic classification of information from topic changes and salience to disagreement/consensus. These maps will be used for generating simple indicative summaries, and off-the-shelf visualization tools will be used for map presentation. The project will build on analyses of 100 hours of meetings. Evaluations will use objective recognition accuracies and expert assessments of automatic summaries. Meeting maps respect the diversity of information present in meeting scenarios, and provide effective support for human-to-human interactions.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121396</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121396</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n675" target="n676">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+SY:Mapping Meetings: Language Technology to make Sense of Human Interaction</data>
      <data key="e_abstract">Meetings are essential and ongoing processes in almost every enterprise. To record meetings is to provide a history of human interactions. However, two central challenges remain: (1) how to make sense of the group dynamics in those meetings and (2) how to search through a history of those interactions to find the information one may want. This research aims to develop automatic information processing systems based on the metaphor of a &quot;meeting map&quot;, a structured representation that supports the presentation of multiple views of a meeting at different scales. The project will focus on two broad map categories: content maps, portraying topics discussed and decisions made; and interaction maps, identifying the roles and relationships of the participants and the level of concurrence. Building content and interaction maps will involve automatic classification of information from topic changes and salience to disagreement/consensus. These maps will be used for generating simple indicative summaries, and off-the-shelf visualization tools will be used for map presentation. The project will build on analyses of 100 hours of meetings. Evaluations will use objective recognition accuracies and expert assessments of automatic summaries. Meeting maps respect the diversity of information present in meeting scenarios, and provide effective support for human-to-human interactions.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121396</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121396</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n675" target="n677">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+SY:Mapping Meetings: Language Technology to make Sense of Human Interaction</data>
      <data key="e_abstract">Meetings are essential and ongoing processes in almost every enterprise. To record meetings is to provide a history of human interactions. However, two central challenges remain: (1) how to make sense of the group dynamics in those meetings and (2) how to search through a history of those interactions to find the information one may want. This research aims to develop automatic information processing systems based on the metaphor of a &quot;meeting map&quot;, a structured representation that supports the presentation of multiple views of a meeting at different scales. The project will focus on two broad map categories: content maps, portraying topics discussed and decisions made; and interaction maps, identifying the roles and relationships of the participants and the level of concurrence. Building content and interaction maps will involve automatic classification of information from topic changes and salience to disagreement/consensus. These maps will be used for generating simple indicative summaries, and off-the-shelf visualization tools will be used for map presentation. The project will build on analyses of 100 hours of meetings. Evaluations will use objective recognition accuracies and expert assessments of automatic summaries. Meeting maps respect the diversity of information present in meeting scenarios, and provide effective support for human-to-human interactions.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121396</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121396</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n675" target="n678">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+SY:Mapping Meetings: Language Technology to make Sense of Human Interaction</data>
      <data key="e_abstract">Meetings are essential and ongoing processes in almost every enterprise. To record meetings is to provide a history of human interactions. However, two central challenges remain: (1) how to make sense of the group dynamics in those meetings and (2) how to search through a history of those interactions to find the information one may want. This research aims to develop automatic information processing systems based on the metaphor of a &quot;meeting map&quot;, a structured representation that supports the presentation of multiple views of a meeting at different scales. The project will focus on two broad map categories: content maps, portraying topics discussed and decisions made; and interaction maps, identifying the roles and relationships of the participants and the level of concurrence. Building content and interaction maps will involve automatic classification of information from topic changes and salience to disagreement/consensus. These maps will be used for generating simple indicative summaries, and off-the-shelf visualization tools will be used for map presentation. The project will build on analyses of 100 hours of meetings. Evaluations will use objective recognition accuracies and expert assessments of automatic summaries. Meeting maps respect the diversity of information present in meeting scenarios, and provide effective support for human-to-human interactions.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121396</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121396</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n676" target="n677">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+SY:Mapping Meetings: Language Technology to make Sense of Human Interaction</data>
      <data key="e_abstract">Meetings are essential and ongoing processes in almost every enterprise. To record meetings is to provide a history of human interactions. However, two central challenges remain: (1) how to make sense of the group dynamics in those meetings and (2) how to search through a history of those interactions to find the information one may want. This research aims to develop automatic information processing systems based on the metaphor of a &quot;meeting map&quot;, a structured representation that supports the presentation of multiple views of a meeting at different scales. The project will focus on two broad map categories: content maps, portraying topics discussed and decisions made; and interaction maps, identifying the roles and relationships of the participants and the level of concurrence. Building content and interaction maps will involve automatic classification of information from topic changes and salience to disagreement/consensus. These maps will be used for generating simple indicative summaries, and off-the-shelf visualization tools will be used for map presentation. The project will build on analyses of 100 hours of meetings. Evaluations will use objective recognition accuracies and expert assessments of automatic summaries. Meeting maps respect the diversity of information present in meeting scenarios, and provide effective support for human-to-human interactions.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121396</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121396</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n676" target="n678">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+SY:Mapping Meetings: Language Technology to make Sense of Human Interaction</data>
      <data key="e_abstract">Meetings are essential and ongoing processes in almost every enterprise. To record meetings is to provide a history of human interactions. However, two central challenges remain: (1) how to make sense of the group dynamics in those meetings and (2) how to search through a history of those interactions to find the information one may want. This research aims to develop automatic information processing systems based on the metaphor of a &quot;meeting map&quot;, a structured representation that supports the presentation of multiple views of a meeting at different scales. The project will focus on two broad map categories: content maps, portraying topics discussed and decisions made; and interaction maps, identifying the roles and relationships of the participants and the level of concurrence. Building content and interaction maps will involve automatic classification of information from topic changes and salience to disagreement/consensus. These maps will be used for generating simple indicative summaries, and off-the-shelf visualization tools will be used for map presentation. The project will build on analyses of 100 hours of meetings. Evaluations will use objective recognition accuracies and expert assessments of automatic summaries. Meeting maps respect the diversity of information present in meeting scenarios, and provide effective support for human-to-human interactions.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121396</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121396</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n677" target="n678">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+SY:Mapping Meetings: Language Technology to make Sense of Human Interaction</data>
      <data key="e_abstract">Meetings are essential and ongoing processes in almost every enterprise. To record meetings is to provide a history of human interactions. However, two central challenges remain: (1) how to make sense of the group dynamics in those meetings and (2) how to search through a history of those interactions to find the information one may want. This research aims to develop automatic information processing systems based on the metaphor of a &quot;meeting map&quot;, a structured representation that supports the presentation of multiple views of a meeting at different scales. The project will focus on two broad map categories: content maps, portraying topics discussed and decisions made; and interaction maps, identifying the roles and relationships of the participants and the level of concurrence. Building content and interaction maps will involve automatic classification of information from topic changes and salience to disagreement/consensus. These maps will be used for generating simple indicative summaries, and off-the-shelf visualization tools will be used for map presentation. The project will build on analyses of 100 hours of meetings. Evaluations will use objective recognition accuracies and expert assessments of automatic summaries. Meeting maps respect the diversity of information present in meeting scenarios, and provide effective support for human-to-human interactions.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121396</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121396</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n34" target="n97">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Graph-Based Data Mining</data>
      <data key="e_abstract">With the increasing amount and complexity of today&apos;s data, there is an urgent need to accelerate the development of knowledge discovery and concept learning methods for mining large databases. Furthermore, much of this data is structural in nature, or is composed of entities and relationships between those entities. Hence, there exists a need to develop scalable methods for discovering new knowledge in structural databases. The main objective of this project is to investigate and implement new methods for performing knowledge discovery and concept learning on structural databases represented as graphs. This work builds upon existing methods for graph-based knowledge discovery implemented in the Subdue structural discovery system. The graph-based discovery algorithm is extended to perform structural concept learning and structural, hierarchical conceptual clustering. To achieve greater scalability, database management techniques are integrated into the graph-based discovery and learning processes. One targeted application is the use of Subdue as the core of a structural Web seach engine. Domain experts provide guidance and feedback on applications to molecular biology, geology, telecommunications, and software engineering. Achievement of the above objectives impacts the ability to automatically extract useful knowledge from the ever-increasing amount of data. By disseminating the Subdue discovery algorithm, databases, and discovered results over the Internet, scientists in all areas benefit from similar analyses of their own databases. Through integration of our research ideas into classes taught at UTA and into student research, this project impacts education at UTA and at other universities.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">97517</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">97517</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n35" target="n97">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Graph-Based Data Mining</data>
      <data key="e_abstract">With the increasing amount and complexity of today&apos;s data, there is an urgent need to accelerate the development of knowledge discovery and concept learning methods for mining large databases. Furthermore, much of this data is structural in nature, or is composed of entities and relationships between those entities. Hence, there exists a need to develop scalable methods for discovering new knowledge in structural databases. The main objective of this project is to investigate and implement new methods for performing knowledge discovery and concept learning on structural databases represented as graphs. This work builds upon existing methods for graph-based knowledge discovery implemented in the Subdue structural discovery system. The graph-based discovery algorithm is extended to perform structural concept learning and structural, hierarchical conceptual clustering. To achieve greater scalability, database management techniques are integrated into the graph-based discovery and learning processes. One targeted application is the use of Subdue as the core of a structural Web seach engine. Domain experts provide guidance and feedback on applications to molecular biology, geology, telecommunications, and software engineering. Achievement of the above objectives impacts the ability to automatically extract useful knowledge from the ever-increasing amount of data. By disseminating the Subdue discovery algorithm, databases, and discovered results over the Internet, scientists in all areas benefit from similar analyses of their own databases. Through integration of our research ideas into classes taught at UTA and into student research, this project impacts education at UTA and at other universities.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">97517</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">97517</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n34" target="n35">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Graph-Based Data Mining</data>
      <data key="e_abstract">With the increasing amount and complexity of today&apos;s data, there is an urgent need to accelerate the development of knowledge discovery and concept learning methods for mining large databases. Furthermore, much of this data is structural in nature, or is composed of entities and relationships between those entities. Hence, there exists a need to develop scalable methods for discovering new knowledge in structural databases. The main objective of this project is to investigate and implement new methods for performing knowledge discovery and concept learning on structural databases represented as graphs. This work builds upon existing methods for graph-based knowledge discovery implemented in the Subdue structural discovery system. The graph-based discovery algorithm is extended to perform structural concept learning and structural, hierarchical conceptual clustering. To achieve greater scalability, database management techniques are integrated into the graph-based discovery and learning processes. One targeted application is the use of Subdue as the core of a structural Web seach engine. Domain experts provide guidance and feedback on applications to molecular biology, geology, telecommunications, and software engineering. Achievement of the above objectives impacts the ability to automatically extract useful knowledge from the ever-increasing amount of data. By disseminating the Subdue discovery algorithm, databases, and discovered results over the Internet, scientists in all areas benefit from similar analyses of their own databases. Through integration of our research ideas into classes taught at UTA and into student research, this project impacts education at UTA and at other universities.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">97517</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">97517</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n683" target="n684">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY: Augmented Cognition: Combining Human and Digital Memory</data>
      <data key="e_abstract">The PI will design and build human-computer interfaces that improve human memory, which he terms infocockpits. The basic approach is to take well-understood psychology principles and apply them to the design of information displays, in particular the fact that human beings are adept at remembering information based on its location relative to their body and on the place where they were when they learned it. The implementations will use two basic strategies: multiple spatial displays surrounding the user, to engage human memory for location; and ambient context displays (both visual and auditory), to engage human memory for place. This work leverages prior collaborative efforts by the PI and Co-PI in virtual reality. The project relates to, and represents a new paradigm for, the retention of information instead of its manipulation. Although the PIs postulate just two fundamental design principles, the design space is very large. The basic research apparatus - the infocockpit - will be a computer system with a number of traditional display monitors arrayed relative to the user&apos;s body. These display screens are then placed in a room where images can be projected onto the walls. This projected imagery, plus ambient 3D surround sound, creates a distinctive place in which information is viewed. The PI will systematically vary the configuration, and will run controlled experiments, in which users access information and then later are tested for their ability to recall it, to examine the benefits of: multiple monitors arrayed around the user vs. a single monitor; the addition of projected background context; stationary vs. animated background contexts; having the context semantically related to the accessed information; having ambient and/or localized sound as part of the context; and using hierarchical places to avoid having confusions between many different contexts. The PI will partner with the Virginia Center for Digital History which will employ the new design principles to build and provide content for infocockpit systems for teaching American history; this will allow the PI to observe how the new techniques work in a real world educational application developed by others. A preliminary study conducted by the PI has found a 63% increase in users&apos; memory capabilities; moreover, functional brain imaging assessment of the participants from this study and found that experience in the infocockpit resulted in brain activations in areas associated with spatial representation, working memory, and visualization. Thus, the question is no longer whether this approach can improve the user&apos;s ability to remember information; the question is by how much.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121629</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121629</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n689" target="n690">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics - BDEI -Quantifying Forest Ground Flora Biomass and Diversity Using Close-Range Remote Sensing</data>
      <data key="e_abstract">EIA-0131801&lt;br/&gt;Doruska, Paul &lt;br/&gt;University of Arkansas&lt;br/&gt;&lt;br/&gt;BDEI: Quantifying Forest Ground Flora Biomass and Diversity Using Close-Range Remote Sensing.&lt;br/&gt;&lt;br/&gt;Summary&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Researchers propose the use of close-range remote sensing to estimate biomass per&lt;br/&gt;unit area and species diversity of forest ground flora. Ground flora biomass and diversity&lt;br/&gt;information is useful when describing forested ecosystems, gauging results in scientific&lt;br/&gt;experiments and determining the need for herbicide application to reduce unwanted&lt;br/&gt;competition in forest crops. Scientists will use both color and color infrared digital&lt;br/&gt;imagery to study forest ground flora biomass and species diversity of pine stands in&lt;br/&gt;southeastern Arkansas and mixed pine-hardwood stands in the Ouachita Mountains in&lt;br/&gt;central Arkansas.&lt;br/&gt;&lt;br/&gt;Airborne and/or space-borne imagery have been used to estimate forest overstory&lt;br/&gt;biomass. However, such platforms cannot be used to estimate biomass of forest ground&lt;br/&gt;flora species because (a.) spatial resolution is too coarse for the required level of detail,&lt;br/&gt;and (b.) the presence of the overstory canopy can hinder use of such imagery. The&lt;br/&gt;objectives of this project are to (1) determine if these metrics can be collected through&lt;br/&gt;close-range remote sensing, and (2) identify spectral signatures for a number of species&lt;br/&gt;of forest ground flora.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">131801</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">131801</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n700" target="n701">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">CISE Research Infrastructure: The Metaverse: A Laboratory for Digital Media Networks</data>
      <data key="e_abstract">EIA-0101242&lt;br/&gt;James Griffioen&lt;br/&gt;University of Kentucky&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: The Metaverse: A Laboratory for Digital Media Networks&lt;br/&gt;&lt;br/&gt;The primary goal of our research is to investigate and develop new techniques to support networked, collaborative, visually immersive environments and applications. The objective is to design visually compelling collaborative spaces (where people interact with computer simulations and each other) that are inexpensive, extensible, automatically configurable, adaptable, and scalable. The work involves an interdisciplinary team of researchers exploring system-level issues including visualization, network communication, and computer vision, with others studying application-level issues such as scientific (CFD) visualization, presentation of new-media/art, and the educational &lt;br/&gt;efficacy and impact of the technology.&lt;br/&gt;&lt;br/&gt;The requested infrastructure will be used to create three physically separate, networked visualization laboratories supported by two new technical staff. Each visualization environment will have a distinct configuration and objective. The CORE (COllaborative Rendering Environment) will explore compelling collaborative immersive spaces, while the VIDE (Visually Immersive Display Environment) investigates stereo visualization. Unlike the CORE and VIDE, where users are immersed in pixels, the DOME (Digital Object Media Environment) will be a head-tracked ``outside looking in&apos;&apos; configuration. The environments will demonstrate the versatility of the underlying base technology by using the same techniques to support multiple application domains. A fourth environment will be deployed at the University of Puerto Rico.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101242</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101242</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n517" target="n700">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">CISE Research Infrastructure: The Metaverse: A Laboratory for Digital Media Networks</data>
      <data key="e_abstract">EIA-0101242&lt;br/&gt;James Griffioen&lt;br/&gt;University of Kentucky&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: The Metaverse: A Laboratory for Digital Media Networks&lt;br/&gt;&lt;br/&gt;The primary goal of our research is to investigate and develop new techniques to support networked, collaborative, visually immersive environments and applications. The objective is to design visually compelling collaborative spaces (where people interact with computer simulations and each other) that are inexpensive, extensible, automatically configurable, adaptable, and scalable. The work involves an interdisciplinary team of researchers exploring system-level issues including visualization, network communication, and computer vision, with others studying application-level issues such as scientific (CFD) visualization, presentation of new-media/art, and the educational &lt;br/&gt;efficacy and impact of the technology.&lt;br/&gt;&lt;br/&gt;The requested infrastructure will be used to create three physically separate, networked visualization laboratories supported by two new technical staff. Each visualization environment will have a distinct configuration and objective. The CORE (COllaborative Rendering Environment) will explore compelling collaborative immersive spaces, while the VIDE (Visually Immersive Display Environment) investigates stereo visualization. Unlike the CORE and VIDE, where users are immersed in pixels, the DOME (Digital Object Media Environment) will be a head-tracked ``outside looking in&apos;&apos; configuration. The environments will demonstrate the versatility of the underlying base technology by using the same techniques to support multiple application domains. A fourth environment will be deployed at the University of Puerto Rico.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101242</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101242</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n700" target="n703">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">CISE Research Infrastructure: The Metaverse: A Laboratory for Digital Media Networks</data>
      <data key="e_abstract">EIA-0101242&lt;br/&gt;James Griffioen&lt;br/&gt;University of Kentucky&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: The Metaverse: A Laboratory for Digital Media Networks&lt;br/&gt;&lt;br/&gt;The primary goal of our research is to investigate and develop new techniques to support networked, collaborative, visually immersive environments and applications. The objective is to design visually compelling collaborative spaces (where people interact with computer simulations and each other) that are inexpensive, extensible, automatically configurable, adaptable, and scalable. The work involves an interdisciplinary team of researchers exploring system-level issues including visualization, network communication, and computer vision, with others studying application-level issues such as scientific (CFD) visualization, presentation of new-media/art, and the educational &lt;br/&gt;efficacy and impact of the technology.&lt;br/&gt;&lt;br/&gt;The requested infrastructure will be used to create three physically separate, networked visualization laboratories supported by two new technical staff. Each visualization environment will have a distinct configuration and objective. The CORE (COllaborative Rendering Environment) will explore compelling collaborative immersive spaces, while the VIDE (Visually Immersive Display Environment) investigates stereo visualization. Unlike the CORE and VIDE, where users are immersed in pixels, the DOME (Digital Object Media Environment) will be a head-tracked ``outside looking in&apos;&apos; configuration. The environments will demonstrate the versatility of the underlying base technology by using the same techniques to support multiple application domains. A fourth environment will be deployed at the University of Puerto Rico.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101242</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101242</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n446" target="n700">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">CISE Research Infrastructure: The Metaverse: A Laboratory for Digital Media Networks</data>
      <data key="e_abstract">EIA-0101242&lt;br/&gt;James Griffioen&lt;br/&gt;University of Kentucky&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: The Metaverse: A Laboratory for Digital Media Networks&lt;br/&gt;&lt;br/&gt;The primary goal of our research is to investigate and develop new techniques to support networked, collaborative, visually immersive environments and applications. The objective is to design visually compelling collaborative spaces (where people interact with computer simulations and each other) that are inexpensive, extensible, automatically configurable, adaptable, and scalable. The work involves an interdisciplinary team of researchers exploring system-level issues including visualization, network communication, and computer vision, with others studying application-level issues such as scientific (CFD) visualization, presentation of new-media/art, and the educational &lt;br/&gt;efficacy and impact of the technology.&lt;br/&gt;&lt;br/&gt;The requested infrastructure will be used to create three physically separate, networked visualization laboratories supported by two new technical staff. Each visualization environment will have a distinct configuration and objective. The CORE (COllaborative Rendering Environment) will explore compelling collaborative immersive spaces, while the VIDE (Visually Immersive Display Environment) investigates stereo visualization. Unlike the CORE and VIDE, where users are immersed in pixels, the DOME (Digital Object Media Environment) will be a head-tracked ``outside looking in&apos;&apos; configuration. The environments will demonstrate the versatility of the underlying base technology by using the same techniques to support multiple application domains. A fourth environment will be deployed at the University of Puerto Rico.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101242</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101242</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n517" target="n701">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">CISE Research Infrastructure: The Metaverse: A Laboratory for Digital Media Networks</data>
      <data key="e_abstract">EIA-0101242&lt;br/&gt;James Griffioen&lt;br/&gt;University of Kentucky&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: The Metaverse: A Laboratory for Digital Media Networks&lt;br/&gt;&lt;br/&gt;The primary goal of our research is to investigate and develop new techniques to support networked, collaborative, visually immersive environments and applications. The objective is to design visually compelling collaborative spaces (where people interact with computer simulations and each other) that are inexpensive, extensible, automatically configurable, adaptable, and scalable. The work involves an interdisciplinary team of researchers exploring system-level issues including visualization, network communication, and computer vision, with others studying application-level issues such as scientific (CFD) visualization, presentation of new-media/art, and the educational &lt;br/&gt;efficacy and impact of the technology.&lt;br/&gt;&lt;br/&gt;The requested infrastructure will be used to create three physically separate, networked visualization laboratories supported by two new technical staff. Each visualization environment will have a distinct configuration and objective. The CORE (COllaborative Rendering Environment) will explore compelling collaborative immersive spaces, while the VIDE (Visually Immersive Display Environment) investigates stereo visualization. Unlike the CORE and VIDE, where users are immersed in pixels, the DOME (Digital Object Media Environment) will be a head-tracked ``outside looking in&apos;&apos; configuration. The environments will demonstrate the versatility of the underlying base technology by using the same techniques to support multiple application domains. A fourth environment will be deployed at the University of Puerto Rico.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101242</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101242</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n701" target="n703">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">CISE Research Infrastructure: The Metaverse: A Laboratory for Digital Media Networks</data>
      <data key="e_abstract">EIA-0101242&lt;br/&gt;James Griffioen&lt;br/&gt;University of Kentucky&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: The Metaverse: A Laboratory for Digital Media Networks&lt;br/&gt;&lt;br/&gt;The primary goal of our research is to investigate and develop new techniques to support networked, collaborative, visually immersive environments and applications. The objective is to design visually compelling collaborative spaces (where people interact with computer simulations and each other) that are inexpensive, extensible, automatically configurable, adaptable, and scalable. The work involves an interdisciplinary team of researchers exploring system-level issues including visualization, network communication, and computer vision, with others studying application-level issues such as scientific (CFD) visualization, presentation of new-media/art, and the educational &lt;br/&gt;efficacy and impact of the technology.&lt;br/&gt;&lt;br/&gt;The requested infrastructure will be used to create three physically separate, networked visualization laboratories supported by two new technical staff. Each visualization environment will have a distinct configuration and objective. The CORE (COllaborative Rendering Environment) will explore compelling collaborative immersive spaces, while the VIDE (Visually Immersive Display Environment) investigates stereo visualization. Unlike the CORE and VIDE, where users are immersed in pixels, the DOME (Digital Object Media Environment) will be a head-tracked ``outside looking in&apos;&apos; configuration. The environments will demonstrate the versatility of the underlying base technology by using the same techniques to support multiple application domains. A fourth environment will be deployed at the University of Puerto Rico.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101242</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101242</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n446" target="n701">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">CISE Research Infrastructure: The Metaverse: A Laboratory for Digital Media Networks</data>
      <data key="e_abstract">EIA-0101242&lt;br/&gt;James Griffioen&lt;br/&gt;University of Kentucky&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: The Metaverse: A Laboratory for Digital Media Networks&lt;br/&gt;&lt;br/&gt;The primary goal of our research is to investigate and develop new techniques to support networked, collaborative, visually immersive environments and applications. The objective is to design visually compelling collaborative spaces (where people interact with computer simulations and each other) that are inexpensive, extensible, automatically configurable, adaptable, and scalable. The work involves an interdisciplinary team of researchers exploring system-level issues including visualization, network communication, and computer vision, with others studying application-level issues such as scientific (CFD) visualization, presentation of new-media/art, and the educational &lt;br/&gt;efficacy and impact of the technology.&lt;br/&gt;&lt;br/&gt;The requested infrastructure will be used to create three physically separate, networked visualization laboratories supported by two new technical staff. Each visualization environment will have a distinct configuration and objective. The CORE (COllaborative Rendering Environment) will explore compelling collaborative immersive spaces, while the VIDE (Visually Immersive Display Environment) investigates stereo visualization. Unlike the CORE and VIDE, where users are immersed in pixels, the DOME (Digital Object Media Environment) will be a head-tracked ``outside looking in&apos;&apos; configuration. The environments will demonstrate the versatility of the underlying base technology by using the same techniques to support multiple application domains. A fourth environment will be deployed at the University of Puerto Rico.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101242</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101242</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n517" target="n703">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">CISE Research Infrastructure: The Metaverse: A Laboratory for Digital Media Networks</data>
      <data key="e_abstract">EIA-0101242&lt;br/&gt;James Griffioen&lt;br/&gt;University of Kentucky&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: The Metaverse: A Laboratory for Digital Media Networks&lt;br/&gt;&lt;br/&gt;The primary goal of our research is to investigate and develop new techniques to support networked, collaborative, visually immersive environments and applications. The objective is to design visually compelling collaborative spaces (where people interact with computer simulations and each other) that are inexpensive, extensible, automatically configurable, adaptable, and scalable. The work involves an interdisciplinary team of researchers exploring system-level issues including visualization, network communication, and computer vision, with others studying application-level issues such as scientific (CFD) visualization, presentation of new-media/art, and the educational &lt;br/&gt;efficacy and impact of the technology.&lt;br/&gt;&lt;br/&gt;The requested infrastructure will be used to create three physically separate, networked visualization laboratories supported by two new technical staff. Each visualization environment will have a distinct configuration and objective. The CORE (COllaborative Rendering Environment) will explore compelling collaborative immersive spaces, while the VIDE (Visually Immersive Display Environment) investigates stereo visualization. Unlike the CORE and VIDE, where users are immersed in pixels, the DOME (Digital Object Media Environment) will be a head-tracked ``outside looking in&apos;&apos; configuration. The environments will demonstrate the versatility of the underlying base technology by using the same techniques to support multiple application domains. A fourth environment will be deployed at the University of Puerto Rico.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101242</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101242</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n446" target="n517">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">CISE Research Infrastructure: The Metaverse: A Laboratory for Digital Media Networks</data>
      <data key="e_abstract">EIA-0101242&lt;br/&gt;James Griffioen&lt;br/&gt;University of Kentucky&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: The Metaverse: A Laboratory for Digital Media Networks&lt;br/&gt;&lt;br/&gt;The primary goal of our research is to investigate and develop new techniques to support networked, collaborative, visually immersive environments and applications. The objective is to design visually compelling collaborative spaces (where people interact with computer simulations and each other) that are inexpensive, extensible, automatically configurable, adaptable, and scalable. The work involves an interdisciplinary team of researchers exploring system-level issues including visualization, network communication, and computer vision, with others studying application-level issues such as scientific (CFD) visualization, presentation of new-media/art, and the educational &lt;br/&gt;efficacy and impact of the technology.&lt;br/&gt;&lt;br/&gt;The requested infrastructure will be used to create three physically separate, networked visualization laboratories supported by two new technical staff. Each visualization environment will have a distinct configuration and objective. The CORE (COllaborative Rendering Environment) will explore compelling collaborative immersive spaces, while the VIDE (Visually Immersive Display Environment) investigates stereo visualization. Unlike the CORE and VIDE, where users are immersed in pixels, the DOME (Digital Object Media Environment) will be a head-tracked ``outside looking in&apos;&apos; configuration. The environments will demonstrate the versatility of the underlying base technology by using the same techniques to support multiple application domains. A fourth environment will be deployed at the University of Puerto Rico.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101242</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101242</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n446" target="n703">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">CISE Research Infrastructure: The Metaverse: A Laboratory for Digital Media Networks</data>
      <data key="e_abstract">EIA-0101242&lt;br/&gt;James Griffioen&lt;br/&gt;University of Kentucky&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: The Metaverse: A Laboratory for Digital Media Networks&lt;br/&gt;&lt;br/&gt;The primary goal of our research is to investigate and develop new techniques to support networked, collaborative, visually immersive environments and applications. The objective is to design visually compelling collaborative spaces (where people interact with computer simulations and each other) that are inexpensive, extensible, automatically configurable, adaptable, and scalable. The work involves an interdisciplinary team of researchers exploring system-level issues including visualization, network communication, and computer vision, with others studying application-level issues such as scientific (CFD) visualization, presentation of new-media/art, and the educational &lt;br/&gt;efficacy and impact of the technology.&lt;br/&gt;&lt;br/&gt;The requested infrastructure will be used to create three physically separate, networked visualization laboratories supported by two new technical staff. Each visualization environment will have a distinct configuration and objective. The CORE (COllaborative Rendering Environment) will explore compelling collaborative immersive spaces, while the VIDE (Visually Immersive Display Environment) investigates stereo visualization. Unlike the CORE and VIDE, where users are immersed in pixels, the DOME (Digital Object Media Environment) will be a head-tracked ``outside looking in&apos;&apos; configuration. The environments will demonstrate the versatility of the underlying base technology by using the same techniques to support multiple application domains. A fourth environment will be deployed at the University of Puerto Rico.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101242</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101242</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n705" target="n706">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">CISE Research Infrastructure: MultiStore: A Research Infrastructure for Management, Analysis and Visualization of Large-Scale Multidimensional Data Sets</data>
      <data key="e_abstract">EIA-0101244&lt;br/&gt;Aidong Zhang&lt;br/&gt;SUNY at Buffalo&lt;br/&gt;&lt;br/&gt;MultiStore: A Research Infrastructure for Management, Analysis and Visualization of Large-Scale Multi-dimensional Data Sets&lt;br/&gt;&lt;br/&gt;This project establishes a research infrastructure (MultiStore) for supporting integrated research in specific targeted areas of Computer Science, including Multimedia, visualization, Geographical Information Systems (GIS) and Bioinformatics. The research objective is to develop computational theories and algorithms for storing, managing, analyzing, querying and visualizing multi-dimensional data sets that are generated from the related fields. The research components include: (1) Data storage and management. We develop approaches to manage large-scale multi-dimensional data sets. Particular research issues include: multi-dimensional data storage, indexing, and clustering. (2) Data visualization. We develop effective graphics and visualization techniques that can help the user in information processing tasks. Particular research topics addressed include graph visualization and detecting clusters in a multidimensional data set through visualization. The visualization tools will be used in biomedical image understanding and analysis. (3) Data analysis and querying. We focus on geographical image understanding, analysis and querying. The particular research issues include geographical metadata/knowledge extraction, geographical metadata/knowledge representation and management, and geographical metadata/knowledge querying. (4) Data mining and bioinformatics. We develop data mining techniques for determination of protein structures and detection of gene expression patterns. Through these research activities, the fundamental understanding and novel techniques will be provided to support the management of various large-scale multi-dimensional data sets.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101244</data>
      <data key="e_expirationDate">2008-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101244</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n705" target="n707">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">CISE Research Infrastructure: MultiStore: A Research Infrastructure for Management, Analysis and Visualization of Large-Scale Multidimensional Data Sets</data>
      <data key="e_abstract">EIA-0101244&lt;br/&gt;Aidong Zhang&lt;br/&gt;SUNY at Buffalo&lt;br/&gt;&lt;br/&gt;MultiStore: A Research Infrastructure for Management, Analysis and Visualization of Large-Scale Multi-dimensional Data Sets&lt;br/&gt;&lt;br/&gt;This project establishes a research infrastructure (MultiStore) for supporting integrated research in specific targeted areas of Computer Science, including Multimedia, visualization, Geographical Information Systems (GIS) and Bioinformatics. The research objective is to develop computational theories and algorithms for storing, managing, analyzing, querying and visualizing multi-dimensional data sets that are generated from the related fields. The research components include: (1) Data storage and management. We develop approaches to manage large-scale multi-dimensional data sets. Particular research issues include: multi-dimensional data storage, indexing, and clustering. (2) Data visualization. We develop effective graphics and visualization techniques that can help the user in information processing tasks. Particular research topics addressed include graph visualization and detecting clusters in a multidimensional data set through visualization. The visualization tools will be used in biomedical image understanding and analysis. (3) Data analysis and querying. We focus on geographical image understanding, analysis and querying. The particular research issues include geographical metadata/knowledge extraction, geographical metadata/knowledge representation and management, and geographical metadata/knowledge querying. (4) Data mining and bioinformatics. We develop data mining techniques for determination of protein structures and detection of gene expression patterns. Through these research activities, the fundamental understanding and novel techniques will be provided to support the management of various large-scale multi-dimensional data sets.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101244</data>
      <data key="e_expirationDate">2008-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101244</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n273" target="n705">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">CISE Research Infrastructure: MultiStore: A Research Infrastructure for Management, Analysis and Visualization of Large-Scale Multidimensional Data Sets</data>
      <data key="e_abstract">EIA-0101244&lt;br/&gt;Aidong Zhang&lt;br/&gt;SUNY at Buffalo&lt;br/&gt;&lt;br/&gt;MultiStore: A Research Infrastructure for Management, Analysis and Visualization of Large-Scale Multi-dimensional Data Sets&lt;br/&gt;&lt;br/&gt;This project establishes a research infrastructure (MultiStore) for supporting integrated research in specific targeted areas of Computer Science, including Multimedia, visualization, Geographical Information Systems (GIS) and Bioinformatics. The research objective is to develop computational theories and algorithms for storing, managing, analyzing, querying and visualizing multi-dimensional data sets that are generated from the related fields. The research components include: (1) Data storage and management. We develop approaches to manage large-scale multi-dimensional data sets. Particular research issues include: multi-dimensional data storage, indexing, and clustering. (2) Data visualization. We develop effective graphics and visualization techniques that can help the user in information processing tasks. Particular research topics addressed include graph visualization and detecting clusters in a multidimensional data set through visualization. The visualization tools will be used in biomedical image understanding and analysis. (3) Data analysis and querying. We focus on geographical image understanding, analysis and querying. The particular research issues include geographical metadata/knowledge extraction, geographical metadata/knowledge representation and management, and geographical metadata/knowledge querying. (4) Data mining and bioinformatics. We develop data mining techniques for determination of protein structures and detection of gene expression patterns. Through these research activities, the fundamental understanding and novel techniques will be provided to support the management of various large-scale multi-dimensional data sets.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101244</data>
      <data key="e_expirationDate">2008-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101244</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n705" target="n709">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">CISE Research Infrastructure: MultiStore: A Research Infrastructure for Management, Analysis and Visualization of Large-Scale Multidimensional Data Sets</data>
      <data key="e_abstract">EIA-0101244&lt;br/&gt;Aidong Zhang&lt;br/&gt;SUNY at Buffalo&lt;br/&gt;&lt;br/&gt;MultiStore: A Research Infrastructure for Management, Analysis and Visualization of Large-Scale Multi-dimensional Data Sets&lt;br/&gt;&lt;br/&gt;This project establishes a research infrastructure (MultiStore) for supporting integrated research in specific targeted areas of Computer Science, including Multimedia, visualization, Geographical Information Systems (GIS) and Bioinformatics. The research objective is to develop computational theories and algorithms for storing, managing, analyzing, querying and visualizing multi-dimensional data sets that are generated from the related fields. The research components include: (1) Data storage and management. We develop approaches to manage large-scale multi-dimensional data sets. Particular research issues include: multi-dimensional data storage, indexing, and clustering. (2) Data visualization. We develop effective graphics and visualization techniques that can help the user in information processing tasks. Particular research topics addressed include graph visualization and detecting clusters in a multidimensional data set through visualization. The visualization tools will be used in biomedical image understanding and analysis. (3) Data analysis and querying. We focus on geographical image understanding, analysis and querying. The particular research issues include geographical metadata/knowledge extraction, geographical metadata/knowledge representation and management, and geographical metadata/knowledge querying. (4) Data mining and bioinformatics. We develop data mining techniques for determination of protein structures and detection of gene expression patterns. Through these research activities, the fundamental understanding and novel techniques will be provided to support the management of various large-scale multi-dimensional data sets.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101244</data>
      <data key="e_expirationDate">2008-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101244</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n706" target="n707">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">CISE Research Infrastructure: MultiStore: A Research Infrastructure for Management, Analysis and Visualization of Large-Scale Multidimensional Data Sets</data>
      <data key="e_abstract">EIA-0101244&lt;br/&gt;Aidong Zhang&lt;br/&gt;SUNY at Buffalo&lt;br/&gt;&lt;br/&gt;MultiStore: A Research Infrastructure for Management, Analysis and Visualization of Large-Scale Multi-dimensional Data Sets&lt;br/&gt;&lt;br/&gt;This project establishes a research infrastructure (MultiStore) for supporting integrated research in specific targeted areas of Computer Science, including Multimedia, visualization, Geographical Information Systems (GIS) and Bioinformatics. The research objective is to develop computational theories and algorithms for storing, managing, analyzing, querying and visualizing multi-dimensional data sets that are generated from the related fields. The research components include: (1) Data storage and management. We develop approaches to manage large-scale multi-dimensional data sets. Particular research issues include: multi-dimensional data storage, indexing, and clustering. (2) Data visualization. We develop effective graphics and visualization techniques that can help the user in information processing tasks. Particular research topics addressed include graph visualization and detecting clusters in a multidimensional data set through visualization. The visualization tools will be used in biomedical image understanding and analysis. (3) Data analysis and querying. We focus on geographical image understanding, analysis and querying. The particular research issues include geographical metadata/knowledge extraction, geographical metadata/knowledge representation and management, and geographical metadata/knowledge querying. (4) Data mining and bioinformatics. We develop data mining techniques for determination of protein structures and detection of gene expression patterns. Through these research activities, the fundamental understanding and novel techniques will be provided to support the management of various large-scale multi-dimensional data sets.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101244</data>
      <data key="e_expirationDate">2008-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101244</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n273" target="n706">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">CISE Research Infrastructure: MultiStore: A Research Infrastructure for Management, Analysis and Visualization of Large-Scale Multidimensional Data Sets</data>
      <data key="e_abstract">EIA-0101244&lt;br/&gt;Aidong Zhang&lt;br/&gt;SUNY at Buffalo&lt;br/&gt;&lt;br/&gt;MultiStore: A Research Infrastructure for Management, Analysis and Visualization of Large-Scale Multi-dimensional Data Sets&lt;br/&gt;&lt;br/&gt;This project establishes a research infrastructure (MultiStore) for supporting integrated research in specific targeted areas of Computer Science, including Multimedia, visualization, Geographical Information Systems (GIS) and Bioinformatics. The research objective is to develop computational theories and algorithms for storing, managing, analyzing, querying and visualizing multi-dimensional data sets that are generated from the related fields. The research components include: (1) Data storage and management. We develop approaches to manage large-scale multi-dimensional data sets. Particular research issues include: multi-dimensional data storage, indexing, and clustering. (2) Data visualization. We develop effective graphics and visualization techniques that can help the user in information processing tasks. Particular research topics addressed include graph visualization and detecting clusters in a multidimensional data set through visualization. The visualization tools will be used in biomedical image understanding and analysis. (3) Data analysis and querying. We focus on geographical image understanding, analysis and querying. The particular research issues include geographical metadata/knowledge extraction, geographical metadata/knowledge representation and management, and geographical metadata/knowledge querying. (4) Data mining and bioinformatics. We develop data mining techniques for determination of protein structures and detection of gene expression patterns. Through these research activities, the fundamental understanding and novel techniques will be provided to support the management of various large-scale multi-dimensional data sets.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101244</data>
      <data key="e_expirationDate">2008-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101244</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n706" target="n709">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">CISE Research Infrastructure: MultiStore: A Research Infrastructure for Management, Analysis and Visualization of Large-Scale Multidimensional Data Sets</data>
      <data key="e_abstract">EIA-0101244&lt;br/&gt;Aidong Zhang&lt;br/&gt;SUNY at Buffalo&lt;br/&gt;&lt;br/&gt;MultiStore: A Research Infrastructure for Management, Analysis and Visualization of Large-Scale Multi-dimensional Data Sets&lt;br/&gt;&lt;br/&gt;This project establishes a research infrastructure (MultiStore) for supporting integrated research in specific targeted areas of Computer Science, including Multimedia, visualization, Geographical Information Systems (GIS) and Bioinformatics. The research objective is to develop computational theories and algorithms for storing, managing, analyzing, querying and visualizing multi-dimensional data sets that are generated from the related fields. The research components include: (1) Data storage and management. We develop approaches to manage large-scale multi-dimensional data sets. Particular research issues include: multi-dimensional data storage, indexing, and clustering. (2) Data visualization. We develop effective graphics and visualization techniques that can help the user in information processing tasks. Particular research topics addressed include graph visualization and detecting clusters in a multidimensional data set through visualization. The visualization tools will be used in biomedical image understanding and analysis. (3) Data analysis and querying. We focus on geographical image understanding, analysis and querying. The particular research issues include geographical metadata/knowledge extraction, geographical metadata/knowledge representation and management, and geographical metadata/knowledge querying. (4) Data mining and bioinformatics. We develop data mining techniques for determination of protein structures and detection of gene expression patterns. Through these research activities, the fundamental understanding and novel techniques will be provided to support the management of various large-scale multi-dimensional data sets.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101244</data>
      <data key="e_expirationDate">2008-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101244</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n273" target="n707">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">CISE Research Infrastructure: MultiStore: A Research Infrastructure for Management, Analysis and Visualization of Large-Scale Multidimensional Data Sets</data>
      <data key="e_abstract">EIA-0101244&lt;br/&gt;Aidong Zhang&lt;br/&gt;SUNY at Buffalo&lt;br/&gt;&lt;br/&gt;MultiStore: A Research Infrastructure for Management, Analysis and Visualization of Large-Scale Multi-dimensional Data Sets&lt;br/&gt;&lt;br/&gt;This project establishes a research infrastructure (MultiStore) for supporting integrated research in specific targeted areas of Computer Science, including Multimedia, visualization, Geographical Information Systems (GIS) and Bioinformatics. The research objective is to develop computational theories and algorithms for storing, managing, analyzing, querying and visualizing multi-dimensional data sets that are generated from the related fields. The research components include: (1) Data storage and management. We develop approaches to manage large-scale multi-dimensional data sets. Particular research issues include: multi-dimensional data storage, indexing, and clustering. (2) Data visualization. We develop effective graphics and visualization techniques that can help the user in information processing tasks. Particular research topics addressed include graph visualization and detecting clusters in a multidimensional data set through visualization. The visualization tools will be used in biomedical image understanding and analysis. (3) Data analysis and querying. We focus on geographical image understanding, analysis and querying. The particular research issues include geographical metadata/knowledge extraction, geographical metadata/knowledge representation and management, and geographical metadata/knowledge querying. (4) Data mining and bioinformatics. We develop data mining techniques for determination of protein structures and detection of gene expression patterns. Through these research activities, the fundamental understanding and novel techniques will be provided to support the management of various large-scale multi-dimensional data sets.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101244</data>
      <data key="e_expirationDate">2008-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101244</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n707" target="n709">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">CISE Research Infrastructure: MultiStore: A Research Infrastructure for Management, Analysis and Visualization of Large-Scale Multidimensional Data Sets</data>
      <data key="e_abstract">EIA-0101244&lt;br/&gt;Aidong Zhang&lt;br/&gt;SUNY at Buffalo&lt;br/&gt;&lt;br/&gt;MultiStore: A Research Infrastructure for Management, Analysis and Visualization of Large-Scale Multi-dimensional Data Sets&lt;br/&gt;&lt;br/&gt;This project establishes a research infrastructure (MultiStore) for supporting integrated research in specific targeted areas of Computer Science, including Multimedia, visualization, Geographical Information Systems (GIS) and Bioinformatics. The research objective is to develop computational theories and algorithms for storing, managing, analyzing, querying and visualizing multi-dimensional data sets that are generated from the related fields. The research components include: (1) Data storage and management. We develop approaches to manage large-scale multi-dimensional data sets. Particular research issues include: multi-dimensional data storage, indexing, and clustering. (2) Data visualization. We develop effective graphics and visualization techniques that can help the user in information processing tasks. Particular research topics addressed include graph visualization and detecting clusters in a multidimensional data set through visualization. The visualization tools will be used in biomedical image understanding and analysis. (3) Data analysis and querying. We focus on geographical image understanding, analysis and querying. The particular research issues include geographical metadata/knowledge extraction, geographical metadata/knowledge representation and management, and geographical metadata/knowledge querying. (4) Data mining and bioinformatics. We develop data mining techniques for determination of protein structures and detection of gene expression patterns. Through these research activities, the fundamental understanding and novel techniques will be provided to support the management of various large-scale multi-dimensional data sets.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101244</data>
      <data key="e_expirationDate">2008-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101244</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n273" target="n709">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">CISE Research Infrastructure: MultiStore: A Research Infrastructure for Management, Analysis and Visualization of Large-Scale Multidimensional Data Sets</data>
      <data key="e_abstract">EIA-0101244&lt;br/&gt;Aidong Zhang&lt;br/&gt;SUNY at Buffalo&lt;br/&gt;&lt;br/&gt;MultiStore: A Research Infrastructure for Management, Analysis and Visualization of Large-Scale Multi-dimensional Data Sets&lt;br/&gt;&lt;br/&gt;This project establishes a research infrastructure (MultiStore) for supporting integrated research in specific targeted areas of Computer Science, including Multimedia, visualization, Geographical Information Systems (GIS) and Bioinformatics. The research objective is to develop computational theories and algorithms for storing, managing, analyzing, querying and visualizing multi-dimensional data sets that are generated from the related fields. The research components include: (1) Data storage and management. We develop approaches to manage large-scale multi-dimensional data sets. Particular research issues include: multi-dimensional data storage, indexing, and clustering. (2) Data visualization. We develop effective graphics and visualization techniques that can help the user in information processing tasks. Particular research topics addressed include graph visualization and detecting clusters in a multidimensional data set through visualization. The visualization tools will be used in biomedical image understanding and analysis. (3) Data analysis and querying. We focus on geographical image understanding, analysis and querying. The particular research issues include geographical metadata/knowledge extraction, geographical metadata/knowledge representation and management, and geographical metadata/knowledge querying. (4) Data mining and bioinformatics. We develop data mining techniques for determination of protein structures and detection of gene expression patterns. Through these research activities, the fundamental understanding and novel techniques will be provided to support the management of various large-scale multi-dimensional data sets.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101244</data>
      <data key="e_expirationDate">2008-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101244</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n710" target="n711">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Infrastructure: CISE Pervasive Computing: Applications and Systems</data>
      <data key="e_abstract">EIA-0101247&lt;br/&gt;David P. Dobkin&lt;br/&gt;Princeton University&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: CISE Pervasive Computing: Applications and Systems&lt;br/&gt;&lt;br/&gt;We are entering a new era in computing, the era of ubiquitous computing. In this world, our classrooms, labs, offices, and homes will be filled with a diverse collection of sensor, display and computing devices. Ubiquitous and pervasive displays will revolutionize the way we use computers.&lt;br/&gt;&lt;br/&gt;In such an environment, the conventional view of the network as providing bit-pipes between clients and servers will no longer be appropriate. Many of the devices available in the environment will have limited computational capabilities and be connected by limited-capacity networks. So, we need an intelligent network that will be implemented by a collection of servers and programmable routers that overlay the physical network substrate.&lt;br/&gt;&lt;br/&gt;The award is to build a research infrastructure consisting of three components. At the &quot;edge&quot; of the system, will be a variety of display technologies and sensors. At the &quot;core&apos;&apos; of the system, will be an intelligent network using commodity PCs and emerging network processors. Underlying everything will be commodity wired and wireless&lt;br/&gt;networks to provide connectivity among the edge devices and nodes in the intelligent network. This network will augment the CS Department&apos;s current network, which already includes both wired and wireless components.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101247</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101247</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n710" target="n712">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Infrastructure: CISE Pervasive Computing: Applications and Systems</data>
      <data key="e_abstract">EIA-0101247&lt;br/&gt;David P. Dobkin&lt;br/&gt;Princeton University&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: CISE Pervasive Computing: Applications and Systems&lt;br/&gt;&lt;br/&gt;We are entering a new era in computing, the era of ubiquitous computing. In this world, our classrooms, labs, offices, and homes will be filled with a diverse collection of sensor, display and computing devices. Ubiquitous and pervasive displays will revolutionize the way we use computers.&lt;br/&gt;&lt;br/&gt;In such an environment, the conventional view of the network as providing bit-pipes between clients and servers will no longer be appropriate. Many of the devices available in the environment will have limited computational capabilities and be connected by limited-capacity networks. So, we need an intelligent network that will be implemented by a collection of servers and programmable routers that overlay the physical network substrate.&lt;br/&gt;&lt;br/&gt;The award is to build a research infrastructure consisting of three components. At the &quot;edge&quot; of the system, will be a variety of display technologies and sensors. At the &quot;core&apos;&apos; of the system, will be an intelligent network using commodity PCs and emerging network processors. Underlying everything will be commodity wired and wireless&lt;br/&gt;networks to provide connectivity among the edge devices and nodes in the intelligent network. This network will augment the CS Department&apos;s current network, which already includes both wired and wireless components.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101247</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101247</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n710" target="n713">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Infrastructure: CISE Pervasive Computing: Applications and Systems</data>
      <data key="e_abstract">EIA-0101247&lt;br/&gt;David P. Dobkin&lt;br/&gt;Princeton University&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: CISE Pervasive Computing: Applications and Systems&lt;br/&gt;&lt;br/&gt;We are entering a new era in computing, the era of ubiquitous computing. In this world, our classrooms, labs, offices, and homes will be filled with a diverse collection of sensor, display and computing devices. Ubiquitous and pervasive displays will revolutionize the way we use computers.&lt;br/&gt;&lt;br/&gt;In such an environment, the conventional view of the network as providing bit-pipes between clients and servers will no longer be appropriate. Many of the devices available in the environment will have limited computational capabilities and be connected by limited-capacity networks. So, we need an intelligent network that will be implemented by a collection of servers and programmable routers that overlay the physical network substrate.&lt;br/&gt;&lt;br/&gt;The award is to build a research infrastructure consisting of three components. At the &quot;edge&quot; of the system, will be a variety of display technologies and sensors. At the &quot;core&apos;&apos; of the system, will be an intelligent network using commodity PCs and emerging network processors. Underlying everything will be commodity wired and wireless&lt;br/&gt;networks to provide connectivity among the edge devices and nodes in the intelligent network. This network will augment the CS Department&apos;s current network, which already includes both wired and wireless components.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101247</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101247</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n710" target="n714">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Infrastructure: CISE Pervasive Computing: Applications and Systems</data>
      <data key="e_abstract">EIA-0101247&lt;br/&gt;David P. Dobkin&lt;br/&gt;Princeton University&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: CISE Pervasive Computing: Applications and Systems&lt;br/&gt;&lt;br/&gt;We are entering a new era in computing, the era of ubiquitous computing. In this world, our classrooms, labs, offices, and homes will be filled with a diverse collection of sensor, display and computing devices. Ubiquitous and pervasive displays will revolutionize the way we use computers.&lt;br/&gt;&lt;br/&gt;In such an environment, the conventional view of the network as providing bit-pipes between clients and servers will no longer be appropriate. Many of the devices available in the environment will have limited computational capabilities and be connected by limited-capacity networks. So, we need an intelligent network that will be implemented by a collection of servers and programmable routers that overlay the physical network substrate.&lt;br/&gt;&lt;br/&gt;The award is to build a research infrastructure consisting of three components. At the &quot;edge&quot; of the system, will be a variety of display technologies and sensors. At the &quot;core&apos;&apos; of the system, will be an intelligent network using commodity PCs and emerging network processors. Underlying everything will be commodity wired and wireless&lt;br/&gt;networks to provide connectivity among the edge devices and nodes in the intelligent network. This network will augment the CS Department&apos;s current network, which already includes both wired and wireless components.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101247</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101247</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n711" target="n712">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Infrastructure: CISE Pervasive Computing: Applications and Systems</data>
      <data key="e_abstract">EIA-0101247&lt;br/&gt;David P. Dobkin&lt;br/&gt;Princeton University&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: CISE Pervasive Computing: Applications and Systems&lt;br/&gt;&lt;br/&gt;We are entering a new era in computing, the era of ubiquitous computing. In this world, our classrooms, labs, offices, and homes will be filled with a diverse collection of sensor, display and computing devices. Ubiquitous and pervasive displays will revolutionize the way we use computers.&lt;br/&gt;&lt;br/&gt;In such an environment, the conventional view of the network as providing bit-pipes between clients and servers will no longer be appropriate. Many of the devices available in the environment will have limited computational capabilities and be connected by limited-capacity networks. So, we need an intelligent network that will be implemented by a collection of servers and programmable routers that overlay the physical network substrate.&lt;br/&gt;&lt;br/&gt;The award is to build a research infrastructure consisting of three components. At the &quot;edge&quot; of the system, will be a variety of display technologies and sensors. At the &quot;core&apos;&apos; of the system, will be an intelligent network using commodity PCs and emerging network processors. Underlying everything will be commodity wired and wireless&lt;br/&gt;networks to provide connectivity among the edge devices and nodes in the intelligent network. This network will augment the CS Department&apos;s current network, which already includes both wired and wireless components.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101247</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101247</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n711" target="n713">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Infrastructure: CISE Pervasive Computing: Applications and Systems</data>
      <data key="e_abstract">EIA-0101247&lt;br/&gt;David P. Dobkin&lt;br/&gt;Princeton University&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: CISE Pervasive Computing: Applications and Systems&lt;br/&gt;&lt;br/&gt;We are entering a new era in computing, the era of ubiquitous computing. In this world, our classrooms, labs, offices, and homes will be filled with a diverse collection of sensor, display and computing devices. Ubiquitous and pervasive displays will revolutionize the way we use computers.&lt;br/&gt;&lt;br/&gt;In such an environment, the conventional view of the network as providing bit-pipes between clients and servers will no longer be appropriate. Many of the devices available in the environment will have limited computational capabilities and be connected by limited-capacity networks. So, we need an intelligent network that will be implemented by a collection of servers and programmable routers that overlay the physical network substrate.&lt;br/&gt;&lt;br/&gt;The award is to build a research infrastructure consisting of three components. At the &quot;edge&quot; of the system, will be a variety of display technologies and sensors. At the &quot;core&apos;&apos; of the system, will be an intelligent network using commodity PCs and emerging network processors. Underlying everything will be commodity wired and wireless&lt;br/&gt;networks to provide connectivity among the edge devices and nodes in the intelligent network. This network will augment the CS Department&apos;s current network, which already includes both wired and wireless components.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101247</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101247</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n711" target="n714">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Infrastructure: CISE Pervasive Computing: Applications and Systems</data>
      <data key="e_abstract">EIA-0101247&lt;br/&gt;David P. Dobkin&lt;br/&gt;Princeton University&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: CISE Pervasive Computing: Applications and Systems&lt;br/&gt;&lt;br/&gt;We are entering a new era in computing, the era of ubiquitous computing. In this world, our classrooms, labs, offices, and homes will be filled with a diverse collection of sensor, display and computing devices. Ubiquitous and pervasive displays will revolutionize the way we use computers.&lt;br/&gt;&lt;br/&gt;In such an environment, the conventional view of the network as providing bit-pipes between clients and servers will no longer be appropriate. Many of the devices available in the environment will have limited computational capabilities and be connected by limited-capacity networks. So, we need an intelligent network that will be implemented by a collection of servers and programmable routers that overlay the physical network substrate.&lt;br/&gt;&lt;br/&gt;The award is to build a research infrastructure consisting of three components. At the &quot;edge&quot; of the system, will be a variety of display technologies and sensors. At the &quot;core&apos;&apos; of the system, will be an intelligent network using commodity PCs and emerging network processors. Underlying everything will be commodity wired and wireless&lt;br/&gt;networks to provide connectivity among the edge devices and nodes in the intelligent network. This network will augment the CS Department&apos;s current network, which already includes both wired and wireless components.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101247</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101247</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n712" target="n713">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Infrastructure: CISE Pervasive Computing: Applications and Systems</data>
      <data key="e_abstract">EIA-0101247&lt;br/&gt;David P. Dobkin&lt;br/&gt;Princeton University&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: CISE Pervasive Computing: Applications and Systems&lt;br/&gt;&lt;br/&gt;We are entering a new era in computing, the era of ubiquitous computing. In this world, our classrooms, labs, offices, and homes will be filled with a diverse collection of sensor, display and computing devices. Ubiquitous and pervasive displays will revolutionize the way we use computers.&lt;br/&gt;&lt;br/&gt;In such an environment, the conventional view of the network as providing bit-pipes between clients and servers will no longer be appropriate. Many of the devices available in the environment will have limited computational capabilities and be connected by limited-capacity networks. So, we need an intelligent network that will be implemented by a collection of servers and programmable routers that overlay the physical network substrate.&lt;br/&gt;&lt;br/&gt;The award is to build a research infrastructure consisting of three components. At the &quot;edge&quot; of the system, will be a variety of display technologies and sensors. At the &quot;core&apos;&apos; of the system, will be an intelligent network using commodity PCs and emerging network processors. Underlying everything will be commodity wired and wireless&lt;br/&gt;networks to provide connectivity among the edge devices and nodes in the intelligent network. This network will augment the CS Department&apos;s current network, which already includes both wired and wireless components.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101247</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101247</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n712" target="n714">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Infrastructure: CISE Pervasive Computing: Applications and Systems</data>
      <data key="e_abstract">EIA-0101247&lt;br/&gt;David P. Dobkin&lt;br/&gt;Princeton University&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: CISE Pervasive Computing: Applications and Systems&lt;br/&gt;&lt;br/&gt;We are entering a new era in computing, the era of ubiquitous computing. In this world, our classrooms, labs, offices, and homes will be filled with a diverse collection of sensor, display and computing devices. Ubiquitous and pervasive displays will revolutionize the way we use computers.&lt;br/&gt;&lt;br/&gt;In such an environment, the conventional view of the network as providing bit-pipes between clients and servers will no longer be appropriate. Many of the devices available in the environment will have limited computational capabilities and be connected by limited-capacity networks. So, we need an intelligent network that will be implemented by a collection of servers and programmable routers that overlay the physical network substrate.&lt;br/&gt;&lt;br/&gt;The award is to build a research infrastructure consisting of three components. At the &quot;edge&quot; of the system, will be a variety of display technologies and sensors. At the &quot;core&apos;&apos; of the system, will be an intelligent network using commodity PCs and emerging network processors. Underlying everything will be commodity wired and wireless&lt;br/&gt;networks to provide connectivity among the edge devices and nodes in the intelligent network. This network will augment the CS Department&apos;s current network, which already includes both wired and wireless components.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101247</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101247</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n713" target="n714">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Infrastructure: CISE Pervasive Computing: Applications and Systems</data>
      <data key="e_abstract">EIA-0101247&lt;br/&gt;David P. Dobkin&lt;br/&gt;Princeton University&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: CISE Pervasive Computing: Applications and Systems&lt;br/&gt;&lt;br/&gt;We are entering a new era in computing, the era of ubiquitous computing. In this world, our classrooms, labs, offices, and homes will be filled with a diverse collection of sensor, display and computing devices. Ubiquitous and pervasive displays will revolutionize the way we use computers.&lt;br/&gt;&lt;br/&gt;In such an environment, the conventional view of the network as providing bit-pipes between clients and servers will no longer be appropriate. Many of the devices available in the environment will have limited computational capabilities and be connected by limited-capacity networks. So, we need an intelligent network that will be implemented by a collection of servers and programmable routers that overlay the physical network substrate.&lt;br/&gt;&lt;br/&gt;The award is to build a research infrastructure consisting of three components. At the &quot;edge&quot; of the system, will be a variety of display technologies and sensors. At the &quot;core&apos;&apos; of the system, will be an intelligent network using commodity PCs and emerging network processors. Underlying everything will be commodity wired and wireless&lt;br/&gt;networks to provide connectivity among the edge devices and nodes in the intelligent network. This network will augment the CS Department&apos;s current network, which already includes both wired and wireless components.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101247</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101247</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n716" target="n717">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Infrastructure: An Infrastructure for Integrated Systems Education and Innovation</data>
      <data key="e_abstract">0101254&lt;br/&gt;Scott A. Hauk&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: An Infrastructure for Integrated Systems Education and Innovation &lt;br/&gt;&lt;br/&gt;The research contained in this proposal represents a wide-ranging investigation into the future of single-chip systems. We will seek to develop a design methodology that can provide the benefits of multiple different resource types for numerous design domains. To support the design of such cutting-edge silicon systems, we will develop innovative techniques to handle numerous design issues. These will include investigations into the following critical issues in chip design: Development of techniques for integrating RF and Analog components into future 1V SoC designs. Creation of high-performance, power efficient digital logic families for supporting the stringent requirements of these systems. Investigation into reconfigurable subsystems for SoC designs, providing post-fabrication customization for support of multi-protocol and multi-algorithm systems. Integrated testing methodologies for complex, heterogeneous systems that can provide complete system test. Complete simulation and design methodologies that can handle complete system integration, architectural exploration, and validation. In addition to the development of new approaches to future chip design, we will also develop innovative techniques for educating future chip designers. By providing an integrated curriculum in VLSI/CAD, embedded systems, and complex system design, we will help create system architects capable of harnessing these radically new design techniques and opportunities. We will also seek to increase the opportunities in chip design for new constituents, especially under-represented groups to help increase the pipeline of new designers</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101254</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101254</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n716" target="n718">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Infrastructure: An Infrastructure for Integrated Systems Education and Innovation</data>
      <data key="e_abstract">0101254&lt;br/&gt;Scott A. Hauk&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: An Infrastructure for Integrated Systems Education and Innovation &lt;br/&gt;&lt;br/&gt;The research contained in this proposal represents a wide-ranging investigation into the future of single-chip systems. We will seek to develop a design methodology that can provide the benefits of multiple different resource types for numerous design domains. To support the design of such cutting-edge silicon systems, we will develop innovative techniques to handle numerous design issues. These will include investigations into the following critical issues in chip design: Development of techniques for integrating RF and Analog components into future 1V SoC designs. Creation of high-performance, power efficient digital logic families for supporting the stringent requirements of these systems. Investigation into reconfigurable subsystems for SoC designs, providing post-fabrication customization for support of multi-protocol and multi-algorithm systems. Integrated testing methodologies for complex, heterogeneous systems that can provide complete system test. Complete simulation and design methodologies that can handle complete system integration, architectural exploration, and validation. In addition to the development of new approaches to future chip design, we will also develop innovative techniques for educating future chip designers. By providing an integrated curriculum in VLSI/CAD, embedded systems, and complex system design, we will help create system architects capable of harnessing these radically new design techniques and opportunities. We will also seek to increase the opportunities in chip design for new constituents, especially under-represented groups to help increase the pipeline of new designers</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101254</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101254</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n716" target="n719">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Infrastructure: An Infrastructure for Integrated Systems Education and Innovation</data>
      <data key="e_abstract">0101254&lt;br/&gt;Scott A. Hauk&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: An Infrastructure for Integrated Systems Education and Innovation &lt;br/&gt;&lt;br/&gt;The research contained in this proposal represents a wide-ranging investigation into the future of single-chip systems. We will seek to develop a design methodology that can provide the benefits of multiple different resource types for numerous design domains. To support the design of such cutting-edge silicon systems, we will develop innovative techniques to handle numerous design issues. These will include investigations into the following critical issues in chip design: Development of techniques for integrating RF and Analog components into future 1V SoC designs. Creation of high-performance, power efficient digital logic families for supporting the stringent requirements of these systems. Investigation into reconfigurable subsystems for SoC designs, providing post-fabrication customization for support of multi-protocol and multi-algorithm systems. Integrated testing methodologies for complex, heterogeneous systems that can provide complete system test. Complete simulation and design methodologies that can handle complete system integration, architectural exploration, and validation. In addition to the development of new approaches to future chip design, we will also develop innovative techniques for educating future chip designers. By providing an integrated curriculum in VLSI/CAD, embedded systems, and complex system design, we will help create system architects capable of harnessing these radically new design techniques and opportunities. We will also seek to increase the opportunities in chip design for new constituents, especially under-represented groups to help increase the pipeline of new designers</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101254</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101254</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n716" target="n720">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Infrastructure: An Infrastructure for Integrated Systems Education and Innovation</data>
      <data key="e_abstract">0101254&lt;br/&gt;Scott A. Hauk&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: An Infrastructure for Integrated Systems Education and Innovation &lt;br/&gt;&lt;br/&gt;The research contained in this proposal represents a wide-ranging investigation into the future of single-chip systems. We will seek to develop a design methodology that can provide the benefits of multiple different resource types for numerous design domains. To support the design of such cutting-edge silicon systems, we will develop innovative techniques to handle numerous design issues. These will include investigations into the following critical issues in chip design: Development of techniques for integrating RF and Analog components into future 1V SoC designs. Creation of high-performance, power efficient digital logic families for supporting the stringent requirements of these systems. Investigation into reconfigurable subsystems for SoC designs, providing post-fabrication customization for support of multi-protocol and multi-algorithm systems. Integrated testing methodologies for complex, heterogeneous systems that can provide complete system test. Complete simulation and design methodologies that can handle complete system integration, architectural exploration, and validation. In addition to the development of new approaches to future chip design, we will also develop innovative techniques for educating future chip designers. By providing an integrated curriculum in VLSI/CAD, embedded systems, and complex system design, we will help create system architects capable of harnessing these radically new design techniques and opportunities. We will also seek to increase the opportunities in chip design for new constituents, especially under-represented groups to help increase the pipeline of new designers</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101254</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101254</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n717" target="n718">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Infrastructure: An Infrastructure for Integrated Systems Education and Innovation</data>
      <data key="e_abstract">0101254&lt;br/&gt;Scott A. Hauk&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: An Infrastructure for Integrated Systems Education and Innovation &lt;br/&gt;&lt;br/&gt;The research contained in this proposal represents a wide-ranging investigation into the future of single-chip systems. We will seek to develop a design methodology that can provide the benefits of multiple different resource types for numerous design domains. To support the design of such cutting-edge silicon systems, we will develop innovative techniques to handle numerous design issues. These will include investigations into the following critical issues in chip design: Development of techniques for integrating RF and Analog components into future 1V SoC designs. Creation of high-performance, power efficient digital logic families for supporting the stringent requirements of these systems. Investigation into reconfigurable subsystems for SoC designs, providing post-fabrication customization for support of multi-protocol and multi-algorithm systems. Integrated testing methodologies for complex, heterogeneous systems that can provide complete system test. Complete simulation and design methodologies that can handle complete system integration, architectural exploration, and validation. In addition to the development of new approaches to future chip design, we will also develop innovative techniques for educating future chip designers. By providing an integrated curriculum in VLSI/CAD, embedded systems, and complex system design, we will help create system architects capable of harnessing these radically new design techniques and opportunities. We will also seek to increase the opportunities in chip design for new constituents, especially under-represented groups to help increase the pipeline of new designers</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101254</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101254</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n717" target="n719">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Infrastructure: An Infrastructure for Integrated Systems Education and Innovation</data>
      <data key="e_abstract">0101254&lt;br/&gt;Scott A. Hauk&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: An Infrastructure for Integrated Systems Education and Innovation &lt;br/&gt;&lt;br/&gt;The research contained in this proposal represents a wide-ranging investigation into the future of single-chip systems. We will seek to develop a design methodology that can provide the benefits of multiple different resource types for numerous design domains. To support the design of such cutting-edge silicon systems, we will develop innovative techniques to handle numerous design issues. These will include investigations into the following critical issues in chip design: Development of techniques for integrating RF and Analog components into future 1V SoC designs. Creation of high-performance, power efficient digital logic families for supporting the stringent requirements of these systems. Investigation into reconfigurable subsystems for SoC designs, providing post-fabrication customization for support of multi-protocol and multi-algorithm systems. Integrated testing methodologies for complex, heterogeneous systems that can provide complete system test. Complete simulation and design methodologies that can handle complete system integration, architectural exploration, and validation. In addition to the development of new approaches to future chip design, we will also develop innovative techniques for educating future chip designers. By providing an integrated curriculum in VLSI/CAD, embedded systems, and complex system design, we will help create system architects capable of harnessing these radically new design techniques and opportunities. We will also seek to increase the opportunities in chip design for new constituents, especially under-represented groups to help increase the pipeline of new designers</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101254</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101254</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n717" target="n720">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Infrastructure: An Infrastructure for Integrated Systems Education and Innovation</data>
      <data key="e_abstract">0101254&lt;br/&gt;Scott A. Hauk&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: An Infrastructure for Integrated Systems Education and Innovation &lt;br/&gt;&lt;br/&gt;The research contained in this proposal represents a wide-ranging investigation into the future of single-chip systems. We will seek to develop a design methodology that can provide the benefits of multiple different resource types for numerous design domains. To support the design of such cutting-edge silicon systems, we will develop innovative techniques to handle numerous design issues. These will include investigations into the following critical issues in chip design: Development of techniques for integrating RF and Analog components into future 1V SoC designs. Creation of high-performance, power efficient digital logic families for supporting the stringent requirements of these systems. Investigation into reconfigurable subsystems for SoC designs, providing post-fabrication customization for support of multi-protocol and multi-algorithm systems. Integrated testing methodologies for complex, heterogeneous systems that can provide complete system test. Complete simulation and design methodologies that can handle complete system integration, architectural exploration, and validation. In addition to the development of new approaches to future chip design, we will also develop innovative techniques for educating future chip designers. By providing an integrated curriculum in VLSI/CAD, embedded systems, and complex system design, we will help create system architects capable of harnessing these radically new design techniques and opportunities. We will also seek to increase the opportunities in chip design for new constituents, especially under-represented groups to help increase the pipeline of new designers</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101254</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101254</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n718" target="n719">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Infrastructure: An Infrastructure for Integrated Systems Education and Innovation</data>
      <data key="e_abstract">0101254&lt;br/&gt;Scott A. Hauk&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: An Infrastructure for Integrated Systems Education and Innovation &lt;br/&gt;&lt;br/&gt;The research contained in this proposal represents a wide-ranging investigation into the future of single-chip systems. We will seek to develop a design methodology that can provide the benefits of multiple different resource types for numerous design domains. To support the design of such cutting-edge silicon systems, we will develop innovative techniques to handle numerous design issues. These will include investigations into the following critical issues in chip design: Development of techniques for integrating RF and Analog components into future 1V SoC designs. Creation of high-performance, power efficient digital logic families for supporting the stringent requirements of these systems. Investigation into reconfigurable subsystems for SoC designs, providing post-fabrication customization for support of multi-protocol and multi-algorithm systems. Integrated testing methodologies for complex, heterogeneous systems that can provide complete system test. Complete simulation and design methodologies that can handle complete system integration, architectural exploration, and validation. In addition to the development of new approaches to future chip design, we will also develop innovative techniques for educating future chip designers. By providing an integrated curriculum in VLSI/CAD, embedded systems, and complex system design, we will help create system architects capable of harnessing these radically new design techniques and opportunities. We will also seek to increase the opportunities in chip design for new constituents, especially under-represented groups to help increase the pipeline of new designers</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101254</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101254</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n718" target="n720">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Infrastructure: An Infrastructure for Integrated Systems Education and Innovation</data>
      <data key="e_abstract">0101254&lt;br/&gt;Scott A. Hauk&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: An Infrastructure for Integrated Systems Education and Innovation &lt;br/&gt;&lt;br/&gt;The research contained in this proposal represents a wide-ranging investigation into the future of single-chip systems. We will seek to develop a design methodology that can provide the benefits of multiple different resource types for numerous design domains. To support the design of such cutting-edge silicon systems, we will develop innovative techniques to handle numerous design issues. These will include investigations into the following critical issues in chip design: Development of techniques for integrating RF and Analog components into future 1V SoC designs. Creation of high-performance, power efficient digital logic families for supporting the stringent requirements of these systems. Investigation into reconfigurable subsystems for SoC designs, providing post-fabrication customization for support of multi-protocol and multi-algorithm systems. Integrated testing methodologies for complex, heterogeneous systems that can provide complete system test. Complete simulation and design methodologies that can handle complete system integration, architectural exploration, and validation. In addition to the development of new approaches to future chip design, we will also develop innovative techniques for educating future chip designers. By providing an integrated curriculum in VLSI/CAD, embedded systems, and complex system design, we will help create system architects capable of harnessing these radically new design techniques and opportunities. We will also seek to increase the opportunities in chip design for new constituents, especially under-represented groups to help increase the pipeline of new designers</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101254</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101254</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n719" target="n720">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Infrastructure: An Infrastructure for Integrated Systems Education and Innovation</data>
      <data key="e_abstract">0101254&lt;br/&gt;Scott A. Hauk&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: An Infrastructure for Integrated Systems Education and Innovation &lt;br/&gt;&lt;br/&gt;The research contained in this proposal represents a wide-ranging investigation into the future of single-chip systems. We will seek to develop a design methodology that can provide the benefits of multiple different resource types for numerous design domains. To support the design of such cutting-edge silicon systems, we will develop innovative techniques to handle numerous design issues. These will include investigations into the following critical issues in chip design: Development of techniques for integrating RF and Analog components into future 1V SoC designs. Creation of high-performance, power efficient digital logic families for supporting the stringent requirements of these systems. Investigation into reconfigurable subsystems for SoC designs, providing post-fabrication customization for support of multi-protocol and multi-algorithm systems. Integrated testing methodologies for complex, heterogeneous systems that can provide complete system test. Complete simulation and design methodologies that can handle complete system integration, architectural exploration, and validation. In addition to the development of new approaches to future chip design, we will also develop innovative techniques for educating future chip designers. By providing an integrated curriculum in VLSI/CAD, embedded systems, and complex system design, we will help create system architects capable of harnessing these radically new design techniques and opportunities. We will also seek to increase the opportunities in chip design for new constituents, especially under-represented groups to help increase the pipeline of new designers</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">101254</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">101254</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n721" target="n722">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Algorithmic and Differential-Geometric Trajectory Design</data>
      <data key="e_abstract">The project focuses on general purpose trajectory design algorithms for high dimensional, highly nonlinear systems evolving in complex environments. The goal is to solve the currently intractable problem of trajectory generation and optimization for high-fidelity models of various types of autonomous vehicles, using an approach that combines methods from differential geometry, nonlinear control theory, robot motion planning, randomized algorithms, and mathematical programming.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">118146</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">118146</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n162" target="n725">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY(CISE): Biomolecular Computing by DNA/Enzyme Systems</data>
      <data key="e_abstract">EIA-0113443&lt;br/&gt;Winfree, Erik&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;Title: Biomolecular Computing by DNA/Enzyme Systems&lt;br/&gt;&lt;br/&gt;Dr. Erik Winfree and Dr. Hideo Mabuchi are working together to develop techniques and instruments for high-precision quantitative analysis of the DNA molecular devices. These are being designed, characterized and optimized to investigate issues such as robustness and error-tolerance of these DNA molecular devices. The technical objectives being achieved in this project are: development of spFRET instrument capable of counting individual photons from single molecules; characterization of conformal states, kinetics, and thermodynamics of DNA switches; characterization of the activities of two enzymes, RNAP and RNase, on the DNA switches; development of stochastic models of in vitro transcriptional circuits; and investigation of robust algorithms and error-control for transcriptional circuits.&lt;br/&gt;&lt;br/&gt;Through this project, the PIs are establishing a set of experimental systems and techniques for exploring computation by biological molecules. This will provide fundamental knowledge and principles for nanoscale computation, such as models of computation, molecular algorithms, physical limits, sources of error and error correction strategies. Thus the aim is to leverage the advanced control over biochemical systems to begin establishing a broader foundation for reliable molecular computing.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113443</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113443</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n727" target="n728">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Instrumentation for Communications Research in Wireless Ad-Hoc Networking</data>
      <data key="e_abstract">EIA-0130599 &lt;br/&gt;James Krogmeier&lt;br/&gt;Purdue University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Instrumentation for Communications Research in Wireless Ad-Hoc Networking&lt;br/&gt;&lt;br/&gt;The Schools of ECE and Civil Engineering at Purdue University will purchase radio frequency test equipment (a network analyzer, an air-interface measurement tool, and radio frequency channel emulators), workstations, Bluetooth Developer&apos;s Kits, laptops, and supporting electronic and interface supplies to enhance the capabilities of the Wireless Communications Research Laboratory and the Harold L. Michael Traffic Operations Laboratory in the area of experimental wireless communications research and intelligent transportation systems. &lt;br/&gt;&lt;br/&gt;Four research projects will be directly enhanced by the availability of the above resources: 1) Wireless ad-hoc networking for dedicated short range communications applications in intelligent transportation systems, 2) Reduced complexity receivers for continuous phase modulations, 3) Reduced-dimension decision feedback equalizers for high-speed wireless communications, and 4) Embedded signal processing for intelligent transportation systems.&lt;br/&gt;&lt;br/&gt;The communications research involved in the projects above is focused on synchronization, channel estimation and equalization, and low complexity receivers for both linear and non-linear modulations. End-to-end network performance is also considered, as is the design of simplified protocols for use in ad-hoc networking. The signal processing research is largely experimental involving the integration of different intelligent transportation subsystems including communications and embedded signal processing for traffic signal control and weigh-in-motion.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130599</data>
      <data key="e_expirationDate">2005-11-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130599</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n727" target="n729">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Instrumentation for Communications Research in Wireless Ad-Hoc Networking</data>
      <data key="e_abstract">EIA-0130599 &lt;br/&gt;James Krogmeier&lt;br/&gt;Purdue University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Instrumentation for Communications Research in Wireless Ad-Hoc Networking&lt;br/&gt;&lt;br/&gt;The Schools of ECE and Civil Engineering at Purdue University will purchase radio frequency test equipment (a network analyzer, an air-interface measurement tool, and radio frequency channel emulators), workstations, Bluetooth Developer&apos;s Kits, laptops, and supporting electronic and interface supplies to enhance the capabilities of the Wireless Communications Research Laboratory and the Harold L. Michael Traffic Operations Laboratory in the area of experimental wireless communications research and intelligent transportation systems. &lt;br/&gt;&lt;br/&gt;Four research projects will be directly enhanced by the availability of the above resources: 1) Wireless ad-hoc networking for dedicated short range communications applications in intelligent transportation systems, 2) Reduced complexity receivers for continuous phase modulations, 3) Reduced-dimension decision feedback equalizers for high-speed wireless communications, and 4) Embedded signal processing for intelligent transportation systems.&lt;br/&gt;&lt;br/&gt;The communications research involved in the projects above is focused on synchronization, channel estimation and equalization, and low complexity receivers for both linear and non-linear modulations. End-to-end network performance is also considered, as is the design of simplified protocols for use in ad-hoc networking. The signal processing research is largely experimental involving the integration of different intelligent transportation subsystems including communications and embedded signal processing for traffic signal control and weigh-in-motion.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130599</data>
      <data key="e_expirationDate">2005-11-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130599</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n727" target="n730">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Instrumentation for Communications Research in Wireless Ad-Hoc Networking</data>
      <data key="e_abstract">EIA-0130599 &lt;br/&gt;James Krogmeier&lt;br/&gt;Purdue University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Instrumentation for Communications Research in Wireless Ad-Hoc Networking&lt;br/&gt;&lt;br/&gt;The Schools of ECE and Civil Engineering at Purdue University will purchase radio frequency test equipment (a network analyzer, an air-interface measurement tool, and radio frequency channel emulators), workstations, Bluetooth Developer&apos;s Kits, laptops, and supporting electronic and interface supplies to enhance the capabilities of the Wireless Communications Research Laboratory and the Harold L. Michael Traffic Operations Laboratory in the area of experimental wireless communications research and intelligent transportation systems. &lt;br/&gt;&lt;br/&gt;Four research projects will be directly enhanced by the availability of the above resources: 1) Wireless ad-hoc networking for dedicated short range communications applications in intelligent transportation systems, 2) Reduced complexity receivers for continuous phase modulations, 3) Reduced-dimension decision feedback equalizers for high-speed wireless communications, and 4) Embedded signal processing for intelligent transportation systems.&lt;br/&gt;&lt;br/&gt;The communications research involved in the projects above is focused on synchronization, channel estimation and equalization, and low complexity receivers for both linear and non-linear modulations. End-to-end network performance is also considered, as is the design of simplified protocols for use in ad-hoc networking. The signal processing research is largely experimental involving the integration of different intelligent transportation subsystems including communications and embedded signal processing for traffic signal control and weigh-in-motion.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130599</data>
      <data key="e_expirationDate">2005-11-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130599</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n728" target="n729">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Instrumentation for Communications Research in Wireless Ad-Hoc Networking</data>
      <data key="e_abstract">EIA-0130599 &lt;br/&gt;James Krogmeier&lt;br/&gt;Purdue University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Instrumentation for Communications Research in Wireless Ad-Hoc Networking&lt;br/&gt;&lt;br/&gt;The Schools of ECE and Civil Engineering at Purdue University will purchase radio frequency test equipment (a network analyzer, an air-interface measurement tool, and radio frequency channel emulators), workstations, Bluetooth Developer&apos;s Kits, laptops, and supporting electronic and interface supplies to enhance the capabilities of the Wireless Communications Research Laboratory and the Harold L. Michael Traffic Operations Laboratory in the area of experimental wireless communications research and intelligent transportation systems. &lt;br/&gt;&lt;br/&gt;Four research projects will be directly enhanced by the availability of the above resources: 1) Wireless ad-hoc networking for dedicated short range communications applications in intelligent transportation systems, 2) Reduced complexity receivers for continuous phase modulations, 3) Reduced-dimension decision feedback equalizers for high-speed wireless communications, and 4) Embedded signal processing for intelligent transportation systems.&lt;br/&gt;&lt;br/&gt;The communications research involved in the projects above is focused on synchronization, channel estimation and equalization, and low complexity receivers for both linear and non-linear modulations. End-to-end network performance is also considered, as is the design of simplified protocols for use in ad-hoc networking. The signal processing research is largely experimental involving the integration of different intelligent transportation subsystems including communications and embedded signal processing for traffic signal control and weigh-in-motion.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130599</data>
      <data key="e_expirationDate">2005-11-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130599</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n728" target="n730">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Instrumentation for Communications Research in Wireless Ad-Hoc Networking</data>
      <data key="e_abstract">EIA-0130599 &lt;br/&gt;James Krogmeier&lt;br/&gt;Purdue University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Instrumentation for Communications Research in Wireless Ad-Hoc Networking&lt;br/&gt;&lt;br/&gt;The Schools of ECE and Civil Engineering at Purdue University will purchase radio frequency test equipment (a network analyzer, an air-interface measurement tool, and radio frequency channel emulators), workstations, Bluetooth Developer&apos;s Kits, laptops, and supporting electronic and interface supplies to enhance the capabilities of the Wireless Communications Research Laboratory and the Harold L. Michael Traffic Operations Laboratory in the area of experimental wireless communications research and intelligent transportation systems. &lt;br/&gt;&lt;br/&gt;Four research projects will be directly enhanced by the availability of the above resources: 1) Wireless ad-hoc networking for dedicated short range communications applications in intelligent transportation systems, 2) Reduced complexity receivers for continuous phase modulations, 3) Reduced-dimension decision feedback equalizers for high-speed wireless communications, and 4) Embedded signal processing for intelligent transportation systems.&lt;br/&gt;&lt;br/&gt;The communications research involved in the projects above is focused on synchronization, channel estimation and equalization, and low complexity receivers for both linear and non-linear modulations. End-to-end network performance is also considered, as is the design of simplified protocols for use in ad-hoc networking. The signal processing research is largely experimental involving the integration of different intelligent transportation subsystems including communications and embedded signal processing for traffic signal control and weigh-in-motion.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130599</data>
      <data key="e_expirationDate">2005-11-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130599</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n729" target="n730">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Instrumentation for Communications Research in Wireless Ad-Hoc Networking</data>
      <data key="e_abstract">EIA-0130599 &lt;br/&gt;James Krogmeier&lt;br/&gt;Purdue University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Instrumentation for Communications Research in Wireless Ad-Hoc Networking&lt;br/&gt;&lt;br/&gt;The Schools of ECE and Civil Engineering at Purdue University will purchase radio frequency test equipment (a network analyzer, an air-interface measurement tool, and radio frequency channel emulators), workstations, Bluetooth Developer&apos;s Kits, laptops, and supporting electronic and interface supplies to enhance the capabilities of the Wireless Communications Research Laboratory and the Harold L. Michael Traffic Operations Laboratory in the area of experimental wireless communications research and intelligent transportation systems. &lt;br/&gt;&lt;br/&gt;Four research projects will be directly enhanced by the availability of the above resources: 1) Wireless ad-hoc networking for dedicated short range communications applications in intelligent transportation systems, 2) Reduced complexity receivers for continuous phase modulations, 3) Reduced-dimension decision feedback equalizers for high-speed wireless communications, and 4) Embedded signal processing for intelligent transportation systems.&lt;br/&gt;&lt;br/&gt;The communications research involved in the projects above is focused on synchronization, channel estimation and equalization, and low complexity receivers for both linear and non-linear modulations. End-to-end network performance is also considered, as is the design of simplified protocols for use in ad-hoc networking. The signal processing research is largely experimental involving the integration of different intelligent transportation subsystems including communications and embedded signal processing for traffic signal control and weigh-in-motion.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130599</data>
      <data key="e_expirationDate">2005-11-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130599</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n251" target="n734">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Peer-to-Peer Networks for Self-Organizing Virtual Communities</data>
      <data key="e_abstract">This project extends peer-to-peer communication networks to better support formation of virtual communities in wide area computer networks. Virtual communities bring together individuals with similar interests, but the difficulty of forming them and sustaining critical mass discourages communities that serve small populations or compete with existing communities. Large-scale peer-to-peer networks offer the possibility of self-organizing communities, in which nodes recognize and create relatively stable connections to other nodes with similar interests. The solution includes nodes that learn about their network neighborhoods, nodes that offer partial (and competing) directory services, new methods of routing messages efficiently in peer-to-peer networks, more accurate methods of making resource selection decisions in environments containing many resources, and a utility-theoretic model for decision-making by individual nodes that incorporate multiple task requirements (e.g., cost, accuracy, and reliability). The scientific results will be more robust and efficient peer-to-peer networks, new techniques for forming virtual communities, and a better understanding of how complex peer-to-peer networks work. A software simulator will enable CS, MIS, and Business students to study virtual communities, for example testing hypotheses about why marketplaces fail or policies that encourage community formation. The basic science can be used to build search tools that explicitly consider tens of thousands of databases, software that supports dynamic creation of virtual communities within organizational intranets in response to unforeseen developments (e.g., the DoD), and wireless networks in which devices work whenever they are in range of another device.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">118767</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">118767</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n734" target="n736">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Peer-to-Peer Networks for Self-Organizing Virtual Communities</data>
      <data key="e_abstract">This project extends peer-to-peer communication networks to better support formation of virtual communities in wide area computer networks. Virtual communities bring together individuals with similar interests, but the difficulty of forming them and sustaining critical mass discourages communities that serve small populations or compete with existing communities. Large-scale peer-to-peer networks offer the possibility of self-organizing communities, in which nodes recognize and create relatively stable connections to other nodes with similar interests. The solution includes nodes that learn about their network neighborhoods, nodes that offer partial (and competing) directory services, new methods of routing messages efficiently in peer-to-peer networks, more accurate methods of making resource selection decisions in environments containing many resources, and a utility-theoretic model for decision-making by individual nodes that incorporate multiple task requirements (e.g., cost, accuracy, and reliability). The scientific results will be more robust and efficient peer-to-peer networks, new techniques for forming virtual communities, and a better understanding of how complex peer-to-peer networks work. A software simulator will enable CS, MIS, and Business students to study virtual communities, for example testing hypotheses about why marketplaces fail or policies that encourage community formation. The basic science can be used to build search tools that explicitly consider tens of thousands of databases, software that supports dynamic creation of virtual communities within organizational intranets in response to unforeseen developments (e.g., the DoD), and wireless networks in which devices work whenever they are in range of another device.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">118767</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">118767</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n251" target="n736">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Peer-to-Peer Networks for Self-Organizing Virtual Communities</data>
      <data key="e_abstract">This project extends peer-to-peer communication networks to better support formation of virtual communities in wide area computer networks. Virtual communities bring together individuals with similar interests, but the difficulty of forming them and sustaining critical mass discourages communities that serve small populations or compete with existing communities. Large-scale peer-to-peer networks offer the possibility of self-organizing communities, in which nodes recognize and create relatively stable connections to other nodes with similar interests. The solution includes nodes that learn about their network neighborhoods, nodes that offer partial (and competing) directory services, new methods of routing messages efficiently in peer-to-peer networks, more accurate methods of making resource selection decisions in environments containing many resources, and a utility-theoretic model for decision-making by individual nodes that incorporate multiple task requirements (e.g., cost, accuracy, and reliability). The scientific results will be more robust and efficient peer-to-peer networks, new techniques for forming virtual communities, and a better understanding of how complex peer-to-peer networks work. A software simulator will enable CS, MIS, and Business students to study virtual communities, for example testing hypotheses about why marketplaces fail or policies that encourage community formation. The basic science can be used to build search tools that explicitly consider tens of thousands of databases, software that supports dynamic creation of virtual communities within organizational intranets in response to unforeseen developments (e.g., the DoD), and wireless networks in which devices work whenever they are in range of another device.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">118767</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">118767</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n740" target="n741">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Collaborative/RUI Research on the Perceptual Aspects of Locomotion Interfaces</data>
      <data key="e_abstract">No current system allows a person to naturally walk through a large-scale virtual environment. The availability of such a locomotion interface would have impacts on a broad range of applications, including education and training, design and prototyping, physical fitness, and rehabilitation; for some of these applications natural walking provides a level of realism not obtainable if movement through the simulated world is controlled by devices such as a joystick, while for others realistic walking is a fundamental requirement. Prototypes have been built for a variety of computer-controlled devices on which a person can walk, but there has been little investigation of the utility of such devices as interfaces to a virtual world and almost no study at all of the interactions of visual and biomechanical perceptual cues in such devices. This project addresses key open questions, the answers to which are needed if locomotion interfaces are to offer effective interaction between users and computer simulations. An effective locomotion interface must provide users with accurate visual and biomechanical sensations of walking; thus, a key objective of this work is to determine how to synergistically combine visual information generated by computer graphics with biomechanical information generated by devices that simulate walking on real surfaces. The PI and his collaborators will investigates methods that allow more accurate walking in a locomotion interface while accurately conveying a sense of the spaces being walked through. Specific issues to be considered include how to facilitate the perception of speed and distance traveled, how to provide a compelling sense of turning when actual walking along a curved path is not possible, how to give a user the sense that he/she is walking over a sloped surface, and more generally how to give a user a clear sense of the scale and structure of the spaces being walked through. The PI&apos;s findings on these issues will be relevant across the spectrum of possible approaches to locomotion interfaces.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121084</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121084</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n740" target="n742">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Collaborative/RUI Research on the Perceptual Aspects of Locomotion Interfaces</data>
      <data key="e_abstract">No current system allows a person to naturally walk through a large-scale virtual environment. The availability of such a locomotion interface would have impacts on a broad range of applications, including education and training, design and prototyping, physical fitness, and rehabilitation; for some of these applications natural walking provides a level of realism not obtainable if movement through the simulated world is controlled by devices such as a joystick, while for others realistic walking is a fundamental requirement. Prototypes have been built for a variety of computer-controlled devices on which a person can walk, but there has been little investigation of the utility of such devices as interfaces to a virtual world and almost no study at all of the interactions of visual and biomechanical perceptual cues in such devices. This project addresses key open questions, the answers to which are needed if locomotion interfaces are to offer effective interaction between users and computer simulations. An effective locomotion interface must provide users with accurate visual and biomechanical sensations of walking; thus, a key objective of this work is to determine how to synergistically combine visual information generated by computer graphics with biomechanical information generated by devices that simulate walking on real surfaces. The PI and his collaborators will investigates methods that allow more accurate walking in a locomotion interface while accurately conveying a sense of the spaces being walked through. Specific issues to be considered include how to facilitate the perception of speed and distance traveled, how to provide a compelling sense of turning when actual walking along a curved path is not possible, how to give a user the sense that he/she is walking over a sloped surface, and more generally how to give a user a clear sense of the scale and structure of the spaces being walked through. The PI&apos;s findings on these issues will be relevant across the spectrum of possible approaches to locomotion interfaces.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121084</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121084</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n740" target="n743">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Collaborative/RUI Research on the Perceptual Aspects of Locomotion Interfaces</data>
      <data key="e_abstract">No current system allows a person to naturally walk through a large-scale virtual environment. The availability of such a locomotion interface would have impacts on a broad range of applications, including education and training, design and prototyping, physical fitness, and rehabilitation; for some of these applications natural walking provides a level of realism not obtainable if movement through the simulated world is controlled by devices such as a joystick, while for others realistic walking is a fundamental requirement. Prototypes have been built for a variety of computer-controlled devices on which a person can walk, but there has been little investigation of the utility of such devices as interfaces to a virtual world and almost no study at all of the interactions of visual and biomechanical perceptual cues in such devices. This project addresses key open questions, the answers to which are needed if locomotion interfaces are to offer effective interaction between users and computer simulations. An effective locomotion interface must provide users with accurate visual and biomechanical sensations of walking; thus, a key objective of this work is to determine how to synergistically combine visual information generated by computer graphics with biomechanical information generated by devices that simulate walking on real surfaces. The PI and his collaborators will investigates methods that allow more accurate walking in a locomotion interface while accurately conveying a sense of the spaces being walked through. Specific issues to be considered include how to facilitate the perception of speed and distance traveled, how to provide a compelling sense of turning when actual walking along a curved path is not possible, how to give a user the sense that he/she is walking over a sloped surface, and more generally how to give a user a clear sense of the scale and structure of the spaces being walked through. The PI&apos;s findings on these issues will be relevant across the spectrum of possible approaches to locomotion interfaces.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121084</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121084</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n741" target="n742">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Collaborative/RUI Research on the Perceptual Aspects of Locomotion Interfaces</data>
      <data key="e_abstract">No current system allows a person to naturally walk through a large-scale virtual environment. The availability of such a locomotion interface would have impacts on a broad range of applications, including education and training, design and prototyping, physical fitness, and rehabilitation; for some of these applications natural walking provides a level of realism not obtainable if movement through the simulated world is controlled by devices such as a joystick, while for others realistic walking is a fundamental requirement. Prototypes have been built for a variety of computer-controlled devices on which a person can walk, but there has been little investigation of the utility of such devices as interfaces to a virtual world and almost no study at all of the interactions of visual and biomechanical perceptual cues in such devices. This project addresses key open questions, the answers to which are needed if locomotion interfaces are to offer effective interaction between users and computer simulations. An effective locomotion interface must provide users with accurate visual and biomechanical sensations of walking; thus, a key objective of this work is to determine how to synergistically combine visual information generated by computer graphics with biomechanical information generated by devices that simulate walking on real surfaces. The PI and his collaborators will investigates methods that allow more accurate walking in a locomotion interface while accurately conveying a sense of the spaces being walked through. Specific issues to be considered include how to facilitate the perception of speed and distance traveled, how to provide a compelling sense of turning when actual walking along a curved path is not possible, how to give a user the sense that he/she is walking over a sloped surface, and more generally how to give a user a clear sense of the scale and structure of the spaces being walked through. The PI&apos;s findings on these issues will be relevant across the spectrum of possible approaches to locomotion interfaces.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121084</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121084</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n741" target="n743">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Collaborative/RUI Research on the Perceptual Aspects of Locomotion Interfaces</data>
      <data key="e_abstract">No current system allows a person to naturally walk through a large-scale virtual environment. The availability of such a locomotion interface would have impacts on a broad range of applications, including education and training, design and prototyping, physical fitness, and rehabilitation; for some of these applications natural walking provides a level of realism not obtainable if movement through the simulated world is controlled by devices such as a joystick, while for others realistic walking is a fundamental requirement. Prototypes have been built for a variety of computer-controlled devices on which a person can walk, but there has been little investigation of the utility of such devices as interfaces to a virtual world and almost no study at all of the interactions of visual and biomechanical perceptual cues in such devices. This project addresses key open questions, the answers to which are needed if locomotion interfaces are to offer effective interaction between users and computer simulations. An effective locomotion interface must provide users with accurate visual and biomechanical sensations of walking; thus, a key objective of this work is to determine how to synergistically combine visual information generated by computer graphics with biomechanical information generated by devices that simulate walking on real surfaces. The PI and his collaborators will investigates methods that allow more accurate walking in a locomotion interface while accurately conveying a sense of the spaces being walked through. Specific issues to be considered include how to facilitate the perception of speed and distance traveled, how to provide a compelling sense of turning when actual walking along a curved path is not possible, how to give a user the sense that he/she is walking over a sloped surface, and more generally how to give a user a clear sense of the scale and structure of the spaces being walked through. The PI&apos;s findings on these issues will be relevant across the spectrum of possible approaches to locomotion interfaces.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121084</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121084</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n742" target="n743">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Collaborative/RUI Research on the Perceptual Aspects of Locomotion Interfaces</data>
      <data key="e_abstract">No current system allows a person to naturally walk through a large-scale virtual environment. The availability of such a locomotion interface would have impacts on a broad range of applications, including education and training, design and prototyping, physical fitness, and rehabilitation; for some of these applications natural walking provides a level of realism not obtainable if movement through the simulated world is controlled by devices such as a joystick, while for others realistic walking is a fundamental requirement. Prototypes have been built for a variety of computer-controlled devices on which a person can walk, but there has been little investigation of the utility of such devices as interfaces to a virtual world and almost no study at all of the interactions of visual and biomechanical perceptual cues in such devices. This project addresses key open questions, the answers to which are needed if locomotion interfaces are to offer effective interaction between users and computer simulations. An effective locomotion interface must provide users with accurate visual and biomechanical sensations of walking; thus, a key objective of this work is to determine how to synergistically combine visual information generated by computer graphics with biomechanical information generated by devices that simulate walking on real surfaces. The PI and his collaborators will investigates methods that allow more accurate walking in a locomotion interface while accurately conveying a sense of the spaces being walked through. Specific issues to be considered include how to facilitate the perception of speed and distance traveled, how to provide a compelling sense of turning when actual walking along a curved path is not possible, how to give a user the sense that he/she is walking over a sloped surface, and more generally how to give a user a clear sense of the scale and structure of the spaces being walked through. The PI&apos;s findings on these issues will be relevant across the spectrum of possible approaches to locomotion interfaces.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121084</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121084</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n745" target="n746">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Digital Government: Software Architectures for Microsimulation of Urban Development, Transportation, and Environmental Impact</data>
      <data key="e_abstract">EIA-0090832&lt;br/&gt;Alan Borning&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;TITLE: Software Architectures for Microsimulation of Urban Development, Transportation and Environmental Impact&lt;br/&gt;&lt;br/&gt;Patterns of land use and available transportation systems play critical role in determining the economic vitality, livability, and sustainability of urban areas. Transportation interacts strongly with land use; different kinds of transportation systems induce different patterns of land use, while at the same time, different kinds of land use induce demands for different kinds of transportation systems. Both land use and transportation have substantial environmental effects, in particular on emissions, resource consumption and open space. Government policies and investments affect patterns of land use and transportation in many complex and sometimes unintended ways.&lt;br/&gt;&lt;br/&gt;This proposal will develop a fully disaggregated Microsimulation system for modeling urban development and government investments and policies related to transportation, land use and environment. Technical support can play a critical role in fostering informed civic deliberation and debate on these issues by allowing urban planners and stakeholders to be able to consider different scenarios-packages of possible policies and investments-and then, based on these alternatives, model the effects of these scenarios on patterns of urban growth and redevelopment, of transportation usage, and resource consumption, over periods of twenty or more years. This proposal will concentrate on two related computer science areas; the software engineering issues that arise in the design and construction of such a large, complex model, and the human computer interaction issues that arise in using it.&lt;br/&gt;&lt;br/&gt;A set of government partnerships is an integral part of this research. At the federal level, there are commitments from the Federal Highway Administration and the Federal Transit Administration (both units in the Department of Transportation), and at the local level, from the Puget Sound Regional Council, the governmental organization charged with land use and transportation planning.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">90832</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">90832</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n745" target="n747">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Digital Government: Software Architectures for Microsimulation of Urban Development, Transportation, and Environmental Impact</data>
      <data key="e_abstract">EIA-0090832&lt;br/&gt;Alan Borning&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;TITLE: Software Architectures for Microsimulation of Urban Development, Transportation and Environmental Impact&lt;br/&gt;&lt;br/&gt;Patterns of land use and available transportation systems play critical role in determining the economic vitality, livability, and sustainability of urban areas. Transportation interacts strongly with land use; different kinds of transportation systems induce different patterns of land use, while at the same time, different kinds of land use induce demands for different kinds of transportation systems. Both land use and transportation have substantial environmental effects, in particular on emissions, resource consumption and open space. Government policies and investments affect patterns of land use and transportation in many complex and sometimes unintended ways.&lt;br/&gt;&lt;br/&gt;This proposal will develop a fully disaggregated Microsimulation system for modeling urban development and government investments and policies related to transportation, land use and environment. Technical support can play a critical role in fostering informed civic deliberation and debate on these issues by allowing urban planners and stakeholders to be able to consider different scenarios-packages of possible policies and investments-and then, based on these alternatives, model the effects of these scenarios on patterns of urban growth and redevelopment, of transportation usage, and resource consumption, over periods of twenty or more years. This proposal will concentrate on two related computer science areas; the software engineering issues that arise in the design and construction of such a large, complex model, and the human computer interaction issues that arise in using it.&lt;br/&gt;&lt;br/&gt;A set of government partnerships is an integral part of this research. At the federal level, there are commitments from the Federal Highway Administration and the Federal Transit Administration (both units in the Department of Transportation), and at the local level, from the Puget Sound Regional Council, the governmental organization charged with land use and transportation planning.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">90832</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">90832</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n746" target="n747">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Digital Government: Software Architectures for Microsimulation of Urban Development, Transportation, and Environmental Impact</data>
      <data key="e_abstract">EIA-0090832&lt;br/&gt;Alan Borning&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;TITLE: Software Architectures for Microsimulation of Urban Development, Transportation and Environmental Impact&lt;br/&gt;&lt;br/&gt;Patterns of land use and available transportation systems play critical role in determining the economic vitality, livability, and sustainability of urban areas. Transportation interacts strongly with land use; different kinds of transportation systems induce different patterns of land use, while at the same time, different kinds of land use induce demands for different kinds of transportation systems. Both land use and transportation have substantial environmental effects, in particular on emissions, resource consumption and open space. Government policies and investments affect patterns of land use and transportation in many complex and sometimes unintended ways.&lt;br/&gt;&lt;br/&gt;This proposal will develop a fully disaggregated Microsimulation system for modeling urban development and government investments and policies related to transportation, land use and environment. Technical support can play a critical role in fostering informed civic deliberation and debate on these issues by allowing urban planners and stakeholders to be able to consider different scenarios-packages of possible policies and investments-and then, based on these alternatives, model the effects of these scenarios on patterns of urban growth and redevelopment, of transportation usage, and resource consumption, over periods of twenty or more years. This proposal will concentrate on two related computer science areas; the software engineering issues that arise in the design and construction of such a large, complex model, and the human computer interaction issues that arise in using it.&lt;br/&gt;&lt;br/&gt;A set of government partnerships is an integral part of this research. At the federal level, there are commitments from the Federal Highway Administration and the Federal Transit Administration (both units in the Department of Transportation), and at the local level, from the Puget Sound Regional Council, the governmental organization charged with land use and transportation planning.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">90832</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">90832</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n748" target="n749">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: Multiple-Word DNA Computing on Surfaces</data>
      <data key="e_abstract">EIA-0130108&lt;br/&gt;Lloyd M. Smith&lt;br/&gt;University of Wisconsin-Madison&lt;br/&gt;&lt;br/&gt;Title: Multiple-Word CNA Computing on Surfaces&lt;br/&gt;&lt;br/&gt;Under this project tools are being developed to increase the computational generality of surface-based DNA computing. It is well known that for a computing model to be general, that is, capable of efficiently simulating algorithms used in conventional electronic computing, it must be able to efficiently simulate circuits. It has been shown theoretically that the surface-based approach, when using multiple words and the MARK, DESTROY-UNMARKED, UNMARK, and APPEND operations, is a generalizable approach to computing, but this has not been implemented experimentally. The necessary tools for such an implementation is being developed. Particular tasks, which are being addressed to this end, include:&lt;br/&gt;&lt;br/&gt;-improved designs of sets of DNA &quot;words&quot; that do not interfere with one another in hybridization experiments and do not contain strong secondary structure motifs.&lt;br/&gt;&lt;br/&gt;-methods for the efficient purification of colloidal gold nanoparticles and the implementation of surface-based DNA computing using such particles as supports. &lt;br/&gt;&lt;br/&gt;-development of a new DESTROY-UNMARKED operation, multiple-word AND operation, and multiple-word APPEND-MARKED operation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">130108</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130108</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n748" target="n750">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: Multiple-Word DNA Computing on Surfaces</data>
      <data key="e_abstract">EIA-0130108&lt;br/&gt;Lloyd M. Smith&lt;br/&gt;University of Wisconsin-Madison&lt;br/&gt;&lt;br/&gt;Title: Multiple-Word CNA Computing on Surfaces&lt;br/&gt;&lt;br/&gt;Under this project tools are being developed to increase the computational generality of surface-based DNA computing. It is well known that for a computing model to be general, that is, capable of efficiently simulating algorithms used in conventional electronic computing, it must be able to efficiently simulate circuits. It has been shown theoretically that the surface-based approach, when using multiple words and the MARK, DESTROY-UNMARKED, UNMARK, and APPEND operations, is a generalizable approach to computing, but this has not been implemented experimentally. The necessary tools for such an implementation is being developed. Particular tasks, which are being addressed to this end, include:&lt;br/&gt;&lt;br/&gt;-improved designs of sets of DNA &quot;words&quot; that do not interfere with one another in hybridization experiments and do not contain strong secondary structure motifs.&lt;br/&gt;&lt;br/&gt;-methods for the efficient purification of colloidal gold nanoparticles and the implementation of surface-based DNA computing using such particles as supports. &lt;br/&gt;&lt;br/&gt;-development of a new DESTROY-UNMARKED operation, multiple-word AND operation, and multiple-word APPEND-MARKED operation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">130108</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130108</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n748" target="n751">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: Multiple-Word DNA Computing on Surfaces</data>
      <data key="e_abstract">EIA-0130108&lt;br/&gt;Lloyd M. Smith&lt;br/&gt;University of Wisconsin-Madison&lt;br/&gt;&lt;br/&gt;Title: Multiple-Word CNA Computing on Surfaces&lt;br/&gt;&lt;br/&gt;Under this project tools are being developed to increase the computational generality of surface-based DNA computing. It is well known that for a computing model to be general, that is, capable of efficiently simulating algorithms used in conventional electronic computing, it must be able to efficiently simulate circuits. It has been shown theoretically that the surface-based approach, when using multiple words and the MARK, DESTROY-UNMARKED, UNMARK, and APPEND operations, is a generalizable approach to computing, but this has not been implemented experimentally. The necessary tools for such an implementation is being developed. Particular tasks, which are being addressed to this end, include:&lt;br/&gt;&lt;br/&gt;-improved designs of sets of DNA &quot;words&quot; that do not interfere with one another in hybridization experiments and do not contain strong secondary structure motifs.&lt;br/&gt;&lt;br/&gt;-methods for the efficient purification of colloidal gold nanoparticles and the implementation of surface-based DNA computing using such particles as supports. &lt;br/&gt;&lt;br/&gt;-development of a new DESTROY-UNMARKED operation, multiple-word AND operation, and multiple-word APPEND-MARKED operation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">130108</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130108</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n749" target="n750">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: Multiple-Word DNA Computing on Surfaces</data>
      <data key="e_abstract">EIA-0130108&lt;br/&gt;Lloyd M. Smith&lt;br/&gt;University of Wisconsin-Madison&lt;br/&gt;&lt;br/&gt;Title: Multiple-Word CNA Computing on Surfaces&lt;br/&gt;&lt;br/&gt;Under this project tools are being developed to increase the computational generality of surface-based DNA computing. It is well known that for a computing model to be general, that is, capable of efficiently simulating algorithms used in conventional electronic computing, it must be able to efficiently simulate circuits. It has been shown theoretically that the surface-based approach, when using multiple words and the MARK, DESTROY-UNMARKED, UNMARK, and APPEND operations, is a generalizable approach to computing, but this has not been implemented experimentally. The necessary tools for such an implementation is being developed. Particular tasks, which are being addressed to this end, include:&lt;br/&gt;&lt;br/&gt;-improved designs of sets of DNA &quot;words&quot; that do not interfere with one another in hybridization experiments and do not contain strong secondary structure motifs.&lt;br/&gt;&lt;br/&gt;-methods for the efficient purification of colloidal gold nanoparticles and the implementation of surface-based DNA computing using such particles as supports. &lt;br/&gt;&lt;br/&gt;-development of a new DESTROY-UNMARKED operation, multiple-word AND operation, and multiple-word APPEND-MARKED operation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">130108</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130108</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n749" target="n751">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: Multiple-Word DNA Computing on Surfaces</data>
      <data key="e_abstract">EIA-0130108&lt;br/&gt;Lloyd M. Smith&lt;br/&gt;University of Wisconsin-Madison&lt;br/&gt;&lt;br/&gt;Title: Multiple-Word CNA Computing on Surfaces&lt;br/&gt;&lt;br/&gt;Under this project tools are being developed to increase the computational generality of surface-based DNA computing. It is well known that for a computing model to be general, that is, capable of efficiently simulating algorithms used in conventional electronic computing, it must be able to efficiently simulate circuits. It has been shown theoretically that the surface-based approach, when using multiple words and the MARK, DESTROY-UNMARKED, UNMARK, and APPEND operations, is a generalizable approach to computing, but this has not been implemented experimentally. The necessary tools for such an implementation is being developed. Particular tasks, which are being addressed to this end, include:&lt;br/&gt;&lt;br/&gt;-improved designs of sets of DNA &quot;words&quot; that do not interfere with one another in hybridization experiments and do not contain strong secondary structure motifs.&lt;br/&gt;&lt;br/&gt;-methods for the efficient purification of colloidal gold nanoparticles and the implementation of surface-based DNA computing using such particles as supports. &lt;br/&gt;&lt;br/&gt;-development of a new DESTROY-UNMARKED operation, multiple-word AND operation, and multiple-word APPEND-MARKED operation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">130108</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130108</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n750" target="n751">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: Multiple-Word DNA Computing on Surfaces</data>
      <data key="e_abstract">EIA-0130108&lt;br/&gt;Lloyd M. Smith&lt;br/&gt;University of Wisconsin-Madison&lt;br/&gt;&lt;br/&gt;Title: Multiple-Word CNA Computing on Surfaces&lt;br/&gt;&lt;br/&gt;Under this project tools are being developed to increase the computational generality of surface-based DNA computing. It is well known that for a computing model to be general, that is, capable of efficiently simulating algorithms used in conventional electronic computing, it must be able to efficiently simulate circuits. It has been shown theoretically that the surface-based approach, when using multiple words and the MARK, DESTROY-UNMARKED, UNMARK, and APPEND operations, is a generalizable approach to computing, but this has not been implemented experimentally. The necessary tools for such an implementation is being developed. Particular tasks, which are being addressed to this end, include:&lt;br/&gt;&lt;br/&gt;-improved designs of sets of DNA &quot;words&quot; that do not interfere with one another in hybridization experiments and do not contain strong secondary structure motifs.&lt;br/&gt;&lt;br/&gt;-methods for the efficient purification of colloidal gold nanoparticles and the implementation of surface-based DNA computing using such particles as supports. &lt;br/&gt;&lt;br/&gt;-development of a new DESTROY-UNMARKED operation, multiple-word AND operation, and multiple-word APPEND-MARKED operation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">130108</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130108</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n753" target="n754">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: Causes of Robustness and Vulnerability in Real-world Networks: Lessons from Molecular Biology</data>
      <data key="e_abstract">EIA-0130059&lt;br/&gt;Animesh Ray&lt;br/&gt;Keck Graduate Institute&lt;br/&gt;&lt;br/&gt;Title: Causes of Robustness and Vulnerability in Real-world Networks: Lessons from molecular biology&lt;br/&gt;&lt;br/&gt;Vulnerability of natural networks (such as the Internet, power supply grid, or molecular regulatory circuits of cells) to perturbations is an important area of study. Prior work has traditionally involved observations on static networks or on computer simulations, and has revealed certain general fundamental properties. Specifically, the study of one representative natural network by direct experimental manipulation should illuminate general properties of most networks. The molecular machinery regulating the synthesis of RNA molecules in the nucleus of budding yeast forms a natural network that can be experimentally perturbed, and their effects studied by transcriptional profiling on DNA microarrays. In this initial funding period, study of the responses of a sub-section (comprised of approximately 300 nodes) of this natural network as a function of precise perturbations in the form of gene knockout mutations. A queriable, static, database model of the network is being made, which is queried by simple attack plans, and responses are tested for consistency with experimental results. This interplay is allowing a proof-of-principle demonstration of this empirical approach to studying properties of complex networks. Insights obtained from these studies is forming the basis of more sophisticated investigation of complex network properties.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">130059</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130059</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n753" target="n755">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: Causes of Robustness and Vulnerability in Real-world Networks: Lessons from Molecular Biology</data>
      <data key="e_abstract">EIA-0130059&lt;br/&gt;Animesh Ray&lt;br/&gt;Keck Graduate Institute&lt;br/&gt;&lt;br/&gt;Title: Causes of Robustness and Vulnerability in Real-world Networks: Lessons from molecular biology&lt;br/&gt;&lt;br/&gt;Vulnerability of natural networks (such as the Internet, power supply grid, or molecular regulatory circuits of cells) to perturbations is an important area of study. Prior work has traditionally involved observations on static networks or on computer simulations, and has revealed certain general fundamental properties. Specifically, the study of one representative natural network by direct experimental manipulation should illuminate general properties of most networks. The molecular machinery regulating the synthesis of RNA molecules in the nucleus of budding yeast forms a natural network that can be experimentally perturbed, and their effects studied by transcriptional profiling on DNA microarrays. In this initial funding period, study of the responses of a sub-section (comprised of approximately 300 nodes) of this natural network as a function of precise perturbations in the form of gene knockout mutations. A queriable, static, database model of the network is being made, which is queried by simple attack plans, and responses are tested for consistency with experimental results. This interplay is allowing a proof-of-principle demonstration of this empirical approach to studying properties of complex networks. Insights obtained from these studies is forming the basis of more sophisticated investigation of complex network properties.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">130059</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130059</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n753" target="n756">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: Causes of Robustness and Vulnerability in Real-world Networks: Lessons from Molecular Biology</data>
      <data key="e_abstract">EIA-0130059&lt;br/&gt;Animesh Ray&lt;br/&gt;Keck Graduate Institute&lt;br/&gt;&lt;br/&gt;Title: Causes of Robustness and Vulnerability in Real-world Networks: Lessons from molecular biology&lt;br/&gt;&lt;br/&gt;Vulnerability of natural networks (such as the Internet, power supply grid, or molecular regulatory circuits of cells) to perturbations is an important area of study. Prior work has traditionally involved observations on static networks or on computer simulations, and has revealed certain general fundamental properties. Specifically, the study of one representative natural network by direct experimental manipulation should illuminate general properties of most networks. The molecular machinery regulating the synthesis of RNA molecules in the nucleus of budding yeast forms a natural network that can be experimentally perturbed, and their effects studied by transcriptional profiling on DNA microarrays. In this initial funding period, study of the responses of a sub-section (comprised of approximately 300 nodes) of this natural network as a function of precise perturbations in the form of gene knockout mutations. A queriable, static, database model of the network is being made, which is queried by simple attack plans, and responses are tested for consistency with experimental results. This interplay is allowing a proof-of-principle demonstration of this empirical approach to studying properties of complex networks. Insights obtained from these studies is forming the basis of more sophisticated investigation of complex network properties.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">130059</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130059</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n754" target="n755">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: Causes of Robustness and Vulnerability in Real-world Networks: Lessons from Molecular Biology</data>
      <data key="e_abstract">EIA-0130059&lt;br/&gt;Animesh Ray&lt;br/&gt;Keck Graduate Institute&lt;br/&gt;&lt;br/&gt;Title: Causes of Robustness and Vulnerability in Real-world Networks: Lessons from molecular biology&lt;br/&gt;&lt;br/&gt;Vulnerability of natural networks (such as the Internet, power supply grid, or molecular regulatory circuits of cells) to perturbations is an important area of study. Prior work has traditionally involved observations on static networks or on computer simulations, and has revealed certain general fundamental properties. Specifically, the study of one representative natural network by direct experimental manipulation should illuminate general properties of most networks. The molecular machinery regulating the synthesis of RNA molecules in the nucleus of budding yeast forms a natural network that can be experimentally perturbed, and their effects studied by transcriptional profiling on DNA microarrays. In this initial funding period, study of the responses of a sub-section (comprised of approximately 300 nodes) of this natural network as a function of precise perturbations in the form of gene knockout mutations. A queriable, static, database model of the network is being made, which is queried by simple attack plans, and responses are tested for consistency with experimental results. This interplay is allowing a proof-of-principle demonstration of this empirical approach to studying properties of complex networks. Insights obtained from these studies is forming the basis of more sophisticated investigation of complex network properties.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">130059</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130059</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n754" target="n756">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: Causes of Robustness and Vulnerability in Real-world Networks: Lessons from Molecular Biology</data>
      <data key="e_abstract">EIA-0130059&lt;br/&gt;Animesh Ray&lt;br/&gt;Keck Graduate Institute&lt;br/&gt;&lt;br/&gt;Title: Causes of Robustness and Vulnerability in Real-world Networks: Lessons from molecular biology&lt;br/&gt;&lt;br/&gt;Vulnerability of natural networks (such as the Internet, power supply grid, or molecular regulatory circuits of cells) to perturbations is an important area of study. Prior work has traditionally involved observations on static networks or on computer simulations, and has revealed certain general fundamental properties. Specifically, the study of one representative natural network by direct experimental manipulation should illuminate general properties of most networks. The molecular machinery regulating the synthesis of RNA molecules in the nucleus of budding yeast forms a natural network that can be experimentally perturbed, and their effects studied by transcriptional profiling on DNA microarrays. In this initial funding period, study of the responses of a sub-section (comprised of approximately 300 nodes) of this natural network as a function of precise perturbations in the form of gene knockout mutations. A queriable, static, database model of the network is being made, which is queried by simple attack plans, and responses are tested for consistency with experimental results. This interplay is allowing a proof-of-principle demonstration of this empirical approach to studying properties of complex networks. Insights obtained from these studies is forming the basis of more sophisticated investigation of complex network properties.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">130059</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130059</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n755" target="n756">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: Causes of Robustness and Vulnerability in Real-world Networks: Lessons from Molecular Biology</data>
      <data key="e_abstract">EIA-0130059&lt;br/&gt;Animesh Ray&lt;br/&gt;Keck Graduate Institute&lt;br/&gt;&lt;br/&gt;Title: Causes of Robustness and Vulnerability in Real-world Networks: Lessons from molecular biology&lt;br/&gt;&lt;br/&gt;Vulnerability of natural networks (such as the Internet, power supply grid, or molecular regulatory circuits of cells) to perturbations is an important area of study. Prior work has traditionally involved observations on static networks or on computer simulations, and has revealed certain general fundamental properties. Specifically, the study of one representative natural network by direct experimental manipulation should illuminate general properties of most networks. The molecular machinery regulating the synthesis of RNA molecules in the nucleus of budding yeast forms a natural network that can be experimentally perturbed, and their effects studied by transcriptional profiling on DNA microarrays. In this initial funding period, study of the responses of a sub-section (comprised of approximately 300 nodes) of this natural network as a function of precise perturbations in the form of gene knockout mutations. A queriable, static, database model of the network is being made, which is queried by simple attack plans, and responses are tested for consistency with experimental results. This interplay is allowing a proof-of-principle demonstration of this empirical approach to studying properties of complex networks. Insights obtained from these studies is forming the basis of more sophisticated investigation of complex network properties.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">130059</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130059</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n757" target="n758">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Incorporating Fault Tolerance at the Application Level</data>
      <data key="e_abstract">Preliminary work by the PIs has shown that application-level information can be exploited to greatly reduce the amount of redundancy required to deal with transient failures, which are by far the most common type of failure. For example, in a radar target-tracking application, our approach required only 15% redundancy to provide complete fault-tolerance against transient faults. Another use of ALFT is in providing a temporary patch in the event of a permanent processor failure, allowing the system more time to execute a recovery algorithm.&lt;br/&gt;&lt;br/&gt;ALFT is orthogonal to other approaches to fault-tolerance, so that it can be used either by itself or in combination with them. For example, a designer might use ALFT to guard against transients, and make a small amount of hardware redundancy available, in the form of line-replaceable spares, to deal with permanent failures.&lt;br/&gt;&lt;br/&gt;The objective of this project is to develop Application-Level Fault Tolerance strategies and investigate their&lt;br/&gt;effectiveness for various classes of real-time applications. The main focus will be to generalize our approach to include as many different types of applications as possible, develop the most suitable strategy for each application type and evaluate the efficiency of the developed scheme.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">104482</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">104482</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n760" target="n761">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Repeated Maintenance of Open-Source Software</data>
      <data key="e_abstract">This proposal studies the evolution and subsequent maintenance behavior of code in two very large open-source projects, Linux and GCC free software, to provide insight into the software engineering aspects of open-source software. Focusing on the coupling between components and how this coupling changes over time, successive versions of Linux and GCC are examined, analyzing the change from version to version of each product. The coupling between two units of a software product, a measure of the degree of interaction between those units, is used as a measure of maintainability. Tools are built to compute these changes in coupling, and the output from these tools will be subjected to statistical analysis. The research sheds light on the importance in software maintenance of the skill of the individual software engineer and ameliorates the lack of data as to the effectiveness of &quot;open software&quot; development as compared to commercial software development.</data>
      <data key="e_pgm">2880</data>
      <data key="e_label">97056</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">97056</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n762" target="n763">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM(EHR): Course Capsules: Persistent Personalized Courseware</data>
      <data key="e_abstract">EIA-0113919&lt;br/&gt;Klaus Sutner&lt;br/&gt;Carnegie Mellon University&lt;br/&gt;&lt;br/&gt;Title: ITR/IM(EHR) Course Capsules: Persistent, Personalized Courseware&lt;br/&gt;&lt;br/&gt;This project will design and implement a system that will store, organize, index and make searchable course content, and preserve that content reliably over long periods of time. The specific plans include the systemic use of emerging XML standards to design a course model that supports both content and presentation markup. Course Capsules will not only archive the university generated course materials, but also the creative work of the students. Content markup will allow the Capsules to respond intelligently to requests by a user, and provide each student with a personalized depot of knowledge that remains available even after graduation. For the faculty, Capsules will form a trusted repository of course material and will encourage sharing and reuse. The archive in conjunction with extensive use tracking will provide a sound basis for ongoing review and improvement of curricula.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113919</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113919</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n762" target="n764">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM(EHR): Course Capsules: Persistent Personalized Courseware</data>
      <data key="e_abstract">EIA-0113919&lt;br/&gt;Klaus Sutner&lt;br/&gt;Carnegie Mellon University&lt;br/&gt;&lt;br/&gt;Title: ITR/IM(EHR) Course Capsules: Persistent, Personalized Courseware&lt;br/&gt;&lt;br/&gt;This project will design and implement a system that will store, organize, index and make searchable course content, and preserve that content reliably over long periods of time. The specific plans include the systemic use of emerging XML standards to design a course model that supports both content and presentation markup. Course Capsules will not only archive the university generated course materials, but also the creative work of the students. Content markup will allow the Capsules to respond intelligently to requests by a user, and provide each student with a personalized depot of knowledge that remains available even after graduation. For the faculty, Capsules will form a trusted repository of course material and will encourage sharing and reuse. The archive in conjunction with extensive use tracking will provide a sound basis for ongoing review and improvement of curricula.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113919</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113919</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n762" target="n765">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM(EHR): Course Capsules: Persistent Personalized Courseware</data>
      <data key="e_abstract">EIA-0113919&lt;br/&gt;Klaus Sutner&lt;br/&gt;Carnegie Mellon University&lt;br/&gt;&lt;br/&gt;Title: ITR/IM(EHR) Course Capsules: Persistent, Personalized Courseware&lt;br/&gt;&lt;br/&gt;This project will design and implement a system that will store, organize, index and make searchable course content, and preserve that content reliably over long periods of time. The specific plans include the systemic use of emerging XML standards to design a course model that supports both content and presentation markup. Course Capsules will not only archive the university generated course materials, but also the creative work of the students. Content markup will allow the Capsules to respond intelligently to requests by a user, and provide each student with a personalized depot of knowledge that remains available even after graduation. For the faculty, Capsules will form a trusted repository of course material and will encourage sharing and reuse. The archive in conjunction with extensive use tracking will provide a sound basis for ongoing review and improvement of curricula.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113919</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113919</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n762" target="n766">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM(EHR): Course Capsules: Persistent Personalized Courseware</data>
      <data key="e_abstract">EIA-0113919&lt;br/&gt;Klaus Sutner&lt;br/&gt;Carnegie Mellon University&lt;br/&gt;&lt;br/&gt;Title: ITR/IM(EHR) Course Capsules: Persistent, Personalized Courseware&lt;br/&gt;&lt;br/&gt;This project will design and implement a system that will store, organize, index and make searchable course content, and preserve that content reliably over long periods of time. The specific plans include the systemic use of emerging XML standards to design a course model that supports both content and presentation markup. Course Capsules will not only archive the university generated course materials, but also the creative work of the students. Content markup will allow the Capsules to respond intelligently to requests by a user, and provide each student with a personalized depot of knowledge that remains available even after graduation. For the faculty, Capsules will form a trusted repository of course material and will encourage sharing and reuse. The archive in conjunction with extensive use tracking will provide a sound basis for ongoing review and improvement of curricula.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113919</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113919</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n763" target="n764">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM(EHR): Course Capsules: Persistent Personalized Courseware</data>
      <data key="e_abstract">EIA-0113919&lt;br/&gt;Klaus Sutner&lt;br/&gt;Carnegie Mellon University&lt;br/&gt;&lt;br/&gt;Title: ITR/IM(EHR) Course Capsules: Persistent, Personalized Courseware&lt;br/&gt;&lt;br/&gt;This project will design and implement a system that will store, organize, index and make searchable course content, and preserve that content reliably over long periods of time. The specific plans include the systemic use of emerging XML standards to design a course model that supports both content and presentation markup. Course Capsules will not only archive the university generated course materials, but also the creative work of the students. Content markup will allow the Capsules to respond intelligently to requests by a user, and provide each student with a personalized depot of knowledge that remains available even after graduation. For the faculty, Capsules will form a trusted repository of course material and will encourage sharing and reuse. The archive in conjunction with extensive use tracking will provide a sound basis for ongoing review and improvement of curricula.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113919</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113919</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n763" target="n765">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM(EHR): Course Capsules: Persistent Personalized Courseware</data>
      <data key="e_abstract">EIA-0113919&lt;br/&gt;Klaus Sutner&lt;br/&gt;Carnegie Mellon University&lt;br/&gt;&lt;br/&gt;Title: ITR/IM(EHR) Course Capsules: Persistent, Personalized Courseware&lt;br/&gt;&lt;br/&gt;This project will design and implement a system that will store, organize, index and make searchable course content, and preserve that content reliably over long periods of time. The specific plans include the systemic use of emerging XML standards to design a course model that supports both content and presentation markup. Course Capsules will not only archive the university generated course materials, but also the creative work of the students. Content markup will allow the Capsules to respond intelligently to requests by a user, and provide each student with a personalized depot of knowledge that remains available even after graduation. For the faculty, Capsules will form a trusted repository of course material and will encourage sharing and reuse. The archive in conjunction with extensive use tracking will provide a sound basis for ongoing review and improvement of curricula.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113919</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113919</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n763" target="n766">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM(EHR): Course Capsules: Persistent Personalized Courseware</data>
      <data key="e_abstract">EIA-0113919&lt;br/&gt;Klaus Sutner&lt;br/&gt;Carnegie Mellon University&lt;br/&gt;&lt;br/&gt;Title: ITR/IM(EHR) Course Capsules: Persistent, Personalized Courseware&lt;br/&gt;&lt;br/&gt;This project will design and implement a system that will store, organize, index and make searchable course content, and preserve that content reliably over long periods of time. The specific plans include the systemic use of emerging XML standards to design a course model that supports both content and presentation markup. Course Capsules will not only archive the university generated course materials, but also the creative work of the students. Content markup will allow the Capsules to respond intelligently to requests by a user, and provide each student with a personalized depot of knowledge that remains available even after graduation. For the faculty, Capsules will form a trusted repository of course material and will encourage sharing and reuse. The archive in conjunction with extensive use tracking will provide a sound basis for ongoing review and improvement of curricula.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113919</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113919</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n764" target="n765">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM(EHR): Course Capsules: Persistent Personalized Courseware</data>
      <data key="e_abstract">EIA-0113919&lt;br/&gt;Klaus Sutner&lt;br/&gt;Carnegie Mellon University&lt;br/&gt;&lt;br/&gt;Title: ITR/IM(EHR) Course Capsules: Persistent, Personalized Courseware&lt;br/&gt;&lt;br/&gt;This project will design and implement a system that will store, organize, index and make searchable course content, and preserve that content reliably over long periods of time. The specific plans include the systemic use of emerging XML standards to design a course model that supports both content and presentation markup. Course Capsules will not only archive the university generated course materials, but also the creative work of the students. Content markup will allow the Capsules to respond intelligently to requests by a user, and provide each student with a personalized depot of knowledge that remains available even after graduation. For the faculty, Capsules will form a trusted repository of course material and will encourage sharing and reuse. The archive in conjunction with extensive use tracking will provide a sound basis for ongoing review and improvement of curricula.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113919</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113919</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n764" target="n766">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM(EHR): Course Capsules: Persistent Personalized Courseware</data>
      <data key="e_abstract">EIA-0113919&lt;br/&gt;Klaus Sutner&lt;br/&gt;Carnegie Mellon University&lt;br/&gt;&lt;br/&gt;Title: ITR/IM(EHR) Course Capsules: Persistent, Personalized Courseware&lt;br/&gt;&lt;br/&gt;This project will design and implement a system that will store, organize, index and make searchable course content, and preserve that content reliably over long periods of time. The specific plans include the systemic use of emerging XML standards to design a course model that supports both content and presentation markup. Course Capsules will not only archive the university generated course materials, but also the creative work of the students. Content markup will allow the Capsules to respond intelligently to requests by a user, and provide each student with a personalized depot of knowledge that remains available even after graduation. For the faculty, Capsules will form a trusted repository of course material and will encourage sharing and reuse. The archive in conjunction with extensive use tracking will provide a sound basis for ongoing review and improvement of curricula.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113919</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113919</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n765" target="n766">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM(EHR): Course Capsules: Persistent Personalized Courseware</data>
      <data key="e_abstract">EIA-0113919&lt;br/&gt;Klaus Sutner&lt;br/&gt;Carnegie Mellon University&lt;br/&gt;&lt;br/&gt;Title: ITR/IM(EHR) Course Capsules: Persistent, Personalized Courseware&lt;br/&gt;&lt;br/&gt;This project will design and implement a system that will store, organize, index and make searchable course content, and preserve that content reliably over long periods of time. The specific plans include the systemic use of emerging XML standards to design a course model that supports both content and presentation markup. Course Capsules will not only archive the university generated course materials, but also the creative work of the students. Content markup will allow the Capsules to respond intelligently to requests by a user, and provide each student with a personalized depot of knowledge that remains available even after graduation. For the faculty, Capsules will form a trusted repository of course material and will encourage sharing and reuse. The archive in conjunction with extensive use tracking will provide a sound basis for ongoing review and improvement of curricula.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113919</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113919</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n767" target="n768">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Mandatory Human Participation: A New Paradigm for Building Secure Systems</data>
      <data key="e_abstract">Currently, automatic attacks are a major threat to computer security. For example, the cheapest home PC can try thousands of &quot;probes&quot; against a targeted system. A brute-force password (or PIN number) guessing program can generate and try tens of thousands of candidate passwords each second. Or a home PC could attempt to flood a web site with thousands of &quot;bogus&quot; requests. &lt;br/&gt;&lt;br/&gt;While there methods that attempt to stop such attacks they all can be defeated to some degree. We propose a new approach to this security based on technology that can tell the difference between robots and humans. Thus, we can disallow automatic attacks. Our technology allows a new kind of restriction: now systems can insist that only humans have access to their valuable resources and they can disallow robots.&lt;br/&gt;&lt;br/&gt;The proposed solution to the problem is inspired by Turing&apos;s test for artificial intelligence. The fundamental idea of the solution is for a computer system to first ask the author of every transaction to solve a puzzle before accepting or executing the transaction. The content of the puzzle will be based on grand challenge problems in the domains of pattern recognition, visual interpretation, and natural language understanding. These problems have the essential property that people can solve them easily while computers are not likely to solve them in the foreseeable future. A typical puzzle would consist of the computer system sending the agent a bit-mapped image and the agent replying with an ascii string. The image might include a picture and a question and a question about that picture, such as &quot;Please type the following handwritten word&quot; or &quot;Which of the objects in this picture are edible?&quot; The computer system determines whether the transaction author is a human based on the answer supplied.&lt;br/&gt;&lt;br/&gt;This puzzle-solving process leads to a new framework for building secure computer systems. In this framework, a human being has to be directly involved (by solving the puzzle and typing in the answer) in the authentication or other processes that are vulnerable to automatic attacks, referred to as Mandatory Human Participation (MHP). Apparently, no automatic attack to the protected process would be possible under this framework.&lt;br/&gt;&lt;br/&gt;Our proposed research is to build a pilot system that can be used to demonstrate the basic idea of MHP. This will be based mostly on character based methods. We then, plan to carefuly test and measure how well our system performs and how well it is received by users.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113933</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">113933</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n767" target="n769">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Mandatory Human Participation: A New Paradigm for Building Secure Systems</data>
      <data key="e_abstract">Currently, automatic attacks are a major threat to computer security. For example, the cheapest home PC can try thousands of &quot;probes&quot; against a targeted system. A brute-force password (or PIN number) guessing program can generate and try tens of thousands of candidate passwords each second. Or a home PC could attempt to flood a web site with thousands of &quot;bogus&quot; requests. &lt;br/&gt;&lt;br/&gt;While there methods that attempt to stop such attacks they all can be defeated to some degree. We propose a new approach to this security based on technology that can tell the difference between robots and humans. Thus, we can disallow automatic attacks. Our technology allows a new kind of restriction: now systems can insist that only humans have access to their valuable resources and they can disallow robots.&lt;br/&gt;&lt;br/&gt;The proposed solution to the problem is inspired by Turing&apos;s test for artificial intelligence. The fundamental idea of the solution is for a computer system to first ask the author of every transaction to solve a puzzle before accepting or executing the transaction. The content of the puzzle will be based on grand challenge problems in the domains of pattern recognition, visual interpretation, and natural language understanding. These problems have the essential property that people can solve them easily while computers are not likely to solve them in the foreseeable future. A typical puzzle would consist of the computer system sending the agent a bit-mapped image and the agent replying with an ascii string. The image might include a picture and a question and a question about that picture, such as &quot;Please type the following handwritten word&quot; or &quot;Which of the objects in this picture are edible?&quot; The computer system determines whether the transaction author is a human based on the answer supplied.&lt;br/&gt;&lt;br/&gt;This puzzle-solving process leads to a new framework for building secure computer systems. In this framework, a human being has to be directly involved (by solving the puzzle and typing in the answer) in the authentication or other processes that are vulnerable to automatic attacks, referred to as Mandatory Human Participation (MHP). Apparently, no automatic attack to the protected process would be possible under this framework.&lt;br/&gt;&lt;br/&gt;Our proposed research is to build a pilot system that can be used to demonstrate the basic idea of MHP. This will be based mostly on character based methods. We then, plan to carefuly test and measure how well our system performs and how well it is received by users.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113933</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">113933</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n768" target="n769">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Mandatory Human Participation: A New Paradigm for Building Secure Systems</data>
      <data key="e_abstract">Currently, automatic attacks are a major threat to computer security. For example, the cheapest home PC can try thousands of &quot;probes&quot; against a targeted system. A brute-force password (or PIN number) guessing program can generate and try tens of thousands of candidate passwords each second. Or a home PC could attempt to flood a web site with thousands of &quot;bogus&quot; requests. &lt;br/&gt;&lt;br/&gt;While there methods that attempt to stop such attacks they all can be defeated to some degree. We propose a new approach to this security based on technology that can tell the difference between robots and humans. Thus, we can disallow automatic attacks. Our technology allows a new kind of restriction: now systems can insist that only humans have access to their valuable resources and they can disallow robots.&lt;br/&gt;&lt;br/&gt;The proposed solution to the problem is inspired by Turing&apos;s test for artificial intelligence. The fundamental idea of the solution is for a computer system to first ask the author of every transaction to solve a puzzle before accepting or executing the transaction. The content of the puzzle will be based on grand challenge problems in the domains of pattern recognition, visual interpretation, and natural language understanding. These problems have the essential property that people can solve them easily while computers are not likely to solve them in the foreseeable future. A typical puzzle would consist of the computer system sending the agent a bit-mapped image and the agent replying with an ascii string. The image might include a picture and a question and a question about that picture, such as &quot;Please type the following handwritten word&quot; or &quot;Which of the objects in this picture are edible?&quot; The computer system determines whether the transaction author is a human based on the answer supplied.&lt;br/&gt;&lt;br/&gt;This puzzle-solving process leads to a new framework for building secure computer systems. In this framework, a human being has to be directly involved (by solving the puzzle and typing in the answer) in the authentication or other processes that are vulnerable to automatic attacks, referred to as Mandatory Human Participation (MHP). Apparently, no automatic attack to the protected process would be possible under this framework.&lt;br/&gt;&lt;br/&gt;Our proposed research is to build a pilot system that can be used to demonstrate the basic idea of MHP. This will be based mostly on character based methods. We then, plan to carefuly test and measure how well our system performs and how well it is received by users.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113933</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">113933</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n772" target="n773">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Development of Instrumentation for AGAVE: The Access Grid Autostereo Virtual Environment</data>
      <data key="e_abstract">EIA-0115809&lt;br/&gt;Thomas A. DeFanti&lt;br/&gt;University of Illinois at Chicago&lt;br/&gt;&lt;br/&gt;MRI: Development of Instrumentation for AGAVE: the Access Grid Autostereo Virtual Environment &lt;br/&gt;&lt;br/&gt;This is a proposal for instrumentation development under the Major Research Instrumentation (MRI) program to support research and student training in tele-immersion technologies for a networked, collaborative virtual-reality environment. The focus is on AGAVE, a tiled, high-resolution autostereo display that integrates well with very-high-speed networks.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">115809</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">115809</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n772" target="n774">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Development of Instrumentation for AGAVE: The Access Grid Autostereo Virtual Environment</data>
      <data key="e_abstract">EIA-0115809&lt;br/&gt;Thomas A. DeFanti&lt;br/&gt;University of Illinois at Chicago&lt;br/&gt;&lt;br/&gt;MRI: Development of Instrumentation for AGAVE: the Access Grid Autostereo Virtual Environment &lt;br/&gt;&lt;br/&gt;This is a proposal for instrumentation development under the Major Research Instrumentation (MRI) program to support research and student training in tele-immersion technologies for a networked, collaborative virtual-reality environment. The focus is on AGAVE, a tiled, high-resolution autostereo display that integrates well with very-high-speed networks.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">115809</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">115809</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n772" target="n775">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Development of Instrumentation for AGAVE: The Access Grid Autostereo Virtual Environment</data>
      <data key="e_abstract">EIA-0115809&lt;br/&gt;Thomas A. DeFanti&lt;br/&gt;University of Illinois at Chicago&lt;br/&gt;&lt;br/&gt;MRI: Development of Instrumentation for AGAVE: the Access Grid Autostereo Virtual Environment &lt;br/&gt;&lt;br/&gt;This is a proposal for instrumentation development under the Major Research Instrumentation (MRI) program to support research and student training in tele-immersion technologies for a networked, collaborative virtual-reality environment. The focus is on AGAVE, a tiled, high-resolution autostereo display that integrates well with very-high-speed networks.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">115809</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">115809</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n773" target="n774">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Development of Instrumentation for AGAVE: The Access Grid Autostereo Virtual Environment</data>
      <data key="e_abstract">EIA-0115809&lt;br/&gt;Thomas A. DeFanti&lt;br/&gt;University of Illinois at Chicago&lt;br/&gt;&lt;br/&gt;MRI: Development of Instrumentation for AGAVE: the Access Grid Autostereo Virtual Environment &lt;br/&gt;&lt;br/&gt;This is a proposal for instrumentation development under the Major Research Instrumentation (MRI) program to support research and student training in tele-immersion technologies for a networked, collaborative virtual-reality environment. The focus is on AGAVE, a tiled, high-resolution autostereo display that integrates well with very-high-speed networks.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">115809</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">115809</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n773" target="n775">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Development of Instrumentation for AGAVE: The Access Grid Autostereo Virtual Environment</data>
      <data key="e_abstract">EIA-0115809&lt;br/&gt;Thomas A. DeFanti&lt;br/&gt;University of Illinois at Chicago&lt;br/&gt;&lt;br/&gt;MRI: Development of Instrumentation for AGAVE: the Access Grid Autostereo Virtual Environment &lt;br/&gt;&lt;br/&gt;This is a proposal for instrumentation development under the Major Research Instrumentation (MRI) program to support research and student training in tele-immersion technologies for a networked, collaborative virtual-reality environment. The focus is on AGAVE, a tiled, high-resolution autostereo display that integrates well with very-high-speed networks.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">115809</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">115809</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n774" target="n775">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Development of Instrumentation for AGAVE: The Access Grid Autostereo Virtual Environment</data>
      <data key="e_abstract">EIA-0115809&lt;br/&gt;Thomas A. DeFanti&lt;br/&gt;University of Illinois at Chicago&lt;br/&gt;&lt;br/&gt;MRI: Development of Instrumentation for AGAVE: the Access Grid Autostereo Virtual Environment &lt;br/&gt;&lt;br/&gt;This is a proposal for instrumentation development under the Major Research Instrumentation (MRI) program to support research and student training in tele-immersion technologies for a networked, collaborative virtual-reality environment. The focus is on AGAVE, a tiled, high-resolution autostereo display that integrates well with very-high-speed networks.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">115809</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">115809</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n776" target="n777">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Protein Logic</data>
      <data key="e_abstract">Divisions of Chemistry, Molecular and Cellular Biosciences, Chemical Transport Systems, Computer-Communications Research, and Experimental and Integrated Activities support this multidivisional award to University of Illinois Urbana-Champaign. This Nanoscale Interdisciplinary Research Team (NIRT) award is part of the Nanoscale Science and Engineering program. Under this project, an interdisciplinary team with Joseph Lyding as the principal investigator will develop protein-based logic chips that interfaces between biochemical reactions and conventional microfabricated silicon-based electronics such as metal-oxide semiconductor (MOSFETs) taking advantage of biocomplexity and electronic speed. These protein interfaced MOSFETs will help to create atomically accurate protein arrays to function as cellular nonlinear/neural network, and this in turn will help to over come the 100 nm limit in miniaturization of the present transistor technology. Industrial collaborations and outreach programs in the K-12 system will be part of the project. &lt;br/&gt;&lt;br/&gt;Under the award, ordered and atomically accurate protein arrays that interfaces between biochemical reactions and conventional microfabricated silicon-based electronics will be developed. Strong industrial collaboration will help in the industrial development and technology transfer of this science. In addition, the research program will provide multidisciplinary education and training opportunities in materials chemistry, protein chemistry and electronics to students from K-12 to post doctoral candidates.</data>
      <data key="e_pgm">1674</data>
      <data key="e_label">103447</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">103447</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n776" target="n778">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Protein Logic</data>
      <data key="e_abstract">Divisions of Chemistry, Molecular and Cellular Biosciences, Chemical Transport Systems, Computer-Communications Research, and Experimental and Integrated Activities support this multidivisional award to University of Illinois Urbana-Champaign. This Nanoscale Interdisciplinary Research Team (NIRT) award is part of the Nanoscale Science and Engineering program. Under this project, an interdisciplinary team with Joseph Lyding as the principal investigator will develop protein-based logic chips that interfaces between biochemical reactions and conventional microfabricated silicon-based electronics such as metal-oxide semiconductor (MOSFETs) taking advantage of biocomplexity and electronic speed. These protein interfaced MOSFETs will help to create atomically accurate protein arrays to function as cellular nonlinear/neural network, and this in turn will help to over come the 100 nm limit in miniaturization of the present transistor technology. Industrial collaborations and outreach programs in the K-12 system will be part of the project. &lt;br/&gt;&lt;br/&gt;Under the award, ordered and atomically accurate protein arrays that interfaces between biochemical reactions and conventional microfabricated silicon-based electronics will be developed. Strong industrial collaboration will help in the industrial development and technology transfer of this science. In addition, the research program will provide multidisciplinary education and training opportunities in materials chemistry, protein chemistry and electronics to students from K-12 to post doctoral candidates.</data>
      <data key="e_pgm">1674</data>
      <data key="e_label">103447</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">103447</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n776" target="n779">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Protein Logic</data>
      <data key="e_abstract">Divisions of Chemistry, Molecular and Cellular Biosciences, Chemical Transport Systems, Computer-Communications Research, and Experimental and Integrated Activities support this multidivisional award to University of Illinois Urbana-Champaign. This Nanoscale Interdisciplinary Research Team (NIRT) award is part of the Nanoscale Science and Engineering program. Under this project, an interdisciplinary team with Joseph Lyding as the principal investigator will develop protein-based logic chips that interfaces between biochemical reactions and conventional microfabricated silicon-based electronics such as metal-oxide semiconductor (MOSFETs) taking advantage of biocomplexity and electronic speed. These protein interfaced MOSFETs will help to create atomically accurate protein arrays to function as cellular nonlinear/neural network, and this in turn will help to over come the 100 nm limit in miniaturization of the present transistor technology. Industrial collaborations and outreach programs in the K-12 system will be part of the project. &lt;br/&gt;&lt;br/&gt;Under the award, ordered and atomically accurate protein arrays that interfaces between biochemical reactions and conventional microfabricated silicon-based electronics will be developed. Strong industrial collaboration will help in the industrial development and technology transfer of this science. In addition, the research program will provide multidisciplinary education and training opportunities in materials chemistry, protein chemistry and electronics to students from K-12 to post doctoral candidates.</data>
      <data key="e_pgm">1674</data>
      <data key="e_label">103447</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">103447</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n776" target="n780">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Protein Logic</data>
      <data key="e_abstract">Divisions of Chemistry, Molecular and Cellular Biosciences, Chemical Transport Systems, Computer-Communications Research, and Experimental and Integrated Activities support this multidivisional award to University of Illinois Urbana-Champaign. This Nanoscale Interdisciplinary Research Team (NIRT) award is part of the Nanoscale Science and Engineering program. Under this project, an interdisciplinary team with Joseph Lyding as the principal investigator will develop protein-based logic chips that interfaces between biochemical reactions and conventional microfabricated silicon-based electronics such as metal-oxide semiconductor (MOSFETs) taking advantage of biocomplexity and electronic speed. These protein interfaced MOSFETs will help to create atomically accurate protein arrays to function as cellular nonlinear/neural network, and this in turn will help to over come the 100 nm limit in miniaturization of the present transistor technology. Industrial collaborations and outreach programs in the K-12 system will be part of the project. &lt;br/&gt;&lt;br/&gt;Under the award, ordered and atomically accurate protein arrays that interfaces between biochemical reactions and conventional microfabricated silicon-based electronics will be developed. Strong industrial collaboration will help in the industrial development and technology transfer of this science. In addition, the research program will provide multidisciplinary education and training opportunities in materials chemistry, protein chemistry and electronics to students from K-12 to post doctoral candidates.</data>
      <data key="e_pgm">1674</data>
      <data key="e_label">103447</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">103447</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n777" target="n778">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Protein Logic</data>
      <data key="e_abstract">Divisions of Chemistry, Molecular and Cellular Biosciences, Chemical Transport Systems, Computer-Communications Research, and Experimental and Integrated Activities support this multidivisional award to University of Illinois Urbana-Champaign. This Nanoscale Interdisciplinary Research Team (NIRT) award is part of the Nanoscale Science and Engineering program. Under this project, an interdisciplinary team with Joseph Lyding as the principal investigator will develop protein-based logic chips that interfaces between biochemical reactions and conventional microfabricated silicon-based electronics such as metal-oxide semiconductor (MOSFETs) taking advantage of biocomplexity and electronic speed. These protein interfaced MOSFETs will help to create atomically accurate protein arrays to function as cellular nonlinear/neural network, and this in turn will help to over come the 100 nm limit in miniaturization of the present transistor technology. Industrial collaborations and outreach programs in the K-12 system will be part of the project. &lt;br/&gt;&lt;br/&gt;Under the award, ordered and atomically accurate protein arrays that interfaces between biochemical reactions and conventional microfabricated silicon-based electronics will be developed. Strong industrial collaboration will help in the industrial development and technology transfer of this science. In addition, the research program will provide multidisciplinary education and training opportunities in materials chemistry, protein chemistry and electronics to students from K-12 to post doctoral candidates.</data>
      <data key="e_pgm">1674</data>
      <data key="e_label">103447</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">103447</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n777" target="n779">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Protein Logic</data>
      <data key="e_abstract">Divisions of Chemistry, Molecular and Cellular Biosciences, Chemical Transport Systems, Computer-Communications Research, and Experimental and Integrated Activities support this multidivisional award to University of Illinois Urbana-Champaign. This Nanoscale Interdisciplinary Research Team (NIRT) award is part of the Nanoscale Science and Engineering program. Under this project, an interdisciplinary team with Joseph Lyding as the principal investigator will develop protein-based logic chips that interfaces between biochemical reactions and conventional microfabricated silicon-based electronics such as metal-oxide semiconductor (MOSFETs) taking advantage of biocomplexity and electronic speed. These protein interfaced MOSFETs will help to create atomically accurate protein arrays to function as cellular nonlinear/neural network, and this in turn will help to over come the 100 nm limit in miniaturization of the present transistor technology. Industrial collaborations and outreach programs in the K-12 system will be part of the project. &lt;br/&gt;&lt;br/&gt;Under the award, ordered and atomically accurate protein arrays that interfaces between biochemical reactions and conventional microfabricated silicon-based electronics will be developed. Strong industrial collaboration will help in the industrial development and technology transfer of this science. In addition, the research program will provide multidisciplinary education and training opportunities in materials chemistry, protein chemistry and electronics to students from K-12 to post doctoral candidates.</data>
      <data key="e_pgm">1674</data>
      <data key="e_label">103447</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">103447</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n777" target="n780">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Protein Logic</data>
      <data key="e_abstract">Divisions of Chemistry, Molecular and Cellular Biosciences, Chemical Transport Systems, Computer-Communications Research, and Experimental and Integrated Activities support this multidivisional award to University of Illinois Urbana-Champaign. This Nanoscale Interdisciplinary Research Team (NIRT) award is part of the Nanoscale Science and Engineering program. Under this project, an interdisciplinary team with Joseph Lyding as the principal investigator will develop protein-based logic chips that interfaces between biochemical reactions and conventional microfabricated silicon-based electronics such as metal-oxide semiconductor (MOSFETs) taking advantage of biocomplexity and electronic speed. These protein interfaced MOSFETs will help to create atomically accurate protein arrays to function as cellular nonlinear/neural network, and this in turn will help to over come the 100 nm limit in miniaturization of the present transistor technology. Industrial collaborations and outreach programs in the K-12 system will be part of the project. &lt;br/&gt;&lt;br/&gt;Under the award, ordered and atomically accurate protein arrays that interfaces between biochemical reactions and conventional microfabricated silicon-based electronics will be developed. Strong industrial collaboration will help in the industrial development and technology transfer of this science. In addition, the research program will provide multidisciplinary education and training opportunities in materials chemistry, protein chemistry and electronics to students from K-12 to post doctoral candidates.</data>
      <data key="e_pgm">1674</data>
      <data key="e_label">103447</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">103447</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n778" target="n779">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Protein Logic</data>
      <data key="e_abstract">Divisions of Chemistry, Molecular and Cellular Biosciences, Chemical Transport Systems, Computer-Communications Research, and Experimental and Integrated Activities support this multidivisional award to University of Illinois Urbana-Champaign. This Nanoscale Interdisciplinary Research Team (NIRT) award is part of the Nanoscale Science and Engineering program. Under this project, an interdisciplinary team with Joseph Lyding as the principal investigator will develop protein-based logic chips that interfaces between biochemical reactions and conventional microfabricated silicon-based electronics such as metal-oxide semiconductor (MOSFETs) taking advantage of biocomplexity and electronic speed. These protein interfaced MOSFETs will help to create atomically accurate protein arrays to function as cellular nonlinear/neural network, and this in turn will help to over come the 100 nm limit in miniaturization of the present transistor technology. Industrial collaborations and outreach programs in the K-12 system will be part of the project. &lt;br/&gt;&lt;br/&gt;Under the award, ordered and atomically accurate protein arrays that interfaces between biochemical reactions and conventional microfabricated silicon-based electronics will be developed. Strong industrial collaboration will help in the industrial development and technology transfer of this science. In addition, the research program will provide multidisciplinary education and training opportunities in materials chemistry, protein chemistry and electronics to students from K-12 to post doctoral candidates.</data>
      <data key="e_pgm">1674</data>
      <data key="e_label">103447</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">103447</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n778" target="n780">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Protein Logic</data>
      <data key="e_abstract">Divisions of Chemistry, Molecular and Cellular Biosciences, Chemical Transport Systems, Computer-Communications Research, and Experimental and Integrated Activities support this multidivisional award to University of Illinois Urbana-Champaign. This Nanoscale Interdisciplinary Research Team (NIRT) award is part of the Nanoscale Science and Engineering program. Under this project, an interdisciplinary team with Joseph Lyding as the principal investigator will develop protein-based logic chips that interfaces between biochemical reactions and conventional microfabricated silicon-based electronics such as metal-oxide semiconductor (MOSFETs) taking advantage of biocomplexity and electronic speed. These protein interfaced MOSFETs will help to create atomically accurate protein arrays to function as cellular nonlinear/neural network, and this in turn will help to over come the 100 nm limit in miniaturization of the present transistor technology. Industrial collaborations and outreach programs in the K-12 system will be part of the project. &lt;br/&gt;&lt;br/&gt;Under the award, ordered and atomically accurate protein arrays that interfaces between biochemical reactions and conventional microfabricated silicon-based electronics will be developed. Strong industrial collaboration will help in the industrial development and technology transfer of this science. In addition, the research program will provide multidisciplinary education and training opportunities in materials chemistry, protein chemistry and electronics to students from K-12 to post doctoral candidates.</data>
      <data key="e_pgm">1674</data>
      <data key="e_label">103447</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">103447</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n779" target="n780">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Protein Logic</data>
      <data key="e_abstract">Divisions of Chemistry, Molecular and Cellular Biosciences, Chemical Transport Systems, Computer-Communications Research, and Experimental and Integrated Activities support this multidivisional award to University of Illinois Urbana-Champaign. This Nanoscale Interdisciplinary Research Team (NIRT) award is part of the Nanoscale Science and Engineering program. Under this project, an interdisciplinary team with Joseph Lyding as the principal investigator will develop protein-based logic chips that interfaces between biochemical reactions and conventional microfabricated silicon-based electronics such as metal-oxide semiconductor (MOSFETs) taking advantage of biocomplexity and electronic speed. These protein interfaced MOSFETs will help to create atomically accurate protein arrays to function as cellular nonlinear/neural network, and this in turn will help to over come the 100 nm limit in miniaturization of the present transistor technology. Industrial collaborations and outreach programs in the K-12 system will be part of the project. &lt;br/&gt;&lt;br/&gt;Under the award, ordered and atomically accurate protein arrays that interfaces between biochemical reactions and conventional microfabricated silicon-based electronics will be developed. Strong industrial collaboration will help in the industrial development and technology transfer of this science. In addition, the research program will provide multidisciplinary education and training opportunities in materials chemistry, protein chemistry and electronics to students from K-12 to post doctoral candidates.</data>
      <data key="e_pgm">1674</data>
      <data key="e_label">103447</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">103447</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n782" target="n783">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">REU Site: Summer Research Experience for Undergraduates in the Geosciences</data>
      <data key="e_abstract">This award is to continue an REU site program at the Geophysical Institute of the University of Alaska. Building on success since 1987, this program will give further research opportunities to undergraduates majoring in the physical sciences. The program objective is to acquaint undergraduates with life and work at a research institute. The objective is achieved by providing summer educational work opportunities in which undergraduates become interns participating in the research activities at the Institute. Research topics concentrate on space physics and aeronomy, but also extend to atmospheric science, and in a minor way to solid state physics and laser physics. Students are assigned to conduct research with individual principal investigators who direct their work and serve as mentors during their internship. A common program of weekly lectures and field trips is designed to expose interns to fields of research beyond the scope of their project. Through their close relationships with their mentors, contacts with graduate students, attendance at seminars, lectures, and thesis defenses, and participation in field trips, the interns gain a first hand perspective on life and work as a research scientist.</data>
      <data key="e_pgm">T377</data>
      <data key="e_label">97871</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">97871</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n782" target="n784">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">REU Site: Summer Research Experience for Undergraduates in the Geosciences</data>
      <data key="e_abstract">This award is to continue an REU site program at the Geophysical Institute of the University of Alaska. Building on success since 1987, this program will give further research opportunities to undergraduates majoring in the physical sciences. The program objective is to acquaint undergraduates with life and work at a research institute. The objective is achieved by providing summer educational work opportunities in which undergraduates become interns participating in the research activities at the Institute. Research topics concentrate on space physics and aeronomy, but also extend to atmospheric science, and in a minor way to solid state physics and laser physics. Students are assigned to conduct research with individual principal investigators who direct their work and serve as mentors during their internship. A common program of weekly lectures and field trips is designed to expose interns to fields of research beyond the scope of their project. Through their close relationships with their mentors, contacts with graduate students, attendance at seminars, lectures, and thesis defenses, and participation in field trips, the interns gain a first hand perspective on life and work as a research scientist.</data>
      <data key="e_pgm">T377</data>
      <data key="e_label">97871</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">97871</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n783" target="n784">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">REU Site: Summer Research Experience for Undergraduates in the Geosciences</data>
      <data key="e_abstract">This award is to continue an REU site program at the Geophysical Institute of the University of Alaska. Building on success since 1987, this program will give further research opportunities to undergraduates majoring in the physical sciences. The program objective is to acquaint undergraduates with life and work at a research institute. The objective is achieved by providing summer educational work opportunities in which undergraduates become interns participating in the research activities at the Institute. Research topics concentrate on space physics and aeronomy, but also extend to atmospheric science, and in a minor way to solid state physics and laser physics. Students are assigned to conduct research with individual principal investigators who direct their work and serve as mentors during their internship. A common program of weekly lectures and field trips is designed to expose interns to fields of research beyond the scope of their project. Through their close relationships with their mentors, contacts with graduate students, attendance at seminars, lectures, and thesis defenses, and participation in field trips, the interns gain a first hand perspective on life and work as a research scientist.</data>
      <data key="e_pgm">T377</data>
      <data key="e_label">97871</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">97871</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n787" target="n788">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Critical Path Computing</data>
      <data key="e_abstract">Critical path prediction is a processor architecture technique that uses &lt;br/&gt;the past behavior of instructions in the instruction stream to predict which&lt;br/&gt;fetched instructions will be on the critical path; that is, which &lt;br/&gt;instructions will have a significant impact on processor performance, and &lt;br/&gt;which will not. This information can then be used to guide the selective &lt;br/&gt;application of a variety of processor optimizations.&lt;br/&gt;&lt;br/&gt;Modern processors remove most artificial constraints on execution&lt;br/&gt;throughput. Therefore, the bottleneck for many workloads on current &lt;br/&gt;processors is the true dependences in the code. Chains of dependent &lt;br/&gt;instructions constrain the overall throughput of the machine, often leaving &lt;br/&gt;aggressive processor technology highly underutilized. These chains of &lt;br/&gt;dependent instructions constitute the critical performance path, or &lt;br/&gt;critical path (CP), though the code.&lt;br/&gt;&lt;br/&gt;The performance of the processor is thus determined by the speed at&lt;br/&gt;which it executes the instructions along this critical path. In our&lt;br/&gt;efforts to get the maximum performance from the processor, it is no&lt;br/&gt;longer reasonable to treat all instructions the same. If we can know&lt;br/&gt;which instructions are critical to performance, we can accelerate&lt;br/&gt;their execution, possibly at the expense of instructions not on the&lt;br/&gt;critical path.&lt;br/&gt;&lt;br/&gt;This research will attempt to identify these critical instructions&lt;br/&gt;dynamically in hardware. We call this critical path prediction. This &lt;br/&gt;prediction is based on the behavior of previous invocations of the &lt;br/&gt;instruction in the pipeline. This prediction will enable the processor to &lt;br/&gt;make better decisions about where to apply certain policies and &lt;br/&gt;optimizations. A variety of critical path predictors will be examined.&lt;br/&gt;&lt;br/&gt;In many cases, critical path prediction will enable more effective &lt;br/&gt;application of other resources or optimizations. Possible applications of &lt;br/&gt;critical path prediction include guiding value prediction, instruction &lt;br/&gt;reuse, instruction issue priority, instruction scheduling on a clustered &lt;br/&gt;architecture, speculation control on a power-constrained processor, &lt;br/&gt;arbitration between instructions or threads on a multithreaded &lt;br/&gt;architecture, or to guide the spawning of speculative threads in a &lt;br/&gt;speculative multithreaded processor.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">105743</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105743</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n789" target="n790">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR: New Approaches to Human Capital Development through Information Technology Research</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">1359</data>
      <data key="e_label">296169</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">296169</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n789" target="n791">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR: New Approaches to Human Capital Development through Information Technology Research</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">1359</data>
      <data key="e_label">296169</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">296169</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n789" target="n792">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR: New Approaches to Human Capital Development through Information Technology Research</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">1359</data>
      <data key="e_label">296169</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">296169</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n789" target="n793">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR: New Approaches to Human Capital Development through Information Technology Research</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">1359</data>
      <data key="e_label">296169</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">296169</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n790" target="n791">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR: New Approaches to Human Capital Development through Information Technology Research</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">1359</data>
      <data key="e_label">296169</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">296169</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n790" target="n792">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR: New Approaches to Human Capital Development through Information Technology Research</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">1359</data>
      <data key="e_label">296169</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">296169</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n790" target="n793">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR: New Approaches to Human Capital Development through Information Technology Research</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">1359</data>
      <data key="e_label">296169</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">296169</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n791" target="n792">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR: New Approaches to Human Capital Development through Information Technology Research</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">1359</data>
      <data key="e_label">296169</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">296169</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n791" target="n793">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR: New Approaches to Human Capital Development through Information Technology Research</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">1359</data>
      <data key="e_label">296169</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">296169</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n792" target="n793">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR: New Approaches to Human Capital Development through Information Technology Research</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">1359</data>
      <data key="e_label">296169</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">296169</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n794" target="n795">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Infrastructure for Context-Aware Wireless Network Applications</data>
      <data key="e_abstract">Information systems are now mobile, wearable, multimodal, real-time, scalable from workstations to&lt;br/&gt;desktops to notebooks to palmtops to cellular, collaborative and ubiquitous. Network access is becoming&lt;br/&gt;increasingly important as a part of the computing infrastructure. Unfortunately, for the near term, un-limited bandwidth, anytime access to a network is not feasible, particularly in large urban environments&lt;br/&gt;where interference, occlusion and collision are ongoing problems. To alleviate this, we are developing a&lt;br/&gt;set of context-aware Autonomous Information Retrieval (AIR) pods. These are small, hardened low-cost&lt;br/&gt;computers that require only electric power. AIR pods have sufficient local storage to hold relatively static&lt;br/&gt;information and are equipped with two wireless interfaces: one short-range, unlicensed high-speed interface&lt;br/&gt;such as IEEE 802.11 and one long-range, low-speed interface such as CDPD or other Wide Area Network&lt;br/&gt;connection which may be intermittent. AIR pods can be either stationary or mobile. Stationary AIR pods&lt;br/&gt;can be attached to lamp posts and traffic lights, hidden in lighted store signs or in subway stations. Mobile&lt;br/&gt;AIR pods can be attached to delivery trucks, postal service vehicles, buses, police cruisers, taxis or other&lt;br/&gt;vehicles that roam city streets. AIR pods are also small enough to be carried in backpacks.&lt;br/&gt; We propose to develop and implement the prototype hardware and software AIR idea, and explore what&lt;br/&gt;kinds of infrastructure support are necessary to make these devices a key component of network applications.&lt;br/&gt;Research issues addressed will be cooperative data sharing, resource scheduling and anticipatory caching,&lt;br/&gt;message propagation and wide-area resource discovery.&lt;br/&gt; The proposed mobile networking infrastructure will be tested in a demanding set of context-aware mo-bile research projects. The first project is a wearable augmented reality system that allows outdoor users to tour a campus interactively. A second related project allows indoor users to collaborate with those outside over wireless networks. The third project is a mobile robot sensing system that can autonomously explore the campus and create rich 3D, texture mapped, site models. All of these applications need to interact with host computers through limited network access, and we propose to optimize this interaction over bandwidth, devices, and locality using context-aware wireless networks.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">99184</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">99184</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n794" target="n796">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Infrastructure for Context-Aware Wireless Network Applications</data>
      <data key="e_abstract">Information systems are now mobile, wearable, multimodal, real-time, scalable from workstations to&lt;br/&gt;desktops to notebooks to palmtops to cellular, collaborative and ubiquitous. Network access is becoming&lt;br/&gt;increasingly important as a part of the computing infrastructure. Unfortunately, for the near term, un-limited bandwidth, anytime access to a network is not feasible, particularly in large urban environments&lt;br/&gt;where interference, occlusion and collision are ongoing problems. To alleviate this, we are developing a&lt;br/&gt;set of context-aware Autonomous Information Retrieval (AIR) pods. These are small, hardened low-cost&lt;br/&gt;computers that require only electric power. AIR pods have sufficient local storage to hold relatively static&lt;br/&gt;information and are equipped with two wireless interfaces: one short-range, unlicensed high-speed interface&lt;br/&gt;such as IEEE 802.11 and one long-range, low-speed interface such as CDPD or other Wide Area Network&lt;br/&gt;connection which may be intermittent. AIR pods can be either stationary or mobile. Stationary AIR pods&lt;br/&gt;can be attached to lamp posts and traffic lights, hidden in lighted store signs or in subway stations. Mobile&lt;br/&gt;AIR pods can be attached to delivery trucks, postal service vehicles, buses, police cruisers, taxis or other&lt;br/&gt;vehicles that roam city streets. AIR pods are also small enough to be carried in backpacks.&lt;br/&gt; We propose to develop and implement the prototype hardware and software AIR idea, and explore what&lt;br/&gt;kinds of infrastructure support are necessary to make these devices a key component of network applications.&lt;br/&gt;Research issues addressed will be cooperative data sharing, resource scheduling and anticipatory caching,&lt;br/&gt;message propagation and wide-area resource discovery.&lt;br/&gt; The proposed mobile networking infrastructure will be tested in a demanding set of context-aware mo-bile research projects. The first project is a wearable augmented reality system that allows outdoor users to tour a campus interactively. A second related project allows indoor users to collaborate with those outside over wireless networks. The third project is a mobile robot sensing system that can autonomously explore the campus and create rich 3D, texture mapped, site models. All of these applications need to interact with host computers through limited network access, and we propose to optimize this interaction over bandwidth, devices, and locality using context-aware wireless networks.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">99184</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">99184</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n795" target="n796">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Infrastructure for Context-Aware Wireless Network Applications</data>
      <data key="e_abstract">Information systems are now mobile, wearable, multimodal, real-time, scalable from workstations to&lt;br/&gt;desktops to notebooks to palmtops to cellular, collaborative and ubiquitous. Network access is becoming&lt;br/&gt;increasingly important as a part of the computing infrastructure. Unfortunately, for the near term, un-limited bandwidth, anytime access to a network is not feasible, particularly in large urban environments&lt;br/&gt;where interference, occlusion and collision are ongoing problems. To alleviate this, we are developing a&lt;br/&gt;set of context-aware Autonomous Information Retrieval (AIR) pods. These are small, hardened low-cost&lt;br/&gt;computers that require only electric power. AIR pods have sufficient local storage to hold relatively static&lt;br/&gt;information and are equipped with two wireless interfaces: one short-range, unlicensed high-speed interface&lt;br/&gt;such as IEEE 802.11 and one long-range, low-speed interface such as CDPD or other Wide Area Network&lt;br/&gt;connection which may be intermittent. AIR pods can be either stationary or mobile. Stationary AIR pods&lt;br/&gt;can be attached to lamp posts and traffic lights, hidden in lighted store signs or in subway stations. Mobile&lt;br/&gt;AIR pods can be attached to delivery trucks, postal service vehicles, buses, police cruisers, taxis or other&lt;br/&gt;vehicles that roam city streets. AIR pods are also small enough to be carried in backpacks.&lt;br/&gt; We propose to develop and implement the prototype hardware and software AIR idea, and explore what&lt;br/&gt;kinds of infrastructure support are necessary to make these devices a key component of network applications.&lt;br/&gt;Research issues addressed will be cooperative data sharing, resource scheduling and anticipatory caching,&lt;br/&gt;message propagation and wide-area resource discovery.&lt;br/&gt; The proposed mobile networking infrastructure will be tested in a demanding set of context-aware mo-bile research projects. The first project is a wearable augmented reality system that allows outdoor users to tour a campus interactively. A second related project allows indoor users to collaborate with those outside over wireless networks. The third project is a mobile robot sensing system that can autonomously explore the campus and create rich 3D, texture mapped, site models. All of these applications need to interact with host computers through limited network access, and we propose to optimize this interaction over bandwidth, devices, and locality using context-aware wireless networks.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">99184</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">99184</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n797" target="n797">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Towards enabling a 2-3 orders of magnitude improvement in call handling capacities of switches</data>
      <data key="e_abstract">Toward building large-scale switches, the current industry focus is on increasing packet handling&lt;br/&gt;capacities of switch fabrics from Gb/s to Tb/s. Although an increase in packet handling&lt;br/&gt;capacities and line card data rates requires a corresponding increase in call handling capacities of&lt;br/&gt;switches, this problem has received little attention. This is because most of the work on scalability&lt;br/&gt;of packet switch fabrics has targeted connectionless internet protocol (IP) routers, while call handling&lt;br/&gt;arises only in connection-oriented networks. However, in the last few years, resource reservation&lt;br/&gt;to support Quality-of-Service (QoS) guaranteed flows has gained attention. Internet&lt;br/&gt;Engineering Task Force (IETF) is addressing this issue by augmenting IP routers with connection-oriented&lt;br/&gt;capabilities.&lt;br/&gt; In a connection-oriented network, the signaling protocol that is used to set up and release connections&lt;br/&gt;impacts its call handling capacity. Signaling messages can be complex with many parameters&lt;br/&gt;and timers and the state information associated with calls can become unwieldy.&lt;br/&gt;Consequently, signaling protocols have traditionally been implemented in software. QoS control&lt;br/&gt;solutions are being developed and evaluated based on the premise that call handling capacities do&lt;br/&gt;not scale with the packet handling capacities of switch fabrics. This assumption regarding call&lt;br/&gt;handling capacities has also relegated circuit-switched networks, including high-speed Wave-length&lt;br/&gt;Division Multiplexed networks, to just serve as wires. This proposal challenges this basic&lt;br/&gt;assumption by demonstrating call handling capacities in the order of millions of calls/sec. Changing&lt;br/&gt;this basic assumption regarding call handling capacities would indeed have a far-reaching&lt;br/&gt;impact on both QoS control mechanisms for packet-switched networks, and on the use of emerging&lt;br/&gt;high-speed circuit-switched networks for challenging new applications.&lt;br/&gt; Our solution approach is to implement signaling protocols in reconfigurable Field Programmable&lt;br/&gt;Gate Array (FPGA) hardware. FPGAs can be reprogrammed as signaling protocols evolve&lt;br/&gt;while significantly improving the call handling capacities relative to software implementation. To&lt;br/&gt;manage complexity, we propose to implement the basic and frequently-used operations of the protocol&lt;br/&gt;in hardware, and relegate the complex and infrequently-used operations to software. In contrast&lt;br/&gt;to stateless protocols, signaling protocols maintain state information for each call. To&lt;br/&gt;manage associated memory requirements, we propose to maintain only the essential state information&lt;br/&gt;for each call in hardware. In this project we propose to (i) implement a typical signaling&lt;br/&gt;protocol in FPGAs, (ii) design a switch controller board using the signaling protocol FPGAs, and&lt;br/&gt;(iii) quantify measures of the implementation to demonstrate achievable call handling capacities.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">87487</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">87487</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n797" target="n799">
      <data key="e_effectiveDate">None</data>
      <data key="e_title">None</data>
      <data key="e_abstract">None</data>
      <data key="e_pgm">None</data>
      <data key="e_expirationDate">None</data>
      <data key="e_div">None</data>
      <data key="e_dir">None</data>
    </edge>
    <edge source="n797" target="n799">
      <data key="e_effectiveDate">None</data>
      <data key="e_title">None</data>
      <data key="e_abstract">None</data>
      <data key="e_pgm">None</data>
      <data key="e_expirationDate">None</data>
      <data key="e_div">None</data>
      <data key="e_dir">None</data>
    </edge>
    <edge source="n797" target="n799">
      <data key="e_effectiveDate">None</data>
      <data key="e_title">None</data>
      <data key="e_abstract">None</data>
      <data key="e_pgm">None</data>
      <data key="e_expirationDate">None</data>
      <data key="e_div">None</data>
      <data key="e_dir">None</data>
    </edge>
    <edge source="n797" target="n799">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Towards enabling a 2-3 orders of magnitude improvement in call handling capacities of switches</data>
      <data key="e_abstract">Toward building large-scale switches, the current industry focus is on increasing packet handling&lt;br/&gt;capacities of switch fabrics from Gb/s to Tb/s. Although an increase in packet handling&lt;br/&gt;capacities and line card data rates requires a corresponding increase in call handling capacities of&lt;br/&gt;switches, this problem has received little attention. This is because most of the work on scalability&lt;br/&gt;of packet switch fabrics has targeted connectionless internet protocol (IP) routers, while call handling&lt;br/&gt;arises only in connection-oriented networks. However, in the last few years, resource reservation&lt;br/&gt;to support Quality-of-Service (QoS) guaranteed flows has gained attention. Internet&lt;br/&gt;Engineering Task Force (IETF) is addressing this issue by augmenting IP routers with connection-oriented&lt;br/&gt;capabilities.&lt;br/&gt; In a connection-oriented network, the signaling protocol that is used to set up and release connections&lt;br/&gt;impacts its call handling capacity. Signaling messages can be complex with many parameters&lt;br/&gt;and timers and the state information associated with calls can become unwieldy.&lt;br/&gt;Consequently, signaling protocols have traditionally been implemented in software. QoS control&lt;br/&gt;solutions are being developed and evaluated based on the premise that call handling capacities do&lt;br/&gt;not scale with the packet handling capacities of switch fabrics. This assumption regarding call&lt;br/&gt;handling capacities has also relegated circuit-switched networks, including high-speed Wave-length&lt;br/&gt;Division Multiplexed networks, to just serve as wires. This proposal challenges this basic&lt;br/&gt;assumption by demonstrating call handling capacities in the order of millions of calls/sec. Changing&lt;br/&gt;this basic assumption regarding call handling capacities would indeed have a far-reaching&lt;br/&gt;impact on both QoS control mechanisms for packet-switched networks, and on the use of emerging&lt;br/&gt;high-speed circuit-switched networks for challenging new applications.&lt;br/&gt; Our solution approach is to implement signaling protocols in reconfigurable Field Programmable&lt;br/&gt;Gate Array (FPGA) hardware. FPGAs can be reprogrammed as signaling protocols evolve&lt;br/&gt;while significantly improving the call handling capacities relative to software implementation. To&lt;br/&gt;manage complexity, we propose to implement the basic and frequently-used operations of the protocol&lt;br/&gt;in hardware, and relegate the complex and infrequently-used operations to software. In contrast&lt;br/&gt;to stateless protocols, signaling protocols maintain state information for each call. To&lt;br/&gt;manage associated memory requirements, we propose to maintain only the essential state information&lt;br/&gt;for each call in hardware. In this project we propose to (i) implement a typical signaling&lt;br/&gt;protocol in FPGAs, (ii) design a switch controller board using the signaling protocol FPGAs, and&lt;br/&gt;(iii) quantify measures of the implementation to demonstrate achievable call handling capacities.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">87487</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">87487</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n799" target="n799">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Towards enabling a 2-3 orders of magnitude improvement in call handling capacities of switches</data>
      <data key="e_abstract">Toward building large-scale switches, the current industry focus is on increasing packet handling&lt;br/&gt;capacities of switch fabrics from Gb/s to Tb/s. Although an increase in packet handling&lt;br/&gt;capacities and line card data rates requires a corresponding increase in call handling capacities of&lt;br/&gt;switches, this problem has received little attention. This is because most of the work on scalability&lt;br/&gt;of packet switch fabrics has targeted connectionless internet protocol (IP) routers, while call handling&lt;br/&gt;arises only in connection-oriented networks. However, in the last few years, resource reservation&lt;br/&gt;to support Quality-of-Service (QoS) guaranteed flows has gained attention. Internet&lt;br/&gt;Engineering Task Force (IETF) is addressing this issue by augmenting IP routers with connection-oriented&lt;br/&gt;capabilities.&lt;br/&gt; In a connection-oriented network, the signaling protocol that is used to set up and release connections&lt;br/&gt;impacts its call handling capacity. Signaling messages can be complex with many parameters&lt;br/&gt;and timers and the state information associated with calls can become unwieldy.&lt;br/&gt;Consequently, signaling protocols have traditionally been implemented in software. QoS control&lt;br/&gt;solutions are being developed and evaluated based on the premise that call handling capacities do&lt;br/&gt;not scale with the packet handling capacities of switch fabrics. This assumption regarding call&lt;br/&gt;handling capacities has also relegated circuit-switched networks, including high-speed Wave-length&lt;br/&gt;Division Multiplexed networks, to just serve as wires. This proposal challenges this basic&lt;br/&gt;assumption by demonstrating call handling capacities in the order of millions of calls/sec. Changing&lt;br/&gt;this basic assumption regarding call handling capacities would indeed have a far-reaching&lt;br/&gt;impact on both QoS control mechanisms for packet-switched networks, and on the use of emerging&lt;br/&gt;high-speed circuit-switched networks for challenging new applications.&lt;br/&gt; Our solution approach is to implement signaling protocols in reconfigurable Field Programmable&lt;br/&gt;Gate Array (FPGA) hardware. FPGAs can be reprogrammed as signaling protocols evolve&lt;br/&gt;while significantly improving the call handling capacities relative to software implementation. To&lt;br/&gt;manage complexity, we propose to implement the basic and frequently-used operations of the protocol&lt;br/&gt;in hardware, and relegate the complex and infrequently-used operations to software. In contrast&lt;br/&gt;to stateless protocols, signaling protocols maintain state information for each call. To&lt;br/&gt;manage associated memory requirements, we propose to maintain only the essential state information&lt;br/&gt;for each call in hardware. In this project we propose to (i) implement a typical signaling&lt;br/&gt;protocol in FPGAs, (ii) design a switch controller board using the signaling protocol FPGAs, and&lt;br/&gt;(iii) quantify measures of the implementation to demonstrate achievable call handling capacities.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">87487</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">87487</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n802" target="n803">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Collaborative Research: Restricted Caches, An Experimental and Theoretical Study</data>
      <data key="e_abstract">The gap between processor speed and main memory access speed&lt;br/&gt;can cause processors to spend much of their time waiting on memory accesses.&lt;br/&gt;As the gap has grown, this memory latency has become an&lt;br/&gt;increasingly significant bottleneck in processor performance.&lt;br/&gt;Existing cache designs have worked well to fill the gap,&lt;br/&gt;but new cache designs are needed as the gap continues to grow.&lt;br/&gt;A promising new class, restricted caches, includes&lt;br/&gt;skew caches, assist caches, victim caches, and other multi-lateral caches.&lt;br/&gt;Experiments have indicated that some restricted caches&lt;br/&gt;offer significant potential for improvement over traditional&lt;br/&gt;set-associative caches. They also have revealed some interesting&lt;br/&gt;phenomenon that are not possible in traditional caches. For example,&lt;br/&gt;skew caches seem to exhibit self-reorganization.&lt;br/&gt;However, no theoretical explanation exists for this behavior or for&lt;br/&gt;why these restricted caches perform well.&lt;br/&gt;&lt;br/&gt;The investigators study the performance of distinct restricted cache&lt;br/&gt;structures and algorithms for managing them.&lt;br/&gt;The investigators first identify an underlying structural difference&lt;br/&gt;between restricted caches and traditional fully-associative&lt;br/&gt;caches: all cache lines are not identical. Specifically, in a&lt;br/&gt;restricted cache, unlike in a traditional set-associative cache,&lt;br/&gt;there exist pairs of memory blocks whose sets of legal cache lines&lt;br/&gt;are not identical and have a non-empty intersection.&lt;br/&gt;Using this insight, the investigators evaluate and&lt;br/&gt;compare different cache structures using new techniques.&lt;br/&gt;Most other analytical studies of caches focus only on the performance&lt;br/&gt;of algorithms for a given cache structure and do not explicitly&lt;br/&gt;compare the effectiveness of distinct cache structures. The&lt;br/&gt;investigators also study the performance of various algorithms&lt;br/&gt;for these cache structures using a variety of techniques such&lt;br/&gt;as resource augmentation, standard competitive analysis,&lt;br/&gt;and trace-based simulation. Their results indicate that traditional&lt;br/&gt;cache management algorithms behave very differently on restricted&lt;br/&gt;caches than they do on traditional set-associative caches.&lt;br/&gt;For example, the least recently used (LRU) algorithm that is strongly&lt;br/&gt;competitive for traditional caches is not competitive at all for&lt;br/&gt;restricted caches unless explicit rearrangement of items in the&lt;br/&gt;cache is allowed. Finally, the investigators construct a&lt;br/&gt;trace warehouse to facilitate the comparison of distinct trace-based&lt;br/&gt;simulation studies as well as to help new researchers learn this&lt;br/&gt;this evaluation technique.</data>
      <data key="e_pgm">2860</data>
      <data key="e_label">105283</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105283</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n804" target="n805">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+AP Strategic Positioning in Information Product Space</data>
      <data key="e_abstract">Networked information technology has led to unprecedented opportunities for exchanging information in a population of participants whose interests and needs vary over time. In particular, this project concerns populations of consumers looking for information products, and producers who possess products that others might be seeking. Substantial research has gone into studying how producers and consumers can settle the terms of a transaction for a particular good. However, there are many possible variations and combinations of information products and their prices that can be offered. A critical and poorly understood problem is how parties should position themselves in this vast information product and price space to differentiate themselves from competitors and attract those with whom they should transact. To address this problem, the investigators will use economic analysis and computer simulation to study how producers of information goods can learn to position themselves based on criteria such as price schedules and information content, and can adapt to changing consumer tastes where consumers might be making strategic buying decisions to affect producer positioning. This project will extend economic theory to account for these concerns, and create computational agents that can make adaptive, strategic decisions about product positioning.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112669</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112669</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n806" target="n807">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Supervised Dexterous Manipulation with Haptic Feedback</data>
      <data key="e_abstract">The project focuses on a system for bringing enhanced tactile sensitivity and dexterity to supervised tele-manipulation. The proposed structure will utilize a combination of tactile, force, audio, and visual feedback channels between a human operator and a slave system consisting of a robot arm manipulator and a multi-fingered hand. The proposed approach builds on upon existing work in grasping and dexterous manipulation, telemanipulation, and tactile sensing. Future applications of this work include, maintenance, repair, exploration, and salvage operation in remote and hazardous environments.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">99636</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">99636</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n808" target="n809">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Foundations of Solid-State Quantum Information Processing</data>
      <data key="e_abstract">EIA-0121568&lt;br/&gt;Kwait, Paul G&lt;br/&gt;University of Illinois Urbana-Champaign&lt;br/&gt;&lt;br/&gt;ITR/SY: Foundations of Solid-State Quantum Information Processing&lt;br/&gt;&lt;br/&gt;&lt;br/&gt; Quantum Information Processing (QIP) lies at the forefront of revolutionary computing research, promising radically new powers to computation and communication, e.g., unconditionally secure quantum cryptography and quantum logic for greatly enhanced speed on certain computational problems. This project addresses the critical question of how to achieve a physical system capable of meeting the two most challenging requirements for building a quantum computer -- scalability, the fabrication and coupling of a large number of quantum bits (&quot;qubits&quot;), and quantum coherence, the control of noise and external coupling effects so that the exquisitely fragile quantum mechanical circuits will not be perturbed by unwanted influences.&lt;br/&gt;&lt;br/&gt; An interdisciplinary research team at the University of Illinois at Urbana-Champaign is exploring a wide range of solid state systems based on the manipulation and measurement of magnetic moments to perform quantum logic operations. By studying the full range, from single spins to small clusters of spins (in quantum dots), to large current loops in superconductors, they are attempting to assess the relative merit of different techniques, and determine the physical size limits for magnetic systems acting as qubits. The ultimate goal is the physical realization of a small system for performing quantum logic operations. A key component is the integration of research and education via a highly interactive program involving undergraduates, graduate students, and postdoctoral investigators. This project is providing a crucial role for an explosive new field such as QIP, by providing general awareness of the issues involved and by training a pool of experienced researchers.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121568</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121568</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n808" target="n810">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Foundations of Solid-State Quantum Information Processing</data>
      <data key="e_abstract">EIA-0121568&lt;br/&gt;Kwait, Paul G&lt;br/&gt;University of Illinois Urbana-Champaign&lt;br/&gt;&lt;br/&gt;ITR/SY: Foundations of Solid-State Quantum Information Processing&lt;br/&gt;&lt;br/&gt;&lt;br/&gt; Quantum Information Processing (QIP) lies at the forefront of revolutionary computing research, promising radically new powers to computation and communication, e.g., unconditionally secure quantum cryptography and quantum logic for greatly enhanced speed on certain computational problems. This project addresses the critical question of how to achieve a physical system capable of meeting the two most challenging requirements for building a quantum computer -- scalability, the fabrication and coupling of a large number of quantum bits (&quot;qubits&quot;), and quantum coherence, the control of noise and external coupling effects so that the exquisitely fragile quantum mechanical circuits will not be perturbed by unwanted influences.&lt;br/&gt;&lt;br/&gt; An interdisciplinary research team at the University of Illinois at Urbana-Champaign is exploring a wide range of solid state systems based on the manipulation and measurement of magnetic moments to perform quantum logic operations. By studying the full range, from single spins to small clusters of spins (in quantum dots), to large current loops in superconductors, they are attempting to assess the relative merit of different techniques, and determine the physical size limits for magnetic systems acting as qubits. The ultimate goal is the physical realization of a small system for performing quantum logic operations. A key component is the integration of research and education via a highly interactive program involving undergraduates, graduate students, and postdoctoral investigators. This project is providing a crucial role for an explosive new field such as QIP, by providing general awareness of the issues involved and by training a pool of experienced researchers.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121568</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121568</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n808" target="n811">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Foundations of Solid-State Quantum Information Processing</data>
      <data key="e_abstract">EIA-0121568&lt;br/&gt;Kwait, Paul G&lt;br/&gt;University of Illinois Urbana-Champaign&lt;br/&gt;&lt;br/&gt;ITR/SY: Foundations of Solid-State Quantum Information Processing&lt;br/&gt;&lt;br/&gt;&lt;br/&gt; Quantum Information Processing (QIP) lies at the forefront of revolutionary computing research, promising radically new powers to computation and communication, e.g., unconditionally secure quantum cryptography and quantum logic for greatly enhanced speed on certain computational problems. This project addresses the critical question of how to achieve a physical system capable of meeting the two most challenging requirements for building a quantum computer -- scalability, the fabrication and coupling of a large number of quantum bits (&quot;qubits&quot;), and quantum coherence, the control of noise and external coupling effects so that the exquisitely fragile quantum mechanical circuits will not be perturbed by unwanted influences.&lt;br/&gt;&lt;br/&gt; An interdisciplinary research team at the University of Illinois at Urbana-Champaign is exploring a wide range of solid state systems based on the manipulation and measurement of magnetic moments to perform quantum logic operations. By studying the full range, from single spins to small clusters of spins (in quantum dots), to large current loops in superconductors, they are attempting to assess the relative merit of different techniques, and determine the physical size limits for magnetic systems acting as qubits. The ultimate goal is the physical realization of a small system for performing quantum logic operations. A key component is the integration of research and education via a highly interactive program involving undergraduates, graduate students, and postdoctoral investigators. This project is providing a crucial role for an explosive new field such as QIP, by providing general awareness of the issues involved and by training a pool of experienced researchers.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121568</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121568</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n808" target="n812">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Foundations of Solid-State Quantum Information Processing</data>
      <data key="e_abstract">EIA-0121568&lt;br/&gt;Kwait, Paul G&lt;br/&gt;University of Illinois Urbana-Champaign&lt;br/&gt;&lt;br/&gt;ITR/SY: Foundations of Solid-State Quantum Information Processing&lt;br/&gt;&lt;br/&gt;&lt;br/&gt; Quantum Information Processing (QIP) lies at the forefront of revolutionary computing research, promising radically new powers to computation and communication, e.g., unconditionally secure quantum cryptography and quantum logic for greatly enhanced speed on certain computational problems. This project addresses the critical question of how to achieve a physical system capable of meeting the two most challenging requirements for building a quantum computer -- scalability, the fabrication and coupling of a large number of quantum bits (&quot;qubits&quot;), and quantum coherence, the control of noise and external coupling effects so that the exquisitely fragile quantum mechanical circuits will not be perturbed by unwanted influences.&lt;br/&gt;&lt;br/&gt; An interdisciplinary research team at the University of Illinois at Urbana-Champaign is exploring a wide range of solid state systems based on the manipulation and measurement of magnetic moments to perform quantum logic operations. By studying the full range, from single spins to small clusters of spins (in quantum dots), to large current loops in superconductors, they are attempting to assess the relative merit of different techniques, and determine the physical size limits for magnetic systems acting as qubits. The ultimate goal is the physical realization of a small system for performing quantum logic operations. A key component is the integration of research and education via a highly interactive program involving undergraduates, graduate students, and postdoctoral investigators. This project is providing a crucial role for an explosive new field such as QIP, by providing general awareness of the issues involved and by training a pool of experienced researchers.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121568</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121568</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n809" target="n810">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Foundations of Solid-State Quantum Information Processing</data>
      <data key="e_abstract">EIA-0121568&lt;br/&gt;Kwait, Paul G&lt;br/&gt;University of Illinois Urbana-Champaign&lt;br/&gt;&lt;br/&gt;ITR/SY: Foundations of Solid-State Quantum Information Processing&lt;br/&gt;&lt;br/&gt;&lt;br/&gt; Quantum Information Processing (QIP) lies at the forefront of revolutionary computing research, promising radically new powers to computation and communication, e.g., unconditionally secure quantum cryptography and quantum logic for greatly enhanced speed on certain computational problems. This project addresses the critical question of how to achieve a physical system capable of meeting the two most challenging requirements for building a quantum computer -- scalability, the fabrication and coupling of a large number of quantum bits (&quot;qubits&quot;), and quantum coherence, the control of noise and external coupling effects so that the exquisitely fragile quantum mechanical circuits will not be perturbed by unwanted influences.&lt;br/&gt;&lt;br/&gt; An interdisciplinary research team at the University of Illinois at Urbana-Champaign is exploring a wide range of solid state systems based on the manipulation and measurement of magnetic moments to perform quantum logic operations. By studying the full range, from single spins to small clusters of spins (in quantum dots), to large current loops in superconductors, they are attempting to assess the relative merit of different techniques, and determine the physical size limits for magnetic systems acting as qubits. The ultimate goal is the physical realization of a small system for performing quantum logic operations. A key component is the integration of research and education via a highly interactive program involving undergraduates, graduate students, and postdoctoral investigators. This project is providing a crucial role for an explosive new field such as QIP, by providing general awareness of the issues involved and by training a pool of experienced researchers.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121568</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121568</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n809" target="n811">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Foundations of Solid-State Quantum Information Processing</data>
      <data key="e_abstract">EIA-0121568&lt;br/&gt;Kwait, Paul G&lt;br/&gt;University of Illinois Urbana-Champaign&lt;br/&gt;&lt;br/&gt;ITR/SY: Foundations of Solid-State Quantum Information Processing&lt;br/&gt;&lt;br/&gt;&lt;br/&gt; Quantum Information Processing (QIP) lies at the forefront of revolutionary computing research, promising radically new powers to computation and communication, e.g., unconditionally secure quantum cryptography and quantum logic for greatly enhanced speed on certain computational problems. This project addresses the critical question of how to achieve a physical system capable of meeting the two most challenging requirements for building a quantum computer -- scalability, the fabrication and coupling of a large number of quantum bits (&quot;qubits&quot;), and quantum coherence, the control of noise and external coupling effects so that the exquisitely fragile quantum mechanical circuits will not be perturbed by unwanted influences.&lt;br/&gt;&lt;br/&gt; An interdisciplinary research team at the University of Illinois at Urbana-Champaign is exploring a wide range of solid state systems based on the manipulation and measurement of magnetic moments to perform quantum logic operations. By studying the full range, from single spins to small clusters of spins (in quantum dots), to large current loops in superconductors, they are attempting to assess the relative merit of different techniques, and determine the physical size limits for magnetic systems acting as qubits. The ultimate goal is the physical realization of a small system for performing quantum logic operations. A key component is the integration of research and education via a highly interactive program involving undergraduates, graduate students, and postdoctoral investigators. This project is providing a crucial role for an explosive new field such as QIP, by providing general awareness of the issues involved and by training a pool of experienced researchers.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121568</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121568</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n809" target="n812">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Foundations of Solid-State Quantum Information Processing</data>
      <data key="e_abstract">EIA-0121568&lt;br/&gt;Kwait, Paul G&lt;br/&gt;University of Illinois Urbana-Champaign&lt;br/&gt;&lt;br/&gt;ITR/SY: Foundations of Solid-State Quantum Information Processing&lt;br/&gt;&lt;br/&gt;&lt;br/&gt; Quantum Information Processing (QIP) lies at the forefront of revolutionary computing research, promising radically new powers to computation and communication, e.g., unconditionally secure quantum cryptography and quantum logic for greatly enhanced speed on certain computational problems. This project addresses the critical question of how to achieve a physical system capable of meeting the two most challenging requirements for building a quantum computer -- scalability, the fabrication and coupling of a large number of quantum bits (&quot;qubits&quot;), and quantum coherence, the control of noise and external coupling effects so that the exquisitely fragile quantum mechanical circuits will not be perturbed by unwanted influences.&lt;br/&gt;&lt;br/&gt; An interdisciplinary research team at the University of Illinois at Urbana-Champaign is exploring a wide range of solid state systems based on the manipulation and measurement of magnetic moments to perform quantum logic operations. By studying the full range, from single spins to small clusters of spins (in quantum dots), to large current loops in superconductors, they are attempting to assess the relative merit of different techniques, and determine the physical size limits for magnetic systems acting as qubits. The ultimate goal is the physical realization of a small system for performing quantum logic operations. A key component is the integration of research and education via a highly interactive program involving undergraduates, graduate students, and postdoctoral investigators. This project is providing a crucial role for an explosive new field such as QIP, by providing general awareness of the issues involved and by training a pool of experienced researchers.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121568</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121568</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n810" target="n811">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Foundations of Solid-State Quantum Information Processing</data>
      <data key="e_abstract">EIA-0121568&lt;br/&gt;Kwait, Paul G&lt;br/&gt;University of Illinois Urbana-Champaign&lt;br/&gt;&lt;br/&gt;ITR/SY: Foundations of Solid-State Quantum Information Processing&lt;br/&gt;&lt;br/&gt;&lt;br/&gt; Quantum Information Processing (QIP) lies at the forefront of revolutionary computing research, promising radically new powers to computation and communication, e.g., unconditionally secure quantum cryptography and quantum logic for greatly enhanced speed on certain computational problems. This project addresses the critical question of how to achieve a physical system capable of meeting the two most challenging requirements for building a quantum computer -- scalability, the fabrication and coupling of a large number of quantum bits (&quot;qubits&quot;), and quantum coherence, the control of noise and external coupling effects so that the exquisitely fragile quantum mechanical circuits will not be perturbed by unwanted influences.&lt;br/&gt;&lt;br/&gt; An interdisciplinary research team at the University of Illinois at Urbana-Champaign is exploring a wide range of solid state systems based on the manipulation and measurement of magnetic moments to perform quantum logic operations. By studying the full range, from single spins to small clusters of spins (in quantum dots), to large current loops in superconductors, they are attempting to assess the relative merit of different techniques, and determine the physical size limits for magnetic systems acting as qubits. The ultimate goal is the physical realization of a small system for performing quantum logic operations. A key component is the integration of research and education via a highly interactive program involving undergraduates, graduate students, and postdoctoral investigators. This project is providing a crucial role for an explosive new field such as QIP, by providing general awareness of the issues involved and by training a pool of experienced researchers.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121568</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121568</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n810" target="n812">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Foundations of Solid-State Quantum Information Processing</data>
      <data key="e_abstract">EIA-0121568&lt;br/&gt;Kwait, Paul G&lt;br/&gt;University of Illinois Urbana-Champaign&lt;br/&gt;&lt;br/&gt;ITR/SY: Foundations of Solid-State Quantum Information Processing&lt;br/&gt;&lt;br/&gt;&lt;br/&gt; Quantum Information Processing (QIP) lies at the forefront of revolutionary computing research, promising radically new powers to computation and communication, e.g., unconditionally secure quantum cryptography and quantum logic for greatly enhanced speed on certain computational problems. This project addresses the critical question of how to achieve a physical system capable of meeting the two most challenging requirements for building a quantum computer -- scalability, the fabrication and coupling of a large number of quantum bits (&quot;qubits&quot;), and quantum coherence, the control of noise and external coupling effects so that the exquisitely fragile quantum mechanical circuits will not be perturbed by unwanted influences.&lt;br/&gt;&lt;br/&gt; An interdisciplinary research team at the University of Illinois at Urbana-Champaign is exploring a wide range of solid state systems based on the manipulation and measurement of magnetic moments to perform quantum logic operations. By studying the full range, from single spins to small clusters of spins (in quantum dots), to large current loops in superconductors, they are attempting to assess the relative merit of different techniques, and determine the physical size limits for magnetic systems acting as qubits. The ultimate goal is the physical realization of a small system for performing quantum logic operations. A key component is the integration of research and education via a highly interactive program involving undergraduates, graduate students, and postdoctoral investigators. This project is providing a crucial role for an explosive new field such as QIP, by providing general awareness of the issues involved and by training a pool of experienced researchers.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121568</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121568</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n811" target="n812">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Foundations of Solid-State Quantum Information Processing</data>
      <data key="e_abstract">EIA-0121568&lt;br/&gt;Kwait, Paul G&lt;br/&gt;University of Illinois Urbana-Champaign&lt;br/&gt;&lt;br/&gt;ITR/SY: Foundations of Solid-State Quantum Information Processing&lt;br/&gt;&lt;br/&gt;&lt;br/&gt; Quantum Information Processing (QIP) lies at the forefront of revolutionary computing research, promising radically new powers to computation and communication, e.g., unconditionally secure quantum cryptography and quantum logic for greatly enhanced speed on certain computational problems. This project addresses the critical question of how to achieve a physical system capable of meeting the two most challenging requirements for building a quantum computer -- scalability, the fabrication and coupling of a large number of quantum bits (&quot;qubits&quot;), and quantum coherence, the control of noise and external coupling effects so that the exquisitely fragile quantum mechanical circuits will not be perturbed by unwanted influences.&lt;br/&gt;&lt;br/&gt; An interdisciplinary research team at the University of Illinois at Urbana-Champaign is exploring a wide range of solid state systems based on the manipulation and measurement of magnetic moments to perform quantum logic operations. By studying the full range, from single spins to small clusters of spins (in quantum dots), to large current loops in superconductors, they are attempting to assess the relative merit of different techniques, and determine the physical size limits for magnetic systems acting as qubits. The ultimate goal is the physical realization of a small system for performing quantum logic operations. A key component is the integration of research and education via a highly interactive program involving undergraduates, graduate students, and postdoctoral investigators. This project is providing a crucial role for an explosive new field such as QIP, by providing general awareness of the issues involved and by training a pool of experienced researchers.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121568</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121568</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n805" target="n814">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">IGERT: Socio-Technical Infrastructure for Electronic Transactions</data>
      <data key="e_abstract">This IGERT program is a multi-disciplinary doctoral traineeship in Socio-Technical Infrastructure for Electronic Transactions (STIET). The extraordinarily rapid changes in communications and computing technology, and in the uses and requirements people have for these technologies, have given rise to problems that are not well-suited to narrow, traditional solutions. That these problems are important should be self-evident: electronic transactions are increasingly central to social, economic and political activity in nearly every realm of human endeavor, within and between nearly every location on the planet. The infrastructure to support safe, meaningful, efficient, equitable and productive transactions will determine the extent to which the information revolution is socially beneficial. STIET will offer a comprehensive program from matriculation to graduation that will focus on the interaction between social and technical mechanisms in order to respond to these needs through current research and the training of a corps of scholars to carry forward teaching and research in this area. The program will: (i) provide fellowships for the first two years of graduate study; (ii) require 3 STIET core courses and 2 advanced STIET electives; (iii) provide a weekly research seminar and biannual day-long workshops; (iv) develop a multi-disciplinary, cross-school community of scholars within Michigan through collective and collaborative activities, both synchronous and asynchronous (with computer-supported community and collaboration technologies); and (v) build connections to the external multi-disciplinary research community through its Web site, conference travel, and research experiences at industrial partner facilities. When fully implemented, the program is expected to engage about 35 doctoral students at various stages of their degree. Through the resources and activities of STIET, the students will be encouraged and supported so that they receive serious preparation for multi-disciplinary research and pursue multi-disciplinary approaches to understanding and solving important socio-technical problems in their dissertation research. A large group of faculty are involved from several disciplines (computer science, economics, information, business, public policy) with a long-standing commitment to multi-disciplinary and collaborative research. The team includes collaborative research partners from prominent industrial labs. A professional Master&apos;s programs in this area is already in place, which provide a graduate student community, a student services infrastructure, and a fertile recruiting ground for promising doctoral students.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114368</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114368</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n814" target="n816">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">IGERT: Socio-Technical Infrastructure for Electronic Transactions</data>
      <data key="e_abstract">This IGERT program is a multi-disciplinary doctoral traineeship in Socio-Technical Infrastructure for Electronic Transactions (STIET). The extraordinarily rapid changes in communications and computing technology, and in the uses and requirements people have for these technologies, have given rise to problems that are not well-suited to narrow, traditional solutions. That these problems are important should be self-evident: electronic transactions are increasingly central to social, economic and political activity in nearly every realm of human endeavor, within and between nearly every location on the planet. The infrastructure to support safe, meaningful, efficient, equitable and productive transactions will determine the extent to which the information revolution is socially beneficial. STIET will offer a comprehensive program from matriculation to graduation that will focus on the interaction between social and technical mechanisms in order to respond to these needs through current research and the training of a corps of scholars to carry forward teaching and research in this area. The program will: (i) provide fellowships for the first two years of graduate study; (ii) require 3 STIET core courses and 2 advanced STIET electives; (iii) provide a weekly research seminar and biannual day-long workshops; (iv) develop a multi-disciplinary, cross-school community of scholars within Michigan through collective and collaborative activities, both synchronous and asynchronous (with computer-supported community and collaboration technologies); and (v) build connections to the external multi-disciplinary research community through its Web site, conference travel, and research experiences at industrial partner facilities. When fully implemented, the program is expected to engage about 35 doctoral students at various stages of their degree. Through the resources and activities of STIET, the students will be encouraged and supported so that they receive serious preparation for multi-disciplinary research and pursue multi-disciplinary approaches to understanding and solving important socio-technical problems in their dissertation research. A large group of faculty are involved from several disciplines (computer science, economics, information, business, public policy) with a long-standing commitment to multi-disciplinary and collaborative research. The team includes collaborative research partners from prominent industrial labs. A professional Master&apos;s programs in this area is already in place, which provide a graduate student community, a student services infrastructure, and a fertile recruiting ground for promising doctoral students.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114368</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114368</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n805" target="n816">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">IGERT: Socio-Technical Infrastructure for Electronic Transactions</data>
      <data key="e_abstract">This IGERT program is a multi-disciplinary doctoral traineeship in Socio-Technical Infrastructure for Electronic Transactions (STIET). The extraordinarily rapid changes in communications and computing technology, and in the uses and requirements people have for these technologies, have given rise to problems that are not well-suited to narrow, traditional solutions. That these problems are important should be self-evident: electronic transactions are increasingly central to social, economic and political activity in nearly every realm of human endeavor, within and between nearly every location on the planet. The infrastructure to support safe, meaningful, efficient, equitable and productive transactions will determine the extent to which the information revolution is socially beneficial. STIET will offer a comprehensive program from matriculation to graduation that will focus on the interaction between social and technical mechanisms in order to respond to these needs through current research and the training of a corps of scholars to carry forward teaching and research in this area. The program will: (i) provide fellowships for the first two years of graduate study; (ii) require 3 STIET core courses and 2 advanced STIET electives; (iii) provide a weekly research seminar and biannual day-long workshops; (iv) develop a multi-disciplinary, cross-school community of scholars within Michigan through collective and collaborative activities, both synchronous and asynchronous (with computer-supported community and collaboration technologies); and (v) build connections to the external multi-disciplinary research community through its Web site, conference travel, and research experiences at industrial partner facilities. When fully implemented, the program is expected to engage about 35 doctoral students at various stages of their degree. Through the resources and activities of STIET, the students will be encouraged and supported so that they receive serious preparation for multi-disciplinary research and pursue multi-disciplinary approaches to understanding and solving important socio-technical problems in their dissertation research. A large group of faculty are involved from several disciplines (computer science, economics, information, business, public policy) with a long-standing commitment to multi-disciplinary and collaborative research. The team includes collaborative research partners from prominent industrial labs. A professional Master&apos;s programs in this area is already in place, which provide a graduate student community, a student services infrastructure, and a fertile recruiting ground for promising doctoral students.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114368</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114368</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n39" target="n40">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Universal access for situationally induced impairments: Modeling, prototyping, and evaluation</data>
      <data key="e_abstract">As mobile computing becomes more pervasive, users enjoy increased flexibility in terms of where and when they record, retrieve, and transmit information; at the same time, the conditions under which these devices are used are becoming more variable, less predictable, and in many situations less hospitable. With increasing frequency, computers are being used when lighting is poor, noise is unpredictable, or when the user is on the move (e.g., walking, driving a vehicle). In addition, mobile devices often cause users to interrupt an ongoing activity in order to perform secondary computer-based tasks: examples include individuals replying to e-mails during meetings, doctors reviewing operating room schedules while interacting with patients, and individuals retrieving directions from their in-vehicle navigation system while driving. The goal of this research is to address the issues involved in developing effective computer systems for individuals experiencing such situationally-induced impairments (SII). Like disability-induced impairments (DII), SII exist when the physical, cognitive, or perceptual demands placed on the user exceed their abilities. Unlike DII, SII are temporary, resulting from the environment in which the work is being performed or the tasks in which the user is engaged. Through this research, the PI and his team will develop new techniques for identifying and documenting the factors that contribute to SII, will identify methods for developing solutions that address the temporary and dynamic nature of SII, and will compare the interaction strategies of individuals experiencing SII to those of individuals with comparable DII.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121570</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121570</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n39" target="n819">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Universal access for situationally induced impairments: Modeling, prototyping, and evaluation</data>
      <data key="e_abstract">As mobile computing becomes more pervasive, users enjoy increased flexibility in terms of where and when they record, retrieve, and transmit information; at the same time, the conditions under which these devices are used are becoming more variable, less predictable, and in many situations less hospitable. With increasing frequency, computers are being used when lighting is poor, noise is unpredictable, or when the user is on the move (e.g., walking, driving a vehicle). In addition, mobile devices often cause users to interrupt an ongoing activity in order to perform secondary computer-based tasks: examples include individuals replying to e-mails during meetings, doctors reviewing operating room schedules while interacting with patients, and individuals retrieving directions from their in-vehicle navigation system while driving. The goal of this research is to address the issues involved in developing effective computer systems for individuals experiencing such situationally-induced impairments (SII). Like disability-induced impairments (DII), SII exist when the physical, cognitive, or perceptual demands placed on the user exceed their abilities. Unlike DII, SII are temporary, resulting from the environment in which the work is being performed or the tasks in which the user is engaged. Through this research, the PI and his team will develop new techniques for identifying and documenting the factors that contribute to SII, will identify methods for developing solutions that address the temporary and dynamic nature of SII, and will compare the interaction strategies of individuals experiencing SII to those of individuals with comparable DII.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121570</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121570</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n40" target="n819">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Universal access for situationally induced impairments: Modeling, prototyping, and evaluation</data>
      <data key="e_abstract">As mobile computing becomes more pervasive, users enjoy increased flexibility in terms of where and when they record, retrieve, and transmit information; at the same time, the conditions under which these devices are used are becoming more variable, less predictable, and in many situations less hospitable. With increasing frequency, computers are being used when lighting is poor, noise is unpredictable, or when the user is on the move (e.g., walking, driving a vehicle). In addition, mobile devices often cause users to interrupt an ongoing activity in order to perform secondary computer-based tasks: examples include individuals replying to e-mails during meetings, doctors reviewing operating room schedules while interacting with patients, and individuals retrieving directions from their in-vehicle navigation system while driving. The goal of this research is to address the issues involved in developing effective computer systems for individuals experiencing such situationally-induced impairments (SII). Like disability-induced impairments (DII), SII exist when the physical, cognitive, or perceptual demands placed on the user exceed their abilities. Unlike DII, SII are temporary, resulting from the environment in which the work is being performed or the tasks in which the user is engaged. Through this research, the PI and his team will develop new techniques for identifying and documenting the factors that contribute to SII, will identify methods for developing solutions that address the temporary and dynamic nature of SII, and will compare the interaction strategies of individuals experiencing SII to those of individuals with comparable DII.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121570</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121570</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n824" target="n825">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Collaborative Research: A Control Theoretic Approach to the Design of Internet Traffic Managers</data>
      <data key="e_abstract">The scalability of the Internet hinges on our ability to tame the unpredictability associated with its open architecture. This project investigates the development of basic control strategies for reducing traffic burstiness and improving network utilization. Such strategies can be applied through Traffic Managers (TMs)-special network elements strategically placed in the Internet (e.g., in front of clients/servers or at exchange/peering points between administrative domains). We believe that the incorporation of such control functionalities will be key to the ability of the network infrastructure to sustain its own growth and to nurture the Quality-of-Service (QoS) needs of emerging applications.&lt;br/&gt;&lt;br/&gt;Although there have been some recent advances in building network elements capable of wire-speed processing, there is a need for fundamental research into the basic QoS control capabilities that these TMs should implement. This set of capabilities have to be identified and implemented in a programmable, scalable architecture that allows for the easy and effective composition of services. Such a flexible architecture is highly desirable as the Internet continues to evolve and users demand new kinds of service for their applications. &lt;br/&gt;&lt;br/&gt;TMs should be capable of quickly inspecting and classifying packets as they go by (e.g., marking packets into precedence classes), and should control the transmission of these packets (e.g., through pacing, scheduling, or selective dropping) to ensure desirable properties (e.g., satisfaction of jitter requirements, compliance with TCP friendliness, or improved fairness across flows).&lt;br/&gt;&lt;br/&gt;In this proposal, we will address the design of dynamic dos control programmable TMs. We focus on basic capabilities that could be employed at different levels of the control architecture. These capabilities include differentiated, aggregate and proxy controls. The following are examples of how such control strategies would be employed by TMs.&lt;br/&gt;&lt;br/&gt;Differentiated Control enables TMs to route flow aggregates with divergent characteristics on separate communication paths. Unlike traditional routing, our routing metrics will respect bursitis measures, such as self-similarity and traffic correlation:&lt;br/&gt;&lt;br/&gt;Aggregate Control enables TMs to use congestion control mechanisms for collections of flows that share the same bottleneck. Unlike traditional congestion control, &quot;Congestion-equivalent&quot; flows are identified based on measures of relationship (such as cross-correlation and cross-covariance) and managed as a set; &lt;br/&gt;&lt;br/&gt;Proxy Control enables TMs to filter out variability (e.g., loss, delay jitter) at shorter time-scales. Such a functionality is crucial for improving the stability and effectiveness of control mechanisms that operate over longer time-scales (e.g., end-to-end). Unlike traditional a-hoc proxy approaches, our approach will take into account the length and characteristics of the control loops that get formed between the TM and the end-systems.&lt;br/&gt;&lt;br/&gt;Our design will be based on mathematical foundations from control theory and wavelet analysis. These methods enable thorough analysis and control of system dynamics at different time-scales and an understanding of the complex interactions among them. Specifically, functionality&apos;s at different levels of a TM architecture will be developed based integrated control-theoretic models. These models will account for &quot;nested&quot; control loops that are driven by system characteristics, which are identified using wavelet analysis of passive measurements. TMs that are designed in such and integrated fashion, could increase flow throughput, reduce flow jitter and response time, and improve the stability, utilization, and scalability of the network.&lt;br/&gt;&lt;br/&gt;We plan to implement our dos controls in a tested deployed in a controlled local setting as well as over the Internet. Our implementations will be based on emerging technologies, such as Diffserv and MPLS, and will be stressed by bandwidth-and QoS-demanding applications. Our testbed will provide a programming interface to softservices, in which capabilities can be turned on or off and control parameters can be dynamically adjusted. To this end, we have secured the support of industrial research laboratories and start-up companies-namely Lucent&apos;s Bell Labs, Cisco Systems, Nortel Networks, and Quarry Technologies. Specifically, we intend to use Lucent&apos;s Network Element for Programmable Packet Injection (NEPPI). NEPPI provides an ideal foundation upon which to implement the control policies we propose to develop. This project is a collaborative efforts between Boston University (Is: Ibrahim Matta, Azer Bestavros, and Mark Crovella) with expertize in characterization, measurements and control of Internet traffic, and University of Arizona (PI: Marwan Krunz) with expertize in traffic modeling, multimedia and wireless QoS.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">95988</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">95988</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n824" target="n826">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Collaborative Research: A Control Theoretic Approach to the Design of Internet Traffic Managers</data>
      <data key="e_abstract">The scalability of the Internet hinges on our ability to tame the unpredictability associated with its open architecture. This project investigates the development of basic control strategies for reducing traffic burstiness and improving network utilization. Such strategies can be applied through Traffic Managers (TMs)-special network elements strategically placed in the Internet (e.g., in front of clients/servers or at exchange/peering points between administrative domains). We believe that the incorporation of such control functionalities will be key to the ability of the network infrastructure to sustain its own growth and to nurture the Quality-of-Service (QoS) needs of emerging applications.&lt;br/&gt;&lt;br/&gt;Although there have been some recent advances in building network elements capable of wire-speed processing, there is a need for fundamental research into the basic QoS control capabilities that these TMs should implement. This set of capabilities have to be identified and implemented in a programmable, scalable architecture that allows for the easy and effective composition of services. Such a flexible architecture is highly desirable as the Internet continues to evolve and users demand new kinds of service for their applications. &lt;br/&gt;&lt;br/&gt;TMs should be capable of quickly inspecting and classifying packets as they go by (e.g., marking packets into precedence classes), and should control the transmission of these packets (e.g., through pacing, scheduling, or selective dropping) to ensure desirable properties (e.g., satisfaction of jitter requirements, compliance with TCP friendliness, or improved fairness across flows).&lt;br/&gt;&lt;br/&gt;In this proposal, we will address the design of dynamic dos control programmable TMs. We focus on basic capabilities that could be employed at different levels of the control architecture. These capabilities include differentiated, aggregate and proxy controls. The following are examples of how such control strategies would be employed by TMs.&lt;br/&gt;&lt;br/&gt;Differentiated Control enables TMs to route flow aggregates with divergent characteristics on separate communication paths. Unlike traditional routing, our routing metrics will respect bursitis measures, such as self-similarity and traffic correlation:&lt;br/&gt;&lt;br/&gt;Aggregate Control enables TMs to use congestion control mechanisms for collections of flows that share the same bottleneck. Unlike traditional congestion control, &quot;Congestion-equivalent&quot; flows are identified based on measures of relationship (such as cross-correlation and cross-covariance) and managed as a set; &lt;br/&gt;&lt;br/&gt;Proxy Control enables TMs to filter out variability (e.g., loss, delay jitter) at shorter time-scales. Such a functionality is crucial for improving the stability and effectiveness of control mechanisms that operate over longer time-scales (e.g., end-to-end). Unlike traditional a-hoc proxy approaches, our approach will take into account the length and characteristics of the control loops that get formed between the TM and the end-systems.&lt;br/&gt;&lt;br/&gt;Our design will be based on mathematical foundations from control theory and wavelet analysis. These methods enable thorough analysis and control of system dynamics at different time-scales and an understanding of the complex interactions among them. Specifically, functionality&apos;s at different levels of a TM architecture will be developed based integrated control-theoretic models. These models will account for &quot;nested&quot; control loops that are driven by system characteristics, which are identified using wavelet analysis of passive measurements. TMs that are designed in such and integrated fashion, could increase flow throughput, reduce flow jitter and response time, and improve the stability, utilization, and scalability of the network.&lt;br/&gt;&lt;br/&gt;We plan to implement our dos controls in a tested deployed in a controlled local setting as well as over the Internet. Our implementations will be based on emerging technologies, such as Diffserv and MPLS, and will be stressed by bandwidth-and QoS-demanding applications. Our testbed will provide a programming interface to softservices, in which capabilities can be turned on or off and control parameters can be dynamically adjusted. To this end, we have secured the support of industrial research laboratories and start-up companies-namely Lucent&apos;s Bell Labs, Cisco Systems, Nortel Networks, and Quarry Technologies. Specifically, we intend to use Lucent&apos;s Network Element for Programmable Packet Injection (NEPPI). NEPPI provides an ideal foundation upon which to implement the control policies we propose to develop. This project is a collaborative efforts between Boston University (Is: Ibrahim Matta, Azer Bestavros, and Mark Crovella) with expertize in characterization, measurements and control of Internet traffic, and University of Arizona (PI: Marwan Krunz) with expertize in traffic modeling, multimedia and wireless QoS.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">95988</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">95988</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n825" target="n826">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Collaborative Research: A Control Theoretic Approach to the Design of Internet Traffic Managers</data>
      <data key="e_abstract">The scalability of the Internet hinges on our ability to tame the unpredictability associated with its open architecture. This project investigates the development of basic control strategies for reducing traffic burstiness and improving network utilization. Such strategies can be applied through Traffic Managers (TMs)-special network elements strategically placed in the Internet (e.g., in front of clients/servers or at exchange/peering points between administrative domains). We believe that the incorporation of such control functionalities will be key to the ability of the network infrastructure to sustain its own growth and to nurture the Quality-of-Service (QoS) needs of emerging applications.&lt;br/&gt;&lt;br/&gt;Although there have been some recent advances in building network elements capable of wire-speed processing, there is a need for fundamental research into the basic QoS control capabilities that these TMs should implement. This set of capabilities have to be identified and implemented in a programmable, scalable architecture that allows for the easy and effective composition of services. Such a flexible architecture is highly desirable as the Internet continues to evolve and users demand new kinds of service for their applications. &lt;br/&gt;&lt;br/&gt;TMs should be capable of quickly inspecting and classifying packets as they go by (e.g., marking packets into precedence classes), and should control the transmission of these packets (e.g., through pacing, scheduling, or selective dropping) to ensure desirable properties (e.g., satisfaction of jitter requirements, compliance with TCP friendliness, or improved fairness across flows).&lt;br/&gt;&lt;br/&gt;In this proposal, we will address the design of dynamic dos control programmable TMs. We focus on basic capabilities that could be employed at different levels of the control architecture. These capabilities include differentiated, aggregate and proxy controls. The following are examples of how such control strategies would be employed by TMs.&lt;br/&gt;&lt;br/&gt;Differentiated Control enables TMs to route flow aggregates with divergent characteristics on separate communication paths. Unlike traditional routing, our routing metrics will respect bursitis measures, such as self-similarity and traffic correlation:&lt;br/&gt;&lt;br/&gt;Aggregate Control enables TMs to use congestion control mechanisms for collections of flows that share the same bottleneck. Unlike traditional congestion control, &quot;Congestion-equivalent&quot; flows are identified based on measures of relationship (such as cross-correlation and cross-covariance) and managed as a set; &lt;br/&gt;&lt;br/&gt;Proxy Control enables TMs to filter out variability (e.g., loss, delay jitter) at shorter time-scales. Such a functionality is crucial for improving the stability and effectiveness of control mechanisms that operate over longer time-scales (e.g., end-to-end). Unlike traditional a-hoc proxy approaches, our approach will take into account the length and characteristics of the control loops that get formed between the TM and the end-systems.&lt;br/&gt;&lt;br/&gt;Our design will be based on mathematical foundations from control theory and wavelet analysis. These methods enable thorough analysis and control of system dynamics at different time-scales and an understanding of the complex interactions among them. Specifically, functionality&apos;s at different levels of a TM architecture will be developed based integrated control-theoretic models. These models will account for &quot;nested&quot; control loops that are driven by system characteristics, which are identified using wavelet analysis of passive measurements. TMs that are designed in such and integrated fashion, could increase flow throughput, reduce flow jitter and response time, and improve the stability, utilization, and scalability of the network.&lt;br/&gt;&lt;br/&gt;We plan to implement our dos controls in a tested deployed in a controlled local setting as well as over the Internet. Our implementations will be based on emerging technologies, such as Diffserv and MPLS, and will be stressed by bandwidth-and QoS-demanding applications. Our testbed will provide a programming interface to softservices, in which capabilities can be turned on or off and control parameters can be dynamically adjusted. To this end, we have secured the support of industrial research laboratories and start-up companies-namely Lucent&apos;s Bell Labs, Cisco Systems, Nortel Networks, and Quarry Technologies. Specifically, we intend to use Lucent&apos;s Network Element for Programmable Packet Injection (NEPPI). NEPPI provides an ideal foundation upon which to implement the control policies we propose to develop. This project is a collaborative efforts between Boston University (Is: Ibrahim Matta, Azer Bestavros, and Mark Crovella) with expertize in characterization, measurements and control of Internet traffic, and University of Arizona (PI: Marwan Krunz) with expertize in traffic modeling, multimedia and wireless QoS.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">95988</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">95988</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n827" target="n828">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Spatial Dynamics and Fluctuations at Population Margins</data>
      <data key="e_abstract">There has been an increasing interest in incorporating explicit spatial structure into ecological models. This research will investigate the effect of spatial structure on the dynamics of populations at the edges and borders of their ranges. The investigators will develop analytic and numerical approaches using methods from theoretical physics. They will study the spatial patterns that develop at different types of population margins, and how these patterns change when margins are either expanding or retreating. They will investigate how the range of a population in one dimension (as along a stream or a coastline) might differ from that of a two-dimensional population in an identical environmental gradient. The investigators will also study the effect of biotic factors, e.g. pathogens, on marginal populations, and whether margins can act as host refugia.&lt;br/&gt;&lt;br/&gt;The goal of this project is to increase general understanding of the dynamics of population margins, as well as to stimulate and direct the detailed study of such margins. In the past decade, the spatial analysis of populations has been greatly facilitated by rapid technical advances in mapping, remote sensing, and computation. A strong theoretical basis for the spatial structure of population margins is an essential guide for the interpretation of such data.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">108513</data>
      <data key="e_expirationDate">2003-04-30</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">108513</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n831" target="n832">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/(PE+SY): Responsive Virtual Human Technology Research</data>
      <data key="e_abstract">ITR/\(PE+SY\): Responsive Virtual Human Technology Research&lt;br/&gt;&lt;br/&gt;Responsive virtual human technology (RVHT) is used in diverse fields (computer generated forces, manufacturing, medicine, theater), but not for interaction skills training. Yet interaction skills are usually critical. Specific situations identified where improved interaction skills would be important include: &lt;br/&gt;Medical practitioners taking patient histories or interacting with children;&lt;br/&gt;Law officers handling crisis situations involving mental illness, trauma, or violence; and Military officers interviewing refugees or settling stressed civilians.&lt;br/&gt;This project will address multiple research issues relevant for RVHT to reach the sophistication required for robust interaction skills training. Important questions to be answered include:&lt;br/&gt; How is behavior modeled under normal conditions (i.e., a calm adult) and derivative conditions (e.g., anger, schizophrenia, pain, and childhood)?&lt;br/&gt; What expressions, gestures, movement, and other behaviors will users interpret as serene, angry, schizophrenic, pained, or childlike?&lt;br/&gt;What skills can be acquired, practiced, and validated using RVHT? What is involved in providing a convincing simulation of human interaction where acquired skills transfer to a live environment?&lt;br/&gt;The research results will expose a range of additional training and educational opportunities, such as interviewing risky behavior and presenting rare, traumatic events. Combinations of RVHT-based training and instructor-led training offer significantly reduced training development and delivery costs, and increased student throughput, while maintaining training effectiveness and consistency.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121211</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121211</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n833" target="n834">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Ultra-High-Capacity Optical Communications and Networking: &quot;Smart RF/Photonic Antennas&quot; for Ultra-High Capacity Wireless Communications</data>
      <data key="e_abstract">The need for more bandwidth and capacity in wireless systems currently is the main culprit for the&lt;br/&gt;great interest in the development of wireless communications systems operating at millimeter wave&lt;br/&gt;frequencies and higher. The future needs of broad-band interactive services (1Gb/s) demand the&lt;br/&gt;application of optical fiber feed networks for distribution of the radio signals to and from the antennas at&lt;br/&gt;the various base stations. Fiber-optic technologies have reached the stage where insertions into various&lt;br/&gt;commercial RF systems are being considered. Today, there are three main steps in the evolution of&lt;br/&gt;RF/Photonics systems for wireless communications. The first step has been in the direction of using&lt;br/&gt;photonics to slowly replace conventional RF components, such as, the coax that is used to interconnect&lt;br/&gt;the antenna to the electronics. Optical fibers, in contrast to coaxial cable, provide a more ideal medium for&lt;br/&gt;broadband RF communication systems. The light weight property of fibers, and its immunity from other&lt;br/&gt;signal interference make them very critical in the development of future RF distribution systems. The&lt;br/&gt;second, and more challenging step, is in the seamless integration of photonics and RF wireless circuits.&lt;br/&gt;The challenge in this step is to use photonics and RF circuits as complementary systems and blend them&lt;br/&gt;together. Finally, the third step is towards the development of optically coupled antennas. In this step the&lt;br/&gt;aim is to eliminate the need of local oscillators, mixers, amplifiers and a host of other parts by directly&lt;br/&gt;feeding an antenna through a fiber at millimeter wave frequencies.&lt;br/&gt; Here, it is proposed that an array of RF modulator/photodetectors be integrated directly to an array&lt;br/&gt;of antennas. This new RF/photonic antenna array system, with the appropriate space-time processing and&lt;br/&gt;coding, will form a iosmart antennaln that can enhance network coverage, capacity, and quality. It is&lt;br/&gt;envisioned that a large number of such RF/Photonic antenna elements could be networked together into a&lt;br/&gt;star configuration, feeding in and out of a radio hub.&lt;br/&gt; As a transmitter, the proposed optoelectronic device operates as a photodiode, while as a receiver&lt;br/&gt;the device operates as an optical modulator. It has already been demonstrated that this dual function of a&lt;br/&gt;semiconductor electroabsorption modulator and photodiode in the same device for duplex operation, can&lt;br/&gt;occur, using bias control as a transmit/receive mode control. For full duplex operation, two&lt;br/&gt;modulator/photodiode devices need to be incorporated in the each transceiver element.&lt;br/&gt; We propose to directly drive a coplanar waveguide (CPW)-fed slot antenna by converting optical&lt;br/&gt;power into microwave power and vice versa using these RF modulator/photodetectors. As a transmitter,&lt;br/&gt;the CPW line is connected to the active surface of the photodetector, from which the microwave power&lt;br/&gt;propagates to feed the radiating slot. The photodetector is fed via an optical fiber from beneath. When the&lt;br/&gt;device functions as an optical modulator, the receive function can also be achieved. Preliminary results&lt;br/&gt;for a single antenna show that a very good bandwidth and radiation patterns can be achieved.&lt;br/&gt; It should be noted that these elements can be interconnected via the fiber to achieve summation,&lt;br/&gt;mixing and other signal processing functions, at the antenna site or at a remote site. Some preliminary&lt;br/&gt;results have been achieved in the area of multiple functionality for the optoelectronic components, such as&lt;br/&gt;modulation, photodetection, self-biasing and RF frequency mixing. They have shown properties, such as&lt;br/&gt;high bandwidth and high power, that are desirable for the antenna applications. A main emphasis here is&lt;br/&gt;to further investigate the material and device designs for the optoelectronic component that can&lt;br/&gt;incorporate into the smart antenna architecture.&lt;br/&gt; The proposed approach will have significant impacts on wireless communication systems by&lt;br/&gt;providing higher system bandwidth capacity and enhancing their reliability. It may lead to a new type of&lt;br/&gt;long distance, broadband network infrastructure that supports transparent transport of optical signals.&lt;br/&gt; Our team is formed to provide the expertise in the four key elements for this proposed research.&lt;br/&gt;Our project will provide a good opportunity to train graduate and undergraduate students in one of the&lt;br/&gt;most exciting interdisciplinary areas in science (RF, photonics, signal processing and communications).&lt;br/&gt;The interactions between the researchers at the different institutions will be aided by the close&lt;br/&gt;collaboration that exists between the members of the group.</data>
      <data key="e_pgm">4090</data>
      <data key="e_label">123421</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">123421</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n833" target="n835">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Ultra-High-Capacity Optical Communications and Networking: &quot;Smart RF/Photonic Antennas&quot; for Ultra-High Capacity Wireless Communications</data>
      <data key="e_abstract">The need for more bandwidth and capacity in wireless systems currently is the main culprit for the&lt;br/&gt;great interest in the development of wireless communications systems operating at millimeter wave&lt;br/&gt;frequencies and higher. The future needs of broad-band interactive services (1Gb/s) demand the&lt;br/&gt;application of optical fiber feed networks for distribution of the radio signals to and from the antennas at&lt;br/&gt;the various base stations. Fiber-optic technologies have reached the stage where insertions into various&lt;br/&gt;commercial RF systems are being considered. Today, there are three main steps in the evolution of&lt;br/&gt;RF/Photonics systems for wireless communications. The first step has been in the direction of using&lt;br/&gt;photonics to slowly replace conventional RF components, such as, the coax that is used to interconnect&lt;br/&gt;the antenna to the electronics. Optical fibers, in contrast to coaxial cable, provide a more ideal medium for&lt;br/&gt;broadband RF communication systems. The light weight property of fibers, and its immunity from other&lt;br/&gt;signal interference make them very critical in the development of future RF distribution systems. The&lt;br/&gt;second, and more challenging step, is in the seamless integration of photonics and RF wireless circuits.&lt;br/&gt;The challenge in this step is to use photonics and RF circuits as complementary systems and blend them&lt;br/&gt;together. Finally, the third step is towards the development of optically coupled antennas. In this step the&lt;br/&gt;aim is to eliminate the need of local oscillators, mixers, amplifiers and a host of other parts by directly&lt;br/&gt;feeding an antenna through a fiber at millimeter wave frequencies.&lt;br/&gt; Here, it is proposed that an array of RF modulator/photodetectors be integrated directly to an array&lt;br/&gt;of antennas. This new RF/photonic antenna array system, with the appropriate space-time processing and&lt;br/&gt;coding, will form a iosmart antennaln that can enhance network coverage, capacity, and quality. It is&lt;br/&gt;envisioned that a large number of such RF/Photonic antenna elements could be networked together into a&lt;br/&gt;star configuration, feeding in and out of a radio hub.&lt;br/&gt; As a transmitter, the proposed optoelectronic device operates as a photodiode, while as a receiver&lt;br/&gt;the device operates as an optical modulator. It has already been demonstrated that this dual function of a&lt;br/&gt;semiconductor electroabsorption modulator and photodiode in the same device for duplex operation, can&lt;br/&gt;occur, using bias control as a transmit/receive mode control. For full duplex operation, two&lt;br/&gt;modulator/photodiode devices need to be incorporated in the each transceiver element.&lt;br/&gt; We propose to directly drive a coplanar waveguide (CPW)-fed slot antenna by converting optical&lt;br/&gt;power into microwave power and vice versa using these RF modulator/photodetectors. As a transmitter,&lt;br/&gt;the CPW line is connected to the active surface of the photodetector, from which the microwave power&lt;br/&gt;propagates to feed the radiating slot. The photodetector is fed via an optical fiber from beneath. When the&lt;br/&gt;device functions as an optical modulator, the receive function can also be achieved. Preliminary results&lt;br/&gt;for a single antenna show that a very good bandwidth and radiation patterns can be achieved.&lt;br/&gt; It should be noted that these elements can be interconnected via the fiber to achieve summation,&lt;br/&gt;mixing and other signal processing functions, at the antenna site or at a remote site. Some preliminary&lt;br/&gt;results have been achieved in the area of multiple functionality for the optoelectronic components, such as&lt;br/&gt;modulation, photodetection, self-biasing and RF frequency mixing. They have shown properties, such as&lt;br/&gt;high bandwidth and high power, that are desirable for the antenna applications. A main emphasis here is&lt;br/&gt;to further investigate the material and device designs for the optoelectronic component that can&lt;br/&gt;incorporate into the smart antenna architecture.&lt;br/&gt; The proposed approach will have significant impacts on wireless communication systems by&lt;br/&gt;providing higher system bandwidth capacity and enhancing their reliability. It may lead to a new type of&lt;br/&gt;long distance, broadband network infrastructure that supports transparent transport of optical signals.&lt;br/&gt; Our team is formed to provide the expertise in the four key elements for this proposed research.&lt;br/&gt;Our project will provide a good opportunity to train graduate and undergraduate students in one of the&lt;br/&gt;most exciting interdisciplinary areas in science (RF, photonics, signal processing and communications).&lt;br/&gt;The interactions between the researchers at the different institutions will be aided by the close&lt;br/&gt;collaboration that exists between the members of the group.</data>
      <data key="e_pgm">4090</data>
      <data key="e_label">123421</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">123421</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n834" target="n835">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Ultra-High-Capacity Optical Communications and Networking: &quot;Smart RF/Photonic Antennas&quot; for Ultra-High Capacity Wireless Communications</data>
      <data key="e_abstract">The need for more bandwidth and capacity in wireless systems currently is the main culprit for the&lt;br/&gt;great interest in the development of wireless communications systems operating at millimeter wave&lt;br/&gt;frequencies and higher. The future needs of broad-band interactive services (1Gb/s) demand the&lt;br/&gt;application of optical fiber feed networks for distribution of the radio signals to and from the antennas at&lt;br/&gt;the various base stations. Fiber-optic technologies have reached the stage where insertions into various&lt;br/&gt;commercial RF systems are being considered. Today, there are three main steps in the evolution of&lt;br/&gt;RF/Photonics systems for wireless communications. The first step has been in the direction of using&lt;br/&gt;photonics to slowly replace conventional RF components, such as, the coax that is used to interconnect&lt;br/&gt;the antenna to the electronics. Optical fibers, in contrast to coaxial cable, provide a more ideal medium for&lt;br/&gt;broadband RF communication systems. The light weight property of fibers, and its immunity from other&lt;br/&gt;signal interference make them very critical in the development of future RF distribution systems. The&lt;br/&gt;second, and more challenging step, is in the seamless integration of photonics and RF wireless circuits.&lt;br/&gt;The challenge in this step is to use photonics and RF circuits as complementary systems and blend them&lt;br/&gt;together. Finally, the third step is towards the development of optically coupled antennas. In this step the&lt;br/&gt;aim is to eliminate the need of local oscillators, mixers, amplifiers and a host of other parts by directly&lt;br/&gt;feeding an antenna through a fiber at millimeter wave frequencies.&lt;br/&gt; Here, it is proposed that an array of RF modulator/photodetectors be integrated directly to an array&lt;br/&gt;of antennas. This new RF/photonic antenna array system, with the appropriate space-time processing and&lt;br/&gt;coding, will form a iosmart antennaln that can enhance network coverage, capacity, and quality. It is&lt;br/&gt;envisioned that a large number of such RF/Photonic antenna elements could be networked together into a&lt;br/&gt;star configuration, feeding in and out of a radio hub.&lt;br/&gt; As a transmitter, the proposed optoelectronic device operates as a photodiode, while as a receiver&lt;br/&gt;the device operates as an optical modulator. It has already been demonstrated that this dual function of a&lt;br/&gt;semiconductor electroabsorption modulator and photodiode in the same device for duplex operation, can&lt;br/&gt;occur, using bias control as a transmit/receive mode control. For full duplex operation, two&lt;br/&gt;modulator/photodiode devices need to be incorporated in the each transceiver element.&lt;br/&gt; We propose to directly drive a coplanar waveguide (CPW)-fed slot antenna by converting optical&lt;br/&gt;power into microwave power and vice versa using these RF modulator/photodetectors. As a transmitter,&lt;br/&gt;the CPW line is connected to the active surface of the photodetector, from which the microwave power&lt;br/&gt;propagates to feed the radiating slot. The photodetector is fed via an optical fiber from beneath. When the&lt;br/&gt;device functions as an optical modulator, the receive function can also be achieved. Preliminary results&lt;br/&gt;for a single antenna show that a very good bandwidth and radiation patterns can be achieved.&lt;br/&gt; It should be noted that these elements can be interconnected via the fiber to achieve summation,&lt;br/&gt;mixing and other signal processing functions, at the antenna site or at a remote site. Some preliminary&lt;br/&gt;results have been achieved in the area of multiple functionality for the optoelectronic components, such as&lt;br/&gt;modulation, photodetection, self-biasing and RF frequency mixing. They have shown properties, such as&lt;br/&gt;high bandwidth and high power, that are desirable for the antenna applications. A main emphasis here is&lt;br/&gt;to further investigate the material and device designs for the optoelectronic component that can&lt;br/&gt;incorporate into the smart antenna architecture.&lt;br/&gt; The proposed approach will have significant impacts on wireless communication systems by&lt;br/&gt;providing higher system bandwidth capacity and enhancing their reliability. It may lead to a new type of&lt;br/&gt;long distance, broadband network infrastructure that supports transparent transport of optical signals.&lt;br/&gt; Our team is formed to provide the expertise in the four key elements for this proposed research.&lt;br/&gt;Our project will provide a good opportunity to train graduate and undergraduate students in one of the&lt;br/&gt;most exciting interdisciplinary areas in science (RF, photonics, signal processing and communications).&lt;br/&gt;The interactions between the researchers at the different institutions will be aided by the close&lt;br/&gt;collaboration that exists between the members of the group.</data>
      <data key="e_pgm">4090</data>
      <data key="e_label">123421</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">123421</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n794" target="n837">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Computational Tools for Modeling, Visualizing and Analyzing Historic and Archaeological Sites</data>
      <data key="e_abstract">e are proposing to develop computational tools for researchers&lt;br/&gt;and students to model, visualize, and analyze historic and ancient&lt;br/&gt;sites. This proposal addresses four major scientific components&lt;br/&gt;to support this research. First, we are proposing new methods&lt;br/&gt;of creating complex, 3-D, photorealistic models of large sites.&lt;br/&gt;This includes a mobile robot sensing system that can be used as&lt;br/&gt;an intelligent sensing device over a large scale. Second, we are&lt;br/&gt;proposing to develop new methods to image below-ground data accurately&lt;br/&gt;and efficiently. These methods are especially suited to modeling&lt;br/&gt;the wealth of subsurface information at archaeological sites.&lt;br/&gt;Third, we will be developing new database technology to catalog&lt;br/&gt;and access a site&apos;s structures, artifacts, objects, and historical&lt;br/&gt;references. This will significantly improve a user&apos;s ability to&lt;br/&gt;query and analyze a site&apos;s information. Fourth, we have created&lt;br/&gt;a wearable augmented reality system for presenting georegistered&lt;br/&gt;information to mobile users, using overlaid graphics and sound.&lt;br/&gt;We will extend this system to create a new class of information&lt;br/&gt;visualization systems that integrate 3-D above- and below-ground&lt;br/&gt;models, 2-D images, text and other web-based resources to annotate&lt;br/&gt;the physical environment We will apply this system to support&lt;br/&gt;scientists in the field, as well to allow on-site and remote tours&lt;br/&gt;of historic and ancient sites. The research will utilize a local&lt;br/&gt;testbed, the Cathedral of St. John the Divine in New York City,&lt;br/&gt;and a unique and important archaeological excavation at the Dakhleh&lt;br/&gt;Oasis in Egypt. The project will attract students and the public&lt;br/&gt;to the study of world heritage; provide an exceptional opportunity&lt;br/&gt;for active learning; and develop our ability to explore, analyze,&lt;br/&gt;critically evaluate and interpret material culture within historical&lt;br/&gt;contexts. The challenge is to bring the on-site experiences that&lt;br/&gt;develop these skills to the classroom and the public in general.&lt;br/&gt;Through critical inquiry and a variety of techniques, teachers&lt;br/&gt;and students can reconstruct examples of material culture to develop&lt;br/&gt;a complex understanding of the past. In this way, we can resist&lt;br/&gt;the temptation to replicate with new technologies what we have&lt;br/&gt;done successfully with other means and instead expand the possibilities&lt;br/&gt;for learning. Through learning in context, this project will bring&lt;br/&gt;together the primary sources of various fields and draw the social&lt;br/&gt;sciences out of the classroom into the historical milieu. This&lt;br/&gt;project will hopefully redefine the relationships among technology,&lt;br/&gt;faculty research and curriculum content. Most important, it will&lt;br/&gt;disseminate this information to as wide an audience as possible.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121239</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121239</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n794" target="n795">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Computational Tools for Modeling, Visualizing and Analyzing Historic and Archaeological Sites</data>
      <data key="e_abstract">e are proposing to develop computational tools for researchers&lt;br/&gt;and students to model, visualize, and analyze historic and ancient&lt;br/&gt;sites. This proposal addresses four major scientific components&lt;br/&gt;to support this research. First, we are proposing new methods&lt;br/&gt;of creating complex, 3-D, photorealistic models of large sites.&lt;br/&gt;This includes a mobile robot sensing system that can be used as&lt;br/&gt;an intelligent sensing device over a large scale. Second, we are&lt;br/&gt;proposing to develop new methods to image below-ground data accurately&lt;br/&gt;and efficiently. These methods are especially suited to modeling&lt;br/&gt;the wealth of subsurface information at archaeological sites.&lt;br/&gt;Third, we will be developing new database technology to catalog&lt;br/&gt;and access a site&apos;s structures, artifacts, objects, and historical&lt;br/&gt;references. This will significantly improve a user&apos;s ability to&lt;br/&gt;query and analyze a site&apos;s information. Fourth, we have created&lt;br/&gt;a wearable augmented reality system for presenting georegistered&lt;br/&gt;information to mobile users, using overlaid graphics and sound.&lt;br/&gt;We will extend this system to create a new class of information&lt;br/&gt;visualization systems that integrate 3-D above- and below-ground&lt;br/&gt;models, 2-D images, text and other web-based resources to annotate&lt;br/&gt;the physical environment We will apply this system to support&lt;br/&gt;scientists in the field, as well to allow on-site and remote tours&lt;br/&gt;of historic and ancient sites. The research will utilize a local&lt;br/&gt;testbed, the Cathedral of St. John the Divine in New York City,&lt;br/&gt;and a unique and important archaeological excavation at the Dakhleh&lt;br/&gt;Oasis in Egypt. The project will attract students and the public&lt;br/&gt;to the study of world heritage; provide an exceptional opportunity&lt;br/&gt;for active learning; and develop our ability to explore, analyze,&lt;br/&gt;critically evaluate and interpret material culture within historical&lt;br/&gt;contexts. The challenge is to bring the on-site experiences that&lt;br/&gt;develop these skills to the classroom and the public in general.&lt;br/&gt;Through critical inquiry and a variety of techniques, teachers&lt;br/&gt;and students can reconstruct examples of material culture to develop&lt;br/&gt;a complex understanding of the past. In this way, we can resist&lt;br/&gt;the temptation to replicate with new technologies what we have&lt;br/&gt;done successfully with other means and instead expand the possibilities&lt;br/&gt;for learning. Through learning in context, this project will bring&lt;br/&gt;together the primary sources of various fields and draw the social&lt;br/&gt;sciences out of the classroom into the historical milieu. This&lt;br/&gt;project will hopefully redefine the relationships among technology,&lt;br/&gt;faculty research and curriculum content. Most important, it will&lt;br/&gt;disseminate this information to as wide an audience as possible.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121239</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121239</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n794" target="n839">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Computational Tools for Modeling, Visualizing and Analyzing Historic and Archaeological Sites</data>
      <data key="e_abstract">e are proposing to develop computational tools for researchers&lt;br/&gt;and students to model, visualize, and analyze historic and ancient&lt;br/&gt;sites. This proposal addresses four major scientific components&lt;br/&gt;to support this research. First, we are proposing new methods&lt;br/&gt;of creating complex, 3-D, photorealistic models of large sites.&lt;br/&gt;This includes a mobile robot sensing system that can be used as&lt;br/&gt;an intelligent sensing device over a large scale. Second, we are&lt;br/&gt;proposing to develop new methods to image below-ground data accurately&lt;br/&gt;and efficiently. These methods are especially suited to modeling&lt;br/&gt;the wealth of subsurface information at archaeological sites.&lt;br/&gt;Third, we will be developing new database technology to catalog&lt;br/&gt;and access a site&apos;s structures, artifacts, objects, and historical&lt;br/&gt;references. This will significantly improve a user&apos;s ability to&lt;br/&gt;query and analyze a site&apos;s information. Fourth, we have created&lt;br/&gt;a wearable augmented reality system for presenting georegistered&lt;br/&gt;information to mobile users, using overlaid graphics and sound.&lt;br/&gt;We will extend this system to create a new class of information&lt;br/&gt;visualization systems that integrate 3-D above- and below-ground&lt;br/&gt;models, 2-D images, text and other web-based resources to annotate&lt;br/&gt;the physical environment We will apply this system to support&lt;br/&gt;scientists in the field, as well to allow on-site and remote tours&lt;br/&gt;of historic and ancient sites. The research will utilize a local&lt;br/&gt;testbed, the Cathedral of St. John the Divine in New York City,&lt;br/&gt;and a unique and important archaeological excavation at the Dakhleh&lt;br/&gt;Oasis in Egypt. The project will attract students and the public&lt;br/&gt;to the study of world heritage; provide an exceptional opportunity&lt;br/&gt;for active learning; and develop our ability to explore, analyze,&lt;br/&gt;critically evaluate and interpret material culture within historical&lt;br/&gt;contexts. The challenge is to bring the on-site experiences that&lt;br/&gt;develop these skills to the classroom and the public in general.&lt;br/&gt;Through critical inquiry and a variety of techniques, teachers&lt;br/&gt;and students can reconstruct examples of material culture to develop&lt;br/&gt;a complex understanding of the past. In this way, we can resist&lt;br/&gt;the temptation to replicate with new technologies what we have&lt;br/&gt;done successfully with other means and instead expand the possibilities&lt;br/&gt;for learning. Through learning in context, this project will bring&lt;br/&gt;together the primary sources of various fields and draw the social&lt;br/&gt;sciences out of the classroom into the historical milieu. This&lt;br/&gt;project will hopefully redefine the relationships among technology,&lt;br/&gt;faculty research and curriculum content. Most important, it will&lt;br/&gt;disseminate this information to as wide an audience as possible.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121239</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121239</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n794" target="n840">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Computational Tools for Modeling, Visualizing and Analyzing Historic and Archaeological Sites</data>
      <data key="e_abstract">e are proposing to develop computational tools for researchers&lt;br/&gt;and students to model, visualize, and analyze historic and ancient&lt;br/&gt;sites. This proposal addresses four major scientific components&lt;br/&gt;to support this research. First, we are proposing new methods&lt;br/&gt;of creating complex, 3-D, photorealistic models of large sites.&lt;br/&gt;This includes a mobile robot sensing system that can be used as&lt;br/&gt;an intelligent sensing device over a large scale. Second, we are&lt;br/&gt;proposing to develop new methods to image below-ground data accurately&lt;br/&gt;and efficiently. These methods are especially suited to modeling&lt;br/&gt;the wealth of subsurface information at archaeological sites.&lt;br/&gt;Third, we will be developing new database technology to catalog&lt;br/&gt;and access a site&apos;s structures, artifacts, objects, and historical&lt;br/&gt;references. This will significantly improve a user&apos;s ability to&lt;br/&gt;query and analyze a site&apos;s information. Fourth, we have created&lt;br/&gt;a wearable augmented reality system for presenting georegistered&lt;br/&gt;information to mobile users, using overlaid graphics and sound.&lt;br/&gt;We will extend this system to create a new class of information&lt;br/&gt;visualization systems that integrate 3-D above- and below-ground&lt;br/&gt;models, 2-D images, text and other web-based resources to annotate&lt;br/&gt;the physical environment We will apply this system to support&lt;br/&gt;scientists in the field, as well to allow on-site and remote tours&lt;br/&gt;of historic and ancient sites. The research will utilize a local&lt;br/&gt;testbed, the Cathedral of St. John the Divine in New York City,&lt;br/&gt;and a unique and important archaeological excavation at the Dakhleh&lt;br/&gt;Oasis in Egypt. The project will attract students and the public&lt;br/&gt;to the study of world heritage; provide an exceptional opportunity&lt;br/&gt;for active learning; and develop our ability to explore, analyze,&lt;br/&gt;critically evaluate and interpret material culture within historical&lt;br/&gt;contexts. The challenge is to bring the on-site experiences that&lt;br/&gt;develop these skills to the classroom and the public in general.&lt;br/&gt;Through critical inquiry and a variety of techniques, teachers&lt;br/&gt;and students can reconstruct examples of material culture to develop&lt;br/&gt;a complex understanding of the past. In this way, we can resist&lt;br/&gt;the temptation to replicate with new technologies what we have&lt;br/&gt;done successfully with other means and instead expand the possibilities&lt;br/&gt;for learning. Through learning in context, this project will bring&lt;br/&gt;together the primary sources of various fields and draw the social&lt;br/&gt;sciences out of the classroom into the historical milieu. This&lt;br/&gt;project will hopefully redefine the relationships among technology,&lt;br/&gt;faculty research and curriculum content. Most important, it will&lt;br/&gt;disseminate this information to as wide an audience as possible.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121239</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121239</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n795" target="n837">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Computational Tools for Modeling, Visualizing and Analyzing Historic and Archaeological Sites</data>
      <data key="e_abstract">e are proposing to develop computational tools for researchers&lt;br/&gt;and students to model, visualize, and analyze historic and ancient&lt;br/&gt;sites. This proposal addresses four major scientific components&lt;br/&gt;to support this research. First, we are proposing new methods&lt;br/&gt;of creating complex, 3-D, photorealistic models of large sites.&lt;br/&gt;This includes a mobile robot sensing system that can be used as&lt;br/&gt;an intelligent sensing device over a large scale. Second, we are&lt;br/&gt;proposing to develop new methods to image below-ground data accurately&lt;br/&gt;and efficiently. These methods are especially suited to modeling&lt;br/&gt;the wealth of subsurface information at archaeological sites.&lt;br/&gt;Third, we will be developing new database technology to catalog&lt;br/&gt;and access a site&apos;s structures, artifacts, objects, and historical&lt;br/&gt;references. This will significantly improve a user&apos;s ability to&lt;br/&gt;query and analyze a site&apos;s information. Fourth, we have created&lt;br/&gt;a wearable augmented reality system for presenting georegistered&lt;br/&gt;information to mobile users, using overlaid graphics and sound.&lt;br/&gt;We will extend this system to create a new class of information&lt;br/&gt;visualization systems that integrate 3-D above- and below-ground&lt;br/&gt;models, 2-D images, text and other web-based resources to annotate&lt;br/&gt;the physical environment We will apply this system to support&lt;br/&gt;scientists in the field, as well to allow on-site and remote tours&lt;br/&gt;of historic and ancient sites. The research will utilize a local&lt;br/&gt;testbed, the Cathedral of St. John the Divine in New York City,&lt;br/&gt;and a unique and important archaeological excavation at the Dakhleh&lt;br/&gt;Oasis in Egypt. The project will attract students and the public&lt;br/&gt;to the study of world heritage; provide an exceptional opportunity&lt;br/&gt;for active learning; and develop our ability to explore, analyze,&lt;br/&gt;critically evaluate and interpret material culture within historical&lt;br/&gt;contexts. The challenge is to bring the on-site experiences that&lt;br/&gt;develop these skills to the classroom and the public in general.&lt;br/&gt;Through critical inquiry and a variety of techniques, teachers&lt;br/&gt;and students can reconstruct examples of material culture to develop&lt;br/&gt;a complex understanding of the past. In this way, we can resist&lt;br/&gt;the temptation to replicate with new technologies what we have&lt;br/&gt;done successfully with other means and instead expand the possibilities&lt;br/&gt;for learning. Through learning in context, this project will bring&lt;br/&gt;together the primary sources of various fields and draw the social&lt;br/&gt;sciences out of the classroom into the historical milieu. This&lt;br/&gt;project will hopefully redefine the relationships among technology,&lt;br/&gt;faculty research and curriculum content. Most important, it will&lt;br/&gt;disseminate this information to as wide an audience as possible.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121239</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121239</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n837" target="n839">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Computational Tools for Modeling, Visualizing and Analyzing Historic and Archaeological Sites</data>
      <data key="e_abstract">e are proposing to develop computational tools for researchers&lt;br/&gt;and students to model, visualize, and analyze historic and ancient&lt;br/&gt;sites. This proposal addresses four major scientific components&lt;br/&gt;to support this research. First, we are proposing new methods&lt;br/&gt;of creating complex, 3-D, photorealistic models of large sites.&lt;br/&gt;This includes a mobile robot sensing system that can be used as&lt;br/&gt;an intelligent sensing device over a large scale. Second, we are&lt;br/&gt;proposing to develop new methods to image below-ground data accurately&lt;br/&gt;and efficiently. These methods are especially suited to modeling&lt;br/&gt;the wealth of subsurface information at archaeological sites.&lt;br/&gt;Third, we will be developing new database technology to catalog&lt;br/&gt;and access a site&apos;s structures, artifacts, objects, and historical&lt;br/&gt;references. This will significantly improve a user&apos;s ability to&lt;br/&gt;query and analyze a site&apos;s information. Fourth, we have created&lt;br/&gt;a wearable augmented reality system for presenting georegistered&lt;br/&gt;information to mobile users, using overlaid graphics and sound.&lt;br/&gt;We will extend this system to create a new class of information&lt;br/&gt;visualization systems that integrate 3-D above- and below-ground&lt;br/&gt;models, 2-D images, text and other web-based resources to annotate&lt;br/&gt;the physical environment We will apply this system to support&lt;br/&gt;scientists in the field, as well to allow on-site and remote tours&lt;br/&gt;of historic and ancient sites. The research will utilize a local&lt;br/&gt;testbed, the Cathedral of St. John the Divine in New York City,&lt;br/&gt;and a unique and important archaeological excavation at the Dakhleh&lt;br/&gt;Oasis in Egypt. The project will attract students and the public&lt;br/&gt;to the study of world heritage; provide an exceptional opportunity&lt;br/&gt;for active learning; and develop our ability to explore, analyze,&lt;br/&gt;critically evaluate and interpret material culture within historical&lt;br/&gt;contexts. The challenge is to bring the on-site experiences that&lt;br/&gt;develop these skills to the classroom and the public in general.&lt;br/&gt;Through critical inquiry and a variety of techniques, teachers&lt;br/&gt;and students can reconstruct examples of material culture to develop&lt;br/&gt;a complex understanding of the past. In this way, we can resist&lt;br/&gt;the temptation to replicate with new technologies what we have&lt;br/&gt;done successfully with other means and instead expand the possibilities&lt;br/&gt;for learning. Through learning in context, this project will bring&lt;br/&gt;together the primary sources of various fields and draw the social&lt;br/&gt;sciences out of the classroom into the historical milieu. This&lt;br/&gt;project will hopefully redefine the relationships among technology,&lt;br/&gt;faculty research and curriculum content. Most important, it will&lt;br/&gt;disseminate this information to as wide an audience as possible.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121239</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121239</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n837" target="n840">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Computational Tools for Modeling, Visualizing and Analyzing Historic and Archaeological Sites</data>
      <data key="e_abstract">e are proposing to develop computational tools for researchers&lt;br/&gt;and students to model, visualize, and analyze historic and ancient&lt;br/&gt;sites. This proposal addresses four major scientific components&lt;br/&gt;to support this research. First, we are proposing new methods&lt;br/&gt;of creating complex, 3-D, photorealistic models of large sites.&lt;br/&gt;This includes a mobile robot sensing system that can be used as&lt;br/&gt;an intelligent sensing device over a large scale. Second, we are&lt;br/&gt;proposing to develop new methods to image below-ground data accurately&lt;br/&gt;and efficiently. These methods are especially suited to modeling&lt;br/&gt;the wealth of subsurface information at archaeological sites.&lt;br/&gt;Third, we will be developing new database technology to catalog&lt;br/&gt;and access a site&apos;s structures, artifacts, objects, and historical&lt;br/&gt;references. This will significantly improve a user&apos;s ability to&lt;br/&gt;query and analyze a site&apos;s information. Fourth, we have created&lt;br/&gt;a wearable augmented reality system for presenting georegistered&lt;br/&gt;information to mobile users, using overlaid graphics and sound.&lt;br/&gt;We will extend this system to create a new class of information&lt;br/&gt;visualization systems that integrate 3-D above- and below-ground&lt;br/&gt;models, 2-D images, text and other web-based resources to annotate&lt;br/&gt;the physical environment We will apply this system to support&lt;br/&gt;scientists in the field, as well to allow on-site and remote tours&lt;br/&gt;of historic and ancient sites. The research will utilize a local&lt;br/&gt;testbed, the Cathedral of St. John the Divine in New York City,&lt;br/&gt;and a unique and important archaeological excavation at the Dakhleh&lt;br/&gt;Oasis in Egypt. The project will attract students and the public&lt;br/&gt;to the study of world heritage; provide an exceptional opportunity&lt;br/&gt;for active learning; and develop our ability to explore, analyze,&lt;br/&gt;critically evaluate and interpret material culture within historical&lt;br/&gt;contexts. The challenge is to bring the on-site experiences that&lt;br/&gt;develop these skills to the classroom and the public in general.&lt;br/&gt;Through critical inquiry and a variety of techniques, teachers&lt;br/&gt;and students can reconstruct examples of material culture to develop&lt;br/&gt;a complex understanding of the past. In this way, we can resist&lt;br/&gt;the temptation to replicate with new technologies what we have&lt;br/&gt;done successfully with other means and instead expand the possibilities&lt;br/&gt;for learning. Through learning in context, this project will bring&lt;br/&gt;together the primary sources of various fields and draw the social&lt;br/&gt;sciences out of the classroom into the historical milieu. This&lt;br/&gt;project will hopefully redefine the relationships among technology,&lt;br/&gt;faculty research and curriculum content. Most important, it will&lt;br/&gt;disseminate this information to as wide an audience as possible.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121239</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121239</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n795" target="n839">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Computational Tools for Modeling, Visualizing and Analyzing Historic and Archaeological Sites</data>
      <data key="e_abstract">e are proposing to develop computational tools for researchers&lt;br/&gt;and students to model, visualize, and analyze historic and ancient&lt;br/&gt;sites. This proposal addresses four major scientific components&lt;br/&gt;to support this research. First, we are proposing new methods&lt;br/&gt;of creating complex, 3-D, photorealistic models of large sites.&lt;br/&gt;This includes a mobile robot sensing system that can be used as&lt;br/&gt;an intelligent sensing device over a large scale. Second, we are&lt;br/&gt;proposing to develop new methods to image below-ground data accurately&lt;br/&gt;and efficiently. These methods are especially suited to modeling&lt;br/&gt;the wealth of subsurface information at archaeological sites.&lt;br/&gt;Third, we will be developing new database technology to catalog&lt;br/&gt;and access a site&apos;s structures, artifacts, objects, and historical&lt;br/&gt;references. This will significantly improve a user&apos;s ability to&lt;br/&gt;query and analyze a site&apos;s information. Fourth, we have created&lt;br/&gt;a wearable augmented reality system for presenting georegistered&lt;br/&gt;information to mobile users, using overlaid graphics and sound.&lt;br/&gt;We will extend this system to create a new class of information&lt;br/&gt;visualization systems that integrate 3-D above- and below-ground&lt;br/&gt;models, 2-D images, text and other web-based resources to annotate&lt;br/&gt;the physical environment We will apply this system to support&lt;br/&gt;scientists in the field, as well to allow on-site and remote tours&lt;br/&gt;of historic and ancient sites. The research will utilize a local&lt;br/&gt;testbed, the Cathedral of St. John the Divine in New York City,&lt;br/&gt;and a unique and important archaeological excavation at the Dakhleh&lt;br/&gt;Oasis in Egypt. The project will attract students and the public&lt;br/&gt;to the study of world heritage; provide an exceptional opportunity&lt;br/&gt;for active learning; and develop our ability to explore, analyze,&lt;br/&gt;critically evaluate and interpret material culture within historical&lt;br/&gt;contexts. The challenge is to bring the on-site experiences that&lt;br/&gt;develop these skills to the classroom and the public in general.&lt;br/&gt;Through critical inquiry and a variety of techniques, teachers&lt;br/&gt;and students can reconstruct examples of material culture to develop&lt;br/&gt;a complex understanding of the past. In this way, we can resist&lt;br/&gt;the temptation to replicate with new technologies what we have&lt;br/&gt;done successfully with other means and instead expand the possibilities&lt;br/&gt;for learning. Through learning in context, this project will bring&lt;br/&gt;together the primary sources of various fields and draw the social&lt;br/&gt;sciences out of the classroom into the historical milieu. This&lt;br/&gt;project will hopefully redefine the relationships among technology,&lt;br/&gt;faculty research and curriculum content. Most important, it will&lt;br/&gt;disseminate this information to as wide an audience as possible.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121239</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121239</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n795" target="n840">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Computational Tools for Modeling, Visualizing and Analyzing Historic and Archaeological Sites</data>
      <data key="e_abstract">e are proposing to develop computational tools for researchers&lt;br/&gt;and students to model, visualize, and analyze historic and ancient&lt;br/&gt;sites. This proposal addresses four major scientific components&lt;br/&gt;to support this research. First, we are proposing new methods&lt;br/&gt;of creating complex, 3-D, photorealistic models of large sites.&lt;br/&gt;This includes a mobile robot sensing system that can be used as&lt;br/&gt;an intelligent sensing device over a large scale. Second, we are&lt;br/&gt;proposing to develop new methods to image below-ground data accurately&lt;br/&gt;and efficiently. These methods are especially suited to modeling&lt;br/&gt;the wealth of subsurface information at archaeological sites.&lt;br/&gt;Third, we will be developing new database technology to catalog&lt;br/&gt;and access a site&apos;s structures, artifacts, objects, and historical&lt;br/&gt;references. This will significantly improve a user&apos;s ability to&lt;br/&gt;query and analyze a site&apos;s information. Fourth, we have created&lt;br/&gt;a wearable augmented reality system for presenting georegistered&lt;br/&gt;information to mobile users, using overlaid graphics and sound.&lt;br/&gt;We will extend this system to create a new class of information&lt;br/&gt;visualization systems that integrate 3-D above- and below-ground&lt;br/&gt;models, 2-D images, text and other web-based resources to annotate&lt;br/&gt;the physical environment We will apply this system to support&lt;br/&gt;scientists in the field, as well to allow on-site and remote tours&lt;br/&gt;of historic and ancient sites. The research will utilize a local&lt;br/&gt;testbed, the Cathedral of St. John the Divine in New York City,&lt;br/&gt;and a unique and important archaeological excavation at the Dakhleh&lt;br/&gt;Oasis in Egypt. The project will attract students and the public&lt;br/&gt;to the study of world heritage; provide an exceptional opportunity&lt;br/&gt;for active learning; and develop our ability to explore, analyze,&lt;br/&gt;critically evaluate and interpret material culture within historical&lt;br/&gt;contexts. The challenge is to bring the on-site experiences that&lt;br/&gt;develop these skills to the classroom and the public in general.&lt;br/&gt;Through critical inquiry and a variety of techniques, teachers&lt;br/&gt;and students can reconstruct examples of material culture to develop&lt;br/&gt;a complex understanding of the past. In this way, we can resist&lt;br/&gt;the temptation to replicate with new technologies what we have&lt;br/&gt;done successfully with other means and instead expand the possibilities&lt;br/&gt;for learning. Through learning in context, this project will bring&lt;br/&gt;together the primary sources of various fields and draw the social&lt;br/&gt;sciences out of the classroom into the historical milieu. This&lt;br/&gt;project will hopefully redefine the relationships among technology,&lt;br/&gt;faculty research and curriculum content. Most important, it will&lt;br/&gt;disseminate this information to as wide an audience as possible.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121239</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121239</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n839" target="n840">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Computational Tools for Modeling, Visualizing and Analyzing Historic and Archaeological Sites</data>
      <data key="e_abstract">e are proposing to develop computational tools for researchers&lt;br/&gt;and students to model, visualize, and analyze historic and ancient&lt;br/&gt;sites. This proposal addresses four major scientific components&lt;br/&gt;to support this research. First, we are proposing new methods&lt;br/&gt;of creating complex, 3-D, photorealistic models of large sites.&lt;br/&gt;This includes a mobile robot sensing system that can be used as&lt;br/&gt;an intelligent sensing device over a large scale. Second, we are&lt;br/&gt;proposing to develop new methods to image below-ground data accurately&lt;br/&gt;and efficiently. These methods are especially suited to modeling&lt;br/&gt;the wealth of subsurface information at archaeological sites.&lt;br/&gt;Third, we will be developing new database technology to catalog&lt;br/&gt;and access a site&apos;s structures, artifacts, objects, and historical&lt;br/&gt;references. This will significantly improve a user&apos;s ability to&lt;br/&gt;query and analyze a site&apos;s information. Fourth, we have created&lt;br/&gt;a wearable augmented reality system for presenting georegistered&lt;br/&gt;information to mobile users, using overlaid graphics and sound.&lt;br/&gt;We will extend this system to create a new class of information&lt;br/&gt;visualization systems that integrate 3-D above- and below-ground&lt;br/&gt;models, 2-D images, text and other web-based resources to annotate&lt;br/&gt;the physical environment We will apply this system to support&lt;br/&gt;scientists in the field, as well to allow on-site and remote tours&lt;br/&gt;of historic and ancient sites. The research will utilize a local&lt;br/&gt;testbed, the Cathedral of St. John the Divine in New York City,&lt;br/&gt;and a unique and important archaeological excavation at the Dakhleh&lt;br/&gt;Oasis in Egypt. The project will attract students and the public&lt;br/&gt;to the study of world heritage; provide an exceptional opportunity&lt;br/&gt;for active learning; and develop our ability to explore, analyze,&lt;br/&gt;critically evaluate and interpret material culture within historical&lt;br/&gt;contexts. The challenge is to bring the on-site experiences that&lt;br/&gt;develop these skills to the classroom and the public in general.&lt;br/&gt;Through critical inquiry and a variety of techniques, teachers&lt;br/&gt;and students can reconstruct examples of material culture to develop&lt;br/&gt;a complex understanding of the past. In this way, we can resist&lt;br/&gt;the temptation to replicate with new technologies what we have&lt;br/&gt;done successfully with other means and instead expand the possibilities&lt;br/&gt;for learning. Through learning in context, this project will bring&lt;br/&gt;together the primary sources of various fields and draw the social&lt;br/&gt;sciences out of the classroom into the historical milieu. This&lt;br/&gt;project will hopefully redefine the relationships among technology,&lt;br/&gt;faculty research and curriculum content. Most important, it will&lt;br/&gt;disseminate this information to as wide an audience as possible.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121239</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121239</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n841" target="n842">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Poseidon - Rapid Real-Time Interdisciplinary Ocean Forecasting: Adaptive Sampling and Adaptive Modeling in a Distributed Environment</data>
      <data key="e_abstract">EIA-0121263&lt;br/&gt;Patrikalakis, Nicholas M&lt;br/&gt;MIT&lt;br/&gt;&lt;br/&gt;ITR/AP+IM: Poseidon- Rapid Real-Time Interdisciplinary Ocean Forecasting: Adaptive Sampling and Adaptive Modeling in a Distributed Environment&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Progress in understanding the complex coupled physics, biology and acoustics of the oceans is accelerating via research on realistic nonlinear multiscale interdisciplinary processes, interactions and variabilities.&lt;br/&gt;&lt;br/&gt;To cope with the variabilities of such economies in space and time, dynamical model structures must evolve during the prediction, i.e., by adaptive modeling. The objective of this project is to enable by an effective union of information technologies and ocean sciences, efficient mulitscale interdisciplinary ocean prediction with real-time objective adaptive sampling, assimilation of multiple streams of interdisciplinary data, and autonomous adaptive modeling</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121263</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121263</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n841" target="n843">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Poseidon - Rapid Real-Time Interdisciplinary Ocean Forecasting: Adaptive Sampling and Adaptive Modeling in a Distributed Environment</data>
      <data key="e_abstract">EIA-0121263&lt;br/&gt;Patrikalakis, Nicholas M&lt;br/&gt;MIT&lt;br/&gt;&lt;br/&gt;ITR/AP+IM: Poseidon- Rapid Real-Time Interdisciplinary Ocean Forecasting: Adaptive Sampling and Adaptive Modeling in a Distributed Environment&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Progress in understanding the complex coupled physics, biology and acoustics of the oceans is accelerating via research on realistic nonlinear multiscale interdisciplinary processes, interactions and variabilities.&lt;br/&gt;&lt;br/&gt;To cope with the variabilities of such economies in space and time, dynamical model structures must evolve during the prediction, i.e., by adaptive modeling. The objective of this project is to enable by an effective union of information technologies and ocean sciences, efficient mulitscale interdisciplinary ocean prediction with real-time objective adaptive sampling, assimilation of multiple streams of interdisciplinary data, and autonomous adaptive modeling</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121263</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121263</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n841" target="n844">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Poseidon - Rapid Real-Time Interdisciplinary Ocean Forecasting: Adaptive Sampling and Adaptive Modeling in a Distributed Environment</data>
      <data key="e_abstract">EIA-0121263&lt;br/&gt;Patrikalakis, Nicholas M&lt;br/&gt;MIT&lt;br/&gt;&lt;br/&gt;ITR/AP+IM: Poseidon- Rapid Real-Time Interdisciplinary Ocean Forecasting: Adaptive Sampling and Adaptive Modeling in a Distributed Environment&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Progress in understanding the complex coupled physics, biology and acoustics of the oceans is accelerating via research on realistic nonlinear multiscale interdisciplinary processes, interactions and variabilities.&lt;br/&gt;&lt;br/&gt;To cope with the variabilities of such economies in space and time, dynamical model structures must evolve during the prediction, i.e., by adaptive modeling. The objective of this project is to enable by an effective union of information technologies and ocean sciences, efficient mulitscale interdisciplinary ocean prediction with real-time objective adaptive sampling, assimilation of multiple streams of interdisciplinary data, and autonomous adaptive modeling</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121263</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121263</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n842" target="n843">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Poseidon - Rapid Real-Time Interdisciplinary Ocean Forecasting: Adaptive Sampling and Adaptive Modeling in a Distributed Environment</data>
      <data key="e_abstract">EIA-0121263&lt;br/&gt;Patrikalakis, Nicholas M&lt;br/&gt;MIT&lt;br/&gt;&lt;br/&gt;ITR/AP+IM: Poseidon- Rapid Real-Time Interdisciplinary Ocean Forecasting: Adaptive Sampling and Adaptive Modeling in a Distributed Environment&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Progress in understanding the complex coupled physics, biology and acoustics of the oceans is accelerating via research on realistic nonlinear multiscale interdisciplinary processes, interactions and variabilities.&lt;br/&gt;&lt;br/&gt;To cope with the variabilities of such economies in space and time, dynamical model structures must evolve during the prediction, i.e., by adaptive modeling. The objective of this project is to enable by an effective union of information technologies and ocean sciences, efficient mulitscale interdisciplinary ocean prediction with real-time objective adaptive sampling, assimilation of multiple streams of interdisciplinary data, and autonomous adaptive modeling</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121263</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121263</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n842" target="n844">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Poseidon - Rapid Real-Time Interdisciplinary Ocean Forecasting: Adaptive Sampling and Adaptive Modeling in a Distributed Environment</data>
      <data key="e_abstract">EIA-0121263&lt;br/&gt;Patrikalakis, Nicholas M&lt;br/&gt;MIT&lt;br/&gt;&lt;br/&gt;ITR/AP+IM: Poseidon- Rapid Real-Time Interdisciplinary Ocean Forecasting: Adaptive Sampling and Adaptive Modeling in a Distributed Environment&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Progress in understanding the complex coupled physics, biology and acoustics of the oceans is accelerating via research on realistic nonlinear multiscale interdisciplinary processes, interactions and variabilities.&lt;br/&gt;&lt;br/&gt;To cope with the variabilities of such economies in space and time, dynamical model structures must evolve during the prediction, i.e., by adaptive modeling. The objective of this project is to enable by an effective union of information technologies and ocean sciences, efficient mulitscale interdisciplinary ocean prediction with real-time objective adaptive sampling, assimilation of multiple streams of interdisciplinary data, and autonomous adaptive modeling</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121263</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121263</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n843" target="n844">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Poseidon - Rapid Real-Time Interdisciplinary Ocean Forecasting: Adaptive Sampling and Adaptive Modeling in a Distributed Environment</data>
      <data key="e_abstract">EIA-0121263&lt;br/&gt;Patrikalakis, Nicholas M&lt;br/&gt;MIT&lt;br/&gt;&lt;br/&gt;ITR/AP+IM: Poseidon- Rapid Real-Time Interdisciplinary Ocean Forecasting: Adaptive Sampling and Adaptive Modeling in a Distributed Environment&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Progress in understanding the complex coupled physics, biology and acoustics of the oceans is accelerating via research on realistic nonlinear multiscale interdisciplinary processes, interactions and variabilities.&lt;br/&gt;&lt;br/&gt;To cope with the variabilities of such economies in space and time, dynamical model structures must evolve during the prediction, i.e., by adaptive modeling. The objective of this project is to enable by an effective union of information technologies and ocean sciences, efficient mulitscale interdisciplinary ocean prediction with real-time objective adaptive sampling, assimilation of multiple streams of interdisciplinary data, and autonomous adaptive modeling</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121263</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121263</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n707" target="n845">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Information Processing for Integrated Observation and Simulation BAsed Risk Management of Geophysical Mass Flows</data>
      <data key="e_abstract">Real-time Environmental Observation and Forecasting Systems (EOFS) will revolutionize the way scientists share information about the environment and represent an opportunity to break traditional information barriers separating scientists from society at large. EOFS are already in use, but they tend to be small-scale, application- and domain- specific, stand-alone systems. There is a need for evolution towards multi-purpose shared systems designed to adapt flexibly to evolving needs of information consumers. What is required are large-scale, shared, heterogeneous distributed systems that make extensive use of diverse sensor-based inputs, sophisticated numerical simulations, mobile and embedded real-time system components, wireless and wired communications, high-performance computers, and high capacity storage systems. &lt;br/&gt;&lt;br/&gt;This ITR medium project has assembled an inter-disciplinary team, including computer science and environmental science researchers in addition to a heterogeneous base of pilot users. This group will collaborate to develop software technology which will enable EOFS to evolve efficiently, and to deliver quantifiably reliable information about the environment at the right time and in the right form to the right users. The project focus is on EOFS for estuarine and coastal regions. These regions are selected because they are highly variable natural systems subject to intense human activity and with great social, environmental, economic and cultural value.&lt;br/&gt;&lt;br/&gt;The research will include:&lt;br/&gt;&lt;br/&gt;i. Developing missing integration concepts and technologies for EOFS, with emphasis on quality-scalable information processing, storage and access (the computer science research).&lt;br/&gt;ii. Closing the loop between environmental models and sensors, and implementing a next generation EOFS based on an existing prototype for an estuary with multiple and often conflicting uses (the environmental observation and forecasting systems research);&lt;br/&gt;iii. Using, evaluating and refining the EOFS prototype for scientific discovery, natural resources stewardship and emergency response, thus incorporating sound science in operational and management decisions of critical regional importance and national significance (the environmental science and management applications);&lt;br/&gt;iv. Developing pilot multi-level, inter-disciplinary educational programs that cross-train young people, computer scientists, environmental scientists and practitioners in the conceptualization, development and use of environmental information technology (the education impact).</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121254</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121254</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n845" target="n847">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Information Processing for Integrated Observation and Simulation BAsed Risk Management of Geophysical Mass Flows</data>
      <data key="e_abstract">Real-time Environmental Observation and Forecasting Systems (EOFS) will revolutionize the way scientists share information about the environment and represent an opportunity to break traditional information barriers separating scientists from society at large. EOFS are already in use, but they tend to be small-scale, application- and domain- specific, stand-alone systems. There is a need for evolution towards multi-purpose shared systems designed to adapt flexibly to evolving needs of information consumers. What is required are large-scale, shared, heterogeneous distributed systems that make extensive use of diverse sensor-based inputs, sophisticated numerical simulations, mobile and embedded real-time system components, wireless and wired communications, high-performance computers, and high capacity storage systems. &lt;br/&gt;&lt;br/&gt;This ITR medium project has assembled an inter-disciplinary team, including computer science and environmental science researchers in addition to a heterogeneous base of pilot users. This group will collaborate to develop software technology which will enable EOFS to evolve efficiently, and to deliver quantifiably reliable information about the environment at the right time and in the right form to the right users. The project focus is on EOFS for estuarine and coastal regions. These regions are selected because they are highly variable natural systems subject to intense human activity and with great social, environmental, economic and cultural value.&lt;br/&gt;&lt;br/&gt;The research will include:&lt;br/&gt;&lt;br/&gt;i. Developing missing integration concepts and technologies for EOFS, with emphasis on quality-scalable information processing, storage and access (the computer science research).&lt;br/&gt;ii. Closing the loop between environmental models and sensors, and implementing a next generation EOFS based on an existing prototype for an estuary with multiple and often conflicting uses (the environmental observation and forecasting systems research);&lt;br/&gt;iii. Using, evaluating and refining the EOFS prototype for scientific discovery, natural resources stewardship and emergency response, thus incorporating sound science in operational and management decisions of critical regional importance and national significance (the environmental science and management applications);&lt;br/&gt;iv. Developing pilot multi-level, inter-disciplinary educational programs that cross-train young people, computer scientists, environmental scientists and practitioners in the conceptualization, development and use of environmental information technology (the education impact).</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121254</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121254</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n845" target="n848">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Information Processing for Integrated Observation and Simulation BAsed Risk Management of Geophysical Mass Flows</data>
      <data key="e_abstract">Real-time Environmental Observation and Forecasting Systems (EOFS) will revolutionize the way scientists share information about the environment and represent an opportunity to break traditional information barriers separating scientists from society at large. EOFS are already in use, but they tend to be small-scale, application- and domain- specific, stand-alone systems. There is a need for evolution towards multi-purpose shared systems designed to adapt flexibly to evolving needs of information consumers. What is required are large-scale, shared, heterogeneous distributed systems that make extensive use of diverse sensor-based inputs, sophisticated numerical simulations, mobile and embedded real-time system components, wireless and wired communications, high-performance computers, and high capacity storage systems. &lt;br/&gt;&lt;br/&gt;This ITR medium project has assembled an inter-disciplinary team, including computer science and environmental science researchers in addition to a heterogeneous base of pilot users. This group will collaborate to develop software technology which will enable EOFS to evolve efficiently, and to deliver quantifiably reliable information about the environment at the right time and in the right form to the right users. The project focus is on EOFS for estuarine and coastal regions. These regions are selected because they are highly variable natural systems subject to intense human activity and with great social, environmental, economic and cultural value.&lt;br/&gt;&lt;br/&gt;The research will include:&lt;br/&gt;&lt;br/&gt;i. Developing missing integration concepts and technologies for EOFS, with emphasis on quality-scalable information processing, storage and access (the computer science research).&lt;br/&gt;ii. Closing the loop between environmental models and sensors, and implementing a next generation EOFS based on an existing prototype for an estuary with multiple and often conflicting uses (the environmental observation and forecasting systems research);&lt;br/&gt;iii. Using, evaluating and refining the EOFS prototype for scientific discovery, natural resources stewardship and emergency response, thus incorporating sound science in operational and management decisions of critical regional importance and national significance (the environmental science and management applications);&lt;br/&gt;iv. Developing pilot multi-level, inter-disciplinary educational programs that cross-train young people, computer scientists, environmental scientists and practitioners in the conceptualization, development and use of environmental information technology (the education impact).</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121254</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121254</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n392" target="n845">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Information Processing for Integrated Observation and Simulation BAsed Risk Management of Geophysical Mass Flows</data>
      <data key="e_abstract">Real-time Environmental Observation and Forecasting Systems (EOFS) will revolutionize the way scientists share information about the environment and represent an opportunity to break traditional information barriers separating scientists from society at large. EOFS are already in use, but they tend to be small-scale, application- and domain- specific, stand-alone systems. There is a need for evolution towards multi-purpose shared systems designed to adapt flexibly to evolving needs of information consumers. What is required are large-scale, shared, heterogeneous distributed systems that make extensive use of diverse sensor-based inputs, sophisticated numerical simulations, mobile and embedded real-time system components, wireless and wired communications, high-performance computers, and high capacity storage systems. &lt;br/&gt;&lt;br/&gt;This ITR medium project has assembled an inter-disciplinary team, including computer science and environmental science researchers in addition to a heterogeneous base of pilot users. This group will collaborate to develop software technology which will enable EOFS to evolve efficiently, and to deliver quantifiably reliable information about the environment at the right time and in the right form to the right users. The project focus is on EOFS for estuarine and coastal regions. These regions are selected because they are highly variable natural systems subject to intense human activity and with great social, environmental, economic and cultural value.&lt;br/&gt;&lt;br/&gt;The research will include:&lt;br/&gt;&lt;br/&gt;i. Developing missing integration concepts and technologies for EOFS, with emphasis on quality-scalable information processing, storage and access (the computer science research).&lt;br/&gt;ii. Closing the loop between environmental models and sensors, and implementing a next generation EOFS based on an existing prototype for an estuary with multiple and often conflicting uses (the environmental observation and forecasting systems research);&lt;br/&gt;iii. Using, evaluating and refining the EOFS prototype for scientific discovery, natural resources stewardship and emergency response, thus incorporating sound science in operational and management decisions of critical regional importance and national significance (the environmental science and management applications);&lt;br/&gt;iv. Developing pilot multi-level, inter-disciplinary educational programs that cross-train young people, computer scientists, environmental scientists and practitioners in the conceptualization, development and use of environmental information technology (the education impact).</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121254</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121254</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n707" target="n847">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Information Processing for Integrated Observation and Simulation BAsed Risk Management of Geophysical Mass Flows</data>
      <data key="e_abstract">Real-time Environmental Observation and Forecasting Systems (EOFS) will revolutionize the way scientists share information about the environment and represent an opportunity to break traditional information barriers separating scientists from society at large. EOFS are already in use, but they tend to be small-scale, application- and domain- specific, stand-alone systems. There is a need for evolution towards multi-purpose shared systems designed to adapt flexibly to evolving needs of information consumers. What is required are large-scale, shared, heterogeneous distributed systems that make extensive use of diverse sensor-based inputs, sophisticated numerical simulations, mobile and embedded real-time system components, wireless and wired communications, high-performance computers, and high capacity storage systems. &lt;br/&gt;&lt;br/&gt;This ITR medium project has assembled an inter-disciplinary team, including computer science and environmental science researchers in addition to a heterogeneous base of pilot users. This group will collaborate to develop software technology which will enable EOFS to evolve efficiently, and to deliver quantifiably reliable information about the environment at the right time and in the right form to the right users. The project focus is on EOFS for estuarine and coastal regions. These regions are selected because they are highly variable natural systems subject to intense human activity and with great social, environmental, economic and cultural value.&lt;br/&gt;&lt;br/&gt;The research will include:&lt;br/&gt;&lt;br/&gt;i. Developing missing integration concepts and technologies for EOFS, with emphasis on quality-scalable information processing, storage and access (the computer science research).&lt;br/&gt;ii. Closing the loop between environmental models and sensors, and implementing a next generation EOFS based on an existing prototype for an estuary with multiple and often conflicting uses (the environmental observation and forecasting systems research);&lt;br/&gt;iii. Using, evaluating and refining the EOFS prototype for scientific discovery, natural resources stewardship and emergency response, thus incorporating sound science in operational and management decisions of critical regional importance and national significance (the environmental science and management applications);&lt;br/&gt;iv. Developing pilot multi-level, inter-disciplinary educational programs that cross-train young people, computer scientists, environmental scientists and practitioners in the conceptualization, development and use of environmental information technology (the education impact).</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121254</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121254</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n707" target="n848">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Information Processing for Integrated Observation and Simulation BAsed Risk Management of Geophysical Mass Flows</data>
      <data key="e_abstract">Real-time Environmental Observation and Forecasting Systems (EOFS) will revolutionize the way scientists share information about the environment and represent an opportunity to break traditional information barriers separating scientists from society at large. EOFS are already in use, but they tend to be small-scale, application- and domain- specific, stand-alone systems. There is a need for evolution towards multi-purpose shared systems designed to adapt flexibly to evolving needs of information consumers. What is required are large-scale, shared, heterogeneous distributed systems that make extensive use of diverse sensor-based inputs, sophisticated numerical simulations, mobile and embedded real-time system components, wireless and wired communications, high-performance computers, and high capacity storage systems. &lt;br/&gt;&lt;br/&gt;This ITR medium project has assembled an inter-disciplinary team, including computer science and environmental science researchers in addition to a heterogeneous base of pilot users. This group will collaborate to develop software technology which will enable EOFS to evolve efficiently, and to deliver quantifiably reliable information about the environment at the right time and in the right form to the right users. The project focus is on EOFS for estuarine and coastal regions. These regions are selected because they are highly variable natural systems subject to intense human activity and with great social, environmental, economic and cultural value.&lt;br/&gt;&lt;br/&gt;The research will include:&lt;br/&gt;&lt;br/&gt;i. Developing missing integration concepts and technologies for EOFS, with emphasis on quality-scalable information processing, storage and access (the computer science research).&lt;br/&gt;ii. Closing the loop between environmental models and sensors, and implementing a next generation EOFS based on an existing prototype for an estuary with multiple and often conflicting uses (the environmental observation and forecasting systems research);&lt;br/&gt;iii. Using, evaluating and refining the EOFS prototype for scientific discovery, natural resources stewardship and emergency response, thus incorporating sound science in operational and management decisions of critical regional importance and national significance (the environmental science and management applications);&lt;br/&gt;iv. Developing pilot multi-level, inter-disciplinary educational programs that cross-train young people, computer scientists, environmental scientists and practitioners in the conceptualization, development and use of environmental information technology (the education impact).</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121254</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121254</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n392" target="n707">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Information Processing for Integrated Observation and Simulation BAsed Risk Management of Geophysical Mass Flows</data>
      <data key="e_abstract">Real-time Environmental Observation and Forecasting Systems (EOFS) will revolutionize the way scientists share information about the environment and represent an opportunity to break traditional information barriers separating scientists from society at large. EOFS are already in use, but they tend to be small-scale, application- and domain- specific, stand-alone systems. There is a need for evolution towards multi-purpose shared systems designed to adapt flexibly to evolving needs of information consumers. What is required are large-scale, shared, heterogeneous distributed systems that make extensive use of diverse sensor-based inputs, sophisticated numerical simulations, mobile and embedded real-time system components, wireless and wired communications, high-performance computers, and high capacity storage systems. &lt;br/&gt;&lt;br/&gt;This ITR medium project has assembled an inter-disciplinary team, including computer science and environmental science researchers in addition to a heterogeneous base of pilot users. This group will collaborate to develop software technology which will enable EOFS to evolve efficiently, and to deliver quantifiably reliable information about the environment at the right time and in the right form to the right users. The project focus is on EOFS for estuarine and coastal regions. These regions are selected because they are highly variable natural systems subject to intense human activity and with great social, environmental, economic and cultural value.&lt;br/&gt;&lt;br/&gt;The research will include:&lt;br/&gt;&lt;br/&gt;i. Developing missing integration concepts and technologies for EOFS, with emphasis on quality-scalable information processing, storage and access (the computer science research).&lt;br/&gt;ii. Closing the loop between environmental models and sensors, and implementing a next generation EOFS based on an existing prototype for an estuary with multiple and often conflicting uses (the environmental observation and forecasting systems research);&lt;br/&gt;iii. Using, evaluating and refining the EOFS prototype for scientific discovery, natural resources stewardship and emergency response, thus incorporating sound science in operational and management decisions of critical regional importance and national significance (the environmental science and management applications);&lt;br/&gt;iv. Developing pilot multi-level, inter-disciplinary educational programs that cross-train young people, computer scientists, environmental scientists and practitioners in the conceptualization, development and use of environmental information technology (the education impact).</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121254</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121254</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n847" target="n848">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Information Processing for Integrated Observation and Simulation BAsed Risk Management of Geophysical Mass Flows</data>
      <data key="e_abstract">Real-time Environmental Observation and Forecasting Systems (EOFS) will revolutionize the way scientists share information about the environment and represent an opportunity to break traditional information barriers separating scientists from society at large. EOFS are already in use, but they tend to be small-scale, application- and domain- specific, stand-alone systems. There is a need for evolution towards multi-purpose shared systems designed to adapt flexibly to evolving needs of information consumers. What is required are large-scale, shared, heterogeneous distributed systems that make extensive use of diverse sensor-based inputs, sophisticated numerical simulations, mobile and embedded real-time system components, wireless and wired communications, high-performance computers, and high capacity storage systems. &lt;br/&gt;&lt;br/&gt;This ITR medium project has assembled an inter-disciplinary team, including computer science and environmental science researchers in addition to a heterogeneous base of pilot users. This group will collaborate to develop software technology which will enable EOFS to evolve efficiently, and to deliver quantifiably reliable information about the environment at the right time and in the right form to the right users. The project focus is on EOFS for estuarine and coastal regions. These regions are selected because they are highly variable natural systems subject to intense human activity and with great social, environmental, economic and cultural value.&lt;br/&gt;&lt;br/&gt;The research will include:&lt;br/&gt;&lt;br/&gt;i. Developing missing integration concepts and technologies for EOFS, with emphasis on quality-scalable information processing, storage and access (the computer science research).&lt;br/&gt;ii. Closing the loop between environmental models and sensors, and implementing a next generation EOFS based on an existing prototype for an estuary with multiple and often conflicting uses (the environmental observation and forecasting systems research);&lt;br/&gt;iii. Using, evaluating and refining the EOFS prototype for scientific discovery, natural resources stewardship and emergency response, thus incorporating sound science in operational and management decisions of critical regional importance and national significance (the environmental science and management applications);&lt;br/&gt;iv. Developing pilot multi-level, inter-disciplinary educational programs that cross-train young people, computer scientists, environmental scientists and practitioners in the conceptualization, development and use of environmental information technology (the education impact).</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121254</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121254</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n392" target="n847">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Information Processing for Integrated Observation and Simulation BAsed Risk Management of Geophysical Mass Flows</data>
      <data key="e_abstract">Real-time Environmental Observation and Forecasting Systems (EOFS) will revolutionize the way scientists share information about the environment and represent an opportunity to break traditional information barriers separating scientists from society at large. EOFS are already in use, but they tend to be small-scale, application- and domain- specific, stand-alone systems. There is a need for evolution towards multi-purpose shared systems designed to adapt flexibly to evolving needs of information consumers. What is required are large-scale, shared, heterogeneous distributed systems that make extensive use of diverse sensor-based inputs, sophisticated numerical simulations, mobile and embedded real-time system components, wireless and wired communications, high-performance computers, and high capacity storage systems. &lt;br/&gt;&lt;br/&gt;This ITR medium project has assembled an inter-disciplinary team, including computer science and environmental science researchers in addition to a heterogeneous base of pilot users. This group will collaborate to develop software technology which will enable EOFS to evolve efficiently, and to deliver quantifiably reliable information about the environment at the right time and in the right form to the right users. The project focus is on EOFS for estuarine and coastal regions. These regions are selected because they are highly variable natural systems subject to intense human activity and with great social, environmental, economic and cultural value.&lt;br/&gt;&lt;br/&gt;The research will include:&lt;br/&gt;&lt;br/&gt;i. Developing missing integration concepts and technologies for EOFS, with emphasis on quality-scalable information processing, storage and access (the computer science research).&lt;br/&gt;ii. Closing the loop between environmental models and sensors, and implementing a next generation EOFS based on an existing prototype for an estuary with multiple and often conflicting uses (the environmental observation and forecasting systems research);&lt;br/&gt;iii. Using, evaluating and refining the EOFS prototype for scientific discovery, natural resources stewardship and emergency response, thus incorporating sound science in operational and management decisions of critical regional importance and national significance (the environmental science and management applications);&lt;br/&gt;iv. Developing pilot multi-level, inter-disciplinary educational programs that cross-train young people, computer scientists, environmental scientists and practitioners in the conceptualization, development and use of environmental information technology (the education impact).</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121254</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121254</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n392" target="n848">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Information Processing for Integrated Observation and Simulation BAsed Risk Management of Geophysical Mass Flows</data>
      <data key="e_abstract">Real-time Environmental Observation and Forecasting Systems (EOFS) will revolutionize the way scientists share information about the environment and represent an opportunity to break traditional information barriers separating scientists from society at large. EOFS are already in use, but they tend to be small-scale, application- and domain- specific, stand-alone systems. There is a need for evolution towards multi-purpose shared systems designed to adapt flexibly to evolving needs of information consumers. What is required are large-scale, shared, heterogeneous distributed systems that make extensive use of diverse sensor-based inputs, sophisticated numerical simulations, mobile and embedded real-time system components, wireless and wired communications, high-performance computers, and high capacity storage systems. &lt;br/&gt;&lt;br/&gt;This ITR medium project has assembled an inter-disciplinary team, including computer science and environmental science researchers in addition to a heterogeneous base of pilot users. This group will collaborate to develop software technology which will enable EOFS to evolve efficiently, and to deliver quantifiably reliable information about the environment at the right time and in the right form to the right users. The project focus is on EOFS for estuarine and coastal regions. These regions are selected because they are highly variable natural systems subject to intense human activity and with great social, environmental, economic and cultural value.&lt;br/&gt;&lt;br/&gt;The research will include:&lt;br/&gt;&lt;br/&gt;i. Developing missing integration concepts and technologies for EOFS, with emphasis on quality-scalable information processing, storage and access (the computer science research).&lt;br/&gt;ii. Closing the loop between environmental models and sensors, and implementing a next generation EOFS based on an existing prototype for an estuary with multiple and often conflicting uses (the environmental observation and forecasting systems research);&lt;br/&gt;iii. Using, evaluating and refining the EOFS prototype for scientific discovery, natural resources stewardship and emergency response, thus incorporating sound science in operational and management decisions of critical regional importance and national significance (the environmental science and management applications);&lt;br/&gt;iv. Developing pilot multi-level, inter-disciplinary educational programs that cross-train young people, computer scientists, environmental scientists and practitioners in the conceptualization, development and use of environmental information technology (the education impact).</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121254</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121254</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n850" target="n851">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/IM: Enabling the Creation and Use of GeoGrids for Next Generation Geospatial Information</data>
      <data key="e_abstract">The objective of this project is to advance science in information management, focusing in particular on geospatial information. It addresses the development of concepts, algorithms, and system architectures to enable users on a grid to query, analyze, and contribute to multivariate, quality-aware geospatial information. The approach consists of three complementary research areas: (1) establishing a statistical framework for assessing geospatial data quality; (2) developing uncertainty-based query processing capabilities; and (3) supporting the development of space- and accuracy-aware adaptive systems for geospatial datasets. The results of this project will support the extension of the concept of the computational grid to facilitate ubiquitous access, interaction, and contributions of quality-aware next generation geospatial information. By developing novel query processes as well as quality and similarity metrics the project aims to enable the integration and use of large collections of disperse information of varying quality and accuracy. This supports the evolution of a novel geocomputational paradigm, moving away from current standards-driven approaches to an inclusive, adaptive system, with example potential applications in mobile computing, bioinformatics, and geographic information systems. This experimental research is linked to educational activities in three different academic programs among the three participating sites. The outreach activities of this project include collaboration with U.S. federal agencies involved in geospatial data collection, an international partner (Brazil&apos;s National Institute for Space Research), and the organization of a 2-day workshop with the participation of U.S. and international experts.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121269</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121269</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n850" target="n852">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/IM: Enabling the Creation and Use of GeoGrids for Next Generation Geospatial Information</data>
      <data key="e_abstract">The objective of this project is to advance science in information management, focusing in particular on geospatial information. It addresses the development of concepts, algorithms, and system architectures to enable users on a grid to query, analyze, and contribute to multivariate, quality-aware geospatial information. The approach consists of three complementary research areas: (1) establishing a statistical framework for assessing geospatial data quality; (2) developing uncertainty-based query processing capabilities; and (3) supporting the development of space- and accuracy-aware adaptive systems for geospatial datasets. The results of this project will support the extension of the concept of the computational grid to facilitate ubiquitous access, interaction, and contributions of quality-aware next generation geospatial information. By developing novel query processes as well as quality and similarity metrics the project aims to enable the integration and use of large collections of disperse information of varying quality and accuracy. This supports the evolution of a novel geocomputational paradigm, moving away from current standards-driven approaches to an inclusive, adaptive system, with example potential applications in mobile computing, bioinformatics, and geographic information systems. This experimental research is linked to educational activities in three different academic programs among the three participating sites. The outreach activities of this project include collaboration with U.S. federal agencies involved in geospatial data collection, an international partner (Brazil&apos;s National Institute for Space Research), and the organization of a 2-day workshop with the participation of U.S. and international experts.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121269</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121269</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n850" target="n853">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/IM: Enabling the Creation and Use of GeoGrids for Next Generation Geospatial Information</data>
      <data key="e_abstract">The objective of this project is to advance science in information management, focusing in particular on geospatial information. It addresses the development of concepts, algorithms, and system architectures to enable users on a grid to query, analyze, and contribute to multivariate, quality-aware geospatial information. The approach consists of three complementary research areas: (1) establishing a statistical framework for assessing geospatial data quality; (2) developing uncertainty-based query processing capabilities; and (3) supporting the development of space- and accuracy-aware adaptive systems for geospatial datasets. The results of this project will support the extension of the concept of the computational grid to facilitate ubiquitous access, interaction, and contributions of quality-aware next generation geospatial information. By developing novel query processes as well as quality and similarity metrics the project aims to enable the integration and use of large collections of disperse information of varying quality and accuracy. This supports the evolution of a novel geocomputational paradigm, moving away from current standards-driven approaches to an inclusive, adaptive system, with example potential applications in mobile computing, bioinformatics, and geographic information systems. This experimental research is linked to educational activities in three different academic programs among the three participating sites. The outreach activities of this project include collaboration with U.S. federal agencies involved in geospatial data collection, an international partner (Brazil&apos;s National Institute for Space Research), and the organization of a 2-day workshop with the participation of U.S. and international experts.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121269</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121269</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n850" target="n854">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/IM: Enabling the Creation and Use of GeoGrids for Next Generation Geospatial Information</data>
      <data key="e_abstract">The objective of this project is to advance science in information management, focusing in particular on geospatial information. It addresses the development of concepts, algorithms, and system architectures to enable users on a grid to query, analyze, and contribute to multivariate, quality-aware geospatial information. The approach consists of three complementary research areas: (1) establishing a statistical framework for assessing geospatial data quality; (2) developing uncertainty-based query processing capabilities; and (3) supporting the development of space- and accuracy-aware adaptive systems for geospatial datasets. The results of this project will support the extension of the concept of the computational grid to facilitate ubiquitous access, interaction, and contributions of quality-aware next generation geospatial information. By developing novel query processes as well as quality and similarity metrics the project aims to enable the integration and use of large collections of disperse information of varying quality and accuracy. This supports the evolution of a novel geocomputational paradigm, moving away from current standards-driven approaches to an inclusive, adaptive system, with example potential applications in mobile computing, bioinformatics, and geographic information systems. This experimental research is linked to educational activities in three different academic programs among the three participating sites. The outreach activities of this project include collaboration with U.S. federal agencies involved in geospatial data collection, an international partner (Brazil&apos;s National Institute for Space Research), and the organization of a 2-day workshop with the participation of U.S. and international experts.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121269</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121269</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n851" target="n852">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/IM: Enabling the Creation and Use of GeoGrids for Next Generation Geospatial Information</data>
      <data key="e_abstract">The objective of this project is to advance science in information management, focusing in particular on geospatial information. It addresses the development of concepts, algorithms, and system architectures to enable users on a grid to query, analyze, and contribute to multivariate, quality-aware geospatial information. The approach consists of three complementary research areas: (1) establishing a statistical framework for assessing geospatial data quality; (2) developing uncertainty-based query processing capabilities; and (3) supporting the development of space- and accuracy-aware adaptive systems for geospatial datasets. The results of this project will support the extension of the concept of the computational grid to facilitate ubiquitous access, interaction, and contributions of quality-aware next generation geospatial information. By developing novel query processes as well as quality and similarity metrics the project aims to enable the integration and use of large collections of disperse information of varying quality and accuracy. This supports the evolution of a novel geocomputational paradigm, moving away from current standards-driven approaches to an inclusive, adaptive system, with example potential applications in mobile computing, bioinformatics, and geographic information systems. This experimental research is linked to educational activities in three different academic programs among the three participating sites. The outreach activities of this project include collaboration with U.S. federal agencies involved in geospatial data collection, an international partner (Brazil&apos;s National Institute for Space Research), and the organization of a 2-day workshop with the participation of U.S. and international experts.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121269</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121269</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n851" target="n853">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/IM: Enabling the Creation and Use of GeoGrids for Next Generation Geospatial Information</data>
      <data key="e_abstract">The objective of this project is to advance science in information management, focusing in particular on geospatial information. It addresses the development of concepts, algorithms, and system architectures to enable users on a grid to query, analyze, and contribute to multivariate, quality-aware geospatial information. The approach consists of three complementary research areas: (1) establishing a statistical framework for assessing geospatial data quality; (2) developing uncertainty-based query processing capabilities; and (3) supporting the development of space- and accuracy-aware adaptive systems for geospatial datasets. The results of this project will support the extension of the concept of the computational grid to facilitate ubiquitous access, interaction, and contributions of quality-aware next generation geospatial information. By developing novel query processes as well as quality and similarity metrics the project aims to enable the integration and use of large collections of disperse information of varying quality and accuracy. This supports the evolution of a novel geocomputational paradigm, moving away from current standards-driven approaches to an inclusive, adaptive system, with example potential applications in mobile computing, bioinformatics, and geographic information systems. This experimental research is linked to educational activities in three different academic programs among the three participating sites. The outreach activities of this project include collaboration with U.S. federal agencies involved in geospatial data collection, an international partner (Brazil&apos;s National Institute for Space Research), and the organization of a 2-day workshop with the participation of U.S. and international experts.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121269</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121269</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n851" target="n854">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/IM: Enabling the Creation and Use of GeoGrids for Next Generation Geospatial Information</data>
      <data key="e_abstract">The objective of this project is to advance science in information management, focusing in particular on geospatial information. It addresses the development of concepts, algorithms, and system architectures to enable users on a grid to query, analyze, and contribute to multivariate, quality-aware geospatial information. The approach consists of three complementary research areas: (1) establishing a statistical framework for assessing geospatial data quality; (2) developing uncertainty-based query processing capabilities; and (3) supporting the development of space- and accuracy-aware adaptive systems for geospatial datasets. The results of this project will support the extension of the concept of the computational grid to facilitate ubiquitous access, interaction, and contributions of quality-aware next generation geospatial information. By developing novel query processes as well as quality and similarity metrics the project aims to enable the integration and use of large collections of disperse information of varying quality and accuracy. This supports the evolution of a novel geocomputational paradigm, moving away from current standards-driven approaches to an inclusive, adaptive system, with example potential applications in mobile computing, bioinformatics, and geographic information systems. This experimental research is linked to educational activities in three different academic programs among the three participating sites. The outreach activities of this project include collaboration with U.S. federal agencies involved in geospatial data collection, an international partner (Brazil&apos;s National Institute for Space Research), and the organization of a 2-day workshop with the participation of U.S. and international experts.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121269</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121269</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n852" target="n853">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/IM: Enabling the Creation and Use of GeoGrids for Next Generation Geospatial Information</data>
      <data key="e_abstract">The objective of this project is to advance science in information management, focusing in particular on geospatial information. It addresses the development of concepts, algorithms, and system architectures to enable users on a grid to query, analyze, and contribute to multivariate, quality-aware geospatial information. The approach consists of three complementary research areas: (1) establishing a statistical framework for assessing geospatial data quality; (2) developing uncertainty-based query processing capabilities; and (3) supporting the development of space- and accuracy-aware adaptive systems for geospatial datasets. The results of this project will support the extension of the concept of the computational grid to facilitate ubiquitous access, interaction, and contributions of quality-aware next generation geospatial information. By developing novel query processes as well as quality and similarity metrics the project aims to enable the integration and use of large collections of disperse information of varying quality and accuracy. This supports the evolution of a novel geocomputational paradigm, moving away from current standards-driven approaches to an inclusive, adaptive system, with example potential applications in mobile computing, bioinformatics, and geographic information systems. This experimental research is linked to educational activities in three different academic programs among the three participating sites. The outreach activities of this project include collaboration with U.S. federal agencies involved in geospatial data collection, an international partner (Brazil&apos;s National Institute for Space Research), and the organization of a 2-day workshop with the participation of U.S. and international experts.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121269</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121269</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n852" target="n854">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/IM: Enabling the Creation and Use of GeoGrids for Next Generation Geospatial Information</data>
      <data key="e_abstract">The objective of this project is to advance science in information management, focusing in particular on geospatial information. It addresses the development of concepts, algorithms, and system architectures to enable users on a grid to query, analyze, and contribute to multivariate, quality-aware geospatial information. The approach consists of three complementary research areas: (1) establishing a statistical framework for assessing geospatial data quality; (2) developing uncertainty-based query processing capabilities; and (3) supporting the development of space- and accuracy-aware adaptive systems for geospatial datasets. The results of this project will support the extension of the concept of the computational grid to facilitate ubiquitous access, interaction, and contributions of quality-aware next generation geospatial information. By developing novel query processes as well as quality and similarity metrics the project aims to enable the integration and use of large collections of disperse information of varying quality and accuracy. This supports the evolution of a novel geocomputational paradigm, moving away from current standards-driven approaches to an inclusive, adaptive system, with example potential applications in mobile computing, bioinformatics, and geographic information systems. This experimental research is linked to educational activities in three different academic programs among the three participating sites. The outreach activities of this project include collaboration with U.S. federal agencies involved in geospatial data collection, an international partner (Brazil&apos;s National Institute for Space Research), and the organization of a 2-day workshop with the participation of U.S. and international experts.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121269</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121269</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n853" target="n854">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/IM: Enabling the Creation and Use of GeoGrids for Next Generation Geospatial Information</data>
      <data key="e_abstract">The objective of this project is to advance science in information management, focusing in particular on geospatial information. It addresses the development of concepts, algorithms, and system architectures to enable users on a grid to query, analyze, and contribute to multivariate, quality-aware geospatial information. The approach consists of three complementary research areas: (1) establishing a statistical framework for assessing geospatial data quality; (2) developing uncertainty-based query processing capabilities; and (3) supporting the development of space- and accuracy-aware adaptive systems for geospatial datasets. The results of this project will support the extension of the concept of the computational grid to facilitate ubiquitous access, interaction, and contributions of quality-aware next generation geospatial information. By developing novel query processes as well as quality and similarity metrics the project aims to enable the integration and use of large collections of disperse information of varying quality and accuracy. This supports the evolution of a novel geocomputational paradigm, moving away from current standards-driven approaches to an inclusive, adaptive system, with example potential applications in mobile computing, bioinformatics, and geographic information systems. This experimental research is linked to educational activities in three different academic programs among the three participating sites. The outreach activities of this project include collaboration with U.S. federal agencies involved in geospatial data collection, an international partner (Brazil&apos;s National Institute for Space Research), and the organization of a 2-day workshop with the participation of U.S. and international experts.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121269</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121269</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n855" target="n856">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">SGER: Integrating Actor Model with Real-Time Elastic Control Theory</data>
      <data key="e_abstract">The rapid advancement of networking technologies and increasing use of embedded devices has extended the scope of traditional computational systems to include intelligent control of physical environments. There are many challenges in such embedded hybrid control systems. Traditionally, the programming language community focused on modeling and reasoning about the semantics of interactions between distributed agents, while the real-time computing community focused on how to manage CPU and network communication resources so that real-time tasks can predictably meet their end-to-end timing constraints. &lt;br/&gt;&lt;br/&gt;This proposed effort integrates the Actor theory with the theory of real-time elastic control. Real time elastic control theory is an innovative approach that integrates the design of a feedback controller with the design of a real-time scheduler. Traditionally, feedback control is a prototypical example of hard real-time applications. This work allows the controller to adapt to unpredictable surges in workload by slowing down its sampling frequencies and adjusting its gains. Such workload surges are unschedulable without the controller adaptation. The word &quot;elastic&quot; refers to the dynamic changes of controller deadlines to improve the management of available computing resources. This work broke the barrier between real-time scheduling theory and feedback control theory. By integrating this theory with Actor model, the barriers between programming language and concurrency control theory, real-time scheduling theory, and feedback control theory will be broken. In addition to the usual qualitative properties (such as eventuality) handled by current formalisms for distributed systems, this work will lay the foundation for a unified framework that will allow users to reason about quantitative properties including whether the timing requirements can be met and the physical system under control is stable.</data>
      <data key="e_pgm">2878</data>
      <data key="e_label">137090</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">137090</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n857" target="n858">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics - BDEI - Overcoming nomenclatural complications while searching in a distributed database environment: One step toward true interoperability</data>
      <data key="e_abstract">EIA-0131928&lt;br/&gt;Lane, Meredith &lt;br/&gt;Academy of Natural Sciences - Philadelphia&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;BDEI: Overcoming nomenclatural complications while searching in a distributed database environment: One step toward true interoperability.&lt;br/&gt;&lt;br/&gt;Summary&lt;br/&gt;&lt;br/&gt;With biological information content growing exponentially on the Web, the average member of the public&lt;br/&gt;(and for that matter, many scientists) may not be able to access all of the high-quality information actually&lt;br/&gt;available and/or that they need because organisms are often known by more than one scientific name (many of which will perhaps be unknown to the user), and different information providers may use different names for the same organism.&lt;br/&gt;&lt;br/&gt;Important international projects such as the Global Biodiversity Information Facility (see www.gbif.org)&lt;br/&gt;have identified organism names as the core means for interconnecting databases from different domains&lt;br/&gt;(e.g., specimen databases with GenBank or ecological databases, etc.) because the organism name is&lt;br/&gt;highly likely to be the only database field common to both databases. True interoperability among&lt;br/&gt;databases is required to answer complex biological questions. This goal cannot be reached without a&lt;br/&gt;mechanism to allow access to all available information on a particular organism.&lt;br/&gt;&lt;br/&gt;Therefore, a foundation component in the development of the GBIF and associated national efforts&lt;br/&gt;must be a means to provide a list of all the names that should be searched for a given organism. The need&lt;br/&gt;for this component of interoperability was clearly recognized by the proposers of GBIF, but to date there&lt;br/&gt;have been no efforts specifically directed at providing software tools to make it possible. In this project,&lt;br/&gt;we will implement such a mechanism. It will allow even the most naive Web user to obtain biological&lt;br/&gt;information without needing to understand the complexities of nomenclature and associated arcana. To do&lt;br/&gt;this, we will develop a query interface/portal understandable to any user; a set of standards and protocols for representing search requests (which will be sent as a URL) and result sets (which will be returned as XML using HTTP) that is platform-independent; and a set of platform-specific translators and search routines for retrieving data from participating&lt;br/&gt;nomenclatural data providers.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">131928</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">131928</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n859" target="n860">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/AP: Modeling and Simulation of Sub-micron VLSI</data>
      <data key="e_abstract">For the next generation Very Large Scale Integration (VLSI) circuits, the signal delay will be dominated by parasitic resistance (R), capacitance (C), and inductance (L) of the interconnect. The ability to extract RCL&lt;br/&gt;parasitic quickly and accurately is crucial to the design and verification of large VLSI circuits. This project will develop innovative algorithms and software for fast and accurate extraction of RCL parasitic of VLSI&lt;br/&gt;circuits. The main goal of the project is the design of preconditioned iterative methods for solving the linear systems arising in inductance and capacitance extraction problems. Solvers for the inductance problem will&lt;br/&gt;use a novel solenoidal basis approach to precondition a reduced system implicitly, leading to rapid convergence of the iterative methods. Fast approximations to the matrix-vector products with dense system matrices&lt;br/&gt;will be computed using efficient hierarchical methods. &lt;br/&gt;&lt;br/&gt;Parallelism in the algorithms will be exploited to develop high-performance software that is capable of tackling large problems. Software developed for this project will be portable across a variety of&lt;br/&gt;parallel architectures. It is anticipated that the code will deliver the performance necessary for parasitic RCL extraction of deep sub-micron VLSI circuits of realistic sizes.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113668</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113668</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n862" target="n863">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Robust Active Vision Systems</data>
      <data key="e_abstract">The topic of the project is development of a framework for robust active vision, with the goal of obtaining systems capable of performing satisfactorily in the presence of uncertainty, such as due to poor calibration, noise, or a changing environment. The work will be complemented by a comprehensive experimental validation and performance characterization.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">117387</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">117387</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n671" target="n865">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Collaborative Proposal-Using the Web as a Corpus for Empirical Linguistic Research</data>
      <data key="e_abstract">. This project will develop tools that make it possible to retrieve naturally occurring sentences from the World Wide Web on the basis of lexical content and syntactic structure, providing linguists with an immediate, easily accessible source of raw linguistic data. The PIs will investigate specific linguistic hypotheses at the lexical semantics/syntax interface as an illustrative application of these tools. At a high level, the planned work constitutes an important step toward a new paradigm for linguistic research. Rather than relying entirely on introspective data generated by the linguist who is trying to (dis)prove a particular hypothesis, Web-enabled linguistics research will draw on the methodology and the tools developed by the PIs to supply naturally occurring data on which theories can rest. With regard to specific linguistic questions, the goal is to provide an explanation of the rules and constraints that govern three transitivity alternations (Middle, Unaccusative, Unspecified Object Deletion), and the PIs expect data made available by their tools to shed light on the &quot;grey&quot; area between competence and performance, that is, the linguistic behavior that seems to fall outside of rule-governed behavior. Although naturally occurring data are not accorded great emphasis in generative syntax, the use of text corpora has a tradition in the greater linguistic enterprise. An explosive new phenomenon in the world of naturally occurring text, the World Wide Web is an essentially untapped resource that embodies the rich and dynamic nature of language, presenting a data resource of unparalleled size and diversity.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">113641</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">113641</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n867" target="n868">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: Optimized Bacteriorhodopsin Proteins for Photonic and Holographic Memory Storage</data>
      <data key="e_abstract">EIA-0129731&lt;br/&gt;Robert R. Birge&lt;br/&gt;Syracuse University&lt;br/&gt;&lt;br/&gt;Title: Optimized Bacteriorhodopsin Proteins for Photonic and HolographicMemory Storage&lt;br/&gt;&lt;br/&gt;The protein bacteriorhodopsin is being optimized for bioelectronic and photonic applications through site directed mutagenesis, random mutagenesis and directed evolution. The native protein has many characteristics that make it a nearly optimal photonic material, but further optimization is necessary to achieve competitive performance in computer applications. The goal is to generate protein variants that are optimized for holographic associative memories and large scale volumetric memories. The former memory system is based on Fourier-transform holography, and the protein will be optimized for thermal stability, quantum efficiency and holographic efficiency. &lt;br/&gt;&lt;br/&gt;The large scale volumetric memory is based on a branching reaction out of the main photocycle. The protein is being optimized for this application by increasing thermal stability, branching efficiency and long-term stability of the branched photoproduct. Genetic engineering is the best method for achieving all three goals simultaneously, and directed evolution is being studied as one possible method of rapidly achieving these goals simultaneously. This is a collaborative project involving researchers at the W. M. Keck Center for Molecular Electronics at Syracuse University and molecular biologists at the University of Connecticut.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">129731</data>
      <data key="e_expirationDate">2004-11-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">129731</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n867" target="n869">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: Optimized Bacteriorhodopsin Proteins for Photonic and Holographic Memory Storage</data>
      <data key="e_abstract">EIA-0129731&lt;br/&gt;Robert R. Birge&lt;br/&gt;Syracuse University&lt;br/&gt;&lt;br/&gt;Title: Optimized Bacteriorhodopsin Proteins for Photonic and HolographicMemory Storage&lt;br/&gt;&lt;br/&gt;The protein bacteriorhodopsin is being optimized for bioelectronic and photonic applications through site directed mutagenesis, random mutagenesis and directed evolution. The native protein has many characteristics that make it a nearly optimal photonic material, but further optimization is necessary to achieve competitive performance in computer applications. The goal is to generate protein variants that are optimized for holographic associative memories and large scale volumetric memories. The former memory system is based on Fourier-transform holography, and the protein will be optimized for thermal stability, quantum efficiency and holographic efficiency. &lt;br/&gt;&lt;br/&gt;The large scale volumetric memory is based on a branching reaction out of the main photocycle. The protein is being optimized for this application by increasing thermal stability, branching efficiency and long-term stability of the branched photoproduct. Genetic engineering is the best method for achieving all three goals simultaneously, and directed evolution is being studied as one possible method of rapidly achieving these goals simultaneously. This is a collaborative project involving researchers at the W. M. Keck Center for Molecular Electronics at Syracuse University and molecular biologists at the University of Connecticut.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">129731</data>
      <data key="e_expirationDate">2004-11-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">129731</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n868" target="n869">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: Optimized Bacteriorhodopsin Proteins for Photonic and Holographic Memory Storage</data>
      <data key="e_abstract">EIA-0129731&lt;br/&gt;Robert R. Birge&lt;br/&gt;Syracuse University&lt;br/&gt;&lt;br/&gt;Title: Optimized Bacteriorhodopsin Proteins for Photonic and HolographicMemory Storage&lt;br/&gt;&lt;br/&gt;The protein bacteriorhodopsin is being optimized for bioelectronic and photonic applications through site directed mutagenesis, random mutagenesis and directed evolution. The native protein has many characteristics that make it a nearly optimal photonic material, but further optimization is necessary to achieve competitive performance in computer applications. The goal is to generate protein variants that are optimized for holographic associative memories and large scale volumetric memories. The former memory system is based on Fourier-transform holography, and the protein will be optimized for thermal stability, quantum efficiency and holographic efficiency. &lt;br/&gt;&lt;br/&gt;The large scale volumetric memory is based on a branching reaction out of the main photocycle. The protein is being optimized for this application by increasing thermal stability, branching efficiency and long-term stability of the branched photoproduct. Genetic engineering is the best method for achieving all three goals simultaneously, and directed evolution is being studied as one possible method of rapidly achieving these goals simultaneously. This is a collaborative project involving researchers at the W. M. Keck Center for Molecular Electronics at Syracuse University and molecular biologists at the University of Connecticut.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">129731</data>
      <data key="e_expirationDate">2004-11-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">129731</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n871" target="n872">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Collaborative Research: ITR/AP Reconstructing Complex Evolutionary Histories</data>
      <data key="e_abstract">EIA-0121377&lt;br/&gt;Moret, Bernard M&lt;br/&gt;University of New Mexico&lt;br/&gt;&lt;br/&gt;Collaborative Research: ITR/AP: Reconstructing Complex Evolutionary Histories&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Reconstruction of the evolutionary history of a group of organisms has changed the face of biology and is being used increasingly in drug discovery, epidemiology, and genetic engineering. Unfortunately, such reconstructions typically involve solving difficult optimization problems, so that even moderately large datasets can require months to years of computation. In addition, almost all evolutionary reconstructions presently assume that the historical pattern is one of strict divergence that can be represented by a binary tree. This assumption is frequently violated, especially by plants which often hybridize readily and thus produce networks of relationships.&lt;br/&gt;&lt;br/&gt;This project brings together computer scientists and biologists from two institutions to develop new models and algorithms to address these two problems. Successful completion of this project will have an enormous impact by providing tools for reconstructing phylogenies of large datasets, and the first tools for inferring network models of evolution appropriate to hybridizing speciation. Such network models will alter how biologists think about speciation, while the development of methods for large-scale analyses will strongly benefit medical and pharmaceutical practice. &lt;br/&gt;Information technology will be advanced in fundamental ways as well, as the project will demonstrate how algorithm design and high-performance algorithm engineering can jointly solve very difficult discrete optimization problems.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121377</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121377</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n875" target="n876">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">The Columbia Hot Spot Rescue Service</data>
      <data key="e_abstract">Although Internet traffic is routinely quite heavy, there has usually been more than enough storage,&lt;br/&gt;processing and bandwidth capacity to provide acceptable performance. However, it is well known that, more&lt;br/&gt;and more frequently, demands for network resources are mushrooming locally into hotspots or data storms,&lt;br/&gt;and in such cases the affected web sites and subnetworks founder almost completely, creating revenue&lt;br/&gt;losses and client dissatisfaction on a large scale. We propose a novel collaborative technology to alleviate&lt;br/&gt;the effects of these hotspots, a technology that we will apply by designing and prototyping a Hotspot Rescue&lt;br/&gt;Service (HRS). This work will build on the past research of the PI&apos;s in networking, operating systems, and&lt;br/&gt;distributed caching.&lt;br/&gt; A key premise on which the technology is founded lies in the observation that existing Internet band-width&lt;br/&gt;resources are sufficient to deal effectively with hotspots. In other words, rescues of heavily-overloaded&lt;br/&gt;sites can be assembled from underutilized resources lying elsewhere. It follows that there is no inherent need&lt;br/&gt;for resources held in reserve uniquely for this purpose, i.e., there is no need for over-provisioned, under-utilized resources such as distributed caches to protect against hotspots. We propose instead a paradigm shift in which efficient mechanisms that we provide will enable communities of participating sites to share their resources to suppress hotspots. The service in action will be transparent, in part self-regulating and will take the form of automated traffic redirection to sites with available bandwidth.&lt;br/&gt; The proposed Columbia HRS will be proactive as well as reactive. We will amass hotspot data that&lt;br/&gt;will be modeled and analyzed with the aim of designing hotspot daemons or plugins, software devices for&lt;br/&gt;monitoring traffic behavior and signalling incipient hotspots via hotspot watches and advisories, along with&lt;br/&gt;relevant statistics. We will implement two complementary approaches to the technology, a server-based&lt;br/&gt;approach and a client-based, peer-to-peer (P2P) approach.&lt;br/&gt; In the server-based approach, servers monitor their own loads, the loads of a small set of servers that&lt;br/&gt;they would service in the event that the other server overloads, and, via probes between servers, network&lt;br/&gt;conditions. When a server or network component is identified as going into possible overload, the system&lt;br/&gt;activates a replication mechanism to duplicate the hot content. Clients can then retrieve the content from&lt;br/&gt;the server sites acting as replicas, alleviating the load on the original overloaded resource.&lt;br/&gt; In the P2P approach, clients install a plugin into their browser that communicates with similar plugins&lt;br/&gt;installed on other clients&apos; browsers, as well as with a distributed directory service. Clients cache their recent downloads, and, through the plugin, inform the directory service of the objects that are cached. The directory service can then identify the most popular content, as well as cached locations and notify additional clients of these alternative locations for download. By using client machines to store and deliver the hot content and if requests for the given content can be redirected to the client machines, the hotspot at the server can be eliminated. In this way, hotspot response becomes self-organizing and self-regulatory.&lt;br/&gt; There are several issues that need to be addressed as we develop this rescue service. First, we will use&lt;br/&gt;experimentation and analysis of collected data (including data sets obtained via a company partnerships)&lt;br/&gt;to develop models of causes and effects of network hotspot activity. Next, we will analytically evaluate&lt;br/&gt;the effect on server and network load that techniques such as caching, redirecting, and migrating have&lt;br/&gt;upon hotspots within the network. Last, we will implement and evaluate prototype systems to validate&lt;br/&gt;their effectiveness, either upon simulated hotspot activity within a testbed, or if possible, on actual hotspots&lt;br/&gt;through agreements with content providers.&lt;br/&gt; Development of the HRS will provide research opportunities to multiple students at Columbia, and&lt;br/&gt;its deployment will improve web performance of objects served from academic institutions and non-profit&lt;br/&gt;companies whose content is to date not hosted by commerical third-party content delivery companies.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">117738</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">117738</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n70" target="n875">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">The Columbia Hot Spot Rescue Service</data>
      <data key="e_abstract">Although Internet traffic is routinely quite heavy, there has usually been more than enough storage,&lt;br/&gt;processing and bandwidth capacity to provide acceptable performance. However, it is well known that, more&lt;br/&gt;and more frequently, demands for network resources are mushrooming locally into hotspots or data storms,&lt;br/&gt;and in such cases the affected web sites and subnetworks founder almost completely, creating revenue&lt;br/&gt;losses and client dissatisfaction on a large scale. We propose a novel collaborative technology to alleviate&lt;br/&gt;the effects of these hotspots, a technology that we will apply by designing and prototyping a Hotspot Rescue&lt;br/&gt;Service (HRS). This work will build on the past research of the PI&apos;s in networking, operating systems, and&lt;br/&gt;distributed caching.&lt;br/&gt; A key premise on which the technology is founded lies in the observation that existing Internet band-width&lt;br/&gt;resources are sufficient to deal effectively with hotspots. In other words, rescues of heavily-overloaded&lt;br/&gt;sites can be assembled from underutilized resources lying elsewhere. It follows that there is no inherent need&lt;br/&gt;for resources held in reserve uniquely for this purpose, i.e., there is no need for over-provisioned, under-utilized resources such as distributed caches to protect against hotspots. We propose instead a paradigm shift in which efficient mechanisms that we provide will enable communities of participating sites to share their resources to suppress hotspots. The service in action will be transparent, in part self-regulating and will take the form of automated traffic redirection to sites with available bandwidth.&lt;br/&gt; The proposed Columbia HRS will be proactive as well as reactive. We will amass hotspot data that&lt;br/&gt;will be modeled and analyzed with the aim of designing hotspot daemons or plugins, software devices for&lt;br/&gt;monitoring traffic behavior and signalling incipient hotspots via hotspot watches and advisories, along with&lt;br/&gt;relevant statistics. We will implement two complementary approaches to the technology, a server-based&lt;br/&gt;approach and a client-based, peer-to-peer (P2P) approach.&lt;br/&gt; In the server-based approach, servers monitor their own loads, the loads of a small set of servers that&lt;br/&gt;they would service in the event that the other server overloads, and, via probes between servers, network&lt;br/&gt;conditions. When a server or network component is identified as going into possible overload, the system&lt;br/&gt;activates a replication mechanism to duplicate the hot content. Clients can then retrieve the content from&lt;br/&gt;the server sites acting as replicas, alleviating the load on the original overloaded resource.&lt;br/&gt; In the P2P approach, clients install a plugin into their browser that communicates with similar plugins&lt;br/&gt;installed on other clients&apos; browsers, as well as with a distributed directory service. Clients cache their recent downloads, and, through the plugin, inform the directory service of the objects that are cached. The directory service can then identify the most popular content, as well as cached locations and notify additional clients of these alternative locations for download. By using client machines to store and deliver the hot content and if requests for the given content can be redirected to the client machines, the hotspot at the server can be eliminated. In this way, hotspot response becomes self-organizing and self-regulatory.&lt;br/&gt; There are several issues that need to be addressed as we develop this rescue service. First, we will use&lt;br/&gt;experimentation and analysis of collected data (including data sets obtained via a company partnerships)&lt;br/&gt;to develop models of causes and effects of network hotspot activity. Next, we will analytically evaluate&lt;br/&gt;the effect on server and network load that techniques such as caching, redirecting, and migrating have&lt;br/&gt;upon hotspots within the network. Last, we will implement and evaluate prototype systems to validate&lt;br/&gt;their effectiveness, either upon simulated hotspot activity within a testbed, or if possible, on actual hotspots&lt;br/&gt;through agreements with content providers.&lt;br/&gt; Development of the HRS will provide research opportunities to multiple students at Columbia, and&lt;br/&gt;its deployment will improve web performance of objects served from academic institutions and non-profit&lt;br/&gt;companies whose content is to date not hosted by commerical third-party content delivery companies.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">117738</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">117738</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n875" target="n878">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">The Columbia Hot Spot Rescue Service</data>
      <data key="e_abstract">Although Internet traffic is routinely quite heavy, there has usually been more than enough storage,&lt;br/&gt;processing and bandwidth capacity to provide acceptable performance. However, it is well known that, more&lt;br/&gt;and more frequently, demands for network resources are mushrooming locally into hotspots or data storms,&lt;br/&gt;and in such cases the affected web sites and subnetworks founder almost completely, creating revenue&lt;br/&gt;losses and client dissatisfaction on a large scale. We propose a novel collaborative technology to alleviate&lt;br/&gt;the effects of these hotspots, a technology that we will apply by designing and prototyping a Hotspot Rescue&lt;br/&gt;Service (HRS). This work will build on the past research of the PI&apos;s in networking, operating systems, and&lt;br/&gt;distributed caching.&lt;br/&gt; A key premise on which the technology is founded lies in the observation that existing Internet band-width&lt;br/&gt;resources are sufficient to deal effectively with hotspots. In other words, rescues of heavily-overloaded&lt;br/&gt;sites can be assembled from underutilized resources lying elsewhere. It follows that there is no inherent need&lt;br/&gt;for resources held in reserve uniquely for this purpose, i.e., there is no need for over-provisioned, under-utilized resources such as distributed caches to protect against hotspots. We propose instead a paradigm shift in which efficient mechanisms that we provide will enable communities of participating sites to share their resources to suppress hotspots. The service in action will be transparent, in part self-regulating and will take the form of automated traffic redirection to sites with available bandwidth.&lt;br/&gt; The proposed Columbia HRS will be proactive as well as reactive. We will amass hotspot data that&lt;br/&gt;will be modeled and analyzed with the aim of designing hotspot daemons or plugins, software devices for&lt;br/&gt;monitoring traffic behavior and signalling incipient hotspots via hotspot watches and advisories, along with&lt;br/&gt;relevant statistics. We will implement two complementary approaches to the technology, a server-based&lt;br/&gt;approach and a client-based, peer-to-peer (P2P) approach.&lt;br/&gt; In the server-based approach, servers monitor their own loads, the loads of a small set of servers that&lt;br/&gt;they would service in the event that the other server overloads, and, via probes between servers, network&lt;br/&gt;conditions. When a server or network component is identified as going into possible overload, the system&lt;br/&gt;activates a replication mechanism to duplicate the hot content. Clients can then retrieve the content from&lt;br/&gt;the server sites acting as replicas, alleviating the load on the original overloaded resource.&lt;br/&gt; In the P2P approach, clients install a plugin into their browser that communicates with similar plugins&lt;br/&gt;installed on other clients&apos; browsers, as well as with a distributed directory service. Clients cache their recent downloads, and, through the plugin, inform the directory service of the objects that are cached. The directory service can then identify the most popular content, as well as cached locations and notify additional clients of these alternative locations for download. By using client machines to store and deliver the hot content and if requests for the given content can be redirected to the client machines, the hotspot at the server can be eliminated. In this way, hotspot response becomes self-organizing and self-regulatory.&lt;br/&gt; There are several issues that need to be addressed as we develop this rescue service. First, we will use&lt;br/&gt;experimentation and analysis of collected data (including data sets obtained via a company partnerships)&lt;br/&gt;to develop models of causes and effects of network hotspot activity. Next, we will analytically evaluate&lt;br/&gt;the effect on server and network load that techniques such as caching, redirecting, and migrating have&lt;br/&gt;upon hotspots within the network. Last, we will implement and evaluate prototype systems to validate&lt;br/&gt;their effectiveness, either upon simulated hotspot activity within a testbed, or if possible, on actual hotspots&lt;br/&gt;through agreements with content providers.&lt;br/&gt; Development of the HRS will provide research opportunities to multiple students at Columbia, and&lt;br/&gt;its deployment will improve web performance of objects served from academic institutions and non-profit&lt;br/&gt;companies whose content is to date not hosted by commerical third-party content delivery companies.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">117738</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">117738</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n70" target="n876">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">The Columbia Hot Spot Rescue Service</data>
      <data key="e_abstract">Although Internet traffic is routinely quite heavy, there has usually been more than enough storage,&lt;br/&gt;processing and bandwidth capacity to provide acceptable performance. However, it is well known that, more&lt;br/&gt;and more frequently, demands for network resources are mushrooming locally into hotspots or data storms,&lt;br/&gt;and in such cases the affected web sites and subnetworks founder almost completely, creating revenue&lt;br/&gt;losses and client dissatisfaction on a large scale. We propose a novel collaborative technology to alleviate&lt;br/&gt;the effects of these hotspots, a technology that we will apply by designing and prototyping a Hotspot Rescue&lt;br/&gt;Service (HRS). This work will build on the past research of the PI&apos;s in networking, operating systems, and&lt;br/&gt;distributed caching.&lt;br/&gt; A key premise on which the technology is founded lies in the observation that existing Internet band-width&lt;br/&gt;resources are sufficient to deal effectively with hotspots. In other words, rescues of heavily-overloaded&lt;br/&gt;sites can be assembled from underutilized resources lying elsewhere. It follows that there is no inherent need&lt;br/&gt;for resources held in reserve uniquely for this purpose, i.e., there is no need for over-provisioned, under-utilized resources such as distributed caches to protect against hotspots. We propose instead a paradigm shift in which efficient mechanisms that we provide will enable communities of participating sites to share their resources to suppress hotspots. The service in action will be transparent, in part self-regulating and will take the form of automated traffic redirection to sites with available bandwidth.&lt;br/&gt; The proposed Columbia HRS will be proactive as well as reactive. We will amass hotspot data that&lt;br/&gt;will be modeled and analyzed with the aim of designing hotspot daemons or plugins, software devices for&lt;br/&gt;monitoring traffic behavior and signalling incipient hotspots via hotspot watches and advisories, along with&lt;br/&gt;relevant statistics. We will implement two complementary approaches to the technology, a server-based&lt;br/&gt;approach and a client-based, peer-to-peer (P2P) approach.&lt;br/&gt; In the server-based approach, servers monitor their own loads, the loads of a small set of servers that&lt;br/&gt;they would service in the event that the other server overloads, and, via probes between servers, network&lt;br/&gt;conditions. When a server or network component is identified as going into possible overload, the system&lt;br/&gt;activates a replication mechanism to duplicate the hot content. Clients can then retrieve the content from&lt;br/&gt;the server sites acting as replicas, alleviating the load on the original overloaded resource.&lt;br/&gt; In the P2P approach, clients install a plugin into their browser that communicates with similar plugins&lt;br/&gt;installed on other clients&apos; browsers, as well as with a distributed directory service. Clients cache their recent downloads, and, through the plugin, inform the directory service of the objects that are cached. The directory service can then identify the most popular content, as well as cached locations and notify additional clients of these alternative locations for download. By using client machines to store and deliver the hot content and if requests for the given content can be redirected to the client machines, the hotspot at the server can be eliminated. In this way, hotspot response becomes self-organizing and self-regulatory.&lt;br/&gt; There are several issues that need to be addressed as we develop this rescue service. First, we will use&lt;br/&gt;experimentation and analysis of collected data (including data sets obtained via a company partnerships)&lt;br/&gt;to develop models of causes and effects of network hotspot activity. Next, we will analytically evaluate&lt;br/&gt;the effect on server and network load that techniques such as caching, redirecting, and migrating have&lt;br/&gt;upon hotspots within the network. Last, we will implement and evaluate prototype systems to validate&lt;br/&gt;their effectiveness, either upon simulated hotspot activity within a testbed, or if possible, on actual hotspots&lt;br/&gt;through agreements with content providers.&lt;br/&gt; Development of the HRS will provide research opportunities to multiple students at Columbia, and&lt;br/&gt;its deployment will improve web performance of objects served from academic institutions and non-profit&lt;br/&gt;companies whose content is to date not hosted by commerical third-party content delivery companies.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">117738</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">117738</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n876" target="n878">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">The Columbia Hot Spot Rescue Service</data>
      <data key="e_abstract">Although Internet traffic is routinely quite heavy, there has usually been more than enough storage,&lt;br/&gt;processing and bandwidth capacity to provide acceptable performance. However, it is well known that, more&lt;br/&gt;and more frequently, demands for network resources are mushrooming locally into hotspots or data storms,&lt;br/&gt;and in such cases the affected web sites and subnetworks founder almost completely, creating revenue&lt;br/&gt;losses and client dissatisfaction on a large scale. We propose a novel collaborative technology to alleviate&lt;br/&gt;the effects of these hotspots, a technology that we will apply by designing and prototyping a Hotspot Rescue&lt;br/&gt;Service (HRS). This work will build on the past research of the PI&apos;s in networking, operating systems, and&lt;br/&gt;distributed caching.&lt;br/&gt; A key premise on which the technology is founded lies in the observation that existing Internet band-width&lt;br/&gt;resources are sufficient to deal effectively with hotspots. In other words, rescues of heavily-overloaded&lt;br/&gt;sites can be assembled from underutilized resources lying elsewhere. It follows that there is no inherent need&lt;br/&gt;for resources held in reserve uniquely for this purpose, i.e., there is no need for over-provisioned, under-utilized resources such as distributed caches to protect against hotspots. We propose instead a paradigm shift in which efficient mechanisms that we provide will enable communities of participating sites to share their resources to suppress hotspots. The service in action will be transparent, in part self-regulating and will take the form of automated traffic redirection to sites with available bandwidth.&lt;br/&gt; The proposed Columbia HRS will be proactive as well as reactive. We will amass hotspot data that&lt;br/&gt;will be modeled and analyzed with the aim of designing hotspot daemons or plugins, software devices for&lt;br/&gt;monitoring traffic behavior and signalling incipient hotspots via hotspot watches and advisories, along with&lt;br/&gt;relevant statistics. We will implement two complementary approaches to the technology, a server-based&lt;br/&gt;approach and a client-based, peer-to-peer (P2P) approach.&lt;br/&gt; In the server-based approach, servers monitor their own loads, the loads of a small set of servers that&lt;br/&gt;they would service in the event that the other server overloads, and, via probes between servers, network&lt;br/&gt;conditions. When a server or network component is identified as going into possible overload, the system&lt;br/&gt;activates a replication mechanism to duplicate the hot content. Clients can then retrieve the content from&lt;br/&gt;the server sites acting as replicas, alleviating the load on the original overloaded resource.&lt;br/&gt; In the P2P approach, clients install a plugin into their browser that communicates with similar plugins&lt;br/&gt;installed on other clients&apos; browsers, as well as with a distributed directory service. Clients cache their recent downloads, and, through the plugin, inform the directory service of the objects that are cached. The directory service can then identify the most popular content, as well as cached locations and notify additional clients of these alternative locations for download. By using client machines to store and deliver the hot content and if requests for the given content can be redirected to the client machines, the hotspot at the server can be eliminated. In this way, hotspot response becomes self-organizing and self-regulatory.&lt;br/&gt; There are several issues that need to be addressed as we develop this rescue service. First, we will use&lt;br/&gt;experimentation and analysis of collected data (including data sets obtained via a company partnerships)&lt;br/&gt;to develop models of causes and effects of network hotspot activity. Next, we will analytically evaluate&lt;br/&gt;the effect on server and network load that techniques such as caching, redirecting, and migrating have&lt;br/&gt;upon hotspots within the network. Last, we will implement and evaluate prototype systems to validate&lt;br/&gt;their effectiveness, either upon simulated hotspot activity within a testbed, or if possible, on actual hotspots&lt;br/&gt;through agreements with content providers.&lt;br/&gt; Development of the HRS will provide research opportunities to multiple students at Columbia, and&lt;br/&gt;its deployment will improve web performance of objects served from academic institutions and non-profit&lt;br/&gt;companies whose content is to date not hosted by commerical third-party content delivery companies.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">117738</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">117738</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n70" target="n878">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">The Columbia Hot Spot Rescue Service</data>
      <data key="e_abstract">Although Internet traffic is routinely quite heavy, there has usually been more than enough storage,&lt;br/&gt;processing and bandwidth capacity to provide acceptable performance. However, it is well known that, more&lt;br/&gt;and more frequently, demands for network resources are mushrooming locally into hotspots or data storms,&lt;br/&gt;and in such cases the affected web sites and subnetworks founder almost completely, creating revenue&lt;br/&gt;losses and client dissatisfaction on a large scale. We propose a novel collaborative technology to alleviate&lt;br/&gt;the effects of these hotspots, a technology that we will apply by designing and prototyping a Hotspot Rescue&lt;br/&gt;Service (HRS). This work will build on the past research of the PI&apos;s in networking, operating systems, and&lt;br/&gt;distributed caching.&lt;br/&gt; A key premise on which the technology is founded lies in the observation that existing Internet band-width&lt;br/&gt;resources are sufficient to deal effectively with hotspots. In other words, rescues of heavily-overloaded&lt;br/&gt;sites can be assembled from underutilized resources lying elsewhere. It follows that there is no inherent need&lt;br/&gt;for resources held in reserve uniquely for this purpose, i.e., there is no need for over-provisioned, under-utilized resources such as distributed caches to protect against hotspots. We propose instead a paradigm shift in which efficient mechanisms that we provide will enable communities of participating sites to share their resources to suppress hotspots. The service in action will be transparent, in part self-regulating and will take the form of automated traffic redirection to sites with available bandwidth.&lt;br/&gt; The proposed Columbia HRS will be proactive as well as reactive. We will amass hotspot data that&lt;br/&gt;will be modeled and analyzed with the aim of designing hotspot daemons or plugins, software devices for&lt;br/&gt;monitoring traffic behavior and signalling incipient hotspots via hotspot watches and advisories, along with&lt;br/&gt;relevant statistics. We will implement two complementary approaches to the technology, a server-based&lt;br/&gt;approach and a client-based, peer-to-peer (P2P) approach.&lt;br/&gt; In the server-based approach, servers monitor their own loads, the loads of a small set of servers that&lt;br/&gt;they would service in the event that the other server overloads, and, via probes between servers, network&lt;br/&gt;conditions. When a server or network component is identified as going into possible overload, the system&lt;br/&gt;activates a replication mechanism to duplicate the hot content. Clients can then retrieve the content from&lt;br/&gt;the server sites acting as replicas, alleviating the load on the original overloaded resource.&lt;br/&gt; In the P2P approach, clients install a plugin into their browser that communicates with similar plugins&lt;br/&gt;installed on other clients&apos; browsers, as well as with a distributed directory service. Clients cache their recent downloads, and, through the plugin, inform the directory service of the objects that are cached. The directory service can then identify the most popular content, as well as cached locations and notify additional clients of these alternative locations for download. By using client machines to store and deliver the hot content and if requests for the given content can be redirected to the client machines, the hotspot at the server can be eliminated. In this way, hotspot response becomes self-organizing and self-regulatory.&lt;br/&gt; There are several issues that need to be addressed as we develop this rescue service. First, we will use&lt;br/&gt;experimentation and analysis of collected data (including data sets obtained via a company partnerships)&lt;br/&gt;to develop models of causes and effects of network hotspot activity. Next, we will analytically evaluate&lt;br/&gt;the effect on server and network load that techniques such as caching, redirecting, and migrating have&lt;br/&gt;upon hotspots within the network. Last, we will implement and evaluate prototype systems to validate&lt;br/&gt;their effectiveness, either upon simulated hotspot activity within a testbed, or if possible, on actual hotspots&lt;br/&gt;through agreements with content providers.&lt;br/&gt; Development of the HRS will provide research opportunities to multiple students at Columbia, and&lt;br/&gt;its deployment will improve web performance of objects served from academic institutions and non-profit&lt;br/&gt;companies whose content is to date not hosted by commerical third-party content delivery companies.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">117738</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">117738</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n879" target="n880">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Development of Haptic Instrumentation and Software for Computer Science Research and Training Using Surgical Simulation as the Application</data>
      <data key="e_abstract">EIA-0116616&lt;br/&gt;Roger W. Webster&lt;br/&gt;Millersville University&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;MRI: Development of Haptic Instrumentation and Software for Computer Science Research and Training Using Surgical Simulation as the Application&lt;br/&gt;&lt;br/&gt;This is a proposal for instrumentation development under the Major Research Instrumentation (MRI) program to support research and student training in the area of haptic human-computer interaction using surgical simulation as the application. The design and development of haptic surgical instruments which attach to a commercially available device will enable users to practice virtual surgery and will enhance the human-computer interface in realistic simulation applications.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116616</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116616</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n879" target="n881">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Development of Haptic Instrumentation and Software for Computer Science Research and Training Using Surgical Simulation as the Application</data>
      <data key="e_abstract">EIA-0116616&lt;br/&gt;Roger W. Webster&lt;br/&gt;Millersville University&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;MRI: Development of Haptic Instrumentation and Software for Computer Science Research and Training Using Surgical Simulation as the Application&lt;br/&gt;&lt;br/&gt;This is a proposal for instrumentation development under the Major Research Instrumentation (MRI) program to support research and student training in the area of haptic human-computer interaction using surgical simulation as the application. The design and development of haptic surgical instruments which attach to a commercially available device will enable users to practice virtual surgery and will enhance the human-computer interface in realistic simulation applications.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116616</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116616</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n880" target="n881">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Development of Haptic Instrumentation and Software for Computer Science Research and Training Using Surgical Simulation as the Application</data>
      <data key="e_abstract">EIA-0116616&lt;br/&gt;Roger W. Webster&lt;br/&gt;Millersville University&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;MRI: Development of Haptic Instrumentation and Software for Computer Science Research and Training Using Surgical Simulation as the Application&lt;br/&gt;&lt;br/&gt;This is a proposal for instrumentation development under the Major Research Instrumentation (MRI) program to support research and student training in the area of haptic human-computer interaction using surgical simulation as the application. The design and development of haptic surgical instruments which attach to a commercially available device will enable users to practice virtual surgery and will enhance the human-computer interface in realistic simulation applications.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116616</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116616</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n882" target="n883">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">&quot;ITR/PE: Bridging the Digital Divide with Tangible and Ubiquitous Computing&quot;</data>
      <data key="e_abstract">The aim of this project is to put new and cutting-edge computational applications in the hands of high school students in after-school club settings in underserved communities. These tools will allow the students to build tangible and ubiquitous computing applications that will find genuine utility in the communities that surround the schools. For the first half of the project, we will be planning with our partner schools and making connections in the adjacent communities. We will also be crafting the initial technology platforms for the students. The final phase of the project will entail high school students working jointly with researchers to build community-centered applications. New insights will be gained on how to develop toolkits for young designers and, for the young designers, how to design and evaluate computational applications. This work will also suggest new ways to engage underserved communities in building and using the nation&apos;s information infrastructure.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112937</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112937</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n882" target="n884">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">&quot;ITR/PE: Bridging the Digital Divide with Tangible and Ubiquitous Computing&quot;</data>
      <data key="e_abstract">The aim of this project is to put new and cutting-edge computational applications in the hands of high school students in after-school club settings in underserved communities. These tools will allow the students to build tangible and ubiquitous computing applications that will find genuine utility in the communities that surround the schools. For the first half of the project, we will be planning with our partner schools and making connections in the adjacent communities. We will also be crafting the initial technology platforms for the students. The final phase of the project will entail high school students working jointly with researchers to build community-centered applications. New insights will be gained on how to develop toolkits for young designers and, for the young designers, how to design and evaluate computational applications. This work will also suggest new ways to engage underserved communities in building and using the nation&apos;s information infrastructure.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112937</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112937</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n883" target="n884">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">&quot;ITR/PE: Bridging the Digital Divide with Tangible and Ubiquitous Computing&quot;</data>
      <data key="e_abstract">The aim of this project is to put new and cutting-edge computational applications in the hands of high school students in after-school club settings in underserved communities. These tools will allow the students to build tangible and ubiquitous computing applications that will find genuine utility in the communities that surround the schools. For the first half of the project, we will be planning with our partner schools and making connections in the adjacent communities. We will also be crafting the initial technology platforms for the students. The final phase of the project will entail high school students working jointly with researchers to build community-centered applications. New insights will be gained on how to develop toolkits for young designers and, for the young designers, how to design and evaluate computational applications. This work will also suggest new ways to engage underserved communities in building and using the nation&apos;s information infrastructure.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112937</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112937</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n886" target="n887">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/AP: A Live-Data Simulation with Application to Bridge Performance</data>
      <data key="e_abstract">This project will develop a new generation of numerical simulation systems using advanced parallel computers, mathematical models, and real-time data. Modern sensor technology and the Internet have made it possible to monitor closely the performance of structures such as highway bridges. However, there are often limits on the possible number of embedded sensors, and accurate prediction the overall structural performance therefore requires other technologies, such as computer modeling, at the same time. This project will merge the two technologies, creating a computer modeling system that incorporates the live data in the numerical simulation.&lt;br/&gt;&lt;br/&gt;Using measured data as an integrated part of a numerical simulation is a challenging research project. Because data collected by the sensors must be moved continuously into the numerical simulation, the traditional paradigm of reading control parameters at the start of the computation is not possible. Instead, this project will use the ALICE memory snooper from Argonne National Laboratory to allow the constant interruption introduced by the transmission of the measured data. This will in turn allow a parallel computer to exchange information with remote sites without going through slow disk I/O. To fully integrate the sensor data with the computation, the project will develop new numerical schemes based on classical multigrid methods, but using the measured data to build the coarse space. The measured data will also be used as a basis for calibrating and validating the parameters in the mathematical model.&lt;br/&gt;&lt;br/&gt;The new simulation system will be used in a high cycle fatigue test of bridge decks, which will be conducted in the Structures Laboratory at the University of Colorado at Boulder. Field tests will also be scheduled on new bridges to be constructed with a variety of installed sensors. These tests are part of ongoing projects sponsored by other agencies. The synergy of these projects will help develop and validate the proposed simulation system.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112930</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">112930</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n889" target="n890">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Biodiversity &amp; Ecosystem Informatics - BDEI : Radar Remote Sensing of Habitat Structure for Biodiversity Informatics</data>
      <data key="e_abstract">EIA-0131281&lt;br/&gt;Bergen, Kathleen&lt;br/&gt;University of Michigan - Ann Arbor&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;BDEI: Radar Remote Sensing of Habitat Structure for Biodiversity Informatics&lt;br/&gt;&lt;br/&gt;Project Summary&lt;br/&gt;&lt;br/&gt;Mapping for biodiversity informatics requires information on habitat so that known species&lt;br/&gt;occurrences may be extrapolated to maps of potential species occurrences. That is, given the conversion&lt;br/&gt;of known specimen locations into maps, how can we best predict where additional individuals or&lt;br/&gt;populations of that species may occur? In the context of landscape ecology, landscape structure - its&lt;br/&gt;multi-dimensional components - is a primary basis for habitat preferences of bird species. Vegetation&lt;br/&gt;spatial composition, heterogeneity, variability, and scale are among the variables contributing to the&lt;br/&gt;horizontal structure, while vegetation height, layering, and biomass are examples of variables in the&lt;br/&gt;volumetric dimension. Combined together these variables, and others, describe the real multi-dimensional&lt;br/&gt;structure of habitat.</data>
      <data key="e_pgm">W100</data>
      <data key="e_label">131281</data>
      <data key="e_expirationDate">2003-11-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">131281</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n889" target="n891">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Biodiversity &amp; Ecosystem Informatics - BDEI : Radar Remote Sensing of Habitat Structure for Biodiversity Informatics</data>
      <data key="e_abstract">EIA-0131281&lt;br/&gt;Bergen, Kathleen&lt;br/&gt;University of Michigan - Ann Arbor&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;BDEI: Radar Remote Sensing of Habitat Structure for Biodiversity Informatics&lt;br/&gt;&lt;br/&gt;Project Summary&lt;br/&gt;&lt;br/&gt;Mapping for biodiversity informatics requires information on habitat so that known species&lt;br/&gt;occurrences may be extrapolated to maps of potential species occurrences. That is, given the conversion&lt;br/&gt;of known specimen locations into maps, how can we best predict where additional individuals or&lt;br/&gt;populations of that species may occur? In the context of landscape ecology, landscape structure - its&lt;br/&gt;multi-dimensional components - is a primary basis for habitat preferences of bird species. Vegetation&lt;br/&gt;spatial composition, heterogeneity, variability, and scale are among the variables contributing to the&lt;br/&gt;horizontal structure, while vegetation height, layering, and biomass are examples of variables in the&lt;br/&gt;volumetric dimension. Combined together these variables, and others, describe the real multi-dimensional&lt;br/&gt;structure of habitat.</data>
      <data key="e_pgm">W100</data>
      <data key="e_label">131281</data>
      <data key="e_expirationDate">2003-11-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">131281</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n890" target="n891">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Biodiversity &amp; Ecosystem Informatics - BDEI : Radar Remote Sensing of Habitat Structure for Biodiversity Informatics</data>
      <data key="e_abstract">EIA-0131281&lt;br/&gt;Bergen, Kathleen&lt;br/&gt;University of Michigan - Ann Arbor&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;BDEI: Radar Remote Sensing of Habitat Structure for Biodiversity Informatics&lt;br/&gt;&lt;br/&gt;Project Summary&lt;br/&gt;&lt;br/&gt;Mapping for biodiversity informatics requires information on habitat so that known species&lt;br/&gt;occurrences may be extrapolated to maps of potential species occurrences. That is, given the conversion&lt;br/&gt;of known specimen locations into maps, how can we best predict where additional individuals or&lt;br/&gt;populations of that species may occur? In the context of landscape ecology, landscape structure - its&lt;br/&gt;multi-dimensional components - is a primary basis for habitat preferences of bird species. Vegetation&lt;br/&gt;spatial composition, heterogeneity, variability, and scale are among the variables contributing to the&lt;br/&gt;horizontal structure, while vegetation height, layering, and biomass are examples of variables in the&lt;br/&gt;volumetric dimension. Combined together these variables, and others, describe the real multi-dimensional&lt;br/&gt;structure of habitat.</data>
      <data key="e_pgm">W100</data>
      <data key="e_label">131281</data>
      <data key="e_expirationDate">2003-11-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">131281</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n892" target="n893">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR-SY: System Hardening through Security Aware Compilation and Processor Architecture</data>
      <data key="e_abstract">Networked computer systems are vulnerable to malicious attack. These attacks&lt;br/&gt;try to take over the control of a victim computer system by re-pointing the processor program counter (PC) to the attacker&apos;s code. This proposal explores a role for security aware compilation and processor microarchitecture in preventing unauthorized PC modifications. The two most common instances of PC compromise arise from the corruption of (1) the return address in an activation record and (2) function pointers. The basic approach to guarding PC is to apply an encoding function before any potential PC value (such as return address, or function pointer table entry) is stored in any memory location (such as a stack frame or function pointer table in the data or heap space). &lt;br/&gt;&lt;br/&gt;Any read of a memory value into the PC first has to go through a decoding function. A compromised PC value would go only through the decoding function and hence would render the malicious attack ineffective. This research investigates several variations of PC encoding/decoding schemes and evaluates computational overhead of these schemes and their effectiveness. This research plans to build a hardened Linux system, gcc compiler and other public domain utilities such as Apache web server incorporating the proposed return address and function pointer protection schemes.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113409</data>
      <data key="e_expirationDate">2002-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113409</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n894" target="n895">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">VISUALIZATION: Integrated Compression and Out-of-Core Techniques for Large Time-Varying Data Visualization</data>
      <data key="e_abstract">Scientific Visualization is fast becoming a key technology that provides scientists with insights that enable them to steer their numerical simulations towards solving previously unsolvable problems. However, the size of scientific datasets has witnessed exponential growth in the past few years. This sheer size often makes interactive exploration impossible, as only a small portion of data can fit into main memory at a time and the computation cost is often too high to run in real-time. Despite the importance of time-varying datasets, most previous research has focused on the visualization of steady-state data (i.e., data with only a single time step). This project will attack the challenges of large input sizes posed by time-varying data visualization. There are two important and promising research directions towards handling large-scale problems: data compression techniques and out-of-core techniques. This project will develop integrated lossless compression and out-of-core techniques for large time-varying data visualization, including isosurface extraction and direct volume rendering. It will mainly consider the class of irregular-grid volume datasets represented as tetrahedral meshes, which often arises in computational fluid dynamics, partial differential equation solvers, and other fields.&lt;br/&gt;&lt;br/&gt;Specifically, the project will develop new lossless compression techniques for vertex coordinates and scalar values for tetrahedral time-varying volume data. It will also develop new out-of-core isosurface extraction and direct volume rendering techniques for tetrahedral time-varying volume data, and integrate the compression and out-of-core visualization techniques together under a unified infrastructure. The expected results would be a collection of new techniques and a unified, proof-of-the-concept visualization system that will minimize the disk space requirement and the visualization rendering time cost. If successful, the system will efficiently support full visualization functionalities (isosurface extraction and volume rendering) for time-varying datasets much larger than can fit in main memory, with performance expected to be independent of the main memory size available.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">118915</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">118915</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n894" target="n896">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">VISUALIZATION: Integrated Compression and Out-of-Core Techniques for Large Time-Varying Data Visualization</data>
      <data key="e_abstract">Scientific Visualization is fast becoming a key technology that provides scientists with insights that enable them to steer their numerical simulations towards solving previously unsolvable problems. However, the size of scientific datasets has witnessed exponential growth in the past few years. This sheer size often makes interactive exploration impossible, as only a small portion of data can fit into main memory at a time and the computation cost is often too high to run in real-time. Despite the importance of time-varying datasets, most previous research has focused on the visualization of steady-state data (i.e., data with only a single time step). This project will attack the challenges of large input sizes posed by time-varying data visualization. There are two important and promising research directions towards handling large-scale problems: data compression techniques and out-of-core techniques. This project will develop integrated lossless compression and out-of-core techniques for large time-varying data visualization, including isosurface extraction and direct volume rendering. It will mainly consider the class of irregular-grid volume datasets represented as tetrahedral meshes, which often arises in computational fluid dynamics, partial differential equation solvers, and other fields.&lt;br/&gt;&lt;br/&gt;Specifically, the project will develop new lossless compression techniques for vertex coordinates and scalar values for tetrahedral time-varying volume data. It will also develop new out-of-core isosurface extraction and direct volume rendering techniques for tetrahedral time-varying volume data, and integrate the compression and out-of-core visualization techniques together under a unified infrastructure. The expected results would be a collection of new techniques and a unified, proof-of-the-concept visualization system that will minimize the disk space requirement and the visualization rendering time cost. If successful, the system will efficiently support full visualization functionalities (isosurface extraction and volume rendering) for time-varying datasets much larger than can fit in main memory, with performance expected to be independent of the main memory size available.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">118915</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">118915</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n895" target="n896">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">VISUALIZATION: Integrated Compression and Out-of-Core Techniques for Large Time-Varying Data Visualization</data>
      <data key="e_abstract">Scientific Visualization is fast becoming a key technology that provides scientists with insights that enable them to steer their numerical simulations towards solving previously unsolvable problems. However, the size of scientific datasets has witnessed exponential growth in the past few years. This sheer size often makes interactive exploration impossible, as only a small portion of data can fit into main memory at a time and the computation cost is often too high to run in real-time. Despite the importance of time-varying datasets, most previous research has focused on the visualization of steady-state data (i.e., data with only a single time step). This project will attack the challenges of large input sizes posed by time-varying data visualization. There are two important and promising research directions towards handling large-scale problems: data compression techniques and out-of-core techniques. This project will develop integrated lossless compression and out-of-core techniques for large time-varying data visualization, including isosurface extraction and direct volume rendering. It will mainly consider the class of irregular-grid volume datasets represented as tetrahedral meshes, which often arises in computational fluid dynamics, partial differential equation solvers, and other fields.&lt;br/&gt;&lt;br/&gt;Specifically, the project will develop new lossless compression techniques for vertex coordinates and scalar values for tetrahedral time-varying volume data. It will also develop new out-of-core isosurface extraction and direct volume rendering techniques for tetrahedral time-varying volume data, and integrate the compression and out-of-core visualization techniques together under a unified infrastructure. The expected results would be a collection of new techniques and a unified, proof-of-the-concept visualization system that will minimize the disk space requirement and the visualization rendering time cost. If successful, the system will efficiently support full visualization functionalities (isosurface extraction and volume rendering) for time-varying datasets much larger than can fit in main memory, with performance expected to be independent of the main memory size available.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">118915</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">118915</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n287" target="n684">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: A State-of-the-Art Immersive Display for Research in Rendering, Animation and Simulation, and Cognitive Human-Computer Interface Design</data>
      <data key="e_abstract">EIA-0130800 &lt;br/&gt;David Luebke&lt;br/&gt;University of Virginia&lt;br/&gt;&lt;br/&gt;This project will construct a state-of-art immersive display at the University of Virginia. Specifically, the investigators will build a wide field-of-view tiled display, using passive stereo projection, 6-DOF head tracking, and spatialized audio to create an extremely immersive 3-D audio-visual display environment. This display will benefit and enable three separate research projects in the Computer Science and Psychology departments: gaze-directed rendering, perceptually driven physical simulation and animation, and cognitive design of human-computer interfaces. Tiling multiple projectors will create a display spanning a very wide field of view; two projectors per tile will enable passive stereo display with lightweight polarizing glasses. An audio system, a head tracker, and realistic scanned 3-D environments will enable immersive and convincing virtual worlds. The wide field-of-view will provide an ideal testbed for gaze-directed rendering, which accelerates interactive rendering by exploiting reduced visual acuity (e.g., for peripheral or fast-moving objects), and for perceptually driven physical simulation, which selectively degrades simulation accuracy according to perceptual metrics. The human-computer interface project investigates immersive ambient context to improve human memory. The stereo head-tracked capabilities of the new display will literally add a new dimension to the investigation, enabling full 3-D environmental cues.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130800</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130800</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n684" target="n899">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: A State-of-the-Art Immersive Display for Research in Rendering, Animation and Simulation, and Cognitive Human-Computer Interface Design</data>
      <data key="e_abstract">EIA-0130800 &lt;br/&gt;David Luebke&lt;br/&gt;University of Virginia&lt;br/&gt;&lt;br/&gt;This project will construct a state-of-art immersive display at the University of Virginia. Specifically, the investigators will build a wide field-of-view tiled display, using passive stereo projection, 6-DOF head tracking, and spatialized audio to create an extremely immersive 3-D audio-visual display environment. This display will benefit and enable three separate research projects in the Computer Science and Psychology departments: gaze-directed rendering, perceptually driven physical simulation and animation, and cognitive design of human-computer interfaces. Tiling multiple projectors will create a display spanning a very wide field of view; two projectors per tile will enable passive stereo display with lightweight polarizing glasses. An audio system, a head tracker, and realistic scanned 3-D environments will enable immersive and convincing virtual worlds. The wide field-of-view will provide an ideal testbed for gaze-directed rendering, which accelerates interactive rendering by exploiting reduced visual acuity (e.g., for peripheral or fast-moving objects), and for perceptually driven physical simulation, which selectively degrades simulation accuracy according to perceptual metrics. The human-computer interface project investigates immersive ambient context to improve human memory. The stereo head-tracked capabilities of the new display will literally add a new dimension to the investigation, enabling full 3-D environmental cues.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130800</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130800</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n287" target="n899">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: A State-of-the-Art Immersive Display for Research in Rendering, Animation and Simulation, and Cognitive Human-Computer Interface Design</data>
      <data key="e_abstract">EIA-0130800 &lt;br/&gt;David Luebke&lt;br/&gt;University of Virginia&lt;br/&gt;&lt;br/&gt;This project will construct a state-of-art immersive display at the University of Virginia. Specifically, the investigators will build a wide field-of-view tiled display, using passive stereo projection, 6-DOF head tracking, and spatialized audio to create an extremely immersive 3-D audio-visual display environment. This display will benefit and enable three separate research projects in the Computer Science and Psychology departments: gaze-directed rendering, perceptually driven physical simulation and animation, and cognitive design of human-computer interfaces. Tiling multiple projectors will create a display spanning a very wide field of view; two projectors per tile will enable passive stereo display with lightweight polarizing glasses. An audio system, a head tracker, and realistic scanned 3-D environments will enable immersive and convincing virtual worlds. The wide field-of-view will provide an ideal testbed for gaze-directed rendering, which accelerates interactive rendering by exploiting reduced visual acuity (e.g., for peripheral or fast-moving objects), and for perceptually driven physical simulation, which selectively degrades simulation accuracy according to perceptual metrics. The human-computer interface project investigates immersive ambient context to improve human memory. The stereo head-tracked capabilities of the new display will literally add a new dimension to the investigation, enabling full 3-D environmental cues.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130800</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130800</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n134" target="n901">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Experimental Testbed for Mobile Network Protocols</data>
      <data key="e_abstract">EIA-0130799 &lt;br/&gt;Teresa Dahlberg&lt;br/&gt;University of North Carolina Charlotte&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Experimental Testbed for Mobile Network Protocols&lt;br/&gt;&lt;br/&gt;The overall objective of this project is to experimentally analyze mobile network protocols that support multimedia services. A wireless, mobile multimedia network will be built to add an experimental component to four ongoing research projects at UNC Charlotte. The experimental work will focus on the component of each project that involves development and analysis of mobile network protocols. Experimentation will enable critical analysis of protocol behavior in dynamic environments where real-world entities replace simulation models, especially, network traffic models, wireless channel models, fault and vulnerability models, and power usage models. The testbed will encompass both cellular and ad hoc network architectures with components that include PCs and laptops with IEEE 802.11 radios and FreeBSD operating system. Network nodes to be configured include multimedia nodes that generate variable bit rate streaming audio and video and a security authentication node. The testbed will support individual research activities as well as facilitate synergy among the researchers who possess expertise in the areas of multimedia, security, ad hoc networking, and cellular networking. The outcome of the experimental studies will contribute to the limited body of knowledge of mobile network protocol behavior within highly dynamic environments.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130799</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130799</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n134" target="n902">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Experimental Testbed for Mobile Network Protocols</data>
      <data key="e_abstract">EIA-0130799 &lt;br/&gt;Teresa Dahlberg&lt;br/&gt;University of North Carolina Charlotte&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Experimental Testbed for Mobile Network Protocols&lt;br/&gt;&lt;br/&gt;The overall objective of this project is to experimentally analyze mobile network protocols that support multimedia services. A wireless, mobile multimedia network will be built to add an experimental component to four ongoing research projects at UNC Charlotte. The experimental work will focus on the component of each project that involves development and analysis of mobile network protocols. Experimentation will enable critical analysis of protocol behavior in dynamic environments where real-world entities replace simulation models, especially, network traffic models, wireless channel models, fault and vulnerability models, and power usage models. The testbed will encompass both cellular and ad hoc network architectures with components that include PCs and laptops with IEEE 802.11 radios and FreeBSD operating system. Network nodes to be configured include multimedia nodes that generate variable bit rate streaming audio and video and a security authentication node. The testbed will support individual research activities as well as facilitate synergy among the researchers who possess expertise in the areas of multimedia, security, ad hoc networking, and cellular networking. The outcome of the experimental studies will contribute to the limited body of knowledge of mobile network protocol behavior within highly dynamic environments.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130799</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130799</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n134" target="n903">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Experimental Testbed for Mobile Network Protocols</data>
      <data key="e_abstract">EIA-0130799 &lt;br/&gt;Teresa Dahlberg&lt;br/&gt;University of North Carolina Charlotte&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Experimental Testbed for Mobile Network Protocols&lt;br/&gt;&lt;br/&gt;The overall objective of this project is to experimentally analyze mobile network protocols that support multimedia services. A wireless, mobile multimedia network will be built to add an experimental component to four ongoing research projects at UNC Charlotte. The experimental work will focus on the component of each project that involves development and analysis of mobile network protocols. Experimentation will enable critical analysis of protocol behavior in dynamic environments where real-world entities replace simulation models, especially, network traffic models, wireless channel models, fault and vulnerability models, and power usage models. The testbed will encompass both cellular and ad hoc network architectures with components that include PCs and laptops with IEEE 802.11 radios and FreeBSD operating system. Network nodes to be configured include multimedia nodes that generate variable bit rate streaming audio and video and a security authentication node. The testbed will support individual research activities as well as facilitate synergy among the researchers who possess expertise in the areas of multimedia, security, ad hoc networking, and cellular networking. The outcome of the experimental studies will contribute to the limited body of knowledge of mobile network protocol behavior within highly dynamic environments.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130799</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130799</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n901" target="n902">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Experimental Testbed for Mobile Network Protocols</data>
      <data key="e_abstract">EIA-0130799 &lt;br/&gt;Teresa Dahlberg&lt;br/&gt;University of North Carolina Charlotte&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Experimental Testbed for Mobile Network Protocols&lt;br/&gt;&lt;br/&gt;The overall objective of this project is to experimentally analyze mobile network protocols that support multimedia services. A wireless, mobile multimedia network will be built to add an experimental component to four ongoing research projects at UNC Charlotte. The experimental work will focus on the component of each project that involves development and analysis of mobile network protocols. Experimentation will enable critical analysis of protocol behavior in dynamic environments where real-world entities replace simulation models, especially, network traffic models, wireless channel models, fault and vulnerability models, and power usage models. The testbed will encompass both cellular and ad hoc network architectures with components that include PCs and laptops with IEEE 802.11 radios and FreeBSD operating system. Network nodes to be configured include multimedia nodes that generate variable bit rate streaming audio and video and a security authentication node. The testbed will support individual research activities as well as facilitate synergy among the researchers who possess expertise in the areas of multimedia, security, ad hoc networking, and cellular networking. The outcome of the experimental studies will contribute to the limited body of knowledge of mobile network protocol behavior within highly dynamic environments.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130799</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130799</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n901" target="n903">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Experimental Testbed for Mobile Network Protocols</data>
      <data key="e_abstract">EIA-0130799 &lt;br/&gt;Teresa Dahlberg&lt;br/&gt;University of North Carolina Charlotte&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Experimental Testbed for Mobile Network Protocols&lt;br/&gt;&lt;br/&gt;The overall objective of this project is to experimentally analyze mobile network protocols that support multimedia services. A wireless, mobile multimedia network will be built to add an experimental component to four ongoing research projects at UNC Charlotte. The experimental work will focus on the component of each project that involves development and analysis of mobile network protocols. Experimentation will enable critical analysis of protocol behavior in dynamic environments where real-world entities replace simulation models, especially, network traffic models, wireless channel models, fault and vulnerability models, and power usage models. The testbed will encompass both cellular and ad hoc network architectures with components that include PCs and laptops with IEEE 802.11 radios and FreeBSD operating system. Network nodes to be configured include multimedia nodes that generate variable bit rate streaming audio and video and a security authentication node. The testbed will support individual research activities as well as facilitate synergy among the researchers who possess expertise in the areas of multimedia, security, ad hoc networking, and cellular networking. The outcome of the experimental studies will contribute to the limited body of knowledge of mobile network protocol behavior within highly dynamic environments.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130799</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130799</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n902" target="n903">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Experimental Testbed for Mobile Network Protocols</data>
      <data key="e_abstract">EIA-0130799 &lt;br/&gt;Teresa Dahlberg&lt;br/&gt;University of North Carolina Charlotte&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Experimental Testbed for Mobile Network Protocols&lt;br/&gt;&lt;br/&gt;The overall objective of this project is to experimentally analyze mobile network protocols that support multimedia services. A wireless, mobile multimedia network will be built to add an experimental component to four ongoing research projects at UNC Charlotte. The experimental work will focus on the component of each project that involves development and analysis of mobile network protocols. Experimentation will enable critical analysis of protocol behavior in dynamic environments where real-world entities replace simulation models, especially, network traffic models, wireless channel models, fault and vulnerability models, and power usage models. The testbed will encompass both cellular and ad hoc network architectures with components that include PCs and laptops with IEEE 802.11 radios and FreeBSD operating system. Network nodes to be configured include multimedia nodes that generate variable bit rate streaming audio and video and a security authentication node. The testbed will support individual research activities as well as facilitate synergy among the researchers who possess expertise in the areas of multimedia, security, ad hoc networking, and cellular networking. The outcome of the experimental studies will contribute to the limited body of knowledge of mobile network protocol behavior within highly dynamic environments.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130799</data>
      <data key="e_expirationDate">2005-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130799</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n904" target="n905">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Purpose-Driven Natural Language Processing</data>
      <data key="e_abstract">EIA-0130798 &lt;br/&gt;Miroslav Martinovic&lt;br/&gt;The College of New Jersey&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Purpose-Driven Natural Language Processing&lt;br/&gt;&lt;br/&gt;The unifying theme of the three projects (WHAT, QASTIIR, and Hopewell) is the integration of methodologies of computational linguistics with statistical techniques and their cooperative application to question answering and information retrieval systems. QASTIIR (Question Answering System Through Intelligent Information Retrieval) considers where and when techniques of computational linguistics could best improve performance of a hybrid statistical/linguistic question answering system. WHAT (Web Host Access Tool) addresses client-side personalization of web search queries, exploiting semantics to disambiguate keyword meaning. The Hopewell project automates K-12 curriculum mapping (essentially a digital library of curriculum resources achieved through teacher consensus). NLP/IR techniques should reduce the need for middle layer expertise to manipulate map databases. All three projects address how to disambiguate logical and set expression queries. QASTIIR takes the most theoretical approach, developing theory through established protocols for testing. The WHAT domain provides a highly ambiguous real-world domain using well established databases. Hopewell project provides a second real-world domain whose semantics are more constrained, but that requires a database design responsive to novice querying. Anticipated outcomes are (1) new theory development in natural language processing, (2) highly private user profile development, (3) application of NLP technology to digital library development and retrieval.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130798</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130798</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n907" target="n908">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: Performance Modeling and Programming Environments for PetaFlop Computers and the Blue Gene Machine</data>
      <data key="e_abstract">EIA-0103645&lt;br/&gt;Laxmikant V. Kale&lt;br/&gt;University of Illinois&lt;br/&gt;&lt;br/&gt;Performance Modeling and Programming Environments for PetaFlop Computers and the Blue Gene Machine&lt;br/&gt;&lt;br/&gt;The objective of the proposal is to develop performance simulation capabilities to allow system level analysis and prediction of performance of the next generation complex PetaFlop machines that include multiple levels of memory hierarchy and interconnects. The performance simulator that will be developed will be used to test parallel data structures and algorithms implemented in programming environments used in these machines, as well as frameworks to enable the development of applications for these machine classes. A number of important applications will be used to test and validate the CS technology advances.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103645</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103645</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n907" target="n909">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: Performance Modeling and Programming Environments for PetaFlop Computers and the Blue Gene Machine</data>
      <data key="e_abstract">EIA-0103645&lt;br/&gt;Laxmikant V. Kale&lt;br/&gt;University of Illinois&lt;br/&gt;&lt;br/&gt;Performance Modeling and Programming Environments for PetaFlop Computers and the Blue Gene Machine&lt;br/&gt;&lt;br/&gt;The objective of the proposal is to develop performance simulation capabilities to allow system level analysis and prediction of performance of the next generation complex PetaFlop machines that include multiple levels of memory hierarchy and interconnects. The performance simulator that will be developed will be used to test parallel data structures and algorithms implemented in programming environments used in these machines, as well as frameworks to enable the development of applications for these machine classes. A number of important applications will be used to test and validate the CS technology advances.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103645</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103645</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n907" target="n910">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: Performance Modeling and Programming Environments for PetaFlop Computers and the Blue Gene Machine</data>
      <data key="e_abstract">EIA-0103645&lt;br/&gt;Laxmikant V. Kale&lt;br/&gt;University of Illinois&lt;br/&gt;&lt;br/&gt;Performance Modeling and Programming Environments for PetaFlop Computers and the Blue Gene Machine&lt;br/&gt;&lt;br/&gt;The objective of the proposal is to develop performance simulation capabilities to allow system level analysis and prediction of performance of the next generation complex PetaFlop machines that include multiple levels of memory hierarchy and interconnects. The performance simulator that will be developed will be used to test parallel data structures and algorithms implemented in programming environments used in these machines, as well as frameworks to enable the development of applications for these machine classes. A number of important applications will be used to test and validate the CS technology advances.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103645</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103645</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n908" target="n909">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: Performance Modeling and Programming Environments for PetaFlop Computers and the Blue Gene Machine</data>
      <data key="e_abstract">EIA-0103645&lt;br/&gt;Laxmikant V. Kale&lt;br/&gt;University of Illinois&lt;br/&gt;&lt;br/&gt;Performance Modeling and Programming Environments for PetaFlop Computers and the Blue Gene Machine&lt;br/&gt;&lt;br/&gt;The objective of the proposal is to develop performance simulation capabilities to allow system level analysis and prediction of performance of the next generation complex PetaFlop machines that include multiple levels of memory hierarchy and interconnects. The performance simulator that will be developed will be used to test parallel data structures and algorithms implemented in programming environments used in these machines, as well as frameworks to enable the development of applications for these machine classes. A number of important applications will be used to test and validate the CS technology advances.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103645</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103645</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n908" target="n910">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: Performance Modeling and Programming Environments for PetaFlop Computers and the Blue Gene Machine</data>
      <data key="e_abstract">EIA-0103645&lt;br/&gt;Laxmikant V. Kale&lt;br/&gt;University of Illinois&lt;br/&gt;&lt;br/&gt;Performance Modeling and Programming Environments for PetaFlop Computers and the Blue Gene Machine&lt;br/&gt;&lt;br/&gt;The objective of the proposal is to develop performance simulation capabilities to allow system level analysis and prediction of performance of the next generation complex PetaFlop machines that include multiple levels of memory hierarchy and interconnects. The performance simulator that will be developed will be used to test parallel data structures and algorithms implemented in programming environments used in these machines, as well as frameworks to enable the development of applications for these machine classes. A number of important applications will be used to test and validate the CS technology advances.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103645</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103645</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n909" target="n910">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: Performance Modeling and Programming Environments for PetaFlop Computers and the Blue Gene Machine</data>
      <data key="e_abstract">EIA-0103645&lt;br/&gt;Laxmikant V. Kale&lt;br/&gt;University of Illinois&lt;br/&gt;&lt;br/&gt;Performance Modeling and Programming Environments for PetaFlop Computers and the Blue Gene Machine&lt;br/&gt;&lt;br/&gt;The objective of the proposal is to develop performance simulation capabilities to allow system level analysis and prediction of performance of the next generation complex PetaFlop machines that include multiple levels of memory hierarchy and interconnects. The performance simulator that will be developed will be used to test parallel data structures and algorithms implemented in programming environments used in these machines, as well as frameworks to enable the development of applications for these machine classes. A number of important applications will be used to test and validate the CS technology advances.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103645</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103645</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n912" target="n913">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: Cache Efficient and Parallel Householder Bidiagonalization</data>
      <data key="e_abstract">EIA-0103642&lt;br/&gt;Gary W. Howell&lt;br/&gt;Florida Institute of Technology&lt;br/&gt;&lt;br/&gt;CISE NGS: Cache Efficient and Parallel Bidiagonalization&lt;br/&gt;&lt;br/&gt;Cache efficient reduction to bidiagonal form allows a significant speedup in computation of the singular value decomposition (SVD) for rectangular matrices. The SVD provides the most standard and stable means of solving least square problems and of providing low rank approximations (allowing compressions of data stored in matrix form). As such it is used constantly by researchers around the nation and the world.&lt;br/&gt;&lt;br/&gt;This proposal will result in speedups in the standard LAPACK implementation and also in the parallel computation package SCALAPACK. The work will also extend to the parallel case with a view for inclusion in SCALPACK. In the parallel case the optimizer must take into account not only details of the local computer architecture and memory hierarchy but also communication among the processors.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">103642</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">103642</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n417" target="n915">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP(CISE): Capturing and Modeling Physics from Images</data>
      <data key="e_abstract">The objective of this project is to capture physical properties of real world phenomenon by analyzing photographs and video. The problem of capturing motion of objects has emerged as a major research area in the fields of computer vision and computer graphics with a wide range of possible applications. This project will attempt to devise new computer vision techniques for capturing and modeling the dynamics and physical characteristics of a number of differing objects in different contexts. If successful, the new techniques will enable more accurate 3D motion estimation from a single video stream and other limited sensor data. Finally, the project will explore potentially valuable application areas for these research products.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113007</data>
      <data key="e_expirationDate">2005-11-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">113007</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n916" target="n917">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Cable Suspended Robots: Coordination, Control and Configuration Design</data>
      <data key="e_abstract">The project addresses an important, and somewhat neglected so far, topic in robotics - long reach robots. Cable-based robotic devices will be explored, covering questions of configuration design, trajectory planning, feedback control, fine motion control, and metrics for intelligent design. Experimental validation will be performed on an eight degrees-of-freedom cable robot built in the PI&apos;s laboratory. The PI has adequate access to outside resources critical for the project.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">117733</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">117733</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n921" target="n922">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Scalable Performance Models for Large Scale Networks with Correlated Traffic</data>
      <data key="e_abstract">It is well established that long-range autocorrelation and heavy-tailed inter-packet time distributions are dominant characteristics of modern multimedia high speed network traffic, determining limits to performance and radically influencing policies for management. Incorporating such characteristics in parametric performance models has so far been difficult. This project will center about the development of powerful and efficient models to capture network behavior for varying levels of abstraction while incorporating approximations to long-range dependence and heavy tails. The model does not solely rely on steady state results, but rather on a finite horizon that can be used for online monitoring and early detection of rare events. In particular, this means that time dependent threads can be modeled which are the root cause of such dependency related problems. Decomposability techniques will be used to decrease the granularity if possible and to scale the model to time scales suitable for online analysis and control. The approach will be to extend the successful and innovative model developed for a high-speed network. Highly correlated arrival processes can be viewed as nearly-completely decomposable processes, which in turn impose a nearly-completely decomposable structure on the system as a whole. Each of these nearly decomposed systems can be solved separately and in parallel and their solutions combined to accurately evaluate the performance of the system. Theoretical work must be done to determine error bounds and robust algorithms for solutions in a wide range of applications, to allow flexible approximation of measured correlations, and to broaden the classes of results that can be computed. Additionally, experimental work must be done to exercise models and compare them with published and other artifactual data. Finally, a prototype software package will be developed that demonstrates the viability of placing this tool in the hands of practicing network engineers and equipment designers.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">106640</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">106640</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n921" target="n923">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Scalable Performance Models for Large Scale Networks with Correlated Traffic</data>
      <data key="e_abstract">It is well established that long-range autocorrelation and heavy-tailed inter-packet time distributions are dominant characteristics of modern multimedia high speed network traffic, determining limits to performance and radically influencing policies for management. Incorporating such characteristics in parametric performance models has so far been difficult. This project will center about the development of powerful and efficient models to capture network behavior for varying levels of abstraction while incorporating approximations to long-range dependence and heavy tails. The model does not solely rely on steady state results, but rather on a finite horizon that can be used for online monitoring and early detection of rare events. In particular, this means that time dependent threads can be modeled which are the root cause of such dependency related problems. Decomposability techniques will be used to decrease the granularity if possible and to scale the model to time scales suitable for online analysis and control. The approach will be to extend the successful and innovative model developed for a high-speed network. Highly correlated arrival processes can be viewed as nearly-completely decomposable processes, which in turn impose a nearly-completely decomposable structure on the system as a whole. Each of these nearly decomposed systems can be solved separately and in parallel and their solutions combined to accurately evaluate the performance of the system. Theoretical work must be done to determine error bounds and robust algorithms for solutions in a wide range of applications, to allow flexible approximation of measured correlations, and to broaden the classes of results that can be computed. Additionally, experimental work must be done to exercise models and compare them with published and other artifactual data. Finally, a prototype software package will be developed that demonstrates the viability of placing this tool in the hands of practicing network engineers and equipment designers.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">106640</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">106640</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n922" target="n923">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Scalable Performance Models for Large Scale Networks with Correlated Traffic</data>
      <data key="e_abstract">It is well established that long-range autocorrelation and heavy-tailed inter-packet time distributions are dominant characteristics of modern multimedia high speed network traffic, determining limits to performance and radically influencing policies for management. Incorporating such characteristics in parametric performance models has so far been difficult. This project will center about the development of powerful and efficient models to capture network behavior for varying levels of abstraction while incorporating approximations to long-range dependence and heavy tails. The model does not solely rely on steady state results, but rather on a finite horizon that can be used for online monitoring and early detection of rare events. In particular, this means that time dependent threads can be modeled which are the root cause of such dependency related problems. Decomposability techniques will be used to decrease the granularity if possible and to scale the model to time scales suitable for online analysis and control. The approach will be to extend the successful and innovative model developed for a high-speed network. Highly correlated arrival processes can be viewed as nearly-completely decomposable processes, which in turn impose a nearly-completely decomposable structure on the system as a whole. Each of these nearly decomposed systems can be solved separately and in parallel and their solutions combined to accurately evaluate the performance of the system. Theoretical work must be done to determine error bounds and robust algorithms for solutions in a wide range of applications, to allow flexible approximation of measured correlations, and to broaden the classes of results that can be computed. Additionally, experimental work must be done to exercise models and compare them with published and other artifactual data. Finally, a prototype software package will be developed that demonstrates the viability of placing this tool in the hands of practicing network engineers and equipment designers.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">106640</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">106640</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n929" target="n930">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/AP: Realistic Uncertainty Bounds for Complex Dynamic Models</data>
      <data key="e_abstract">The present problems facing society-such as global warming, earthquake preparedness, safety of transport of nuclear waste, and pollutant emission from automobile engines-call for integration of a variety of computer programs, each solving numerical problems in a different discipline. The overriding concern for such complex models is their reliability: predictability, authenticity, and uncertainty. The focus of the present effort is modeling realistic uncertainty in predictions from multi-response, large-scale, nonlinear dynamic models, using a new strategy to attack this problem.&lt;br/&gt;This work re-examines the concept of a mathematical model associated with complex physical systems, considering experiment and theory to be an integral part of the model and treating uncertain parameters of the model as internal, &quot;state&quot; variables. In this way, the uncertainties of the experimental and theoretical foundation are transferred &quot;directly&quot; into uncertainties of model predictions. Establishing this direct relationship allows one also to address the reverse problem: to identify which specific data contribute the most to the prediction uncertainty; to determine the required accuracy of an experiment to bring the prediction uncertainty to a given level; or to assess whether a planned experiment will be able to improve the prediction uncertainty.&lt;br/&gt;This is accomplished by merging convex relaxations from control theory with the technique of solution mapping developed and applied to numerical modeling of chemical kinetics typical of fossil-fuel combustion. The solution mapping technique uses statistical design of computer experiments to replace complex ODE models with surrogate polynomial models. These simpler, though accurate, algebraic models are more suited to numerical optimization. The convex relaxations allow for optimization problems described by a polynomial objective and polynomial constraints (generally nonconvex) to be attacked by convex optimization, namely linear objectives with linear-matrix-inequality constraints. Nearly twenty years of use in robust control has shown these relaxations to be remarkably useful in a wide variety of physically motivated problems and applications.&lt;br/&gt;In this approach, surrogate models are developed for all responses, both from the training set and from the prediction set. Each surrogate model is expressed as a quadratic form in terms of internal model parameters, developed in a series of direct ODE integrations performed according to a factorial design covering a subspace of parameter uncertainties. The quadratic form of the surrogate response models is then explored by an optimization algorithm. In the initial effort, the problem of propagation of uncertainties in a natural-gas-combustion model is cast in the form amenable for the S-procedure, a method of convex optimization widely used in control theory. &lt;br/&gt;Even more sophisticated convex relaxations have recently been developed, centering on the observation that determining if a given polynomial is a sum-of-squares (and hence globally nonnegative), can be cast as a convex feasibility problem, and verified in polynomial-time (in the order of the polynomial). This work will investigate such possibilities for exploring novel avenues for numerically economical assessment of realistic error bounds of complex dynamic models.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">113985</data>
      <data key="e_expirationDate">2006-06-30</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">113985</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n932" target="n933">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: NSF QuBIC: Modeling and Manufacture of Huge DNA Oligonucleotide Libraries for Computation</data>
      <data key="e_abstract">EIA-0130385&lt;br/&gt;Russell J. Deaton&lt;br/&gt;University of Arkansas&lt;br/&gt;&lt;br/&gt;Title: Modeling and Manufacture of Huge DNA Oligonucleotide Libraries for Computation&lt;br/&gt;&lt;br/&gt;Computing with DNA, with its advantages of massive parallelism and huge information density, promises a number of revolutionary applications, as well as the potential to solve problems beyond the capabilities of conventional computers. A critical barrier, however, is unplanned crosshybridization among oligonucleotides. In order for the computations to be reliable and efficient, and to scale to larger problems, the DNA sequences have to be designed to minimize these unplanned crosshybridizations. Though pairwise hybridization is well modeled and understood, design of such libraries is challenging because of the huge number of pairwise hybridization&apos;s, and the conflicting constraints of maximizing the library size while minimizing crosshybridization.Therefore, to overcome these limitations, huge libraries of non-crosshybridizing DNA oligonucleotides are manufactured by in vitro evolution with a PCR-based protocol that selects from a random pool those oligonucleotides that are maximally mismatched. In addition, because enumeration of all pairwise hybridization energetic in a huge library is computationally prohibitive, a statistical approach, which is based upon spin glass physics, is used to model the library. The model is the basis for a set of analysis and design tools for application to the libraries.&lt;br/&gt;&lt;br/&gt;Because of the fundamental importance of DNA hybridization in DNA computing, the modeling and manufacture of huge libraries of DNA oligonucleotides is producing foundational principles and results for the field. The size of the largest libraries of non-crosshybridizing oligonucleotides is also the limit on the size of feasible computation. The libraries are an enabling resource not only for large-scale DNA computations, but also biotechnology applications, such as reusable, universal DNA microarrays. In addition, the libraries, as well as the software tools, are available for reproduction and use by other researchers in DNA computing and biotechnology.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">130385</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130385</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n932" target="n934">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: NSF QuBIC: Modeling and Manufacture of Huge DNA Oligonucleotide Libraries for Computation</data>
      <data key="e_abstract">EIA-0130385&lt;br/&gt;Russell J. Deaton&lt;br/&gt;University of Arkansas&lt;br/&gt;&lt;br/&gt;Title: Modeling and Manufacture of Huge DNA Oligonucleotide Libraries for Computation&lt;br/&gt;&lt;br/&gt;Computing with DNA, with its advantages of massive parallelism and huge information density, promises a number of revolutionary applications, as well as the potential to solve problems beyond the capabilities of conventional computers. A critical barrier, however, is unplanned crosshybridization among oligonucleotides. In order for the computations to be reliable and efficient, and to scale to larger problems, the DNA sequences have to be designed to minimize these unplanned crosshybridizations. Though pairwise hybridization is well modeled and understood, design of such libraries is challenging because of the huge number of pairwise hybridization&apos;s, and the conflicting constraints of maximizing the library size while minimizing crosshybridization.Therefore, to overcome these limitations, huge libraries of non-crosshybridizing DNA oligonucleotides are manufactured by in vitro evolution with a PCR-based protocol that selects from a random pool those oligonucleotides that are maximally mismatched. In addition, because enumeration of all pairwise hybridization energetic in a huge library is computationally prohibitive, a statistical approach, which is based upon spin glass physics, is used to model the library. The model is the basis for a set of analysis and design tools for application to the libraries.&lt;br/&gt;&lt;br/&gt;Because of the fundamental importance of DNA hybridization in DNA computing, the modeling and manufacture of huge libraries of DNA oligonucleotides is producing foundational principles and results for the field. The size of the largest libraries of non-crosshybridizing oligonucleotides is also the limit on the size of feasible computation. The libraries are an enabling resource not only for large-scale DNA computations, but also biotechnology applications, such as reusable, universal DNA microarrays. In addition, the libraries, as well as the software tools, are available for reproduction and use by other researchers in DNA computing and biotechnology.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">130385</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130385</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n932" target="n935">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: NSF QuBIC: Modeling and Manufacture of Huge DNA Oligonucleotide Libraries for Computation</data>
      <data key="e_abstract">EIA-0130385&lt;br/&gt;Russell J. Deaton&lt;br/&gt;University of Arkansas&lt;br/&gt;&lt;br/&gt;Title: Modeling and Manufacture of Huge DNA Oligonucleotide Libraries for Computation&lt;br/&gt;&lt;br/&gt;Computing with DNA, with its advantages of massive parallelism and huge information density, promises a number of revolutionary applications, as well as the potential to solve problems beyond the capabilities of conventional computers. A critical barrier, however, is unplanned crosshybridization among oligonucleotides. In order for the computations to be reliable and efficient, and to scale to larger problems, the DNA sequences have to be designed to minimize these unplanned crosshybridizations. Though pairwise hybridization is well modeled and understood, design of such libraries is challenging because of the huge number of pairwise hybridization&apos;s, and the conflicting constraints of maximizing the library size while minimizing crosshybridization.Therefore, to overcome these limitations, huge libraries of non-crosshybridizing DNA oligonucleotides are manufactured by in vitro evolution with a PCR-based protocol that selects from a random pool those oligonucleotides that are maximally mismatched. In addition, because enumeration of all pairwise hybridization energetic in a huge library is computationally prohibitive, a statistical approach, which is based upon spin glass physics, is used to model the library. The model is the basis for a set of analysis and design tools for application to the libraries.&lt;br/&gt;&lt;br/&gt;Because of the fundamental importance of DNA hybridization in DNA computing, the modeling and manufacture of huge libraries of DNA oligonucleotides is producing foundational principles and results for the field. The size of the largest libraries of non-crosshybridizing oligonucleotides is also the limit on the size of feasible computation. The libraries are an enabling resource not only for large-scale DNA computations, but also biotechnology applications, such as reusable, universal DNA microarrays. In addition, the libraries, as well as the software tools, are available for reproduction and use by other researchers in DNA computing and biotechnology.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">130385</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130385</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n932" target="n936">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: NSF QuBIC: Modeling and Manufacture of Huge DNA Oligonucleotide Libraries for Computation</data>
      <data key="e_abstract">EIA-0130385&lt;br/&gt;Russell J. Deaton&lt;br/&gt;University of Arkansas&lt;br/&gt;&lt;br/&gt;Title: Modeling and Manufacture of Huge DNA Oligonucleotide Libraries for Computation&lt;br/&gt;&lt;br/&gt;Computing with DNA, with its advantages of massive parallelism and huge information density, promises a number of revolutionary applications, as well as the potential to solve problems beyond the capabilities of conventional computers. A critical barrier, however, is unplanned crosshybridization among oligonucleotides. In order for the computations to be reliable and efficient, and to scale to larger problems, the DNA sequences have to be designed to minimize these unplanned crosshybridizations. Though pairwise hybridization is well modeled and understood, design of such libraries is challenging because of the huge number of pairwise hybridization&apos;s, and the conflicting constraints of maximizing the library size while minimizing crosshybridization.Therefore, to overcome these limitations, huge libraries of non-crosshybridizing DNA oligonucleotides are manufactured by in vitro evolution with a PCR-based protocol that selects from a random pool those oligonucleotides that are maximally mismatched. In addition, because enumeration of all pairwise hybridization energetic in a huge library is computationally prohibitive, a statistical approach, which is based upon spin glass physics, is used to model the library. The model is the basis for a set of analysis and design tools for application to the libraries.&lt;br/&gt;&lt;br/&gt;Because of the fundamental importance of DNA hybridization in DNA computing, the modeling and manufacture of huge libraries of DNA oligonucleotides is producing foundational principles and results for the field. The size of the largest libraries of non-crosshybridizing oligonucleotides is also the limit on the size of feasible computation. The libraries are an enabling resource not only for large-scale DNA computations, but also biotechnology applications, such as reusable, universal DNA microarrays. In addition, the libraries, as well as the software tools, are available for reproduction and use by other researchers in DNA computing and biotechnology.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">130385</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130385</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n933" target="n934">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: NSF QuBIC: Modeling and Manufacture of Huge DNA Oligonucleotide Libraries for Computation</data>
      <data key="e_abstract">EIA-0130385&lt;br/&gt;Russell J. Deaton&lt;br/&gt;University of Arkansas&lt;br/&gt;&lt;br/&gt;Title: Modeling and Manufacture of Huge DNA Oligonucleotide Libraries for Computation&lt;br/&gt;&lt;br/&gt;Computing with DNA, with its advantages of massive parallelism and huge information density, promises a number of revolutionary applications, as well as the potential to solve problems beyond the capabilities of conventional computers. A critical barrier, however, is unplanned crosshybridization among oligonucleotides. In order for the computations to be reliable and efficient, and to scale to larger problems, the DNA sequences have to be designed to minimize these unplanned crosshybridizations. Though pairwise hybridization is well modeled and understood, design of such libraries is challenging because of the huge number of pairwise hybridization&apos;s, and the conflicting constraints of maximizing the library size while minimizing crosshybridization.Therefore, to overcome these limitations, huge libraries of non-crosshybridizing DNA oligonucleotides are manufactured by in vitro evolution with a PCR-based protocol that selects from a random pool those oligonucleotides that are maximally mismatched. In addition, because enumeration of all pairwise hybridization energetic in a huge library is computationally prohibitive, a statistical approach, which is based upon spin glass physics, is used to model the library. The model is the basis for a set of analysis and design tools for application to the libraries.&lt;br/&gt;&lt;br/&gt;Because of the fundamental importance of DNA hybridization in DNA computing, the modeling and manufacture of huge libraries of DNA oligonucleotides is producing foundational principles and results for the field. The size of the largest libraries of non-crosshybridizing oligonucleotides is also the limit on the size of feasible computation. The libraries are an enabling resource not only for large-scale DNA computations, but also biotechnology applications, such as reusable, universal DNA microarrays. In addition, the libraries, as well as the software tools, are available for reproduction and use by other researchers in DNA computing and biotechnology.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">130385</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130385</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n933" target="n935">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: NSF QuBIC: Modeling and Manufacture of Huge DNA Oligonucleotide Libraries for Computation</data>
      <data key="e_abstract">EIA-0130385&lt;br/&gt;Russell J. Deaton&lt;br/&gt;University of Arkansas&lt;br/&gt;&lt;br/&gt;Title: Modeling and Manufacture of Huge DNA Oligonucleotide Libraries for Computation&lt;br/&gt;&lt;br/&gt;Computing with DNA, with its advantages of massive parallelism and huge information density, promises a number of revolutionary applications, as well as the potential to solve problems beyond the capabilities of conventional computers. A critical barrier, however, is unplanned crosshybridization among oligonucleotides. In order for the computations to be reliable and efficient, and to scale to larger problems, the DNA sequences have to be designed to minimize these unplanned crosshybridizations. Though pairwise hybridization is well modeled and understood, design of such libraries is challenging because of the huge number of pairwise hybridization&apos;s, and the conflicting constraints of maximizing the library size while minimizing crosshybridization.Therefore, to overcome these limitations, huge libraries of non-crosshybridizing DNA oligonucleotides are manufactured by in vitro evolution with a PCR-based protocol that selects from a random pool those oligonucleotides that are maximally mismatched. In addition, because enumeration of all pairwise hybridization energetic in a huge library is computationally prohibitive, a statistical approach, which is based upon spin glass physics, is used to model the library. The model is the basis for a set of analysis and design tools for application to the libraries.&lt;br/&gt;&lt;br/&gt;Because of the fundamental importance of DNA hybridization in DNA computing, the modeling and manufacture of huge libraries of DNA oligonucleotides is producing foundational principles and results for the field. The size of the largest libraries of non-crosshybridizing oligonucleotides is also the limit on the size of feasible computation. The libraries are an enabling resource not only for large-scale DNA computations, but also biotechnology applications, such as reusable, universal DNA microarrays. In addition, the libraries, as well as the software tools, are available for reproduction and use by other researchers in DNA computing and biotechnology.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">130385</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130385</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n933" target="n936">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: NSF QuBIC: Modeling and Manufacture of Huge DNA Oligonucleotide Libraries for Computation</data>
      <data key="e_abstract">EIA-0130385&lt;br/&gt;Russell J. Deaton&lt;br/&gt;University of Arkansas&lt;br/&gt;&lt;br/&gt;Title: Modeling and Manufacture of Huge DNA Oligonucleotide Libraries for Computation&lt;br/&gt;&lt;br/&gt;Computing with DNA, with its advantages of massive parallelism and huge information density, promises a number of revolutionary applications, as well as the potential to solve problems beyond the capabilities of conventional computers. A critical barrier, however, is unplanned crosshybridization among oligonucleotides. In order for the computations to be reliable and efficient, and to scale to larger problems, the DNA sequences have to be designed to minimize these unplanned crosshybridizations. Though pairwise hybridization is well modeled and understood, design of such libraries is challenging because of the huge number of pairwise hybridization&apos;s, and the conflicting constraints of maximizing the library size while minimizing crosshybridization.Therefore, to overcome these limitations, huge libraries of non-crosshybridizing DNA oligonucleotides are manufactured by in vitro evolution with a PCR-based protocol that selects from a random pool those oligonucleotides that are maximally mismatched. In addition, because enumeration of all pairwise hybridization energetic in a huge library is computationally prohibitive, a statistical approach, which is based upon spin glass physics, is used to model the library. The model is the basis for a set of analysis and design tools for application to the libraries.&lt;br/&gt;&lt;br/&gt;Because of the fundamental importance of DNA hybridization in DNA computing, the modeling and manufacture of huge libraries of DNA oligonucleotides is producing foundational principles and results for the field. The size of the largest libraries of non-crosshybridizing oligonucleotides is also the limit on the size of feasible computation. The libraries are an enabling resource not only for large-scale DNA computations, but also biotechnology applications, such as reusable, universal DNA microarrays. In addition, the libraries, as well as the software tools, are available for reproduction and use by other researchers in DNA computing and biotechnology.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">130385</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130385</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n934" target="n935">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: NSF QuBIC: Modeling and Manufacture of Huge DNA Oligonucleotide Libraries for Computation</data>
      <data key="e_abstract">EIA-0130385&lt;br/&gt;Russell J. Deaton&lt;br/&gt;University of Arkansas&lt;br/&gt;&lt;br/&gt;Title: Modeling and Manufacture of Huge DNA Oligonucleotide Libraries for Computation&lt;br/&gt;&lt;br/&gt;Computing with DNA, with its advantages of massive parallelism and huge information density, promises a number of revolutionary applications, as well as the potential to solve problems beyond the capabilities of conventional computers. A critical barrier, however, is unplanned crosshybridization among oligonucleotides. In order for the computations to be reliable and efficient, and to scale to larger problems, the DNA sequences have to be designed to minimize these unplanned crosshybridizations. Though pairwise hybridization is well modeled and understood, design of such libraries is challenging because of the huge number of pairwise hybridization&apos;s, and the conflicting constraints of maximizing the library size while minimizing crosshybridization.Therefore, to overcome these limitations, huge libraries of non-crosshybridizing DNA oligonucleotides are manufactured by in vitro evolution with a PCR-based protocol that selects from a random pool those oligonucleotides that are maximally mismatched. In addition, because enumeration of all pairwise hybridization energetic in a huge library is computationally prohibitive, a statistical approach, which is based upon spin glass physics, is used to model the library. The model is the basis for a set of analysis and design tools for application to the libraries.&lt;br/&gt;&lt;br/&gt;Because of the fundamental importance of DNA hybridization in DNA computing, the modeling and manufacture of huge libraries of DNA oligonucleotides is producing foundational principles and results for the field. The size of the largest libraries of non-crosshybridizing oligonucleotides is also the limit on the size of feasible computation. The libraries are an enabling resource not only for large-scale DNA computations, but also biotechnology applications, such as reusable, universal DNA microarrays. In addition, the libraries, as well as the software tools, are available for reproduction and use by other researchers in DNA computing and biotechnology.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">130385</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130385</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n934" target="n936">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: NSF QuBIC: Modeling and Manufacture of Huge DNA Oligonucleotide Libraries for Computation</data>
      <data key="e_abstract">EIA-0130385&lt;br/&gt;Russell J. Deaton&lt;br/&gt;University of Arkansas&lt;br/&gt;&lt;br/&gt;Title: Modeling and Manufacture of Huge DNA Oligonucleotide Libraries for Computation&lt;br/&gt;&lt;br/&gt;Computing with DNA, with its advantages of massive parallelism and huge information density, promises a number of revolutionary applications, as well as the potential to solve problems beyond the capabilities of conventional computers. A critical barrier, however, is unplanned crosshybridization among oligonucleotides. In order for the computations to be reliable and efficient, and to scale to larger problems, the DNA sequences have to be designed to minimize these unplanned crosshybridizations. Though pairwise hybridization is well modeled and understood, design of such libraries is challenging because of the huge number of pairwise hybridization&apos;s, and the conflicting constraints of maximizing the library size while minimizing crosshybridization.Therefore, to overcome these limitations, huge libraries of non-crosshybridizing DNA oligonucleotides are manufactured by in vitro evolution with a PCR-based protocol that selects from a random pool those oligonucleotides that are maximally mismatched. In addition, because enumeration of all pairwise hybridization energetic in a huge library is computationally prohibitive, a statistical approach, which is based upon spin glass physics, is used to model the library. The model is the basis for a set of analysis and design tools for application to the libraries.&lt;br/&gt;&lt;br/&gt;Because of the fundamental importance of DNA hybridization in DNA computing, the modeling and manufacture of huge libraries of DNA oligonucleotides is producing foundational principles and results for the field. The size of the largest libraries of non-crosshybridizing oligonucleotides is also the limit on the size of feasible computation. The libraries are an enabling resource not only for large-scale DNA computations, but also biotechnology applications, such as reusable, universal DNA microarrays. In addition, the libraries, as well as the software tools, are available for reproduction and use by other researchers in DNA computing and biotechnology.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">130385</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130385</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n935" target="n936">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: NSF QuBIC: Modeling and Manufacture of Huge DNA Oligonucleotide Libraries for Computation</data>
      <data key="e_abstract">EIA-0130385&lt;br/&gt;Russell J. Deaton&lt;br/&gt;University of Arkansas&lt;br/&gt;&lt;br/&gt;Title: Modeling and Manufacture of Huge DNA Oligonucleotide Libraries for Computation&lt;br/&gt;&lt;br/&gt;Computing with DNA, with its advantages of massive parallelism and huge information density, promises a number of revolutionary applications, as well as the potential to solve problems beyond the capabilities of conventional computers. A critical barrier, however, is unplanned crosshybridization among oligonucleotides. In order for the computations to be reliable and efficient, and to scale to larger problems, the DNA sequences have to be designed to minimize these unplanned crosshybridizations. Though pairwise hybridization is well modeled and understood, design of such libraries is challenging because of the huge number of pairwise hybridization&apos;s, and the conflicting constraints of maximizing the library size while minimizing crosshybridization.Therefore, to overcome these limitations, huge libraries of non-crosshybridizing DNA oligonucleotides are manufactured by in vitro evolution with a PCR-based protocol that selects from a random pool those oligonucleotides that are maximally mismatched. In addition, because enumeration of all pairwise hybridization energetic in a huge library is computationally prohibitive, a statistical approach, which is based upon spin glass physics, is used to model the library. The model is the basis for a set of analysis and design tools for application to the libraries.&lt;br/&gt;&lt;br/&gt;Because of the fundamental importance of DNA hybridization in DNA computing, the modeling and manufacture of huge libraries of DNA oligonucleotides is producing foundational principles and results for the field. The size of the largest libraries of non-crosshybridizing oligonucleotides is also the limit on the size of feasible computation. The libraries are an enabling resource not only for large-scale DNA computations, but also biotechnology applications, such as reusable, universal DNA microarrays. In addition, the libraries, as well as the software tools, are available for reproduction and use by other researchers in DNA computing and biotechnology.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">130385</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130385</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n937" target="n938">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Quantum-QuBIC: Topological Quantum Computation</data>
      <data key="e_abstract">EIA-0130388&lt;br/&gt;Zhenghan Wang&lt;br/&gt;Indiana University Bloomington&lt;br/&gt;&lt;br/&gt;Title: Topological Quantum Computation&lt;br/&gt;&lt;br/&gt;The theory of quantum computation is being constructed from abstract study of topological properties of collective electron systems. One example is the dancing pattern of quasi-particles in fractional Quantum Hall effects. The dancing patterns of quasi-particles are described mathematically by braids. In this context, the Berry phase of the quasi-particles gives rise representations of braids. In mathematical terms, these are modular functors.&lt;br/&gt;&lt;br/&gt;This project is using insights from modular functors to investigate the possibilities for physical realization of quantum computers. The chief advantage of topological quantum computation is physical error correction. The rich mathematical structure of modular functors is also employed to design new quantum algorithms. Topological Quantum Computation Numerical Laboratory is being established as an intermediate step towards the physical implementation of a real quantum computer based on the principles of modular functors. A lecture and seminar series at Indiana University is organized to training students. This cross-disciplinary project involves topology, condensed matter physics, and computer science.</data>
      <data key="e_pgm">1708</data>
      <data key="e_label">130388</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130388</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n940" target="n941">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: A Learning Environment for Information Technology Concepts Using Intensive, Unobtrusive Assessment</data>
      <data key="e_abstract">EIA-0121345 &lt;br/&gt;Tanimoto, Steven&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;ITR/PE: A Learning Environment for Information Technology Concepts Using Intensive, Unobtrusive Assessment&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Although many learning environments have been developed that effectively engage students in assembling computational objects, simulations, or mathematical constructions, for the most part these systems fail to take pedagogical advantage of the wealth of assessment-related data that results from the fact that the students are working on computers. This is due less to the newness of these environments than to the challenges of effectively utilizing the event logs, student writing, and student constructions as evidence of cognitive state, learning preferences, and skills. We propose to integrate and extend two software systems for online education in order to perform a series of experiments that assess the impact of using intensive, unobtrusive assessment tightly integrated with a constructive learning environment upon learning outcomes and efficiencies. The integrated learning environment will be tested primarily with University of Washington freshmen but also with Mercer Island High School seniors, in problem solving and construction activities involving digital image representation and processing, web-based communication, and computer programming. The students&apos; writing, online constructions, online sketches and computer-generated activity logs will be analyzed using a combination of computer-assisted and automated mark-up. The results of mark-up will trigger pedagogical recommendations to instructors, and in some cases, directly to the students. Such interventions may take a variety of forms; for example, a group of students stuck at the point of frustration in solving some problem may benefit by having a key step, performed well by a student in another group, called to their attention. Our objective will be to determine effective methods both for extracting pedagogically useful information from the products and byproducts of online learning and for designing the instruction to enable and benefit from the use of this information.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121345</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121345</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n940" target="n942">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: A Learning Environment for Information Technology Concepts Using Intensive, Unobtrusive Assessment</data>
      <data key="e_abstract">EIA-0121345 &lt;br/&gt;Tanimoto, Steven&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;ITR/PE: A Learning Environment for Information Technology Concepts Using Intensive, Unobtrusive Assessment&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Although many learning environments have been developed that effectively engage students in assembling computational objects, simulations, or mathematical constructions, for the most part these systems fail to take pedagogical advantage of the wealth of assessment-related data that results from the fact that the students are working on computers. This is due less to the newness of these environments than to the challenges of effectively utilizing the event logs, student writing, and student constructions as evidence of cognitive state, learning preferences, and skills. We propose to integrate and extend two software systems for online education in order to perform a series of experiments that assess the impact of using intensive, unobtrusive assessment tightly integrated with a constructive learning environment upon learning outcomes and efficiencies. The integrated learning environment will be tested primarily with University of Washington freshmen but also with Mercer Island High School seniors, in problem solving and construction activities involving digital image representation and processing, web-based communication, and computer programming. The students&apos; writing, online constructions, online sketches and computer-generated activity logs will be analyzed using a combination of computer-assisted and automated mark-up. The results of mark-up will trigger pedagogical recommendations to instructors, and in some cases, directly to the students. Such interventions may take a variety of forms; for example, a group of students stuck at the point of frustration in solving some problem may benefit by having a key step, performed well by a student in another group, called to their attention. Our objective will be to determine effective methods both for extracting pedagogically useful information from the products and byproducts of online learning and for designing the instruction to enable and benefit from the use of this information.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121345</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121345</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n940" target="n943">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: A Learning Environment for Information Technology Concepts Using Intensive, Unobtrusive Assessment</data>
      <data key="e_abstract">EIA-0121345 &lt;br/&gt;Tanimoto, Steven&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;ITR/PE: A Learning Environment for Information Technology Concepts Using Intensive, Unobtrusive Assessment&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Although many learning environments have been developed that effectively engage students in assembling computational objects, simulations, or mathematical constructions, for the most part these systems fail to take pedagogical advantage of the wealth of assessment-related data that results from the fact that the students are working on computers. This is due less to the newness of these environments than to the challenges of effectively utilizing the event logs, student writing, and student constructions as evidence of cognitive state, learning preferences, and skills. We propose to integrate and extend two software systems for online education in order to perform a series of experiments that assess the impact of using intensive, unobtrusive assessment tightly integrated with a constructive learning environment upon learning outcomes and efficiencies. The integrated learning environment will be tested primarily with University of Washington freshmen but also with Mercer Island High School seniors, in problem solving and construction activities involving digital image representation and processing, web-based communication, and computer programming. The students&apos; writing, online constructions, online sketches and computer-generated activity logs will be analyzed using a combination of computer-assisted and automated mark-up. The results of mark-up will trigger pedagogical recommendations to instructors, and in some cases, directly to the students. Such interventions may take a variety of forms; for example, a group of students stuck at the point of frustration in solving some problem may benefit by having a key step, performed well by a student in another group, called to their attention. Our objective will be to determine effective methods both for extracting pedagogically useful information from the products and byproducts of online learning and for designing the instruction to enable and benefit from the use of this information.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121345</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121345</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n941" target="n942">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: A Learning Environment for Information Technology Concepts Using Intensive, Unobtrusive Assessment</data>
      <data key="e_abstract">EIA-0121345 &lt;br/&gt;Tanimoto, Steven&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;ITR/PE: A Learning Environment for Information Technology Concepts Using Intensive, Unobtrusive Assessment&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Although many learning environments have been developed that effectively engage students in assembling computational objects, simulations, or mathematical constructions, for the most part these systems fail to take pedagogical advantage of the wealth of assessment-related data that results from the fact that the students are working on computers. This is due less to the newness of these environments than to the challenges of effectively utilizing the event logs, student writing, and student constructions as evidence of cognitive state, learning preferences, and skills. We propose to integrate and extend two software systems for online education in order to perform a series of experiments that assess the impact of using intensive, unobtrusive assessment tightly integrated with a constructive learning environment upon learning outcomes and efficiencies. The integrated learning environment will be tested primarily with University of Washington freshmen but also with Mercer Island High School seniors, in problem solving and construction activities involving digital image representation and processing, web-based communication, and computer programming. The students&apos; writing, online constructions, online sketches and computer-generated activity logs will be analyzed using a combination of computer-assisted and automated mark-up. The results of mark-up will trigger pedagogical recommendations to instructors, and in some cases, directly to the students. Such interventions may take a variety of forms; for example, a group of students stuck at the point of frustration in solving some problem may benefit by having a key step, performed well by a student in another group, called to their attention. Our objective will be to determine effective methods both for extracting pedagogically useful information from the products and byproducts of online learning and for designing the instruction to enable and benefit from the use of this information.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121345</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121345</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n941" target="n943">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: A Learning Environment for Information Technology Concepts Using Intensive, Unobtrusive Assessment</data>
      <data key="e_abstract">EIA-0121345 &lt;br/&gt;Tanimoto, Steven&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;ITR/PE: A Learning Environment for Information Technology Concepts Using Intensive, Unobtrusive Assessment&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Although many learning environments have been developed that effectively engage students in assembling computational objects, simulations, or mathematical constructions, for the most part these systems fail to take pedagogical advantage of the wealth of assessment-related data that results from the fact that the students are working on computers. This is due less to the newness of these environments than to the challenges of effectively utilizing the event logs, student writing, and student constructions as evidence of cognitive state, learning preferences, and skills. We propose to integrate and extend two software systems for online education in order to perform a series of experiments that assess the impact of using intensive, unobtrusive assessment tightly integrated with a constructive learning environment upon learning outcomes and efficiencies. The integrated learning environment will be tested primarily with University of Washington freshmen but also with Mercer Island High School seniors, in problem solving and construction activities involving digital image representation and processing, web-based communication, and computer programming. The students&apos; writing, online constructions, online sketches and computer-generated activity logs will be analyzed using a combination of computer-assisted and automated mark-up. The results of mark-up will trigger pedagogical recommendations to instructors, and in some cases, directly to the students. Such interventions may take a variety of forms; for example, a group of students stuck at the point of frustration in solving some problem may benefit by having a key step, performed well by a student in another group, called to their attention. Our objective will be to determine effective methods both for extracting pedagogically useful information from the products and byproducts of online learning and for designing the instruction to enable and benefit from the use of this information.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121345</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121345</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n942" target="n943">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: A Learning Environment for Information Technology Concepts Using Intensive, Unobtrusive Assessment</data>
      <data key="e_abstract">EIA-0121345 &lt;br/&gt;Tanimoto, Steven&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;ITR/PE: A Learning Environment for Information Technology Concepts Using Intensive, Unobtrusive Assessment&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Although many learning environments have been developed that effectively engage students in assembling computational objects, simulations, or mathematical constructions, for the most part these systems fail to take pedagogical advantage of the wealth of assessment-related data that results from the fact that the students are working on computers. This is due less to the newness of these environments than to the challenges of effectively utilizing the event logs, student writing, and student constructions as evidence of cognitive state, learning preferences, and skills. We propose to integrate and extend two software systems for online education in order to perform a series of experiments that assess the impact of using intensive, unobtrusive assessment tightly integrated with a constructive learning environment upon learning outcomes and efficiencies. The integrated learning environment will be tested primarily with University of Washington freshmen but also with Mercer Island High School seniors, in problem solving and construction activities involving digital image representation and processing, web-based communication, and computer programming. The students&apos; writing, online constructions, online sketches and computer-generated activity logs will be analyzed using a combination of computer-assisted and automated mark-up. The results of mark-up will trigger pedagogical recommendations to instructors, and in some cases, directly to the students. Such interventions may take a variety of forms; for example, a group of students stuck at the point of frustration in solving some problem may benefit by having a key step, performed well by a student in another group, called to their attention. Our objective will be to determine effective methods both for extracting pedagogically useful information from the products and byproducts of online learning and for designing the instruction to enable and benefit from the use of this information.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121345</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121345</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n944" target="n945">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SI+AP: Active Sensor Networks with Applications in Marine Microorganism Monitoring</data>
      <data key="e_abstract">EIA-0121141&lt;br/&gt;Requicha, Aristides&lt;br/&gt;University of Southern California&lt;br/&gt;&lt;br/&gt;ITR/SI+AP: Active Sensor Networks with Applications in Marine Microorganism Monitoring&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The proposed research combines networking, distributed robotics, nanorobotics, and microbiology in an effort to develop and apply technology for the in-situ, real-time monitoring of microbial populations in aquatic environments, such as the ocean or water supply systems. The application context provides feedback from experiments with realistic systems, and this feedback is essential to the progress of the Information Technology (IT) research proposed here. This project addresses two key challenges for IT during this decade: moving from virtual to physical applications, and moving from macro to micro and nano.&lt;br/&gt;&lt;br/&gt;The IT focus is on the study of Physically-Coupled Scalable Information Infrastructures (PCSIIs), which effectively &quot;embed the internet&quot;. The sensors and actuators in the proposed PCSII must have small physical dimensions, comparable to those of the microorganisms to be monitored. They must be deployed in very large numbers to achieve the unprecedented spatial and temporal resolution necessary to investigate the causal relationships between environmental conditions and microorganisms. Control and coordination of a multitude of such devices of limited and heterogeneous capabilities raise major challenges for networking, distributed coordination and distributed algorithms. Sensing for detection and identification of microorganisms is another challenge, which will be tackled by using nanorobotic Scanning Probe Microscope technology.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121141</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121141</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n944" target="n946">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SI+AP: Active Sensor Networks with Applications in Marine Microorganism Monitoring</data>
      <data key="e_abstract">EIA-0121141&lt;br/&gt;Requicha, Aristides&lt;br/&gt;University of Southern California&lt;br/&gt;&lt;br/&gt;ITR/SI+AP: Active Sensor Networks with Applications in Marine Microorganism Monitoring&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The proposed research combines networking, distributed robotics, nanorobotics, and microbiology in an effort to develop and apply technology for the in-situ, real-time monitoring of microbial populations in aquatic environments, such as the ocean or water supply systems. The application context provides feedback from experiments with realistic systems, and this feedback is essential to the progress of the Information Technology (IT) research proposed here. This project addresses two key challenges for IT during this decade: moving from virtual to physical applications, and moving from macro to micro and nano.&lt;br/&gt;&lt;br/&gt;The IT focus is on the study of Physically-Coupled Scalable Information Infrastructures (PCSIIs), which effectively &quot;embed the internet&quot;. The sensors and actuators in the proposed PCSII must have small physical dimensions, comparable to those of the microorganisms to be monitored. They must be deployed in very large numbers to achieve the unprecedented spatial and temporal resolution necessary to investigate the causal relationships between environmental conditions and microorganisms. Control and coordination of a multitude of such devices of limited and heterogeneous capabilities raise major challenges for networking, distributed coordination and distributed algorithms. Sensing for detection and identification of microorganisms is another challenge, which will be tackled by using nanorobotic Scanning Probe Microscope technology.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121141</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121141</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n944" target="n947">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SI+AP: Active Sensor Networks with Applications in Marine Microorganism Monitoring</data>
      <data key="e_abstract">EIA-0121141&lt;br/&gt;Requicha, Aristides&lt;br/&gt;University of Southern California&lt;br/&gt;&lt;br/&gt;ITR/SI+AP: Active Sensor Networks with Applications in Marine Microorganism Monitoring&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The proposed research combines networking, distributed robotics, nanorobotics, and microbiology in an effort to develop and apply technology for the in-situ, real-time monitoring of microbial populations in aquatic environments, such as the ocean or water supply systems. The application context provides feedback from experiments with realistic systems, and this feedback is essential to the progress of the Information Technology (IT) research proposed here. This project addresses two key challenges for IT during this decade: moving from virtual to physical applications, and moving from macro to micro and nano.&lt;br/&gt;&lt;br/&gt;The IT focus is on the study of Physically-Coupled Scalable Information Infrastructures (PCSIIs), which effectively &quot;embed the internet&quot;. The sensors and actuators in the proposed PCSII must have small physical dimensions, comparable to those of the microorganisms to be monitored. They must be deployed in very large numbers to achieve the unprecedented spatial and temporal resolution necessary to investigate the causal relationships between environmental conditions and microorganisms. Control and coordination of a multitude of such devices of limited and heterogeneous capabilities raise major challenges for networking, distributed coordination and distributed algorithms. Sensing for detection and identification of microorganisms is another challenge, which will be tackled by using nanorobotic Scanning Probe Microscope technology.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121141</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121141</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n944" target="n948">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SI+AP: Active Sensor Networks with Applications in Marine Microorganism Monitoring</data>
      <data key="e_abstract">EIA-0121141&lt;br/&gt;Requicha, Aristides&lt;br/&gt;University of Southern California&lt;br/&gt;&lt;br/&gt;ITR/SI+AP: Active Sensor Networks with Applications in Marine Microorganism Monitoring&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The proposed research combines networking, distributed robotics, nanorobotics, and microbiology in an effort to develop and apply technology for the in-situ, real-time monitoring of microbial populations in aquatic environments, such as the ocean or water supply systems. The application context provides feedback from experiments with realistic systems, and this feedback is essential to the progress of the Information Technology (IT) research proposed here. This project addresses two key challenges for IT during this decade: moving from virtual to physical applications, and moving from macro to micro and nano.&lt;br/&gt;&lt;br/&gt;The IT focus is on the study of Physically-Coupled Scalable Information Infrastructures (PCSIIs), which effectively &quot;embed the internet&quot;. The sensors and actuators in the proposed PCSII must have small physical dimensions, comparable to those of the microorganisms to be monitored. They must be deployed in very large numbers to achieve the unprecedented spatial and temporal resolution necessary to investigate the causal relationships between environmental conditions and microorganisms. Control and coordination of a multitude of such devices of limited and heterogeneous capabilities raise major challenges for networking, distributed coordination and distributed algorithms. Sensing for detection and identification of microorganisms is another challenge, which will be tackled by using nanorobotic Scanning Probe Microscope technology.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121141</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121141</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n945" target="n946">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SI+AP: Active Sensor Networks with Applications in Marine Microorganism Monitoring</data>
      <data key="e_abstract">EIA-0121141&lt;br/&gt;Requicha, Aristides&lt;br/&gt;University of Southern California&lt;br/&gt;&lt;br/&gt;ITR/SI+AP: Active Sensor Networks with Applications in Marine Microorganism Monitoring&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The proposed research combines networking, distributed robotics, nanorobotics, and microbiology in an effort to develop and apply technology for the in-situ, real-time monitoring of microbial populations in aquatic environments, such as the ocean or water supply systems. The application context provides feedback from experiments with realistic systems, and this feedback is essential to the progress of the Information Technology (IT) research proposed here. This project addresses two key challenges for IT during this decade: moving from virtual to physical applications, and moving from macro to micro and nano.&lt;br/&gt;&lt;br/&gt;The IT focus is on the study of Physically-Coupled Scalable Information Infrastructures (PCSIIs), which effectively &quot;embed the internet&quot;. The sensors and actuators in the proposed PCSII must have small physical dimensions, comparable to those of the microorganisms to be monitored. They must be deployed in very large numbers to achieve the unprecedented spatial and temporal resolution necessary to investigate the causal relationships between environmental conditions and microorganisms. Control and coordination of a multitude of such devices of limited and heterogeneous capabilities raise major challenges for networking, distributed coordination and distributed algorithms. Sensing for detection and identification of microorganisms is another challenge, which will be tackled by using nanorobotic Scanning Probe Microscope technology.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121141</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121141</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n945" target="n947">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SI+AP: Active Sensor Networks with Applications in Marine Microorganism Monitoring</data>
      <data key="e_abstract">EIA-0121141&lt;br/&gt;Requicha, Aristides&lt;br/&gt;University of Southern California&lt;br/&gt;&lt;br/&gt;ITR/SI+AP: Active Sensor Networks with Applications in Marine Microorganism Monitoring&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The proposed research combines networking, distributed robotics, nanorobotics, and microbiology in an effort to develop and apply technology for the in-situ, real-time monitoring of microbial populations in aquatic environments, such as the ocean or water supply systems. The application context provides feedback from experiments with realistic systems, and this feedback is essential to the progress of the Information Technology (IT) research proposed here. This project addresses two key challenges for IT during this decade: moving from virtual to physical applications, and moving from macro to micro and nano.&lt;br/&gt;&lt;br/&gt;The IT focus is on the study of Physically-Coupled Scalable Information Infrastructures (PCSIIs), which effectively &quot;embed the internet&quot;. The sensors and actuators in the proposed PCSII must have small physical dimensions, comparable to those of the microorganisms to be monitored. They must be deployed in very large numbers to achieve the unprecedented spatial and temporal resolution necessary to investigate the causal relationships between environmental conditions and microorganisms. Control and coordination of a multitude of such devices of limited and heterogeneous capabilities raise major challenges for networking, distributed coordination and distributed algorithms. Sensing for detection and identification of microorganisms is another challenge, which will be tackled by using nanorobotic Scanning Probe Microscope technology.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121141</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121141</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n945" target="n948">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SI+AP: Active Sensor Networks with Applications in Marine Microorganism Monitoring</data>
      <data key="e_abstract">EIA-0121141&lt;br/&gt;Requicha, Aristides&lt;br/&gt;University of Southern California&lt;br/&gt;&lt;br/&gt;ITR/SI+AP: Active Sensor Networks with Applications in Marine Microorganism Monitoring&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The proposed research combines networking, distributed robotics, nanorobotics, and microbiology in an effort to develop and apply technology for the in-situ, real-time monitoring of microbial populations in aquatic environments, such as the ocean or water supply systems. The application context provides feedback from experiments with realistic systems, and this feedback is essential to the progress of the Information Technology (IT) research proposed here. This project addresses two key challenges for IT during this decade: moving from virtual to physical applications, and moving from macro to micro and nano.&lt;br/&gt;&lt;br/&gt;The IT focus is on the study of Physically-Coupled Scalable Information Infrastructures (PCSIIs), which effectively &quot;embed the internet&quot;. The sensors and actuators in the proposed PCSII must have small physical dimensions, comparable to those of the microorganisms to be monitored. They must be deployed in very large numbers to achieve the unprecedented spatial and temporal resolution necessary to investigate the causal relationships between environmental conditions and microorganisms. Control and coordination of a multitude of such devices of limited and heterogeneous capabilities raise major challenges for networking, distributed coordination and distributed algorithms. Sensing for detection and identification of microorganisms is another challenge, which will be tackled by using nanorobotic Scanning Probe Microscope technology.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121141</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121141</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n946" target="n947">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SI+AP: Active Sensor Networks with Applications in Marine Microorganism Monitoring</data>
      <data key="e_abstract">EIA-0121141&lt;br/&gt;Requicha, Aristides&lt;br/&gt;University of Southern California&lt;br/&gt;&lt;br/&gt;ITR/SI+AP: Active Sensor Networks with Applications in Marine Microorganism Monitoring&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The proposed research combines networking, distributed robotics, nanorobotics, and microbiology in an effort to develop and apply technology for the in-situ, real-time monitoring of microbial populations in aquatic environments, such as the ocean or water supply systems. The application context provides feedback from experiments with realistic systems, and this feedback is essential to the progress of the Information Technology (IT) research proposed here. This project addresses two key challenges for IT during this decade: moving from virtual to physical applications, and moving from macro to micro and nano.&lt;br/&gt;&lt;br/&gt;The IT focus is on the study of Physically-Coupled Scalable Information Infrastructures (PCSIIs), which effectively &quot;embed the internet&quot;. The sensors and actuators in the proposed PCSII must have small physical dimensions, comparable to those of the microorganisms to be monitored. They must be deployed in very large numbers to achieve the unprecedented spatial and temporal resolution necessary to investigate the causal relationships between environmental conditions and microorganisms. Control and coordination of a multitude of such devices of limited and heterogeneous capabilities raise major challenges for networking, distributed coordination and distributed algorithms. Sensing for detection and identification of microorganisms is another challenge, which will be tackled by using nanorobotic Scanning Probe Microscope technology.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121141</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121141</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n946" target="n948">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SI+AP: Active Sensor Networks with Applications in Marine Microorganism Monitoring</data>
      <data key="e_abstract">EIA-0121141&lt;br/&gt;Requicha, Aristides&lt;br/&gt;University of Southern California&lt;br/&gt;&lt;br/&gt;ITR/SI+AP: Active Sensor Networks with Applications in Marine Microorganism Monitoring&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The proposed research combines networking, distributed robotics, nanorobotics, and microbiology in an effort to develop and apply technology for the in-situ, real-time monitoring of microbial populations in aquatic environments, such as the ocean or water supply systems. The application context provides feedback from experiments with realistic systems, and this feedback is essential to the progress of the Information Technology (IT) research proposed here. This project addresses two key challenges for IT during this decade: moving from virtual to physical applications, and moving from macro to micro and nano.&lt;br/&gt;&lt;br/&gt;The IT focus is on the study of Physically-Coupled Scalable Information Infrastructures (PCSIIs), which effectively &quot;embed the internet&quot;. The sensors and actuators in the proposed PCSII must have small physical dimensions, comparable to those of the microorganisms to be monitored. They must be deployed in very large numbers to achieve the unprecedented spatial and temporal resolution necessary to investigate the causal relationships between environmental conditions and microorganisms. Control and coordination of a multitude of such devices of limited and heterogeneous capabilities raise major challenges for networking, distributed coordination and distributed algorithms. Sensing for detection and identification of microorganisms is another challenge, which will be tackled by using nanorobotic Scanning Probe Microscope technology.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121141</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121141</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n947" target="n948">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SI+AP: Active Sensor Networks with Applications in Marine Microorganism Monitoring</data>
      <data key="e_abstract">EIA-0121141&lt;br/&gt;Requicha, Aristides&lt;br/&gt;University of Southern California&lt;br/&gt;&lt;br/&gt;ITR/SI+AP: Active Sensor Networks with Applications in Marine Microorganism Monitoring&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The proposed research combines networking, distributed robotics, nanorobotics, and microbiology in an effort to develop and apply technology for the in-situ, real-time monitoring of microbial populations in aquatic environments, such as the ocean or water supply systems. The application context provides feedback from experiments with realistic systems, and this feedback is essential to the progress of the Information Technology (IT) research proposed here. This project addresses two key challenges for IT during this decade: moving from virtual to physical applications, and moving from macro to micro and nano.&lt;br/&gt;&lt;br/&gt;The IT focus is on the study of Physically-Coupled Scalable Information Infrastructures (PCSIIs), which effectively &quot;embed the internet&quot;. The sensors and actuators in the proposed PCSII must have small physical dimensions, comparable to those of the microorganisms to be monitored. They must be deployed in very large numbers to achieve the unprecedented spatial and temporal resolution necessary to investigate the causal relationships between environmental conditions and microorganisms. Control and coordination of a multitude of such devices of limited and heterogeneous capabilities raise major challenges for networking, distributed coordination and distributed algorithms. Sensing for detection and identification of microorganisms is another challenge, which will be tackled by using nanorobotic Scanning Probe Microscope technology.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121141</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121141</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n949" target="n950">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: AVENUE: Adaptable Voice Translation for Minority Languages</data>
      <data key="e_abstract">Our primary research goal is to develop a prototype voice-enabled&lt;br/&gt;translating communicator which will deliver information services across&lt;br/&gt;the linguistic divide for minority languages in order allow remote&lt;br/&gt;linguistically-diverse users to communicate directly with Internet&lt;br/&gt;content and databases, and more importantly to communicate with others&lt;br/&gt;speaking a different language from their own. The latter will enable&lt;br/&gt;information, education, and, for example, health services, to reach&lt;br/&gt;remote minority-language communities. Achieving this goal requires&lt;br/&gt;major advances in machine learning for translation and in cross-language&lt;br/&gt;speech-recognition adaptability to wider language phenomena.&lt;br/&gt;&lt;br/&gt;Traditional transfer-rule-based MT requires up to a person-century&lt;br/&gt;to build and perfect a new language pair. Statistical and&lt;br/&gt;Example-Based MT replaces human coding effort by vast amounts of&lt;br/&gt;bilingual training data, which are virtually unobtainable for most&lt;br/&gt;minority languages. Without a radical advance, leading to an&lt;br/&gt;over-an-order-of-magnitude improvement in development time, the&lt;br/&gt;only commercially justifiable MT applications involve the major&lt;br/&gt;European languages, Japanese, Chinese, Korean, Arabic and perhaps a&lt;br/&gt;couple more relatively-popular languages. The vast majority of human&lt;br/&gt;languages are currently relegated to the proverbial MT dust heap.&lt;br/&gt;&lt;br/&gt;We propose new MT approaches based on extended and new machine&lt;br/&gt;learning methods. The first approach consists of statistical MT&lt;br/&gt;methods that learn from orders of magnitude less training data,&lt;br/&gt;and that can more effectively incorporate prior linguistic information&lt;br/&gt;(including dictionaries, word classes, and known linguistic&lt;br/&gt;rule classes or constraints) by using the joint source-channel&lt;br/&gt;modeling approach combined with exponential (maximum entropy) models.&lt;br/&gt;The second approach is a new method for acquiring high-quality MT&lt;br/&gt;transfer rules from native informants which decreases dependence on&lt;br/&gt;human experts and reduces development time. Semantically-conditioned&lt;br/&gt;transfer rules are generalized via a new locally-constrained&lt;br/&gt;Seeded Version-Space method based on a controlled bilingual corpus&lt;br/&gt;and interactive tools to elicit information from native informants.&lt;br/&gt;The third method builds general phone models across multiple language&lt;br/&gt;families for speech recognition and adapts the recognizer to new&lt;br/&gt;languages with minimal new- language training data. All of these&lt;br/&gt;methods are based on new and existing machine learning algorithms&lt;br/&gt;that combine prior knowledge with limited amounts of new data in&lt;br/&gt;order to converge quickly on working machine translation and speech&lt;br/&gt;recognition and synthesis systems.&lt;br/&gt;&lt;br/&gt;The primary societal impact will be a significant contribution&lt;br/&gt;to the global democratization of informa- tion, a process that&lt;br/&gt;requires bridging current linguistic barriers, especially for&lt;br/&gt;low-density or economically- disadvantaged languages. Additionally,&lt;br/&gt;preservation and teaching of endangered languages will be directly&lt;br/&gt;enabled by the new linguistic and acoustic knowledge coupled with&lt;br/&gt;existing tutorial software. If successful, Avenue (Adaptable Voice-Enabled&lt;br/&gt;Natural-translator for Universal Empowerment) will be the prototype&lt;br/&gt;of an MT system that will empower world-wide access to multilingual&lt;br/&gt;information.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121631</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121631</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n949" target="n951">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: AVENUE: Adaptable Voice Translation for Minority Languages</data>
      <data key="e_abstract">Our primary research goal is to develop a prototype voice-enabled&lt;br/&gt;translating communicator which will deliver information services across&lt;br/&gt;the linguistic divide for minority languages in order allow remote&lt;br/&gt;linguistically-diverse users to communicate directly with Internet&lt;br/&gt;content and databases, and more importantly to communicate with others&lt;br/&gt;speaking a different language from their own. The latter will enable&lt;br/&gt;information, education, and, for example, health services, to reach&lt;br/&gt;remote minority-language communities. Achieving this goal requires&lt;br/&gt;major advances in machine learning for translation and in cross-language&lt;br/&gt;speech-recognition adaptability to wider language phenomena.&lt;br/&gt;&lt;br/&gt;Traditional transfer-rule-based MT requires up to a person-century&lt;br/&gt;to build and perfect a new language pair. Statistical and&lt;br/&gt;Example-Based MT replaces human coding effort by vast amounts of&lt;br/&gt;bilingual training data, which are virtually unobtainable for most&lt;br/&gt;minority languages. Without a radical advance, leading to an&lt;br/&gt;over-an-order-of-magnitude improvement in development time, the&lt;br/&gt;only commercially justifiable MT applications involve the major&lt;br/&gt;European languages, Japanese, Chinese, Korean, Arabic and perhaps a&lt;br/&gt;couple more relatively-popular languages. The vast majority of human&lt;br/&gt;languages are currently relegated to the proverbial MT dust heap.&lt;br/&gt;&lt;br/&gt;We propose new MT approaches based on extended and new machine&lt;br/&gt;learning methods. The first approach consists of statistical MT&lt;br/&gt;methods that learn from orders of magnitude less training data,&lt;br/&gt;and that can more effectively incorporate prior linguistic information&lt;br/&gt;(including dictionaries, word classes, and known linguistic&lt;br/&gt;rule classes or constraints) by using the joint source-channel&lt;br/&gt;modeling approach combined with exponential (maximum entropy) models.&lt;br/&gt;The second approach is a new method for acquiring high-quality MT&lt;br/&gt;transfer rules from native informants which decreases dependence on&lt;br/&gt;human experts and reduces development time. Semantically-conditioned&lt;br/&gt;transfer rules are generalized via a new locally-constrained&lt;br/&gt;Seeded Version-Space method based on a controlled bilingual corpus&lt;br/&gt;and interactive tools to elicit information from native informants.&lt;br/&gt;The third method builds general phone models across multiple language&lt;br/&gt;families for speech recognition and adapts the recognizer to new&lt;br/&gt;languages with minimal new- language training data. All of these&lt;br/&gt;methods are based on new and existing machine learning algorithms&lt;br/&gt;that combine prior knowledge with limited amounts of new data in&lt;br/&gt;order to converge quickly on working machine translation and speech&lt;br/&gt;recognition and synthesis systems.&lt;br/&gt;&lt;br/&gt;The primary societal impact will be a significant contribution&lt;br/&gt;to the global democratization of informa- tion, a process that&lt;br/&gt;requires bridging current linguistic barriers, especially for&lt;br/&gt;low-density or economically- disadvantaged languages. Additionally,&lt;br/&gt;preservation and teaching of endangered languages will be directly&lt;br/&gt;enabled by the new linguistic and acoustic knowledge coupled with&lt;br/&gt;existing tutorial software. If successful, Avenue (Adaptable Voice-Enabled&lt;br/&gt;Natural-translator for Universal Empowerment) will be the prototype&lt;br/&gt;of an MT system that will empower world-wide access to multilingual&lt;br/&gt;information.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121631</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121631</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n949" target="n952">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: AVENUE: Adaptable Voice Translation for Minority Languages</data>
      <data key="e_abstract">Our primary research goal is to develop a prototype voice-enabled&lt;br/&gt;translating communicator which will deliver information services across&lt;br/&gt;the linguistic divide for minority languages in order allow remote&lt;br/&gt;linguistically-diverse users to communicate directly with Internet&lt;br/&gt;content and databases, and more importantly to communicate with others&lt;br/&gt;speaking a different language from their own. The latter will enable&lt;br/&gt;information, education, and, for example, health services, to reach&lt;br/&gt;remote minority-language communities. Achieving this goal requires&lt;br/&gt;major advances in machine learning for translation and in cross-language&lt;br/&gt;speech-recognition adaptability to wider language phenomena.&lt;br/&gt;&lt;br/&gt;Traditional transfer-rule-based MT requires up to a person-century&lt;br/&gt;to build and perfect a new language pair. Statistical and&lt;br/&gt;Example-Based MT replaces human coding effort by vast amounts of&lt;br/&gt;bilingual training data, which are virtually unobtainable for most&lt;br/&gt;minority languages. Without a radical advance, leading to an&lt;br/&gt;over-an-order-of-magnitude improvement in development time, the&lt;br/&gt;only commercially justifiable MT applications involve the major&lt;br/&gt;European languages, Japanese, Chinese, Korean, Arabic and perhaps a&lt;br/&gt;couple more relatively-popular languages. The vast majority of human&lt;br/&gt;languages are currently relegated to the proverbial MT dust heap.&lt;br/&gt;&lt;br/&gt;We propose new MT approaches based on extended and new machine&lt;br/&gt;learning methods. The first approach consists of statistical MT&lt;br/&gt;methods that learn from orders of magnitude less training data,&lt;br/&gt;and that can more effectively incorporate prior linguistic information&lt;br/&gt;(including dictionaries, word classes, and known linguistic&lt;br/&gt;rule classes or constraints) by using the joint source-channel&lt;br/&gt;modeling approach combined with exponential (maximum entropy) models.&lt;br/&gt;The second approach is a new method for acquiring high-quality MT&lt;br/&gt;transfer rules from native informants which decreases dependence on&lt;br/&gt;human experts and reduces development time. Semantically-conditioned&lt;br/&gt;transfer rules are generalized via a new locally-constrained&lt;br/&gt;Seeded Version-Space method based on a controlled bilingual corpus&lt;br/&gt;and interactive tools to elicit information from native informants.&lt;br/&gt;The third method builds general phone models across multiple language&lt;br/&gt;families for speech recognition and adapts the recognizer to new&lt;br/&gt;languages with minimal new- language training data. All of these&lt;br/&gt;methods are based on new and existing machine learning algorithms&lt;br/&gt;that combine prior knowledge with limited amounts of new data in&lt;br/&gt;order to converge quickly on working machine translation and speech&lt;br/&gt;recognition and synthesis systems.&lt;br/&gt;&lt;br/&gt;The primary societal impact will be a significant contribution&lt;br/&gt;to the global democratization of informa- tion, a process that&lt;br/&gt;requires bridging current linguistic barriers, especially for&lt;br/&gt;low-density or economically- disadvantaged languages. Additionally,&lt;br/&gt;preservation and teaching of endangered languages will be directly&lt;br/&gt;enabled by the new linguistic and acoustic knowledge coupled with&lt;br/&gt;existing tutorial software. If successful, Avenue (Adaptable Voice-Enabled&lt;br/&gt;Natural-translator for Universal Empowerment) will be the prototype&lt;br/&gt;of an MT system that will empower world-wide access to multilingual&lt;br/&gt;information.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121631</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121631</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n949" target="n953">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: AVENUE: Adaptable Voice Translation for Minority Languages</data>
      <data key="e_abstract">Our primary research goal is to develop a prototype voice-enabled&lt;br/&gt;translating communicator which will deliver information services across&lt;br/&gt;the linguistic divide for minority languages in order allow remote&lt;br/&gt;linguistically-diverse users to communicate directly with Internet&lt;br/&gt;content and databases, and more importantly to communicate with others&lt;br/&gt;speaking a different language from their own. The latter will enable&lt;br/&gt;information, education, and, for example, health services, to reach&lt;br/&gt;remote minority-language communities. Achieving this goal requires&lt;br/&gt;major advances in machine learning for translation and in cross-language&lt;br/&gt;speech-recognition adaptability to wider language phenomena.&lt;br/&gt;&lt;br/&gt;Traditional transfer-rule-based MT requires up to a person-century&lt;br/&gt;to build and perfect a new language pair. Statistical and&lt;br/&gt;Example-Based MT replaces human coding effort by vast amounts of&lt;br/&gt;bilingual training data, which are virtually unobtainable for most&lt;br/&gt;minority languages. Without a radical advance, leading to an&lt;br/&gt;over-an-order-of-magnitude improvement in development time, the&lt;br/&gt;only commercially justifiable MT applications involve the major&lt;br/&gt;European languages, Japanese, Chinese, Korean, Arabic and perhaps a&lt;br/&gt;couple more relatively-popular languages. The vast majority of human&lt;br/&gt;languages are currently relegated to the proverbial MT dust heap.&lt;br/&gt;&lt;br/&gt;We propose new MT approaches based on extended and new machine&lt;br/&gt;learning methods. The first approach consists of statistical MT&lt;br/&gt;methods that learn from orders of magnitude less training data,&lt;br/&gt;and that can more effectively incorporate prior linguistic information&lt;br/&gt;(including dictionaries, word classes, and known linguistic&lt;br/&gt;rule classes or constraints) by using the joint source-channel&lt;br/&gt;modeling approach combined with exponential (maximum entropy) models.&lt;br/&gt;The second approach is a new method for acquiring high-quality MT&lt;br/&gt;transfer rules from native informants which decreases dependence on&lt;br/&gt;human experts and reduces development time. Semantically-conditioned&lt;br/&gt;transfer rules are generalized via a new locally-constrained&lt;br/&gt;Seeded Version-Space method based on a controlled bilingual corpus&lt;br/&gt;and interactive tools to elicit information from native informants.&lt;br/&gt;The third method builds general phone models across multiple language&lt;br/&gt;families for speech recognition and adapts the recognizer to new&lt;br/&gt;languages with minimal new- language training data. All of these&lt;br/&gt;methods are based on new and existing machine learning algorithms&lt;br/&gt;that combine prior knowledge with limited amounts of new data in&lt;br/&gt;order to converge quickly on working machine translation and speech&lt;br/&gt;recognition and synthesis systems.&lt;br/&gt;&lt;br/&gt;The primary societal impact will be a significant contribution&lt;br/&gt;to the global democratization of informa- tion, a process that&lt;br/&gt;requires bridging current linguistic barriers, especially for&lt;br/&gt;low-density or economically- disadvantaged languages. Additionally,&lt;br/&gt;preservation and teaching of endangered languages will be directly&lt;br/&gt;enabled by the new linguistic and acoustic knowledge coupled with&lt;br/&gt;existing tutorial software. If successful, Avenue (Adaptable Voice-Enabled&lt;br/&gt;Natural-translator for Universal Empowerment) will be the prototype&lt;br/&gt;of an MT system that will empower world-wide access to multilingual&lt;br/&gt;information.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121631</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121631</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n950" target="n951">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: AVENUE: Adaptable Voice Translation for Minority Languages</data>
      <data key="e_abstract">Our primary research goal is to develop a prototype voice-enabled&lt;br/&gt;translating communicator which will deliver information services across&lt;br/&gt;the linguistic divide for minority languages in order allow remote&lt;br/&gt;linguistically-diverse users to communicate directly with Internet&lt;br/&gt;content and databases, and more importantly to communicate with others&lt;br/&gt;speaking a different language from their own. The latter will enable&lt;br/&gt;information, education, and, for example, health services, to reach&lt;br/&gt;remote minority-language communities. Achieving this goal requires&lt;br/&gt;major advances in machine learning for translation and in cross-language&lt;br/&gt;speech-recognition adaptability to wider language phenomena.&lt;br/&gt;&lt;br/&gt;Traditional transfer-rule-based MT requires up to a person-century&lt;br/&gt;to build and perfect a new language pair. Statistical and&lt;br/&gt;Example-Based MT replaces human coding effort by vast amounts of&lt;br/&gt;bilingual training data, which are virtually unobtainable for most&lt;br/&gt;minority languages. Without a radical advance, leading to an&lt;br/&gt;over-an-order-of-magnitude improvement in development time, the&lt;br/&gt;only commercially justifiable MT applications involve the major&lt;br/&gt;European languages, Japanese, Chinese, Korean, Arabic and perhaps a&lt;br/&gt;couple more relatively-popular languages. The vast majority of human&lt;br/&gt;languages are currently relegated to the proverbial MT dust heap.&lt;br/&gt;&lt;br/&gt;We propose new MT approaches based on extended and new machine&lt;br/&gt;learning methods. The first approach consists of statistical MT&lt;br/&gt;methods that learn from orders of magnitude less training data,&lt;br/&gt;and that can more effectively incorporate prior linguistic information&lt;br/&gt;(including dictionaries, word classes, and known linguistic&lt;br/&gt;rule classes or constraints) by using the joint source-channel&lt;br/&gt;modeling approach combined with exponential (maximum entropy) models.&lt;br/&gt;The second approach is a new method for acquiring high-quality MT&lt;br/&gt;transfer rules from native informants which decreases dependence on&lt;br/&gt;human experts and reduces development time. Semantically-conditioned&lt;br/&gt;transfer rules are generalized via a new locally-constrained&lt;br/&gt;Seeded Version-Space method based on a controlled bilingual corpus&lt;br/&gt;and interactive tools to elicit information from native informants.&lt;br/&gt;The third method builds general phone models across multiple language&lt;br/&gt;families for speech recognition and adapts the recognizer to new&lt;br/&gt;languages with minimal new- language training data. All of these&lt;br/&gt;methods are based on new and existing machine learning algorithms&lt;br/&gt;that combine prior knowledge with limited amounts of new data in&lt;br/&gt;order to converge quickly on working machine translation and speech&lt;br/&gt;recognition and synthesis systems.&lt;br/&gt;&lt;br/&gt;The primary societal impact will be a significant contribution&lt;br/&gt;to the global democratization of informa- tion, a process that&lt;br/&gt;requires bridging current linguistic barriers, especially for&lt;br/&gt;low-density or economically- disadvantaged languages. Additionally,&lt;br/&gt;preservation and teaching of endangered languages will be directly&lt;br/&gt;enabled by the new linguistic and acoustic knowledge coupled with&lt;br/&gt;existing tutorial software. If successful, Avenue (Adaptable Voice-Enabled&lt;br/&gt;Natural-translator for Universal Empowerment) will be the prototype&lt;br/&gt;of an MT system that will empower world-wide access to multilingual&lt;br/&gt;information.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121631</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121631</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n950" target="n952">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: AVENUE: Adaptable Voice Translation for Minority Languages</data>
      <data key="e_abstract">Our primary research goal is to develop a prototype voice-enabled&lt;br/&gt;translating communicator which will deliver information services across&lt;br/&gt;the linguistic divide for minority languages in order allow remote&lt;br/&gt;linguistically-diverse users to communicate directly with Internet&lt;br/&gt;content and databases, and more importantly to communicate with others&lt;br/&gt;speaking a different language from their own. The latter will enable&lt;br/&gt;information, education, and, for example, health services, to reach&lt;br/&gt;remote minority-language communities. Achieving this goal requires&lt;br/&gt;major advances in machine learning for translation and in cross-language&lt;br/&gt;speech-recognition adaptability to wider language phenomena.&lt;br/&gt;&lt;br/&gt;Traditional transfer-rule-based MT requires up to a person-century&lt;br/&gt;to build and perfect a new language pair. Statistical and&lt;br/&gt;Example-Based MT replaces human coding effort by vast amounts of&lt;br/&gt;bilingual training data, which are virtually unobtainable for most&lt;br/&gt;minority languages. Without a radical advance, leading to an&lt;br/&gt;over-an-order-of-magnitude improvement in development time, the&lt;br/&gt;only commercially justifiable MT applications involve the major&lt;br/&gt;European languages, Japanese, Chinese, Korean, Arabic and perhaps a&lt;br/&gt;couple more relatively-popular languages. The vast majority of human&lt;br/&gt;languages are currently relegated to the proverbial MT dust heap.&lt;br/&gt;&lt;br/&gt;We propose new MT approaches based on extended and new machine&lt;br/&gt;learning methods. The first approach consists of statistical MT&lt;br/&gt;methods that learn from orders of magnitude less training data,&lt;br/&gt;and that can more effectively incorporate prior linguistic information&lt;br/&gt;(including dictionaries, word classes, and known linguistic&lt;br/&gt;rule classes or constraints) by using the joint source-channel&lt;br/&gt;modeling approach combined with exponential (maximum entropy) models.&lt;br/&gt;The second approach is a new method for acquiring high-quality MT&lt;br/&gt;transfer rules from native informants which decreases dependence on&lt;br/&gt;human experts and reduces development time. Semantically-conditioned&lt;br/&gt;transfer rules are generalized via a new locally-constrained&lt;br/&gt;Seeded Version-Space method based on a controlled bilingual corpus&lt;br/&gt;and interactive tools to elicit information from native informants.&lt;br/&gt;The third method builds general phone models across multiple language&lt;br/&gt;families for speech recognition and adapts the recognizer to new&lt;br/&gt;languages with minimal new- language training data. All of these&lt;br/&gt;methods are based on new and existing machine learning algorithms&lt;br/&gt;that combine prior knowledge with limited amounts of new data in&lt;br/&gt;order to converge quickly on working machine translation and speech&lt;br/&gt;recognition and synthesis systems.&lt;br/&gt;&lt;br/&gt;The primary societal impact will be a significant contribution&lt;br/&gt;to the global democratization of informa- tion, a process that&lt;br/&gt;requires bridging current linguistic barriers, especially for&lt;br/&gt;low-density or economically- disadvantaged languages. Additionally,&lt;br/&gt;preservation and teaching of endangered languages will be directly&lt;br/&gt;enabled by the new linguistic and acoustic knowledge coupled with&lt;br/&gt;existing tutorial software. If successful, Avenue (Adaptable Voice-Enabled&lt;br/&gt;Natural-translator for Universal Empowerment) will be the prototype&lt;br/&gt;of an MT system that will empower world-wide access to multilingual&lt;br/&gt;information.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121631</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121631</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n950" target="n953">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: AVENUE: Adaptable Voice Translation for Minority Languages</data>
      <data key="e_abstract">Our primary research goal is to develop a prototype voice-enabled&lt;br/&gt;translating communicator which will deliver information services across&lt;br/&gt;the linguistic divide for minority languages in order allow remote&lt;br/&gt;linguistically-diverse users to communicate directly with Internet&lt;br/&gt;content and databases, and more importantly to communicate with others&lt;br/&gt;speaking a different language from their own. The latter will enable&lt;br/&gt;information, education, and, for example, health services, to reach&lt;br/&gt;remote minority-language communities. Achieving this goal requires&lt;br/&gt;major advances in machine learning for translation and in cross-language&lt;br/&gt;speech-recognition adaptability to wider language phenomena.&lt;br/&gt;&lt;br/&gt;Traditional transfer-rule-based MT requires up to a person-century&lt;br/&gt;to build and perfect a new language pair. Statistical and&lt;br/&gt;Example-Based MT replaces human coding effort by vast amounts of&lt;br/&gt;bilingual training data, which are virtually unobtainable for most&lt;br/&gt;minority languages. Without a radical advance, leading to an&lt;br/&gt;over-an-order-of-magnitude improvement in development time, the&lt;br/&gt;only commercially justifiable MT applications involve the major&lt;br/&gt;European languages, Japanese, Chinese, Korean, Arabic and perhaps a&lt;br/&gt;couple more relatively-popular languages. The vast majority of human&lt;br/&gt;languages are currently relegated to the proverbial MT dust heap.&lt;br/&gt;&lt;br/&gt;We propose new MT approaches based on extended and new machine&lt;br/&gt;learning methods. The first approach consists of statistical MT&lt;br/&gt;methods that learn from orders of magnitude less training data,&lt;br/&gt;and that can more effectively incorporate prior linguistic information&lt;br/&gt;(including dictionaries, word classes, and known linguistic&lt;br/&gt;rule classes or constraints) by using the joint source-channel&lt;br/&gt;modeling approach combined with exponential (maximum entropy) models.&lt;br/&gt;The second approach is a new method for acquiring high-quality MT&lt;br/&gt;transfer rules from native informants which decreases dependence on&lt;br/&gt;human experts and reduces development time. Semantically-conditioned&lt;br/&gt;transfer rules are generalized via a new locally-constrained&lt;br/&gt;Seeded Version-Space method based on a controlled bilingual corpus&lt;br/&gt;and interactive tools to elicit information from native informants.&lt;br/&gt;The third method builds general phone models across multiple language&lt;br/&gt;families for speech recognition and adapts the recognizer to new&lt;br/&gt;languages with minimal new- language training data. All of these&lt;br/&gt;methods are based on new and existing machine learning algorithms&lt;br/&gt;that combine prior knowledge with limited amounts of new data in&lt;br/&gt;order to converge quickly on working machine translation and speech&lt;br/&gt;recognition and synthesis systems.&lt;br/&gt;&lt;br/&gt;The primary societal impact will be a significant contribution&lt;br/&gt;to the global democratization of informa- tion, a process that&lt;br/&gt;requires bridging current linguistic barriers, especially for&lt;br/&gt;low-density or economically- disadvantaged languages. Additionally,&lt;br/&gt;preservation and teaching of endangered languages will be directly&lt;br/&gt;enabled by the new linguistic and acoustic knowledge coupled with&lt;br/&gt;existing tutorial software. If successful, Avenue (Adaptable Voice-Enabled&lt;br/&gt;Natural-translator for Universal Empowerment) will be the prototype&lt;br/&gt;of an MT system that will empower world-wide access to multilingual&lt;br/&gt;information.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121631</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121631</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n951" target="n952">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: AVENUE: Adaptable Voice Translation for Minority Languages</data>
      <data key="e_abstract">Our primary research goal is to develop a prototype voice-enabled&lt;br/&gt;translating communicator which will deliver information services across&lt;br/&gt;the linguistic divide for minority languages in order allow remote&lt;br/&gt;linguistically-diverse users to communicate directly with Internet&lt;br/&gt;content and databases, and more importantly to communicate with others&lt;br/&gt;speaking a different language from their own. The latter will enable&lt;br/&gt;information, education, and, for example, health services, to reach&lt;br/&gt;remote minority-language communities. Achieving this goal requires&lt;br/&gt;major advances in machine learning for translation and in cross-language&lt;br/&gt;speech-recognition adaptability to wider language phenomena.&lt;br/&gt;&lt;br/&gt;Traditional transfer-rule-based MT requires up to a person-century&lt;br/&gt;to build and perfect a new language pair. Statistical and&lt;br/&gt;Example-Based MT replaces human coding effort by vast amounts of&lt;br/&gt;bilingual training data, which are virtually unobtainable for most&lt;br/&gt;minority languages. Without a radical advance, leading to an&lt;br/&gt;over-an-order-of-magnitude improvement in development time, the&lt;br/&gt;only commercially justifiable MT applications involve the major&lt;br/&gt;European languages, Japanese, Chinese, Korean, Arabic and perhaps a&lt;br/&gt;couple more relatively-popular languages. The vast majority of human&lt;br/&gt;languages are currently relegated to the proverbial MT dust heap.&lt;br/&gt;&lt;br/&gt;We propose new MT approaches based on extended and new machine&lt;br/&gt;learning methods. The first approach consists of statistical MT&lt;br/&gt;methods that learn from orders of magnitude less training data,&lt;br/&gt;and that can more effectively incorporate prior linguistic information&lt;br/&gt;(including dictionaries, word classes, and known linguistic&lt;br/&gt;rule classes or constraints) by using the joint source-channel&lt;br/&gt;modeling approach combined with exponential (maximum entropy) models.&lt;br/&gt;The second approach is a new method for acquiring high-quality MT&lt;br/&gt;transfer rules from native informants which decreases dependence on&lt;br/&gt;human experts and reduces development time. Semantically-conditioned&lt;br/&gt;transfer rules are generalized via a new locally-constrained&lt;br/&gt;Seeded Version-Space method based on a controlled bilingual corpus&lt;br/&gt;and interactive tools to elicit information from native informants.&lt;br/&gt;The third method builds general phone models across multiple language&lt;br/&gt;families for speech recognition and adapts the recognizer to new&lt;br/&gt;languages with minimal new- language training data. All of these&lt;br/&gt;methods are based on new and existing machine learning algorithms&lt;br/&gt;that combine prior knowledge with limited amounts of new data in&lt;br/&gt;order to converge quickly on working machine translation and speech&lt;br/&gt;recognition and synthesis systems.&lt;br/&gt;&lt;br/&gt;The primary societal impact will be a significant contribution&lt;br/&gt;to the global democratization of informa- tion, a process that&lt;br/&gt;requires bridging current linguistic barriers, especially for&lt;br/&gt;low-density or economically- disadvantaged languages. Additionally,&lt;br/&gt;preservation and teaching of endangered languages will be directly&lt;br/&gt;enabled by the new linguistic and acoustic knowledge coupled with&lt;br/&gt;existing tutorial software. If successful, Avenue (Adaptable Voice-Enabled&lt;br/&gt;Natural-translator for Universal Empowerment) will be the prototype&lt;br/&gt;of an MT system that will empower world-wide access to multilingual&lt;br/&gt;information.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121631</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121631</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n951" target="n953">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: AVENUE: Adaptable Voice Translation for Minority Languages</data>
      <data key="e_abstract">Our primary research goal is to develop a prototype voice-enabled&lt;br/&gt;translating communicator which will deliver information services across&lt;br/&gt;the linguistic divide for minority languages in order allow remote&lt;br/&gt;linguistically-diverse users to communicate directly with Internet&lt;br/&gt;content and databases, and more importantly to communicate with others&lt;br/&gt;speaking a different language from their own. The latter will enable&lt;br/&gt;information, education, and, for example, health services, to reach&lt;br/&gt;remote minority-language communities. Achieving this goal requires&lt;br/&gt;major advances in machine learning for translation and in cross-language&lt;br/&gt;speech-recognition adaptability to wider language phenomena.&lt;br/&gt;&lt;br/&gt;Traditional transfer-rule-based MT requires up to a person-century&lt;br/&gt;to build and perfect a new language pair. Statistical and&lt;br/&gt;Example-Based MT replaces human coding effort by vast amounts of&lt;br/&gt;bilingual training data, which are virtually unobtainable for most&lt;br/&gt;minority languages. Without a radical advance, leading to an&lt;br/&gt;over-an-order-of-magnitude improvement in development time, the&lt;br/&gt;only commercially justifiable MT applications involve the major&lt;br/&gt;European languages, Japanese, Chinese, Korean, Arabic and perhaps a&lt;br/&gt;couple more relatively-popular languages. The vast majority of human&lt;br/&gt;languages are currently relegated to the proverbial MT dust heap.&lt;br/&gt;&lt;br/&gt;We propose new MT approaches based on extended and new machine&lt;br/&gt;learning methods. The first approach consists of statistical MT&lt;br/&gt;methods that learn from orders of magnitude less training data,&lt;br/&gt;and that can more effectively incorporate prior linguistic information&lt;br/&gt;(including dictionaries, word classes, and known linguistic&lt;br/&gt;rule classes or constraints) by using the joint source-channel&lt;br/&gt;modeling approach combined with exponential (maximum entropy) models.&lt;br/&gt;The second approach is a new method for acquiring high-quality MT&lt;br/&gt;transfer rules from native informants which decreases dependence on&lt;br/&gt;human experts and reduces development time. Semantically-conditioned&lt;br/&gt;transfer rules are generalized via a new locally-constrained&lt;br/&gt;Seeded Version-Space method based on a controlled bilingual corpus&lt;br/&gt;and interactive tools to elicit information from native informants.&lt;br/&gt;The third method builds general phone models across multiple language&lt;br/&gt;families for speech recognition and adapts the recognizer to new&lt;br/&gt;languages with minimal new- language training data. All of these&lt;br/&gt;methods are based on new and existing machine learning algorithms&lt;br/&gt;that combine prior knowledge with limited amounts of new data in&lt;br/&gt;order to converge quickly on working machine translation and speech&lt;br/&gt;recognition and synthesis systems.&lt;br/&gt;&lt;br/&gt;The primary societal impact will be a significant contribution&lt;br/&gt;to the global democratization of informa- tion, a process that&lt;br/&gt;requires bridging current linguistic barriers, especially for&lt;br/&gt;low-density or economically- disadvantaged languages. Additionally,&lt;br/&gt;preservation and teaching of endangered languages will be directly&lt;br/&gt;enabled by the new linguistic and acoustic knowledge coupled with&lt;br/&gt;existing tutorial software. If successful, Avenue (Adaptable Voice-Enabled&lt;br/&gt;Natural-translator for Universal Empowerment) will be the prototype&lt;br/&gt;of an MT system that will empower world-wide access to multilingual&lt;br/&gt;information.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121631</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121631</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n952" target="n953">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: AVENUE: Adaptable Voice Translation for Minority Languages</data>
      <data key="e_abstract">Our primary research goal is to develop a prototype voice-enabled&lt;br/&gt;translating communicator which will deliver information services across&lt;br/&gt;the linguistic divide for minority languages in order allow remote&lt;br/&gt;linguistically-diverse users to communicate directly with Internet&lt;br/&gt;content and databases, and more importantly to communicate with others&lt;br/&gt;speaking a different language from their own. The latter will enable&lt;br/&gt;information, education, and, for example, health services, to reach&lt;br/&gt;remote minority-language communities. Achieving this goal requires&lt;br/&gt;major advances in machine learning for translation and in cross-language&lt;br/&gt;speech-recognition adaptability to wider language phenomena.&lt;br/&gt;&lt;br/&gt;Traditional transfer-rule-based MT requires up to a person-century&lt;br/&gt;to build and perfect a new language pair. Statistical and&lt;br/&gt;Example-Based MT replaces human coding effort by vast amounts of&lt;br/&gt;bilingual training data, which are virtually unobtainable for most&lt;br/&gt;minority languages. Without a radical advance, leading to an&lt;br/&gt;over-an-order-of-magnitude improvement in development time, the&lt;br/&gt;only commercially justifiable MT applications involve the major&lt;br/&gt;European languages, Japanese, Chinese, Korean, Arabic and perhaps a&lt;br/&gt;couple more relatively-popular languages. The vast majority of human&lt;br/&gt;languages are currently relegated to the proverbial MT dust heap.&lt;br/&gt;&lt;br/&gt;We propose new MT approaches based on extended and new machine&lt;br/&gt;learning methods. The first approach consists of statistical MT&lt;br/&gt;methods that learn from orders of magnitude less training data,&lt;br/&gt;and that can more effectively incorporate prior linguistic information&lt;br/&gt;(including dictionaries, word classes, and known linguistic&lt;br/&gt;rule classes or constraints) by using the joint source-channel&lt;br/&gt;modeling approach combined with exponential (maximum entropy) models.&lt;br/&gt;The second approach is a new method for acquiring high-quality MT&lt;br/&gt;transfer rules from native informants which decreases dependence on&lt;br/&gt;human experts and reduces development time. Semantically-conditioned&lt;br/&gt;transfer rules are generalized via a new locally-constrained&lt;br/&gt;Seeded Version-Space method based on a controlled bilingual corpus&lt;br/&gt;and interactive tools to elicit information from native informants.&lt;br/&gt;The third method builds general phone models across multiple language&lt;br/&gt;families for speech recognition and adapts the recognizer to new&lt;br/&gt;languages with minimal new- language training data. All of these&lt;br/&gt;methods are based on new and existing machine learning algorithms&lt;br/&gt;that combine prior knowledge with limited amounts of new data in&lt;br/&gt;order to converge quickly on working machine translation and speech&lt;br/&gt;recognition and synthesis systems.&lt;br/&gt;&lt;br/&gt;The primary societal impact will be a significant contribution&lt;br/&gt;to the global democratization of informa- tion, a process that&lt;br/&gt;requires bridging current linguistic barriers, especially for&lt;br/&gt;low-density or economically- disadvantaged languages. Additionally,&lt;br/&gt;preservation and teaching of endangered languages will be directly&lt;br/&gt;enabled by the new linguistic and acoustic knowledge coupled with&lt;br/&gt;existing tutorial software. If successful, Avenue (Adaptable Voice-Enabled&lt;br/&gt;Natural-translator for Universal Empowerment) will be the prototype&lt;br/&gt;of an MT system that will empower world-wide access to multilingual&lt;br/&gt;information.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121631</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121631</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n956" target="n957">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY: A Distributed Programming Infrastructure for Integrating Smart Sensors</data>
      <data key="e_abstract">Real-time, reactive and embedded systems are widely and increasingly used throughout society (e.g., flight control, railway signaling, vehicle management systems, medical devices). This trend is likely to continue, as applications that would have been unthinkable only a few short years ago come into the reach of ever more complex processors. Many such applications are long lived, interact with their environment continuously, and are under important real-time constraints. As these reactive systems permeate our lives, bringing us everything from intelligent pace-makers to tiny freshness-tracking devices in groceries, the need for cost-effective, confidence-inspiring software validation techniques grows proportionately. &lt;br/&gt;&lt;br/&gt;This project focuses on building new tools for checking a common class of reactive real-time systems known as interrupt-driven systems. The proposed research has four facets that complement and support each other. First, this effort will build on preliminary work in analyzing seven commercial microcontrollers to identify a static timing analysis that is sufficiently precise for a single interrupt handler. Second, ways to specify and check timing properties for multiple interrupt handlers will be investigated. Third, a typed assembly language will be designed with time bounds in which timing properties can be specified in a modular way, one handler at a time. Fourth, a timed interrupt-handler calculus will be designed that will embody the results in a language-independent way and make it tractable to prove key properties. The new tools will automatically derive a model of the software by static analysis and type checking, and submit the result to a model checker. The tools can lead to significantly reduced testing requirements, and provide support for maintenance throughout the system life-cycle.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121638</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121638</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n956" target="n958">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY: A Distributed Programming Infrastructure for Integrating Smart Sensors</data>
      <data key="e_abstract">Real-time, reactive and embedded systems are widely and increasingly used throughout society (e.g., flight control, railway signaling, vehicle management systems, medical devices). This trend is likely to continue, as applications that would have been unthinkable only a few short years ago come into the reach of ever more complex processors. Many such applications are long lived, interact with their environment continuously, and are under important real-time constraints. As these reactive systems permeate our lives, bringing us everything from intelligent pace-makers to tiny freshness-tracking devices in groceries, the need for cost-effective, confidence-inspiring software validation techniques grows proportionately. &lt;br/&gt;&lt;br/&gt;This project focuses on building new tools for checking a common class of reactive real-time systems known as interrupt-driven systems. The proposed research has four facets that complement and support each other. First, this effort will build on preliminary work in analyzing seven commercial microcontrollers to identify a static timing analysis that is sufficiently precise for a single interrupt handler. Second, ways to specify and check timing properties for multiple interrupt handlers will be investigated. Third, a typed assembly language will be designed with time bounds in which timing properties can be specified in a modular way, one handler at a time. Fourth, a timed interrupt-handler calculus will be designed that will embody the results in a language-independent way and make it tractable to prove key properties. The new tools will automatically derive a model of the software by static analysis and type checking, and submit the result to a model checker. The tools can lead to significantly reduced testing requirements, and provide support for maintenance throughout the system life-cycle.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121638</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121638</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n139" target="n956">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY: A Distributed Programming Infrastructure for Integrating Smart Sensors</data>
      <data key="e_abstract">Real-time, reactive and embedded systems are widely and increasingly used throughout society (e.g., flight control, railway signaling, vehicle management systems, medical devices). This trend is likely to continue, as applications that would have been unthinkable only a few short years ago come into the reach of ever more complex processors. Many such applications are long lived, interact with their environment continuously, and are under important real-time constraints. As these reactive systems permeate our lives, bringing us everything from intelligent pace-makers to tiny freshness-tracking devices in groceries, the need for cost-effective, confidence-inspiring software validation techniques grows proportionately. &lt;br/&gt;&lt;br/&gt;This project focuses on building new tools for checking a common class of reactive real-time systems known as interrupt-driven systems. The proposed research has four facets that complement and support each other. First, this effort will build on preliminary work in analyzing seven commercial microcontrollers to identify a static timing analysis that is sufficiently precise for a single interrupt handler. Second, ways to specify and check timing properties for multiple interrupt handlers will be investigated. Third, a typed assembly language will be designed with time bounds in which timing properties can be specified in a modular way, one handler at a time. Fourth, a timed interrupt-handler calculus will be designed that will embody the results in a language-independent way and make it tractable to prove key properties. The new tools will automatically derive a model of the software by static analysis and type checking, and submit the result to a model checker. The tools can lead to significantly reduced testing requirements, and provide support for maintenance throughout the system life-cycle.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121638</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121638</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n957" target="n958">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY: A Distributed Programming Infrastructure for Integrating Smart Sensors</data>
      <data key="e_abstract">Real-time, reactive and embedded systems are widely and increasingly used throughout society (e.g., flight control, railway signaling, vehicle management systems, medical devices). This trend is likely to continue, as applications that would have been unthinkable only a few short years ago come into the reach of ever more complex processors. Many such applications are long lived, interact with their environment continuously, and are under important real-time constraints. As these reactive systems permeate our lives, bringing us everything from intelligent pace-makers to tiny freshness-tracking devices in groceries, the need for cost-effective, confidence-inspiring software validation techniques grows proportionately. &lt;br/&gt;&lt;br/&gt;This project focuses on building new tools for checking a common class of reactive real-time systems known as interrupt-driven systems. The proposed research has four facets that complement and support each other. First, this effort will build on preliminary work in analyzing seven commercial microcontrollers to identify a static timing analysis that is sufficiently precise for a single interrupt handler. Second, ways to specify and check timing properties for multiple interrupt handlers will be investigated. Third, a typed assembly language will be designed with time bounds in which timing properties can be specified in a modular way, one handler at a time. Fourth, a timed interrupt-handler calculus will be designed that will embody the results in a language-independent way and make it tractable to prove key properties. The new tools will automatically derive a model of the software by static analysis and type checking, and submit the result to a model checker. The tools can lead to significantly reduced testing requirements, and provide support for maintenance throughout the system life-cycle.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121638</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121638</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n139" target="n957">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY: A Distributed Programming Infrastructure for Integrating Smart Sensors</data>
      <data key="e_abstract">Real-time, reactive and embedded systems are widely and increasingly used throughout society (e.g., flight control, railway signaling, vehicle management systems, medical devices). This trend is likely to continue, as applications that would have been unthinkable only a few short years ago come into the reach of ever more complex processors. Many such applications are long lived, interact with their environment continuously, and are under important real-time constraints. As these reactive systems permeate our lives, bringing us everything from intelligent pace-makers to tiny freshness-tracking devices in groceries, the need for cost-effective, confidence-inspiring software validation techniques grows proportionately. &lt;br/&gt;&lt;br/&gt;This project focuses on building new tools for checking a common class of reactive real-time systems known as interrupt-driven systems. The proposed research has four facets that complement and support each other. First, this effort will build on preliminary work in analyzing seven commercial microcontrollers to identify a static timing analysis that is sufficiently precise for a single interrupt handler. Second, ways to specify and check timing properties for multiple interrupt handlers will be investigated. Third, a typed assembly language will be designed with time bounds in which timing properties can be specified in a modular way, one handler at a time. Fourth, a timed interrupt-handler calculus will be designed that will embody the results in a language-independent way and make it tractable to prove key properties. The new tools will automatically derive a model of the software by static analysis and type checking, and submit the result to a model checker. The tools can lead to significantly reduced testing requirements, and provide support for maintenance throughout the system life-cycle.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121638</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121638</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n139" target="n958">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY: A Distributed Programming Infrastructure for Integrating Smart Sensors</data>
      <data key="e_abstract">Real-time, reactive and embedded systems are widely and increasingly used throughout society (e.g., flight control, railway signaling, vehicle management systems, medical devices). This trend is likely to continue, as applications that would have been unthinkable only a few short years ago come into the reach of ever more complex processors. Many such applications are long lived, interact with their environment continuously, and are under important real-time constraints. As these reactive systems permeate our lives, bringing us everything from intelligent pace-makers to tiny freshness-tracking devices in groceries, the need for cost-effective, confidence-inspiring software validation techniques grows proportionately. &lt;br/&gt;&lt;br/&gt;This project focuses on building new tools for checking a common class of reactive real-time systems known as interrupt-driven systems. The proposed research has four facets that complement and support each other. First, this effort will build on preliminary work in analyzing seven commercial microcontrollers to identify a static timing analysis that is sufficiently precise for a single interrupt handler. Second, ways to specify and check timing properties for multiple interrupt handlers will be investigated. Third, a typed assembly language will be designed with time bounds in which timing properties can be specified in a modular way, one handler at a time. Fourth, a timed interrupt-handler calculus will be designed that will embody the results in a language-independent way and make it tractable to prove key properties. The new tools will automatically derive a model of the software by static analysis and type checking, and submit the result to a model checker. The tools can lead to significantly reduced testing requirements, and provide support for maintenance throughout the system life-cycle.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121638</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121638</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n762" target="n962">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY+SI: Language Technology for Trustless Software Dissemination</data>
      <data key="e_abstract">CR-0121633&lt;br/&gt;ITR/SY+SI: Language Technology for Trustless Software Dissemination&lt;br/&gt;Karl Crary, Robert Harper, Peter Lee, Frank Pfenning&lt;br/&gt;&lt;br/&gt;ABSTRACT:&lt;br/&gt;&lt;br/&gt;The project investigates the theoretical and engineering basis for &lt;br/&gt;the trustless dissemination of software in an untrusted environment. &lt;br/&gt;To make this possible the project investigates machine-checkable&lt;br/&gt;certificates of compliance with security, integrity, and privacy&lt;br/&gt;requirements. Such checkable certificates allow participants to&lt;br/&gt;verify the intrinsic properties of disseminated software, rather than&lt;br/&gt;extrinsic properties such as the software&apos;s point of origin.&lt;br/&gt;&lt;br/&gt;To obtain checkable certificates the project develops certifying&lt;br/&gt;compilers that equip their object code with formal representations &lt;br/&gt;of proofs of properties of the code. Specifically, the project&lt;br/&gt;investigates the use of proof-carrying code, typed intermediate&lt;br/&gt;languages, and typed assembly languages for this purpose. In each&lt;br/&gt;case certificate verification is reduced to type-checking in a&lt;br/&gt;suitable type system.&lt;br/&gt;&lt;br/&gt;To demonstrate the utility of trustless software dissemination, the&lt;br/&gt;project develops an infrastructure for building applications that&lt;br/&gt;exploit the computational resources of a network of computers. The&lt;br/&gt;infrastructure consists of a &quot;steward&quot; running on host computers that&lt;br/&gt;accepts and verifies certified binaries before installing and&lt;br/&gt;executing them, and certifying compilers that generate certified&lt;br/&gt;binaries for distribution on the network.&lt;br/&gt;&lt;br/&gt;The scope of the investigation includes the theory of specification&lt;br/&gt;and certification, and the systems building required to implement&lt;br/&gt;these ideas.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121633</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121633</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n762" target="n963">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY+SI: Language Technology for Trustless Software Dissemination</data>
      <data key="e_abstract">CR-0121633&lt;br/&gt;ITR/SY+SI: Language Technology for Trustless Software Dissemination&lt;br/&gt;Karl Crary, Robert Harper, Peter Lee, Frank Pfenning&lt;br/&gt;&lt;br/&gt;ABSTRACT:&lt;br/&gt;&lt;br/&gt;The project investigates the theoretical and engineering basis for &lt;br/&gt;the trustless dissemination of software in an untrusted environment. &lt;br/&gt;To make this possible the project investigates machine-checkable&lt;br/&gt;certificates of compliance with security, integrity, and privacy&lt;br/&gt;requirements. Such checkable certificates allow participants to&lt;br/&gt;verify the intrinsic properties of disseminated software, rather than&lt;br/&gt;extrinsic properties such as the software&apos;s point of origin.&lt;br/&gt;&lt;br/&gt;To obtain checkable certificates the project develops certifying&lt;br/&gt;compilers that equip their object code with formal representations &lt;br/&gt;of proofs of properties of the code. Specifically, the project&lt;br/&gt;investigates the use of proof-carrying code, typed intermediate&lt;br/&gt;languages, and typed assembly languages for this purpose. In each&lt;br/&gt;case certificate verification is reduced to type-checking in a&lt;br/&gt;suitable type system.&lt;br/&gt;&lt;br/&gt;To demonstrate the utility of trustless software dissemination, the&lt;br/&gt;project develops an infrastructure for building applications that&lt;br/&gt;exploit the computational resources of a network of computers. The&lt;br/&gt;infrastructure consists of a &quot;steward&quot; running on host computers that&lt;br/&gt;accepts and verifies certified binaries before installing and&lt;br/&gt;executing them, and certifying compilers that generate certified&lt;br/&gt;binaries for distribution on the network.&lt;br/&gt;&lt;br/&gt;The scope of the investigation includes the theory of specification&lt;br/&gt;and certification, and the systems building required to implement&lt;br/&gt;these ideas.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121633</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121633</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n762" target="n964">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY+SI: Language Technology for Trustless Software Dissemination</data>
      <data key="e_abstract">CR-0121633&lt;br/&gt;ITR/SY+SI: Language Technology for Trustless Software Dissemination&lt;br/&gt;Karl Crary, Robert Harper, Peter Lee, Frank Pfenning&lt;br/&gt;&lt;br/&gt;ABSTRACT:&lt;br/&gt;&lt;br/&gt;The project investigates the theoretical and engineering basis for &lt;br/&gt;the trustless dissemination of software in an untrusted environment. &lt;br/&gt;To make this possible the project investigates machine-checkable&lt;br/&gt;certificates of compliance with security, integrity, and privacy&lt;br/&gt;requirements. Such checkable certificates allow participants to&lt;br/&gt;verify the intrinsic properties of disseminated software, rather than&lt;br/&gt;extrinsic properties such as the software&apos;s point of origin.&lt;br/&gt;&lt;br/&gt;To obtain checkable certificates the project develops certifying&lt;br/&gt;compilers that equip their object code with formal representations &lt;br/&gt;of proofs of properties of the code. Specifically, the project&lt;br/&gt;investigates the use of proof-carrying code, typed intermediate&lt;br/&gt;languages, and typed assembly languages for this purpose. In each&lt;br/&gt;case certificate verification is reduced to type-checking in a&lt;br/&gt;suitable type system.&lt;br/&gt;&lt;br/&gt;To demonstrate the utility of trustless software dissemination, the&lt;br/&gt;project develops an infrastructure for building applications that&lt;br/&gt;exploit the computational resources of a network of computers. The&lt;br/&gt;infrastructure consists of a &quot;steward&quot; running on host computers that&lt;br/&gt;accepts and verifies certified binaries before installing and&lt;br/&gt;executing them, and certifying compilers that generate certified&lt;br/&gt;binaries for distribution on the network.&lt;br/&gt;&lt;br/&gt;The scope of the investigation includes the theory of specification&lt;br/&gt;and certification, and the systems building required to implement&lt;br/&gt;these ideas.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121633</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121633</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n962" target="n963">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY+SI: Language Technology for Trustless Software Dissemination</data>
      <data key="e_abstract">CR-0121633&lt;br/&gt;ITR/SY+SI: Language Technology for Trustless Software Dissemination&lt;br/&gt;Karl Crary, Robert Harper, Peter Lee, Frank Pfenning&lt;br/&gt;&lt;br/&gt;ABSTRACT:&lt;br/&gt;&lt;br/&gt;The project investigates the theoretical and engineering basis for &lt;br/&gt;the trustless dissemination of software in an untrusted environment. &lt;br/&gt;To make this possible the project investigates machine-checkable&lt;br/&gt;certificates of compliance with security, integrity, and privacy&lt;br/&gt;requirements. Such checkable certificates allow participants to&lt;br/&gt;verify the intrinsic properties of disseminated software, rather than&lt;br/&gt;extrinsic properties such as the software&apos;s point of origin.&lt;br/&gt;&lt;br/&gt;To obtain checkable certificates the project develops certifying&lt;br/&gt;compilers that equip their object code with formal representations &lt;br/&gt;of proofs of properties of the code. Specifically, the project&lt;br/&gt;investigates the use of proof-carrying code, typed intermediate&lt;br/&gt;languages, and typed assembly languages for this purpose. In each&lt;br/&gt;case certificate verification is reduced to type-checking in a&lt;br/&gt;suitable type system.&lt;br/&gt;&lt;br/&gt;To demonstrate the utility of trustless software dissemination, the&lt;br/&gt;project develops an infrastructure for building applications that&lt;br/&gt;exploit the computational resources of a network of computers. The&lt;br/&gt;infrastructure consists of a &quot;steward&quot; running on host computers that&lt;br/&gt;accepts and verifies certified binaries before installing and&lt;br/&gt;executing them, and certifying compilers that generate certified&lt;br/&gt;binaries for distribution on the network.&lt;br/&gt;&lt;br/&gt;The scope of the investigation includes the theory of specification&lt;br/&gt;and certification, and the systems building required to implement&lt;br/&gt;these ideas.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121633</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121633</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n962" target="n964">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY+SI: Language Technology for Trustless Software Dissemination</data>
      <data key="e_abstract">CR-0121633&lt;br/&gt;ITR/SY+SI: Language Technology for Trustless Software Dissemination&lt;br/&gt;Karl Crary, Robert Harper, Peter Lee, Frank Pfenning&lt;br/&gt;&lt;br/&gt;ABSTRACT:&lt;br/&gt;&lt;br/&gt;The project investigates the theoretical and engineering basis for &lt;br/&gt;the trustless dissemination of software in an untrusted environment. &lt;br/&gt;To make this possible the project investigates machine-checkable&lt;br/&gt;certificates of compliance with security, integrity, and privacy&lt;br/&gt;requirements. Such checkable certificates allow participants to&lt;br/&gt;verify the intrinsic properties of disseminated software, rather than&lt;br/&gt;extrinsic properties such as the software&apos;s point of origin.&lt;br/&gt;&lt;br/&gt;To obtain checkable certificates the project develops certifying&lt;br/&gt;compilers that equip their object code with formal representations &lt;br/&gt;of proofs of properties of the code. Specifically, the project&lt;br/&gt;investigates the use of proof-carrying code, typed intermediate&lt;br/&gt;languages, and typed assembly languages for this purpose. In each&lt;br/&gt;case certificate verification is reduced to type-checking in a&lt;br/&gt;suitable type system.&lt;br/&gt;&lt;br/&gt;To demonstrate the utility of trustless software dissemination, the&lt;br/&gt;project develops an infrastructure for building applications that&lt;br/&gt;exploit the computational resources of a network of computers. The&lt;br/&gt;infrastructure consists of a &quot;steward&quot; running on host computers that&lt;br/&gt;accepts and verifies certified binaries before installing and&lt;br/&gt;executing them, and certifying compilers that generate certified&lt;br/&gt;binaries for distribution on the network.&lt;br/&gt;&lt;br/&gt;The scope of the investigation includes the theory of specification&lt;br/&gt;and certification, and the systems building required to implement&lt;br/&gt;these ideas.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121633</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121633</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n963" target="n964">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY+SI: Language Technology for Trustless Software Dissemination</data>
      <data key="e_abstract">CR-0121633&lt;br/&gt;ITR/SY+SI: Language Technology for Trustless Software Dissemination&lt;br/&gt;Karl Crary, Robert Harper, Peter Lee, Frank Pfenning&lt;br/&gt;&lt;br/&gt;ABSTRACT:&lt;br/&gt;&lt;br/&gt;The project investigates the theoretical and engineering basis for &lt;br/&gt;the trustless dissemination of software in an untrusted environment. &lt;br/&gt;To make this possible the project investigates machine-checkable&lt;br/&gt;certificates of compliance with security, integrity, and privacy&lt;br/&gt;requirements. Such checkable certificates allow participants to&lt;br/&gt;verify the intrinsic properties of disseminated software, rather than&lt;br/&gt;extrinsic properties such as the software&apos;s point of origin.&lt;br/&gt;&lt;br/&gt;To obtain checkable certificates the project develops certifying&lt;br/&gt;compilers that equip their object code with formal representations &lt;br/&gt;of proofs of properties of the code. Specifically, the project&lt;br/&gt;investigates the use of proof-carrying code, typed intermediate&lt;br/&gt;languages, and typed assembly languages for this purpose. In each&lt;br/&gt;case certificate verification is reduced to type-checking in a&lt;br/&gt;suitable type system.&lt;br/&gt;&lt;br/&gt;To demonstrate the utility of trustless software dissemination, the&lt;br/&gt;project develops an infrastructure for building applications that&lt;br/&gt;exploit the computational resources of a network of computers. The&lt;br/&gt;infrastructure consists of a &quot;steward&quot; running on host computers that&lt;br/&gt;accepts and verifies certified binaries before installing and&lt;br/&gt;executing them, and certifying compilers that generate certified&lt;br/&gt;binaries for distribution on the network.&lt;br/&gt;&lt;br/&gt;The scope of the investigation includes the theory of specification&lt;br/&gt;and certification, and the systems building required to implement&lt;br/&gt;these ideas.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121633</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121633</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n965" target="n966">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE MII: Research Experience for Minority Students in High-Performance Computing and Communications</data>
      <data key="e_abstract">EIA-0117255&lt;br/&gt;Robert E. Hiromoto&lt;br/&gt;University of Texas - San Antonio&lt;br/&gt;&lt;br/&gt;Research Experiences in High-Performance Computing and Communications&lt;br/&gt;&lt;br/&gt;In this project, our goals are to (1) strengthen our ability to attract and retain undergraduate and graduate minority students, (2) increase the numbers of talented undergraduate minority students pursuing graduate education and high-tech industrial careers, (3) enhance the research productivity of the faculty and students, and (4) provide valuable industry experience to these students through industrial internships.&lt;br/&gt;&lt;br/&gt;We propose to create a dual-mentor partnership program with local industry to enhance the linkage of research and high-tech opportunities for our students. Students selected for this program will participate as NSF student scholars and will be provided financial support in the form of scholarships and research assistantships. Students will be expected to collaborate with a faculty mentor and regional industry research partners.&lt;br/&gt;&lt;br/&gt;Faculty from the Computer Science Department and the College of Engineering, will provide undergraduate student mentoring, work to develop student skills in problem solving activities, and create student research collaborations with faculty and industry. The research strengths in CS alone include: 1) parallel and distributed computing; 2) compiler technology for internet and parallel computing; 3) computer network protocols; 4) network security; 5) high-speed switching architectures; 6) data mining; and 7) biocomputing.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">117255</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">117255</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n965" target="n967">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE MII: Research Experience for Minority Students in High-Performance Computing and Communications</data>
      <data key="e_abstract">EIA-0117255&lt;br/&gt;Robert E. Hiromoto&lt;br/&gt;University of Texas - San Antonio&lt;br/&gt;&lt;br/&gt;Research Experiences in High-Performance Computing and Communications&lt;br/&gt;&lt;br/&gt;In this project, our goals are to (1) strengthen our ability to attract and retain undergraduate and graduate minority students, (2) increase the numbers of talented undergraduate minority students pursuing graduate education and high-tech industrial careers, (3) enhance the research productivity of the faculty and students, and (4) provide valuable industry experience to these students through industrial internships.&lt;br/&gt;&lt;br/&gt;We propose to create a dual-mentor partnership program with local industry to enhance the linkage of research and high-tech opportunities for our students. Students selected for this program will participate as NSF student scholars and will be provided financial support in the form of scholarships and research assistantships. Students will be expected to collaborate with a faculty mentor and regional industry research partners.&lt;br/&gt;&lt;br/&gt;Faculty from the Computer Science Department and the College of Engineering, will provide undergraduate student mentoring, work to develop student skills in problem solving activities, and create student research collaborations with faculty and industry. The research strengths in CS alone include: 1) parallel and distributed computing; 2) compiler technology for internet and parallel computing; 3) computer network protocols; 4) network security; 5) high-speed switching architectures; 6) data mining; and 7) biocomputing.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">117255</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">117255</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n966" target="n967">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE MII: Research Experience for Minority Students in High-Performance Computing and Communications</data>
      <data key="e_abstract">EIA-0117255&lt;br/&gt;Robert E. Hiromoto&lt;br/&gt;University of Texas - San Antonio&lt;br/&gt;&lt;br/&gt;Research Experiences in High-Performance Computing and Communications&lt;br/&gt;&lt;br/&gt;In this project, our goals are to (1) strengthen our ability to attract and retain undergraduate and graduate minority students, (2) increase the numbers of talented undergraduate minority students pursuing graduate education and high-tech industrial careers, (3) enhance the research productivity of the faculty and students, and (4) provide valuable industry experience to these students through industrial internships.&lt;br/&gt;&lt;br/&gt;We propose to create a dual-mentor partnership program with local industry to enhance the linkage of research and high-tech opportunities for our students. Students selected for this program will participate as NSF student scholars and will be provided financial support in the form of scholarships and research assistantships. Students will be expected to collaborate with a faculty mentor and regional industry research partners.&lt;br/&gt;&lt;br/&gt;Faculty from the Computer Science Department and the College of Engineering, will provide undergraduate student mentoring, work to develop student skills in problem solving activities, and create student research collaborations with faculty and industry. The research strengths in CS alone include: 1) parallel and distributed computing; 2) compiler technology for internet and parallel computing; 3) computer network protocols; 4) network security; 5) high-speed switching architectures; 6) data mining; and 7) biocomputing.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">117255</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">117255</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n968" target="n969">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Information Technology Workforce - ITWF: Entry, Earnings Growth, and Retention in IT Careers: An Economic Study</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;Information Technology Workforce (ITWF)&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Proposal Id: EIA-0120111&lt;br/&gt;Investigator: Catherine J. Weinberger and Peter Kuhn&lt;br/&gt;Institution: UC, Santa Barbara&lt;br/&gt;Title: Entry, Earnings Growth, and Retention in IT Careers: An Economic Study&lt;br/&gt;&lt;br/&gt;This proposal provides support for the University of California Santa Barbara to conduct three studies which together address questions pertaining to the college major choices of young women, and to the persistence and labor market success of college-educated women in Information Technology Workforce (IT) careers. The first study will examine the career paths of women who already have IT degrees and will explore whether there are labor market barriers that make IT careers less appealing to women. The second is a longitudinal study that will utilize decision theory to scrutinize common assumptions of economic models of career choice. The third study will survey young women&apos;s beliefs (or stereotypes) about the characteristics of IT jobs and the women who fill them. The information from this last survey will be linked to the longitudinal second study so that correlations between these beliefs and later career outcomes can be determined.</data>
      <data key="e_pgm">1397</data>
      <data key="e_label">120111</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0405</data>
      <data key="e_awardID">120111</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n970" target="n971">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY: Verification Tools for Autonomous and Embedded Systems</data>
      <data key="e_abstract">This research project is developing a new generation of formal verification tools that can be integrated into design environments for the complex, high-assurance embedded and autonomous systems of today and of the future. Such systems are increasingly distributed, complex, and dynamic; they must operate with a high degree of autonomy and survivability in diverse and unpredictable environments. This project will focus on the development of new verification methods and tools to provide a rigorous means for checking the integrity and correctness of designs for these systems before they are deployed. The project has two broad research thrusts: &lt;br/&gt;&lt;br/&gt;1. Verifying System Integrity. System integrity refers to correctness with respect to the interactions among the distributed software and hardware components. Systems must satisfy synchronization, resource, and real-time constraints imposed by the implementation architecture and application requirements. This project will extend automated verification methods that have been successful in hardware and protocol applications to their use with embedded and autonomous systems. &lt;br/&gt;2. Modeling the Environment. Embedded and autonomous systems must interact in complex ways with physical systems and adverse environments. It is thus essential to capture correctly and effectively the continuous dynamics, feedback loops, and unpredictable features of the environment in the models used for formal verification. This project will draw on recent developments in hybrid system verification to integrate continuous state dynamics with discrete-state models used in formal verification.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121547</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121547</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n970" target="n972">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY: Verification Tools for Autonomous and Embedded Systems</data>
      <data key="e_abstract">This research project is developing a new generation of formal verification tools that can be integrated into design environments for the complex, high-assurance embedded and autonomous systems of today and of the future. Such systems are increasingly distributed, complex, and dynamic; they must operate with a high degree of autonomy and survivability in diverse and unpredictable environments. This project will focus on the development of new verification methods and tools to provide a rigorous means for checking the integrity and correctness of designs for these systems before they are deployed. The project has two broad research thrusts: &lt;br/&gt;&lt;br/&gt;1. Verifying System Integrity. System integrity refers to correctness with respect to the interactions among the distributed software and hardware components. Systems must satisfy synchronization, resource, and real-time constraints imposed by the implementation architecture and application requirements. This project will extend automated verification methods that have been successful in hardware and protocol applications to their use with embedded and autonomous systems. &lt;br/&gt;2. Modeling the Environment. Embedded and autonomous systems must interact in complex ways with physical systems and adverse environments. It is thus essential to capture correctly and effectively the continuous dynamics, feedback loops, and unpredictable features of the environment in the models used for formal verification. This project will draw on recent developments in hybrid system verification to integrate continuous state dynamics with discrete-state models used in formal verification.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121547</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121547</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n971" target="n972">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY: Verification Tools for Autonomous and Embedded Systems</data>
      <data key="e_abstract">This research project is developing a new generation of formal verification tools that can be integrated into design environments for the complex, high-assurance embedded and autonomous systems of today and of the future. Such systems are increasingly distributed, complex, and dynamic; they must operate with a high degree of autonomy and survivability in diverse and unpredictable environments. This project will focus on the development of new verification methods and tools to provide a rigorous means for checking the integrity and correctness of designs for these systems before they are deployed. The project has two broad research thrusts: &lt;br/&gt;&lt;br/&gt;1. Verifying System Integrity. System integrity refers to correctness with respect to the interactions among the distributed software and hardware components. Systems must satisfy synchronization, resource, and real-time constraints imposed by the implementation architecture and application requirements. This project will extend automated verification methods that have been successful in hardware and protocol applications to their use with embedded and autonomous systems. &lt;br/&gt;2. Modeling the Environment. Embedded and autonomous systems must interact in complex ways with physical systems and adverse environments. It is thus essential to capture correctly and effectively the continuous dynamics, feedback loops, and unpredictable features of the environment in the models used for formal verification. This project will draw on recent developments in hybrid system verification to integrate continuous state dynamics with discrete-state models used in formal verification.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121547</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121547</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n973" target="n974">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Designing Next-generation Mobile Interfaces for Dynamic Conversational Speech</data>
      <data key="e_abstract">Interactive Multimodal Interfaces: Designing for Human Performance&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This is a creativity extension to the PI&apos;s continuing award. The PI plans to conduct a study involving mobile testing of her multimodal system. Subjects will be a mixture of adults and 7-to-9-year-old children. The goals include: (1) establishing the full research infrastructure needed to support extensive mobile testing and semi-automated data analysis; (2) documenting mutual disambiguation during mobile testing of a multimodal system, and studying the factors associated with its enhancement; (3) examining the relation between system recognition performance in a mobile environment and users&apos; signal characteristics, ambient noise levels, and signal-to-noise ratio information; and (4) exploring mobile speech signal patterns and system recognition performance in diverse user groups. The study will provide critical information about multimodal interface designs appropriate for supporting robust mobile use in real-world contexts and by varied users, and in particular for creating adaptive multimodal architectures that are capable of monitoring the environment on a command-by-command basis and adapting mode weights intelligently to avoid recognition failure and stabilize system performance. The data collected during the use of the PI&apos;s mobile system will assist in identifying a variety of new research issues and interface design challenges that have neither been recognized nor probed by the broader research community</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">117868</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">117868</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n875" target="n876">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Network Caching for Efficient Multimedia Content Delivery</data>
      <data key="e_abstract">Exponentially increasing multimedia information content has become a problem at the&lt;br/&gt;heart of modern communication networks. This is a paradigm shift from traditional com-&lt;br/&gt;munication networks, where the primary function is information transmission, and where&lt;br/&gt;quality of service improvements are attainable through better dimensioning of bandwidth&lt;br/&gt;and switching resources. However, in networks where information storage plays a central&lt;br/&gt;role, major improvements in quality of service can not be successful without the help of&lt;br/&gt;caching, prefetching and/or mirroring of information. It can be argued that even if an&lt;br/&gt;unlimited amount of free bandwidth were available, the concentration of information on a&lt;br/&gt;small number of servers would cause server overloads, resulting in unacceptable download&lt;br/&gt;latencies.&lt;br/&gt; This general problem has created the explosion of research studies that are to be found&lt;br/&gt;in the network systems engineering literature on Web caching. Although this work has&lt;br/&gt;contributed important engineering solutions, much of it is ad hoc and informal. Indeed, in&lt;br/&gt;the large literature on caching research, it is rare to find papers dealing with mathematical&lt;br/&gt;foundations, especially those underpinning stochastic models. Also, one rarely finds proofs in&lt;br/&gt;a rigorous setting of nontrivial stochastic or average-case properties of caching structures and&lt;br/&gt;algorithms. It is our thesis here that more systematic approaches are needed. This proposal&lt;br/&gt;describes a project to meet this need; more generally, we outline a systematic treatment&lt;br/&gt;that focuses on fundamental design issues, one that, in dealing with these issues, integrates&lt;br/&gt;experimentation, analysis and statistical measurements.&lt;br/&gt; Network cache design objectives are reductions in access latency, traffic congestion, and&lt;br/&gt;server loads. These objectives can be attained only through a thoughtful design that ad-&lt;br/&gt;dresses many important and challenging research topics. Some of the fundamental questions&lt;br/&gt;that remain without definitive answers include: dynamic caching and caching with expiration&lt;br/&gt;times; design and analysis of easily implemented heuristic algorithms; impact of locality in&lt;br/&gt;request sequences on caching performance; caching and prefetching in low bandwidth access&lt;br/&gt;environments, with a special emphasis on wireless Web accesses; cache allocation and sizing&lt;br/&gt;problems; and Web-graph performance modeling. Extending the knowledge base and deep-&lt;br/&gt;ening our insight into these and several other equally fundamental caching system design&lt;br/&gt;problems is the main theme of this proposal. The ultimate goal is to utilize this improved&lt;br/&gt;knowledge base to develop an experimental testbed for achieving practically feasible and&lt;br/&gt;efficient network caching systems.&lt;br/&gt; The inherent complexity of the research topics identified here, and the network caching&lt;br/&gt;problem as a whole, necessarily call into play all available research tools. Thus, methodologi-&lt;br/&gt;cally, the scope of the proposed research ranges from mathematical modeling and analysis to&lt;br/&gt;statistical measurements and experimentation. The impact of our results, by their interdis-&lt;br/&gt;ciplinary nature, will not just be limited to designing multimedia network caching systems,&lt;br/&gt;but will potentially lead to improved problem solving techniques in related fields of computer&lt;br/&gt;algorithms, software engineering, probability theory, and operations research.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">92113</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">92113</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n979" target="n980">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Discrete Event System Specification (DEVS) as a Formal Modeling and Simulation Framework for Scaleable Enterprise Design</data>
      <data key="e_abstract">This Scalable Enterprise Systems Phase II project will develop the Discrete Event System Specification (DEVS) Formal Framework for Scalable Enterprise Design and extend earlier-developed DEVS-based modeling and simulation environments to support several real world test cases. As the Internet expands toward 1 billion nodes forming a highly interconnected and computationally powerful medium, and companies increase specialization and horizontal layered organization, new complexity and dynamics are emerging. Scalability, the ability to avoid performance degradation and system breakdown as the scale of activity greatly increases, is one of the urgent global problems that needs to be addressed. This research will seek to enhance scalability at three inter-related levels of abstraction: the Enterprise Architecture level, the Information Technology Infrastructure level, and the Modeling and Simulation level. Earlier research developed a theoretical foundation for architecting a major responsibility of enterprise systems -- to ensure that the right information about the enterprise is available to decision makers at the right time. Having extended the DEVS formalism to express time-critical behaviors in enterprise data management, the researcher proposes to implement the extended DEVS functionality by suitably extending the distributed real-time execution environment previously developed in NSF-sponsored research. This environment will be tested by two diverse applications: a small scale but complete and real factory automation test bed and a large-scale web-hosting service for e-business. &lt;br/&gt;&lt;br/&gt;The Integrated Manufacturing Technology Initiative (IMTI) sponsored by the primary governmental funding agencies (NIST, DOE, NSF, and DARPA) states that modeling and simulation are emerging as key technologies to support manufacturing in the 21st century. This research will attempt to fill in some of the gap between the current state of the art and the IMTI vision of the future. In this vision enterprise processes, equipment and systems are linked via a robust communications infrastructure that delivers the right information at the right time; and integrated enterprise management systems that ensure that decisions to be made in real-time and on the basis of enterprise-wide impact. Achieving scalability in M&amp;S and IT infrastructure will enable a wide array of M&amp;S studies and implementations, as well as supporting the scalability of the future M&amp;S-based networked, extended and distributed enterprise systems envisioned by IMTI.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122227</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">122227</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n418" target="n982">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Collaborative Research: Middleware Support for Multicast Data Dissemination</data>
      <data key="e_abstract">The proposal is a collaborative proposal involving University of Pittsburgh &amp; Case Western Reserve University. The proposal plans to explore fundamental questions and associated middleware development issues to support broadcast and multicast data dissemination. When many people try to access the same information (for example, news sites during the Election), servers are often overwhelmed resulting in long delays. If an effective multicast capability existed across the Internet, a single server could have served all the requests, saving both computer power as well as reducing the amount of overall Internet traffic. &lt;br/&gt;&lt;br/&gt; The approach will highlight gaps in the state-of-the-art and will research and explore new techniques and algorithms. The objectives include&lt;br/&gt;1. transparently provide applications with data management services such as caching, scheduling and consistency maintenance, &lt;br/&gt;2. improve internet functionality, performance, and cost-effectiveness, and&lt;br/&gt;3. reduce the application development time while improving performance and reliability.&lt;br/&gt;&lt;br/&gt;The proposal plans to make the information and developed software available to</data>
      <data key="e_pgm">4089</data>
      <data key="e_label">123705</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">123705</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n23" target="n983">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Adaptive Middleware Services for Situation-Aware Communication in Ubiquitous Computing Environments</data>
      <data key="e_abstract">A major trend in distributed computing is toward environments consisting of numerous wearable, handheld and embedded devices. Rapid growth in inexpensive, short range, and low-power wireless communications and hardware, are now enabling experimentation with ubiquitous computing environments. In order for ubiquitous environments to work well, it is essential that devices be able to detect their current context, that they must be aware of relationships between the users and the devices, and handle ad-hoc groups and their communications.&lt;br/&gt;&lt;br/&gt;The proposal plans to take an integrated hardware and software approach to developing essential services to support a wide variety of distributed applications in heterogeneous environments. The services include investigation of:&lt;br/&gt;situation-aware inter-object communications&lt;br/&gt;group management service to establish device communities&lt;br/&gt;feasibility of using cellular automata computational model to design a scalable dissemination service&lt;br/&gt;a service that will use the concept of context-sensitivity to perform trade-offs between transparent and QoS assisted adaptations.&lt;br/&gt;The proposal will develop a Reconfigurable and Context-Sensitive Middleware, which will be open-source, open-schematic and open standard middleware based on top of Bluetooth standards.</data>
      <data key="e_pgm">4089</data>
      <data key="e_label">123980</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">123980</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n987" target="n988">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: Multidisciplinary Training in Reliability and Risk Engineering, Analysis, and Management</data>
      <data key="e_abstract">This IGERT project will establish a multidisciplinary graduate program in reliability and risk engineering, analysis, and management at Vanderbilt University. Twenty-five faculty participants in this program are drawn from three different schools: Engineering (Civil, Mechanical, Chemical, Electrical Engineering and Computer Science, and Management of Technology), Management, and Arts and Sciences (Mathematics). The research theme consists of three inter-linked areas: (i) large systems reliability and risk, (ii) device- and component-level reliability, and (iii) uncertainty analysis methods. As engineering systems grow in size, complexity and cost, reliability and risk assessment is increasingly dependent on modeling and simulation, rather than on expensive (or impossible) traditional test-based methods. Therefore, the unique features of the research theme are: (i) development of the modeling and simulation-based methodology for reliability and risk assessment, (ii) systematic integration of models and tools across disciplines, and (iii) inclusion of economic, legal, regulatory, and social perspectives in risk assessment and management. The research projects will apply these concepts to infrastructure, environmental, network, mechanical, and electronic systems. The educational goals are to broaden the training with multidisciplinary perspectives, embed information technology, include model integration and high performance computing technologies, and increase the number and diversity of reliability and risk engineers and managers trained in the modeling and simulation methodology. A number of strategies are proposed to achieve these objectives: multidisciplinary coursework and dissertation topics, laboratory rotations, industry and government laboratory internships, seminars, workshops, case studies, training in professional communication and ethics, undergraduate and high school teacher participants, aggressive recruitment (especially among underrepresented groups), and systematic evaluation by industry, government and academia. The program will include strong participation and support from several industries, government agencies and national laboratories, through internships, workshop and seminar participation, educational and research collaboration, and advisory committee. Through these efforts, the graduate program aims to become a self-sustaining center of national and international leadership. The IGERT award will lead to 35 Ph.D.&apos;s over 9 years, fulfilling a critical need of the American industry in this important field. &lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114329</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114329</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n987" target="n989">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: Multidisciplinary Training in Reliability and Risk Engineering, Analysis, and Management</data>
      <data key="e_abstract">This IGERT project will establish a multidisciplinary graduate program in reliability and risk engineering, analysis, and management at Vanderbilt University. Twenty-five faculty participants in this program are drawn from three different schools: Engineering (Civil, Mechanical, Chemical, Electrical Engineering and Computer Science, and Management of Technology), Management, and Arts and Sciences (Mathematics). The research theme consists of three inter-linked areas: (i) large systems reliability and risk, (ii) device- and component-level reliability, and (iii) uncertainty analysis methods. As engineering systems grow in size, complexity and cost, reliability and risk assessment is increasingly dependent on modeling and simulation, rather than on expensive (or impossible) traditional test-based methods. Therefore, the unique features of the research theme are: (i) development of the modeling and simulation-based methodology for reliability and risk assessment, (ii) systematic integration of models and tools across disciplines, and (iii) inclusion of economic, legal, regulatory, and social perspectives in risk assessment and management. The research projects will apply these concepts to infrastructure, environmental, network, mechanical, and electronic systems. The educational goals are to broaden the training with multidisciplinary perspectives, embed information technology, include model integration and high performance computing technologies, and increase the number and diversity of reliability and risk engineers and managers trained in the modeling and simulation methodology. A number of strategies are proposed to achieve these objectives: multidisciplinary coursework and dissertation topics, laboratory rotations, industry and government laboratory internships, seminars, workshops, case studies, training in professional communication and ethics, undergraduate and high school teacher participants, aggressive recruitment (especially among underrepresented groups), and systematic evaluation by industry, government and academia. The program will include strong participation and support from several industries, government agencies and national laboratories, through internships, workshop and seminar participation, educational and research collaboration, and advisory committee. Through these efforts, the graduate program aims to become a self-sustaining center of national and international leadership. The IGERT award will lead to 35 Ph.D.&apos;s over 9 years, fulfilling a critical need of the American industry in this important field. &lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114329</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114329</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n987" target="n990">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: Multidisciplinary Training in Reliability and Risk Engineering, Analysis, and Management</data>
      <data key="e_abstract">This IGERT project will establish a multidisciplinary graduate program in reliability and risk engineering, analysis, and management at Vanderbilt University. Twenty-five faculty participants in this program are drawn from three different schools: Engineering (Civil, Mechanical, Chemical, Electrical Engineering and Computer Science, and Management of Technology), Management, and Arts and Sciences (Mathematics). The research theme consists of three inter-linked areas: (i) large systems reliability and risk, (ii) device- and component-level reliability, and (iii) uncertainty analysis methods. As engineering systems grow in size, complexity and cost, reliability and risk assessment is increasingly dependent on modeling and simulation, rather than on expensive (or impossible) traditional test-based methods. Therefore, the unique features of the research theme are: (i) development of the modeling and simulation-based methodology for reliability and risk assessment, (ii) systematic integration of models and tools across disciplines, and (iii) inclusion of economic, legal, regulatory, and social perspectives in risk assessment and management. The research projects will apply these concepts to infrastructure, environmental, network, mechanical, and electronic systems. The educational goals are to broaden the training with multidisciplinary perspectives, embed information technology, include model integration and high performance computing technologies, and increase the number and diversity of reliability and risk engineers and managers trained in the modeling and simulation methodology. A number of strategies are proposed to achieve these objectives: multidisciplinary coursework and dissertation topics, laboratory rotations, industry and government laboratory internships, seminars, workshops, case studies, training in professional communication and ethics, undergraduate and high school teacher participants, aggressive recruitment (especially among underrepresented groups), and systematic evaluation by industry, government and academia. The program will include strong participation and support from several industries, government agencies and national laboratories, through internships, workshop and seminar participation, educational and research collaboration, and advisory committee. Through these efforts, the graduate program aims to become a self-sustaining center of national and international leadership. The IGERT award will lead to 35 Ph.D.&apos;s over 9 years, fulfilling a critical need of the American industry in this important field. &lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114329</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114329</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n987" target="n991">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: Multidisciplinary Training in Reliability and Risk Engineering, Analysis, and Management</data>
      <data key="e_abstract">This IGERT project will establish a multidisciplinary graduate program in reliability and risk engineering, analysis, and management at Vanderbilt University. Twenty-five faculty participants in this program are drawn from three different schools: Engineering (Civil, Mechanical, Chemical, Electrical Engineering and Computer Science, and Management of Technology), Management, and Arts and Sciences (Mathematics). The research theme consists of three inter-linked areas: (i) large systems reliability and risk, (ii) device- and component-level reliability, and (iii) uncertainty analysis methods. As engineering systems grow in size, complexity and cost, reliability and risk assessment is increasingly dependent on modeling and simulation, rather than on expensive (or impossible) traditional test-based methods. Therefore, the unique features of the research theme are: (i) development of the modeling and simulation-based methodology for reliability and risk assessment, (ii) systematic integration of models and tools across disciplines, and (iii) inclusion of economic, legal, regulatory, and social perspectives in risk assessment and management. The research projects will apply these concepts to infrastructure, environmental, network, mechanical, and electronic systems. The educational goals are to broaden the training with multidisciplinary perspectives, embed information technology, include model integration and high performance computing technologies, and increase the number and diversity of reliability and risk engineers and managers trained in the modeling and simulation methodology. A number of strategies are proposed to achieve these objectives: multidisciplinary coursework and dissertation topics, laboratory rotations, industry and government laboratory internships, seminars, workshops, case studies, training in professional communication and ethics, undergraduate and high school teacher participants, aggressive recruitment (especially among underrepresented groups), and systematic evaluation by industry, government and academia. The program will include strong participation and support from several industries, government agencies and national laboratories, through internships, workshop and seminar participation, educational and research collaboration, and advisory committee. Through these efforts, the graduate program aims to become a self-sustaining center of national and international leadership. The IGERT award will lead to 35 Ph.D.&apos;s over 9 years, fulfilling a critical need of the American industry in this important field. &lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114329</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114329</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n988" target="n989">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: Multidisciplinary Training in Reliability and Risk Engineering, Analysis, and Management</data>
      <data key="e_abstract">This IGERT project will establish a multidisciplinary graduate program in reliability and risk engineering, analysis, and management at Vanderbilt University. Twenty-five faculty participants in this program are drawn from three different schools: Engineering (Civil, Mechanical, Chemical, Electrical Engineering and Computer Science, and Management of Technology), Management, and Arts and Sciences (Mathematics). The research theme consists of three inter-linked areas: (i) large systems reliability and risk, (ii) device- and component-level reliability, and (iii) uncertainty analysis methods. As engineering systems grow in size, complexity and cost, reliability and risk assessment is increasingly dependent on modeling and simulation, rather than on expensive (or impossible) traditional test-based methods. Therefore, the unique features of the research theme are: (i) development of the modeling and simulation-based methodology for reliability and risk assessment, (ii) systematic integration of models and tools across disciplines, and (iii) inclusion of economic, legal, regulatory, and social perspectives in risk assessment and management. The research projects will apply these concepts to infrastructure, environmental, network, mechanical, and electronic systems. The educational goals are to broaden the training with multidisciplinary perspectives, embed information technology, include model integration and high performance computing technologies, and increase the number and diversity of reliability and risk engineers and managers trained in the modeling and simulation methodology. A number of strategies are proposed to achieve these objectives: multidisciplinary coursework and dissertation topics, laboratory rotations, industry and government laboratory internships, seminars, workshops, case studies, training in professional communication and ethics, undergraduate and high school teacher participants, aggressive recruitment (especially among underrepresented groups), and systematic evaluation by industry, government and academia. The program will include strong participation and support from several industries, government agencies and national laboratories, through internships, workshop and seminar participation, educational and research collaboration, and advisory committee. Through these efforts, the graduate program aims to become a self-sustaining center of national and international leadership. The IGERT award will lead to 35 Ph.D.&apos;s over 9 years, fulfilling a critical need of the American industry in this important field. &lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114329</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114329</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n988" target="n990">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: Multidisciplinary Training in Reliability and Risk Engineering, Analysis, and Management</data>
      <data key="e_abstract">This IGERT project will establish a multidisciplinary graduate program in reliability and risk engineering, analysis, and management at Vanderbilt University. Twenty-five faculty participants in this program are drawn from three different schools: Engineering (Civil, Mechanical, Chemical, Electrical Engineering and Computer Science, and Management of Technology), Management, and Arts and Sciences (Mathematics). The research theme consists of three inter-linked areas: (i) large systems reliability and risk, (ii) device- and component-level reliability, and (iii) uncertainty analysis methods. As engineering systems grow in size, complexity and cost, reliability and risk assessment is increasingly dependent on modeling and simulation, rather than on expensive (or impossible) traditional test-based methods. Therefore, the unique features of the research theme are: (i) development of the modeling and simulation-based methodology for reliability and risk assessment, (ii) systematic integration of models and tools across disciplines, and (iii) inclusion of economic, legal, regulatory, and social perspectives in risk assessment and management. The research projects will apply these concepts to infrastructure, environmental, network, mechanical, and electronic systems. The educational goals are to broaden the training with multidisciplinary perspectives, embed information technology, include model integration and high performance computing technologies, and increase the number and diversity of reliability and risk engineers and managers trained in the modeling and simulation methodology. A number of strategies are proposed to achieve these objectives: multidisciplinary coursework and dissertation topics, laboratory rotations, industry and government laboratory internships, seminars, workshops, case studies, training in professional communication and ethics, undergraduate and high school teacher participants, aggressive recruitment (especially among underrepresented groups), and systematic evaluation by industry, government and academia. The program will include strong participation and support from several industries, government agencies and national laboratories, through internships, workshop and seminar participation, educational and research collaboration, and advisory committee. Through these efforts, the graduate program aims to become a self-sustaining center of national and international leadership. The IGERT award will lead to 35 Ph.D.&apos;s over 9 years, fulfilling a critical need of the American industry in this important field. &lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114329</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114329</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n988" target="n991">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: Multidisciplinary Training in Reliability and Risk Engineering, Analysis, and Management</data>
      <data key="e_abstract">This IGERT project will establish a multidisciplinary graduate program in reliability and risk engineering, analysis, and management at Vanderbilt University. Twenty-five faculty participants in this program are drawn from three different schools: Engineering (Civil, Mechanical, Chemical, Electrical Engineering and Computer Science, and Management of Technology), Management, and Arts and Sciences (Mathematics). The research theme consists of three inter-linked areas: (i) large systems reliability and risk, (ii) device- and component-level reliability, and (iii) uncertainty analysis methods. As engineering systems grow in size, complexity and cost, reliability and risk assessment is increasingly dependent on modeling and simulation, rather than on expensive (or impossible) traditional test-based methods. Therefore, the unique features of the research theme are: (i) development of the modeling and simulation-based methodology for reliability and risk assessment, (ii) systematic integration of models and tools across disciplines, and (iii) inclusion of economic, legal, regulatory, and social perspectives in risk assessment and management. The research projects will apply these concepts to infrastructure, environmental, network, mechanical, and electronic systems. The educational goals are to broaden the training with multidisciplinary perspectives, embed information technology, include model integration and high performance computing technologies, and increase the number and diversity of reliability and risk engineers and managers trained in the modeling and simulation methodology. A number of strategies are proposed to achieve these objectives: multidisciplinary coursework and dissertation topics, laboratory rotations, industry and government laboratory internships, seminars, workshops, case studies, training in professional communication and ethics, undergraduate and high school teacher participants, aggressive recruitment (especially among underrepresented groups), and systematic evaluation by industry, government and academia. The program will include strong participation and support from several industries, government agencies and national laboratories, through internships, workshop and seminar participation, educational and research collaboration, and advisory committee. Through these efforts, the graduate program aims to become a self-sustaining center of national and international leadership. The IGERT award will lead to 35 Ph.D.&apos;s over 9 years, fulfilling a critical need of the American industry in this important field. &lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114329</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114329</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n989" target="n990">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: Multidisciplinary Training in Reliability and Risk Engineering, Analysis, and Management</data>
      <data key="e_abstract">This IGERT project will establish a multidisciplinary graduate program in reliability and risk engineering, analysis, and management at Vanderbilt University. Twenty-five faculty participants in this program are drawn from three different schools: Engineering (Civil, Mechanical, Chemical, Electrical Engineering and Computer Science, and Management of Technology), Management, and Arts and Sciences (Mathematics). The research theme consists of three inter-linked areas: (i) large systems reliability and risk, (ii) device- and component-level reliability, and (iii) uncertainty analysis methods. As engineering systems grow in size, complexity and cost, reliability and risk assessment is increasingly dependent on modeling and simulation, rather than on expensive (or impossible) traditional test-based methods. Therefore, the unique features of the research theme are: (i) development of the modeling and simulation-based methodology for reliability and risk assessment, (ii) systematic integration of models and tools across disciplines, and (iii) inclusion of economic, legal, regulatory, and social perspectives in risk assessment and management. The research projects will apply these concepts to infrastructure, environmental, network, mechanical, and electronic systems. The educational goals are to broaden the training with multidisciplinary perspectives, embed information technology, include model integration and high performance computing technologies, and increase the number and diversity of reliability and risk engineers and managers trained in the modeling and simulation methodology. A number of strategies are proposed to achieve these objectives: multidisciplinary coursework and dissertation topics, laboratory rotations, industry and government laboratory internships, seminars, workshops, case studies, training in professional communication and ethics, undergraduate and high school teacher participants, aggressive recruitment (especially among underrepresented groups), and systematic evaluation by industry, government and academia. The program will include strong participation and support from several industries, government agencies and national laboratories, through internships, workshop and seminar participation, educational and research collaboration, and advisory committee. Through these efforts, the graduate program aims to become a self-sustaining center of national and international leadership. The IGERT award will lead to 35 Ph.D.&apos;s over 9 years, fulfilling a critical need of the American industry in this important field. &lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114329</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114329</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n989" target="n991">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: Multidisciplinary Training in Reliability and Risk Engineering, Analysis, and Management</data>
      <data key="e_abstract">This IGERT project will establish a multidisciplinary graduate program in reliability and risk engineering, analysis, and management at Vanderbilt University. Twenty-five faculty participants in this program are drawn from three different schools: Engineering (Civil, Mechanical, Chemical, Electrical Engineering and Computer Science, and Management of Technology), Management, and Arts and Sciences (Mathematics). The research theme consists of three inter-linked areas: (i) large systems reliability and risk, (ii) device- and component-level reliability, and (iii) uncertainty analysis methods. As engineering systems grow in size, complexity and cost, reliability and risk assessment is increasingly dependent on modeling and simulation, rather than on expensive (or impossible) traditional test-based methods. Therefore, the unique features of the research theme are: (i) development of the modeling and simulation-based methodology for reliability and risk assessment, (ii) systematic integration of models and tools across disciplines, and (iii) inclusion of economic, legal, regulatory, and social perspectives in risk assessment and management. The research projects will apply these concepts to infrastructure, environmental, network, mechanical, and electronic systems. The educational goals are to broaden the training with multidisciplinary perspectives, embed information technology, include model integration and high performance computing technologies, and increase the number and diversity of reliability and risk engineers and managers trained in the modeling and simulation methodology. A number of strategies are proposed to achieve these objectives: multidisciplinary coursework and dissertation topics, laboratory rotations, industry and government laboratory internships, seminars, workshops, case studies, training in professional communication and ethics, undergraduate and high school teacher participants, aggressive recruitment (especially among underrepresented groups), and systematic evaluation by industry, government and academia. The program will include strong participation and support from several industries, government agencies and national laboratories, through internships, workshop and seminar participation, educational and research collaboration, and advisory committee. Through these efforts, the graduate program aims to become a self-sustaining center of national and international leadership. The IGERT award will lead to 35 Ph.D.&apos;s over 9 years, fulfilling a critical need of the American industry in this important field. &lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114329</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114329</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n990" target="n991">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: Multidisciplinary Training in Reliability and Risk Engineering, Analysis, and Management</data>
      <data key="e_abstract">This IGERT project will establish a multidisciplinary graduate program in reliability and risk engineering, analysis, and management at Vanderbilt University. Twenty-five faculty participants in this program are drawn from three different schools: Engineering (Civil, Mechanical, Chemical, Electrical Engineering and Computer Science, and Management of Technology), Management, and Arts and Sciences (Mathematics). The research theme consists of three inter-linked areas: (i) large systems reliability and risk, (ii) device- and component-level reliability, and (iii) uncertainty analysis methods. As engineering systems grow in size, complexity and cost, reliability and risk assessment is increasingly dependent on modeling and simulation, rather than on expensive (or impossible) traditional test-based methods. Therefore, the unique features of the research theme are: (i) development of the modeling and simulation-based methodology for reliability and risk assessment, (ii) systematic integration of models and tools across disciplines, and (iii) inclusion of economic, legal, regulatory, and social perspectives in risk assessment and management. The research projects will apply these concepts to infrastructure, environmental, network, mechanical, and electronic systems. The educational goals are to broaden the training with multidisciplinary perspectives, embed information technology, include model integration and high performance computing technologies, and increase the number and diversity of reliability and risk engineers and managers trained in the modeling and simulation methodology. A number of strategies are proposed to achieve these objectives: multidisciplinary coursework and dissertation topics, laboratory rotations, industry and government laboratory internships, seminars, workshops, case studies, training in professional communication and ethics, undergraduate and high school teacher participants, aggressive recruitment (especially among underrepresented groups), and systematic evaluation by industry, government and academia. The program will include strong participation and support from several industries, government agencies and national laboratories, through internships, workshop and seminar participation, educational and research collaboration, and advisory committee. Through these efforts, the graduate program aims to become a self-sustaining center of national and international leadership. The IGERT award will lead to 35 Ph.D.&apos;s over 9 years, fulfilling a critical need of the American industry in this important field. &lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114329</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114329</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n994" target="n995">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Constraint Databases: Optimization Techniques and Applications</data>
      <data key="e_abstract">Constraint databases integrate database and constraint technologies for spatio-temporal database applications including GIS and moving objects. Constraint data models emphasize logical properties while hiding physical representations; they enable general purpose data management capabilities. The project investigates constraint databases in three aspects: algorithms, applications, and foundations. In algorithms, it aims at optimization techniques for constraint database queries. Joins with the intersection predicate are known as spatial joins. Traditional algorithms use heuristics, indexes, and computational geometry techniques to evaluate the join on minimum bounding rectangles of objects as a filter. Recent algorithms allow better approximations for filter, or even perform a direct join on objects. A focus of this project is to study and compare performance of such new algorithms and develop further improvements and models for predicting filter effectiveness in terms of dataset properties and approximations. Moreover, the techniques for intersection join are extended for joins with other topological (e.g., containment, meet), distance, and direction predicates, and to multiway spatial joins. More fundamentally, optimization issues are often related to decision problems of logical properties such as containment, equivalence, and disjointness of transactions (queries/updates). Abstract machines are a recently developed tool for studying such problems. Transactions are often designed in advance with parameters instantiated at runtime. The abstract machine techniques are extended for studying logical properties of parameterized transactions and related computational complexity issues. In applications aspect, the project uses the constraint approach to develop data models and query languages for moving object databases and study query optimization Constraint databases integrate database and constraint technologies for spatio-temporal database applications including GIS and moving objects. Constraint data models emphasize logical properties while hiding physical representations; they enable general purpose data management capabilities. The project investigates constraint databases in three aspects: algorithms, applications, and foundations. In algorithms, it aims at optimization techniques for constraint database queries. Joins with the intersection predicate are known as spatial joins. Traditional algorithms use heuristics, indexes, and computational geometry techniques to evaluate the join on minimum bounding rectangles of objects as a filter. Recent algorithms allow better approximations for filter, or even perform a direct join on objects. A focus of this project is to compare and characterize performance of such new algorithms and develop further improvements and models for predicting filter effectiveness in terms of dataset properties and approximations. Moreover, the techniques for intersection join are extended for joins with other topological (e.g., containment, meet), distance, and direction predicates, and to multiway spatial joins. More fundamentally, optimization issues are often related to decision problems of logical properties such as containment, equivalence, and disjointness of transactions (queries/updates). Abstract machines (in automata theory) have been found to be a very useful tool for studying such problems. Transactions are often designed in advance with parameters instantiated at runtime. The project intends to extend these automata-theoretic techniques for studying logical properties of parameterized transactions and related computational complexity issues. In applications aspect, the project applies the constraint approach to develop data models and query languages for moving object databases. The goal is to provide a conceptual framework and optimization techniques for managing and querying moving objects.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">101134</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">101134</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n859" target="n996">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Delay Fault Modeling, Test, and Diagnosis</data>
      <data key="e_abstract">As integrated circuit speed and density increases, more circuits fail due to delay faults -- manufacturing defects that cause the circuit to operate at a speed slower than intended. Such circuits either cannot be used or must be sold at a lower price. The behavior of delay faults is complex, and traditional manufacturing test approaches are increasingly ineffective in detecting them. This research attacks this problem by developing a novel realistic delay fault model. This model is being used to develop powerful techniques for fault simulation, automatic manufacturing test generation, and diagnosis for next-generation integrated circuits. This work is being done in cooperation with U.S. semiconductor manufacturers. These test and diagnosis techniques are being integrated into a state-of-art software system and will be tested on real manufacturing problems.&lt;br/&gt;&lt;br/&gt;The new realistic delay fault model considers resistive bridges and opens, the impact of process variation on interconnect, device, and defect parameters, and the influence of interconnect parasitics. Fast layout and parasitic extraction algorithms, and model order reduction techniques are being developed to reduce model cost for a given accuracy level. The model is encapsulated in a parameterized static timing analysis engine for use in fault simulation, test generation and diagnosis. A constraint-based fault coverage analysis is used to determine fault coverage over a set of vectors. Together, these techniques can accurately predict fault coverage and achieve very high delay fault coverage for scan-based CMOS logic circuits.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">98329</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98329</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1000" target="n1001">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Bio-QuBIC: Designer Gene Networks for Biocomputing Applications</data>
      <data key="e_abstract">EIA-0130331&lt;br/&gt;James.J. Collins&lt;br/&gt;Boston University&lt;br/&gt;&lt;br/&gt;Title: Designer Gene Networks for Biocomputing Applications&lt;br/&gt;&lt;br/&gt;Many fundamental cellular processes are governed by genetic networks which employ protein-DNA interactions in regulating function. The biochemistry of the feedback loops associated with protein-DNA interactions leads to nonlinear effects, and the tools of nonlinear analysis become invaluable. This project involves the use of techniques from nonlinear dynamics and molecular biology to model, design and construct synthetic gene networks for biocomputing applications. Here biocomputing is defined as representing the ability of cells to make decisions based on external stimuli, and in this context, synthetic gene networks can be viewed as &quot;controllers&quot; for living cells. In this project, a rapidly switching genetic toggle switch is being modeled and constructed in bacterial cells. The genetic toggle switch, which is a fundamental unit of biocomputing memory storage, can be flipped between two stable expression states using transient chemical or thermal stimuli. In addition, as part of this project, synthetic gene networks based on more complicated logic gates (i.e., AND and OR gates) are being designed, modeled and constructed in bacterial cells. These circuits can function as sensors of multiple transient signals, and form the basis for general control schemes requiring an &quot;if/then&quot; structure.&lt;br/&gt;&lt;br/&gt;Synthetic gene networks represent a first step towards logical cellular control, whereby biological processes can be manipulated or monitored at the DNA level. Ultimately, synthetic gene circuits encoded into DNA, might be &quot;downloaded&quot; into cells creating, in effect, a &quot;wet&quot; nano-robot. These cellular robots could be utilized for a variety of functions, including in vivo biosensing, autonomously synthesizing complex biomaterials, executing programmed cell death, and interfacing with microelectronic circuits by transducing biochemical events to and from the electronics.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">130331</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130331</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n478" target="n1003">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Design and Simulation of Biologically-inspired Nanolattice</data>
      <data key="e_abstract">EIA-0135946&lt;br/&gt;Fortes, Jose A&lt;br/&gt;University of Florida&lt;br/&gt;&lt;br/&gt;ITR/SY: Design and Simulation of Biologically inspired Nanolattice&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This joint project between the University of Florida and Purdue University is pursuing scientific principles for designing and engineering biologically inspired neuromorphic computing architectures using radically new molecular electronic devices and biologically inspired, ultra-dense, self-assembled systems. Examples of applications of these architectures include unprecedently small and inexpensive nanoscale intelligent sensors. The architectures can be used to implement neurocomputing models and are well suited for nanotechnologies, thus accelerating the development of useful nanotechnology by providing clear functional targets for nanodevices.&lt;br/&gt;The team of investigators includes computer architects, neurocomputing experts and device physicists working in close collaboration along three highly synergistic thrusts. One of the two thrusts is focused on advancing the understanding of biologically-inspired dynamic information processing systems in order to understand the impact of constraints imposed by architectures and technologies on the properties of these systems. Another thrust investigates neurocomputing system architectures that can be engineered within the constraints of nanotechnologies. The third thrust develops a toolbox of novel mechanisms for integration, self-assembly and interconnection of nanoscale devices. &lt;br/&gt;The architectures are investigated via formal methods and simulation. Internet resources are used to conduct simulation, and to disseminate models, software and other research results. A new course, summer internships and educational materials are being developed to educate students on the key interdisciplinary aspects and results of the project.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">135946</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">135946</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n478" target="n1004">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Design and Simulation of Biologically-inspired Nanolattice</data>
      <data key="e_abstract">EIA-0135946&lt;br/&gt;Fortes, Jose A&lt;br/&gt;University of Florida&lt;br/&gt;&lt;br/&gt;ITR/SY: Design and Simulation of Biologically inspired Nanolattice&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This joint project between the University of Florida and Purdue University is pursuing scientific principles for designing and engineering biologically inspired neuromorphic computing architectures using radically new molecular electronic devices and biologically inspired, ultra-dense, self-assembled systems. Examples of applications of these architectures include unprecedently small and inexpensive nanoscale intelligent sensors. The architectures can be used to implement neurocomputing models and are well suited for nanotechnologies, thus accelerating the development of useful nanotechnology by providing clear functional targets for nanodevices.&lt;br/&gt;The team of investigators includes computer architects, neurocomputing experts and device physicists working in close collaboration along three highly synergistic thrusts. One of the two thrusts is focused on advancing the understanding of biologically-inspired dynamic information processing systems in order to understand the impact of constraints imposed by architectures and technologies on the properties of these systems. Another thrust investigates neurocomputing system architectures that can be engineered within the constraints of nanotechnologies. The third thrust develops a toolbox of novel mechanisms for integration, self-assembly and interconnection of nanoscale devices. &lt;br/&gt;The architectures are investigated via formal methods and simulation. Internet resources are used to conduct simulation, and to disseminate models, software and other research results. A new course, summer internships and educational materials are being developed to educate students on the key interdisciplinary aspects and results of the project.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">135946</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">135946</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1003" target="n1004">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SY: Design and Simulation of Biologically-inspired Nanolattice</data>
      <data key="e_abstract">EIA-0135946&lt;br/&gt;Fortes, Jose A&lt;br/&gt;University of Florida&lt;br/&gt;&lt;br/&gt;ITR/SY: Design and Simulation of Biologically inspired Nanolattice&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This joint project between the University of Florida and Purdue University is pursuing scientific principles for designing and engineering biologically inspired neuromorphic computing architectures using radically new molecular electronic devices and biologically inspired, ultra-dense, self-assembled systems. Examples of applications of these architectures include unprecedently small and inexpensive nanoscale intelligent sensors. The architectures can be used to implement neurocomputing models and are well suited for nanotechnologies, thus accelerating the development of useful nanotechnology by providing clear functional targets for nanodevices.&lt;br/&gt;The team of investigators includes computer architects, neurocomputing experts and device physicists working in close collaboration along three highly synergistic thrusts. One of the two thrusts is focused on advancing the understanding of biologically-inspired dynamic information processing systems in order to understand the impact of constraints imposed by architectures and technologies on the properties of these systems. Another thrust investigates neurocomputing system architectures that can be engineered within the constraints of nanotechnologies. The third thrust develops a toolbox of novel mechanisms for integration, self-assembly and interconnection of nanoscale devices. &lt;br/&gt;The architectures are investigated via formal methods and simulation. Internet resources are used to conduct simulation, and to disseminate models, software and other research results. A new course, summer internships and educational materials are being developed to educate students on the key interdisciplinary aspects and results of the project.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">135946</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">135946</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1005" target="n1006">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Customized Spatial Sound for Human/Computer Interaction</data>
      <data key="e_abstract">Current systems that generate spatial sound for computer-based&lt;br/&gt;applications employ Head-Related Transfer functions (HRTFs) and&lt;br/&gt;simple models of room reflections to provide the acoustic localization&lt;br/&gt;cues. However, the abilities of these systems to generate well-controlled&lt;br/&gt;spatial sound streams are quite limited. Although it is possible to position&lt;br/&gt;virtual sound sources to the left or right rather accurately, front/back &lt;br/&gt;confusion is common, localization in elevation is problematic, and localization&lt;br/&gt;in range is unreliable.&lt;br/&gt;&lt;br/&gt;This proposal describes a comprehensive program of research directed at solving&lt;br/&gt;the major problems that are the cause of these limitations. These problems&lt;br/&gt;are identified as (a) a mismatch between the HRTF used by the system and&lt;br/&gt;the listener&apos;s actual HRTF, (b) a failure to provide the correct dynamic cues&lt;br/&gt;that occur when the listener moves relative to the source, (c) a mismatch between &lt;br/&gt;synthesized room reflections and the listener&apos;s experience or expectations, and&lt;br/&gt;(d) a failure to render the correct spectral cues for familiar sounds,&lt;br/&gt;such as human speech.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">97256</data>
      <data key="e_expirationDate">2006-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">97256</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1008" target="n1009">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR: Quantum Entanglement, Information and Computation</data>
      <data key="e_abstract">EIA-0113137&lt;br/&gt;Yanhua Shih&lt;br/&gt;University of Maryland Baltimore County&lt;br/&gt;&lt;br/&gt;Title: ITR: Quantum Entanglement, Information and Computation&lt;br/&gt;&lt;br/&gt;This project consists of a series of experimental and theoretical investigations on multi-photon entangled states and their fundamental role in quantum information and quantum computing. Major efforts underway include experimental and theoretical investigations on N-photon (N&gt;2 entangled states) and on &quot;partial entanglement&quot; in quantum information and quantum computation theory. &lt;br/&gt;&lt;br/&gt;Theoretical characterization of entanglement in mixed states is at the core of this project. The experimental investigations focus on two projects. The first one is the realization of source of three-photon entangled states. These may lead to experimental generation of GHz states. The second experimental project is quantum teleportation based on the improvement on the realization of Bell-state detection.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113137</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113137</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1008" target="n1010">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR: Quantum Entanglement, Information and Computation</data>
      <data key="e_abstract">EIA-0113137&lt;br/&gt;Yanhua Shih&lt;br/&gt;University of Maryland Baltimore County&lt;br/&gt;&lt;br/&gt;Title: ITR: Quantum Entanglement, Information and Computation&lt;br/&gt;&lt;br/&gt;This project consists of a series of experimental and theoretical investigations on multi-photon entangled states and their fundamental role in quantum information and quantum computing. Major efforts underway include experimental and theoretical investigations on N-photon (N&gt;2 entangled states) and on &quot;partial entanglement&quot; in quantum information and quantum computation theory. &lt;br/&gt;&lt;br/&gt;Theoretical characterization of entanglement in mixed states is at the core of this project. The experimental investigations focus on two projects. The first one is the realization of source of three-photon entangled states. These may lead to experimental generation of GHz states. The second experimental project is quantum teleportation based on the improvement on the realization of Bell-state detection.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113137</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113137</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1009" target="n1010">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR: Quantum Entanglement, Information and Computation</data>
      <data key="e_abstract">EIA-0113137&lt;br/&gt;Yanhua Shih&lt;br/&gt;University of Maryland Baltimore County&lt;br/&gt;&lt;br/&gt;Title: ITR: Quantum Entanglement, Information and Computation&lt;br/&gt;&lt;br/&gt;This project consists of a series of experimental and theoretical investigations on multi-photon entangled states and their fundamental role in quantum information and quantum computing. Major efforts underway include experimental and theoretical investigations on N-photon (N&gt;2 entangled states) and on &quot;partial entanglement&quot; in quantum information and quantum computation theory. &lt;br/&gt;&lt;br/&gt;Theoretical characterization of entanglement in mixed states is at the core of this project. The experimental investigations focus on two projects. The first one is the realization of source of three-photon entangled states. These may lead to experimental generation of GHz states. The second experimental project is quantum teleportation based on the improvement on the realization of Bell-state detection.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113137</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113137</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1011" target="n1012">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY (CISE): Verification and Supervisory Control of Hybrid Embedded Systems</data>
      <data key="e_abstract">Project Abstract&lt;br/&gt;Proposal #0113131&lt;br/&gt;Antsaklis, Panos&lt;br/&gt;U of Notre Dame&lt;br/&gt;&lt;br/&gt;The goals of this project are to develop algorithms and prototype software for the verification and supervision of hybrid embedded control systems; also for the identification of hybrid system models. This project is developing supervisory processors to supervise and control in real time the operation of large number of control processors interacting with the outside world. The control processors interact with the physical world while the supervisory processors are responsible for monitoring and maintaining the health of the&lt;br/&gt;distributed control system in a highly autonomous and fault-tolerant manner. The innovative characteristics of this project are as follows: 1) the development and application of novel discrete event supervisory methods to supervise hybrid embedded systems; 2) the development and application of novel approaches to the verification and supervision of hybrid, piece-wise linear systems; 3) the development of theory and algorithms to extend these results to a class of nonlinear hybrid systems; and, 4) the development of novel&lt;br/&gt;model identification methodologies and algorithms for hybrid systems. The project&apos;s approach will improve the ability of hybrid embedded control systems to deal with high complexity, undecidability and nonlinearity.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113131</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">113131</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1011" target="n1013">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY (CISE): Verification and Supervisory Control of Hybrid Embedded Systems</data>
      <data key="e_abstract">Project Abstract&lt;br/&gt;Proposal #0113131&lt;br/&gt;Antsaklis, Panos&lt;br/&gt;U of Notre Dame&lt;br/&gt;&lt;br/&gt;The goals of this project are to develop algorithms and prototype software for the verification and supervision of hybrid embedded control systems; also for the identification of hybrid system models. This project is developing supervisory processors to supervise and control in real time the operation of large number of control processors interacting with the outside world. The control processors interact with the physical world while the supervisory processors are responsible for monitoring and maintaining the health of the&lt;br/&gt;distributed control system in a highly autonomous and fault-tolerant manner. The innovative characteristics of this project are as follows: 1) the development and application of novel discrete event supervisory methods to supervise hybrid embedded systems; 2) the development and application of novel approaches to the verification and supervision of hybrid, piece-wise linear systems; 3) the development of theory and algorithms to extend these results to a class of nonlinear hybrid systems; and, 4) the development of novel&lt;br/&gt;model identification methodologies and algorithms for hybrid systems. The project&apos;s approach will improve the ability of hybrid embedded control systems to deal with high complexity, undecidability and nonlinearity.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113131</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">113131</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1012" target="n1013">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY (CISE): Verification and Supervisory Control of Hybrid Embedded Systems</data>
      <data key="e_abstract">Project Abstract&lt;br/&gt;Proposal #0113131&lt;br/&gt;Antsaklis, Panos&lt;br/&gt;U of Notre Dame&lt;br/&gt;&lt;br/&gt;The goals of this project are to develop algorithms and prototype software for the verification and supervision of hybrid embedded control systems; also for the identification of hybrid system models. This project is developing supervisory processors to supervise and control in real time the operation of large number of control processors interacting with the outside world. The control processors interact with the physical world while the supervisory processors are responsible for monitoring and maintaining the health of the&lt;br/&gt;distributed control system in a highly autonomous and fault-tolerant manner. The innovative characteristics of this project are as follows: 1) the development and application of novel discrete event supervisory methods to supervise hybrid embedded systems; 2) the development and application of novel approaches to the verification and supervision of hybrid, piece-wise linear systems; 3) the development of theory and algorithms to extend these results to a class of nonlinear hybrid systems; and, 4) the development of novel&lt;br/&gt;model identification methodologies and algorithms for hybrid systems. The project&apos;s approach will improve the ability of hybrid embedded control systems to deal with high complexity, undecidability and nonlinearity.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113131</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">113131</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1016" target="n1017">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ADORE -- A Framework for Adaptive Object Code Re-optimization</data>
      <data key="e_abstract">The research focuses on the design and implementation of a framework,&lt;br/&gt;called the ADaptive Object RE-optimization (ADORE) system, to evaluate&lt;br/&gt;the effectiveness of dynamic binary re-optimization, and to study the&lt;br/&gt;interaction between architecture/micro-architecture and a dynamic&lt;br/&gt;binary re-optimizer. The topics studied include: efficient profiling&lt;br/&gt;techniques; runtime hot region selection and formation; light-weight&lt;br/&gt;runtime optimizations; code patching and region linking techniques;&lt;br/&gt;architectural and micro-architectural support for dynamic binary&lt;br/&gt;optimization; and runtime specialization and value profiling.&lt;br/&gt;The first implementation of the ADORE system will be based on&lt;br/&gt;two different implementations of the IA-64 architecture, the&lt;br/&gt;Itanium and the McKinley. The performance of application code&lt;br/&gt;optimized for Itanium will be tested on McKinley using the&lt;br/&gt;ADORE system.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">105574</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105574</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1019" target="n1020">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Feedforward Control of Data Rate in Wireless Networks</data>
      <data key="e_abstract">Modern wireless networks use feedback control of transmit power to accommodate changing channel conditions, such as propagation loss, shadowing, multi-user interference, etc. This proposal suggests the utilization of feedforward control of data rate, in addition to the existing power control schemes, in order to more effectively combat these disturbances and, in addition, accommodate channel model uncertainties. These controllers use the bit error frequency, observed in the previous packet, in order to calculate the data rate of the packet to be transmitted next. Our preliminary results indicate that ideally this approach leads to a minimum of 20% throughput improvement, without additional power expenditures, or to a minimum of 30% decrease of transmit power, without decreasing the throughput. In some scenarios, this approach may lead to as much as 300% of throughput improvement or to 600% of power saving. Moreover, the efficacy of this&lt;br/&gt;approach is independent of whether a single or multi-user environment is considered, and no data rate wars take place.&lt;br/&gt; The efficacy of feedforward data rate control is mainly due to the following two reasons:&lt;br/&gt;(a) Unlike feedback power control, which adapts relatively slowly due to a finite step of power increase/decrease, feedforward data rate control adapts in the span of one packet transmission time. This leads to a more effective rejection of fast disturbances, such as the level of shadowing. The above-mentioned minima of throughput increase and power decrease are due to this fast adaptation capability.&lt;br/&gt;(b) Unlike feedback power control, which does not adapt to channel uncertainties (e.g., whether the channel is AWGN or Rayleigh), feedforward data rate control does accommodate these effects. The above-mentioned three-fold in-crease of the throughput and six-fold decrease of power are exactly due to this fact.&lt;br/&gt;The approach to the development of feedforward data rate controllers, considered in this proposal, is based on the following three steps: (i) First, a non-causal and non-realistic but optimal feedforward data rate controller is designed. It is non-causal because it calculates the optimal data rate as a function of bit error probability in the packet yet to be transmitted. It is non-realistic, because it uses the probability of bit error rather than the frequency of this event. It is designed solely in order to derive the least upper bound of the achievable throughput.&lt;br/&gt;(ii) Next, this controller is causified and made realistic. The causification is achieved by making the data rate of each packet a function of bit error probability in the previous packet. It is made realistic by using the frequency of bit error rather than its probability. Thus, an implementable controller is obtained and its performance is evaluated. It is shown that causification leads to a relatively small decrease of performance for all practical speeds of mobiles. However, using frequencies instead of probabilities may lead to a substantial performance loss. Thus, a certain level of filtering of bit error frequency is necessary.&lt;br/&gt;(iii) Finally, a filtered version of the above implementable controller is introduced and it is shown that a right level of filtering leads to an efficient performance. At this point, this level of filtering is investigated only experimen-tally (i.e., numerically), and a rigorous method for designing right filters is, along with others, a problem to be addressed in the proposed research.&lt;br/&gt; Based on the results to-date briefly mentioned above, the main tasks of the proposed research are as follows:&lt;br/&gt;1. Develop methods for design of implementable feedforward data rate controllers for wireless networks.&lt;br/&gt;2. Quantify the level of throughput increase and/or transmit power decrease when this technology is used.&lt;br/&gt;3. Develop an architecture in which feedforward data rate control can be used in both cellular and ad-hoc environ-ments.&lt;br/&gt; The impact of the proposed research is in providing wireless network designers with a new method for combating channel disturbances and uncertainties.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">106716</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">106716</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1023" target="n1024">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP: Model-Based Integration of Methods for the Optimization of Process Systems</data>
      <data key="e_abstract">This ITR research medium research project will put together an interdisciplinary research group whose aim will be to develop a new modeling and a solution framework that nontrivially integrates a wide class of solution methods, including mathematical programming, constraint satisfaction, and a variety of heuristic algorithms. The objective of this framework is to be able to effectively tackle challenging engineering optimization problems. The group will be composed of faculty and students from Computer Science, Operations Research and Chemical Engineering. The key goal for the group will be to identify principles according to which algorithms can be integrated. These principles will then form the basis for a modeling language that guides the integration. Furthermore, new solution techniques will be developed for hybrid models and for global optimization of nonconvex discrete/continuous problems.&lt;br/&gt;&lt;br/&gt;Two paths of integration will be investigated. One is to exploit problem structure at the modeling stage by using an idea developed in constraint logic programming: namely, by writing the problem in the form of &quot;global&quot; constraints that individually invoke algorithms tailored to the special structure of each constraint. A second is to make a fundamental distinction, at both the modeling and solution stage, between two types of algorithms: checkers and solvers. The research will be applied primarily to the optimization of process systems, with a focus on synthesis of reactive distillation systems and the integrated testing of new products and batch manufacturing. Because these problems have both discrete and continuous aspects, and the second involves difficult nonlinearities, they provide a natural context for combining optimization and logic-based methods. However, the research will develop a general-purpose approach to modeling and solution, rather than one that is exclusively for process systems applications.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121497</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121497</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n949" target="n1027">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY+IM+AP: Center for Applied Algorithms</data>
      <data key="e_abstract">Algorithms are the basic procedures by which computers solve problems. With the explosion in the use and connectivity of computers, and in the sizes of the data sets being used, the performance of algorithms is becoming increasingly important. Being able to solve a problem ten times faster, for example, could mean designing a drug next year instead of several years later, or reducing the cost of developing a new space structure by allowing faster and more extensive computer simulations. Over the past 30 years there have been significant advances in the basic theory of algorithms. These advances have led to a &quot;core knowledge&quot; concerning algorithms and algorithmic techniques that has now been applied across an amazing diversity of fields and applications---surely more broadly than calculus is now applied. &lt;br/&gt;&lt;br/&gt;The problem, however, is that there is a large gap between ongoing theoretical research, and the current use of algorithms in applications. It often takes more than ten years for the core ideas in a new algorithm to make it into an application, and ongoing theoretical research often does not properly address the needs of the applications. The purpose of the Center is to bridge this gap so that efficient and effective algorithms can be deployed more rapidly. This will be achieved through (1) a set of Problem Oriented Explorations (PROBEs), (2) developing an extensive set of web resources on algorithms, and (3) educational activities including holding workshops for educating teachers. The PROBEs will bring together algorithm designers and domain experts to rapidly deploy new algorithmic ideas within a specific domain.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">122581</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">122581</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n949" target="n1028">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY+IM+AP: Center for Applied Algorithms</data>
      <data key="e_abstract">Algorithms are the basic procedures by which computers solve problems. With the explosion in the use and connectivity of computers, and in the sizes of the data sets being used, the performance of algorithms is becoming increasingly important. Being able to solve a problem ten times faster, for example, could mean designing a drug next year instead of several years later, or reducing the cost of developing a new space structure by allowing faster and more extensive computer simulations. Over the past 30 years there have been significant advances in the basic theory of algorithms. These advances have led to a &quot;core knowledge&quot; concerning algorithms and algorithmic techniques that has now been applied across an amazing diversity of fields and applications---surely more broadly than calculus is now applied. &lt;br/&gt;&lt;br/&gt;The problem, however, is that there is a large gap between ongoing theoretical research, and the current use of algorithms in applications. It often takes more than ten years for the core ideas in a new algorithm to make it into an application, and ongoing theoretical research often does not properly address the needs of the applications. The purpose of the Center is to bridge this gap so that efficient and effective algorithms can be deployed more rapidly. This will be achieved through (1) a set of Problem Oriented Explorations (PROBEs), (2) developing an extensive set of web resources on algorithms, and (3) educational activities including holding workshops for educating teachers. The PROBEs will bring together algorithm designers and domain experts to rapidly deploy new algorithmic ideas within a specific domain.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">122581</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">122581</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n445" target="n949">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY+IM+AP: Center for Applied Algorithms</data>
      <data key="e_abstract">Algorithms are the basic procedures by which computers solve problems. With the explosion in the use and connectivity of computers, and in the sizes of the data sets being used, the performance of algorithms is becoming increasingly important. Being able to solve a problem ten times faster, for example, could mean designing a drug next year instead of several years later, or reducing the cost of developing a new space structure by allowing faster and more extensive computer simulations. Over the past 30 years there have been significant advances in the basic theory of algorithms. These advances have led to a &quot;core knowledge&quot; concerning algorithms and algorithmic techniques that has now been applied across an amazing diversity of fields and applications---surely more broadly than calculus is now applied. &lt;br/&gt;&lt;br/&gt;The problem, however, is that there is a large gap between ongoing theoretical research, and the current use of algorithms in applications. It often takes more than ten years for the core ideas in a new algorithm to make it into an application, and ongoing theoretical research often does not properly address the needs of the applications. The purpose of the Center is to bridge this gap so that efficient and effective algorithms can be deployed more rapidly. This will be achieved through (1) a set of Problem Oriented Explorations (PROBEs), (2) developing an extensive set of web resources on algorithms, and (3) educational activities including holding workshops for educating teachers. The PROBEs will bring together algorithm designers and domain experts to rapidly deploy new algorithmic ideas within a specific domain.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">122581</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">122581</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n949" target="n1030">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY+IM+AP: Center for Applied Algorithms</data>
      <data key="e_abstract">Algorithms are the basic procedures by which computers solve problems. With the explosion in the use and connectivity of computers, and in the sizes of the data sets being used, the performance of algorithms is becoming increasingly important. Being able to solve a problem ten times faster, for example, could mean designing a drug next year instead of several years later, or reducing the cost of developing a new space structure by allowing faster and more extensive computer simulations. Over the past 30 years there have been significant advances in the basic theory of algorithms. These advances have led to a &quot;core knowledge&quot; concerning algorithms and algorithmic techniques that has now been applied across an amazing diversity of fields and applications---surely more broadly than calculus is now applied. &lt;br/&gt;&lt;br/&gt;The problem, however, is that there is a large gap between ongoing theoretical research, and the current use of algorithms in applications. It often takes more than ten years for the core ideas in a new algorithm to make it into an application, and ongoing theoretical research often does not properly address the needs of the applications. The purpose of the Center is to bridge this gap so that efficient and effective algorithms can be deployed more rapidly. This will be achieved through (1) a set of Problem Oriented Explorations (PROBEs), (2) developing an extensive set of web resources on algorithms, and (3) educational activities including holding workshops for educating teachers. The PROBEs will bring together algorithm designers and domain experts to rapidly deploy new algorithmic ideas within a specific domain.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">122581</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">122581</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1027" target="n1028">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY+IM+AP: Center for Applied Algorithms</data>
      <data key="e_abstract">Algorithms are the basic procedures by which computers solve problems. With the explosion in the use and connectivity of computers, and in the sizes of the data sets being used, the performance of algorithms is becoming increasingly important. Being able to solve a problem ten times faster, for example, could mean designing a drug next year instead of several years later, or reducing the cost of developing a new space structure by allowing faster and more extensive computer simulations. Over the past 30 years there have been significant advances in the basic theory of algorithms. These advances have led to a &quot;core knowledge&quot; concerning algorithms and algorithmic techniques that has now been applied across an amazing diversity of fields and applications---surely more broadly than calculus is now applied. &lt;br/&gt;&lt;br/&gt;The problem, however, is that there is a large gap between ongoing theoretical research, and the current use of algorithms in applications. It often takes more than ten years for the core ideas in a new algorithm to make it into an application, and ongoing theoretical research often does not properly address the needs of the applications. The purpose of the Center is to bridge this gap so that efficient and effective algorithms can be deployed more rapidly. This will be achieved through (1) a set of Problem Oriented Explorations (PROBEs), (2) developing an extensive set of web resources on algorithms, and (3) educational activities including holding workshops for educating teachers. The PROBEs will bring together algorithm designers and domain experts to rapidly deploy new algorithmic ideas within a specific domain.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">122581</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">122581</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n445" target="n1027">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY+IM+AP: Center for Applied Algorithms</data>
      <data key="e_abstract">Algorithms are the basic procedures by which computers solve problems. With the explosion in the use and connectivity of computers, and in the sizes of the data sets being used, the performance of algorithms is becoming increasingly important. Being able to solve a problem ten times faster, for example, could mean designing a drug next year instead of several years later, or reducing the cost of developing a new space structure by allowing faster and more extensive computer simulations. Over the past 30 years there have been significant advances in the basic theory of algorithms. These advances have led to a &quot;core knowledge&quot; concerning algorithms and algorithmic techniques that has now been applied across an amazing diversity of fields and applications---surely more broadly than calculus is now applied. &lt;br/&gt;&lt;br/&gt;The problem, however, is that there is a large gap between ongoing theoretical research, and the current use of algorithms in applications. It often takes more than ten years for the core ideas in a new algorithm to make it into an application, and ongoing theoretical research often does not properly address the needs of the applications. The purpose of the Center is to bridge this gap so that efficient and effective algorithms can be deployed more rapidly. This will be achieved through (1) a set of Problem Oriented Explorations (PROBEs), (2) developing an extensive set of web resources on algorithms, and (3) educational activities including holding workshops for educating teachers. The PROBEs will bring together algorithm designers and domain experts to rapidly deploy new algorithmic ideas within a specific domain.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">122581</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">122581</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1027" target="n1030">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY+IM+AP: Center for Applied Algorithms</data>
      <data key="e_abstract">Algorithms are the basic procedures by which computers solve problems. With the explosion in the use and connectivity of computers, and in the sizes of the data sets being used, the performance of algorithms is becoming increasingly important. Being able to solve a problem ten times faster, for example, could mean designing a drug next year instead of several years later, or reducing the cost of developing a new space structure by allowing faster and more extensive computer simulations. Over the past 30 years there have been significant advances in the basic theory of algorithms. These advances have led to a &quot;core knowledge&quot; concerning algorithms and algorithmic techniques that has now been applied across an amazing diversity of fields and applications---surely more broadly than calculus is now applied. &lt;br/&gt;&lt;br/&gt;The problem, however, is that there is a large gap between ongoing theoretical research, and the current use of algorithms in applications. It often takes more than ten years for the core ideas in a new algorithm to make it into an application, and ongoing theoretical research often does not properly address the needs of the applications. The purpose of the Center is to bridge this gap so that efficient and effective algorithms can be deployed more rapidly. This will be achieved through (1) a set of Problem Oriented Explorations (PROBEs), (2) developing an extensive set of web resources on algorithms, and (3) educational activities including holding workshops for educating teachers. The PROBEs will bring together algorithm designers and domain experts to rapidly deploy new algorithmic ideas within a specific domain.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">122581</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">122581</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n445" target="n1028">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY+IM+AP: Center for Applied Algorithms</data>
      <data key="e_abstract">Algorithms are the basic procedures by which computers solve problems. With the explosion in the use and connectivity of computers, and in the sizes of the data sets being used, the performance of algorithms is becoming increasingly important. Being able to solve a problem ten times faster, for example, could mean designing a drug next year instead of several years later, or reducing the cost of developing a new space structure by allowing faster and more extensive computer simulations. Over the past 30 years there have been significant advances in the basic theory of algorithms. These advances have led to a &quot;core knowledge&quot; concerning algorithms and algorithmic techniques that has now been applied across an amazing diversity of fields and applications---surely more broadly than calculus is now applied. &lt;br/&gt;&lt;br/&gt;The problem, however, is that there is a large gap between ongoing theoretical research, and the current use of algorithms in applications. It often takes more than ten years for the core ideas in a new algorithm to make it into an application, and ongoing theoretical research often does not properly address the needs of the applications. The purpose of the Center is to bridge this gap so that efficient and effective algorithms can be deployed more rapidly. This will be achieved through (1) a set of Problem Oriented Explorations (PROBEs), (2) developing an extensive set of web resources on algorithms, and (3) educational activities including holding workshops for educating teachers. The PROBEs will bring together algorithm designers and domain experts to rapidly deploy new algorithmic ideas within a specific domain.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">122581</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">122581</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1028" target="n1030">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY+IM+AP: Center for Applied Algorithms</data>
      <data key="e_abstract">Algorithms are the basic procedures by which computers solve problems. With the explosion in the use and connectivity of computers, and in the sizes of the data sets being used, the performance of algorithms is becoming increasingly important. Being able to solve a problem ten times faster, for example, could mean designing a drug next year instead of several years later, or reducing the cost of developing a new space structure by allowing faster and more extensive computer simulations. Over the past 30 years there have been significant advances in the basic theory of algorithms. These advances have led to a &quot;core knowledge&quot; concerning algorithms and algorithmic techniques that has now been applied across an amazing diversity of fields and applications---surely more broadly than calculus is now applied. &lt;br/&gt;&lt;br/&gt;The problem, however, is that there is a large gap between ongoing theoretical research, and the current use of algorithms in applications. It often takes more than ten years for the core ideas in a new algorithm to make it into an application, and ongoing theoretical research often does not properly address the needs of the applications. The purpose of the Center is to bridge this gap so that efficient and effective algorithms can be deployed more rapidly. This will be achieved through (1) a set of Problem Oriented Explorations (PROBEs), (2) developing an extensive set of web resources on algorithms, and (3) educational activities including holding workshops for educating teachers. The PROBEs will bring together algorithm designers and domain experts to rapidly deploy new algorithmic ideas within a specific domain.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">122581</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">122581</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n445" target="n1030">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY+IM+AP: Center for Applied Algorithms</data>
      <data key="e_abstract">Algorithms are the basic procedures by which computers solve problems. With the explosion in the use and connectivity of computers, and in the sizes of the data sets being used, the performance of algorithms is becoming increasingly important. Being able to solve a problem ten times faster, for example, could mean designing a drug next year instead of several years later, or reducing the cost of developing a new space structure by allowing faster and more extensive computer simulations. Over the past 30 years there have been significant advances in the basic theory of algorithms. These advances have led to a &quot;core knowledge&quot; concerning algorithms and algorithmic techniques that has now been applied across an amazing diversity of fields and applications---surely more broadly than calculus is now applied. &lt;br/&gt;&lt;br/&gt;The problem, however, is that there is a large gap between ongoing theoretical research, and the current use of algorithms in applications. It often takes more than ten years for the core ideas in a new algorithm to make it into an application, and ongoing theoretical research often does not properly address the needs of the applications. The purpose of the Center is to bridge this gap so that efficient and effective algorithms can be deployed more rapidly. This will be achieved through (1) a set of Problem Oriented Explorations (PROBEs), (2) developing an extensive set of web resources on algorithms, and (3) educational activities including holding workshops for educating teachers. The PROBEs will bring together algorithm designers and domain experts to rapidly deploy new algorithmic ideas within a specific domain.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">122581</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">122581</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1031" target="n1032">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">EI: Crossing the Interdisciplinary Barrier: An Integrated Undergraduate Program in Bioinformatics</data>
      <data key="e_abstract">EIA- 0122582&lt;br/&gt;Raymer, Michael&lt;br/&gt;Wright State University&lt;br/&gt;&lt;br/&gt;EI: An Integrated Undergraduate Program in Bioinformatics&lt;br/&gt;&lt;br/&gt;This project involves the development of an interdisciplinary, integrated undergraduate program in bioinformatics. In particular, the project&apos;s goal includes the design of a program of undergraduate study that embodies an interdisciplinary synthesis of the topics in computer science and biology required for graduates to successfully pursue careers in bioinformatics. The project also plans to implement, test, and refine the program of study so that it may be used as a prototype for similar curricula at universities nation-wide. Pedagogy foci are to design, integrate, and test approaches for teaching topics in bioinformatics while providing research experiences for all students in the program. The project is strengthened by the participation of a group of academic, government, and industry bioinformatics researchers and professionals who act as an advisory committee. The emerging science of bioinformatics seeks to develop computational algorithms and techniques for analyzing large biological databases in search of information that can be applied to specific problems in the life sciences. This project addresses the education of professionals with a background in this field and contributes to satisfying a nationally growing workforce need.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">122582</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">122582</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1031" target="n1033">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">EI: Crossing the Interdisciplinary Barrier: An Integrated Undergraduate Program in Bioinformatics</data>
      <data key="e_abstract">EIA- 0122582&lt;br/&gt;Raymer, Michael&lt;br/&gt;Wright State University&lt;br/&gt;&lt;br/&gt;EI: An Integrated Undergraduate Program in Bioinformatics&lt;br/&gt;&lt;br/&gt;This project involves the development of an interdisciplinary, integrated undergraduate program in bioinformatics. In particular, the project&apos;s goal includes the design of a program of undergraduate study that embodies an interdisciplinary synthesis of the topics in computer science and biology required for graduates to successfully pursue careers in bioinformatics. The project also plans to implement, test, and refine the program of study so that it may be used as a prototype for similar curricula at universities nation-wide. Pedagogy foci are to design, integrate, and test approaches for teaching topics in bioinformatics while providing research experiences for all students in the program. The project is strengthened by the participation of a group of academic, government, and industry bioinformatics researchers and professionals who act as an advisory committee. The emerging science of bioinformatics seeks to develop computational algorithms and techniques for analyzing large biological databases in search of information that can be applied to specific problems in the life sciences. This project addresses the education of professionals with a background in this field and contributes to satisfying a nationally growing workforce need.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">122582</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">122582</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1032" target="n1033">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">EI: Crossing the Interdisciplinary Barrier: An Integrated Undergraduate Program in Bioinformatics</data>
      <data key="e_abstract">EIA- 0122582&lt;br/&gt;Raymer, Michael&lt;br/&gt;Wright State University&lt;br/&gt;&lt;br/&gt;EI: An Integrated Undergraduate Program in Bioinformatics&lt;br/&gt;&lt;br/&gt;This project involves the development of an interdisciplinary, integrated undergraduate program in bioinformatics. In particular, the project&apos;s goal includes the design of a program of undergraduate study that embodies an interdisciplinary synthesis of the topics in computer science and biology required for graduates to successfully pursue careers in bioinformatics. The project also plans to implement, test, and refine the program of study so that it may be used as a prototype for similar curricula at universities nation-wide. Pedagogy foci are to design, integrate, and test approaches for teaching topics in bioinformatics while providing research experiences for all students in the program. The project is strengthened by the participation of a group of academic, government, and industry bioinformatics researchers and professionals who act as an advisory committee. The emerging science of bioinformatics seeks to develop computational algorithms and techniques for analyzing large biological databases in search of information that can be applied to specific problems in the life sciences. This project addresses the education of professionals with a background in this field and contributes to satisfying a nationally growing workforce need.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">122582</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">122582</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1034" target="n1035">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/AP: An International Virtual-Data Grid Laboratory for Data Intensive Science</data>
      <data key="e_abstract">The computational GRID vision is that everyone at a desktop machine could eventually have the power of a supercomputer at his or her fingertips. The GRID takes its name from the electrical utility analogy. &lt;br/&gt;&lt;br/&gt;GRID computing holds the promise of transforming the Internet, now used for communication, mainly e- mail and instant messaging, while the Web is an information retrieval system, enabling computer users to have access to text, images and music. Grid computing builds on the result of previous and ongoing research in networking, distributed computing, seamless computing, meta computing, web technologies, and other related topics. &lt;br/&gt;&lt;br/&gt;In this proposal, the National Virtual Observatory, the Sloan Digital Sky Survey, the Laser Interferometer Gravitational-Wave Observatory (LIGO) and the Large Hadron Collider experiments, all well-presented science projects (two of which have support via NSF Major Research Equipment funds) have data needs appropriate for GRID based technologies and are depending on such technologies for the successful operations of their experiments. The experiments will serve as test beds for GRID concepts.</data>
      <data key="e_pgm">1221</data>
      <data key="e_label">122557</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">122557</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1034" target="n1036">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/AP: An International Virtual-Data Grid Laboratory for Data Intensive Science</data>
      <data key="e_abstract">The computational GRID vision is that everyone at a desktop machine could eventually have the power of a supercomputer at his or her fingertips. The GRID takes its name from the electrical utility analogy. &lt;br/&gt;&lt;br/&gt;GRID computing holds the promise of transforming the Internet, now used for communication, mainly e- mail and instant messaging, while the Web is an information retrieval system, enabling computer users to have access to text, images and music. Grid computing builds on the result of previous and ongoing research in networking, distributed computing, seamless computing, meta computing, web technologies, and other related topics. &lt;br/&gt;&lt;br/&gt;In this proposal, the National Virtual Observatory, the Sloan Digital Sky Survey, the Laser Interferometer Gravitational-Wave Observatory (LIGO) and the Large Hadron Collider experiments, all well-presented science projects (two of which have support via NSF Major Research Equipment funds) have data needs appropriate for GRID based technologies and are depending on such technologies for the successful operations of their experiments. The experiments will serve as test beds for GRID concepts.</data>
      <data key="e_pgm">1221</data>
      <data key="e_label">122557</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">122557</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1034" target="n1037">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/AP: An International Virtual-Data Grid Laboratory for Data Intensive Science</data>
      <data key="e_abstract">The computational GRID vision is that everyone at a desktop machine could eventually have the power of a supercomputer at his or her fingertips. The GRID takes its name from the electrical utility analogy. &lt;br/&gt;&lt;br/&gt;GRID computing holds the promise of transforming the Internet, now used for communication, mainly e- mail and instant messaging, while the Web is an information retrieval system, enabling computer users to have access to text, images and music. Grid computing builds on the result of previous and ongoing research in networking, distributed computing, seamless computing, meta computing, web technologies, and other related topics. &lt;br/&gt;&lt;br/&gt;In this proposal, the National Virtual Observatory, the Sloan Digital Sky Survey, the Laser Interferometer Gravitational-Wave Observatory (LIGO) and the Large Hadron Collider experiments, all well-presented science projects (two of which have support via NSF Major Research Equipment funds) have data needs appropriate for GRID based technologies and are depending on such technologies for the successful operations of their experiments. The experiments will serve as test beds for GRID concepts.</data>
      <data key="e_pgm">1221</data>
      <data key="e_label">122557</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">122557</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1034" target="n1038">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/AP: An International Virtual-Data Grid Laboratory for Data Intensive Science</data>
      <data key="e_abstract">The computational GRID vision is that everyone at a desktop machine could eventually have the power of a supercomputer at his or her fingertips. The GRID takes its name from the electrical utility analogy. &lt;br/&gt;&lt;br/&gt;GRID computing holds the promise of transforming the Internet, now used for communication, mainly e- mail and instant messaging, while the Web is an information retrieval system, enabling computer users to have access to text, images and music. Grid computing builds on the result of previous and ongoing research in networking, distributed computing, seamless computing, meta computing, web technologies, and other related topics. &lt;br/&gt;&lt;br/&gt;In this proposal, the National Virtual Observatory, the Sloan Digital Sky Survey, the Laser Interferometer Gravitational-Wave Observatory (LIGO) and the Large Hadron Collider experiments, all well-presented science projects (two of which have support via NSF Major Research Equipment funds) have data needs appropriate for GRID based technologies and are depending on such technologies for the successful operations of their experiments. The experiments will serve as test beds for GRID concepts.</data>
      <data key="e_pgm">1221</data>
      <data key="e_label">122557</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">122557</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1035" target="n1036">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/AP: An International Virtual-Data Grid Laboratory for Data Intensive Science</data>
      <data key="e_abstract">The computational GRID vision is that everyone at a desktop machine could eventually have the power of a supercomputer at his or her fingertips. The GRID takes its name from the electrical utility analogy. &lt;br/&gt;&lt;br/&gt;GRID computing holds the promise of transforming the Internet, now used for communication, mainly e- mail and instant messaging, while the Web is an information retrieval system, enabling computer users to have access to text, images and music. Grid computing builds on the result of previous and ongoing research in networking, distributed computing, seamless computing, meta computing, web technologies, and other related topics. &lt;br/&gt;&lt;br/&gt;In this proposal, the National Virtual Observatory, the Sloan Digital Sky Survey, the Laser Interferometer Gravitational-Wave Observatory (LIGO) and the Large Hadron Collider experiments, all well-presented science projects (two of which have support via NSF Major Research Equipment funds) have data needs appropriate for GRID based technologies and are depending on such technologies for the successful operations of their experiments. The experiments will serve as test beds for GRID concepts.</data>
      <data key="e_pgm">1221</data>
      <data key="e_label">122557</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">122557</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1035" target="n1037">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/AP: An International Virtual-Data Grid Laboratory for Data Intensive Science</data>
      <data key="e_abstract">The computational GRID vision is that everyone at a desktop machine could eventually have the power of a supercomputer at his or her fingertips. The GRID takes its name from the electrical utility analogy. &lt;br/&gt;&lt;br/&gt;GRID computing holds the promise of transforming the Internet, now used for communication, mainly e- mail and instant messaging, while the Web is an information retrieval system, enabling computer users to have access to text, images and music. Grid computing builds on the result of previous and ongoing research in networking, distributed computing, seamless computing, meta computing, web technologies, and other related topics. &lt;br/&gt;&lt;br/&gt;In this proposal, the National Virtual Observatory, the Sloan Digital Sky Survey, the Laser Interferometer Gravitational-Wave Observatory (LIGO) and the Large Hadron Collider experiments, all well-presented science projects (two of which have support via NSF Major Research Equipment funds) have data needs appropriate for GRID based technologies and are depending on such technologies for the successful operations of their experiments. The experiments will serve as test beds for GRID concepts.</data>
      <data key="e_pgm">1221</data>
      <data key="e_label">122557</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">122557</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1035" target="n1038">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/AP: An International Virtual-Data Grid Laboratory for Data Intensive Science</data>
      <data key="e_abstract">The computational GRID vision is that everyone at a desktop machine could eventually have the power of a supercomputer at his or her fingertips. The GRID takes its name from the electrical utility analogy. &lt;br/&gt;&lt;br/&gt;GRID computing holds the promise of transforming the Internet, now used for communication, mainly e- mail and instant messaging, while the Web is an information retrieval system, enabling computer users to have access to text, images and music. Grid computing builds on the result of previous and ongoing research in networking, distributed computing, seamless computing, meta computing, web technologies, and other related topics. &lt;br/&gt;&lt;br/&gt;In this proposal, the National Virtual Observatory, the Sloan Digital Sky Survey, the Laser Interferometer Gravitational-Wave Observatory (LIGO) and the Large Hadron Collider experiments, all well-presented science projects (two of which have support via NSF Major Research Equipment funds) have data needs appropriate for GRID based technologies and are depending on such technologies for the successful operations of their experiments. The experiments will serve as test beds for GRID concepts.</data>
      <data key="e_pgm">1221</data>
      <data key="e_label">122557</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">122557</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1036" target="n1037">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/AP: An International Virtual-Data Grid Laboratory for Data Intensive Science</data>
      <data key="e_abstract">The computational GRID vision is that everyone at a desktop machine could eventually have the power of a supercomputer at his or her fingertips. The GRID takes its name from the electrical utility analogy. &lt;br/&gt;&lt;br/&gt;GRID computing holds the promise of transforming the Internet, now used for communication, mainly e- mail and instant messaging, while the Web is an information retrieval system, enabling computer users to have access to text, images and music. Grid computing builds on the result of previous and ongoing research in networking, distributed computing, seamless computing, meta computing, web technologies, and other related topics. &lt;br/&gt;&lt;br/&gt;In this proposal, the National Virtual Observatory, the Sloan Digital Sky Survey, the Laser Interferometer Gravitational-Wave Observatory (LIGO) and the Large Hadron Collider experiments, all well-presented science projects (two of which have support via NSF Major Research Equipment funds) have data needs appropriate for GRID based technologies and are depending on such technologies for the successful operations of their experiments. The experiments will serve as test beds for GRID concepts.</data>
      <data key="e_pgm">1221</data>
      <data key="e_label">122557</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">122557</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1036" target="n1038">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/AP: An International Virtual-Data Grid Laboratory for Data Intensive Science</data>
      <data key="e_abstract">The computational GRID vision is that everyone at a desktop machine could eventually have the power of a supercomputer at his or her fingertips. The GRID takes its name from the electrical utility analogy. &lt;br/&gt;&lt;br/&gt;GRID computing holds the promise of transforming the Internet, now used for communication, mainly e- mail and instant messaging, while the Web is an information retrieval system, enabling computer users to have access to text, images and music. Grid computing builds on the result of previous and ongoing research in networking, distributed computing, seamless computing, meta computing, web technologies, and other related topics. &lt;br/&gt;&lt;br/&gt;In this proposal, the National Virtual Observatory, the Sloan Digital Sky Survey, the Laser Interferometer Gravitational-Wave Observatory (LIGO) and the Large Hadron Collider experiments, all well-presented science projects (two of which have support via NSF Major Research Equipment funds) have data needs appropriate for GRID based technologies and are depending on such technologies for the successful operations of their experiments. The experiments will serve as test beds for GRID concepts.</data>
      <data key="e_pgm">1221</data>
      <data key="e_label">122557</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">122557</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1037" target="n1038">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/AP: An International Virtual-Data Grid Laboratory for Data Intensive Science</data>
      <data key="e_abstract">The computational GRID vision is that everyone at a desktop machine could eventually have the power of a supercomputer at his or her fingertips. The GRID takes its name from the electrical utility analogy. &lt;br/&gt;&lt;br/&gt;GRID computing holds the promise of transforming the Internet, now used for communication, mainly e- mail and instant messaging, while the Web is an information retrieval system, enabling computer users to have access to text, images and music. Grid computing builds on the result of previous and ongoing research in networking, distributed computing, seamless computing, meta computing, web technologies, and other related topics. &lt;br/&gt;&lt;br/&gt;In this proposal, the National Virtual Observatory, the Sloan Digital Sky Survey, the Laser Interferometer Gravitational-Wave Observatory (LIGO) and the Large Hadron Collider experiments, all well-presented science projects (two of which have support via NSF Major Research Equipment funds) have data needs appropriate for GRID based technologies and are depending on such technologies for the successful operations of their experiments. The experiments will serve as test beds for GRID concepts.</data>
      <data key="e_pgm">1221</data>
      <data key="e_label">122557</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">122557</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n90" target="n1040">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM:3D Shape-Based Retrieval and Its Applications</data>
      <data key="e_abstract">This research will investigate methods for automatic retrieval and analysis&lt;br/&gt;of 3D models. It will develop computational representations of 3D shape&lt;br/&gt;for which indices can be built, similarity queries can be answered&lt;br/&gt;efficiently, and interesting features can be computed robustly. Next, it will&lt;br/&gt;build user interfaces which permit untrained users to specify shape-based&lt;br/&gt;queries. This will include queries specified with text, 3D models, 2D&lt;br/&gt;sketching, and high-level methods based on constraints and rules. It will&lt;br/&gt;combine elements of computer graphics, computer vision, and computational&lt;br/&gt;geometry.&lt;br/&gt;&lt;br/&gt;Applications of shape-based query methods will include Internet search engines,&lt;br/&gt;computer-aided design, molecular biology, medicine, and security. In each&lt;br/&gt;application the researchers will work with domain experts to understand the&lt;br/&gt;critical elements of the 3D databases and the challenging shape queries for&lt;br/&gt;which new methods are required. For example, working with molecular biologists&lt;br/&gt;will help develop query tools for the Protein Data Bank to find macromolecules&lt;br/&gt;matching a given shape. These methods will aid classification of proteins for&lt;br/&gt;which only low-resolution electron density maps are available, and aid searches&lt;br/&gt;for proteins matching a specific binding site.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121446</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121446</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n90" target="n1041">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM:3D Shape-Based Retrieval and Its Applications</data>
      <data key="e_abstract">This research will investigate methods for automatic retrieval and analysis&lt;br/&gt;of 3D models. It will develop computational representations of 3D shape&lt;br/&gt;for which indices can be built, similarity queries can be answered&lt;br/&gt;efficiently, and interesting features can be computed robustly. Next, it will&lt;br/&gt;build user interfaces which permit untrained users to specify shape-based&lt;br/&gt;queries. This will include queries specified with text, 3D models, 2D&lt;br/&gt;sketching, and high-level methods based on constraints and rules. It will&lt;br/&gt;combine elements of computer graphics, computer vision, and computational&lt;br/&gt;geometry.&lt;br/&gt;&lt;br/&gt;Applications of shape-based query methods will include Internet search engines,&lt;br/&gt;computer-aided design, molecular biology, medicine, and security. In each&lt;br/&gt;application the researchers will work with domain experts to understand the&lt;br/&gt;critical elements of the 3D databases and the challenging shape queries for&lt;br/&gt;which new methods are required. For example, working with molecular biologists&lt;br/&gt;will help develop query tools for the Protein Data Bank to find macromolecules&lt;br/&gt;matching a given shape. These methods will aid classification of proteins for&lt;br/&gt;which only low-resolution electron density maps are available, and aid searches&lt;br/&gt;for proteins matching a specific binding site.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121446</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121446</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n90" target="n712">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM:3D Shape-Based Retrieval and Its Applications</data>
      <data key="e_abstract">This research will investigate methods for automatic retrieval and analysis&lt;br/&gt;of 3D models. It will develop computational representations of 3D shape&lt;br/&gt;for which indices can be built, similarity queries can be answered&lt;br/&gt;efficiently, and interesting features can be computed robustly. Next, it will&lt;br/&gt;build user interfaces which permit untrained users to specify shape-based&lt;br/&gt;queries. This will include queries specified with text, 3D models, 2D&lt;br/&gt;sketching, and high-level methods based on constraints and rules. It will&lt;br/&gt;combine elements of computer graphics, computer vision, and computational&lt;br/&gt;geometry.&lt;br/&gt;&lt;br/&gt;Applications of shape-based query methods will include Internet search engines,&lt;br/&gt;computer-aided design, molecular biology, medicine, and security. In each&lt;br/&gt;application the researchers will work with domain experts to understand the&lt;br/&gt;critical elements of the 3D databases and the challenging shape queries for&lt;br/&gt;which new methods are required. For example, working with molecular biologists&lt;br/&gt;will help develop query tools for the Protein Data Bank to find macromolecules&lt;br/&gt;matching a given shape. These methods will aid classification of proteins for&lt;br/&gt;which only low-resolution electron density maps are available, and aid searches&lt;br/&gt;for proteins matching a specific binding site.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121446</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121446</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1040" target="n1041">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM:3D Shape-Based Retrieval and Its Applications</data>
      <data key="e_abstract">This research will investigate methods for automatic retrieval and analysis&lt;br/&gt;of 3D models. It will develop computational representations of 3D shape&lt;br/&gt;for which indices can be built, similarity queries can be answered&lt;br/&gt;efficiently, and interesting features can be computed robustly. Next, it will&lt;br/&gt;build user interfaces which permit untrained users to specify shape-based&lt;br/&gt;queries. This will include queries specified with text, 3D models, 2D&lt;br/&gt;sketching, and high-level methods based on constraints and rules. It will&lt;br/&gt;combine elements of computer graphics, computer vision, and computational&lt;br/&gt;geometry.&lt;br/&gt;&lt;br/&gt;Applications of shape-based query methods will include Internet search engines,&lt;br/&gt;computer-aided design, molecular biology, medicine, and security. In each&lt;br/&gt;application the researchers will work with domain experts to understand the&lt;br/&gt;critical elements of the 3D databases and the challenging shape queries for&lt;br/&gt;which new methods are required. For example, working with molecular biologists&lt;br/&gt;will help develop query tools for the Protein Data Bank to find macromolecules&lt;br/&gt;matching a given shape. These methods will aid classification of proteins for&lt;br/&gt;which only low-resolution electron density maps are available, and aid searches&lt;br/&gt;for proteins matching a specific binding site.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121446</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121446</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n712" target="n1040">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM:3D Shape-Based Retrieval and Its Applications</data>
      <data key="e_abstract">This research will investigate methods for automatic retrieval and analysis&lt;br/&gt;of 3D models. It will develop computational representations of 3D shape&lt;br/&gt;for which indices can be built, similarity queries can be answered&lt;br/&gt;efficiently, and interesting features can be computed robustly. Next, it will&lt;br/&gt;build user interfaces which permit untrained users to specify shape-based&lt;br/&gt;queries. This will include queries specified with text, 3D models, 2D&lt;br/&gt;sketching, and high-level methods based on constraints and rules. It will&lt;br/&gt;combine elements of computer graphics, computer vision, and computational&lt;br/&gt;geometry.&lt;br/&gt;&lt;br/&gt;Applications of shape-based query methods will include Internet search engines,&lt;br/&gt;computer-aided design, molecular biology, medicine, and security. In each&lt;br/&gt;application the researchers will work with domain experts to understand the&lt;br/&gt;critical elements of the 3D databases and the challenging shape queries for&lt;br/&gt;which new methods are required. For example, working with molecular biologists&lt;br/&gt;will help develop query tools for the Protein Data Bank to find macromolecules&lt;br/&gt;matching a given shape. These methods will aid classification of proteins for&lt;br/&gt;which only low-resolution electron density maps are available, and aid searches&lt;br/&gt;for proteins matching a specific binding site.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121446</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121446</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n712" target="n1041">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM:3D Shape-Based Retrieval and Its Applications</data>
      <data key="e_abstract">This research will investigate methods for automatic retrieval and analysis&lt;br/&gt;of 3D models. It will develop computational representations of 3D shape&lt;br/&gt;for which indices can be built, similarity queries can be answered&lt;br/&gt;efficiently, and interesting features can be computed robustly. Next, it will&lt;br/&gt;build user interfaces which permit untrained users to specify shape-based&lt;br/&gt;queries. This will include queries specified with text, 3D models, 2D&lt;br/&gt;sketching, and high-level methods based on constraints and rules. It will&lt;br/&gt;combine elements of computer graphics, computer vision, and computational&lt;br/&gt;geometry.&lt;br/&gt;&lt;br/&gt;Applications of shape-based query methods will include Internet search engines,&lt;br/&gt;computer-aided design, molecular biology, medicine, and security. In each&lt;br/&gt;application the researchers will work with domain experts to understand the&lt;br/&gt;critical elements of the 3D databases and the challenging shape queries for&lt;br/&gt;which new methods are required. For example, working with molecular biologists&lt;br/&gt;will help develop query tools for the Protein Data Bank to find macromolecules&lt;br/&gt;matching a given shape. These methods will aid classification of proteins for&lt;br/&gt;which only low-resolution electron density maps are available, and aid searches&lt;br/&gt;for proteins matching a specific binding site.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121446</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121446</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1043" target="n1044">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY: Formal Design and Analysis of Hybrid Systems</data>
      <data key="e_abstract">Embedded systems, such as controllers in automotive, medical, and avionic systems, consist of a collection of interacting software modules reacting to and controlling an analog environment. Engineering disciplines such as control theory focus on continuous dynamics, and offer foundations for designing robust control laws for ensuring optimal performance of dynamical systems. Computing disciplines such as software engineering focus on discrete programs, and offer structured ways of implementing complex control and analysis tools for validating distributed software. For networked embedded devices with multiple modes of operation, the combination of the complexity in both discrete and continuous aspects leads to fundamental problems that are not yet well understood, and this makes the programming of reliable embedded systems a particularly challenging task. A systematic approach to designing embedded devices requires combining tools from control theory and modern software engineering, and the emerging theory of hybrid systems---systems with tightly integrated discrete and continuous dynamics, has the potential to provide the foundation. Despite the great appeal of hybrid systems as a model, the applicability of the state-of-the-art analysis and design techniques for hybrid systems has been limited to examples of small size due to complexity. This ITR research aims to develop foundations and tools for automatic abstraction and hierarchical decomposition as a means of simplification and scalability. &lt;br/&gt;&lt;br/&gt;To facilitate high-level design of embedded software, modeling concepts such as hierarchy, modularity, reuse, compositionality, and object-orientation, are explored to develop a theory of hierarchical hybrid systems with an accompanying a compositional calculus of refinement. This will be the basis for behavioral interfaces and descriptions of components at different levels of abstractions. For rigorously specifying and evaluating design alternatives and correctness requirements, automated techniques such as model checking are very effective. To apply these techniques for formal analysis of hybrid systems, this research is developing automated schemes for constructing abstractions of hybrid models. The technical directions being pursued include model checking algorithms that exploit hierarchy, algorithms for extracting finite-state approximations using predicate abstraction, counter-example guided refinement of abstractions, property-preserving bisimulation-based reductions of continuous differential equations, and assume-guarantee reasoning. The results of this research are being integrated in software tools for modeling and analysis of hybrid systems. The benefits of the techniques for developing embedded systems with higher assurance for safety and reliability are evaluated in an experimental testbed of multiple, autonomous, mobile robots.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121431</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121431</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1045" target="n1046">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Collaboration-Oriented Aspects</data>
      <data key="e_abstract">Abstract:&lt;br/&gt;&lt;br/&gt;0098643&lt;br/&gt;&lt;br/&gt;Collaboration-Oriented Aspects&lt;br/&gt;&lt;br/&gt;PI: Karl Lieberherr&lt;br/&gt;co-PI: David Lorenz&lt;br/&gt;&lt;br/&gt;The goal of AOP (Aspect-Oriented Programming) is to turn a tangled and scattered implementation of a crosscutting concern into an aspect, i.e. a well-modularized &lt;br/&gt;implementation of the concern. The goal of the proposed research is to design and implement a collaboration-based language for aspect-oriented programming that supports reusable aspects. The potential impact of the proposed research is to improve the development and maintenance of complex software. &lt;br/&gt;&lt;br/&gt;The design and implementation of the new language will be evaluated in two diverse domains: telecommunication applications in collaboration with BBN and banking applications in collaborations with UBS. The success of the project will be measured based on the ease of evolution of the applications that will be built&lt;br/&gt;with our language. It is expected that the tangling control offered by AOP&lt;br/&gt;and the loose coupling between class graphs and path sets offered by &lt;br/&gt;adaptive programming will lead to more flexible software that is easier to evolve.</data>
      <data key="e_pgm">2880</data>
      <data key="e_label">98643</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98643</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1047" target="n1048">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+AP: Quality-Scalable Information Flow Systems for Environmental Observation and Forecasting</data>
      <data key="e_abstract">Real-time Environmental Observation and Forecasting Systems (EOFS) will revolutionize the way scientists share information about the environment and represent an opportunity to break traditional information barriers separating scientists from society at large. EOFS are already in use, but they tend to be small-scale, application- and domain- specific, stand-alone systems. There is a need for evolution towards multi-purpose shared systems designed to adapt flexibly to evolving needs of information consumers. What is required are large-scale, shared, heterogeneous distributed systems that make extensive use of diverse sensor-based inputs, sophisticated numerical simulations, mobile and embedded real-time system components, wireless and wired communications, high-performance computers, and high capacity storage systems. &lt;br/&gt;&lt;br/&gt;This ITR medium project has assembled an inter-disciplinary team, including computer science and environmental science researchers in addition to a heterogeneous base of pilot users. This group will collaborate to develop software technology which will enable EOFS to evolve efficiently, and to deliver quantifiably reliable information about the environment at the right time and in the right form to the right users. The project focus is on EOFS for estuarine and coastal regions. These regions are selected because they are highly variable natural systems subject to intense human activity and with great social, environmental, economic and cultural value.&lt;br/&gt;&lt;br/&gt;The research will include:&lt;br/&gt;&lt;br/&gt;i. Developing missing integration concepts and technologies for EOFS, with emphasis on quality-scalable information processing, storage and access (the computer science research).&lt;br/&gt;ii. Closing the loop between environmental models and sensors, and implementing a next generation EOFS based on an existing prototype for an estuary with multiple and often conflicting uses (the environmental observation and forecasting systems research);&lt;br/&gt;iii. Using, evaluating and refining the EOFS prototype for scientific discovery, natural resources stewardship and emergency response, thus incorporating sound science in operational and management decisions of critical regional importance and national significance (the environmental science and management applications);&lt;br/&gt;iv. Developing pilot multi-level, inter-disciplinary educational programs that cross-train young people, computer scientists, environmental scientists and practitioners in the conceptualization, development and use of environmental information technology (the education impact).</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121475</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121475</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1047" target="n1049">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+AP: Quality-Scalable Information Flow Systems for Environmental Observation and Forecasting</data>
      <data key="e_abstract">Real-time Environmental Observation and Forecasting Systems (EOFS) will revolutionize the way scientists share information about the environment and represent an opportunity to break traditional information barriers separating scientists from society at large. EOFS are already in use, but they tend to be small-scale, application- and domain- specific, stand-alone systems. There is a need for evolution towards multi-purpose shared systems designed to adapt flexibly to evolving needs of information consumers. What is required are large-scale, shared, heterogeneous distributed systems that make extensive use of diverse sensor-based inputs, sophisticated numerical simulations, mobile and embedded real-time system components, wireless and wired communications, high-performance computers, and high capacity storage systems. &lt;br/&gt;&lt;br/&gt;This ITR medium project has assembled an inter-disciplinary team, including computer science and environmental science researchers in addition to a heterogeneous base of pilot users. This group will collaborate to develop software technology which will enable EOFS to evolve efficiently, and to deliver quantifiably reliable information about the environment at the right time and in the right form to the right users. The project focus is on EOFS for estuarine and coastal regions. These regions are selected because they are highly variable natural systems subject to intense human activity and with great social, environmental, economic and cultural value.&lt;br/&gt;&lt;br/&gt;The research will include:&lt;br/&gt;&lt;br/&gt;i. Developing missing integration concepts and technologies for EOFS, with emphasis on quality-scalable information processing, storage and access (the computer science research).&lt;br/&gt;ii. Closing the loop between environmental models and sensors, and implementing a next generation EOFS based on an existing prototype for an estuary with multiple and often conflicting uses (the environmental observation and forecasting systems research);&lt;br/&gt;iii. Using, evaluating and refining the EOFS prototype for scientific discovery, natural resources stewardship and emergency response, thus incorporating sound science in operational and management decisions of critical regional importance and national significance (the environmental science and management applications);&lt;br/&gt;iv. Developing pilot multi-level, inter-disciplinary educational programs that cross-train young people, computer scientists, environmental scientists and practitioners in the conceptualization, development and use of environmental information technology (the education impact).</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121475</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121475</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1047" target="n1050">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+AP: Quality-Scalable Information Flow Systems for Environmental Observation and Forecasting</data>
      <data key="e_abstract">Real-time Environmental Observation and Forecasting Systems (EOFS) will revolutionize the way scientists share information about the environment and represent an opportunity to break traditional information barriers separating scientists from society at large. EOFS are already in use, but they tend to be small-scale, application- and domain- specific, stand-alone systems. There is a need for evolution towards multi-purpose shared systems designed to adapt flexibly to evolving needs of information consumers. What is required are large-scale, shared, heterogeneous distributed systems that make extensive use of diverse sensor-based inputs, sophisticated numerical simulations, mobile and embedded real-time system components, wireless and wired communications, high-performance computers, and high capacity storage systems. &lt;br/&gt;&lt;br/&gt;This ITR medium project has assembled an inter-disciplinary team, including computer science and environmental science researchers in addition to a heterogeneous base of pilot users. This group will collaborate to develop software technology which will enable EOFS to evolve efficiently, and to deliver quantifiably reliable information about the environment at the right time and in the right form to the right users. The project focus is on EOFS for estuarine and coastal regions. These regions are selected because they are highly variable natural systems subject to intense human activity and with great social, environmental, economic and cultural value.&lt;br/&gt;&lt;br/&gt;The research will include:&lt;br/&gt;&lt;br/&gt;i. Developing missing integration concepts and technologies for EOFS, with emphasis on quality-scalable information processing, storage and access (the computer science research).&lt;br/&gt;ii. Closing the loop between environmental models and sensors, and implementing a next generation EOFS based on an existing prototype for an estuary with multiple and often conflicting uses (the environmental observation and forecasting systems research);&lt;br/&gt;iii. Using, evaluating and refining the EOFS prototype for scientific discovery, natural resources stewardship and emergency response, thus incorporating sound science in operational and management decisions of critical regional importance and national significance (the environmental science and management applications);&lt;br/&gt;iv. Developing pilot multi-level, inter-disciplinary educational programs that cross-train young people, computer scientists, environmental scientists and practitioners in the conceptualization, development and use of environmental information technology (the education impact).</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121475</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121475</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n307" target="n1047">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+AP: Quality-Scalable Information Flow Systems for Environmental Observation and Forecasting</data>
      <data key="e_abstract">Real-time Environmental Observation and Forecasting Systems (EOFS) will revolutionize the way scientists share information about the environment and represent an opportunity to break traditional information barriers separating scientists from society at large. EOFS are already in use, but they tend to be small-scale, application- and domain- specific, stand-alone systems. There is a need for evolution towards multi-purpose shared systems designed to adapt flexibly to evolving needs of information consumers. What is required are large-scale, shared, heterogeneous distributed systems that make extensive use of diverse sensor-based inputs, sophisticated numerical simulations, mobile and embedded real-time system components, wireless and wired communications, high-performance computers, and high capacity storage systems. &lt;br/&gt;&lt;br/&gt;This ITR medium project has assembled an inter-disciplinary team, including computer science and environmental science researchers in addition to a heterogeneous base of pilot users. This group will collaborate to develop software technology which will enable EOFS to evolve efficiently, and to deliver quantifiably reliable information about the environment at the right time and in the right form to the right users. The project focus is on EOFS for estuarine and coastal regions. These regions are selected because they are highly variable natural systems subject to intense human activity and with great social, environmental, economic and cultural value.&lt;br/&gt;&lt;br/&gt;The research will include:&lt;br/&gt;&lt;br/&gt;i. Developing missing integration concepts and technologies for EOFS, with emphasis on quality-scalable information processing, storage and access (the computer science research).&lt;br/&gt;ii. Closing the loop between environmental models and sensors, and implementing a next generation EOFS based on an existing prototype for an estuary with multiple and often conflicting uses (the environmental observation and forecasting systems research);&lt;br/&gt;iii. Using, evaluating and refining the EOFS prototype for scientific discovery, natural resources stewardship and emergency response, thus incorporating sound science in operational and management decisions of critical regional importance and national significance (the environmental science and management applications);&lt;br/&gt;iv. Developing pilot multi-level, inter-disciplinary educational programs that cross-train young people, computer scientists, environmental scientists and practitioners in the conceptualization, development and use of environmental information technology (the education impact).</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121475</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121475</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1048" target="n1049">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+AP: Quality-Scalable Information Flow Systems for Environmental Observation and Forecasting</data>
      <data key="e_abstract">Real-time Environmental Observation and Forecasting Systems (EOFS) will revolutionize the way scientists share information about the environment and represent an opportunity to break traditional information barriers separating scientists from society at large. EOFS are already in use, but they tend to be small-scale, application- and domain- specific, stand-alone systems. There is a need for evolution towards multi-purpose shared systems designed to adapt flexibly to evolving needs of information consumers. What is required are large-scale, shared, heterogeneous distributed systems that make extensive use of diverse sensor-based inputs, sophisticated numerical simulations, mobile and embedded real-time system components, wireless and wired communications, high-performance computers, and high capacity storage systems. &lt;br/&gt;&lt;br/&gt;This ITR medium project has assembled an inter-disciplinary team, including computer science and environmental science researchers in addition to a heterogeneous base of pilot users. This group will collaborate to develop software technology which will enable EOFS to evolve efficiently, and to deliver quantifiably reliable information about the environment at the right time and in the right form to the right users. The project focus is on EOFS for estuarine and coastal regions. These regions are selected because they are highly variable natural systems subject to intense human activity and with great social, environmental, economic and cultural value.&lt;br/&gt;&lt;br/&gt;The research will include:&lt;br/&gt;&lt;br/&gt;i. Developing missing integration concepts and technologies for EOFS, with emphasis on quality-scalable information processing, storage and access (the computer science research).&lt;br/&gt;ii. Closing the loop between environmental models and sensors, and implementing a next generation EOFS based on an existing prototype for an estuary with multiple and often conflicting uses (the environmental observation and forecasting systems research);&lt;br/&gt;iii. Using, evaluating and refining the EOFS prototype for scientific discovery, natural resources stewardship and emergency response, thus incorporating sound science in operational and management decisions of critical regional importance and national significance (the environmental science and management applications);&lt;br/&gt;iv. Developing pilot multi-level, inter-disciplinary educational programs that cross-train young people, computer scientists, environmental scientists and practitioners in the conceptualization, development and use of environmental information technology (the education impact).</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121475</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121475</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1048" target="n1050">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+AP: Quality-Scalable Information Flow Systems for Environmental Observation and Forecasting</data>
      <data key="e_abstract">Real-time Environmental Observation and Forecasting Systems (EOFS) will revolutionize the way scientists share information about the environment and represent an opportunity to break traditional information barriers separating scientists from society at large. EOFS are already in use, but they tend to be small-scale, application- and domain- specific, stand-alone systems. There is a need for evolution towards multi-purpose shared systems designed to adapt flexibly to evolving needs of information consumers. What is required are large-scale, shared, heterogeneous distributed systems that make extensive use of diverse sensor-based inputs, sophisticated numerical simulations, mobile and embedded real-time system components, wireless and wired communications, high-performance computers, and high capacity storage systems. &lt;br/&gt;&lt;br/&gt;This ITR medium project has assembled an inter-disciplinary team, including computer science and environmental science researchers in addition to a heterogeneous base of pilot users. This group will collaborate to develop software technology which will enable EOFS to evolve efficiently, and to deliver quantifiably reliable information about the environment at the right time and in the right form to the right users. The project focus is on EOFS for estuarine and coastal regions. These regions are selected because they are highly variable natural systems subject to intense human activity and with great social, environmental, economic and cultural value.&lt;br/&gt;&lt;br/&gt;The research will include:&lt;br/&gt;&lt;br/&gt;i. Developing missing integration concepts and technologies for EOFS, with emphasis on quality-scalable information processing, storage and access (the computer science research).&lt;br/&gt;ii. Closing the loop between environmental models and sensors, and implementing a next generation EOFS based on an existing prototype for an estuary with multiple and often conflicting uses (the environmental observation and forecasting systems research);&lt;br/&gt;iii. Using, evaluating and refining the EOFS prototype for scientific discovery, natural resources stewardship and emergency response, thus incorporating sound science in operational and management decisions of critical regional importance and national significance (the environmental science and management applications);&lt;br/&gt;iv. Developing pilot multi-level, inter-disciplinary educational programs that cross-train young people, computer scientists, environmental scientists and practitioners in the conceptualization, development and use of environmental information technology (the education impact).</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121475</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121475</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n307" target="n1048">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+AP: Quality-Scalable Information Flow Systems for Environmental Observation and Forecasting</data>
      <data key="e_abstract">Real-time Environmental Observation and Forecasting Systems (EOFS) will revolutionize the way scientists share information about the environment and represent an opportunity to break traditional information barriers separating scientists from society at large. EOFS are already in use, but they tend to be small-scale, application- and domain- specific, stand-alone systems. There is a need for evolution towards multi-purpose shared systems designed to adapt flexibly to evolving needs of information consumers. What is required are large-scale, shared, heterogeneous distributed systems that make extensive use of diverse sensor-based inputs, sophisticated numerical simulations, mobile and embedded real-time system components, wireless and wired communications, high-performance computers, and high capacity storage systems. &lt;br/&gt;&lt;br/&gt;This ITR medium project has assembled an inter-disciplinary team, including computer science and environmental science researchers in addition to a heterogeneous base of pilot users. This group will collaborate to develop software technology which will enable EOFS to evolve efficiently, and to deliver quantifiably reliable information about the environment at the right time and in the right form to the right users. The project focus is on EOFS for estuarine and coastal regions. These regions are selected because they are highly variable natural systems subject to intense human activity and with great social, environmental, economic and cultural value.&lt;br/&gt;&lt;br/&gt;The research will include:&lt;br/&gt;&lt;br/&gt;i. Developing missing integration concepts and technologies for EOFS, with emphasis on quality-scalable information processing, storage and access (the computer science research).&lt;br/&gt;ii. Closing the loop between environmental models and sensors, and implementing a next generation EOFS based on an existing prototype for an estuary with multiple and often conflicting uses (the environmental observation and forecasting systems research);&lt;br/&gt;iii. Using, evaluating and refining the EOFS prototype for scientific discovery, natural resources stewardship and emergency response, thus incorporating sound science in operational and management decisions of critical regional importance and national significance (the environmental science and management applications);&lt;br/&gt;iv. Developing pilot multi-level, inter-disciplinary educational programs that cross-train young people, computer scientists, environmental scientists and practitioners in the conceptualization, development and use of environmental information technology (the education impact).</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121475</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121475</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1049" target="n1050">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+AP: Quality-Scalable Information Flow Systems for Environmental Observation and Forecasting</data>
      <data key="e_abstract">Real-time Environmental Observation and Forecasting Systems (EOFS) will revolutionize the way scientists share information about the environment and represent an opportunity to break traditional information barriers separating scientists from society at large. EOFS are already in use, but they tend to be small-scale, application- and domain- specific, stand-alone systems. There is a need for evolution towards multi-purpose shared systems designed to adapt flexibly to evolving needs of information consumers. What is required are large-scale, shared, heterogeneous distributed systems that make extensive use of diverse sensor-based inputs, sophisticated numerical simulations, mobile and embedded real-time system components, wireless and wired communications, high-performance computers, and high capacity storage systems. &lt;br/&gt;&lt;br/&gt;This ITR medium project has assembled an inter-disciplinary team, including computer science and environmental science researchers in addition to a heterogeneous base of pilot users. This group will collaborate to develop software technology which will enable EOFS to evolve efficiently, and to deliver quantifiably reliable information about the environment at the right time and in the right form to the right users. The project focus is on EOFS for estuarine and coastal regions. These regions are selected because they are highly variable natural systems subject to intense human activity and with great social, environmental, economic and cultural value.&lt;br/&gt;&lt;br/&gt;The research will include:&lt;br/&gt;&lt;br/&gt;i. Developing missing integration concepts and technologies for EOFS, with emphasis on quality-scalable information processing, storage and access (the computer science research).&lt;br/&gt;ii. Closing the loop between environmental models and sensors, and implementing a next generation EOFS based on an existing prototype for an estuary with multiple and often conflicting uses (the environmental observation and forecasting systems research);&lt;br/&gt;iii. Using, evaluating and refining the EOFS prototype for scientific discovery, natural resources stewardship and emergency response, thus incorporating sound science in operational and management decisions of critical regional importance and national significance (the environmental science and management applications);&lt;br/&gt;iv. Developing pilot multi-level, inter-disciplinary educational programs that cross-train young people, computer scientists, environmental scientists and practitioners in the conceptualization, development and use of environmental information technology (the education impact).</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121475</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121475</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n307" target="n1049">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+AP: Quality-Scalable Information Flow Systems for Environmental Observation and Forecasting</data>
      <data key="e_abstract">Real-time Environmental Observation and Forecasting Systems (EOFS) will revolutionize the way scientists share information about the environment and represent an opportunity to break traditional information barriers separating scientists from society at large. EOFS are already in use, but they tend to be small-scale, application- and domain- specific, stand-alone systems. There is a need for evolution towards multi-purpose shared systems designed to adapt flexibly to evolving needs of information consumers. What is required are large-scale, shared, heterogeneous distributed systems that make extensive use of diverse sensor-based inputs, sophisticated numerical simulations, mobile and embedded real-time system components, wireless and wired communications, high-performance computers, and high capacity storage systems. &lt;br/&gt;&lt;br/&gt;This ITR medium project has assembled an inter-disciplinary team, including computer science and environmental science researchers in addition to a heterogeneous base of pilot users. This group will collaborate to develop software technology which will enable EOFS to evolve efficiently, and to deliver quantifiably reliable information about the environment at the right time and in the right form to the right users. The project focus is on EOFS for estuarine and coastal regions. These regions are selected because they are highly variable natural systems subject to intense human activity and with great social, environmental, economic and cultural value.&lt;br/&gt;&lt;br/&gt;The research will include:&lt;br/&gt;&lt;br/&gt;i. Developing missing integration concepts and technologies for EOFS, with emphasis on quality-scalable information processing, storage and access (the computer science research).&lt;br/&gt;ii. Closing the loop between environmental models and sensors, and implementing a next generation EOFS based on an existing prototype for an estuary with multiple and often conflicting uses (the environmental observation and forecasting systems research);&lt;br/&gt;iii. Using, evaluating and refining the EOFS prototype for scientific discovery, natural resources stewardship and emergency response, thus incorporating sound science in operational and management decisions of critical regional importance and national significance (the environmental science and management applications);&lt;br/&gt;iv. Developing pilot multi-level, inter-disciplinary educational programs that cross-train young people, computer scientists, environmental scientists and practitioners in the conceptualization, development and use of environmental information technology (the education impact).</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121475</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121475</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n307" target="n1050">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+AP: Quality-Scalable Information Flow Systems for Environmental Observation and Forecasting</data>
      <data key="e_abstract">Real-time Environmental Observation and Forecasting Systems (EOFS) will revolutionize the way scientists share information about the environment and represent an opportunity to break traditional information barriers separating scientists from society at large. EOFS are already in use, but they tend to be small-scale, application- and domain- specific, stand-alone systems. There is a need for evolution towards multi-purpose shared systems designed to adapt flexibly to evolving needs of information consumers. What is required are large-scale, shared, heterogeneous distributed systems that make extensive use of diverse sensor-based inputs, sophisticated numerical simulations, mobile and embedded real-time system components, wireless and wired communications, high-performance computers, and high capacity storage systems. &lt;br/&gt;&lt;br/&gt;This ITR medium project has assembled an inter-disciplinary team, including computer science and environmental science researchers in addition to a heterogeneous base of pilot users. This group will collaborate to develop software technology which will enable EOFS to evolve efficiently, and to deliver quantifiably reliable information about the environment at the right time and in the right form to the right users. The project focus is on EOFS for estuarine and coastal regions. These regions are selected because they are highly variable natural systems subject to intense human activity and with great social, environmental, economic and cultural value.&lt;br/&gt;&lt;br/&gt;The research will include:&lt;br/&gt;&lt;br/&gt;i. Developing missing integration concepts and technologies for EOFS, with emphasis on quality-scalable information processing, storage and access (the computer science research).&lt;br/&gt;ii. Closing the loop between environmental models and sensors, and implementing a next generation EOFS based on an existing prototype for an estuary with multiple and often conflicting uses (the environmental observation and forecasting systems research);&lt;br/&gt;iii. Using, evaluating and refining the EOFS prototype for scientific discovery, natural resources stewardship and emergency response, thus incorporating sound science in operational and management decisions of critical regional importance and national significance (the environmental science and management applications);&lt;br/&gt;iv. Developing pilot multi-level, inter-disciplinary educational programs that cross-train young people, computer scientists, environmental scientists and practitioners in the conceptualization, development and use of environmental information technology (the education impact).</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121475</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121475</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1052" target="n1053">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics - BDEI: Designing an Infrastructure for Heterogeneity of Ecosystem Data, Collaborators and Organizations</data>
      <data key="e_abstract">EIA-0131958&lt;br/&gt;Bowker, Geoffrey&lt;br/&gt;University of California - San Diego&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;BDEI: Designing an Infrastructure for Heterogeneity in Ecosystem Data, Collaborators and Organizations&lt;br/&gt;&lt;br/&gt;Project Summary&lt;br/&gt;&lt;br/&gt;Biodiversity and ecosystems data are currently being gathered in a large range of formats by a&lt;br/&gt;constellation of loosely connected private, government and not-for-profit agencies. The normal response&lt;br/&gt;to this double heterogeneity has been the development and enforcement of metadata (data about data)&lt;br/&gt;standards; in this response one tries to abstract data away from its organizational context in order to&lt;br/&gt;render it universally accessible. This project takes the opposite track, and seeks new ways of grounding&lt;br/&gt;environmental data in its organizational context in such a way that it can both be used more flexibly today&lt;br/&gt;and so it can retain its value longer. The hypothesis, based on the last 25 years of work in the field of&lt;br/&gt;Science Studies, is that formal data descriptions must be wrapped in informal descriptions in order to&lt;br/&gt;be useful. The informal description for short-lived data is provided by face-to-face contact, by&lt;br/&gt;exchanging graduate students, through conference papers and so forth. It is precisely this layer which is&lt;br/&gt;lost in highly distributed data collection efforts characteristic of biodiversity and ecosystem informatics; it&lt;br/&gt;is also this layer which is lost when data is wrapped in formal metadata and saved to disk. The goal of&lt;br/&gt;this project is to open up a major new field of database inquiry tied precisely to the specific problems of&lt;br/&gt;the biodiversity and ecosystems communities generated by their need for very long lasting and highly&lt;br/&gt;distributed data. The project will develop into a larger study of the articulation between metadata and&lt;br/&gt;narrative modes of wrapping data.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">131958</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">131958</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1054" target="n1055">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics - BDEI - Spatio-temporal Models of Biogeophysical Fields for Ecological Forecasting: A Cross-Disciplinary Incubation Activity</data>
      <data key="e_abstract">EIA-0131937&lt;br/&gt;Henebry, Geoffrey&lt;br/&gt;University of Nebraska - Lincoln&lt;br/&gt;&lt;br/&gt;BDEI: Spatio-temporal models of Biogeophysical Fields for Ecological Forecasting:&lt;br/&gt;A Cross-Disciplinary Incubation Activity &lt;br/&gt;&lt;br/&gt;Summary&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;We are now in an era of intensive earth observation: orbital platforms generate myriad remote sensing&lt;br/&gt;datastreams across a range of spatial, temporal, spectral, and radiometric resolutions. The number and&lt;br/&gt;variety of &quot;eyes in the skies&quot; are scheduled to increase significantly over the next few years. This&lt;br/&gt;veritable data deluge necessitates new ways of thinking about transforming remote sensing data into&lt;br/&gt;information about ecological patterns and processes. These datastreams hold the promise for&lt;br/&gt;environmental decision support. Yet, there is a critical need for theories and tools that will enable efficient&lt;br/&gt;and reliable characterization of spatio-temporal patterns contained in image time series. We think that&lt;br/&gt;such tools must be based on ecological expectations of land surface dynamics, analogous to&lt;br/&gt;climatological expectations. Ecological expectations would summarize across specific regions the typical&lt;br/&gt;temporal development of spatial pattern in biogeophysical fields. We have a robust principal method for&lt;br/&gt;extracting ecological expectations from remote sensing datastreams: projecting image time series into&lt;br/&gt;pattern metric spaces. To make ecological forecasting an operational possibility, we need the capability&lt;br/&gt;to establish and to update complex spatio-temporal baselines that will enable prediction of the usual and&lt;br/&gt;identification, quantification, and assessment of the unusual. A recent NASA workshop on Earth Science&lt;br/&gt;data mining identified anomaly detection as a key characteristic of scientific data mining; yet, there are&lt;br/&gt;relatively few examples of spatio-temporal data mining of biogeophysical data. Our approach is spatio-temporal datamining that is informed by relevant domain expertise. Representation of the spatio-temporal&lt;br/&gt;entities and fields in databases must support sophisticated spatio-temporal queries: a capability that does&lt;br/&gt;not currently exist.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">131937</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">131937</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1054" target="n1056">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics - BDEI - Spatio-temporal Models of Biogeophysical Fields for Ecological Forecasting: A Cross-Disciplinary Incubation Activity</data>
      <data key="e_abstract">EIA-0131937&lt;br/&gt;Henebry, Geoffrey&lt;br/&gt;University of Nebraska - Lincoln&lt;br/&gt;&lt;br/&gt;BDEI: Spatio-temporal models of Biogeophysical Fields for Ecological Forecasting:&lt;br/&gt;A Cross-Disciplinary Incubation Activity &lt;br/&gt;&lt;br/&gt;Summary&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;We are now in an era of intensive earth observation: orbital platforms generate myriad remote sensing&lt;br/&gt;datastreams across a range of spatial, temporal, spectral, and radiometric resolutions. The number and&lt;br/&gt;variety of &quot;eyes in the skies&quot; are scheduled to increase significantly over the next few years. This&lt;br/&gt;veritable data deluge necessitates new ways of thinking about transforming remote sensing data into&lt;br/&gt;information about ecological patterns and processes. These datastreams hold the promise for&lt;br/&gt;environmental decision support. Yet, there is a critical need for theories and tools that will enable efficient&lt;br/&gt;and reliable characterization of spatio-temporal patterns contained in image time series. We think that&lt;br/&gt;such tools must be based on ecological expectations of land surface dynamics, analogous to&lt;br/&gt;climatological expectations. Ecological expectations would summarize across specific regions the typical&lt;br/&gt;temporal development of spatial pattern in biogeophysical fields. We have a robust principal method for&lt;br/&gt;extracting ecological expectations from remote sensing datastreams: projecting image time series into&lt;br/&gt;pattern metric spaces. To make ecological forecasting an operational possibility, we need the capability&lt;br/&gt;to establish and to update complex spatio-temporal baselines that will enable prediction of the usual and&lt;br/&gt;identification, quantification, and assessment of the unusual. A recent NASA workshop on Earth Science&lt;br/&gt;data mining identified anomaly detection as a key characteristic of scientific data mining; yet, there are&lt;br/&gt;relatively few examples of spatio-temporal data mining of biogeophysical data. Our approach is spatio-temporal datamining that is informed by relevant domain expertise. Representation of the spatio-temporal&lt;br/&gt;entities and fields in databases must support sophisticated spatio-temporal queries: a capability that does&lt;br/&gt;not currently exist.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">131937</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">131937</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1055" target="n1056">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics - BDEI - Spatio-temporal Models of Biogeophysical Fields for Ecological Forecasting: A Cross-Disciplinary Incubation Activity</data>
      <data key="e_abstract">EIA-0131937&lt;br/&gt;Henebry, Geoffrey&lt;br/&gt;University of Nebraska - Lincoln&lt;br/&gt;&lt;br/&gt;BDEI: Spatio-temporal models of Biogeophysical Fields for Ecological Forecasting:&lt;br/&gt;A Cross-Disciplinary Incubation Activity &lt;br/&gt;&lt;br/&gt;Summary&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;We are now in an era of intensive earth observation: orbital platforms generate myriad remote sensing&lt;br/&gt;datastreams across a range of spatial, temporal, spectral, and radiometric resolutions. The number and&lt;br/&gt;variety of &quot;eyes in the skies&quot; are scheduled to increase significantly over the next few years. This&lt;br/&gt;veritable data deluge necessitates new ways of thinking about transforming remote sensing data into&lt;br/&gt;information about ecological patterns and processes. These datastreams hold the promise for&lt;br/&gt;environmental decision support. Yet, there is a critical need for theories and tools that will enable efficient&lt;br/&gt;and reliable characterization of spatio-temporal patterns contained in image time series. We think that&lt;br/&gt;such tools must be based on ecological expectations of land surface dynamics, analogous to&lt;br/&gt;climatological expectations. Ecological expectations would summarize across specific regions the typical&lt;br/&gt;temporal development of spatial pattern in biogeophysical fields. We have a robust principal method for&lt;br/&gt;extracting ecological expectations from remote sensing datastreams: projecting image time series into&lt;br/&gt;pattern metric spaces. To make ecological forecasting an operational possibility, we need the capability&lt;br/&gt;to establish and to update complex spatio-temporal baselines that will enable prediction of the usual and&lt;br/&gt;identification, quantification, and assessment of the unusual. A recent NASA workshop on Earth Science&lt;br/&gt;data mining identified anomaly detection as a key characteristic of scientific data mining; yet, there are&lt;br/&gt;relatively few examples of spatio-temporal data mining of biogeophysical data. Our approach is spatio-temporal datamining that is informed by relevant domain expertise. Representation of the spatio-temporal&lt;br/&gt;entities and fields in databases must support sophisticated spatio-temporal queries: a capability that does&lt;br/&gt;not currently exist.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">131937</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">131937</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1058" target="n1059">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP: An Ensemble Approach to Data Assimilation in the Earth Sciences</data>
      <data key="e_abstract">ITR/AP: An Ensemble Approach to Data Assimilation in the Earth Sciences&lt;br/&gt;&lt;br/&gt;New data sources are beginning to have a dramatic impact on our ability to understand the earth as an integrated system. Our prospects for dealing with the environmental issues of the 21st century -- climate change, population pressures on natural resources, and major modifications in global element cycles -- depend largely on this new information. However, our ability to process and interpret environmental data is not keeping pace with the dramatic increase in available information, especially information from airborne and orbital remote sensing platforms. If we are to realize the potential benefits of new sensing technologies we will need to develop intelligent environmental data assimilation procedures that are able to efficiently extract useful information about the earth from a diverse set of data sources. &lt;br/&gt;&lt;br/&gt;Environmental data assimilation can be posed as a problem of estimating a large number of unobservable or highly uncertain variables (e.g. sea surface heights, atmospheric pressures, hydrologic fluxes, etc.) from a large number of related but noisy measurements (e.g. microwave radiances or backscatter detected by a satellite sensor). The estimation procedure relies on mathematical models that relate unknowns to measurements. Environmental estimation problems are challenging because the systems of interest: 1) are spatially distributed and highly variable over a wide range of space and time scales, 2) are difficult to describe with precision, 3) are often nonlinear, even chaotic, and 4) are often characterized by non-unique relationships between unknowns and measurements.&lt;br/&gt;&lt;br/&gt;This project is concerned with very large problems (many measurements and many unknowns) which are not amenable to traditional data assimilation techniques but are of crucial interest to researchers in the earth sciences. An interdisciplinary team will develop a better understanding of the issues of dimensionality reduction and uncertainty propagation that are crucial to large-scale data assimilation. So-called ensemble methods provide a particularly informative way to identify these key features. A new generation of &quot;intelligent&quot; data assimilation methods will be developed that build on the understanding gained from the reduced problem. The applicability of these methods will be investigated on problems of broad interest in the earth sciences, including problems that 1) deal with coupled systems, 2) cut across traditional disciplines, and 3) work with remote sensing data sets.&lt;br/&gt;&lt;br/&gt;This ITR project brings together acknowledged experts on environmental data assimilation. It is a group ITR project, rather than several individual projects, which cuts across earth science disciplines. The research will be coordinated with: 1) a seminar series, 2) joint supervision of Ph.D. students and post-doctoral researchers, 3) a Ph.D. mentoring program, 4) a selection of cross-cutting sample problems, and 5) co-authored publications.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121182</data>
      <data key="e_expirationDate">2008-11-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121182</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1058" target="n1060">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP: An Ensemble Approach to Data Assimilation in the Earth Sciences</data>
      <data key="e_abstract">ITR/AP: An Ensemble Approach to Data Assimilation in the Earth Sciences&lt;br/&gt;&lt;br/&gt;New data sources are beginning to have a dramatic impact on our ability to understand the earth as an integrated system. Our prospects for dealing with the environmental issues of the 21st century -- climate change, population pressures on natural resources, and major modifications in global element cycles -- depend largely on this new information. However, our ability to process and interpret environmental data is not keeping pace with the dramatic increase in available information, especially information from airborne and orbital remote sensing platforms. If we are to realize the potential benefits of new sensing technologies we will need to develop intelligent environmental data assimilation procedures that are able to efficiently extract useful information about the earth from a diverse set of data sources. &lt;br/&gt;&lt;br/&gt;Environmental data assimilation can be posed as a problem of estimating a large number of unobservable or highly uncertain variables (e.g. sea surface heights, atmospheric pressures, hydrologic fluxes, etc.) from a large number of related but noisy measurements (e.g. microwave radiances or backscatter detected by a satellite sensor). The estimation procedure relies on mathematical models that relate unknowns to measurements. Environmental estimation problems are challenging because the systems of interest: 1) are spatially distributed and highly variable over a wide range of space and time scales, 2) are difficult to describe with precision, 3) are often nonlinear, even chaotic, and 4) are often characterized by non-unique relationships between unknowns and measurements.&lt;br/&gt;&lt;br/&gt;This project is concerned with very large problems (many measurements and many unknowns) which are not amenable to traditional data assimilation techniques but are of crucial interest to researchers in the earth sciences. An interdisciplinary team will develop a better understanding of the issues of dimensionality reduction and uncertainty propagation that are crucial to large-scale data assimilation. So-called ensemble methods provide a particularly informative way to identify these key features. A new generation of &quot;intelligent&quot; data assimilation methods will be developed that build on the understanding gained from the reduced problem. The applicability of these methods will be investigated on problems of broad interest in the earth sciences, including problems that 1) deal with coupled systems, 2) cut across traditional disciplines, and 3) work with remote sensing data sets.&lt;br/&gt;&lt;br/&gt;This ITR project brings together acknowledged experts on environmental data assimilation. It is a group ITR project, rather than several individual projects, which cuts across earth science disciplines. The research will be coordinated with: 1) a seminar series, 2) joint supervision of Ph.D. students and post-doctoral researchers, 3) a Ph.D. mentoring program, 4) a selection of cross-cutting sample problems, and 5) co-authored publications.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121182</data>
      <data key="e_expirationDate">2008-11-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121182</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1058" target="n1061">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP: An Ensemble Approach to Data Assimilation in the Earth Sciences</data>
      <data key="e_abstract">ITR/AP: An Ensemble Approach to Data Assimilation in the Earth Sciences&lt;br/&gt;&lt;br/&gt;New data sources are beginning to have a dramatic impact on our ability to understand the earth as an integrated system. Our prospects for dealing with the environmental issues of the 21st century -- climate change, population pressures on natural resources, and major modifications in global element cycles -- depend largely on this new information. However, our ability to process and interpret environmental data is not keeping pace with the dramatic increase in available information, especially information from airborne and orbital remote sensing platforms. If we are to realize the potential benefits of new sensing technologies we will need to develop intelligent environmental data assimilation procedures that are able to efficiently extract useful information about the earth from a diverse set of data sources. &lt;br/&gt;&lt;br/&gt;Environmental data assimilation can be posed as a problem of estimating a large number of unobservable or highly uncertain variables (e.g. sea surface heights, atmospheric pressures, hydrologic fluxes, etc.) from a large number of related but noisy measurements (e.g. microwave radiances or backscatter detected by a satellite sensor). The estimation procedure relies on mathematical models that relate unknowns to measurements. Environmental estimation problems are challenging because the systems of interest: 1) are spatially distributed and highly variable over a wide range of space and time scales, 2) are difficult to describe with precision, 3) are often nonlinear, even chaotic, and 4) are often characterized by non-unique relationships between unknowns and measurements.&lt;br/&gt;&lt;br/&gt;This project is concerned with very large problems (many measurements and many unknowns) which are not amenable to traditional data assimilation techniques but are of crucial interest to researchers in the earth sciences. An interdisciplinary team will develop a better understanding of the issues of dimensionality reduction and uncertainty propagation that are crucial to large-scale data assimilation. So-called ensemble methods provide a particularly informative way to identify these key features. A new generation of &quot;intelligent&quot; data assimilation methods will be developed that build on the understanding gained from the reduced problem. The applicability of these methods will be investigated on problems of broad interest in the earth sciences, including problems that 1) deal with coupled systems, 2) cut across traditional disciplines, and 3) work with remote sensing data sets.&lt;br/&gt;&lt;br/&gt;This ITR project brings together acknowledged experts on environmental data assimilation. It is a group ITR project, rather than several individual projects, which cuts across earth science disciplines. The research will be coordinated with: 1) a seminar series, 2) joint supervision of Ph.D. students and post-doctoral researchers, 3) a Ph.D. mentoring program, 4) a selection of cross-cutting sample problems, and 5) co-authored publications.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121182</data>
      <data key="e_expirationDate">2008-11-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121182</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1058" target="n1062">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP: An Ensemble Approach to Data Assimilation in the Earth Sciences</data>
      <data key="e_abstract">ITR/AP: An Ensemble Approach to Data Assimilation in the Earth Sciences&lt;br/&gt;&lt;br/&gt;New data sources are beginning to have a dramatic impact on our ability to understand the earth as an integrated system. Our prospects for dealing with the environmental issues of the 21st century -- climate change, population pressures on natural resources, and major modifications in global element cycles -- depend largely on this new information. However, our ability to process and interpret environmental data is not keeping pace with the dramatic increase in available information, especially information from airborne and orbital remote sensing platforms. If we are to realize the potential benefits of new sensing technologies we will need to develop intelligent environmental data assimilation procedures that are able to efficiently extract useful information about the earth from a diverse set of data sources. &lt;br/&gt;&lt;br/&gt;Environmental data assimilation can be posed as a problem of estimating a large number of unobservable or highly uncertain variables (e.g. sea surface heights, atmospheric pressures, hydrologic fluxes, etc.) from a large number of related but noisy measurements (e.g. microwave radiances or backscatter detected by a satellite sensor). The estimation procedure relies on mathematical models that relate unknowns to measurements. Environmental estimation problems are challenging because the systems of interest: 1) are spatially distributed and highly variable over a wide range of space and time scales, 2) are difficult to describe with precision, 3) are often nonlinear, even chaotic, and 4) are often characterized by non-unique relationships between unknowns and measurements.&lt;br/&gt;&lt;br/&gt;This project is concerned with very large problems (many measurements and many unknowns) which are not amenable to traditional data assimilation techniques but are of crucial interest to researchers in the earth sciences. An interdisciplinary team will develop a better understanding of the issues of dimensionality reduction and uncertainty propagation that are crucial to large-scale data assimilation. So-called ensemble methods provide a particularly informative way to identify these key features. A new generation of &quot;intelligent&quot; data assimilation methods will be developed that build on the understanding gained from the reduced problem. The applicability of these methods will be investigated on problems of broad interest in the earth sciences, including problems that 1) deal with coupled systems, 2) cut across traditional disciplines, and 3) work with remote sensing data sets.&lt;br/&gt;&lt;br/&gt;This ITR project brings together acknowledged experts on environmental data assimilation. It is a group ITR project, rather than several individual projects, which cuts across earth science disciplines. The research will be coordinated with: 1) a seminar series, 2) joint supervision of Ph.D. students and post-doctoral researchers, 3) a Ph.D. mentoring program, 4) a selection of cross-cutting sample problems, and 5) co-authored publications.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121182</data>
      <data key="e_expirationDate">2008-11-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121182</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1059" target="n1060">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP: An Ensemble Approach to Data Assimilation in the Earth Sciences</data>
      <data key="e_abstract">ITR/AP: An Ensemble Approach to Data Assimilation in the Earth Sciences&lt;br/&gt;&lt;br/&gt;New data sources are beginning to have a dramatic impact on our ability to understand the earth as an integrated system. Our prospects for dealing with the environmental issues of the 21st century -- climate change, population pressures on natural resources, and major modifications in global element cycles -- depend largely on this new information. However, our ability to process and interpret environmental data is not keeping pace with the dramatic increase in available information, especially information from airborne and orbital remote sensing platforms. If we are to realize the potential benefits of new sensing technologies we will need to develop intelligent environmental data assimilation procedures that are able to efficiently extract useful information about the earth from a diverse set of data sources. &lt;br/&gt;&lt;br/&gt;Environmental data assimilation can be posed as a problem of estimating a large number of unobservable or highly uncertain variables (e.g. sea surface heights, atmospheric pressures, hydrologic fluxes, etc.) from a large number of related but noisy measurements (e.g. microwave radiances or backscatter detected by a satellite sensor). The estimation procedure relies on mathematical models that relate unknowns to measurements. Environmental estimation problems are challenging because the systems of interest: 1) are spatially distributed and highly variable over a wide range of space and time scales, 2) are difficult to describe with precision, 3) are often nonlinear, even chaotic, and 4) are often characterized by non-unique relationships between unknowns and measurements.&lt;br/&gt;&lt;br/&gt;This project is concerned with very large problems (many measurements and many unknowns) which are not amenable to traditional data assimilation techniques but are of crucial interest to researchers in the earth sciences. An interdisciplinary team will develop a better understanding of the issues of dimensionality reduction and uncertainty propagation that are crucial to large-scale data assimilation. So-called ensemble methods provide a particularly informative way to identify these key features. A new generation of &quot;intelligent&quot; data assimilation methods will be developed that build on the understanding gained from the reduced problem. The applicability of these methods will be investigated on problems of broad interest in the earth sciences, including problems that 1) deal with coupled systems, 2) cut across traditional disciplines, and 3) work with remote sensing data sets.&lt;br/&gt;&lt;br/&gt;This ITR project brings together acknowledged experts on environmental data assimilation. It is a group ITR project, rather than several individual projects, which cuts across earth science disciplines. The research will be coordinated with: 1) a seminar series, 2) joint supervision of Ph.D. students and post-doctoral researchers, 3) a Ph.D. mentoring program, 4) a selection of cross-cutting sample problems, and 5) co-authored publications.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121182</data>
      <data key="e_expirationDate">2008-11-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121182</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1059" target="n1061">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP: An Ensemble Approach to Data Assimilation in the Earth Sciences</data>
      <data key="e_abstract">ITR/AP: An Ensemble Approach to Data Assimilation in the Earth Sciences&lt;br/&gt;&lt;br/&gt;New data sources are beginning to have a dramatic impact on our ability to understand the earth as an integrated system. Our prospects for dealing with the environmental issues of the 21st century -- climate change, population pressures on natural resources, and major modifications in global element cycles -- depend largely on this new information. However, our ability to process and interpret environmental data is not keeping pace with the dramatic increase in available information, especially information from airborne and orbital remote sensing platforms. If we are to realize the potential benefits of new sensing technologies we will need to develop intelligent environmental data assimilation procedures that are able to efficiently extract useful information about the earth from a diverse set of data sources. &lt;br/&gt;&lt;br/&gt;Environmental data assimilation can be posed as a problem of estimating a large number of unobservable or highly uncertain variables (e.g. sea surface heights, atmospheric pressures, hydrologic fluxes, etc.) from a large number of related but noisy measurements (e.g. microwave radiances or backscatter detected by a satellite sensor). The estimation procedure relies on mathematical models that relate unknowns to measurements. Environmental estimation problems are challenging because the systems of interest: 1) are spatially distributed and highly variable over a wide range of space and time scales, 2) are difficult to describe with precision, 3) are often nonlinear, even chaotic, and 4) are often characterized by non-unique relationships between unknowns and measurements.&lt;br/&gt;&lt;br/&gt;This project is concerned with very large problems (many measurements and many unknowns) which are not amenable to traditional data assimilation techniques but are of crucial interest to researchers in the earth sciences. An interdisciplinary team will develop a better understanding of the issues of dimensionality reduction and uncertainty propagation that are crucial to large-scale data assimilation. So-called ensemble methods provide a particularly informative way to identify these key features. A new generation of &quot;intelligent&quot; data assimilation methods will be developed that build on the understanding gained from the reduced problem. The applicability of these methods will be investigated on problems of broad interest in the earth sciences, including problems that 1) deal with coupled systems, 2) cut across traditional disciplines, and 3) work with remote sensing data sets.&lt;br/&gt;&lt;br/&gt;This ITR project brings together acknowledged experts on environmental data assimilation. It is a group ITR project, rather than several individual projects, which cuts across earth science disciplines. The research will be coordinated with: 1) a seminar series, 2) joint supervision of Ph.D. students and post-doctoral researchers, 3) a Ph.D. mentoring program, 4) a selection of cross-cutting sample problems, and 5) co-authored publications.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121182</data>
      <data key="e_expirationDate">2008-11-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121182</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1059" target="n1062">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP: An Ensemble Approach to Data Assimilation in the Earth Sciences</data>
      <data key="e_abstract">ITR/AP: An Ensemble Approach to Data Assimilation in the Earth Sciences&lt;br/&gt;&lt;br/&gt;New data sources are beginning to have a dramatic impact on our ability to understand the earth as an integrated system. Our prospects for dealing with the environmental issues of the 21st century -- climate change, population pressures on natural resources, and major modifications in global element cycles -- depend largely on this new information. However, our ability to process and interpret environmental data is not keeping pace with the dramatic increase in available information, especially information from airborne and orbital remote sensing platforms. If we are to realize the potential benefits of new sensing technologies we will need to develop intelligent environmental data assimilation procedures that are able to efficiently extract useful information about the earth from a diverse set of data sources. &lt;br/&gt;&lt;br/&gt;Environmental data assimilation can be posed as a problem of estimating a large number of unobservable or highly uncertain variables (e.g. sea surface heights, atmospheric pressures, hydrologic fluxes, etc.) from a large number of related but noisy measurements (e.g. microwave radiances or backscatter detected by a satellite sensor). The estimation procedure relies on mathematical models that relate unknowns to measurements. Environmental estimation problems are challenging because the systems of interest: 1) are spatially distributed and highly variable over a wide range of space and time scales, 2) are difficult to describe with precision, 3) are often nonlinear, even chaotic, and 4) are often characterized by non-unique relationships between unknowns and measurements.&lt;br/&gt;&lt;br/&gt;This project is concerned with very large problems (many measurements and many unknowns) which are not amenable to traditional data assimilation techniques but are of crucial interest to researchers in the earth sciences. An interdisciplinary team will develop a better understanding of the issues of dimensionality reduction and uncertainty propagation that are crucial to large-scale data assimilation. So-called ensemble methods provide a particularly informative way to identify these key features. A new generation of &quot;intelligent&quot; data assimilation methods will be developed that build on the understanding gained from the reduced problem. The applicability of these methods will be investigated on problems of broad interest in the earth sciences, including problems that 1) deal with coupled systems, 2) cut across traditional disciplines, and 3) work with remote sensing data sets.&lt;br/&gt;&lt;br/&gt;This ITR project brings together acknowledged experts on environmental data assimilation. It is a group ITR project, rather than several individual projects, which cuts across earth science disciplines. The research will be coordinated with: 1) a seminar series, 2) joint supervision of Ph.D. students and post-doctoral researchers, 3) a Ph.D. mentoring program, 4) a selection of cross-cutting sample problems, and 5) co-authored publications.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121182</data>
      <data key="e_expirationDate">2008-11-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121182</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1060" target="n1061">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP: An Ensemble Approach to Data Assimilation in the Earth Sciences</data>
      <data key="e_abstract">ITR/AP: An Ensemble Approach to Data Assimilation in the Earth Sciences&lt;br/&gt;&lt;br/&gt;New data sources are beginning to have a dramatic impact on our ability to understand the earth as an integrated system. Our prospects for dealing with the environmental issues of the 21st century -- climate change, population pressures on natural resources, and major modifications in global element cycles -- depend largely on this new information. However, our ability to process and interpret environmental data is not keeping pace with the dramatic increase in available information, especially information from airborne and orbital remote sensing platforms. If we are to realize the potential benefits of new sensing technologies we will need to develop intelligent environmental data assimilation procedures that are able to efficiently extract useful information about the earth from a diverse set of data sources. &lt;br/&gt;&lt;br/&gt;Environmental data assimilation can be posed as a problem of estimating a large number of unobservable or highly uncertain variables (e.g. sea surface heights, atmospheric pressures, hydrologic fluxes, etc.) from a large number of related but noisy measurements (e.g. microwave radiances or backscatter detected by a satellite sensor). The estimation procedure relies on mathematical models that relate unknowns to measurements. Environmental estimation problems are challenging because the systems of interest: 1) are spatially distributed and highly variable over a wide range of space and time scales, 2) are difficult to describe with precision, 3) are often nonlinear, even chaotic, and 4) are often characterized by non-unique relationships between unknowns and measurements.&lt;br/&gt;&lt;br/&gt;This project is concerned with very large problems (many measurements and many unknowns) which are not amenable to traditional data assimilation techniques but are of crucial interest to researchers in the earth sciences. An interdisciplinary team will develop a better understanding of the issues of dimensionality reduction and uncertainty propagation that are crucial to large-scale data assimilation. So-called ensemble methods provide a particularly informative way to identify these key features. A new generation of &quot;intelligent&quot; data assimilation methods will be developed that build on the understanding gained from the reduced problem. The applicability of these methods will be investigated on problems of broad interest in the earth sciences, including problems that 1) deal with coupled systems, 2) cut across traditional disciplines, and 3) work with remote sensing data sets.&lt;br/&gt;&lt;br/&gt;This ITR project brings together acknowledged experts on environmental data assimilation. It is a group ITR project, rather than several individual projects, which cuts across earth science disciplines. The research will be coordinated with: 1) a seminar series, 2) joint supervision of Ph.D. students and post-doctoral researchers, 3) a Ph.D. mentoring program, 4) a selection of cross-cutting sample problems, and 5) co-authored publications.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121182</data>
      <data key="e_expirationDate">2008-11-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121182</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1060" target="n1062">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP: An Ensemble Approach to Data Assimilation in the Earth Sciences</data>
      <data key="e_abstract">ITR/AP: An Ensemble Approach to Data Assimilation in the Earth Sciences&lt;br/&gt;&lt;br/&gt;New data sources are beginning to have a dramatic impact on our ability to understand the earth as an integrated system. Our prospects for dealing with the environmental issues of the 21st century -- climate change, population pressures on natural resources, and major modifications in global element cycles -- depend largely on this new information. However, our ability to process and interpret environmental data is not keeping pace with the dramatic increase in available information, especially information from airborne and orbital remote sensing platforms. If we are to realize the potential benefits of new sensing technologies we will need to develop intelligent environmental data assimilation procedures that are able to efficiently extract useful information about the earth from a diverse set of data sources. &lt;br/&gt;&lt;br/&gt;Environmental data assimilation can be posed as a problem of estimating a large number of unobservable or highly uncertain variables (e.g. sea surface heights, atmospheric pressures, hydrologic fluxes, etc.) from a large number of related but noisy measurements (e.g. microwave radiances or backscatter detected by a satellite sensor). The estimation procedure relies on mathematical models that relate unknowns to measurements. Environmental estimation problems are challenging because the systems of interest: 1) are spatially distributed and highly variable over a wide range of space and time scales, 2) are difficult to describe with precision, 3) are often nonlinear, even chaotic, and 4) are often characterized by non-unique relationships between unknowns and measurements.&lt;br/&gt;&lt;br/&gt;This project is concerned with very large problems (many measurements and many unknowns) which are not amenable to traditional data assimilation techniques but are of crucial interest to researchers in the earth sciences. An interdisciplinary team will develop a better understanding of the issues of dimensionality reduction and uncertainty propagation that are crucial to large-scale data assimilation. So-called ensemble methods provide a particularly informative way to identify these key features. A new generation of &quot;intelligent&quot; data assimilation methods will be developed that build on the understanding gained from the reduced problem. The applicability of these methods will be investigated on problems of broad interest in the earth sciences, including problems that 1) deal with coupled systems, 2) cut across traditional disciplines, and 3) work with remote sensing data sets.&lt;br/&gt;&lt;br/&gt;This ITR project brings together acknowledged experts on environmental data assimilation. It is a group ITR project, rather than several individual projects, which cuts across earth science disciplines. The research will be coordinated with: 1) a seminar series, 2) joint supervision of Ph.D. students and post-doctoral researchers, 3) a Ph.D. mentoring program, 4) a selection of cross-cutting sample problems, and 5) co-authored publications.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121182</data>
      <data key="e_expirationDate">2008-11-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121182</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1061" target="n1062">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP: An Ensemble Approach to Data Assimilation in the Earth Sciences</data>
      <data key="e_abstract">ITR/AP: An Ensemble Approach to Data Assimilation in the Earth Sciences&lt;br/&gt;&lt;br/&gt;New data sources are beginning to have a dramatic impact on our ability to understand the earth as an integrated system. Our prospects for dealing with the environmental issues of the 21st century -- climate change, population pressures on natural resources, and major modifications in global element cycles -- depend largely on this new information. However, our ability to process and interpret environmental data is not keeping pace with the dramatic increase in available information, especially information from airborne and orbital remote sensing platforms. If we are to realize the potential benefits of new sensing technologies we will need to develop intelligent environmental data assimilation procedures that are able to efficiently extract useful information about the earth from a diverse set of data sources. &lt;br/&gt;&lt;br/&gt;Environmental data assimilation can be posed as a problem of estimating a large number of unobservable or highly uncertain variables (e.g. sea surface heights, atmospheric pressures, hydrologic fluxes, etc.) from a large number of related but noisy measurements (e.g. microwave radiances or backscatter detected by a satellite sensor). The estimation procedure relies on mathematical models that relate unknowns to measurements. Environmental estimation problems are challenging because the systems of interest: 1) are spatially distributed and highly variable over a wide range of space and time scales, 2) are difficult to describe with precision, 3) are often nonlinear, even chaotic, and 4) are often characterized by non-unique relationships between unknowns and measurements.&lt;br/&gt;&lt;br/&gt;This project is concerned with very large problems (many measurements and many unknowns) which are not amenable to traditional data assimilation techniques but are of crucial interest to researchers in the earth sciences. An interdisciplinary team will develop a better understanding of the issues of dimensionality reduction and uncertainty propagation that are crucial to large-scale data assimilation. So-called ensemble methods provide a particularly informative way to identify these key features. A new generation of &quot;intelligent&quot; data assimilation methods will be developed that build on the understanding gained from the reduced problem. The applicability of these methods will be investigated on problems of broad interest in the earth sciences, including problems that 1) deal with coupled systems, 2) cut across traditional disciplines, and 3) work with remote sensing data sets.&lt;br/&gt;&lt;br/&gt;This ITR project brings together acknowledged experts on environmental data assimilation. It is a group ITR project, rather than several individual projects, which cuts across earth science disciplines. The research will be coordinated with: 1) a seminar series, 2) joint supervision of Ph.D. students and post-doctoral researchers, 3) a Ph.D. mentoring program, 4) a selection of cross-cutting sample problems, and 5) co-authored publications.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121182</data>
      <data key="e_expirationDate">2008-11-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121182</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1063" target="n1064">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Improving Symbolic Analysis of Restructuring Compilers</data>
      <data key="e_abstract">The proposed research investigates new methods for symbolic analysis to&lt;br/&gt;improve various restructuring compiler optimizations. A new algebra on&lt;br/&gt;functions is investigated to manipulate, simplify, and derive normal forms of&lt;br/&gt;scalar functions and (generalized) induction variables in multi-dimensional&lt;br/&gt;loops. The derivation of normal forms for intermediate program constructs&lt;br/&gt;enables reasoning about the semantics of a program under analysis. This is&lt;br/&gt;extremely useful to improve various compiler optimizations to effectively deal&lt;br/&gt;with symbolic expressions in real-world applications. More specifically, the&lt;br/&gt;proposed research aims to improve symbolic analysis methods such as&lt;br/&gt;generalized induction variable recognition, linear and non-linear data&lt;br/&gt;dependence analysis, value range analysis, global value propagation, and&lt;br/&gt;counting the number of solutions to systems of constraints. The effectiveness&lt;br/&gt;of parallelizing compilers depends heavily on the accuracy of these methods.&lt;br/&gt;The research will result in the ability of compilers to more effectively&lt;br/&gt;handle symbolic expressions and constraints. Current methods are not always&lt;br/&gt;effective, resulting in considerable performance losses caused by worst-case&lt;br/&gt;assumptions or when program analysis has to be performed at execution time.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">105422</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105422</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n614" target="n1066">
      <data key="e_effectiveDate">None</data>
      <data key="e_title">None</data>
      <data key="e_abstract">None</data>
      <data key="e_pgm">None</data>
      <data key="e_expirationDate">None</data>
      <data key="e_div">None</data>
      <data key="e_dir">None</data>
    </edge>
    <edge source="n614" target="n1066">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">A Computational Theory of Discovery</data>
      <data key="e_abstract">he models used in inductive inference have their&lt;br/&gt;roots in the models used by the philosophers of&lt;br/&gt;science who were discussing the scientific method.&lt;br/&gt;The goal there, and in prior work in learning theory,&lt;br/&gt;was to come up with an explanation of the phenomenon&lt;br/&gt;under consideration.&lt;br/&gt;However, scientists rarely work directly for the grand goal&lt;br/&gt;of a complete explanation.&lt;br/&gt;The more modest goal of finding features and&lt;br/&gt;facts about the observed data is pursued.&lt;br/&gt;A variety of types of algorithms that could be construed as&lt;br/&gt;discovering their final result will be investigated.&lt;br/&gt;We propose to consider computations that discover rather than compute&lt;br/&gt;their intended result.&lt;br/&gt;A logic of discovery will be developed and investigated.&lt;br/&gt;This study is particularly relevant to contemporary&lt;br/&gt;science as automated data generation techniques&lt;br/&gt;produce sufficient volumes of data to&lt;br/&gt;overwhelm the analysis abilities of humans.&lt;br/&gt;The goal of our work is to illuminate&lt;br/&gt;precisely what can and cannot be accomplished&lt;br/&gt;by automatic data analysis algorithms.&lt;br/&gt;Such algorithms are used in data mining and&lt;br/&gt;text analysis for world wide web search engines.</data>
      <data key="e_pgm">2860</data>
      <data key="e_label">105413</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105413</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1066" target="n1066">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">A Computational Theory of Discovery</data>
      <data key="e_abstract">he models used in inductive inference have their&lt;br/&gt;roots in the models used by the philosophers of&lt;br/&gt;science who were discussing the scientific method.&lt;br/&gt;The goal there, and in prior work in learning theory,&lt;br/&gt;was to come up with an explanation of the phenomenon&lt;br/&gt;under consideration.&lt;br/&gt;However, scientists rarely work directly for the grand goal&lt;br/&gt;of a complete explanation.&lt;br/&gt;The more modest goal of finding features and&lt;br/&gt;facts about the observed data is pursued.&lt;br/&gt;A variety of types of algorithms that could be construed as&lt;br/&gt;discovering their final result will be investigated.&lt;br/&gt;We propose to consider computations that discover rather than compute&lt;br/&gt;their intended result.&lt;br/&gt;A logic of discovery will be developed and investigated.&lt;br/&gt;This study is particularly relevant to contemporary&lt;br/&gt;science as automated data generation techniques&lt;br/&gt;produce sufficient volumes of data to&lt;br/&gt;overwhelm the analysis abilities of humans.&lt;br/&gt;The goal of our work is to illuminate&lt;br/&gt;precisely what can and cannot be accomplished&lt;br/&gt;by automatic data analysis algorithms.&lt;br/&gt;Such algorithms are used in data mining and&lt;br/&gt;text analysis for world wide web search engines.</data>
      <data key="e_pgm">2860</data>
      <data key="e_label">105413</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105413</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1070" target="n1071">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Instrumentation for a Virtual Environment Laboratory to Study Human Behavior</data>
      <data key="e_abstract">EIA-0130864 &lt;br/&gt;James Cremer&lt;br/&gt;University of Iowa&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Instrumentation for a Virtual Environment Laboratory to Study Human Behavior&lt;br/&gt;&lt;br/&gt;The University of Iowa will purchase equipment supporting multidisciplinary research on the use of virtual environments as a medium for the study of human behavior. The primary components of the equipment include a PC-based, real-time image generation system for use in the existing immersive display environment, and hardware for instrumented bicycle and wheelchair interfaces to the virtual environment. The equipment will replace outdated and unstable equipment currently in use, and greatly improve the computational foundation of the virtual environments laboratory.&lt;br/&gt;&lt;br/&gt;The supported research has two primary thrusts: (1) a computational component directed at advancing scenario modeling techniques to meet the special needs of experiments for replicable experiences that adapt to subject behavior and (2) an experimental component investigating children&apos;s bicycle riding behavior in simulated traffic. Experiments conducted in virtual environments will be compared with experiments conducted in real environments to validate the use of simulators as laboratories for the study of human behavior. The work integrates research on high-fidelity simulation, control of complex behaviors of simulated agents, human factors, and developmental psychology. This research will advance virtual environment technology, experimental methods, and simulator validation, and increase the understanding of a leading cause of childhood injuries.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130864</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130864</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1070" target="n1072">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Instrumentation for a Virtual Environment Laboratory to Study Human Behavior</data>
      <data key="e_abstract">EIA-0130864 &lt;br/&gt;James Cremer&lt;br/&gt;University of Iowa&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Instrumentation for a Virtual Environment Laboratory to Study Human Behavior&lt;br/&gt;&lt;br/&gt;The University of Iowa will purchase equipment supporting multidisciplinary research on the use of virtual environments as a medium for the study of human behavior. The primary components of the equipment include a PC-based, real-time image generation system for use in the existing immersive display environment, and hardware for instrumented bicycle and wheelchair interfaces to the virtual environment. The equipment will replace outdated and unstable equipment currently in use, and greatly improve the computational foundation of the virtual environments laboratory.&lt;br/&gt;&lt;br/&gt;The supported research has two primary thrusts: (1) a computational component directed at advancing scenario modeling techniques to meet the special needs of experiments for replicable experiences that adapt to subject behavior and (2) an experimental component investigating children&apos;s bicycle riding behavior in simulated traffic. Experiments conducted in virtual environments will be compared with experiments conducted in real environments to validate the use of simulators as laboratories for the study of human behavior. The work integrates research on high-fidelity simulation, control of complex behaviors of simulated agents, human factors, and developmental psychology. This research will advance virtual environment technology, experimental methods, and simulator validation, and increase the understanding of a leading cause of childhood injuries.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130864</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130864</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1071" target="n1072">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Instrumentation for a Virtual Environment Laboratory to Study Human Behavior</data>
      <data key="e_abstract">EIA-0130864 &lt;br/&gt;James Cremer&lt;br/&gt;University of Iowa&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Instrumentation for a Virtual Environment Laboratory to Study Human Behavior&lt;br/&gt;&lt;br/&gt;The University of Iowa will purchase equipment supporting multidisciplinary research on the use of virtual environments as a medium for the study of human behavior. The primary components of the equipment include a PC-based, real-time image generation system for use in the existing immersive display environment, and hardware for instrumented bicycle and wheelchair interfaces to the virtual environment. The equipment will replace outdated and unstable equipment currently in use, and greatly improve the computational foundation of the virtual environments laboratory.&lt;br/&gt;&lt;br/&gt;The supported research has two primary thrusts: (1) a computational component directed at advancing scenario modeling techniques to meet the special needs of experiments for replicable experiences that adapt to subject behavior and (2) an experimental component investigating children&apos;s bicycle riding behavior in simulated traffic. Experiments conducted in virtual environments will be compared with experiments conducted in real environments to validate the use of simulators as laboratories for the study of human behavior. The work integrates research on high-fidelity simulation, control of complex behaviors of simulated agents, human factors, and developmental psychology. This research will advance virtual environment technology, experimental methods, and simulator validation, and increase the understanding of a leading cause of childhood injuries.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130864</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130864</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1073" target="n1074">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics - BDEI - Reconfigurable Wireless Sensor Networks for Dense Spatio-Temporal Environmental Monitoring</data>
      <data key="e_abstract">EIA-0131691&lt;br/&gt;Flikkema, Paul &lt;br/&gt;Northern Arizona University&lt;br/&gt;&lt;br/&gt;BDEI: Reconfigurable Wireless Sensor Networks for Dense Spatio-Temporal Environmental Monitoring&lt;br/&gt;&lt;br/&gt;Summary&lt;br/&gt;&lt;br/&gt;Improving understanding of ecosystem processes requires the acquisition of multi-component&lt;br/&gt;environmental time series at high spatial resolution. Current environmental sensing systems typically rely on stand alone data loggers or wired sensor arrays. Research and resource management that depend on such&lt;br/&gt;systems are currently limited by initial cost and/or the effort required to acquire the data. The nexus of this project is to integrate cutting-edge, low-cost circuit and system technology into a wireless environmental sensing network based on an evolvable architecture that will meet an immediate and critical need: to dramatically improve coverage and spatial density while greatly reducing the total cost. This first-generation network will also serve as an experimental testbed for fundamental research in wireless sensor networking that targets the unique characteristics of environmental monitoring applications. This research includes distributed, adaptive source coding of spatio-temporally correlated vector processes, multi-hop protocols with inter-layer interaction, and coded macrodiversity for energy-constrained multi-hop networks.&lt;br/&gt;&lt;br/&gt;Since this project brings together wireless networking and ecosystems and microclimate researchers, it also provides an opportunity to initiate the development of a new interdisciplinary program of study: Ecosystem Sciences and Informatics. This program will educate a new generation of students who can develop and use technology for ecosystem monitoring and modeling.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">131691</data>
      <data key="e_expirationDate">2003-05-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">131691</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1073" target="n1075">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics - BDEI - Reconfigurable Wireless Sensor Networks for Dense Spatio-Temporal Environmental Monitoring</data>
      <data key="e_abstract">EIA-0131691&lt;br/&gt;Flikkema, Paul &lt;br/&gt;Northern Arizona University&lt;br/&gt;&lt;br/&gt;BDEI: Reconfigurable Wireless Sensor Networks for Dense Spatio-Temporal Environmental Monitoring&lt;br/&gt;&lt;br/&gt;Summary&lt;br/&gt;&lt;br/&gt;Improving understanding of ecosystem processes requires the acquisition of multi-component&lt;br/&gt;environmental time series at high spatial resolution. Current environmental sensing systems typically rely on stand alone data loggers or wired sensor arrays. Research and resource management that depend on such&lt;br/&gt;systems are currently limited by initial cost and/or the effort required to acquire the data. The nexus of this project is to integrate cutting-edge, low-cost circuit and system technology into a wireless environmental sensing network based on an evolvable architecture that will meet an immediate and critical need: to dramatically improve coverage and spatial density while greatly reducing the total cost. This first-generation network will also serve as an experimental testbed for fundamental research in wireless sensor networking that targets the unique characteristics of environmental monitoring applications. This research includes distributed, adaptive source coding of spatio-temporally correlated vector processes, multi-hop protocols with inter-layer interaction, and coded macrodiversity for energy-constrained multi-hop networks.&lt;br/&gt;&lt;br/&gt;Since this project brings together wireless networking and ecosystems and microclimate researchers, it also provides an opportunity to initiate the development of a new interdisciplinary program of study: Ecosystem Sciences and Informatics. This program will educate a new generation of students who can develop and use technology for ecosystem monitoring and modeling.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">131691</data>
      <data key="e_expirationDate">2003-05-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">131691</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1074" target="n1075">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics - BDEI - Reconfigurable Wireless Sensor Networks for Dense Spatio-Temporal Environmental Monitoring</data>
      <data key="e_abstract">EIA-0131691&lt;br/&gt;Flikkema, Paul &lt;br/&gt;Northern Arizona University&lt;br/&gt;&lt;br/&gt;BDEI: Reconfigurable Wireless Sensor Networks for Dense Spatio-Temporal Environmental Monitoring&lt;br/&gt;&lt;br/&gt;Summary&lt;br/&gt;&lt;br/&gt;Improving understanding of ecosystem processes requires the acquisition of multi-component&lt;br/&gt;environmental time series at high spatial resolution. Current environmental sensing systems typically rely on stand alone data loggers or wired sensor arrays. Research and resource management that depend on such&lt;br/&gt;systems are currently limited by initial cost and/or the effort required to acquire the data. The nexus of this project is to integrate cutting-edge, low-cost circuit and system technology into a wireless environmental sensing network based on an evolvable architecture that will meet an immediate and critical need: to dramatically improve coverage and spatial density while greatly reducing the total cost. This first-generation network will also serve as an experimental testbed for fundamental research in wireless sensor networking that targets the unique characteristics of environmental monitoring applications. This research includes distributed, adaptive source coding of spatio-temporally correlated vector processes, multi-hop protocols with inter-layer interaction, and coded macrodiversity for energy-constrained multi-hop networks.&lt;br/&gt;&lt;br/&gt;Since this project brings together wireless networking and ecosystems and microclimate researchers, it also provides an opportunity to initiate the development of a new interdisciplinary program of study: Ecosystem Sciences and Informatics. This program will educate a new generation of students who can develop and use technology for ecosystem monitoring and modeling.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">131691</data>
      <data key="e_expirationDate">2003-05-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">131691</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n830" target="n1078">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: A Shared Data Cluster For Real Time Interaction With Massive Datasets</data>
      <data key="e_abstract">EIA-0130869 &lt;br/&gt;Benjamin Watson&lt;br/&gt;Northwestern University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: A Shared Data Cluster for Real Time Interaction With Massive Datasets&lt;br/&gt;&lt;br/&gt;The research at Northwestern is increasingly concerned with the issues involved in providing interactive and affordable access to massive (terabyte scale) datasets. In particular, they focus on visualization of these datasets using perceptually based computer graphics techniques and display the visualizations using a combination PC CAVE and Active Mural. They also study the issues involved with ubiquitous access to high-resolution spatial datasets from mobile devices, and leverage a powerful wireless network. They focus on prediction-based adaptation and scheduling for distributed interactive applications and are building a compute cluster to support this work.&lt;br/&gt;&lt;br/&gt;What is missing in the existing shared research infrastructure is a facility to store and serve massive datasets interactively, with low latency and high bandwidth. The researchers will build such a shared data cluster. The data cluster will be very high performance and capacity PC RAIDs interconnected with a multi-GB/sec SAN. The SAN will connect to their compute cluster, CAVE/Mural, mobile devices and wired client machines. The combined data and compute clusters will be sufficient to pump 200 MB/s from disk to the CAVE/Mural screens. The combination of the data cluster, compute cluster, CAVE/Mural, local network infrastructure, and desktop and mobile clients will let them study interactivity in the large.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130869</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130869</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n830" target="n884">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: A Shared Data Cluster For Real Time Interaction With Massive Datasets</data>
      <data key="e_abstract">EIA-0130869 &lt;br/&gt;Benjamin Watson&lt;br/&gt;Northwestern University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: A Shared Data Cluster for Real Time Interaction With Massive Datasets&lt;br/&gt;&lt;br/&gt;The research at Northwestern is increasingly concerned with the issues involved in providing interactive and affordable access to massive (terabyte scale) datasets. In particular, they focus on visualization of these datasets using perceptually based computer graphics techniques and display the visualizations using a combination PC CAVE and Active Mural. They also study the issues involved with ubiquitous access to high-resolution spatial datasets from mobile devices, and leverage a powerful wireless network. They focus on prediction-based adaptation and scheduling for distributed interactive applications and are building a compute cluster to support this work.&lt;br/&gt;&lt;br/&gt;What is missing in the existing shared research infrastructure is a facility to store and serve massive datasets interactively, with low latency and high bandwidth. The researchers will build such a shared data cluster. The data cluster will be very high performance and capacity PC RAIDs interconnected with a multi-GB/sec SAN. The SAN will connect to their compute cluster, CAVE/Mural, mobile devices and wired client machines. The combined data and compute clusters will be sufficient to pump 200 MB/s from disk to the CAVE/Mural screens. The combination of the data cluster, compute cluster, CAVE/Mural, local network infrastructure, and desktop and mobile clients will let them study interactivity in the large.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130869</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130869</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n884" target="n1078">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: A Shared Data Cluster For Real Time Interaction With Massive Datasets</data>
      <data key="e_abstract">EIA-0130869 &lt;br/&gt;Benjamin Watson&lt;br/&gt;Northwestern University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: A Shared Data Cluster for Real Time Interaction With Massive Datasets&lt;br/&gt;&lt;br/&gt;The research at Northwestern is increasingly concerned with the issues involved in providing interactive and affordable access to massive (terabyte scale) datasets. In particular, they focus on visualization of these datasets using perceptually based computer graphics techniques and display the visualizations using a combination PC CAVE and Active Mural. They also study the issues involved with ubiquitous access to high-resolution spatial datasets from mobile devices, and leverage a powerful wireless network. They focus on prediction-based adaptation and scheduling for distributed interactive applications and are building a compute cluster to support this work.&lt;br/&gt;&lt;br/&gt;What is missing in the existing shared research infrastructure is a facility to store and serve massive datasets interactively, with low latency and high bandwidth. The researchers will build such a shared data cluster. The data cluster will be very high performance and capacity PC RAIDs interconnected with a multi-GB/sec SAN. The SAN will connect to their compute cluster, CAVE/Mural, mobile devices and wired client machines. The combined data and compute clusters will be sufficient to pump 200 MB/s from disk to the CAVE/Mural screens. The combination of the data cluster, compute cluster, CAVE/Mural, local network infrastructure, and desktop and mobile clients will let them study interactivity in the large.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130869</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130869</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1081" target="n1082">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">iSWORD: Instrumented Streaming Research and Testbed</data>
      <data key="e_abstract">This proposal seeks funding for research in the design, implementation, and instrumentation of new&lt;br/&gt;methods for scalable wide-area on-demand reliable digital (SWORD) streaming. The larger goal of the&lt;br/&gt;research is to enable new streaming media applications, such as immediate access to a television show&lt;br/&gt;whenever a client anywhere would like to view it, by devising new practical cost-effective methods for on-demand real-time streaming of very popular streaming media content. The key aspects of the research to&lt;br/&gt;be undertaken are:&lt;br/&gt;1. The development of innovative practical new reliable multicast techniques that optimize client stream&lt;br/&gt;sharing to conserve server and network bandwidth, improve bandwidth cost-sharing, and maximize&lt;br/&gt;system scalability.&lt;br/&gt;2. The design of delivery techniques that work over the Internet or over satellite/cable networks, or any&lt;br/&gt;combination of these network technologies.&lt;br/&gt;3. The development of new metrics and models to determine which popular streams or partial streams&lt;br/&gt;should be cached at regional (or proxy) servers in order to minimize delivery cost for the data.&lt;br/&gt;4. The development of new network instrumentation facilities for the streaming media delivery protocols&lt;br/&gt;that enable deep understanding of the impact of network events and conditions on the performance of&lt;br/&gt;the protocols.&lt;br/&gt;5. The use of the new on-line network instrumentation facilities to develop new streaming methods that&lt;br/&gt;optimally adapt to current network conditions.&lt;br/&gt;6. The development of a novel testbed that allows experimentation with alternative new reliable multicast&lt;br/&gt;methods as well as alternative algorithms for caching streaming content closer to the clients, uses the&lt;br/&gt;new instrumentation facilities, and operates seamlessly and transparently in a live environment with&lt;br/&gt;ordinary clients accessing widely used media servers over large-scale networks such as Internet2 or the&lt;br/&gt;Internet MBONE.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">117810</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">117810</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n186" target="n1086">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Resources for Research in Scalable Parallel Computing and Networking Simulation</data>
      <data key="e_abstract">EIA-0130847 &lt;br/&gt;Gopal Gupta&lt;br/&gt;University of Texas Dallas&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Resources for Research in Scalable Parallel Computing and Networking Simulation&lt;br/&gt;&lt;br/&gt;The PIs seek to acquire a network of six 4-cpu multiprocessors and a data storage system connected with 100 Mbit/s Ethernet. This equipment will be used for building scalable parallel and distributed systems for tabled logic programming (LP), for computational geometry projects, as well as for quantitatively studying mobile ad-hoc networks (MANETs). The logic programming project seeks to combine dynamic reordering of alternatives with stack-splitting for realizing or-parallel tabled LP systems on distributed networks of SMPs. The computational geometry projects are related to research on design and implementation of scalable parallel and distributed algorithms for the weighted regions optimal trajectory problem, with applications in surgery planning, geographic information systems, and radiation therapy. The parallel implementations will be carried out on the network of SMPs. The MANET project will conduct a quantitative evaluation of unidirectional wireless links in MANETs: the impact of interference and battery life on the occurrence of unidirectional links, and MAC sub-layer issues for such links. Routing protocols designed to use both bidirectional and unidirectional links will be evaluated via simulations. The network of multiprocessors and the data storage system will be used, respectively, for conducting large simulations and for storing large simulation data.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130847</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130847</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1086" target="n1088">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Resources for Research in Scalable Parallel Computing and Networking Simulation</data>
      <data key="e_abstract">EIA-0130847 &lt;br/&gt;Gopal Gupta&lt;br/&gt;University of Texas Dallas&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Resources for Research in Scalable Parallel Computing and Networking Simulation&lt;br/&gt;&lt;br/&gt;The PIs seek to acquire a network of six 4-cpu multiprocessors and a data storage system connected with 100 Mbit/s Ethernet. This equipment will be used for building scalable parallel and distributed systems for tabled logic programming (LP), for computational geometry projects, as well as for quantitatively studying mobile ad-hoc networks (MANETs). The logic programming project seeks to combine dynamic reordering of alternatives with stack-splitting for realizing or-parallel tabled LP systems on distributed networks of SMPs. The computational geometry projects are related to research on design and implementation of scalable parallel and distributed algorithms for the weighted regions optimal trajectory problem, with applications in surgery planning, geographic information systems, and radiation therapy. The parallel implementations will be carried out on the network of SMPs. The MANET project will conduct a quantitative evaluation of unidirectional wireless links in MANETs: the impact of interference and battery life on the occurrence of unidirectional links, and MAC sub-layer issues for such links. Routing protocols designed to use both bidirectional and unidirectional links will be evaluated via simulations. The network of multiprocessors and the data storage system will be used, respectively, for conducting large simulations and for storing large simulation data.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130847</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130847</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n186" target="n1088">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Resources for Research in Scalable Parallel Computing and Networking Simulation</data>
      <data key="e_abstract">EIA-0130847 &lt;br/&gt;Gopal Gupta&lt;br/&gt;University of Texas Dallas&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Resources for Research in Scalable Parallel Computing and Networking Simulation&lt;br/&gt;&lt;br/&gt;The PIs seek to acquire a network of six 4-cpu multiprocessors and a data storage system connected with 100 Mbit/s Ethernet. This equipment will be used for building scalable parallel and distributed systems for tabled logic programming (LP), for computational geometry projects, as well as for quantitatively studying mobile ad-hoc networks (MANETs). The logic programming project seeks to combine dynamic reordering of alternatives with stack-splitting for realizing or-parallel tabled LP systems on distributed networks of SMPs. The computational geometry projects are related to research on design and implementation of scalable parallel and distributed algorithms for the weighted regions optimal trajectory problem, with applications in surgery planning, geographic information systems, and radiation therapy. The parallel implementations will be carried out on the network of SMPs. The MANET project will conduct a quantitative evaluation of unidirectional wireless links in MANETs: the impact of interference and battery life on the occurrence of unidirectional links, and MAC sub-layer issues for such links. Routing protocols designed to use both bidirectional and unidirectional links will be evaluated via simulations. The network of multiprocessors and the data storage system will be used, respectively, for conducting large simulations and for storing large simulation data.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130847</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130847</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1089" target="n1090">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Low Density Parity Check Codes for Channels with Memory</data>
      <data key="e_abstract">In recent years, low-density parity-check (LDPC) codes have been shown&lt;br/&gt;to have the power to perform within thousandths of decibels of the&lt;br/&gt;Shannon channel capacity of memoryless communications channels. This&lt;br/&gt;project seeks to answer a natural question: how good are these codes&lt;br/&gt;for transmission over channels with memory? The channels under&lt;br/&gt;consideration are Markovian memory channels, including both channels&lt;br/&gt;where the memory is dependent and independent of the transmitted&lt;br/&gt;symbols. Such channels arise in several applications, such as disk&lt;br/&gt;drives or other storage media.&lt;br/&gt;&lt;br/&gt;The analysis and design of LDPC codes has benefited from representing&lt;br/&gt;these codes as graphs, where the decoding is done by passing messages&lt;br/&gt;along the graph edges. This graph model allows an analysis using&lt;br/&gt;martingales that underlies recent advances, including the density&lt;br/&gt;evolution design technique. The investigators study how to extend&lt;br/&gt;this graphical modeling approach to channels with memory. More&lt;br/&gt;specifically, the project is divided into four major tasks: &lt;br/&gt;designing density evolution algorithms under new channel models;&lt;br/&gt;evaluating noise tolerance thresholds; engineering codes for short&lt;br/&gt;block lengths and rapid decoding; and achieving spectral shaping with&lt;br/&gt;low-density parity-check codes.</data>
      <data key="e_pgm">4096</data>
      <data key="e_label">118701</data>
      <data key="e_expirationDate">2005-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">118701</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1091" target="n1092">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Using context-recognition for preventative medicine in the home</data>
      <data key="e_abstract">The U.S. medical system faces an impending crisis: how to provide for the medical needs of a rapidly aging population. This project will investigate new technologies for &quot;just-in-time&quot; preventative health education in the home. Prior work on using computer telephony to deliver health education and counseling to people in their homes will be extended to mobile computing devices. Algorithms will be investigated that passively and actively collect data from healthy users of mobile computing devices and use that data to identify patterns of everyday activity using probabilistic models. The activity of each person will be used to present preventative health information and counseling at &quot;teachable moments&quot; -- the times when that information is most likely to impact the user&apos;s health-related decision making. A prototype system for elderly individuals interested in improving their exercise level and diet will be collaboratively developed by technologists and medical professionals. A participatory design process will be used to ensure the devices are easy to use, even for those who are not computer literate. The prototype system will be evaluated in a small focus group study.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112900</data>
      <data key="e_expirationDate">2004-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112900</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1091" target="n1093">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Using context-recognition for preventative medicine in the home</data>
      <data key="e_abstract">The U.S. medical system faces an impending crisis: how to provide for the medical needs of a rapidly aging population. This project will investigate new technologies for &quot;just-in-time&quot; preventative health education in the home. Prior work on using computer telephony to deliver health education and counseling to people in their homes will be extended to mobile computing devices. Algorithms will be investigated that passively and actively collect data from healthy users of mobile computing devices and use that data to identify patterns of everyday activity using probabilistic models. The activity of each person will be used to present preventative health information and counseling at &quot;teachable moments&quot; -- the times when that information is most likely to impact the user&apos;s health-related decision making. A prototype system for elderly individuals interested in improving their exercise level and diet will be collaboratively developed by technologists and medical professionals. A participatory design process will be used to ensure the devices are easy to use, even for those who are not computer literate. The prototype system will be evaluated in a small focus group study.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112900</data>
      <data key="e_expirationDate">2004-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112900</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1091" target="n1094">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Using context-recognition for preventative medicine in the home</data>
      <data key="e_abstract">The U.S. medical system faces an impending crisis: how to provide for the medical needs of a rapidly aging population. This project will investigate new technologies for &quot;just-in-time&quot; preventative health education in the home. Prior work on using computer telephony to deliver health education and counseling to people in their homes will be extended to mobile computing devices. Algorithms will be investigated that passively and actively collect data from healthy users of mobile computing devices and use that data to identify patterns of everyday activity using probabilistic models. The activity of each person will be used to present preventative health information and counseling at &quot;teachable moments&quot; -- the times when that information is most likely to impact the user&apos;s health-related decision making. A prototype system for elderly individuals interested in improving their exercise level and diet will be collaboratively developed by technologists and medical professionals. A participatory design process will be used to ensure the devices are easy to use, even for those who are not computer literate. The prototype system will be evaluated in a small focus group study.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112900</data>
      <data key="e_expirationDate">2004-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112900</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1092" target="n1093">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Using context-recognition for preventative medicine in the home</data>
      <data key="e_abstract">The U.S. medical system faces an impending crisis: how to provide for the medical needs of a rapidly aging population. This project will investigate new technologies for &quot;just-in-time&quot; preventative health education in the home. Prior work on using computer telephony to deliver health education and counseling to people in their homes will be extended to mobile computing devices. Algorithms will be investigated that passively and actively collect data from healthy users of mobile computing devices and use that data to identify patterns of everyday activity using probabilistic models. The activity of each person will be used to present preventative health information and counseling at &quot;teachable moments&quot; -- the times when that information is most likely to impact the user&apos;s health-related decision making. A prototype system for elderly individuals interested in improving their exercise level and diet will be collaboratively developed by technologists and medical professionals. A participatory design process will be used to ensure the devices are easy to use, even for those who are not computer literate. The prototype system will be evaluated in a small focus group study.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112900</data>
      <data key="e_expirationDate">2004-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112900</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1092" target="n1094">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Using context-recognition for preventative medicine in the home</data>
      <data key="e_abstract">The U.S. medical system faces an impending crisis: how to provide for the medical needs of a rapidly aging population. This project will investigate new technologies for &quot;just-in-time&quot; preventative health education in the home. Prior work on using computer telephony to deliver health education and counseling to people in their homes will be extended to mobile computing devices. Algorithms will be investigated that passively and actively collect data from healthy users of mobile computing devices and use that data to identify patterns of everyday activity using probabilistic models. The activity of each person will be used to present preventative health information and counseling at &quot;teachable moments&quot; -- the times when that information is most likely to impact the user&apos;s health-related decision making. A prototype system for elderly individuals interested in improving their exercise level and diet will be collaboratively developed by technologists and medical professionals. A participatory design process will be used to ensure the devices are easy to use, even for those who are not computer literate. The prototype system will be evaluated in a small focus group study.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112900</data>
      <data key="e_expirationDate">2004-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112900</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1093" target="n1094">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Using context-recognition for preventative medicine in the home</data>
      <data key="e_abstract">The U.S. medical system faces an impending crisis: how to provide for the medical needs of a rapidly aging population. This project will investigate new technologies for &quot;just-in-time&quot; preventative health education in the home. Prior work on using computer telephony to deliver health education and counseling to people in their homes will be extended to mobile computing devices. Algorithms will be investigated that passively and actively collect data from healthy users of mobile computing devices and use that data to identify patterns of everyday activity using probabilistic models. The activity of each person will be used to present preventative health information and counseling at &quot;teachable moments&quot; -- the times when that information is most likely to impact the user&apos;s health-related decision making. A prototype system for elderly individuals interested in improving their exercise level and diet will be collaboratively developed by technologists and medical professionals. A participatory design process will be used to ensure the devices are easy to use, even for those who are not computer literate. The prototype system will be evaluated in a small focus group study.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112900</data>
      <data key="e_expirationDate">2004-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112900</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1097" target="n1098">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR:SI Implementing Public-Key Cryptosystems for Secure Information Infrastructure</data>
      <data key="e_abstract">The research described here addresses information security issues for embedded processors and constrained environments. Embedded processors are found in a vast array of existing and emerging technologies, including mobile phones, personal digital assistants, smart cards, and remote-controlled utility network devices and are distinguished from typical (but far less prevalent) PC-level microprocessors in their relatively low power consumption and inherent limitations on memory and speed.&lt;br/&gt;&lt;br/&gt;It is predicted that the number of applications with embedded microprocessors which will be connected to our telephone and computer networks will increase dramatically over the next few years. For instance, it is anticipated that within the next few years, 50% of all Internet end-devices will have to operate in constrained environments. At the same time, these networks are enabling remote access to, and manipulation of, sensitive resources of all sorts, including bank records, medical information, alarm system, and industrial machinery.&lt;br/&gt;&lt;br/&gt;The need is clear for long-term planning and directed research in the area of cryptographic security for these devices. The challenges represented here require the attention of experts from a variety of disciplines, from engineering to computer science to mathematics. Both fundamental and immediate problems face hardware designers, software engineers, and theoretical crypytographers alike.&lt;br/&gt;&lt;br/&gt;We propose a three-pronged approach in an effort to contribute solutions to these problems. The long-term ambition of such research is to bring cryptographic security solutions to the market which are low-cost, highly scalable, and suitable for constrained environments. This project places particular focus on implementation of public-key algorithms in embedded devices and is divided into three modules as follow:&lt;br/&gt;&lt;br/&gt;Development of power-efficient and scalable cryptographic hardware for pervasive computing.&lt;br/&gt;&lt;br/&gt;Investigation of emerging public-key schemes which appear promising for implementation in constrained environments.&lt;br/&gt;&lt;br/&gt;Evaluation of combinatorial structures for public-key schemes in hardware and on embedded processors.&lt;br/&gt;&lt;br/&gt;This three-pronged approach combines a long view, cutting across disciplines, as well as a variety of very promising short-term objectives which assure practical relevance and payoff.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112889</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">112889</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1097" target="n1099">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR:SI Implementing Public-Key Cryptosystems for Secure Information Infrastructure</data>
      <data key="e_abstract">The research described here addresses information security issues for embedded processors and constrained environments. Embedded processors are found in a vast array of existing and emerging technologies, including mobile phones, personal digital assistants, smart cards, and remote-controlled utility network devices and are distinguished from typical (but far less prevalent) PC-level microprocessors in their relatively low power consumption and inherent limitations on memory and speed.&lt;br/&gt;&lt;br/&gt;It is predicted that the number of applications with embedded microprocessors which will be connected to our telephone and computer networks will increase dramatically over the next few years. For instance, it is anticipated that within the next few years, 50% of all Internet end-devices will have to operate in constrained environments. At the same time, these networks are enabling remote access to, and manipulation of, sensitive resources of all sorts, including bank records, medical information, alarm system, and industrial machinery.&lt;br/&gt;&lt;br/&gt;The need is clear for long-term planning and directed research in the area of cryptographic security for these devices. The challenges represented here require the attention of experts from a variety of disciplines, from engineering to computer science to mathematics. Both fundamental and immediate problems face hardware designers, software engineers, and theoretical crypytographers alike.&lt;br/&gt;&lt;br/&gt;We propose a three-pronged approach in an effort to contribute solutions to these problems. The long-term ambition of such research is to bring cryptographic security solutions to the market which are low-cost, highly scalable, and suitable for constrained environments. This project places particular focus on implementation of public-key algorithms in embedded devices and is divided into three modules as follow:&lt;br/&gt;&lt;br/&gt;Development of power-efficient and scalable cryptographic hardware for pervasive computing.&lt;br/&gt;&lt;br/&gt;Investigation of emerging public-key schemes which appear promising for implementation in constrained environments.&lt;br/&gt;&lt;br/&gt;Evaluation of combinatorial structures for public-key schemes in hardware and on embedded processors.&lt;br/&gt;&lt;br/&gt;This three-pronged approach combines a long view, cutting across disciplines, as well as a variety of very promising short-term objectives which assure practical relevance and payoff.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112889</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">112889</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1098" target="n1099">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR:SI Implementing Public-Key Cryptosystems for Secure Information Infrastructure</data>
      <data key="e_abstract">The research described here addresses information security issues for embedded processors and constrained environments. Embedded processors are found in a vast array of existing and emerging technologies, including mobile phones, personal digital assistants, smart cards, and remote-controlled utility network devices and are distinguished from typical (but far less prevalent) PC-level microprocessors in their relatively low power consumption and inherent limitations on memory and speed.&lt;br/&gt;&lt;br/&gt;It is predicted that the number of applications with embedded microprocessors which will be connected to our telephone and computer networks will increase dramatically over the next few years. For instance, it is anticipated that within the next few years, 50% of all Internet end-devices will have to operate in constrained environments. At the same time, these networks are enabling remote access to, and manipulation of, sensitive resources of all sorts, including bank records, medical information, alarm system, and industrial machinery.&lt;br/&gt;&lt;br/&gt;The need is clear for long-term planning and directed research in the area of cryptographic security for these devices. The challenges represented here require the attention of experts from a variety of disciplines, from engineering to computer science to mathematics. Both fundamental and immediate problems face hardware designers, software engineers, and theoretical crypytographers alike.&lt;br/&gt;&lt;br/&gt;We propose a three-pronged approach in an effort to contribute solutions to these problems. The long-term ambition of such research is to bring cryptographic security solutions to the market which are low-cost, highly scalable, and suitable for constrained environments. This project places particular focus on implementation of public-key algorithms in embedded devices and is divided into three modules as follow:&lt;br/&gt;&lt;br/&gt;Development of power-efficient and scalable cryptographic hardware for pervasive computing.&lt;br/&gt;&lt;br/&gt;Investigation of emerging public-key schemes which appear promising for implementation in constrained environments.&lt;br/&gt;&lt;br/&gt;Evaluation of combinatorial structures for public-key schemes in hardware and on embedded processors.&lt;br/&gt;&lt;br/&gt;This three-pronged approach combines a long view, cutting across disciplines, as well as a variety of very promising short-term objectives which assure practical relevance and payoff.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112889</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">112889</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1100" target="n1101">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Information Visualization and Incremental Knowledge Discovery in a Cluster Computing Environment</data>
      <data key="e_abstract">EIA-0130857 &lt;br/&gt;Elise deDoncker&lt;br/&gt;Western Michigan University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Information Visualization and Incremental Knowledge Discovery in a Cluster Computing Environment&lt;br/&gt;&lt;br/&gt;This proposal seeks funding to enhance a 64-node PC cluster at Western Michigan University, obtained in part on a previous National Science Foundation grant and university funding. Improving the cluster will give hardware support for on-going research and education on parallel and distributed algorithm design and development. Although the cluster is currently used for various research efforts by different groups in the university, this proposal focuses on two of these projects: parallel integration and distributed incremental knowledge discovery. &lt;br/&gt;&lt;br/&gt;The requested infrastructure includes a networking upgrade to support the current communication intensive (high bandwidth/low latency) projects. These include the tailoring of load balancing methods underlying the migration of regions in adaptive partitioning. The system will also provide a better environment for our study of the scalability of task partitioning methods in numerical integration and other areas. The proposed project on the development of methods for incremental knowledge discovery from large databases will further be supported by the visualization components of the infrastructure. This effort includes 3D interactive visual exploration of large relational data sets. The techniques will be incorporated in a systematic way into a software system for incremental knowledge discovery, and evaluated for effectiveness and suitability in different scenarios.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130857</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130857</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1100" target="n1102">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Information Visualization and Incremental Knowledge Discovery in a Cluster Computing Environment</data>
      <data key="e_abstract">EIA-0130857 &lt;br/&gt;Elise deDoncker&lt;br/&gt;Western Michigan University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Information Visualization and Incremental Knowledge Discovery in a Cluster Computing Environment&lt;br/&gt;&lt;br/&gt;This proposal seeks funding to enhance a 64-node PC cluster at Western Michigan University, obtained in part on a previous National Science Foundation grant and university funding. Improving the cluster will give hardware support for on-going research and education on parallel and distributed algorithm design and development. Although the cluster is currently used for various research efforts by different groups in the university, this proposal focuses on two of these projects: parallel integration and distributed incremental knowledge discovery. &lt;br/&gt;&lt;br/&gt;The requested infrastructure includes a networking upgrade to support the current communication intensive (high bandwidth/low latency) projects. These include the tailoring of load balancing methods underlying the migration of regions in adaptive partitioning. The system will also provide a better environment for our study of the scalability of task partitioning methods in numerical integration and other areas. The proposed project on the development of methods for incremental knowledge discovery from large databases will further be supported by the visualization components of the infrastructure. This effort includes 3D interactive visual exploration of large relational data sets. The techniques will be incorporated in a systematic way into a software system for incremental knowledge discovery, and evaluated for effectiveness and suitability in different scenarios.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130857</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130857</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1100" target="n1103">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Information Visualization and Incremental Knowledge Discovery in a Cluster Computing Environment</data>
      <data key="e_abstract">EIA-0130857 &lt;br/&gt;Elise deDoncker&lt;br/&gt;Western Michigan University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Information Visualization and Incremental Knowledge Discovery in a Cluster Computing Environment&lt;br/&gt;&lt;br/&gt;This proposal seeks funding to enhance a 64-node PC cluster at Western Michigan University, obtained in part on a previous National Science Foundation grant and university funding. Improving the cluster will give hardware support for on-going research and education on parallel and distributed algorithm design and development. Although the cluster is currently used for various research efforts by different groups in the university, this proposal focuses on two of these projects: parallel integration and distributed incremental knowledge discovery. &lt;br/&gt;&lt;br/&gt;The requested infrastructure includes a networking upgrade to support the current communication intensive (high bandwidth/low latency) projects. These include the tailoring of load balancing methods underlying the migration of regions in adaptive partitioning. The system will also provide a better environment for our study of the scalability of task partitioning methods in numerical integration and other areas. The proposed project on the development of methods for incremental knowledge discovery from large databases will further be supported by the visualization components of the infrastructure. This effort includes 3D interactive visual exploration of large relational data sets. The techniques will be incorporated in a systematic way into a software system for incremental knowledge discovery, and evaluated for effectiveness and suitability in different scenarios.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130857</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130857</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1101" target="n1102">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Information Visualization and Incremental Knowledge Discovery in a Cluster Computing Environment</data>
      <data key="e_abstract">EIA-0130857 &lt;br/&gt;Elise deDoncker&lt;br/&gt;Western Michigan University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Information Visualization and Incremental Knowledge Discovery in a Cluster Computing Environment&lt;br/&gt;&lt;br/&gt;This proposal seeks funding to enhance a 64-node PC cluster at Western Michigan University, obtained in part on a previous National Science Foundation grant and university funding. Improving the cluster will give hardware support for on-going research and education on parallel and distributed algorithm design and development. Although the cluster is currently used for various research efforts by different groups in the university, this proposal focuses on two of these projects: parallel integration and distributed incremental knowledge discovery. &lt;br/&gt;&lt;br/&gt;The requested infrastructure includes a networking upgrade to support the current communication intensive (high bandwidth/low latency) projects. These include the tailoring of load balancing methods underlying the migration of regions in adaptive partitioning. The system will also provide a better environment for our study of the scalability of task partitioning methods in numerical integration and other areas. The proposed project on the development of methods for incremental knowledge discovery from large databases will further be supported by the visualization components of the infrastructure. This effort includes 3D interactive visual exploration of large relational data sets. The techniques will be incorporated in a systematic way into a software system for incremental knowledge discovery, and evaluated for effectiveness and suitability in different scenarios.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130857</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130857</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1101" target="n1103">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Information Visualization and Incremental Knowledge Discovery in a Cluster Computing Environment</data>
      <data key="e_abstract">EIA-0130857 &lt;br/&gt;Elise deDoncker&lt;br/&gt;Western Michigan University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Information Visualization and Incremental Knowledge Discovery in a Cluster Computing Environment&lt;br/&gt;&lt;br/&gt;This proposal seeks funding to enhance a 64-node PC cluster at Western Michigan University, obtained in part on a previous National Science Foundation grant and university funding. Improving the cluster will give hardware support for on-going research and education on parallel and distributed algorithm design and development. Although the cluster is currently used for various research efforts by different groups in the university, this proposal focuses on two of these projects: parallel integration and distributed incremental knowledge discovery. &lt;br/&gt;&lt;br/&gt;The requested infrastructure includes a networking upgrade to support the current communication intensive (high bandwidth/low latency) projects. These include the tailoring of load balancing methods underlying the migration of regions in adaptive partitioning. The system will also provide a better environment for our study of the scalability of task partitioning methods in numerical integration and other areas. The proposed project on the development of methods for incremental knowledge discovery from large databases will further be supported by the visualization components of the infrastructure. This effort includes 3D interactive visual exploration of large relational data sets. The techniques will be incorporated in a systematic way into a software system for incremental knowledge discovery, and evaluated for effectiveness and suitability in different scenarios.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130857</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130857</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1102" target="n1103">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Information Visualization and Incremental Knowledge Discovery in a Cluster Computing Environment</data>
      <data key="e_abstract">EIA-0130857 &lt;br/&gt;Elise deDoncker&lt;br/&gt;Western Michigan University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Information Visualization and Incremental Knowledge Discovery in a Cluster Computing Environment&lt;br/&gt;&lt;br/&gt;This proposal seeks funding to enhance a 64-node PC cluster at Western Michigan University, obtained in part on a previous National Science Foundation grant and university funding. Improving the cluster will give hardware support for on-going research and education on parallel and distributed algorithm design and development. Although the cluster is currently used for various research efforts by different groups in the university, this proposal focuses on two of these projects: parallel integration and distributed incremental knowledge discovery. &lt;br/&gt;&lt;br/&gt;The requested infrastructure includes a networking upgrade to support the current communication intensive (high bandwidth/low latency) projects. These include the tailoring of load balancing methods underlying the migration of regions in adaptive partitioning. The system will also provide a better environment for our study of the scalability of task partitioning methods in numerical integration and other areas. The proposed project on the development of methods for incremental knowledge discovery from large databases will further be supported by the visualization components of the infrastructure. This effort includes 3D interactive visual exploration of large relational data sets. The techniques will be incorporated in a systematic way into a software system for incremental knowledge discovery, and evaluated for effectiveness and suitability in different scenarios.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130857</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130857</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n907" target="n1107">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: Collaborative Research: An OpenMp Environment for Wide-Area Networked Computing</data>
      <data key="e_abstract">EIA-0103610&lt;br/&gt;David Padua&lt;br/&gt;Univ. of Illinois&lt;br/&gt;&lt;br/&gt;NGS: Collaborative Research: An OpenMP Environment for Wide-Area Network Computing&lt;br/&gt;&lt;br/&gt;The objective of this proposal is to develop a new programming system that will close the gap that exists between programming SMPs and networked computer clusters, by using the emerging standard (OpenMP) for SMPs and develop compiler and runtime support that will enable using OpenMP on distributed platforms with optimized performance. The proposed programming system will make it possible to program the Grid and networked computer systems using OpenMP, the emerging standard for shared-memory programming. The PI expects to attain high efficiency in the execution of OpenMP programs for collection of Grid-connected computers. Through static and dynamic compiler techniques, we expect to harness the complexity and load fluctuations of the target environment.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103610</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103610</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1108" target="n1109">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SI-COLLABORATIVE RESEARCH: Investigations on CDMA Systems using Multiple Level Sequences and Partitioned Reduced Complexity Multiuser Detection Receivers</data>
      <data key="e_abstract">CDMA systems have achieved great commercial success in wireless communications since the first&lt;br/&gt;IS-95 system was deployed in Seattle in 1994. In fact, most of the future wireless communication&lt;br/&gt;systems will be using CDMA technology. Yet, as far as reaching the great potential of CDMA in terms&lt;br/&gt;of higher system capacity, lower bit error rate, and lower packet loss rate, the problem of demodulation in&lt;br/&gt;the presence of multiple access interference remains as a significant bottleneck for all CDMA systems.&lt;br/&gt;To solve this multiple access interference problem, researchers and engineers have worked since the early&lt;br/&gt;1980&apos;s on so-called multiuser detectors (MUD). Unfortunately, thus far none of the multiuser detectors&lt;br/&gt;has been implemented in a real CDMA system, because of the prohibitive complexity of these structures.&lt;br/&gt; There are two major factors that contribute to this situation. First, CDMA systems must use long&lt;br/&gt;spreading sequences for several practical reasons, whereas multiuser detectors must use short spreading&lt;br/&gt;sequences. Second, the complexity of even the simplest multiuser detector is still too great to be&lt;br/&gt;implemented in the fastest electronics, for all but the smallest of data rates. In this work, we propose to&lt;br/&gt;address both of these problems in a collaborative effort. The collaboration will be organized so that Dr.&lt;br/&gt;Qingchong Liu of Oakland University in Rochester, Michigan, will initially focus on the first objective,&lt;br/&gt;and Dr. David W. Matolak of Ohio University in Athens, Ohio, will initially (and concurrently) focus on&lt;br/&gt;the second objective. Results will be exchanged frequently, and meetings will be held quarterly. After&lt;br/&gt;some early progress, our collaboration will grow closer as we combine the work objectives and consider&lt;br/&gt;system (transmitter and receiver) performance.&lt;br/&gt; We propose first to extend some recent results obtained for the construction of long spreading&lt;br/&gt;sequences from short sequences. This can be termed the system objective. These new sequences will&lt;br/&gt;be appropriate for both conventional CDMA systems and for multiuser detectors. This method was&lt;br/&gt;invented by one of us and has been implemented in possibly the fastest wireless network running at&lt;br/&gt;400Mbps in the Spaceway system by Hughes Electronics. It has significantly reduced receiver&lt;br/&gt;complexity and cost, and gives essentially optimum performance. By measuring sequence correlations at&lt;br/&gt;multiple levels, the method provides new insights on sequence design and tremendously reduces&lt;br/&gt;complexity in signal design and detection for broadband wireless communications. This method has the&lt;br/&gt;potential to help bridge a gap between current CDMA systems and multiuser detectors.&lt;br/&gt; We also propose to study reduced complexity multiuser receivers from the perspective of reduced&lt;br/&gt;complexity trellis search techniques, combined with one of the most promising MUD receiver structures,&lt;br/&gt;in a partitioned manner. This can be termed the receiver objective. The MUD receiver of interest is&lt;br/&gt;the minimum mean-squared error (MMSE) receiver, which is attractive for its good performance and&lt;br/&gt;modest complexity. The partitioning approach will aim to share the detection tasks between the MMSE&lt;br/&gt;front-end and the reduced-complexity trellis processor. These receivers will make use of the new&lt;br/&gt;spreading sequences specified for use in future 3 rd and 4 th -generation CDMA wireless communication&lt;br/&gt;system standards, and the multiple level sequences developed in the system objective. The reduced&lt;br/&gt;complexity trellis search techniques will explore use of the analogous techniques researched for&lt;br/&gt;equalization, but not fully applied to the problem of CDMA multiuser detection. Our goal for these two&lt;br/&gt;objectives is to create a fundamental bridge to connect the existing and planned CDMA systems with the&lt;br/&gt;theoretical multiuser detectors so that the bottleneck of the multiple access interference problem in&lt;br/&gt;CDMA systems can be surmounted.&lt;br/&gt; Both objectives of this proposed work will employ analysis, followed by computer simulations.&lt;br/&gt;Both objectives will also require the assistance of graduate and undergraduate students. The modeling&lt;br/&gt;work will aim to reconcile theory with practical implementation and thus provide engineering education&lt;br/&gt;in the best sense: connecting principles and practice. Training students in both system design and&lt;br/&gt;receiver design for CDMA will also naturally be valuable to the wireless industry. In addition, the&lt;br/&gt;research will provide material for inclusion in several new graduate courses, and in undergraduate design&lt;br/&gt;projects, at both universities.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113307</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">113307</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1110" target="n1111">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">QEIB: Uncertainty Analysis, Spatial Interaction and Response Functions in Scaling-up Models of Forest Ecosystems</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;Acevedo&lt;br/&gt;0108563&lt;br/&gt;&lt;br/&gt;Detailed ecological data have been used to develop computer models of forest stands that have been applied to solve resource management issues. Increasingly, however, there is a need to answer questions that are relevant for larger areas, for example the impact of changing land use and of removing forest cover on habitat and water quality. This project will contribute to the methodology for scaling forest simulators into models of vegetation cover change in order to help answer those broad scale questions. Specifically, Dr. Acevedo and colleagues will develop methods to calculate the uncertainty associated with translating the model to larger areas, to scale the effect of neighboring vegetation patches, and to determine general mathematical functions of how those changes vary across a large landscape. The investigators will engage students in research as well as international collaboration with a team that is interdisciplinary and includes modelers, a mathematician, a computer scientist</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">108563</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">108563</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1110" target="n1112">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">QEIB: Uncertainty Analysis, Spatial Interaction and Response Functions in Scaling-up Models of Forest Ecosystems</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;Acevedo&lt;br/&gt;0108563&lt;br/&gt;&lt;br/&gt;Detailed ecological data have been used to develop computer models of forest stands that have been applied to solve resource management issues. Increasingly, however, there is a need to answer questions that are relevant for larger areas, for example the impact of changing land use and of removing forest cover on habitat and water quality. This project will contribute to the methodology for scaling forest simulators into models of vegetation cover change in order to help answer those broad scale questions. Specifically, Dr. Acevedo and colleagues will develop methods to calculate the uncertainty associated with translating the model to larger areas, to scale the effect of neighboring vegetation patches, and to determine general mathematical functions of how those changes vary across a large landscape. The investigators will engage students in research as well as international collaboration with a team that is interdisciplinary and includes modelers, a mathematician, a computer scientist</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">108563</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">108563</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1111" target="n1112">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">QEIB: Uncertainty Analysis, Spatial Interaction and Response Functions in Scaling-up Models of Forest Ecosystems</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;Acevedo&lt;br/&gt;0108563&lt;br/&gt;&lt;br/&gt;Detailed ecological data have been used to develop computer models of forest stands that have been applied to solve resource management issues. Increasingly, however, there is a need to answer questions that are relevant for larger areas, for example the impact of changing land use and of removing forest cover on habitat and water quality. This project will contribute to the methodology for scaling forest simulators into models of vegetation cover change in order to help answer those broad scale questions. Specifically, Dr. Acevedo and colleagues will develop methods to calculate the uncertainty associated with translating the model to larger areas, to scale the effect of neighboring vegetation patches, and to determine general mathematical functions of how those changes vary across a large landscape. The investigators will engage students in research as well as international collaboration with a team that is interdisciplinary and includes modelers, a mathematician, a computer scientist</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">108563</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">108563</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1113" target="n1114">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Automated Analysis of Probabilistic Open Systems</data>
      <data key="e_abstract">0098037&lt;br/&gt;Automated Analysis of Probabilistic Open Systems&lt;br/&gt;S. Purushothaman Iyer&lt;br/&gt;W. Rance Cleaveland (Co-PI)&lt;br/&gt;&lt;br/&gt;Concurrent systems such as network protocols and net-centric programs&lt;br/&gt;are difficult to build and debug because of the potential they exhibit&lt;br/&gt;for unintended process interactions. The development of net-based&lt;br/&gt;applications which have to contend with probabilistic guarantees from&lt;br/&gt;lower-levels is even more difficult as they need to be functionally&lt;br/&gt;correct and also satisfy reliability/performance constraints. This&lt;br/&gt;project will investigate how formal methods can be extended to address&lt;br/&gt;both logical correctness and reliability/performance constraints.&lt;br/&gt;&lt;br/&gt;The current project will explore semantic theories of systems that&lt;br/&gt;have both non-determinism and probabilistic choice. In particular,&lt;br/&gt;notions of equality and approximate equality of system behaviors will&lt;br/&gt;be investigated. Furthermore, the effect of these notions on&lt;br/&gt;compositional reasoning will also be studied.&lt;br/&gt;&lt;br/&gt;The second topic of the proposed work will be a thorough comparison of&lt;br/&gt;the semantic theories developed in this project against traditional&lt;br/&gt;approaches to dealing with non-determinism and probabilistic choice.&lt;br/&gt;&lt;br/&gt;Finally, practical algorithms for process minimization and for&lt;br/&gt;checking equality (and approximate equality) of processes will be&lt;br/&gt;designed and implemented in the Concurrency Workbench of New Century.&lt;br/&gt;Case studies, to evaluate the proposed theories, will also be&lt;br/&gt;constructed and studied.</data>
      <data key="e_pgm">2880</data>
      <data key="e_label">98037</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98037</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1115" target="n1116">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Information Dependencies in Databases and Data Mining</data>
      <data key="e_abstract">This project focuses on Information Dependency (InD) measures and the application of these measures to databases and datamining. InD measures use classical (Shannon) information theory to evaluate the information structure of database relations. This work extends results by the investigators of this project which show how InD measures generalize concepts important in database design, namely functional and multivalued dependencies. Research in this project is taking place across the spectrum from theory to practice. On the theoretical side, deeper details of InD&apos;s are investigated with an eye toward mechanisms for manipulating and applying InD measures. On the theoretic side, properties of InD&apos;s are investigated with an eye toward manipulating and applying InD measures, as well as toward implications of InD&apos;s on modeling. In the center, techniques for computing the measures are being investigated. Because the ultimate goal of datamining is to inform the user, investigations also include the interaction of InD and visualization. On the applied side, the major focus is the application of InD measures on data mining. Recognizing that research into applications requires real rather than &quot;toy&quot; targets, this project seeks collaborations involving data mining: the first such collaboration being with researchers in Biology. All of the activities of this project ultimately lead toward the development of prototype toolkit components based on InD measures.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">82407</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82407</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1115" target="n1117">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Information Dependencies in Databases and Data Mining</data>
      <data key="e_abstract">This project focuses on Information Dependency (InD) measures and the application of these measures to databases and datamining. InD measures use classical (Shannon) information theory to evaluate the information structure of database relations. This work extends results by the investigators of this project which show how InD measures generalize concepts important in database design, namely functional and multivalued dependencies. Research in this project is taking place across the spectrum from theory to practice. On the theoretical side, deeper details of InD&apos;s are investigated with an eye toward mechanisms for manipulating and applying InD measures. On the theoretic side, properties of InD&apos;s are investigated with an eye toward manipulating and applying InD measures, as well as toward implications of InD&apos;s on modeling. In the center, techniques for computing the measures are being investigated. Because the ultimate goal of datamining is to inform the user, investigations also include the interaction of InD and visualization. On the applied side, the major focus is the application of InD measures on data mining. Recognizing that research into applications requires real rather than &quot;toy&quot; targets, this project seeks collaborations involving data mining: the first such collaboration being with researchers in Biology. All of the activities of this project ultimately lead toward the development of prototype toolkit components based on InD measures.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">82407</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82407</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1116" target="n1117">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Information Dependencies in Databases and Data Mining</data>
      <data key="e_abstract">This project focuses on Information Dependency (InD) measures and the application of these measures to databases and datamining. InD measures use classical (Shannon) information theory to evaluate the information structure of database relations. This work extends results by the investigators of this project which show how InD measures generalize concepts important in database design, namely functional and multivalued dependencies. Research in this project is taking place across the spectrum from theory to practice. On the theoretical side, deeper details of InD&apos;s are investigated with an eye toward mechanisms for manipulating and applying InD measures. On the theoretic side, properties of InD&apos;s are investigated with an eye toward manipulating and applying InD measures, as well as toward implications of InD&apos;s on modeling. In the center, techniques for computing the measures are being investigated. Because the ultimate goal of datamining is to inform the user, investigations also include the interaction of InD and visualization. On the applied side, the major focus is the application of InD measures on data mining. Recognizing that research into applications requires real rather than &quot;toy&quot; targets, this project seeks collaborations involving data mining: the first such collaboration being with researchers in Biology. All of the activities of this project ultimately lead toward the development of prototype toolkit components based on InD measures.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">82407</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82407</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1120" target="n1121">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Empirical Validation of Information Theory-Based Software Metrics</data>
      <data key="e_abstract">CCR-0098024&lt;br/&gt;Empirical Validation of Information Theory-Based Software Metrics&lt;br/&gt;&lt;br/&gt;Edward B. Allen, P.I, and Rayford B. Vaughn, co-P.I.&lt;br/&gt;&lt;br/&gt;Empirical Validation of Information Theory-Based Software Metrics is a&lt;br/&gt;research project of Mississippi State University that is empirically&lt;br/&gt;validating the usefulness of a new generation of software metrics, based&lt;br/&gt;on information theory.&lt;br/&gt;&lt;br/&gt;Software engineers employ a wide variety of diagrams during development&lt;br/&gt;of software. Because many abstractions of software are represented by&lt;br/&gt;graphs, metrics of graph attributes have the potential for wide&lt;br/&gt;application. Information theory is an alternative to counting, focusing&lt;br/&gt;on the amount of information in an attribute. The contribution of this&lt;br/&gt;project to the state of the art is empirical evidence that information&lt;br/&gt;theory-based software metrics of size, length, complexity, coupling and&lt;br/&gt;cohesion can be useful timely predictors of software quality, and that&lt;br/&gt;they have advantages over counting-based metrics.&lt;br/&gt;&lt;br/&gt;Case studies in collaboration with industrial and government software&lt;br/&gt;development organizations are providing a meaningful evaluation by&lt;br/&gt;examining a variety of real-world software systems large enough to be&lt;br/&gt;comparable to other industry projects. Collaborators include EDS Inc.,&lt;br/&gt;Ericsson Inc., and MPI Software Technology Inc. Results will be&lt;br/&gt;disseminated throught the Center for Empirically Based Software&lt;br/&gt;Engineering (CeBASE). The empirical evidence generated by this project&lt;br/&gt;is aimed to facilitate a new level of cost-effective improvement to&lt;br/&gt;software quality.</data>
      <data key="e_pgm">2880</data>
      <data key="e_label">98024</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98024</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n757" target="n758">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: Collaborative Research: Adaptive Performance and Power Management for Real-Time Systems</data>
      <data key="e_abstract">EIA-0102696&lt;br/&gt;Israe Koren&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;Adaptive Performance and Power Management for Real-Time Systems&lt;br/&gt;&lt;br/&gt;This research will enable real-time computer systems to monitor and automatically adapt their power usage to specified fault-tolerance and performance requirements, prevailing workload, and current and projected energy/power constraints.&lt;br/&gt;&lt;br/&gt;Many systems, which are power-constrained, do not always have a strict limit on their power consumption. Instead, they have periods of time during which energy supply is very limited, and should be used sparingly, while at other times, the power available is adequate and the system is allowed to exploit all its resources to deliver maximum performability. While there has been a great deal of research in developing circuits and design techniques to build low-power devices, there is very little reported on techniques to adjust power consumption on-the-fly to adapt to changing levels of workload and energy/power availability. There is the need for an integrated approach, ranging from the hardware operation (using voltage-clock scaling or a sleep mode) to the application level. Our preliminary investigations have shown that such an integrated approach, exploiting the synergy between these various layers, is far more effective than single-layer approaches adopted incrementally.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">102696</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">102696</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1126" target="n1127">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Large-Scale Distributed Cortical Networks in Vision</data>
      <data key="e_abstract">Visual processing in the mammalian brain is not done simply with several parallel channels leading to higher areas. We now know that visual input pathways diverge into multiple processing streams, with feedback at all levels in each stream and crosstalk between streams. The visual cortex can be seen in this view at a dynamic system of interconnected areas interacting flexibly in different combinations at different stages of processing. This renewal project builds on technological advances and analytical tools for with high spatial, temporal and frequency resolution, developed from prior support. These comprehensive advances make it possible to monitor multi-area functional interdependency patterns that arise in the cortex, and to measure and analyze how one cortical area can affect others. The novel approach in the current project is to examine the mesoscopic scale of functional brain organization, offering a complementary level between the microscopic recording of single cell activity in a local area or layer, and the macroscopic derivation of images from whole brains using scanning technologies such as PET and fMRI. &lt;br/&gt; Results will have an impact by providing new insights into the dynamics of functional interdependency in the visual cortex, and going beyond visual neuroscience to make available digital signal processing tools potentially useful for a handling large-scale neural systems in a range of cognitive studies, and potentially leading to designing better complex artificial neural networks. This project also provides excellent cross-disciplinary training opportunities for students.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">90717</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">90717</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1129" target="n1130">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Data Mining of Digital Behaviour</data>
      <data key="e_abstract">The goal of this research project is to improve our understanding of how humans behave in information-seeking digital environments such as the Web. The approach consists of using massively large Web logs to infer patterns of behavior. New probabilistic models for modeling human behavior on the Web are under investigation including Markov and switching models, mixture models, and Bayesian hierarchical models. Adaptive statistical techniques form the basis for building up individual user models in an online fashion, automatically learning both the dynamic time-dependent patterns of a user as well as text-vector representations of their interests. Test data sets from large commercial Web sites are being used to develop, validate, and test the models. Data are anonymized to protect individual privacy. The statistical user models are in turn being used to develop two primary software tools. The first tool allows an analyst to explore, cluster, predict, and visualize Web logs with millions of entries, allowing an understanding of dynamic patterns of access and behavior in a manner that is not currently available in research or commercial tools. The second tool, WebMARS, uses adaptive user-models to enhance information retrieval algorithms by interpreting search queries in a personalized manner. More generally, the results from this project will provide tools and techniques to enable a better scientific understanding of modes of human behavior across a broad range of digital environments, with potential applications in wireless information appliances, medical informatics, scientific exploration of massive data sets, and so forth.</data>
      <data key="e_pgm">H228</data>
      <data key="e_label">83489</data>
      <data key="e_expirationDate">2010-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">83489</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1134" target="n1135">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NSF Workshop on Computational Science</data>
      <data key="e_abstract">In the late 1980&apos;s the field of computational science and engineering emerged as a new discipline, one with a research core that generalized from its many applications and led to the creation os several academic programs at leading unversities. Computational science has since proven itself with many successes, but it has also evolved both in its tools, methodologies, and research challenges and goals. This workshop seeks to re-examine the area, and develop a new consensus on the directions it should be taking. A major goal of the workshop will be a document summarizing the accomplishments of the past decade, and prognosticating the future for scientific computing research.</data>
      <data key="e_pgm">2878</data>
      <data key="e_label">136291</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">136291</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1137" target="n1138">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY: Combinatorial Optimization Algorithms for Informaion Access (Fundamental IT Models)</data>
      <data key="e_abstract">Proposal No: 0113371&lt;br/&gt;ITR/SY: Combinatorial Optimization Algorithms for Information Access (Fundamental IT Models)&lt;br/&gt;PI: Eva Tardos&lt;br/&gt;&lt;br/&gt;The enormous growth in the amount of information online makes it vital&lt;br/&gt;to automate the job of searching and organizing large data collections&lt;br/&gt;while maintaining high accuracy. The research will consider two&lt;br/&gt;seemingly unrelated tasks:&lt;br/&gt;&lt;br/&gt;. Searching web pages, and&lt;br/&gt;. Analyzing the content of digital pictures and movies.&lt;br/&gt;&lt;br/&gt;Despite their obvious great importance, and the large efforts invested,&lt;br/&gt;the currently available solutions for these tasks are inadequate. The&lt;br/&gt;proposed research will address a difficult mathematical problem that&lt;br/&gt;underlies both of these tasks, the classification problem with pair-wise&lt;br/&gt;constraints. &lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The traditional classification problems consist of a set of objects to&lt;br/&gt;be classified, and a set of labels (the classes). Classifying the topics&lt;br/&gt;of documents on the Web, or individual pixels or regions of an image are&lt;br/&gt;two examples of this general problem. The proposed research will build&lt;br/&gt;on the PIs recent successes in applying powerful combinatorial&lt;br/&gt;optimization techniques to develop algorithms for classification&lt;br/&gt;problems with pair-wise relationships. Pair-wise constraints can&lt;br/&gt;significantly enhance classification, by modeling, e.g., the relations&lt;br/&gt;of physically close objects in an image, or relations implicit in the&lt;br/&gt;hyperlink structure of the Web. The outcome of this research will be to&lt;br/&gt;provide powerful new tools for two important tasks in searching and&lt;br/&gt;organizing large data collections.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113371</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113371</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1139" target="n1140">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/SI(CISE):Optimal and Robust TCP Congestion Control</data>
      <data key="e_abstract">Internet is undergoing an overhaul unprecedented in size, diversity, and reach, with profound im-&lt;br/&gt;pact in all aspects of our scientific, social, economic and political life through the integration of&lt;br/&gt;networks of communication, transportation, entertainment, utilities, and finance. The stability&lt;br/&gt;and robustness of this vital infrastructure demands a rigorous theory to understand the current&lt;br/&gt;protocols and evolve them to meet emerging challenges. We propose to develop such a theory for&lt;br/&gt;TCP congestion control, and use it to drastically improve the stability, robustness and optimality&lt;br/&gt;of the current protocols.&lt;br/&gt;A key insight is to view congestion control as a distributed asynchronous computation to maxi-&lt;br/&gt;mize aggregate source utility over the Internet; different TCP and active queue management (AQM)&lt;br/&gt;schemes correspond to different utility functions and different algorithms to maximize them.&lt;br/&gt;Our research hastwo components. First, we will develop a new theoretical model of TCP&lt;br/&gt;congestion control based on duality in optimization and multivariate robust control. The theory will&lt;br/&gt;clarify the role of source algorithms, such as Tahoe, Reno and Vegas, and active queue management,&lt;br/&gt;such as DropTail, RED and REM, in the control of networks and establish performance limits of&lt;br/&gt;the current protocols; it will explain the effect on stability when delay, topology, capacity, and load&lt;br/&gt;scale up; and it will provide conditions under which the feedback stability ofTCP/AQM algorithms&lt;br/&gt;are invariant to these effects. Indeed, such a theory is already emerging from our recent works.&lt;br/&gt;Even in its currently preliminary stage, it already provides a fundamental understanding on some&lt;br/&gt;widely observed performance and fairness behavior of the current protocols, and uncovers new and&lt;br/&gt;surprising stability problems. For example, it shows that the current protocols become unstable and&lt;br/&gt;exhibit bifurcation when network capacity increases. Moreover, maintaining stability as capacity&lt;br/&gt;scales up arbitrarily imposes severe constraints on how sources adjust their rates (TCP) and what&lt;br/&gt;congestion information is fed back (AQM). The current protocol does not satisfy the condition&lt;br/&gt;for such stability invariance, and hence may be ill suited for future networks where, pulled by&lt;br/&gt;application demand and pushed by technological advances, the capacity will be large.&lt;br/&gt;The second component of our research is the design of practical TCP and AQM protocols&lt;br/&gt;based on the theory, and the development of prototypes and experiments to demonstrate their&lt;br/&gt;effectiveness. We will use the theory to identify the sources of instability in the current protocols&lt;br/&gt;when delay, network size, capacity, and traffic load scale up. We will design both enhancements that&lt;br/&gt;incrementally evolve the current protocols, and drastically new protocols that have the strongly&lt;br/&gt;robust stability property promised by theory. As a concrete application of our algorithms, we will&lt;br/&gt;apply them to improve TCP performance over wireless links, both because they are ubiquitous and&lt;br/&gt;because they are likely to remain the most important bottlenecks in future networks.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113425</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">113425</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n2" target="n1141">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Acquisition of a cluster for experimental parallel computing research in scientific computing and computational biology</data>
      <data key="e_abstract">0130861 &lt;br/&gt;Srinivas Aluru&lt;br/&gt;Iowa Stae University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Acquisition of a Cluster for Experimental Parallel Computing Research in Scientific Computing and Computational Biology&lt;br/&gt;&lt;br/&gt;This project provides funding to Department of Electrical and Computer Engineering at Iowa State University for acquisition of a cluster of 64 Pentium workstations connected by a high-speed network. The cluster will be used as a distributed memory parallel computer and will be dedicated to support research in parallel computing, bioinformatics and computational biology, and scientific computing. More specifically, the equipment will be used for research in: parallel algorithms and software for gene identification, parallel algorithms and software for electromagnetic scattering analysis, scalable parallelization of tree-based data structures, and integrated software visualization environments for program correctness, validation and optimization. Experimental studies will be conducted to evaluate the performance of the parallel algorithms developed and the knowledge gained from such studies will be used to derive efficient, scalable, parallel implementations. Acquisition of the instrumentation will enable researchers to develop, demonstrate and disseminate comprehensive software systems that are capable of solving large-scale scientific applications of current relevance.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130861</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130861</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n598" target="n1141">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Acquisition of a cluster for experimental parallel computing research in scientific computing and computational biology</data>
      <data key="e_abstract">0130861 &lt;br/&gt;Srinivas Aluru&lt;br/&gt;Iowa Stae University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Acquisition of a Cluster for Experimental Parallel Computing Research in Scientific Computing and Computational Biology&lt;br/&gt;&lt;br/&gt;This project provides funding to Department of Electrical and Computer Engineering at Iowa State University for acquisition of a cluster of 64 Pentium workstations connected by a high-speed network. The cluster will be used as a distributed memory parallel computer and will be dedicated to support research in parallel computing, bioinformatics and computational biology, and scientific computing. More specifically, the equipment will be used for research in: parallel algorithms and software for gene identification, parallel algorithms and software for electromagnetic scattering analysis, scalable parallelization of tree-based data structures, and integrated software visualization environments for program correctness, validation and optimization. Experimental studies will be conducted to evaluate the performance of the parallel algorithms developed and the knowledge gained from such studies will be used to derive efficient, scalable, parallel implementations. Acquisition of the instrumentation will enable researchers to develop, demonstrate and disseminate comprehensive software systems that are capable of solving large-scale scientific applications of current relevance.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130861</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130861</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1141" target="n1144">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Acquisition of a cluster for experimental parallel computing research in scientific computing and computational biology</data>
      <data key="e_abstract">0130861 &lt;br/&gt;Srinivas Aluru&lt;br/&gt;Iowa Stae University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Acquisition of a Cluster for Experimental Parallel Computing Research in Scientific Computing and Computational Biology&lt;br/&gt;&lt;br/&gt;This project provides funding to Department of Electrical and Computer Engineering at Iowa State University for acquisition of a cluster of 64 Pentium workstations connected by a high-speed network. The cluster will be used as a distributed memory parallel computer and will be dedicated to support research in parallel computing, bioinformatics and computational biology, and scientific computing. More specifically, the equipment will be used for research in: parallel algorithms and software for gene identification, parallel algorithms and software for electromagnetic scattering analysis, scalable parallelization of tree-based data structures, and integrated software visualization environments for program correctness, validation and optimization. Experimental studies will be conducted to evaluate the performance of the parallel algorithms developed and the knowledge gained from such studies will be used to derive efficient, scalable, parallel implementations. Acquisition of the instrumentation will enable researchers to develop, demonstrate and disseminate comprehensive software systems that are capable of solving large-scale scientific applications of current relevance.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130861</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130861</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1141" target="n1145">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Acquisition of a cluster for experimental parallel computing research in scientific computing and computational biology</data>
      <data key="e_abstract">0130861 &lt;br/&gt;Srinivas Aluru&lt;br/&gt;Iowa Stae University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Acquisition of a Cluster for Experimental Parallel Computing Research in Scientific Computing and Computational Biology&lt;br/&gt;&lt;br/&gt;This project provides funding to Department of Electrical and Computer Engineering at Iowa State University for acquisition of a cluster of 64 Pentium workstations connected by a high-speed network. The cluster will be used as a distributed memory parallel computer and will be dedicated to support research in parallel computing, bioinformatics and computational biology, and scientific computing. More specifically, the equipment will be used for research in: parallel algorithms and software for gene identification, parallel algorithms and software for electromagnetic scattering analysis, scalable parallelization of tree-based data structures, and integrated software visualization environments for program correctness, validation and optimization. Experimental studies will be conducted to evaluate the performance of the parallel algorithms developed and the knowledge gained from such studies will be used to derive efficient, scalable, parallel implementations. Acquisition of the instrumentation will enable researchers to develop, demonstrate and disseminate comprehensive software systems that are capable of solving large-scale scientific applications of current relevance.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130861</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130861</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n2" target="n598">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Acquisition of a cluster for experimental parallel computing research in scientific computing and computational biology</data>
      <data key="e_abstract">0130861 &lt;br/&gt;Srinivas Aluru&lt;br/&gt;Iowa Stae University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Acquisition of a Cluster for Experimental Parallel Computing Research in Scientific Computing and Computational Biology&lt;br/&gt;&lt;br/&gt;This project provides funding to Department of Electrical and Computer Engineering at Iowa State University for acquisition of a cluster of 64 Pentium workstations connected by a high-speed network. The cluster will be used as a distributed memory parallel computer and will be dedicated to support research in parallel computing, bioinformatics and computational biology, and scientific computing. More specifically, the equipment will be used for research in: parallel algorithms and software for gene identification, parallel algorithms and software for electromagnetic scattering analysis, scalable parallelization of tree-based data structures, and integrated software visualization environments for program correctness, validation and optimization. Experimental studies will be conducted to evaluate the performance of the parallel algorithms developed and the knowledge gained from such studies will be used to derive efficient, scalable, parallel implementations. Acquisition of the instrumentation will enable researchers to develop, demonstrate and disseminate comprehensive software systems that are capable of solving large-scale scientific applications of current relevance.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130861</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130861</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n2" target="n1144">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Acquisition of a cluster for experimental parallel computing research in scientific computing and computational biology</data>
      <data key="e_abstract">0130861 &lt;br/&gt;Srinivas Aluru&lt;br/&gt;Iowa Stae University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Acquisition of a Cluster for Experimental Parallel Computing Research in Scientific Computing and Computational Biology&lt;br/&gt;&lt;br/&gt;This project provides funding to Department of Electrical and Computer Engineering at Iowa State University for acquisition of a cluster of 64 Pentium workstations connected by a high-speed network. The cluster will be used as a distributed memory parallel computer and will be dedicated to support research in parallel computing, bioinformatics and computational biology, and scientific computing. More specifically, the equipment will be used for research in: parallel algorithms and software for gene identification, parallel algorithms and software for electromagnetic scattering analysis, scalable parallelization of tree-based data structures, and integrated software visualization environments for program correctness, validation and optimization. Experimental studies will be conducted to evaluate the performance of the parallel algorithms developed and the knowledge gained from such studies will be used to derive efficient, scalable, parallel implementations. Acquisition of the instrumentation will enable researchers to develop, demonstrate and disseminate comprehensive software systems that are capable of solving large-scale scientific applications of current relevance.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130861</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130861</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n2" target="n1145">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Acquisition of a cluster for experimental parallel computing research in scientific computing and computational biology</data>
      <data key="e_abstract">0130861 &lt;br/&gt;Srinivas Aluru&lt;br/&gt;Iowa Stae University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Acquisition of a Cluster for Experimental Parallel Computing Research in Scientific Computing and Computational Biology&lt;br/&gt;&lt;br/&gt;This project provides funding to Department of Electrical and Computer Engineering at Iowa State University for acquisition of a cluster of 64 Pentium workstations connected by a high-speed network. The cluster will be used as a distributed memory parallel computer and will be dedicated to support research in parallel computing, bioinformatics and computational biology, and scientific computing. More specifically, the equipment will be used for research in: parallel algorithms and software for gene identification, parallel algorithms and software for electromagnetic scattering analysis, scalable parallelization of tree-based data structures, and integrated software visualization environments for program correctness, validation and optimization. Experimental studies will be conducted to evaluate the performance of the parallel algorithms developed and the knowledge gained from such studies will be used to derive efficient, scalable, parallel implementations. Acquisition of the instrumentation will enable researchers to develop, demonstrate and disseminate comprehensive software systems that are capable of solving large-scale scientific applications of current relevance.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130861</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130861</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n598" target="n1144">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Acquisition of a cluster for experimental parallel computing research in scientific computing and computational biology</data>
      <data key="e_abstract">0130861 &lt;br/&gt;Srinivas Aluru&lt;br/&gt;Iowa Stae University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Acquisition of a Cluster for Experimental Parallel Computing Research in Scientific Computing and Computational Biology&lt;br/&gt;&lt;br/&gt;This project provides funding to Department of Electrical and Computer Engineering at Iowa State University for acquisition of a cluster of 64 Pentium workstations connected by a high-speed network. The cluster will be used as a distributed memory parallel computer and will be dedicated to support research in parallel computing, bioinformatics and computational biology, and scientific computing. More specifically, the equipment will be used for research in: parallel algorithms and software for gene identification, parallel algorithms and software for electromagnetic scattering analysis, scalable parallelization of tree-based data structures, and integrated software visualization environments for program correctness, validation and optimization. Experimental studies will be conducted to evaluate the performance of the parallel algorithms developed and the knowledge gained from such studies will be used to derive efficient, scalable, parallel implementations. Acquisition of the instrumentation will enable researchers to develop, demonstrate and disseminate comprehensive software systems that are capable of solving large-scale scientific applications of current relevance.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130861</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130861</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n598" target="n1145">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Acquisition of a cluster for experimental parallel computing research in scientific computing and computational biology</data>
      <data key="e_abstract">0130861 &lt;br/&gt;Srinivas Aluru&lt;br/&gt;Iowa Stae University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Acquisition of a Cluster for Experimental Parallel Computing Research in Scientific Computing and Computational Biology&lt;br/&gt;&lt;br/&gt;This project provides funding to Department of Electrical and Computer Engineering at Iowa State University for acquisition of a cluster of 64 Pentium workstations connected by a high-speed network. The cluster will be used as a distributed memory parallel computer and will be dedicated to support research in parallel computing, bioinformatics and computational biology, and scientific computing. More specifically, the equipment will be used for research in: parallel algorithms and software for gene identification, parallel algorithms and software for electromagnetic scattering analysis, scalable parallelization of tree-based data structures, and integrated software visualization environments for program correctness, validation and optimization. Experimental studies will be conducted to evaluate the performance of the parallel algorithms developed and the knowledge gained from such studies will be used to derive efficient, scalable, parallel implementations. Acquisition of the instrumentation will enable researchers to develop, demonstrate and disseminate comprehensive software systems that are capable of solving large-scale scientific applications of current relevance.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130861</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130861</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1144" target="n1145">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Acquisition of a cluster for experimental parallel computing research in scientific computing and computational biology</data>
      <data key="e_abstract">0130861 &lt;br/&gt;Srinivas Aluru&lt;br/&gt;Iowa Stae University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Acquisition of a Cluster for Experimental Parallel Computing Research in Scientific Computing and Computational Biology&lt;br/&gt;&lt;br/&gt;This project provides funding to Department of Electrical and Computer Engineering at Iowa State University for acquisition of a cluster of 64 Pentium workstations connected by a high-speed network. The cluster will be used as a distributed memory parallel computer and will be dedicated to support research in parallel computing, bioinformatics and computational biology, and scientific computing. More specifically, the equipment will be used for research in: parallel algorithms and software for gene identification, parallel algorithms and software for electromagnetic scattering analysis, scalable parallelization of tree-based data structures, and integrated software visualization environments for program correctness, validation and optimization. Experimental studies will be conducted to evaluate the performance of the parallel algorithms developed and the knowledge gained from such studies will be used to derive efficient, scalable, parallel implementations. Acquisition of the instrumentation will enable researchers to develop, demonstrate and disseminate comprehensive software systems that are capable of solving large-scale scientific applications of current relevance.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130861</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130861</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1146" target="n1147">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Coordination, Control and Communications Strategies for Teams of Mobile Robots</data>
      <data key="e_abstract">EIA-0130858 &lt;br/&gt;Camillo Taylor&lt;br/&gt;University of Pennsylvania&lt;br/&gt;&lt;br/&gt;The idea of deploying teams of small, inexpensive robotic agents to accomplish various sensing, manipulation and communication tasks is one that has gained increasing currency over the last few years. This project will involve three interrelated research thrusts that investigate various aspects of this paradigm. The first thrust deals with the problems associated with coordinating the motion of teams of robots. Some of the questions that are addressed by this effort include the problem of controlling the motion of robots moving in formation and coordinating the action of robots engaged in cooperative manipulation of an object. The second thrust focuses on the issues associated with combining the information obtained from distributed robots to form a coherent model of the environment. The third area of research concerns the problems associated with designing and analyzing networking strategies that are appropriate for use with distributed teams of robotic agents. Since the platforms are mobile, many of the traditional networking strategies, which were designed with fixed infrastructure in mind, are not applicable. As part of this proposal we intend to investigate questions concerning the appropriateness of various wireless networking technologies such as IEEE 802.11b and Bluetooth. This proposal requests funding to purchase the equipment required to develop a fleet of networked robots that would serve as a shared testbed for our research efforts.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130858</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130858</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1146" target="n1148">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Coordination, Control and Communications Strategies for Teams of Mobile Robots</data>
      <data key="e_abstract">EIA-0130858 &lt;br/&gt;Camillo Taylor&lt;br/&gt;University of Pennsylvania&lt;br/&gt;&lt;br/&gt;The idea of deploying teams of small, inexpensive robotic agents to accomplish various sensing, manipulation and communication tasks is one that has gained increasing currency over the last few years. This project will involve three interrelated research thrusts that investigate various aspects of this paradigm. The first thrust deals with the problems associated with coordinating the motion of teams of robots. Some of the questions that are addressed by this effort include the problem of controlling the motion of robots moving in formation and coordinating the action of robots engaged in cooperative manipulation of an object. The second thrust focuses on the issues associated with combining the information obtained from distributed robots to form a coherent model of the environment. The third area of research concerns the problems associated with designing and analyzing networking strategies that are appropriate for use with distributed teams of robotic agents. Since the platforms are mobile, many of the traditional networking strategies, which were designed with fixed infrastructure in mind, are not applicable. As part of this proposal we intend to investigate questions concerning the appropriateness of various wireless networking technologies such as IEEE 802.11b and Bluetooth. This proposal requests funding to purchase the equipment required to develop a fleet of networked robots that would serve as a shared testbed for our research efforts.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130858</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130858</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1147" target="n1148">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Coordination, Control and Communications Strategies for Teams of Mobile Robots</data>
      <data key="e_abstract">EIA-0130858 &lt;br/&gt;Camillo Taylor&lt;br/&gt;University of Pennsylvania&lt;br/&gt;&lt;br/&gt;The idea of deploying teams of small, inexpensive robotic agents to accomplish various sensing, manipulation and communication tasks is one that has gained increasing currency over the last few years. This project will involve three interrelated research thrusts that investigate various aspects of this paradigm. The first thrust deals with the problems associated with coordinating the motion of teams of robots. Some of the questions that are addressed by this effort include the problem of controlling the motion of robots moving in formation and coordinating the action of robots engaged in cooperative manipulation of an object. The second thrust focuses on the issues associated with combining the information obtained from distributed robots to form a coherent model of the environment. The third area of research concerns the problems associated with designing and analyzing networking strategies that are appropriate for use with distributed teams of robotic agents. Since the platforms are mobile, many of the traditional networking strategies, which were designed with fixed infrastructure in mind, are not applicable. As part of this proposal we intend to investigate questions concerning the appropriateness of various wireless networking technologies such as IEEE 802.11b and Bluetooth. This proposal requests funding to purchase the equipment required to develop a fleet of networked robots that would serve as a shared testbed for our research efforts.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130858</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130858</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1149" target="n1150">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/IM(IDM): Developing A Collaborative Information and Knowledge Management Infrastructure</data>
      <data key="e_abstract">The proposed research is aimed at designing and developing new resources for information and knowledge management in a heterogeneous distributed environment for law enforcement. Important problem areas to be addressed are: system scalability, group-based monitoring of dynamically changing data, analyzing group behavior in search and notification, and incremental searching, among others&lt;br/&gt;The technical approach will be implemented in an agent-based framework and empirical studies will be performed to evaluate the proposed prototype in two law enforcement agencies.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">114011</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">114011</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1152" target="n1153">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/IM: Handling Uncertainty in Spatial Databases</data>
      <data key="e_abstract">The goal of this research project is to develop new techniques to handle uncertainty in object-oriented dynamic databases. The approach consists of developing a new data model and associated algebra; developing query optimization techniques for inexact queries; and development of indexing methods to support probabilistic uncertainty in dynamic databases. This research effort develops new theory, tools and technology to support various types of probabilistic uncertainty in object-oriented dynamic databases. The experimental research is linked to spatial databases with applications in the field of Geographic Information Systems. By adding support for uncertainty in database management systems, this research project will substantially increase the power and flexibility of database management in a broad class of business, social, scientific and engineering endeavors where uncertainty is measured and used. &lt;br/&gt;http://www.vislab.ucr.edu/intro.html</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">114036</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">114036</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n43" target="n801">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Meta-MAC Protocols: A New Dimension to Adaptation in Medium Access Control</data>
      <data key="e_abstract">In all networks that have a broadcast channel as the basis of communication, the medium access control (MAC) protocol serves a vital role, as it directly controls the access to communication resources. As the networks and the traffic they carry both become more heterogeneous the question is how to best adapt to the unknown or changing network conditions. The natural answer provided by most existing protocols is to include some kind of adaptivity in order to dynamically adust their operation to the actual network conditions. Examples of adaptivity include hybrid protocols that periodically recompute slot assignments, adjustment of retransmission probabilities (e.g., backoff mechanisms), as well as many other ad hoc solutions that tend to become unstable under high load. Rather than what amounts to essentially tuning parameters of the protocol &quot;on the fly,&quot; we instead propose a new &quot;meta-MAC&quot; protocol framework that implements new dimension of adaptivity, on top of existing MAC protocols. Specifically, we propose research on a method, whose roots are in Artificial Intelligence (A1), to systematically and automatically combine a set of existing protocols into a single MAC protocol in a novel way, such that the resulting combined protocol has provable optimality properties. Each protocol in the set may be a good candidate for certain situations. For example, a randomized contention based protocol is good for low loads, due to its low delay, while an allocation based protocol is desirable for high loads, as it avoids the breakdown induced by too many collisions. Then the meta-protocol will automatically find combined decisions that dynamically represent the &quot;best of the team,&quot; under the actual network conditions, without having to know in advance which of the conditions will actually occur and how they will change. Thus, rather than tuning parameters in an ad hoc manner, we systematically and automatically optimize the medium access approach itself. The proposed research program intends to fully explore the promising potential of the novel meta-MAC protocol aggregation approach, in which encouraging initial results of the PI and co-PI have already shown the principal feasibility. Specifically, the three main research directions include aggregating more sophisticated MAC protocols (such as IEEE 802.11), dynamically altering the protocol mix to support Quality of Service (QoS) at the MAC layer, and an in-depth study of the correctness, stability, and consistency of meta-MAC protocols. Our proposed meta-MAC optimization runs autonomously without any centralized control or any message exchanges. This makes the meta-MAC approach inherently scalable to arbitrarily large networks. Thus, the meta-MAC approach is ideally suited for the evolving application requirements of today&apos;s increasingly heterogeneous networking environments. In addition to the above, we plan to incorporate the general approach into the graduate curriculum in the new Telecommunications Engineering Program at the University of Texas at Dallas, thus enriching the traditional telecommunications curriculum with novel adaptive methodologies that provide intelligent, highly adaptive solutions in large, dynamically changing, heterogrneous networks.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">105985</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">105985</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1157" target="n1158">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: A Low Cost Parallel and Distributed Systems Laboratory</data>
      <data key="e_abstract">EIA-0130806 &lt;br/&gt;Jie Wu&lt;br/&gt;Florida Atlantic University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: A Low Cost Parallel and Distributed Systems Laboratory&lt;br/&gt;&lt;br/&gt;The proposed project includes three parallel applications on a Beowulf cluster. The chief advantage of a Beowulf cluster is its superb price/performance ratio: the proposed cluster will obtain performance in the 3-5 Gigaflop range, for less than one tenth the cost of a comparably powered supercomputer. The low cost parallel and distributed systems laboratory consists of a 16-node Beowulf cluster. Each node is similar to an off-the-shelf PC without a monitor or keyboard. The nodes are networked by 100 Mbit Fast Ethernet lines. Each node will run both Linux and Windows/NT. &lt;br/&gt;&lt;br/&gt;Three parallel applications are proposed: (1) A parallel system for ecological modeling, with its focus on minimizing the simulation time of parallelized ecological models. A central component will be porting NOAA&apos;s NNT-SMS rectilinear parallel modeling package to the Beowulf architecture, (2) A Java runtime framework on Beowulf clusters for parallel execution of multithreaded processes. A new lottery-based job stealing algorithm will be studied for efficient scheduling of large-scale multithreaded computation. (3) Optimal configuration selection for accuracy enhancement of programmable machines. A genetic algorithm solution will be studied to enhance the accuracy of programmable machines. The success of this project will demonstrate the usefulness of Beowulf clusters as a cost-effective alternative to the supercomputer.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130806</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130806</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1157" target="n1159">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: A Low Cost Parallel and Distributed Systems Laboratory</data>
      <data key="e_abstract">EIA-0130806 &lt;br/&gt;Jie Wu&lt;br/&gt;Florida Atlantic University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: A Low Cost Parallel and Distributed Systems Laboratory&lt;br/&gt;&lt;br/&gt;The proposed project includes three parallel applications on a Beowulf cluster. The chief advantage of a Beowulf cluster is its superb price/performance ratio: the proposed cluster will obtain performance in the 3-5 Gigaflop range, for less than one tenth the cost of a comparably powered supercomputer. The low cost parallel and distributed systems laboratory consists of a 16-node Beowulf cluster. Each node is similar to an off-the-shelf PC without a monitor or keyboard. The nodes are networked by 100 Mbit Fast Ethernet lines. Each node will run both Linux and Windows/NT. &lt;br/&gt;&lt;br/&gt;Three parallel applications are proposed: (1) A parallel system for ecological modeling, with its focus on minimizing the simulation time of parallelized ecological models. A central component will be porting NOAA&apos;s NNT-SMS rectilinear parallel modeling package to the Beowulf architecture, (2) A Java runtime framework on Beowulf clusters for parallel execution of multithreaded processes. A new lottery-based job stealing algorithm will be studied for efficient scheduling of large-scale multithreaded computation. (3) Optimal configuration selection for accuracy enhancement of programmable machines. A genetic algorithm solution will be studied to enhance the accuracy of programmable machines. The success of this project will demonstrate the usefulness of Beowulf clusters as a cost-effective alternative to the supercomputer.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130806</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130806</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1158" target="n1159">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: A Low Cost Parallel and Distributed Systems Laboratory</data>
      <data key="e_abstract">EIA-0130806 &lt;br/&gt;Jie Wu&lt;br/&gt;Florida Atlantic University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: A Low Cost Parallel and Distributed Systems Laboratory&lt;br/&gt;&lt;br/&gt;The proposed project includes three parallel applications on a Beowulf cluster. The chief advantage of a Beowulf cluster is its superb price/performance ratio: the proposed cluster will obtain performance in the 3-5 Gigaflop range, for less than one tenth the cost of a comparably powered supercomputer. The low cost parallel and distributed systems laboratory consists of a 16-node Beowulf cluster. Each node is similar to an off-the-shelf PC without a monitor or keyboard. The nodes are networked by 100 Mbit Fast Ethernet lines. Each node will run both Linux and Windows/NT. &lt;br/&gt;&lt;br/&gt;Three parallel applications are proposed: (1) A parallel system for ecological modeling, with its focus on minimizing the simulation time of parallelized ecological models. A central component will be porting NOAA&apos;s NNT-SMS rectilinear parallel modeling package to the Beowulf architecture, (2) A Java runtime framework on Beowulf clusters for parallel execution of multithreaded processes. A new lottery-based job stealing algorithm will be studied for efficient scheduling of large-scale multithreaded computation. (3) Optimal configuration selection for accuracy enhancement of programmable machines. A genetic algorithm solution will be studied to enhance the accuracy of programmable machines. The success of this project will demonstrate the usefulness of Beowulf clusters as a cost-effective alternative to the supercomputer.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130806</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130806</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1148" target="n1161">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Adaptive and Scalable Quality of Service Provisioning in Multirate Multicast Networks</data>
      <data key="e_abstract">Multicasting has become increasingly important for real-time applications since it is the inherent mode of&lt;br/&gt;delivery in several of them, e.g. teleconferencing, audio and video broadcasting, etc. The Internet, however,&lt;br/&gt;does not provide end-to-end bandwidth or delay guarantees for unicast or for multicast data. As a result, a&lt;br/&gt;number of multiple scale encoding schemes for real-time voice and video transmission have been developed;&lt;br/&gt;these schemes allow adaptation both to the available bandwidth and to the receiver capabilities by varying&lt;br/&gt;the number of levels of the information stream and therefore the corresponding bit rates. Thus, bandwidth&lt;br/&gt;and receiver adaptation of multi-level information streams, in multicast delivery in particular, is an issue&lt;br/&gt;of prime importance for the successful, large-scale deployment of those real-time applications. We propose&lt;br/&gt;to investigate and develop methods for bandwidth sharing and adaptation in the context of multicast-&lt;br/&gt;based real-time applications. A set of fairness criteria based on which the performance of different schemes&lt;br/&gt;can be quantified and compared is introduced. A methodology for finding bandwidth allocations that are&lt;br/&gt;optimal or suboptimal with respect to the different criteria is proposed. Scalability with the network size,&lt;br/&gt;amenability to distributed implementation in terms of required control information and speed of convergence&lt;br/&gt;are adopted as prime criteria, in addition to optimality, in the evaluation of the different algorithms. We&lt;br/&gt;propose an approach for the design of the algorithms that allows the trade-off between optimality with&lt;br/&gt;respect to the performance objective and practicality with respect to the other attributes (i.e. scalability&lt;br/&gt;etc.). A methodology for network control in multicasting in the case of unknown traffic and/or network&lt;br/&gt;conditions is proposed as well. It involves explicit back-pressure based congestion notification feedback and&lt;br/&gt;dynamic per ow scheduling. Finally we propose a method for the design of congestion control policies&lt;br/&gt;with the objective of maximizing an aggregate network utility. A relative comparison between the two&lt;br/&gt;approaches (fairness based and aggregate utility maximization) is also proposed in order to be able to select&lt;br/&gt;the most appropriate one in each case.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">106984</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">106984</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n307" target="n1164">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Efficient Integration and QoS Management of Video over Wireless Networks</data>
      <data key="e_abstract">We are currently witnessing an incredible transformation in the way we access information and&lt;br/&gt;computing power. With small handheld devices such as personal digital assistants, cellular phones,&lt;br/&gt;and palmtop computers, users are taking their computers with them more often than not. Fur-&lt;br/&gt;thermore, with the developments in wireless technologies, users are beginning to slowly move away&lt;br/&gt;from their dependence on wired access to their information. While this trend is building significant&lt;br/&gt;momentum, the types of information that are accessed through these devices is still severely lim-&lt;br/&gt;ited by the wireless media. Given the bottle-necks associated with today&apos;s power-limited portable&lt;br/&gt;devices, the limitation due to channel conditions has not yet been felt by the user; however the&lt;br/&gt;strain on the underlying wireless network will only get worse.&lt;br/&gt; The goal of the proposed research is the construction of high performance algorithms for tetherless&lt;br/&gt;access which consider overall system goals through joint optimization across multiple layers. The&lt;br/&gt;objective will be to solve problems which connect the physical layer to the application layer. In the&lt;br/&gt;past, a classical approach to wireless communication system design was undertaken { a modular&lt;br/&gt;approach loosely based on the Open Systems Interconnection reference stack model. For wired&lt;br/&gt;communication networks, such a separation of activities led to functioning systems offering solid&lt;br/&gt;performance. However, the deleterious effects of the wireless communications channel force the&lt;br/&gt;investigation of all possible methods to improve performance. Thus, to provide the desired fidelity&lt;br/&gt;and to transport high-data rate information over the wireless medium, joint optimization across&lt;br/&gt;multiple communication layers will be necessary. To focus the research, wireless transport of video&lt;br/&gt;via Direct Sequence Code-Division Multiple-Access (DS-CDMA) will be considered.&lt;br/&gt; In this project, we propose the research and development of video streaming algorithms that are&lt;br/&gt;more suited for wireless transmission using DS-CDMA as an access methodology. There are several&lt;br/&gt;reasons why this project is necessary. First, current research onmulti-rate detection and estimation&lt;br/&gt;algorithms at the physical layer are not tailored for specific application layer data. Second, many of&lt;br/&gt;the video transmission schemes over wireless networks have assumed fixed bit-error rates that are&lt;br/&gt;introduced by the wireless transmission. In CDMA systems, the bit-error rate is highly dependent&lt;br/&gt;on the channel and the number of active users, both of these processes are time-varying; however,&lt;br/&gt;this variability in bit-error rates has not been fully addressed at the application layer. Third, the&lt;br/&gt;elasticity ofvideo can be exploited to design efficient algorithms which can take advantage of the&lt;br/&gt;shape of video streams as well as time-varying channel conditions to offer desired quality-of-service.&lt;br/&gt; This research project has three principle goals:&lt;br/&gt;&lt;br/&gt;Investigation of efficient multi-rate detection and estimation algorithms for wireless physical&lt;br/&gt;media transmission.&lt;br/&gt;&lt;br/&gt;Investigation of video streaming techniques for wireless networks that use underlying network&lt;br/&gt;condition feedback to dynamically adjust the forward error correction used to protect the&lt;br/&gt;video data.&lt;br/&gt;&lt;br/&gt;Investigation of the integration of physical layer media characteristics and higher-level video&lt;br/&gt;applications to provide the highest quality-of- service to applications.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">87761</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">87761</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1167" target="n1168">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Social Intelligence in Interfaces for Educational Software</data>
      <data key="e_abstract">This project will develop a social intelligence capability for computers, for use in educational software applications. Social intelligence plays a critical role in human tutoring and teaching. By modeling social intelligence in software it should be possible to create educational software that is aware of the learners&apos; attitudes, that interacts harmoniously and responds to learner needs, that is able to develop rapport with learners, and that uses responsiveness and rapport to influence learner motivation. This could be &lt;br/&gt;particularly valuable for learners who have motivational problems such as low self-confidence. The social intelligence model will be realized in an animated pedagogical agent, that uses a combination of speech and nonverbal gestures, that can express emotions and attitudes, and that reacts to and adapts to learners over time. It will respond to a variety of learner inputs, including learner performance on assigned problems, conversational dialog, and eye gaze and facial expressions recognized using computer vision techniques. The model will be applied in the context of the Virtual Factory Teaching System, a simulation-based learning system for teaching industrial engineering &lt;br/&gt;concepts. The effectiveness of the model will be tested by comparing learner performance with and without the assistance of a socially intelligent agent.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121330</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121330</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1167" target="n1169">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Social Intelligence in Interfaces for Educational Software</data>
      <data key="e_abstract">This project will develop a social intelligence capability for computers, for use in educational software applications. Social intelligence plays a critical role in human tutoring and teaching. By modeling social intelligence in software it should be possible to create educational software that is aware of the learners&apos; attitudes, that interacts harmoniously and responds to learner needs, that is able to develop rapport with learners, and that uses responsiveness and rapport to influence learner motivation. This could be &lt;br/&gt;particularly valuable for learners who have motivational problems such as low self-confidence. The social intelligence model will be realized in an animated pedagogical agent, that uses a combination of speech and nonverbal gestures, that can express emotions and attitudes, and that reacts to and adapts to learners over time. It will respond to a variety of learner inputs, including learner performance on assigned problems, conversational dialog, and eye gaze and facial expressions recognized using computer vision techniques. The model will be applied in the context of the Virtual Factory Teaching System, a simulation-based learning system for teaching industrial engineering &lt;br/&gt;concepts. The effectiveness of the model will be tested by comparing learner performance with and without the assistance of a socially intelligent agent.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121330</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121330</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1167" target="n1170">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Social Intelligence in Interfaces for Educational Software</data>
      <data key="e_abstract">This project will develop a social intelligence capability for computers, for use in educational software applications. Social intelligence plays a critical role in human tutoring and teaching. By modeling social intelligence in software it should be possible to create educational software that is aware of the learners&apos; attitudes, that interacts harmoniously and responds to learner needs, that is able to develop rapport with learners, and that uses responsiveness and rapport to influence learner motivation. This could be &lt;br/&gt;particularly valuable for learners who have motivational problems such as low self-confidence. The social intelligence model will be realized in an animated pedagogical agent, that uses a combination of speech and nonverbal gestures, that can express emotions and attitudes, and that reacts to and adapts to learners over time. It will respond to a variety of learner inputs, including learner performance on assigned problems, conversational dialog, and eye gaze and facial expressions recognized using computer vision techniques. The model will be applied in the context of the Virtual Factory Teaching System, a simulation-based learning system for teaching industrial engineering &lt;br/&gt;concepts. The effectiveness of the model will be tested by comparing learner performance with and without the assistance of a socially intelligent agent.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121330</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121330</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1167" target="n1171">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Social Intelligence in Interfaces for Educational Software</data>
      <data key="e_abstract">This project will develop a social intelligence capability for computers, for use in educational software applications. Social intelligence plays a critical role in human tutoring and teaching. By modeling social intelligence in software it should be possible to create educational software that is aware of the learners&apos; attitudes, that interacts harmoniously and responds to learner needs, that is able to develop rapport with learners, and that uses responsiveness and rapport to influence learner motivation. This could be &lt;br/&gt;particularly valuable for learners who have motivational problems such as low self-confidence. The social intelligence model will be realized in an animated pedagogical agent, that uses a combination of speech and nonverbal gestures, that can express emotions and attitudes, and that reacts to and adapts to learners over time. It will respond to a variety of learner inputs, including learner performance on assigned problems, conversational dialog, and eye gaze and facial expressions recognized using computer vision techniques. The model will be applied in the context of the Virtual Factory Teaching System, a simulation-based learning system for teaching industrial engineering &lt;br/&gt;concepts. The effectiveness of the model will be tested by comparing learner performance with and without the assistance of a socially intelligent agent.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121330</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121330</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1168" target="n1169">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Social Intelligence in Interfaces for Educational Software</data>
      <data key="e_abstract">This project will develop a social intelligence capability for computers, for use in educational software applications. Social intelligence plays a critical role in human tutoring and teaching. By modeling social intelligence in software it should be possible to create educational software that is aware of the learners&apos; attitudes, that interacts harmoniously and responds to learner needs, that is able to develop rapport with learners, and that uses responsiveness and rapport to influence learner motivation. This could be &lt;br/&gt;particularly valuable for learners who have motivational problems such as low self-confidence. The social intelligence model will be realized in an animated pedagogical agent, that uses a combination of speech and nonverbal gestures, that can express emotions and attitudes, and that reacts to and adapts to learners over time. It will respond to a variety of learner inputs, including learner performance on assigned problems, conversational dialog, and eye gaze and facial expressions recognized using computer vision techniques. The model will be applied in the context of the Virtual Factory Teaching System, a simulation-based learning system for teaching industrial engineering &lt;br/&gt;concepts. The effectiveness of the model will be tested by comparing learner performance with and without the assistance of a socially intelligent agent.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121330</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121330</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1168" target="n1170">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Social Intelligence in Interfaces for Educational Software</data>
      <data key="e_abstract">This project will develop a social intelligence capability for computers, for use in educational software applications. Social intelligence plays a critical role in human tutoring and teaching. By modeling social intelligence in software it should be possible to create educational software that is aware of the learners&apos; attitudes, that interacts harmoniously and responds to learner needs, that is able to develop rapport with learners, and that uses responsiveness and rapport to influence learner motivation. This could be &lt;br/&gt;particularly valuable for learners who have motivational problems such as low self-confidence. The social intelligence model will be realized in an animated pedagogical agent, that uses a combination of speech and nonverbal gestures, that can express emotions and attitudes, and that reacts to and adapts to learners over time. It will respond to a variety of learner inputs, including learner performance on assigned problems, conversational dialog, and eye gaze and facial expressions recognized using computer vision techniques. The model will be applied in the context of the Virtual Factory Teaching System, a simulation-based learning system for teaching industrial engineering &lt;br/&gt;concepts. The effectiveness of the model will be tested by comparing learner performance with and without the assistance of a socially intelligent agent.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121330</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121330</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1168" target="n1171">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Social Intelligence in Interfaces for Educational Software</data>
      <data key="e_abstract">This project will develop a social intelligence capability for computers, for use in educational software applications. Social intelligence plays a critical role in human tutoring and teaching. By modeling social intelligence in software it should be possible to create educational software that is aware of the learners&apos; attitudes, that interacts harmoniously and responds to learner needs, that is able to develop rapport with learners, and that uses responsiveness and rapport to influence learner motivation. This could be &lt;br/&gt;particularly valuable for learners who have motivational problems such as low self-confidence. The social intelligence model will be realized in an animated pedagogical agent, that uses a combination of speech and nonverbal gestures, that can express emotions and attitudes, and that reacts to and adapts to learners over time. It will respond to a variety of learner inputs, including learner performance on assigned problems, conversational dialog, and eye gaze and facial expressions recognized using computer vision techniques. The model will be applied in the context of the Virtual Factory Teaching System, a simulation-based learning system for teaching industrial engineering &lt;br/&gt;concepts. The effectiveness of the model will be tested by comparing learner performance with and without the assistance of a socially intelligent agent.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121330</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121330</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1169" target="n1170">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Social Intelligence in Interfaces for Educational Software</data>
      <data key="e_abstract">This project will develop a social intelligence capability for computers, for use in educational software applications. Social intelligence plays a critical role in human tutoring and teaching. By modeling social intelligence in software it should be possible to create educational software that is aware of the learners&apos; attitudes, that interacts harmoniously and responds to learner needs, that is able to develop rapport with learners, and that uses responsiveness and rapport to influence learner motivation. This could be &lt;br/&gt;particularly valuable for learners who have motivational problems such as low self-confidence. The social intelligence model will be realized in an animated pedagogical agent, that uses a combination of speech and nonverbal gestures, that can express emotions and attitudes, and that reacts to and adapts to learners over time. It will respond to a variety of learner inputs, including learner performance on assigned problems, conversational dialog, and eye gaze and facial expressions recognized using computer vision techniques. The model will be applied in the context of the Virtual Factory Teaching System, a simulation-based learning system for teaching industrial engineering &lt;br/&gt;concepts. The effectiveness of the model will be tested by comparing learner performance with and without the assistance of a socially intelligent agent.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121330</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121330</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1169" target="n1171">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Social Intelligence in Interfaces for Educational Software</data>
      <data key="e_abstract">This project will develop a social intelligence capability for computers, for use in educational software applications. Social intelligence plays a critical role in human tutoring and teaching. By modeling social intelligence in software it should be possible to create educational software that is aware of the learners&apos; attitudes, that interacts harmoniously and responds to learner needs, that is able to develop rapport with learners, and that uses responsiveness and rapport to influence learner motivation. This could be &lt;br/&gt;particularly valuable for learners who have motivational problems such as low self-confidence. The social intelligence model will be realized in an animated pedagogical agent, that uses a combination of speech and nonverbal gestures, that can express emotions and attitudes, and that reacts to and adapts to learners over time. It will respond to a variety of learner inputs, including learner performance on assigned problems, conversational dialog, and eye gaze and facial expressions recognized using computer vision techniques. The model will be applied in the context of the Virtual Factory Teaching System, a simulation-based learning system for teaching industrial engineering &lt;br/&gt;concepts. The effectiveness of the model will be tested by comparing learner performance with and without the assistance of a socially intelligent agent.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121330</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121330</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1170" target="n1171">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Social Intelligence in Interfaces for Educational Software</data>
      <data key="e_abstract">This project will develop a social intelligence capability for computers, for use in educational software applications. Social intelligence plays a critical role in human tutoring and teaching. By modeling social intelligence in software it should be possible to create educational software that is aware of the learners&apos; attitudes, that interacts harmoniously and responds to learner needs, that is able to develop rapport with learners, and that uses responsiveness and rapport to influence learner motivation. This could be &lt;br/&gt;particularly valuable for learners who have motivational problems such as low self-confidence. The social intelligence model will be realized in an animated pedagogical agent, that uses a combination of speech and nonverbal gestures, that can express emotions and attitudes, and that reacts to and adapts to learners over time. It will respond to a variety of learner inputs, including learner performance on assigned problems, conversational dialog, and eye gaze and facial expressions recognized using computer vision techniques. The model will be applied in the context of the Virtual Factory Teaching System, a simulation-based learning system for teaching industrial engineering &lt;br/&gt;concepts. The effectiveness of the model will be tested by comparing learner performance with and without the assistance of a socially intelligent agent.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121330</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121330</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n291" target="n745">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Interaction and Participation in Integrated Land Use, Transportation, and Environmental Modeling</data>
      <data key="e_abstract">EIA-0121326&lt;br/&gt;Borning, Alan&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;ITR/PE: Interaction and Participation in Integrated Urban Land Use, Transportation, and Environmental Modeling&lt;br/&gt;&lt;br/&gt;Patterns of land use and transportation play a critical role in determining the economic vitality, livability, and sustainability of urban areas. Transportation interacts strongly with land use: different kinds of transportation systems induce different patterns of land use, while at the same time, different kinds of land use induce demands for different kinds of transportation systems. Both have significant environmental effects. This integrated research program will support the construction and deployment of sophisticated models of land use, transportation, and environmental impact. The goal is to provide tools for stakeholders, such as urban planners, government staff, and citizens&apos; groups, to help predict future patterns of urban development under different possible scenarios over periods of twenty or more years, allowing them to make more informed choices. Anticipated scientific advances include: in human-computer interaction, more effective ways of understanding the results from and interacting with complex simulations, and ways of linking stakeholder values with design choices in simulations and their interfaces; in graphics, capabilities for producing simulated street-level animations of urban environments from a policy-driven simulation; and in software engineering, new software structures that allow us to design, integrate, and evolve complex and diverse urban submodels.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121326</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121326</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n417" target="n745">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Interaction and Participation in Integrated Land Use, Transportation, and Environmental Modeling</data>
      <data key="e_abstract">EIA-0121326&lt;br/&gt;Borning, Alan&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;ITR/PE: Interaction and Participation in Integrated Urban Land Use, Transportation, and Environmental Modeling&lt;br/&gt;&lt;br/&gt;Patterns of land use and transportation play a critical role in determining the economic vitality, livability, and sustainability of urban areas. Transportation interacts strongly with land use: different kinds of transportation systems induce different patterns of land use, while at the same time, different kinds of land use induce demands for different kinds of transportation systems. Both have significant environmental effects. This integrated research program will support the construction and deployment of sophisticated models of land use, transportation, and environmental impact. The goal is to provide tools for stakeholders, such as urban planners, government staff, and citizens&apos; groups, to help predict future patterns of urban development under different possible scenarios over periods of twenty or more years, allowing them to make more informed choices. Anticipated scientific advances include: in human-computer interaction, more effective ways of understanding the results from and interacting with complex simulations, and ways of linking stakeholder values with design choices in simulations and their interfaces; in graphics, capabilities for producing simulated street-level animations of urban environments from a policy-driven simulation; and in software engineering, new software structures that allow us to design, integrate, and evolve complex and diverse urban submodels.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121326</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121326</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n745" target="n746">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Interaction and Participation in Integrated Land Use, Transportation, and Environmental Modeling</data>
      <data key="e_abstract">EIA-0121326&lt;br/&gt;Borning, Alan&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;ITR/PE: Interaction and Participation in Integrated Urban Land Use, Transportation, and Environmental Modeling&lt;br/&gt;&lt;br/&gt;Patterns of land use and transportation play a critical role in determining the economic vitality, livability, and sustainability of urban areas. Transportation interacts strongly with land use: different kinds of transportation systems induce different patterns of land use, while at the same time, different kinds of land use induce demands for different kinds of transportation systems. Both have significant environmental effects. This integrated research program will support the construction and deployment of sophisticated models of land use, transportation, and environmental impact. The goal is to provide tools for stakeholders, such as urban planners, government staff, and citizens&apos; groups, to help predict future patterns of urban development under different possible scenarios over periods of twenty or more years, allowing them to make more informed choices. Anticipated scientific advances include: in human-computer interaction, more effective ways of understanding the results from and interacting with complex simulations, and ways of linking stakeholder values with design choices in simulations and their interfaces; in graphics, capabilities for producing simulated street-level animations of urban environments from a policy-driven simulation; and in software engineering, new software structures that allow us to design, integrate, and evolve complex and diverse urban submodels.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121326</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121326</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n745" target="n747">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Interaction and Participation in Integrated Land Use, Transportation, and Environmental Modeling</data>
      <data key="e_abstract">EIA-0121326&lt;br/&gt;Borning, Alan&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;ITR/PE: Interaction and Participation in Integrated Urban Land Use, Transportation, and Environmental Modeling&lt;br/&gt;&lt;br/&gt;Patterns of land use and transportation play a critical role in determining the economic vitality, livability, and sustainability of urban areas. Transportation interacts strongly with land use: different kinds of transportation systems induce different patterns of land use, while at the same time, different kinds of land use induce demands for different kinds of transportation systems. Both have significant environmental effects. This integrated research program will support the construction and deployment of sophisticated models of land use, transportation, and environmental impact. The goal is to provide tools for stakeholders, such as urban planners, government staff, and citizens&apos; groups, to help predict future patterns of urban development under different possible scenarios over periods of twenty or more years, allowing them to make more informed choices. Anticipated scientific advances include: in human-computer interaction, more effective ways of understanding the results from and interacting with complex simulations, and ways of linking stakeholder values with design choices in simulations and their interfaces; in graphics, capabilities for producing simulated street-level animations of urban environments from a policy-driven simulation; and in software engineering, new software structures that allow us to design, integrate, and evolve complex and diverse urban submodels.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121326</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121326</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n291" target="n417">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Interaction and Participation in Integrated Land Use, Transportation, and Environmental Modeling</data>
      <data key="e_abstract">EIA-0121326&lt;br/&gt;Borning, Alan&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;ITR/PE: Interaction and Participation in Integrated Urban Land Use, Transportation, and Environmental Modeling&lt;br/&gt;&lt;br/&gt;Patterns of land use and transportation play a critical role in determining the economic vitality, livability, and sustainability of urban areas. Transportation interacts strongly with land use: different kinds of transportation systems induce different patterns of land use, while at the same time, different kinds of land use induce demands for different kinds of transportation systems. Both have significant environmental effects. This integrated research program will support the construction and deployment of sophisticated models of land use, transportation, and environmental impact. The goal is to provide tools for stakeholders, such as urban planners, government staff, and citizens&apos; groups, to help predict future patterns of urban development under different possible scenarios over periods of twenty or more years, allowing them to make more informed choices. Anticipated scientific advances include: in human-computer interaction, more effective ways of understanding the results from and interacting with complex simulations, and ways of linking stakeholder values with design choices in simulations and their interfaces; in graphics, capabilities for producing simulated street-level animations of urban environments from a policy-driven simulation; and in software engineering, new software structures that allow us to design, integrate, and evolve complex and diverse urban submodels.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121326</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121326</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n291" target="n746">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Interaction and Participation in Integrated Land Use, Transportation, and Environmental Modeling</data>
      <data key="e_abstract">EIA-0121326&lt;br/&gt;Borning, Alan&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;ITR/PE: Interaction and Participation in Integrated Urban Land Use, Transportation, and Environmental Modeling&lt;br/&gt;&lt;br/&gt;Patterns of land use and transportation play a critical role in determining the economic vitality, livability, and sustainability of urban areas. Transportation interacts strongly with land use: different kinds of transportation systems induce different patterns of land use, while at the same time, different kinds of land use induce demands for different kinds of transportation systems. Both have significant environmental effects. This integrated research program will support the construction and deployment of sophisticated models of land use, transportation, and environmental impact. The goal is to provide tools for stakeholders, such as urban planners, government staff, and citizens&apos; groups, to help predict future patterns of urban development under different possible scenarios over periods of twenty or more years, allowing them to make more informed choices. Anticipated scientific advances include: in human-computer interaction, more effective ways of understanding the results from and interacting with complex simulations, and ways of linking stakeholder values with design choices in simulations and their interfaces; in graphics, capabilities for producing simulated street-level animations of urban environments from a policy-driven simulation; and in software engineering, new software structures that allow us to design, integrate, and evolve complex and diverse urban submodels.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121326</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121326</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n291" target="n747">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Interaction and Participation in Integrated Land Use, Transportation, and Environmental Modeling</data>
      <data key="e_abstract">EIA-0121326&lt;br/&gt;Borning, Alan&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;ITR/PE: Interaction and Participation in Integrated Urban Land Use, Transportation, and Environmental Modeling&lt;br/&gt;&lt;br/&gt;Patterns of land use and transportation play a critical role in determining the economic vitality, livability, and sustainability of urban areas. Transportation interacts strongly with land use: different kinds of transportation systems induce different patterns of land use, while at the same time, different kinds of land use induce demands for different kinds of transportation systems. Both have significant environmental effects. This integrated research program will support the construction and deployment of sophisticated models of land use, transportation, and environmental impact. The goal is to provide tools for stakeholders, such as urban planners, government staff, and citizens&apos; groups, to help predict future patterns of urban development under different possible scenarios over periods of twenty or more years, allowing them to make more informed choices. Anticipated scientific advances include: in human-computer interaction, more effective ways of understanding the results from and interacting with complex simulations, and ways of linking stakeholder values with design choices in simulations and their interfaces; in graphics, capabilities for producing simulated street-level animations of urban environments from a policy-driven simulation; and in software engineering, new software structures that allow us to design, integrate, and evolve complex and diverse urban submodels.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121326</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121326</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n417" target="n746">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Interaction and Participation in Integrated Land Use, Transportation, and Environmental Modeling</data>
      <data key="e_abstract">EIA-0121326&lt;br/&gt;Borning, Alan&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;ITR/PE: Interaction and Participation in Integrated Urban Land Use, Transportation, and Environmental Modeling&lt;br/&gt;&lt;br/&gt;Patterns of land use and transportation play a critical role in determining the economic vitality, livability, and sustainability of urban areas. Transportation interacts strongly with land use: different kinds of transportation systems induce different patterns of land use, while at the same time, different kinds of land use induce demands for different kinds of transportation systems. Both have significant environmental effects. This integrated research program will support the construction and deployment of sophisticated models of land use, transportation, and environmental impact. The goal is to provide tools for stakeholders, such as urban planners, government staff, and citizens&apos; groups, to help predict future patterns of urban development under different possible scenarios over periods of twenty or more years, allowing them to make more informed choices. Anticipated scientific advances include: in human-computer interaction, more effective ways of understanding the results from and interacting with complex simulations, and ways of linking stakeholder values with design choices in simulations and their interfaces; in graphics, capabilities for producing simulated street-level animations of urban environments from a policy-driven simulation; and in software engineering, new software structures that allow us to design, integrate, and evolve complex and diverse urban submodels.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121326</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121326</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n417" target="n747">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Interaction and Participation in Integrated Land Use, Transportation, and Environmental Modeling</data>
      <data key="e_abstract">EIA-0121326&lt;br/&gt;Borning, Alan&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;ITR/PE: Interaction and Participation in Integrated Urban Land Use, Transportation, and Environmental Modeling&lt;br/&gt;&lt;br/&gt;Patterns of land use and transportation play a critical role in determining the economic vitality, livability, and sustainability of urban areas. Transportation interacts strongly with land use: different kinds of transportation systems induce different patterns of land use, while at the same time, different kinds of land use induce demands for different kinds of transportation systems. Both have significant environmental effects. This integrated research program will support the construction and deployment of sophisticated models of land use, transportation, and environmental impact. The goal is to provide tools for stakeholders, such as urban planners, government staff, and citizens&apos; groups, to help predict future patterns of urban development under different possible scenarios over periods of twenty or more years, allowing them to make more informed choices. Anticipated scientific advances include: in human-computer interaction, more effective ways of understanding the results from and interacting with complex simulations, and ways of linking stakeholder values with design choices in simulations and their interfaces; in graphics, capabilities for producing simulated street-level animations of urban environments from a policy-driven simulation; and in software engineering, new software structures that allow us to design, integrate, and evolve complex and diverse urban submodels.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121326</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121326</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n746" target="n747">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Interaction and Participation in Integrated Land Use, Transportation, and Environmental Modeling</data>
      <data key="e_abstract">EIA-0121326&lt;br/&gt;Borning, Alan&lt;br/&gt;University of Washington&lt;br/&gt;&lt;br/&gt;ITR/PE: Interaction and Participation in Integrated Urban Land Use, Transportation, and Environmental Modeling&lt;br/&gt;&lt;br/&gt;Patterns of land use and transportation play a critical role in determining the economic vitality, livability, and sustainability of urban areas. Transportation interacts strongly with land use: different kinds of transportation systems induce different patterns of land use, while at the same time, different kinds of land use induce demands for different kinds of transportation systems. Both have significant environmental effects. This integrated research program will support the construction and deployment of sophisticated models of land use, transportation, and environmental impact. The goal is to provide tools for stakeholders, such as urban planners, government staff, and citizens&apos; groups, to help predict future patterns of urban development under different possible scenarios over periods of twenty or more years, allowing them to make more informed choices. Anticipated scientific advances include: in human-computer interaction, more effective ways of understanding the results from and interacting with complex simulations, and ways of linking stakeholder values with design choices in simulations and their interfaces; in graphics, capabilities for producing simulated street-level animations of urban environments from a policy-driven simulation; and in software engineering, new software structures that allow us to design, integrate, and evolve complex and diverse urban submodels.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121326</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121326</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1178" target="n1179">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Order, Spacing, and Clustering in Visual Exploration of Large Scale Data</data>
      <data key="e_abstract">The focus of this research project is to develop visualization, interaction, and data management technologies to address the problems of high dimensionality and data type heterogeneity in large-scale visual data mining. The basic approach is to apply multiresolution clustering strategies across the dimensions of a data set as well as within individual dimensions containing nominal or categorical values, and exploit the ordering and positioning of data axes and data points to emphasize relationships within the data. For visualization, the tasks involve the development of methods for determining ordering and variable spacing within and between data and dimensions as well as clustering of dimensions into multi-resolution abstractions, and integrating them into several existing multivariate display techniques. For interaction, tools for intuitive navigation and exploration within the multiresolution spaces are developed. This includes interactive reclustering tools to allow users to guide the process of splitting and grouping clusters of data objects and dimensions. For data management the tasks involve the development of high-dimensional indexing and multi-resolution data view management for high-dimensional data access, and caching and prefetching strategies to support real-time visual exploration. The ease of use as well as performance of the display and interactive tools over large data sets is assessed. The results of this research will provide data analysts in domains such as bioinformatics, earth and space sciences, and e-commerce the ability to interactively explore the increasingly large and complex data sets being generated.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">119276</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">119276</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1180" target="n1181">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY: Electronic Books for the Tele-immersion Age: A New Paradigm for Teaching Surgical Procedures</data>
      <data key="e_abstract">Tele-immersion will provide a dramatic new medium for groups of people remote from each other to work and share experiences together in an immersive 3D virtual environment, much as if they were co-located in a shared physical space. Immersive electronic books that in effect blend a &quot;time machine&quot; with 3D hypermedia, will add an additional important dimension, that of being able to record experiences in which a viewer, immersed in the 3D reconstruction, can literally walk through the scene or move backward and forward in time. While there are many potential application areas for such novel technologies (e.g., design and virtual prototyping, maintenance and repair, paleontological and archaeological reconstruction), the focus here will be on a societally important and technologically challenging driving application, teaching surgical management of difficult, potentially lethal, injuries. Today, the pace of surgical innovations has increased dramatically, yet the mechanisms for training and re-training suffer from inflexible timing, extended time commitments, and limited content. Traditional videotaped instruction has long been available to help surgeons learn new procedures, but this approach is only marginally effective due to the fixed point of view that is integral to the narration, lack of depth perception and interactivity, and missing information; in short, the experience of watching a video is not sufficiently close to being there and seeing the procedure. In this project the PI will develop a new paradigm for teaching surgical procedures that allows surgeons to witness and explore (in time and space) a past surgical procedure as if they were there, with the added benefit of instruction from the original surgeon or another instructor, as well as integrated 3D illustrations, annotations, and relevant medical metadata. The trainees should be able to freely and naturally walk around a life-sized, high-fidelity, 3D graphical reconstruction of the original time-varying events, pausing or stepping forward and backward in time to satisfy curiosity or allay confusion. To make this reality, the PI and his team bring together experts in several disciplines, and will be able to collectively leverage their prior work in tele-immersion, time-varying 3D scene capture, interaction metaphors, &quot;cinematic&quot; techniques. and authoring tools.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121657</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121657</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1182" target="n1183">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: On A Virtual Tribology System: Future Engineers and Future Powertrain Virtualization Technology</data>
      <data key="e_abstract">This IGERT award supports the establishment of a multidisciplinary graduate training program of education and research in the development of a virtual tribology system for future powertrains, which are the power-delivery systems of automobiles and aircraft. The current development of a powertrain tribological system is time-consuming, requiring integration of new materials, engine technologies, trial-and-error, laboratory experimentation and extensive field-testing. The vision is to shorten the development time by developing a virtual powertrain through advanced computer modeling that simulates the interfacial interactions among critical machine elements. The development of such technology requires new engineers and scientists with cross-disciplinary training. The traditional engineering Ph.D. training model based on one advisor and a single topic does not work well in this new paradigm. Creating Ph.D.s who are educated in a multidisciplinary environment represents the educational focus of this IGERT program. Because of the need for multiscale modeling and multidisciplinary research, an aggressive education plan requiring the creation of a multidisciplinary learning/research environment, electronic education, industrial collaboration, international outreach and faculty re-education will be an integral part of the current program. This program is a joint effort among faculty members in chemistry, chemical engineering, civil engineering, computer engineering, materials science and mechanical engineering, as well as physics. With the successful development of virtualization technology, it is hoped to educate a new generation of engineers and scientists who have strong technical skill and are proficient in multidisciplinary collaboration and in working with computer simulations of complex systems.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114429</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114429</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1182" target="n1184">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: On A Virtual Tribology System: Future Engineers and Future Powertrain Virtualization Technology</data>
      <data key="e_abstract">This IGERT award supports the establishment of a multidisciplinary graduate training program of education and research in the development of a virtual tribology system for future powertrains, which are the power-delivery systems of automobiles and aircraft. The current development of a powertrain tribological system is time-consuming, requiring integration of new materials, engine technologies, trial-and-error, laboratory experimentation and extensive field-testing. The vision is to shorten the development time by developing a virtual powertrain through advanced computer modeling that simulates the interfacial interactions among critical machine elements. The development of such technology requires new engineers and scientists with cross-disciplinary training. The traditional engineering Ph.D. training model based on one advisor and a single topic does not work well in this new paradigm. Creating Ph.D.s who are educated in a multidisciplinary environment represents the educational focus of this IGERT program. Because of the need for multiscale modeling and multidisciplinary research, an aggressive education plan requiring the creation of a multidisciplinary learning/research environment, electronic education, industrial collaboration, international outreach and faculty re-education will be an integral part of the current program. This program is a joint effort among faculty members in chemistry, chemical engineering, civil engineering, computer engineering, materials science and mechanical engineering, as well as physics. With the successful development of virtualization technology, it is hoped to educate a new generation of engineers and scientists who have strong technical skill and are proficient in multidisciplinary collaboration and in working with computer simulations of complex systems.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114429</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114429</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1182" target="n1185">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: On A Virtual Tribology System: Future Engineers and Future Powertrain Virtualization Technology</data>
      <data key="e_abstract">This IGERT award supports the establishment of a multidisciplinary graduate training program of education and research in the development of a virtual tribology system for future powertrains, which are the power-delivery systems of automobiles and aircraft. The current development of a powertrain tribological system is time-consuming, requiring integration of new materials, engine technologies, trial-and-error, laboratory experimentation and extensive field-testing. The vision is to shorten the development time by developing a virtual powertrain through advanced computer modeling that simulates the interfacial interactions among critical machine elements. The development of such technology requires new engineers and scientists with cross-disciplinary training. The traditional engineering Ph.D. training model based on one advisor and a single topic does not work well in this new paradigm. Creating Ph.D.s who are educated in a multidisciplinary environment represents the educational focus of this IGERT program. Because of the need for multiscale modeling and multidisciplinary research, an aggressive education plan requiring the creation of a multidisciplinary learning/research environment, electronic education, industrial collaboration, international outreach and faculty re-education will be an integral part of the current program. This program is a joint effort among faculty members in chemistry, chemical engineering, civil engineering, computer engineering, materials science and mechanical engineering, as well as physics. With the successful development of virtualization technology, it is hoped to educate a new generation of engineers and scientists who have strong technical skill and are proficient in multidisciplinary collaboration and in working with computer simulations of complex systems.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114429</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114429</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1182" target="n1186">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: On A Virtual Tribology System: Future Engineers and Future Powertrain Virtualization Technology</data>
      <data key="e_abstract">This IGERT award supports the establishment of a multidisciplinary graduate training program of education and research in the development of a virtual tribology system for future powertrains, which are the power-delivery systems of automobiles and aircraft. The current development of a powertrain tribological system is time-consuming, requiring integration of new materials, engine technologies, trial-and-error, laboratory experimentation and extensive field-testing. The vision is to shorten the development time by developing a virtual powertrain through advanced computer modeling that simulates the interfacial interactions among critical machine elements. The development of such technology requires new engineers and scientists with cross-disciplinary training. The traditional engineering Ph.D. training model based on one advisor and a single topic does not work well in this new paradigm. Creating Ph.D.s who are educated in a multidisciplinary environment represents the educational focus of this IGERT program. Because of the need for multiscale modeling and multidisciplinary research, an aggressive education plan requiring the creation of a multidisciplinary learning/research environment, electronic education, industrial collaboration, international outreach and faculty re-education will be an integral part of the current program. This program is a joint effort among faculty members in chemistry, chemical engineering, civil engineering, computer engineering, materials science and mechanical engineering, as well as physics. With the successful development of virtualization technology, it is hoped to educate a new generation of engineers and scientists who have strong technical skill and are proficient in multidisciplinary collaboration and in working with computer simulations of complex systems.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114429</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114429</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1182" target="n1187">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: On A Virtual Tribology System: Future Engineers and Future Powertrain Virtualization Technology</data>
      <data key="e_abstract">This IGERT award supports the establishment of a multidisciplinary graduate training program of education and research in the development of a virtual tribology system for future powertrains, which are the power-delivery systems of automobiles and aircraft. The current development of a powertrain tribological system is time-consuming, requiring integration of new materials, engine technologies, trial-and-error, laboratory experimentation and extensive field-testing. The vision is to shorten the development time by developing a virtual powertrain through advanced computer modeling that simulates the interfacial interactions among critical machine elements. The development of such technology requires new engineers and scientists with cross-disciplinary training. The traditional engineering Ph.D. training model based on one advisor and a single topic does not work well in this new paradigm. Creating Ph.D.s who are educated in a multidisciplinary environment represents the educational focus of this IGERT program. Because of the need for multiscale modeling and multidisciplinary research, an aggressive education plan requiring the creation of a multidisciplinary learning/research environment, electronic education, industrial collaboration, international outreach and faculty re-education will be an integral part of the current program. This program is a joint effort among faculty members in chemistry, chemical engineering, civil engineering, computer engineering, materials science and mechanical engineering, as well as physics. With the successful development of virtualization technology, it is hoped to educate a new generation of engineers and scientists who have strong technical skill and are proficient in multidisciplinary collaboration and in working with computer simulations of complex systems.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114429</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114429</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1183" target="n1184">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: On A Virtual Tribology System: Future Engineers and Future Powertrain Virtualization Technology</data>
      <data key="e_abstract">This IGERT award supports the establishment of a multidisciplinary graduate training program of education and research in the development of a virtual tribology system for future powertrains, which are the power-delivery systems of automobiles and aircraft. The current development of a powertrain tribological system is time-consuming, requiring integration of new materials, engine technologies, trial-and-error, laboratory experimentation and extensive field-testing. The vision is to shorten the development time by developing a virtual powertrain through advanced computer modeling that simulates the interfacial interactions among critical machine elements. The development of such technology requires new engineers and scientists with cross-disciplinary training. The traditional engineering Ph.D. training model based on one advisor and a single topic does not work well in this new paradigm. Creating Ph.D.s who are educated in a multidisciplinary environment represents the educational focus of this IGERT program. Because of the need for multiscale modeling and multidisciplinary research, an aggressive education plan requiring the creation of a multidisciplinary learning/research environment, electronic education, industrial collaboration, international outreach and faculty re-education will be an integral part of the current program. This program is a joint effort among faculty members in chemistry, chemical engineering, civil engineering, computer engineering, materials science and mechanical engineering, as well as physics. With the successful development of virtualization technology, it is hoped to educate a new generation of engineers and scientists who have strong technical skill and are proficient in multidisciplinary collaboration and in working with computer simulations of complex systems.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114429</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114429</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1183" target="n1185">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: On A Virtual Tribology System: Future Engineers and Future Powertrain Virtualization Technology</data>
      <data key="e_abstract">This IGERT award supports the establishment of a multidisciplinary graduate training program of education and research in the development of a virtual tribology system for future powertrains, which are the power-delivery systems of automobiles and aircraft. The current development of a powertrain tribological system is time-consuming, requiring integration of new materials, engine technologies, trial-and-error, laboratory experimentation and extensive field-testing. The vision is to shorten the development time by developing a virtual powertrain through advanced computer modeling that simulates the interfacial interactions among critical machine elements. The development of such technology requires new engineers and scientists with cross-disciplinary training. The traditional engineering Ph.D. training model based on one advisor and a single topic does not work well in this new paradigm. Creating Ph.D.s who are educated in a multidisciplinary environment represents the educational focus of this IGERT program. Because of the need for multiscale modeling and multidisciplinary research, an aggressive education plan requiring the creation of a multidisciplinary learning/research environment, electronic education, industrial collaboration, international outreach and faculty re-education will be an integral part of the current program. This program is a joint effort among faculty members in chemistry, chemical engineering, civil engineering, computer engineering, materials science and mechanical engineering, as well as physics. With the successful development of virtualization technology, it is hoped to educate a new generation of engineers and scientists who have strong technical skill and are proficient in multidisciplinary collaboration and in working with computer simulations of complex systems.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114429</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114429</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1183" target="n1186">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: On A Virtual Tribology System: Future Engineers and Future Powertrain Virtualization Technology</data>
      <data key="e_abstract">This IGERT award supports the establishment of a multidisciplinary graduate training program of education and research in the development of a virtual tribology system for future powertrains, which are the power-delivery systems of automobiles and aircraft. The current development of a powertrain tribological system is time-consuming, requiring integration of new materials, engine technologies, trial-and-error, laboratory experimentation and extensive field-testing. The vision is to shorten the development time by developing a virtual powertrain through advanced computer modeling that simulates the interfacial interactions among critical machine elements. The development of such technology requires new engineers and scientists with cross-disciplinary training. The traditional engineering Ph.D. training model based on one advisor and a single topic does not work well in this new paradigm. Creating Ph.D.s who are educated in a multidisciplinary environment represents the educational focus of this IGERT program. Because of the need for multiscale modeling and multidisciplinary research, an aggressive education plan requiring the creation of a multidisciplinary learning/research environment, electronic education, industrial collaboration, international outreach and faculty re-education will be an integral part of the current program. This program is a joint effort among faculty members in chemistry, chemical engineering, civil engineering, computer engineering, materials science and mechanical engineering, as well as physics. With the successful development of virtualization technology, it is hoped to educate a new generation of engineers and scientists who have strong technical skill and are proficient in multidisciplinary collaboration and in working with computer simulations of complex systems.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114429</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114429</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1183" target="n1187">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: On A Virtual Tribology System: Future Engineers and Future Powertrain Virtualization Technology</data>
      <data key="e_abstract">This IGERT award supports the establishment of a multidisciplinary graduate training program of education and research in the development of a virtual tribology system for future powertrains, which are the power-delivery systems of automobiles and aircraft. The current development of a powertrain tribological system is time-consuming, requiring integration of new materials, engine technologies, trial-and-error, laboratory experimentation and extensive field-testing. The vision is to shorten the development time by developing a virtual powertrain through advanced computer modeling that simulates the interfacial interactions among critical machine elements. The development of such technology requires new engineers and scientists with cross-disciplinary training. The traditional engineering Ph.D. training model based on one advisor and a single topic does not work well in this new paradigm. Creating Ph.D.s who are educated in a multidisciplinary environment represents the educational focus of this IGERT program. Because of the need for multiscale modeling and multidisciplinary research, an aggressive education plan requiring the creation of a multidisciplinary learning/research environment, electronic education, industrial collaboration, international outreach and faculty re-education will be an integral part of the current program. This program is a joint effort among faculty members in chemistry, chemical engineering, civil engineering, computer engineering, materials science and mechanical engineering, as well as physics. With the successful development of virtualization technology, it is hoped to educate a new generation of engineers and scientists who have strong technical skill and are proficient in multidisciplinary collaboration and in working with computer simulations of complex systems.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114429</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114429</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1184" target="n1185">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: On A Virtual Tribology System: Future Engineers and Future Powertrain Virtualization Technology</data>
      <data key="e_abstract">This IGERT award supports the establishment of a multidisciplinary graduate training program of education and research in the development of a virtual tribology system for future powertrains, which are the power-delivery systems of automobiles and aircraft. The current development of a powertrain tribological system is time-consuming, requiring integration of new materials, engine technologies, trial-and-error, laboratory experimentation and extensive field-testing. The vision is to shorten the development time by developing a virtual powertrain through advanced computer modeling that simulates the interfacial interactions among critical machine elements. The development of such technology requires new engineers and scientists with cross-disciplinary training. The traditional engineering Ph.D. training model based on one advisor and a single topic does not work well in this new paradigm. Creating Ph.D.s who are educated in a multidisciplinary environment represents the educational focus of this IGERT program. Because of the need for multiscale modeling and multidisciplinary research, an aggressive education plan requiring the creation of a multidisciplinary learning/research environment, electronic education, industrial collaboration, international outreach and faculty re-education will be an integral part of the current program. This program is a joint effort among faculty members in chemistry, chemical engineering, civil engineering, computer engineering, materials science and mechanical engineering, as well as physics. With the successful development of virtualization technology, it is hoped to educate a new generation of engineers and scientists who have strong technical skill and are proficient in multidisciplinary collaboration and in working with computer simulations of complex systems.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114429</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114429</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1184" target="n1186">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: On A Virtual Tribology System: Future Engineers and Future Powertrain Virtualization Technology</data>
      <data key="e_abstract">This IGERT award supports the establishment of a multidisciplinary graduate training program of education and research in the development of a virtual tribology system for future powertrains, which are the power-delivery systems of automobiles and aircraft. The current development of a powertrain tribological system is time-consuming, requiring integration of new materials, engine technologies, trial-and-error, laboratory experimentation and extensive field-testing. The vision is to shorten the development time by developing a virtual powertrain through advanced computer modeling that simulates the interfacial interactions among critical machine elements. The development of such technology requires new engineers and scientists with cross-disciplinary training. The traditional engineering Ph.D. training model based on one advisor and a single topic does not work well in this new paradigm. Creating Ph.D.s who are educated in a multidisciplinary environment represents the educational focus of this IGERT program. Because of the need for multiscale modeling and multidisciplinary research, an aggressive education plan requiring the creation of a multidisciplinary learning/research environment, electronic education, industrial collaboration, international outreach and faculty re-education will be an integral part of the current program. This program is a joint effort among faculty members in chemistry, chemical engineering, civil engineering, computer engineering, materials science and mechanical engineering, as well as physics. With the successful development of virtualization technology, it is hoped to educate a new generation of engineers and scientists who have strong technical skill and are proficient in multidisciplinary collaboration and in working with computer simulations of complex systems.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114429</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114429</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1184" target="n1187">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: On A Virtual Tribology System: Future Engineers and Future Powertrain Virtualization Technology</data>
      <data key="e_abstract">This IGERT award supports the establishment of a multidisciplinary graduate training program of education and research in the development of a virtual tribology system for future powertrains, which are the power-delivery systems of automobiles and aircraft. The current development of a powertrain tribological system is time-consuming, requiring integration of new materials, engine technologies, trial-and-error, laboratory experimentation and extensive field-testing. The vision is to shorten the development time by developing a virtual powertrain through advanced computer modeling that simulates the interfacial interactions among critical machine elements. The development of such technology requires new engineers and scientists with cross-disciplinary training. The traditional engineering Ph.D. training model based on one advisor and a single topic does not work well in this new paradigm. Creating Ph.D.s who are educated in a multidisciplinary environment represents the educational focus of this IGERT program. Because of the need for multiscale modeling and multidisciplinary research, an aggressive education plan requiring the creation of a multidisciplinary learning/research environment, electronic education, industrial collaboration, international outreach and faculty re-education will be an integral part of the current program. This program is a joint effort among faculty members in chemistry, chemical engineering, civil engineering, computer engineering, materials science and mechanical engineering, as well as physics. With the successful development of virtualization technology, it is hoped to educate a new generation of engineers and scientists who have strong technical skill and are proficient in multidisciplinary collaboration and in working with computer simulations of complex systems.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114429</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114429</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1185" target="n1186">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: On A Virtual Tribology System: Future Engineers and Future Powertrain Virtualization Technology</data>
      <data key="e_abstract">This IGERT award supports the establishment of a multidisciplinary graduate training program of education and research in the development of a virtual tribology system for future powertrains, which are the power-delivery systems of automobiles and aircraft. The current development of a powertrain tribological system is time-consuming, requiring integration of new materials, engine technologies, trial-and-error, laboratory experimentation and extensive field-testing. The vision is to shorten the development time by developing a virtual powertrain through advanced computer modeling that simulates the interfacial interactions among critical machine elements. The development of such technology requires new engineers and scientists with cross-disciplinary training. The traditional engineering Ph.D. training model based on one advisor and a single topic does not work well in this new paradigm. Creating Ph.D.s who are educated in a multidisciplinary environment represents the educational focus of this IGERT program. Because of the need for multiscale modeling and multidisciplinary research, an aggressive education plan requiring the creation of a multidisciplinary learning/research environment, electronic education, industrial collaboration, international outreach and faculty re-education will be an integral part of the current program. This program is a joint effort among faculty members in chemistry, chemical engineering, civil engineering, computer engineering, materials science and mechanical engineering, as well as physics. With the successful development of virtualization technology, it is hoped to educate a new generation of engineers and scientists who have strong technical skill and are proficient in multidisciplinary collaboration and in working with computer simulations of complex systems.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114429</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114429</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1185" target="n1187">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: On A Virtual Tribology System: Future Engineers and Future Powertrain Virtualization Technology</data>
      <data key="e_abstract">This IGERT award supports the establishment of a multidisciplinary graduate training program of education and research in the development of a virtual tribology system for future powertrains, which are the power-delivery systems of automobiles and aircraft. The current development of a powertrain tribological system is time-consuming, requiring integration of new materials, engine technologies, trial-and-error, laboratory experimentation and extensive field-testing. The vision is to shorten the development time by developing a virtual powertrain through advanced computer modeling that simulates the interfacial interactions among critical machine elements. The development of such technology requires new engineers and scientists with cross-disciplinary training. The traditional engineering Ph.D. training model based on one advisor and a single topic does not work well in this new paradigm. Creating Ph.D.s who are educated in a multidisciplinary environment represents the educational focus of this IGERT program. Because of the need for multiscale modeling and multidisciplinary research, an aggressive education plan requiring the creation of a multidisciplinary learning/research environment, electronic education, industrial collaboration, international outreach and faculty re-education will be an integral part of the current program. This program is a joint effort among faculty members in chemistry, chemical engineering, civil engineering, computer engineering, materials science and mechanical engineering, as well as physics. With the successful development of virtualization technology, it is hoped to educate a new generation of engineers and scientists who have strong technical skill and are proficient in multidisciplinary collaboration and in working with computer simulations of complex systems.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114429</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114429</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1186" target="n1187">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: On A Virtual Tribology System: Future Engineers and Future Powertrain Virtualization Technology</data>
      <data key="e_abstract">This IGERT award supports the establishment of a multidisciplinary graduate training program of education and research in the development of a virtual tribology system for future powertrains, which are the power-delivery systems of automobiles and aircraft. The current development of a powertrain tribological system is time-consuming, requiring integration of new materials, engine technologies, trial-and-error, laboratory experimentation and extensive field-testing. The vision is to shorten the development time by developing a virtual powertrain through advanced computer modeling that simulates the interfacial interactions among critical machine elements. The development of such technology requires new engineers and scientists with cross-disciplinary training. The traditional engineering Ph.D. training model based on one advisor and a single topic does not work well in this new paradigm. Creating Ph.D.s who are educated in a multidisciplinary environment represents the educational focus of this IGERT program. Because of the need for multiscale modeling and multidisciplinary research, an aggressive education plan requiring the creation of a multidisciplinary learning/research environment, electronic education, industrial collaboration, international outreach and faculty re-education will be an integral part of the current program. This program is a joint effort among faculty members in chemistry, chemical engineering, civil engineering, computer engineering, materials science and mechanical engineering, as well as physics. With the successful development of virtualization technology, it is hoped to educate a new generation of engineers and scientists who have strong technical skill and are proficient in multidisciplinary collaboration and in working with computer simulations of complex systems.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114429</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114429</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1189" target="n1190">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+SY Digital Clay for Shape Input and Display</data>
      <data key="e_abstract">Shape is a key element in successful communication, interpretation, and understanding of complex data in virtually every area of engineering, art, science, and medicine. While in recent years the communication of both form and complex data have been greatly enhanced by visualization that is based on planar images, computational power has reached the point where it is possible to consider real-time interactive 3D physical communication. In this project, the PI will develop a novel interactive 2D or 3D haptic computer interface that enables both user-specified display of shapes as output from a computer, and user-directed input of shapes to a computer. This so-called &quot;digital clay&quot; will allow users to convey and/or sense multiple-element, parallel information strands. It is a distributed input/display device, the surface of which can be shaped by a user and acquired by a computer; alternatively, the clay can be shaped by the computer for the user to examine. Like ordinary clay, digital clay will allow an area of moderate size to be touched, reshaped with pressure, and seen by the user in true 3D form. Unlike ordinary clay, digital clay also provides parameters to the computer that will represent the shape to the computer for further analysis, storage, replication, communication and/or modification; or, will allow the computer to prescribe its shape. This combined input and output feature of the clay enables two-way communication between the computer and the user. Some previous implementations of digital-clay-like devices have focused on reshaping of non-physical volumes of &apos;virtual clay&apos; using glove-like or haptic manipulator interfaces to a computer in which the virtual clay is stored. The PI&apos;s approach is different; digital clay comprises an instrumented, actuated, computer-interfaced physical volume bounded by an actuatable surface that acts as the haptic interface. This surface is displaced by rows or arrays of controllable interconnected fluidic-driven actuators, which together act to convey the surface topography of 3D objects by means of manipulation of a stereolithographed scaffold internal to the volume of the clay. Each actuator comprises a discrete fluidically-inflatable cell that is connected to two common pressurized reservoirs (within a base) through a dedicated two-way miniature valve integrated with a pressure sensor, manufactured by MEMS technology existing at Georgia Tech. The position of each discrete surface element can be altered either by the user or by the host computer. The simple measurement of volume flow rate combined with suitable software-based kinematic analysis allows the determination of the entire volume of the clay, and therefore the coordinates of its surface. A unique feature of the digital clay is that the force that is necessary to actuate the discretized surface is derived entirely from the two fluidic reservoirs, thus eliminating the need for small-scale, electrically-driven actuators that may have limited torque or linear force. This fluidic approach overcomes the constraints imposed by actuator energy density limits, and distributed wiring and sensing requirements, that have heretofore prevented structures such as digital clay from becoming a reality. Furthermore, the user can activate the device interactively with the host computer by sensing and overcoming the force that is exerted by the liquid pressure to concomitantly set (or reset) the shape of the device to a desired state. In this project the PI will develop and demonstrate the digital clay hardware, its computer interface, and associated software, and will further illustrate its efficacy in applications of interest (e.g., computer-aided design, medical and bioengineering diagnostics, and reconfigurable input/output displays). Of particular note is the potential of digital clay to aid visually impaired persons in receiving/sending haptic information from/to a computer.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121663</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121663</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1189" target="n1191">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+SY Digital Clay for Shape Input and Display</data>
      <data key="e_abstract">Shape is a key element in successful communication, interpretation, and understanding of complex data in virtually every area of engineering, art, science, and medicine. While in recent years the communication of both form and complex data have been greatly enhanced by visualization that is based on planar images, computational power has reached the point where it is possible to consider real-time interactive 3D physical communication. In this project, the PI will develop a novel interactive 2D or 3D haptic computer interface that enables both user-specified display of shapes as output from a computer, and user-directed input of shapes to a computer. This so-called &quot;digital clay&quot; will allow users to convey and/or sense multiple-element, parallel information strands. It is a distributed input/display device, the surface of which can be shaped by a user and acquired by a computer; alternatively, the clay can be shaped by the computer for the user to examine. Like ordinary clay, digital clay will allow an area of moderate size to be touched, reshaped with pressure, and seen by the user in true 3D form. Unlike ordinary clay, digital clay also provides parameters to the computer that will represent the shape to the computer for further analysis, storage, replication, communication and/or modification; or, will allow the computer to prescribe its shape. This combined input and output feature of the clay enables two-way communication between the computer and the user. Some previous implementations of digital-clay-like devices have focused on reshaping of non-physical volumes of &apos;virtual clay&apos; using glove-like or haptic manipulator interfaces to a computer in which the virtual clay is stored. The PI&apos;s approach is different; digital clay comprises an instrumented, actuated, computer-interfaced physical volume bounded by an actuatable surface that acts as the haptic interface. This surface is displaced by rows or arrays of controllable interconnected fluidic-driven actuators, which together act to convey the surface topography of 3D objects by means of manipulation of a stereolithographed scaffold internal to the volume of the clay. Each actuator comprises a discrete fluidically-inflatable cell that is connected to two common pressurized reservoirs (within a base) through a dedicated two-way miniature valve integrated with a pressure sensor, manufactured by MEMS technology existing at Georgia Tech. The position of each discrete surface element can be altered either by the user or by the host computer. The simple measurement of volume flow rate combined with suitable software-based kinematic analysis allows the determination of the entire volume of the clay, and therefore the coordinates of its surface. A unique feature of the digital clay is that the force that is necessary to actuate the discretized surface is derived entirely from the two fluidic reservoirs, thus eliminating the need for small-scale, electrically-driven actuators that may have limited torque or linear force. This fluidic approach overcomes the constraints imposed by actuator energy density limits, and distributed wiring and sensing requirements, that have heretofore prevented structures such as digital clay from becoming a reality. Furthermore, the user can activate the device interactively with the host computer by sensing and overcoming the force that is exerted by the liquid pressure to concomitantly set (or reset) the shape of the device to a desired state. In this project the PI will develop and demonstrate the digital clay hardware, its computer interface, and associated software, and will further illustrate its efficacy in applications of interest (e.g., computer-aided design, medical and bioengineering diagnostics, and reconfigurable input/output displays). Of particular note is the potential of digital clay to aid visually impaired persons in receiving/sending haptic information from/to a computer.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121663</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121663</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1189" target="n1192">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+SY Digital Clay for Shape Input and Display</data>
      <data key="e_abstract">Shape is a key element in successful communication, interpretation, and understanding of complex data in virtually every area of engineering, art, science, and medicine. While in recent years the communication of both form and complex data have been greatly enhanced by visualization that is based on planar images, computational power has reached the point where it is possible to consider real-time interactive 3D physical communication. In this project, the PI will develop a novel interactive 2D or 3D haptic computer interface that enables both user-specified display of shapes as output from a computer, and user-directed input of shapes to a computer. This so-called &quot;digital clay&quot; will allow users to convey and/or sense multiple-element, parallel information strands. It is a distributed input/display device, the surface of which can be shaped by a user and acquired by a computer; alternatively, the clay can be shaped by the computer for the user to examine. Like ordinary clay, digital clay will allow an area of moderate size to be touched, reshaped with pressure, and seen by the user in true 3D form. Unlike ordinary clay, digital clay also provides parameters to the computer that will represent the shape to the computer for further analysis, storage, replication, communication and/or modification; or, will allow the computer to prescribe its shape. This combined input and output feature of the clay enables two-way communication between the computer and the user. Some previous implementations of digital-clay-like devices have focused on reshaping of non-physical volumes of &apos;virtual clay&apos; using glove-like or haptic manipulator interfaces to a computer in which the virtual clay is stored. The PI&apos;s approach is different; digital clay comprises an instrumented, actuated, computer-interfaced physical volume bounded by an actuatable surface that acts as the haptic interface. This surface is displaced by rows or arrays of controllable interconnected fluidic-driven actuators, which together act to convey the surface topography of 3D objects by means of manipulation of a stereolithographed scaffold internal to the volume of the clay. Each actuator comprises a discrete fluidically-inflatable cell that is connected to two common pressurized reservoirs (within a base) through a dedicated two-way miniature valve integrated with a pressure sensor, manufactured by MEMS technology existing at Georgia Tech. The position of each discrete surface element can be altered either by the user or by the host computer. The simple measurement of volume flow rate combined with suitable software-based kinematic analysis allows the determination of the entire volume of the clay, and therefore the coordinates of its surface. A unique feature of the digital clay is that the force that is necessary to actuate the discretized surface is derived entirely from the two fluidic reservoirs, thus eliminating the need for small-scale, electrically-driven actuators that may have limited torque or linear force. This fluidic approach overcomes the constraints imposed by actuator energy density limits, and distributed wiring and sensing requirements, that have heretofore prevented structures such as digital clay from becoming a reality. Furthermore, the user can activate the device interactively with the host computer by sensing and overcoming the force that is exerted by the liquid pressure to concomitantly set (or reset) the shape of the device to a desired state. In this project the PI will develop and demonstrate the digital clay hardware, its computer interface, and associated software, and will further illustrate its efficacy in applications of interest (e.g., computer-aided design, medical and bioengineering diagnostics, and reconfigurable input/output displays). Of particular note is the potential of digital clay to aid visually impaired persons in receiving/sending haptic information from/to a computer.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121663</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121663</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1189" target="n1193">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+SY Digital Clay for Shape Input and Display</data>
      <data key="e_abstract">Shape is a key element in successful communication, interpretation, and understanding of complex data in virtually every area of engineering, art, science, and medicine. While in recent years the communication of both form and complex data have been greatly enhanced by visualization that is based on planar images, computational power has reached the point where it is possible to consider real-time interactive 3D physical communication. In this project, the PI will develop a novel interactive 2D or 3D haptic computer interface that enables both user-specified display of shapes as output from a computer, and user-directed input of shapes to a computer. This so-called &quot;digital clay&quot; will allow users to convey and/or sense multiple-element, parallel information strands. It is a distributed input/display device, the surface of which can be shaped by a user and acquired by a computer; alternatively, the clay can be shaped by the computer for the user to examine. Like ordinary clay, digital clay will allow an area of moderate size to be touched, reshaped with pressure, and seen by the user in true 3D form. Unlike ordinary clay, digital clay also provides parameters to the computer that will represent the shape to the computer for further analysis, storage, replication, communication and/or modification; or, will allow the computer to prescribe its shape. This combined input and output feature of the clay enables two-way communication between the computer and the user. Some previous implementations of digital-clay-like devices have focused on reshaping of non-physical volumes of &apos;virtual clay&apos; using glove-like or haptic manipulator interfaces to a computer in which the virtual clay is stored. The PI&apos;s approach is different; digital clay comprises an instrumented, actuated, computer-interfaced physical volume bounded by an actuatable surface that acts as the haptic interface. This surface is displaced by rows or arrays of controllable interconnected fluidic-driven actuators, which together act to convey the surface topography of 3D objects by means of manipulation of a stereolithographed scaffold internal to the volume of the clay. Each actuator comprises a discrete fluidically-inflatable cell that is connected to two common pressurized reservoirs (within a base) through a dedicated two-way miniature valve integrated with a pressure sensor, manufactured by MEMS technology existing at Georgia Tech. The position of each discrete surface element can be altered either by the user or by the host computer. The simple measurement of volume flow rate combined with suitable software-based kinematic analysis allows the determination of the entire volume of the clay, and therefore the coordinates of its surface. A unique feature of the digital clay is that the force that is necessary to actuate the discretized surface is derived entirely from the two fluidic reservoirs, thus eliminating the need for small-scale, electrically-driven actuators that may have limited torque or linear force. This fluidic approach overcomes the constraints imposed by actuator energy density limits, and distributed wiring and sensing requirements, that have heretofore prevented structures such as digital clay from becoming a reality. Furthermore, the user can activate the device interactively with the host computer by sensing and overcoming the force that is exerted by the liquid pressure to concomitantly set (or reset) the shape of the device to a desired state. In this project the PI will develop and demonstrate the digital clay hardware, its computer interface, and associated software, and will further illustrate its efficacy in applications of interest (e.g., computer-aided design, medical and bioengineering diagnostics, and reconfigurable input/output displays). Of particular note is the potential of digital clay to aid visually impaired persons in receiving/sending haptic information from/to a computer.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121663</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121663</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1190" target="n1191">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+SY Digital Clay for Shape Input and Display</data>
      <data key="e_abstract">Shape is a key element in successful communication, interpretation, and understanding of complex data in virtually every area of engineering, art, science, and medicine. While in recent years the communication of both form and complex data have been greatly enhanced by visualization that is based on planar images, computational power has reached the point where it is possible to consider real-time interactive 3D physical communication. In this project, the PI will develop a novel interactive 2D or 3D haptic computer interface that enables both user-specified display of shapes as output from a computer, and user-directed input of shapes to a computer. This so-called &quot;digital clay&quot; will allow users to convey and/or sense multiple-element, parallel information strands. It is a distributed input/display device, the surface of which can be shaped by a user and acquired by a computer; alternatively, the clay can be shaped by the computer for the user to examine. Like ordinary clay, digital clay will allow an area of moderate size to be touched, reshaped with pressure, and seen by the user in true 3D form. Unlike ordinary clay, digital clay also provides parameters to the computer that will represent the shape to the computer for further analysis, storage, replication, communication and/or modification; or, will allow the computer to prescribe its shape. This combined input and output feature of the clay enables two-way communication between the computer and the user. Some previous implementations of digital-clay-like devices have focused on reshaping of non-physical volumes of &apos;virtual clay&apos; using glove-like or haptic manipulator interfaces to a computer in which the virtual clay is stored. The PI&apos;s approach is different; digital clay comprises an instrumented, actuated, computer-interfaced physical volume bounded by an actuatable surface that acts as the haptic interface. This surface is displaced by rows or arrays of controllable interconnected fluidic-driven actuators, which together act to convey the surface topography of 3D objects by means of manipulation of a stereolithographed scaffold internal to the volume of the clay. Each actuator comprises a discrete fluidically-inflatable cell that is connected to two common pressurized reservoirs (within a base) through a dedicated two-way miniature valve integrated with a pressure sensor, manufactured by MEMS technology existing at Georgia Tech. The position of each discrete surface element can be altered either by the user or by the host computer. The simple measurement of volume flow rate combined with suitable software-based kinematic analysis allows the determination of the entire volume of the clay, and therefore the coordinates of its surface. A unique feature of the digital clay is that the force that is necessary to actuate the discretized surface is derived entirely from the two fluidic reservoirs, thus eliminating the need for small-scale, electrically-driven actuators that may have limited torque or linear force. This fluidic approach overcomes the constraints imposed by actuator energy density limits, and distributed wiring and sensing requirements, that have heretofore prevented structures such as digital clay from becoming a reality. Furthermore, the user can activate the device interactively with the host computer by sensing and overcoming the force that is exerted by the liquid pressure to concomitantly set (or reset) the shape of the device to a desired state. In this project the PI will develop and demonstrate the digital clay hardware, its computer interface, and associated software, and will further illustrate its efficacy in applications of interest (e.g., computer-aided design, medical and bioengineering diagnostics, and reconfigurable input/output displays). Of particular note is the potential of digital clay to aid visually impaired persons in receiving/sending haptic information from/to a computer.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121663</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121663</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1190" target="n1192">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+SY Digital Clay for Shape Input and Display</data>
      <data key="e_abstract">Shape is a key element in successful communication, interpretation, and understanding of complex data in virtually every area of engineering, art, science, and medicine. While in recent years the communication of both form and complex data have been greatly enhanced by visualization that is based on planar images, computational power has reached the point where it is possible to consider real-time interactive 3D physical communication. In this project, the PI will develop a novel interactive 2D or 3D haptic computer interface that enables both user-specified display of shapes as output from a computer, and user-directed input of shapes to a computer. This so-called &quot;digital clay&quot; will allow users to convey and/or sense multiple-element, parallel information strands. It is a distributed input/display device, the surface of which can be shaped by a user and acquired by a computer; alternatively, the clay can be shaped by the computer for the user to examine. Like ordinary clay, digital clay will allow an area of moderate size to be touched, reshaped with pressure, and seen by the user in true 3D form. Unlike ordinary clay, digital clay also provides parameters to the computer that will represent the shape to the computer for further analysis, storage, replication, communication and/or modification; or, will allow the computer to prescribe its shape. This combined input and output feature of the clay enables two-way communication between the computer and the user. Some previous implementations of digital-clay-like devices have focused on reshaping of non-physical volumes of &apos;virtual clay&apos; using glove-like or haptic manipulator interfaces to a computer in which the virtual clay is stored. The PI&apos;s approach is different; digital clay comprises an instrumented, actuated, computer-interfaced physical volume bounded by an actuatable surface that acts as the haptic interface. This surface is displaced by rows or arrays of controllable interconnected fluidic-driven actuators, which together act to convey the surface topography of 3D objects by means of manipulation of a stereolithographed scaffold internal to the volume of the clay. Each actuator comprises a discrete fluidically-inflatable cell that is connected to two common pressurized reservoirs (within a base) through a dedicated two-way miniature valve integrated with a pressure sensor, manufactured by MEMS technology existing at Georgia Tech. The position of each discrete surface element can be altered either by the user or by the host computer. The simple measurement of volume flow rate combined with suitable software-based kinematic analysis allows the determination of the entire volume of the clay, and therefore the coordinates of its surface. A unique feature of the digital clay is that the force that is necessary to actuate the discretized surface is derived entirely from the two fluidic reservoirs, thus eliminating the need for small-scale, electrically-driven actuators that may have limited torque or linear force. This fluidic approach overcomes the constraints imposed by actuator energy density limits, and distributed wiring and sensing requirements, that have heretofore prevented structures such as digital clay from becoming a reality. Furthermore, the user can activate the device interactively with the host computer by sensing and overcoming the force that is exerted by the liquid pressure to concomitantly set (or reset) the shape of the device to a desired state. In this project the PI will develop and demonstrate the digital clay hardware, its computer interface, and associated software, and will further illustrate its efficacy in applications of interest (e.g., computer-aided design, medical and bioengineering diagnostics, and reconfigurable input/output displays). Of particular note is the potential of digital clay to aid visually impaired persons in receiving/sending haptic information from/to a computer.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121663</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121663</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1190" target="n1193">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+SY Digital Clay for Shape Input and Display</data>
      <data key="e_abstract">Shape is a key element in successful communication, interpretation, and understanding of complex data in virtually every area of engineering, art, science, and medicine. While in recent years the communication of both form and complex data have been greatly enhanced by visualization that is based on planar images, computational power has reached the point where it is possible to consider real-time interactive 3D physical communication. In this project, the PI will develop a novel interactive 2D or 3D haptic computer interface that enables both user-specified display of shapes as output from a computer, and user-directed input of shapes to a computer. This so-called &quot;digital clay&quot; will allow users to convey and/or sense multiple-element, parallel information strands. It is a distributed input/display device, the surface of which can be shaped by a user and acquired by a computer; alternatively, the clay can be shaped by the computer for the user to examine. Like ordinary clay, digital clay will allow an area of moderate size to be touched, reshaped with pressure, and seen by the user in true 3D form. Unlike ordinary clay, digital clay also provides parameters to the computer that will represent the shape to the computer for further analysis, storage, replication, communication and/or modification; or, will allow the computer to prescribe its shape. This combined input and output feature of the clay enables two-way communication between the computer and the user. Some previous implementations of digital-clay-like devices have focused on reshaping of non-physical volumes of &apos;virtual clay&apos; using glove-like or haptic manipulator interfaces to a computer in which the virtual clay is stored. The PI&apos;s approach is different; digital clay comprises an instrumented, actuated, computer-interfaced physical volume bounded by an actuatable surface that acts as the haptic interface. This surface is displaced by rows or arrays of controllable interconnected fluidic-driven actuators, which together act to convey the surface topography of 3D objects by means of manipulation of a stereolithographed scaffold internal to the volume of the clay. Each actuator comprises a discrete fluidically-inflatable cell that is connected to two common pressurized reservoirs (within a base) through a dedicated two-way miniature valve integrated with a pressure sensor, manufactured by MEMS technology existing at Georgia Tech. The position of each discrete surface element can be altered either by the user or by the host computer. The simple measurement of volume flow rate combined with suitable software-based kinematic analysis allows the determination of the entire volume of the clay, and therefore the coordinates of its surface. A unique feature of the digital clay is that the force that is necessary to actuate the discretized surface is derived entirely from the two fluidic reservoirs, thus eliminating the need for small-scale, electrically-driven actuators that may have limited torque or linear force. This fluidic approach overcomes the constraints imposed by actuator energy density limits, and distributed wiring and sensing requirements, that have heretofore prevented structures such as digital clay from becoming a reality. Furthermore, the user can activate the device interactively with the host computer by sensing and overcoming the force that is exerted by the liquid pressure to concomitantly set (or reset) the shape of the device to a desired state. In this project the PI will develop and demonstrate the digital clay hardware, its computer interface, and associated software, and will further illustrate its efficacy in applications of interest (e.g., computer-aided design, medical and bioengineering diagnostics, and reconfigurable input/output displays). Of particular note is the potential of digital clay to aid visually impaired persons in receiving/sending haptic information from/to a computer.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121663</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121663</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1191" target="n1192">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+SY Digital Clay for Shape Input and Display</data>
      <data key="e_abstract">Shape is a key element in successful communication, interpretation, and understanding of complex data in virtually every area of engineering, art, science, and medicine. While in recent years the communication of both form and complex data have been greatly enhanced by visualization that is based on planar images, computational power has reached the point where it is possible to consider real-time interactive 3D physical communication. In this project, the PI will develop a novel interactive 2D or 3D haptic computer interface that enables both user-specified display of shapes as output from a computer, and user-directed input of shapes to a computer. This so-called &quot;digital clay&quot; will allow users to convey and/or sense multiple-element, parallel information strands. It is a distributed input/display device, the surface of which can be shaped by a user and acquired by a computer; alternatively, the clay can be shaped by the computer for the user to examine. Like ordinary clay, digital clay will allow an area of moderate size to be touched, reshaped with pressure, and seen by the user in true 3D form. Unlike ordinary clay, digital clay also provides parameters to the computer that will represent the shape to the computer for further analysis, storage, replication, communication and/or modification; or, will allow the computer to prescribe its shape. This combined input and output feature of the clay enables two-way communication between the computer and the user. Some previous implementations of digital-clay-like devices have focused on reshaping of non-physical volumes of &apos;virtual clay&apos; using glove-like or haptic manipulator interfaces to a computer in which the virtual clay is stored. The PI&apos;s approach is different; digital clay comprises an instrumented, actuated, computer-interfaced physical volume bounded by an actuatable surface that acts as the haptic interface. This surface is displaced by rows or arrays of controllable interconnected fluidic-driven actuators, which together act to convey the surface topography of 3D objects by means of manipulation of a stereolithographed scaffold internal to the volume of the clay. Each actuator comprises a discrete fluidically-inflatable cell that is connected to two common pressurized reservoirs (within a base) through a dedicated two-way miniature valve integrated with a pressure sensor, manufactured by MEMS technology existing at Georgia Tech. The position of each discrete surface element can be altered either by the user or by the host computer. The simple measurement of volume flow rate combined with suitable software-based kinematic analysis allows the determination of the entire volume of the clay, and therefore the coordinates of its surface. A unique feature of the digital clay is that the force that is necessary to actuate the discretized surface is derived entirely from the two fluidic reservoirs, thus eliminating the need for small-scale, electrically-driven actuators that may have limited torque or linear force. This fluidic approach overcomes the constraints imposed by actuator energy density limits, and distributed wiring and sensing requirements, that have heretofore prevented structures such as digital clay from becoming a reality. Furthermore, the user can activate the device interactively with the host computer by sensing and overcoming the force that is exerted by the liquid pressure to concomitantly set (or reset) the shape of the device to a desired state. In this project the PI will develop and demonstrate the digital clay hardware, its computer interface, and associated software, and will further illustrate its efficacy in applications of interest (e.g., computer-aided design, medical and bioengineering diagnostics, and reconfigurable input/output displays). Of particular note is the potential of digital clay to aid visually impaired persons in receiving/sending haptic information from/to a computer.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121663</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121663</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1191" target="n1193">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+SY Digital Clay for Shape Input and Display</data>
      <data key="e_abstract">Shape is a key element in successful communication, interpretation, and understanding of complex data in virtually every area of engineering, art, science, and medicine. While in recent years the communication of both form and complex data have been greatly enhanced by visualization that is based on planar images, computational power has reached the point where it is possible to consider real-time interactive 3D physical communication. In this project, the PI will develop a novel interactive 2D or 3D haptic computer interface that enables both user-specified display of shapes as output from a computer, and user-directed input of shapes to a computer. This so-called &quot;digital clay&quot; will allow users to convey and/or sense multiple-element, parallel information strands. It is a distributed input/display device, the surface of which can be shaped by a user and acquired by a computer; alternatively, the clay can be shaped by the computer for the user to examine. Like ordinary clay, digital clay will allow an area of moderate size to be touched, reshaped with pressure, and seen by the user in true 3D form. Unlike ordinary clay, digital clay also provides parameters to the computer that will represent the shape to the computer for further analysis, storage, replication, communication and/or modification; or, will allow the computer to prescribe its shape. This combined input and output feature of the clay enables two-way communication between the computer and the user. Some previous implementations of digital-clay-like devices have focused on reshaping of non-physical volumes of &apos;virtual clay&apos; using glove-like or haptic manipulator interfaces to a computer in which the virtual clay is stored. The PI&apos;s approach is different; digital clay comprises an instrumented, actuated, computer-interfaced physical volume bounded by an actuatable surface that acts as the haptic interface. This surface is displaced by rows or arrays of controllable interconnected fluidic-driven actuators, which together act to convey the surface topography of 3D objects by means of manipulation of a stereolithographed scaffold internal to the volume of the clay. Each actuator comprises a discrete fluidically-inflatable cell that is connected to two common pressurized reservoirs (within a base) through a dedicated two-way miniature valve integrated with a pressure sensor, manufactured by MEMS technology existing at Georgia Tech. The position of each discrete surface element can be altered either by the user or by the host computer. The simple measurement of volume flow rate combined with suitable software-based kinematic analysis allows the determination of the entire volume of the clay, and therefore the coordinates of its surface. A unique feature of the digital clay is that the force that is necessary to actuate the discretized surface is derived entirely from the two fluidic reservoirs, thus eliminating the need for small-scale, electrically-driven actuators that may have limited torque or linear force. This fluidic approach overcomes the constraints imposed by actuator energy density limits, and distributed wiring and sensing requirements, that have heretofore prevented structures such as digital clay from becoming a reality. Furthermore, the user can activate the device interactively with the host computer by sensing and overcoming the force that is exerted by the liquid pressure to concomitantly set (or reset) the shape of the device to a desired state. In this project the PI will develop and demonstrate the digital clay hardware, its computer interface, and associated software, and will further illustrate its efficacy in applications of interest (e.g., computer-aided design, medical and bioengineering diagnostics, and reconfigurable input/output displays). Of particular note is the potential of digital clay to aid visually impaired persons in receiving/sending haptic information from/to a computer.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121663</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121663</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1192" target="n1193">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/PE+SY Digital Clay for Shape Input and Display</data>
      <data key="e_abstract">Shape is a key element in successful communication, interpretation, and understanding of complex data in virtually every area of engineering, art, science, and medicine. While in recent years the communication of both form and complex data have been greatly enhanced by visualization that is based on planar images, computational power has reached the point where it is possible to consider real-time interactive 3D physical communication. In this project, the PI will develop a novel interactive 2D or 3D haptic computer interface that enables both user-specified display of shapes as output from a computer, and user-directed input of shapes to a computer. This so-called &quot;digital clay&quot; will allow users to convey and/or sense multiple-element, parallel information strands. It is a distributed input/display device, the surface of which can be shaped by a user and acquired by a computer; alternatively, the clay can be shaped by the computer for the user to examine. Like ordinary clay, digital clay will allow an area of moderate size to be touched, reshaped with pressure, and seen by the user in true 3D form. Unlike ordinary clay, digital clay also provides parameters to the computer that will represent the shape to the computer for further analysis, storage, replication, communication and/or modification; or, will allow the computer to prescribe its shape. This combined input and output feature of the clay enables two-way communication between the computer and the user. Some previous implementations of digital-clay-like devices have focused on reshaping of non-physical volumes of &apos;virtual clay&apos; using glove-like or haptic manipulator interfaces to a computer in which the virtual clay is stored. The PI&apos;s approach is different; digital clay comprises an instrumented, actuated, computer-interfaced physical volume bounded by an actuatable surface that acts as the haptic interface. This surface is displaced by rows or arrays of controllable interconnected fluidic-driven actuators, which together act to convey the surface topography of 3D objects by means of manipulation of a stereolithographed scaffold internal to the volume of the clay. Each actuator comprises a discrete fluidically-inflatable cell that is connected to two common pressurized reservoirs (within a base) through a dedicated two-way miniature valve integrated with a pressure sensor, manufactured by MEMS technology existing at Georgia Tech. The position of each discrete surface element can be altered either by the user or by the host computer. The simple measurement of volume flow rate combined with suitable software-based kinematic analysis allows the determination of the entire volume of the clay, and therefore the coordinates of its surface. A unique feature of the digital clay is that the force that is necessary to actuate the discretized surface is derived entirely from the two fluidic reservoirs, thus eliminating the need for small-scale, electrically-driven actuators that may have limited torque or linear force. This fluidic approach overcomes the constraints imposed by actuator energy density limits, and distributed wiring and sensing requirements, that have heretofore prevented structures such as digital clay from becoming a reality. Furthermore, the user can activate the device interactively with the host computer by sensing and overcoming the force that is exerted by the liquid pressure to concomitantly set (or reset) the shape of the device to a desired state. In this project the PI will develop and demonstrate the digital clay hardware, its computer interface, and associated software, and will further illustrate its efficacy in applications of interest (e.g., computer-aided design, medical and bioengineering diagnostics, and reconfigurable input/output displays). Of particular note is the potential of digital clay to aid visually impaired persons in receiving/sending haptic information from/to a computer.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121663</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121663</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n925" target="n927">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: Collaborative Research: SmartApps: An Application Centric Approach to High Performance Computing</data>
      <data key="e_abstract">EIA-0103742&lt;br/&gt;Lawrence Rauchwerger&lt;br/&gt;Texas Engineering Experiment Station&lt;br/&gt;&lt;br/&gt;NGS: Collaborative Research: SmartApps: An Application Centric Approach to High Performance Computing&lt;br/&gt;&lt;br/&gt;The objective of this proposal is to develop new methods that allows runtime performance optimization by an enhanced monitoring, resource modeling, evaluation and remapping of the application at runtime, depending on runtime resources and application performance. The specific approaches to be pursued include application algorithm adaptation, run-time software optimization, system configuration selection and tuning reconfigurable OS services.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103742</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103742</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n231" target="n282">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">An Adaptive Protocol Suite for The Next Generation Wireless Internet</data>
      <data key="e_abstract">The technology and business of wireless communications systems have madespectacular progress since the&lt;br/&gt;first systems were introduced fifteen years ago. Currently proposed third generation (3G) systems offer roaming, higher capacity, and increased exibility for low bandwidth services, such as voice calls and messaging. While future wireless systems (fourth generation (4G) and higher) are envisioned to provide higher capacity, they are also expected to support heterogeneity in the following aspects: (i) physical environments (ranging from high-bandwidth low error-rate pico-cells to low-bandwidth high error-rate macro-cells); (ii) network architectures (cellular, peer-to-peer, or other hybrid network models); and (iii) applications (ranging from low-data rate non-real-time applications to high-data rate real-time applications). In this project, we will investigate several key elements that are necessary to realize a protocol stack for a mobile station in 4G wireless systems. The ultimate objective is to develop an adaptive protocol suite that would adapt itself to the different aspects of heterogeneity exhibited by the next generation wireless systems. In particular we willinvestigate the following problems:&lt;br/&gt;1. Application Layer&lt;br/&gt;Error-rate scalability: We propose an adaptive multiple description algorithm for error-rate scalability.&lt;br/&gt;Data-rate scalability: We introduce an optimal dynamic rate shaping technique to be utilized in the case of&lt;br/&gt;congestion or bandwidth variations.&lt;br/&gt;Jointly optimal real-time video streaming system: We propose a real-time video streaming system integrates the proposed unequal error protection mechanism, the multiple description coding technique, and the error-concealment method.&lt;br/&gt;2. Security&lt;br/&gt;An adaptive and scalable security protocols: We propose to employ finite-field wavelets to develop novel and&lt;br/&gt;innovative public key security techniques that address ubiquity, scalability, mobility, and usability.&lt;br/&gt;We propose efficient security protocols for mobile devices with limited processing power.&lt;br/&gt;3. Transport Layer&lt;br/&gt;Heterogeneous Packet Flows: We proposetoinvestigate transport protocols that can support a much richer&lt;br/&gt;set of semantics, adapting to the reliability semantics chosen by applications on a per-frame basis.&lt;br/&gt;Heterogeneous Network Characteristics: We propose to build a single transport protocol that will adapt itself&lt;br/&gt;to the characteristics of a variety of wireless network environments.&lt;br/&gt;Heterogeneous Network Architectures: We propose to explore transport layer adaptivity to the underlying&lt;br/&gt;network model (cellular, peer-to-peer, or hybrid).&lt;br/&gt;4. Network Layer&lt;br/&gt;Application Requirements: We will explore network layer constructs that enable heterogeneous applications&lt;br/&gt;to implement custom network layer policies.&lt;br/&gt;Heterogeneous Network Architectures: We will explore the use of novel state propagation schemes and virtual&lt;br/&gt;backbone approaches to enhance transport layer performance and help in adapting to heterogeneous network&lt;br/&gt;models.&lt;br/&gt;5. Data Link Layer&lt;br/&gt;Rate-compatible error control coding: We develop new types of rate-compatible convolutional codes using&lt;br/&gt;finite-field wavelet transforms.&lt;br/&gt; The protocol suite requires a new Protocol Suite Integration Plan. The project will consist of four phases. During the first phase we will design the details of all the proposed components/protocols/methods of the protocol suite. During the second phase we will assess their performance in a simulation testbed. During the third phase we will integrate the individual modules into the suite and finally in the fourth phase we willprototype the proposed protocol suite and we will assess its performance on a physical testbed.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">117840</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">117840</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n231" target="n644">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">An Adaptive Protocol Suite for The Next Generation Wireless Internet</data>
      <data key="e_abstract">The technology and business of wireless communications systems have madespectacular progress since the&lt;br/&gt;first systems were introduced fifteen years ago. Currently proposed third generation (3G) systems offer roaming, higher capacity, and increased exibility for low bandwidth services, such as voice calls and messaging. While future wireless systems (fourth generation (4G) and higher) are envisioned to provide higher capacity, they are also expected to support heterogeneity in the following aspects: (i) physical environments (ranging from high-bandwidth low error-rate pico-cells to low-bandwidth high error-rate macro-cells); (ii) network architectures (cellular, peer-to-peer, or other hybrid network models); and (iii) applications (ranging from low-data rate non-real-time applications to high-data rate real-time applications). In this project, we will investigate several key elements that are necessary to realize a protocol stack for a mobile station in 4G wireless systems. The ultimate objective is to develop an adaptive protocol suite that would adapt itself to the different aspects of heterogeneity exhibited by the next generation wireless systems. In particular we willinvestigate the following problems:&lt;br/&gt;1. Application Layer&lt;br/&gt;Error-rate scalability: We propose an adaptive multiple description algorithm for error-rate scalability.&lt;br/&gt;Data-rate scalability: We introduce an optimal dynamic rate shaping technique to be utilized in the case of&lt;br/&gt;congestion or bandwidth variations.&lt;br/&gt;Jointly optimal real-time video streaming system: We propose a real-time video streaming system integrates the proposed unequal error protection mechanism, the multiple description coding technique, and the error-concealment method.&lt;br/&gt;2. Security&lt;br/&gt;An adaptive and scalable security protocols: We propose to employ finite-field wavelets to develop novel and&lt;br/&gt;innovative public key security techniques that address ubiquity, scalability, mobility, and usability.&lt;br/&gt;We propose efficient security protocols for mobile devices with limited processing power.&lt;br/&gt;3. Transport Layer&lt;br/&gt;Heterogeneous Packet Flows: We proposetoinvestigate transport protocols that can support a much richer&lt;br/&gt;set of semantics, adapting to the reliability semantics chosen by applications on a per-frame basis.&lt;br/&gt;Heterogeneous Network Characteristics: We propose to build a single transport protocol that will adapt itself&lt;br/&gt;to the characteristics of a variety of wireless network environments.&lt;br/&gt;Heterogeneous Network Architectures: We propose to explore transport layer adaptivity to the underlying&lt;br/&gt;network model (cellular, peer-to-peer, or hybrid).&lt;br/&gt;4. Network Layer&lt;br/&gt;Application Requirements: We will explore network layer constructs that enable heterogeneous applications&lt;br/&gt;to implement custom network layer policies.&lt;br/&gt;Heterogeneous Network Architectures: We will explore the use of novel state propagation schemes and virtual&lt;br/&gt;backbone approaches to enhance transport layer performance and help in adapting to heterogeneous network&lt;br/&gt;models.&lt;br/&gt;5. Data Link Layer&lt;br/&gt;Rate-compatible error control coding: We develop new types of rate-compatible convolutional codes using&lt;br/&gt;finite-field wavelet transforms.&lt;br/&gt; The protocol suite requires a new Protocol Suite Integration Plan. The project will consist of four phases. During the first phase we will design the details of all the proposed components/protocols/methods of the protocol suite. During the second phase we will assess their performance in a simulation testbed. During the third phase we will integrate the individual modules into the suite and finally in the fourth phase we willprototype the proposed protocol suite and we will assess its performance on a physical testbed.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">117840</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">117840</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n231" target="n1204">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">An Adaptive Protocol Suite for The Next Generation Wireless Internet</data>
      <data key="e_abstract">The technology and business of wireless communications systems have madespectacular progress since the&lt;br/&gt;first systems were introduced fifteen years ago. Currently proposed third generation (3G) systems offer roaming, higher capacity, and increased exibility for low bandwidth services, such as voice calls and messaging. While future wireless systems (fourth generation (4G) and higher) are envisioned to provide higher capacity, they are also expected to support heterogeneity in the following aspects: (i) physical environments (ranging from high-bandwidth low error-rate pico-cells to low-bandwidth high error-rate macro-cells); (ii) network architectures (cellular, peer-to-peer, or other hybrid network models); and (iii) applications (ranging from low-data rate non-real-time applications to high-data rate real-time applications). In this project, we will investigate several key elements that are necessary to realize a protocol stack for a mobile station in 4G wireless systems. The ultimate objective is to develop an adaptive protocol suite that would adapt itself to the different aspects of heterogeneity exhibited by the next generation wireless systems. In particular we willinvestigate the following problems:&lt;br/&gt;1. Application Layer&lt;br/&gt;Error-rate scalability: We propose an adaptive multiple description algorithm for error-rate scalability.&lt;br/&gt;Data-rate scalability: We introduce an optimal dynamic rate shaping technique to be utilized in the case of&lt;br/&gt;congestion or bandwidth variations.&lt;br/&gt;Jointly optimal real-time video streaming system: We propose a real-time video streaming system integrates the proposed unequal error protection mechanism, the multiple description coding technique, and the error-concealment method.&lt;br/&gt;2. Security&lt;br/&gt;An adaptive and scalable security protocols: We propose to employ finite-field wavelets to develop novel and&lt;br/&gt;innovative public key security techniques that address ubiquity, scalability, mobility, and usability.&lt;br/&gt;We propose efficient security protocols for mobile devices with limited processing power.&lt;br/&gt;3. Transport Layer&lt;br/&gt;Heterogeneous Packet Flows: We proposetoinvestigate transport protocols that can support a much richer&lt;br/&gt;set of semantics, adapting to the reliability semantics chosen by applications on a per-frame basis.&lt;br/&gt;Heterogeneous Network Characteristics: We propose to build a single transport protocol that will adapt itself&lt;br/&gt;to the characteristics of a variety of wireless network environments.&lt;br/&gt;Heterogeneous Network Architectures: We propose to explore transport layer adaptivity to the underlying&lt;br/&gt;network model (cellular, peer-to-peer, or hybrid).&lt;br/&gt;4. Network Layer&lt;br/&gt;Application Requirements: We will explore network layer constructs that enable heterogeneous applications&lt;br/&gt;to implement custom network layer policies.&lt;br/&gt;Heterogeneous Network Architectures: We will explore the use of novel state propagation schemes and virtual&lt;br/&gt;backbone approaches to enhance transport layer performance and help in adapting to heterogeneous network&lt;br/&gt;models.&lt;br/&gt;5. Data Link Layer&lt;br/&gt;Rate-compatible error control coding: We develop new types of rate-compatible convolutional codes using&lt;br/&gt;finite-field wavelet transforms.&lt;br/&gt; The protocol suite requires a new Protocol Suite Integration Plan. The project will consist of four phases. During the first phase we will design the details of all the proposed components/protocols/methods of the protocol suite. During the second phase we will assess their performance in a simulation testbed. During the third phase we will integrate the individual modules into the suite and finally in the fourth phase we willprototype the proposed protocol suite and we will assess its performance on a physical testbed.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">117840</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">117840</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n282" target="n644">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">An Adaptive Protocol Suite for The Next Generation Wireless Internet</data>
      <data key="e_abstract">The technology and business of wireless communications systems have madespectacular progress since the&lt;br/&gt;first systems were introduced fifteen years ago. Currently proposed third generation (3G) systems offer roaming, higher capacity, and increased exibility for low bandwidth services, such as voice calls and messaging. While future wireless systems (fourth generation (4G) and higher) are envisioned to provide higher capacity, they are also expected to support heterogeneity in the following aspects: (i) physical environments (ranging from high-bandwidth low error-rate pico-cells to low-bandwidth high error-rate macro-cells); (ii) network architectures (cellular, peer-to-peer, or other hybrid network models); and (iii) applications (ranging from low-data rate non-real-time applications to high-data rate real-time applications). In this project, we will investigate several key elements that are necessary to realize a protocol stack for a mobile station in 4G wireless systems. The ultimate objective is to develop an adaptive protocol suite that would adapt itself to the different aspects of heterogeneity exhibited by the next generation wireless systems. In particular we willinvestigate the following problems:&lt;br/&gt;1. Application Layer&lt;br/&gt;Error-rate scalability: We propose an adaptive multiple description algorithm for error-rate scalability.&lt;br/&gt;Data-rate scalability: We introduce an optimal dynamic rate shaping technique to be utilized in the case of&lt;br/&gt;congestion or bandwidth variations.&lt;br/&gt;Jointly optimal real-time video streaming system: We propose a real-time video streaming system integrates the proposed unequal error protection mechanism, the multiple description coding technique, and the error-concealment method.&lt;br/&gt;2. Security&lt;br/&gt;An adaptive and scalable security protocols: We propose to employ finite-field wavelets to develop novel and&lt;br/&gt;innovative public key security techniques that address ubiquity, scalability, mobility, and usability.&lt;br/&gt;We propose efficient security protocols for mobile devices with limited processing power.&lt;br/&gt;3. Transport Layer&lt;br/&gt;Heterogeneous Packet Flows: We proposetoinvestigate transport protocols that can support a much richer&lt;br/&gt;set of semantics, adapting to the reliability semantics chosen by applications on a per-frame basis.&lt;br/&gt;Heterogeneous Network Characteristics: We propose to build a single transport protocol that will adapt itself&lt;br/&gt;to the characteristics of a variety of wireless network environments.&lt;br/&gt;Heterogeneous Network Architectures: We propose to explore transport layer adaptivity to the underlying&lt;br/&gt;network model (cellular, peer-to-peer, or hybrid).&lt;br/&gt;4. Network Layer&lt;br/&gt;Application Requirements: We will explore network layer constructs that enable heterogeneous applications&lt;br/&gt;to implement custom network layer policies.&lt;br/&gt;Heterogeneous Network Architectures: We will explore the use of novel state propagation schemes and virtual&lt;br/&gt;backbone approaches to enhance transport layer performance and help in adapting to heterogeneous network&lt;br/&gt;models.&lt;br/&gt;5. Data Link Layer&lt;br/&gt;Rate-compatible error control coding: We develop new types of rate-compatible convolutional codes using&lt;br/&gt;finite-field wavelet transforms.&lt;br/&gt; The protocol suite requires a new Protocol Suite Integration Plan. The project will consist of four phases. During the first phase we will design the details of all the proposed components/protocols/methods of the protocol suite. During the second phase we will assess their performance in a simulation testbed. During the third phase we will integrate the individual modules into the suite and finally in the fourth phase we willprototype the proposed protocol suite and we will assess its performance on a physical testbed.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">117840</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">117840</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n282" target="n1204">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">An Adaptive Protocol Suite for The Next Generation Wireless Internet</data>
      <data key="e_abstract">The technology and business of wireless communications systems have madespectacular progress since the&lt;br/&gt;first systems were introduced fifteen years ago. Currently proposed third generation (3G) systems offer roaming, higher capacity, and increased exibility for low bandwidth services, such as voice calls and messaging. While future wireless systems (fourth generation (4G) and higher) are envisioned to provide higher capacity, they are also expected to support heterogeneity in the following aspects: (i) physical environments (ranging from high-bandwidth low error-rate pico-cells to low-bandwidth high error-rate macro-cells); (ii) network architectures (cellular, peer-to-peer, or other hybrid network models); and (iii) applications (ranging from low-data rate non-real-time applications to high-data rate real-time applications). In this project, we will investigate several key elements that are necessary to realize a protocol stack for a mobile station in 4G wireless systems. The ultimate objective is to develop an adaptive protocol suite that would adapt itself to the different aspects of heterogeneity exhibited by the next generation wireless systems. In particular we willinvestigate the following problems:&lt;br/&gt;1. Application Layer&lt;br/&gt;Error-rate scalability: We propose an adaptive multiple description algorithm for error-rate scalability.&lt;br/&gt;Data-rate scalability: We introduce an optimal dynamic rate shaping technique to be utilized in the case of&lt;br/&gt;congestion or bandwidth variations.&lt;br/&gt;Jointly optimal real-time video streaming system: We propose a real-time video streaming system integrates the proposed unequal error protection mechanism, the multiple description coding technique, and the error-concealment method.&lt;br/&gt;2. Security&lt;br/&gt;An adaptive and scalable security protocols: We propose to employ finite-field wavelets to develop novel and&lt;br/&gt;innovative public key security techniques that address ubiquity, scalability, mobility, and usability.&lt;br/&gt;We propose efficient security protocols for mobile devices with limited processing power.&lt;br/&gt;3. Transport Layer&lt;br/&gt;Heterogeneous Packet Flows: We proposetoinvestigate transport protocols that can support a much richer&lt;br/&gt;set of semantics, adapting to the reliability semantics chosen by applications on a per-frame basis.&lt;br/&gt;Heterogeneous Network Characteristics: We propose to build a single transport protocol that will adapt itself&lt;br/&gt;to the characteristics of a variety of wireless network environments.&lt;br/&gt;Heterogeneous Network Architectures: We propose to explore transport layer adaptivity to the underlying&lt;br/&gt;network model (cellular, peer-to-peer, or hybrid).&lt;br/&gt;4. Network Layer&lt;br/&gt;Application Requirements: We will explore network layer constructs that enable heterogeneous applications&lt;br/&gt;to implement custom network layer policies.&lt;br/&gt;Heterogeneous Network Architectures: We will explore the use of novel state propagation schemes and virtual&lt;br/&gt;backbone approaches to enhance transport layer performance and help in adapting to heterogeneous network&lt;br/&gt;models.&lt;br/&gt;5. Data Link Layer&lt;br/&gt;Rate-compatible error control coding: We develop new types of rate-compatible convolutional codes using&lt;br/&gt;finite-field wavelet transforms.&lt;br/&gt; The protocol suite requires a new Protocol Suite Integration Plan. The project will consist of four phases. During the first phase we will design the details of all the proposed components/protocols/methods of the protocol suite. During the second phase we will assess their performance in a simulation testbed. During the third phase we will integrate the individual modules into the suite and finally in the fourth phase we willprototype the proposed protocol suite and we will assess its performance on a physical testbed.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">117840</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">117840</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n644" target="n1204">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">An Adaptive Protocol Suite for The Next Generation Wireless Internet</data>
      <data key="e_abstract">The technology and business of wireless communications systems have madespectacular progress since the&lt;br/&gt;first systems were introduced fifteen years ago. Currently proposed third generation (3G) systems offer roaming, higher capacity, and increased exibility for low bandwidth services, such as voice calls and messaging. While future wireless systems (fourth generation (4G) and higher) are envisioned to provide higher capacity, they are also expected to support heterogeneity in the following aspects: (i) physical environments (ranging from high-bandwidth low error-rate pico-cells to low-bandwidth high error-rate macro-cells); (ii) network architectures (cellular, peer-to-peer, or other hybrid network models); and (iii) applications (ranging from low-data rate non-real-time applications to high-data rate real-time applications). In this project, we will investigate several key elements that are necessary to realize a protocol stack for a mobile station in 4G wireless systems. The ultimate objective is to develop an adaptive protocol suite that would adapt itself to the different aspects of heterogeneity exhibited by the next generation wireless systems. In particular we willinvestigate the following problems:&lt;br/&gt;1. Application Layer&lt;br/&gt;Error-rate scalability: We propose an adaptive multiple description algorithm for error-rate scalability.&lt;br/&gt;Data-rate scalability: We introduce an optimal dynamic rate shaping technique to be utilized in the case of&lt;br/&gt;congestion or bandwidth variations.&lt;br/&gt;Jointly optimal real-time video streaming system: We propose a real-time video streaming system integrates the proposed unequal error protection mechanism, the multiple description coding technique, and the error-concealment method.&lt;br/&gt;2. Security&lt;br/&gt;An adaptive and scalable security protocols: We propose to employ finite-field wavelets to develop novel and&lt;br/&gt;innovative public key security techniques that address ubiquity, scalability, mobility, and usability.&lt;br/&gt;We propose efficient security protocols for mobile devices with limited processing power.&lt;br/&gt;3. Transport Layer&lt;br/&gt;Heterogeneous Packet Flows: We proposetoinvestigate transport protocols that can support a much richer&lt;br/&gt;set of semantics, adapting to the reliability semantics chosen by applications on a per-frame basis.&lt;br/&gt;Heterogeneous Network Characteristics: We propose to build a single transport protocol that will adapt itself&lt;br/&gt;to the characteristics of a variety of wireless network environments.&lt;br/&gt;Heterogeneous Network Architectures: We propose to explore transport layer adaptivity to the underlying&lt;br/&gt;network model (cellular, peer-to-peer, or hybrid).&lt;br/&gt;4. Network Layer&lt;br/&gt;Application Requirements: We will explore network layer constructs that enable heterogeneous applications&lt;br/&gt;to implement custom network layer policies.&lt;br/&gt;Heterogeneous Network Architectures: We will explore the use of novel state propagation schemes and virtual&lt;br/&gt;backbone approaches to enhance transport layer performance and help in adapting to heterogeneous network&lt;br/&gt;models.&lt;br/&gt;5. Data Link Layer&lt;br/&gt;Rate-compatible error control coding: We develop new types of rate-compatible convolutional codes using&lt;br/&gt;finite-field wavelet transforms.&lt;br/&gt; The protocol suite requires a new Protocol Suite Integration Plan. The project will consist of four phases. During the first phase we will design the details of all the proposed components/protocols/methods of the protocol suite. During the second phase we will assess their performance in a simulation testbed. During the third phase we will integrate the individual modules into the suite and finally in the fourth phase we willprototype the proposed protocol suite and we will assess its performance on a physical testbed.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">117840</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">117840</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1205" target="n1206">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Reading a Balanced Diet: Foraging in Information Communities</data>
      <data key="e_abstract">This project explores the role of balanced information diets in information filtering. Information filtering is an increasingly important response to the problem of information overload. Community-based information filters are particularly beneficial because they can they can strengthen communities and recognize community tastes. The goal of this project is to investigate the benefits that can be realized by having the information filter balance the information presented to the user across the available topics. The project will explore how people forage in information abundance, what interfaces best support balanced information diets, and how much users like balanced information filters. The results will be interfaces to information through which users will receive the right amount of information on each topic, according to their interest in that topic. The information filter will learn which of the disparate sources of information are best for which topics, and present articles from the best sources. The research will also explore the effect of balanced information filtering on communities. The result will be balanced information filters that strengthen communities by ensuring sufficient overlap in information diets among community members.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">102229</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">102229</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1208" target="n1209">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Educational Innovation: An Investigation into Aesthetic Computing within the Digital Arts and Sciences Curricula</data>
      <data key="e_abstract">Abstract 0119532 Paul Fishwick University of Florida&lt;br/&gt;Title: An Investigation into Aesthetic Computing Within the Digital Arts and Sciences Curricula&lt;br/&gt;&lt;br/&gt;This project integrates research in &quot;aesthetic computing&quot; directly into a new set of CISE programs, collectively called Digital Arts and Sciences (DAS). The thrust of DAS is to create a student who is endowed with a hybrid-knowledge of computer engineering and the arts. This enables the student to work effectively in production-oriented teams centered on projects involving videos, interactive games, education, scientific and engineering visualization, and software engineering. The research integration is accomplished through the addition of an Aesthetic Computing course and a series of Digital World Production Studio courses. Both Fine Arts as well as CISE students take these courses, and the PIs team-teach the studio course. Aesthetic Computing refers to the use of genres and styles in fine art employed as metaphors for formal and diagrammatically rendered model structures commonly found in computing, such as automata, data flow graphs, data models and the comprehensive Unified Modeling Language (UML). This work builds upon many areas outside of CS such as semiotics, linguistics, analogy, metaphor, and the arts. Ongoing progress in the information and program visualization literature is most related; however, the project&apos;s focus is on a stronger arts component that both personalizes and enriches the user&apos;s modeling interface. For example, the representation of a finite state machine may be crafted, through metaphor mapping, to a scale or virtual model of a building. The style of the building can be taken from the huge variety of possible genres existing in architecture, without limiting the representation to abstract entities. And, elements of music and story schemata can be simultaneously mapped onto the architecture, further personalizing the interface. The goal of the project is to develop practitioners who understand both the formalisms of visualization and the practical aspects of human communication that deal with aesthetic interpretation.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">119532</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">119532</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1208" target="n1210">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Educational Innovation: An Investigation into Aesthetic Computing within the Digital Arts and Sciences Curricula</data>
      <data key="e_abstract">Abstract 0119532 Paul Fishwick University of Florida&lt;br/&gt;Title: An Investigation into Aesthetic Computing Within the Digital Arts and Sciences Curricula&lt;br/&gt;&lt;br/&gt;This project integrates research in &quot;aesthetic computing&quot; directly into a new set of CISE programs, collectively called Digital Arts and Sciences (DAS). The thrust of DAS is to create a student who is endowed with a hybrid-knowledge of computer engineering and the arts. This enables the student to work effectively in production-oriented teams centered on projects involving videos, interactive games, education, scientific and engineering visualization, and software engineering. The research integration is accomplished through the addition of an Aesthetic Computing course and a series of Digital World Production Studio courses. Both Fine Arts as well as CISE students take these courses, and the PIs team-teach the studio course. Aesthetic Computing refers to the use of genres and styles in fine art employed as metaphors for formal and diagrammatically rendered model structures commonly found in computing, such as automata, data flow graphs, data models and the comprehensive Unified Modeling Language (UML). This work builds upon many areas outside of CS such as semiotics, linguistics, analogy, metaphor, and the arts. Ongoing progress in the information and program visualization literature is most related; however, the project&apos;s focus is on a stronger arts component that both personalizes and enriches the user&apos;s modeling interface. For example, the representation of a finite state machine may be crafted, through metaphor mapping, to a scale or virtual model of a building. The style of the building can be taken from the huge variety of possible genres existing in architecture, without limiting the representation to abstract entities. And, elements of music and story schemata can be simultaneously mapped onto the architecture, further personalizing the interface. The goal of the project is to develop practitioners who understand both the formalisms of visualization and the practical aspects of human communication that deal with aesthetic interpretation.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">119532</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">119532</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1209" target="n1210">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Educational Innovation: An Investigation into Aesthetic Computing within the Digital Arts and Sciences Curricula</data>
      <data key="e_abstract">Abstract 0119532 Paul Fishwick University of Florida&lt;br/&gt;Title: An Investigation into Aesthetic Computing Within the Digital Arts and Sciences Curricula&lt;br/&gt;&lt;br/&gt;This project integrates research in &quot;aesthetic computing&quot; directly into a new set of CISE programs, collectively called Digital Arts and Sciences (DAS). The thrust of DAS is to create a student who is endowed with a hybrid-knowledge of computer engineering and the arts. This enables the student to work effectively in production-oriented teams centered on projects involving videos, interactive games, education, scientific and engineering visualization, and software engineering. The research integration is accomplished through the addition of an Aesthetic Computing course and a series of Digital World Production Studio courses. Both Fine Arts as well as CISE students take these courses, and the PIs team-teach the studio course. Aesthetic Computing refers to the use of genres and styles in fine art employed as metaphors for formal and diagrammatically rendered model structures commonly found in computing, such as automata, data flow graphs, data models and the comprehensive Unified Modeling Language (UML). This work builds upon many areas outside of CS such as semiotics, linguistics, analogy, metaphor, and the arts. Ongoing progress in the information and program visualization literature is most related; however, the project&apos;s focus is on a stronger arts component that both personalizes and enriches the user&apos;s modeling interface. For example, the representation of a finite state machine may be crafted, through metaphor mapping, to a scale or virtual model of a building. The style of the building can be taken from the huge variety of possible genres existing in architecture, without limiting the representation to abstract entities. And, elements of music and story schemata can be simultaneously mapped onto the architecture, further personalizing the interface. The goal of the project is to develop practitioners who understand both the formalisms of visualization and the practical aspects of human communication that deal with aesthetic interpretation.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">119532</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">119532</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1211" target="n1212">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Computer System Design Hierarchy for Simulation</data>
      <data key="e_abstract">EIA-0103706&lt;br/&gt;Donald E. Thomas&lt;br/&gt;Carnegie Mellon University&lt;br/&gt;&lt;br/&gt;Next Generation Software: A Computer System Design Hierarchy for Simulation&lt;br/&gt;&lt;br/&gt;Computer system design requires reasoning about the interaction of software with the underlying models of hardware resources without limiting such models to single system views. Computer system elements and their interactions are not purely software-functional or purely hardware-structural-they have properties of both. A challenge in computer system design is to define the elements and the means of resolving their interactions, so that software interacting with underlying hardware resources can be modeled flexibly; alternate designs, ones with different numbers and types of processors, network interconnections, and/or software schedulers, may be easily considered.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103706</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103706</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n527" target="n1213">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Acquisition of a Computing Cluster and Access Grid Node for Information Technology Research</data>
      <data key="e_abstract">EIA-0130673 &lt;br/&gt;Xian-He Sun&lt;br/&gt;Illinois Institute of Technology&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Acquisition of a Computing Cluster and Access Grid Node for Information Technology Research&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at the Illinois Institute of Technology (IIT) will purchase an 82 node Linux cluster, an Access Grid node, and several workstations, all of which will be connected via a high-speed network. These computational resources will be dedicated to the support of several on-going research projects in the area of information technology. Such projects include the development of a process migration environment for heterogeneous distributed computing, design and implementation of scalable information systems, scalable distributed simulation in support of complex systems management, and scalable parallel numerical algorithms and simulations. These projects require significant computing power and high-performance networks. Such resources are not currently available within the university. The acquisition of these resources through the NSF CISE Instrumentation program is critical to the success of these research projects, and to the development of a strong computer science program at IIT. Additionally, the requested equipment will offer an excellent platform upon which students can learn about and experience state-of-the-art research via course work and term projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130673</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130673</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1213" target="n1215">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Acquisition of a Computing Cluster and Access Grid Node for Information Technology Research</data>
      <data key="e_abstract">EIA-0130673 &lt;br/&gt;Xian-He Sun&lt;br/&gt;Illinois Institute of Technology&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Acquisition of a Computing Cluster and Access Grid Node for Information Technology Research&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at the Illinois Institute of Technology (IIT) will purchase an 82 node Linux cluster, an Access Grid node, and several workstations, all of which will be connected via a high-speed network. These computational resources will be dedicated to the support of several on-going research projects in the area of information technology. Such projects include the development of a process migration environment for heterogeneous distributed computing, design and implementation of scalable information systems, scalable distributed simulation in support of complex systems management, and scalable parallel numerical algorithms and simulations. These projects require significant computing power and high-performance networks. Such resources are not currently available within the university. The acquisition of these resources through the NSF CISE Instrumentation program is critical to the success of these research projects, and to the development of a strong computer science program at IIT. Additionally, the requested equipment will offer an excellent platform upon which students can learn about and experience state-of-the-art research via course work and term projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130673</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130673</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1213" target="n1216">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Acquisition of a Computing Cluster and Access Grid Node for Information Technology Research</data>
      <data key="e_abstract">EIA-0130673 &lt;br/&gt;Xian-He Sun&lt;br/&gt;Illinois Institute of Technology&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Acquisition of a Computing Cluster and Access Grid Node for Information Technology Research&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at the Illinois Institute of Technology (IIT) will purchase an 82 node Linux cluster, an Access Grid node, and several workstations, all of which will be connected via a high-speed network. These computational resources will be dedicated to the support of several on-going research projects in the area of information technology. Such projects include the development of a process migration environment for heterogeneous distributed computing, design and implementation of scalable information systems, scalable distributed simulation in support of complex systems management, and scalable parallel numerical algorithms and simulations. These projects require significant computing power and high-performance networks. Such resources are not currently available within the university. The acquisition of these resources through the NSF CISE Instrumentation program is critical to the success of these research projects, and to the development of a strong computer science program at IIT. Additionally, the requested equipment will offer an excellent platform upon which students can learn about and experience state-of-the-art research via course work and term projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130673</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130673</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1213" target="n1217">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Acquisition of a Computing Cluster and Access Grid Node for Information Technology Research</data>
      <data key="e_abstract">EIA-0130673 &lt;br/&gt;Xian-He Sun&lt;br/&gt;Illinois Institute of Technology&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Acquisition of a Computing Cluster and Access Grid Node for Information Technology Research&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at the Illinois Institute of Technology (IIT) will purchase an 82 node Linux cluster, an Access Grid node, and several workstations, all of which will be connected via a high-speed network. These computational resources will be dedicated to the support of several on-going research projects in the area of information technology. Such projects include the development of a process migration environment for heterogeneous distributed computing, design and implementation of scalable information systems, scalable distributed simulation in support of complex systems management, and scalable parallel numerical algorithms and simulations. These projects require significant computing power and high-performance networks. Such resources are not currently available within the university. The acquisition of these resources through the NSF CISE Instrumentation program is critical to the success of these research projects, and to the development of a strong computer science program at IIT. Additionally, the requested equipment will offer an excellent platform upon which students can learn about and experience state-of-the-art research via course work and term projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130673</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130673</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n527" target="n1215">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Acquisition of a Computing Cluster and Access Grid Node for Information Technology Research</data>
      <data key="e_abstract">EIA-0130673 &lt;br/&gt;Xian-He Sun&lt;br/&gt;Illinois Institute of Technology&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Acquisition of a Computing Cluster and Access Grid Node for Information Technology Research&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at the Illinois Institute of Technology (IIT) will purchase an 82 node Linux cluster, an Access Grid node, and several workstations, all of which will be connected via a high-speed network. These computational resources will be dedicated to the support of several on-going research projects in the area of information technology. Such projects include the development of a process migration environment for heterogeneous distributed computing, design and implementation of scalable information systems, scalable distributed simulation in support of complex systems management, and scalable parallel numerical algorithms and simulations. These projects require significant computing power and high-performance networks. Such resources are not currently available within the university. The acquisition of these resources through the NSF CISE Instrumentation program is critical to the success of these research projects, and to the development of a strong computer science program at IIT. Additionally, the requested equipment will offer an excellent platform upon which students can learn about and experience state-of-the-art research via course work and term projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130673</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130673</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n527" target="n1216">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Acquisition of a Computing Cluster and Access Grid Node for Information Technology Research</data>
      <data key="e_abstract">EIA-0130673 &lt;br/&gt;Xian-He Sun&lt;br/&gt;Illinois Institute of Technology&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Acquisition of a Computing Cluster and Access Grid Node for Information Technology Research&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at the Illinois Institute of Technology (IIT) will purchase an 82 node Linux cluster, an Access Grid node, and several workstations, all of which will be connected via a high-speed network. These computational resources will be dedicated to the support of several on-going research projects in the area of information technology. Such projects include the development of a process migration environment for heterogeneous distributed computing, design and implementation of scalable information systems, scalable distributed simulation in support of complex systems management, and scalable parallel numerical algorithms and simulations. These projects require significant computing power and high-performance networks. Such resources are not currently available within the university. The acquisition of these resources through the NSF CISE Instrumentation program is critical to the success of these research projects, and to the development of a strong computer science program at IIT. Additionally, the requested equipment will offer an excellent platform upon which students can learn about and experience state-of-the-art research via course work and term projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130673</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130673</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n527" target="n1217">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Acquisition of a Computing Cluster and Access Grid Node for Information Technology Research</data>
      <data key="e_abstract">EIA-0130673 &lt;br/&gt;Xian-He Sun&lt;br/&gt;Illinois Institute of Technology&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Acquisition of a Computing Cluster and Access Grid Node for Information Technology Research&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at the Illinois Institute of Technology (IIT) will purchase an 82 node Linux cluster, an Access Grid node, and several workstations, all of which will be connected via a high-speed network. These computational resources will be dedicated to the support of several on-going research projects in the area of information technology. Such projects include the development of a process migration environment for heterogeneous distributed computing, design and implementation of scalable information systems, scalable distributed simulation in support of complex systems management, and scalable parallel numerical algorithms and simulations. These projects require significant computing power and high-performance networks. Such resources are not currently available within the university. The acquisition of these resources through the NSF CISE Instrumentation program is critical to the success of these research projects, and to the development of a strong computer science program at IIT. Additionally, the requested equipment will offer an excellent platform upon which students can learn about and experience state-of-the-art research via course work and term projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130673</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130673</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1215" target="n1216">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Acquisition of a Computing Cluster and Access Grid Node for Information Technology Research</data>
      <data key="e_abstract">EIA-0130673 &lt;br/&gt;Xian-He Sun&lt;br/&gt;Illinois Institute of Technology&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Acquisition of a Computing Cluster and Access Grid Node for Information Technology Research&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at the Illinois Institute of Technology (IIT) will purchase an 82 node Linux cluster, an Access Grid node, and several workstations, all of which will be connected via a high-speed network. These computational resources will be dedicated to the support of several on-going research projects in the area of information technology. Such projects include the development of a process migration environment for heterogeneous distributed computing, design and implementation of scalable information systems, scalable distributed simulation in support of complex systems management, and scalable parallel numerical algorithms and simulations. These projects require significant computing power and high-performance networks. Such resources are not currently available within the university. The acquisition of these resources through the NSF CISE Instrumentation program is critical to the success of these research projects, and to the development of a strong computer science program at IIT. Additionally, the requested equipment will offer an excellent platform upon which students can learn about and experience state-of-the-art research via course work and term projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130673</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130673</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1215" target="n1217">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Acquisition of a Computing Cluster and Access Grid Node for Information Technology Research</data>
      <data key="e_abstract">EIA-0130673 &lt;br/&gt;Xian-He Sun&lt;br/&gt;Illinois Institute of Technology&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Acquisition of a Computing Cluster and Access Grid Node for Information Technology Research&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at the Illinois Institute of Technology (IIT) will purchase an 82 node Linux cluster, an Access Grid node, and several workstations, all of which will be connected via a high-speed network. These computational resources will be dedicated to the support of several on-going research projects in the area of information technology. Such projects include the development of a process migration environment for heterogeneous distributed computing, design and implementation of scalable information systems, scalable distributed simulation in support of complex systems management, and scalable parallel numerical algorithms and simulations. These projects require significant computing power and high-performance networks. Such resources are not currently available within the university. The acquisition of these resources through the NSF CISE Instrumentation program is critical to the success of these research projects, and to the development of a strong computer science program at IIT. Additionally, the requested equipment will offer an excellent platform upon which students can learn about and experience state-of-the-art research via course work and term projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130673</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130673</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1216" target="n1217">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Acquisition of a Computing Cluster and Access Grid Node for Information Technology Research</data>
      <data key="e_abstract">EIA-0130673 &lt;br/&gt;Xian-He Sun&lt;br/&gt;Illinois Institute of Technology&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Acquisition of a Computing Cluster and Access Grid Node for Information Technology Research&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at the Illinois Institute of Technology (IIT) will purchase an 82 node Linux cluster, an Access Grid node, and several workstations, all of which will be connected via a high-speed network. These computational resources will be dedicated to the support of several on-going research projects in the area of information technology. Such projects include the development of a process migration environment for heterogeneous distributed computing, design and implementation of scalable information systems, scalable distributed simulation in support of complex systems management, and scalable parallel numerical algorithms and simulations. These projects require significant computing power and high-performance networks. Such resources are not currently available within the university. The acquisition of these resources through the NSF CISE Instrumentation program is critical to the success of these research projects, and to the development of a strong computer science program at IIT. Additionally, the requested equipment will offer an excellent platform upon which students can learn about and experience state-of-the-art research via course work and term projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130673</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130673</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1219" target="n1220">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: Performance Mining of Large-Scale Data-Intensive Distributed Object Applications</data>
      <data key="e_abstract">EIA-0103708&lt;br/&gt;Mohammed J. Zaki&lt;br/&gt;Rensselaer Polytechnic Institute&lt;br/&gt;&lt;br/&gt;NGS Performance Mining of large-scale Data-Intensive Distributed Object Applications&lt;br/&gt;&lt;br/&gt;The objective of the proposal is to develop a performance measurements-based run-time environment for supporting large data-intensive distributed object applications. The system will provide continuous and adaptive performance optimization via a combination of performance data mining, critical path discovery and speculative execution.&lt;br/&gt;&lt;br/&gt;To address these challenges for next generation software systems we propose to develop the PERFMINER engine for the performance mining. PERFMINER, a system for continuous performance optimization via mining, will enable a distributed object system to: 1) discover its own critical path, 2) detect new opportunities for speculative processing, and 3) to facilitate modifying an object&apos;s behaviors (i.e., methods) at run-time in response to newly acquired knowledge.</data>
      <data key="e_pgm">H228</data>
      <data key="e_label">103708</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">103708</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n731" target="n1219">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: Performance Mining of Large-Scale Data-Intensive Distributed Object Applications</data>
      <data key="e_abstract">EIA-0103708&lt;br/&gt;Mohammed J. Zaki&lt;br/&gt;Rensselaer Polytechnic Institute&lt;br/&gt;&lt;br/&gt;NGS Performance Mining of large-scale Data-Intensive Distributed Object Applications&lt;br/&gt;&lt;br/&gt;The objective of the proposal is to develop a performance measurements-based run-time environment for supporting large data-intensive distributed object applications. The system will provide continuous and adaptive performance optimization via a combination of performance data mining, critical path discovery and speculative execution.&lt;br/&gt;&lt;br/&gt;To address these challenges for next generation software systems we propose to develop the PERFMINER engine for the performance mining. PERFMINER, a system for continuous performance optimization via mining, will enable a distributed object system to: 1) discover its own critical path, 2) detect new opportunities for speculative processing, and 3) to facilitate modifying an object&apos;s behaviors (i.e., methods) at run-time in response to newly acquired knowledge.</data>
      <data key="e_pgm">H228</data>
      <data key="e_label">103708</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">103708</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n731" target="n1220">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: Performance Mining of Large-Scale Data-Intensive Distributed Object Applications</data>
      <data key="e_abstract">EIA-0103708&lt;br/&gt;Mohammed J. Zaki&lt;br/&gt;Rensselaer Polytechnic Institute&lt;br/&gt;&lt;br/&gt;NGS Performance Mining of large-scale Data-Intensive Distributed Object Applications&lt;br/&gt;&lt;br/&gt;The objective of the proposal is to develop a performance measurements-based run-time environment for supporting large data-intensive distributed object applications. The system will provide continuous and adaptive performance optimization via a combination of performance data mining, critical path discovery and speculative execution.&lt;br/&gt;&lt;br/&gt;To address these challenges for next generation software systems we propose to develop the PERFMINER engine for the performance mining. PERFMINER, a system for continuous performance optimization via mining, will enable a distributed object system to: 1) discover its own critical path, 2) detect new opportunities for speculative processing, and 3) to facilitate modifying an object&apos;s behaviors (i.e., methods) at run-time in response to newly acquired knowledge.</data>
      <data key="e_pgm">H228</data>
      <data key="e_label">103708</data>
      <data key="e_expirationDate">2010-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">103708</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1222" target="n1223">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Robust Adaptive Network-Attached Storage</data>
      <data key="e_abstract">The basic elements of the storage industry are in the midst of radical change&lt;br/&gt;with the advent of network-attached storage devices. Storage systems comprised&lt;br/&gt;of network-attached drives provide many potential advantages over traditional&lt;br/&gt;storage architectures, but also introduce additional challenges, particularly&lt;br/&gt;regarding manageability. In this proposal, the WiND project (Wisconsin&lt;br/&gt;Network Disks) is described, which has the goal of developing the software&lt;br/&gt;techniques required to build a truly manageable network-attached storage&lt;br/&gt;system.&lt;br/&gt;&lt;br/&gt;The key to manageability is adaptation. In traditional systems, such&lt;br/&gt;adaptation is performed by a human administrator. Future storage systems must&lt;br/&gt;themselves adapt, and in doing so, reduce the need for manual&lt;br/&gt;intervention. The WiND system will gracefully and efficiently adapt to changes&lt;br/&gt;in the environment, reducing the burden of administration and increasing the&lt;br/&gt;flexibility of storage for an eclectic range of clients. In particular, WiND&lt;br/&gt;will automatically handle the addition of new heterogeneous disks to the&lt;br/&gt;system, the failure of existing disks, and changes in client workload. Within&lt;br/&gt;this proposal, three specific sub-areas of WiND are developed: adaptive data&lt;br/&gt;layout and access with SToRM, adaptive caching via Clouds, and the underlying&lt;br/&gt;information substrate.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">98274</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">98274</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1224" target="n1225">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Self-Timed FPGA Systems</data>
      <data key="e_abstract">This research proposes a new generation of Field Programmable Gate Arrays (FPGAs) based upon a self-timed design methodology known as Phased Logic. One of the principle attractions of FPGAs is that they&lt;br/&gt;give users a streamlined methodology for implementing large gate-count digital designs that are specified in a Hardware Description Language (HDL) such as Verilog. FPGAs shield designers from the time-consuming&lt;br/&gt;physical design details that Application Specific Integrated Circuit (ASICs) designers must face and offer a flexible implementation substrate with a quicker time to market. However, design complexity is increasing significantly for both FPGA designers and users due to timing issues related to global clock distribution over larger arrays operating at higher frequencies. Spending more design effort on reaching timing closure can increase time to market and threatens to undermine one of the key benefits of FPGAs for its users.&lt;br/&gt;&lt;br/&gt;Phased Logic (PL) is a self-timed, delay-insensitive methodology that allows automatic mapping of clocked netlists to netlists of PL gates. Preliminary work has indicated that PL gates based upon a four input&lt;br/&gt;LookUp-Table (LUT4) can implement designs that are competitive with clocked approaches in both power and performance. This research investigates new FPGA architectures using both LUT4-based gates and&lt;br/&gt;traditional product-term-based gates. PL offers a general capability for data dependent computing; synthesis techniques that take advantage of this for general logic are investigated. Extensions for supporting these new architectures and PL gate designs are made to the current mapping tool that transforms clocked designs to PL designs. Tradeoffs that sacrifice some delay insensitivity for extra performance are studied.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">98272</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98272</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1224" target="n1226">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Self-Timed FPGA Systems</data>
      <data key="e_abstract">This research proposes a new generation of Field Programmable Gate Arrays (FPGAs) based upon a self-timed design methodology known as Phased Logic. One of the principle attractions of FPGAs is that they&lt;br/&gt;give users a streamlined methodology for implementing large gate-count digital designs that are specified in a Hardware Description Language (HDL) such as Verilog. FPGAs shield designers from the time-consuming&lt;br/&gt;physical design details that Application Specific Integrated Circuit (ASICs) designers must face and offer a flexible implementation substrate with a quicker time to market. However, design complexity is increasing significantly for both FPGA designers and users due to timing issues related to global clock distribution over larger arrays operating at higher frequencies. Spending more design effort on reaching timing closure can increase time to market and threatens to undermine one of the key benefits of FPGAs for its users.&lt;br/&gt;&lt;br/&gt;Phased Logic (PL) is a self-timed, delay-insensitive methodology that allows automatic mapping of clocked netlists to netlists of PL gates. Preliminary work has indicated that PL gates based upon a four input&lt;br/&gt;LookUp-Table (LUT4) can implement designs that are competitive with clocked approaches in both power and performance. This research investigates new FPGA architectures using both LUT4-based gates and&lt;br/&gt;traditional product-term-based gates. PL offers a general capability for data dependent computing; synthesis techniques that take advantage of this for general logic are investigated. Extensions for supporting these new architectures and PL gate designs are made to the current mapping tool that transforms clocked designs to PL designs. Tradeoffs that sacrifice some delay insensitivity for extra performance are studied.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">98272</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98272</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1225" target="n1226">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Self-Timed FPGA Systems</data>
      <data key="e_abstract">This research proposes a new generation of Field Programmable Gate Arrays (FPGAs) based upon a self-timed design methodology known as Phased Logic. One of the principle attractions of FPGAs is that they&lt;br/&gt;give users a streamlined methodology for implementing large gate-count digital designs that are specified in a Hardware Description Language (HDL) such as Verilog. FPGAs shield designers from the time-consuming&lt;br/&gt;physical design details that Application Specific Integrated Circuit (ASICs) designers must face and offer a flexible implementation substrate with a quicker time to market. However, design complexity is increasing significantly for both FPGA designers and users due to timing issues related to global clock distribution over larger arrays operating at higher frequencies. Spending more design effort on reaching timing closure can increase time to market and threatens to undermine one of the key benefits of FPGAs for its users.&lt;br/&gt;&lt;br/&gt;Phased Logic (PL) is a self-timed, delay-insensitive methodology that allows automatic mapping of clocked netlists to netlists of PL gates. Preliminary work has indicated that PL gates based upon a four input&lt;br/&gt;LookUp-Table (LUT4) can implement designs that are competitive with clocked approaches in both power and performance. This research investigates new FPGA architectures using both LUT4-based gates and&lt;br/&gt;traditional product-term-based gates. PL offers a general capability for data dependent computing; synthesis techniques that take advantage of this for general logic are investigated. Extensions for supporting these new architectures and PL gate designs are made to the current mapping tool that transforms clocked designs to PL designs. Tradeoffs that sacrifice some delay insensitivity for extra performance are studied.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">98272</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98272</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1227" target="n1228">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: System and Compiler Support for Component-Based Construction of Scalable Internet Services</data>
      <data key="e_abstract">EIA-0103722&lt;br/&gt;Thu D. Nguyen&lt;br/&gt;Rutgers University&lt;br/&gt;&lt;br/&gt;System and Compiler Support for Component-Based Construction of Scalable Internet Services&lt;br/&gt;&lt;br/&gt;The principal investigators propose to investigate system and compiler support for an emerging class of Scalable Internet Service (SIS) applications. This proposed work will be motivated by the emergence of the Internet as the global, ubiquitous networking infrastructure, and its accompanying computing model where much of the computing takes place on servers rather than local machines. SIS applications provide a rich set of services such as on-line auctions, stock exchanges, and instant messaging to diverse clients worldwide.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103722</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103722</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1227" target="n1229">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: System and Compiler Support for Component-Based Construction of Scalable Internet Services</data>
      <data key="e_abstract">EIA-0103722&lt;br/&gt;Thu D. Nguyen&lt;br/&gt;Rutgers University&lt;br/&gt;&lt;br/&gt;System and Compiler Support for Component-Based Construction of Scalable Internet Services&lt;br/&gt;&lt;br/&gt;The principal investigators propose to investigate system and compiler support for an emerging class of Scalable Internet Service (SIS) applications. This proposed work will be motivated by the emergence of the Internet as the global, ubiquitous networking infrastructure, and its accompanying computing model where much of the computing takes place on servers rather than local machines. SIS applications provide a rich set of services such as on-line auctions, stock exchanges, and instant messaging to diverse clients worldwide.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103722</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103722</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1228" target="n1229">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: System and Compiler Support for Component-Based Construction of Scalable Internet Services</data>
      <data key="e_abstract">EIA-0103722&lt;br/&gt;Thu D. Nguyen&lt;br/&gt;Rutgers University&lt;br/&gt;&lt;br/&gt;System and Compiler Support for Component-Based Construction of Scalable Internet Services&lt;br/&gt;&lt;br/&gt;The principal investigators propose to investigate system and compiler support for an emerging class of Scalable Internet Service (SIS) applications. This proposed work will be motivated by the emergence of the Internet as the global, ubiquitous networking infrastructure, and its accompanying computing model where much of the computing takes place on servers rather than local machines. SIS applications provide a rich set of services such as on-line auctions, stock exchanges, and instant messaging to diverse clients worldwide.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103722</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103722</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1231" target="n1232">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Distributed Component Repository for Rapid Synthesis of Adaptive Real-Time Systems</data>
      <data key="e_abstract">EIA-0103709&lt;br/&gt;I-Ling Yen&lt;br/&gt;University of Texas-Dallas&lt;br/&gt;&lt;br/&gt;The PI&apos;s proposed to develop new innovative approaches and techniques to advance the component-based development ( CDB) of complex applications. The proposed effort in developing advanced CBD techniques spans three dimensions: The research will enable such capabilities by enabling ontology-based repository, code pattern based component composition and tool suite for component comprehension and customization. &lt;br/&gt; &lt;br/&gt;Technological advances, such as rapidly increasing computing power and network bandwidth, are enabling advanced high-performance distributed applications that can significantly enhance the quality of education, health care, remote monitoring and control, early warning systems, scientific collaborations, and other high-performance distributed applications.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103709</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103709</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1231" target="n1233">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Distributed Component Repository for Rapid Synthesis of Adaptive Real-Time Systems</data>
      <data key="e_abstract">EIA-0103709&lt;br/&gt;I-Ling Yen&lt;br/&gt;University of Texas-Dallas&lt;br/&gt;&lt;br/&gt;The PI&apos;s proposed to develop new innovative approaches and techniques to advance the component-based development ( CDB) of complex applications. The proposed effort in developing advanced CBD techniques spans three dimensions: The research will enable such capabilities by enabling ontology-based repository, code pattern based component composition and tool suite for component comprehension and customization. &lt;br/&gt; &lt;br/&gt;Technological advances, such as rapidly increasing computing power and network bandwidth, are enabling advanced high-performance distributed applications that can significantly enhance the quality of education, health care, remote monitoring and control, early warning systems, scientific collaborations, and other high-performance distributed applications.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103709</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103709</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1231" target="n1234">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Distributed Component Repository for Rapid Synthesis of Adaptive Real-Time Systems</data>
      <data key="e_abstract">EIA-0103709&lt;br/&gt;I-Ling Yen&lt;br/&gt;University of Texas-Dallas&lt;br/&gt;&lt;br/&gt;The PI&apos;s proposed to develop new innovative approaches and techniques to advance the component-based development ( CDB) of complex applications. The proposed effort in developing advanced CBD techniques spans three dimensions: The research will enable such capabilities by enabling ontology-based repository, code pattern based component composition and tool suite for component comprehension and customization. &lt;br/&gt; &lt;br/&gt;Technological advances, such as rapidly increasing computing power and network bandwidth, are enabling advanced high-performance distributed applications that can significantly enhance the quality of education, health care, remote monitoring and control, early warning systems, scientific collaborations, and other high-performance distributed applications.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103709</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103709</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1231" target="n1235">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Distributed Component Repository for Rapid Synthesis of Adaptive Real-Time Systems</data>
      <data key="e_abstract">EIA-0103709&lt;br/&gt;I-Ling Yen&lt;br/&gt;University of Texas-Dallas&lt;br/&gt;&lt;br/&gt;The PI&apos;s proposed to develop new innovative approaches and techniques to advance the component-based development ( CDB) of complex applications. The proposed effort in developing advanced CBD techniques spans three dimensions: The research will enable such capabilities by enabling ontology-based repository, code pattern based component composition and tool suite for component comprehension and customization. &lt;br/&gt; &lt;br/&gt;Technological advances, such as rapidly increasing computing power and network bandwidth, are enabling advanced high-performance distributed applications that can significantly enhance the quality of education, health care, remote monitoring and control, early warning systems, scientific collaborations, and other high-performance distributed applications.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103709</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103709</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1232" target="n1233">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Distributed Component Repository for Rapid Synthesis of Adaptive Real-Time Systems</data>
      <data key="e_abstract">EIA-0103709&lt;br/&gt;I-Ling Yen&lt;br/&gt;University of Texas-Dallas&lt;br/&gt;&lt;br/&gt;The PI&apos;s proposed to develop new innovative approaches and techniques to advance the component-based development ( CDB) of complex applications. The proposed effort in developing advanced CBD techniques spans three dimensions: The research will enable such capabilities by enabling ontology-based repository, code pattern based component composition and tool suite for component comprehension and customization. &lt;br/&gt; &lt;br/&gt;Technological advances, such as rapidly increasing computing power and network bandwidth, are enabling advanced high-performance distributed applications that can significantly enhance the quality of education, health care, remote monitoring and control, early warning systems, scientific collaborations, and other high-performance distributed applications.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103709</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103709</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1232" target="n1234">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Distributed Component Repository for Rapid Synthesis of Adaptive Real-Time Systems</data>
      <data key="e_abstract">EIA-0103709&lt;br/&gt;I-Ling Yen&lt;br/&gt;University of Texas-Dallas&lt;br/&gt;&lt;br/&gt;The PI&apos;s proposed to develop new innovative approaches and techniques to advance the component-based development ( CDB) of complex applications. The proposed effort in developing advanced CBD techniques spans three dimensions: The research will enable such capabilities by enabling ontology-based repository, code pattern based component composition and tool suite for component comprehension and customization. &lt;br/&gt; &lt;br/&gt;Technological advances, such as rapidly increasing computing power and network bandwidth, are enabling advanced high-performance distributed applications that can significantly enhance the quality of education, health care, remote monitoring and control, early warning systems, scientific collaborations, and other high-performance distributed applications.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103709</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103709</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1232" target="n1235">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Distributed Component Repository for Rapid Synthesis of Adaptive Real-Time Systems</data>
      <data key="e_abstract">EIA-0103709&lt;br/&gt;I-Ling Yen&lt;br/&gt;University of Texas-Dallas&lt;br/&gt;&lt;br/&gt;The PI&apos;s proposed to develop new innovative approaches and techniques to advance the component-based development ( CDB) of complex applications. The proposed effort in developing advanced CBD techniques spans three dimensions: The research will enable such capabilities by enabling ontology-based repository, code pattern based component composition and tool suite for component comprehension and customization. &lt;br/&gt; &lt;br/&gt;Technological advances, such as rapidly increasing computing power and network bandwidth, are enabling advanced high-performance distributed applications that can significantly enhance the quality of education, health care, remote monitoring and control, early warning systems, scientific collaborations, and other high-performance distributed applications.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103709</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103709</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1233" target="n1234">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Distributed Component Repository for Rapid Synthesis of Adaptive Real-Time Systems</data>
      <data key="e_abstract">EIA-0103709&lt;br/&gt;I-Ling Yen&lt;br/&gt;University of Texas-Dallas&lt;br/&gt;&lt;br/&gt;The PI&apos;s proposed to develop new innovative approaches and techniques to advance the component-based development ( CDB) of complex applications. The proposed effort in developing advanced CBD techniques spans three dimensions: The research will enable such capabilities by enabling ontology-based repository, code pattern based component composition and tool suite for component comprehension and customization. &lt;br/&gt; &lt;br/&gt;Technological advances, such as rapidly increasing computing power and network bandwidth, are enabling advanced high-performance distributed applications that can significantly enhance the quality of education, health care, remote monitoring and control, early warning systems, scientific collaborations, and other high-performance distributed applications.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103709</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103709</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1233" target="n1235">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Distributed Component Repository for Rapid Synthesis of Adaptive Real-Time Systems</data>
      <data key="e_abstract">EIA-0103709&lt;br/&gt;I-Ling Yen&lt;br/&gt;University of Texas-Dallas&lt;br/&gt;&lt;br/&gt;The PI&apos;s proposed to develop new innovative approaches and techniques to advance the component-based development ( CDB) of complex applications. The proposed effort in developing advanced CBD techniques spans three dimensions: The research will enable such capabilities by enabling ontology-based repository, code pattern based component composition and tool suite for component comprehension and customization. &lt;br/&gt; &lt;br/&gt;Technological advances, such as rapidly increasing computing power and network bandwidth, are enabling advanced high-performance distributed applications that can significantly enhance the quality of education, health care, remote monitoring and control, early warning systems, scientific collaborations, and other high-performance distributed applications.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103709</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103709</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1234" target="n1235">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Distributed Component Repository for Rapid Synthesis of Adaptive Real-Time Systems</data>
      <data key="e_abstract">EIA-0103709&lt;br/&gt;I-Ling Yen&lt;br/&gt;University of Texas-Dallas&lt;br/&gt;&lt;br/&gt;The PI&apos;s proposed to develop new innovative approaches and techniques to advance the component-based development ( CDB) of complex applications. The proposed effort in developing advanced CBD techniques spans three dimensions: The research will enable such capabilities by enabling ontology-based repository, code pattern based component composition and tool suite for component comprehension and customization. &lt;br/&gt; &lt;br/&gt;Technological advances, such as rapidly increasing computing power and network bandwidth, are enabling advanced high-performance distributed applications that can significantly enhance the quality of education, health care, remote monitoring and control, early warning systems, scientific collaborations, and other high-performance distributed applications.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103709</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103709</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1237" target="n1238">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: Collaborative Research : Performance-Driven Adaptive Software Design and Control</data>
      <data key="e_abstract">EIA-0103725&lt;br/&gt;James C. Browne&lt;br/&gt;University of Texas&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The goal of this project is to develop the tools needed for performance-directed integrated design and control of complex applications running on distributed computational systems. Its target computational systems are complex, incorporating the difficult heterogeneity, latency, and adaptive properties of computational grids. Its target applications are at the cutting edge of computational science: very large, complex applications with adaptive characteristics that do not allow their optimal system configurations or computational requirements to be estimated prior to run time. Each application will be viewed as a composition of components, with a formal, high fidelity model of performance to be designed for each component. The approach is to use model-based adaptive run-time control, based on these composed performance models, to control the execution of the application to meet specified performance goals. The control strategy will make real-time changes to parameters that modify the behavior of both application and computational platform.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103725</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103725</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1240" target="n1241">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Control of Systems with MEMS Sensors and Actuators via Data Mining Techniques</data>
      <data key="e_abstract">The goal of this interdisciplinary research is to analyze the vast amounts of MEMS sensor data using datamining techniques to discover relationships among actions at MEMS actuators and their impact on the system state. These relationships (captured in the form of rules) are then used to build a feedback loop for aircraft control. The input-output relationships for most systems (e.g., the delta wing aircraft) are highly non-linear. Traditional datamining approaches discard much important information from the datasets and cannot provide sufficient transfer function information, which makes them unsuitable for system control. This project develops a scalable multivariate datamining technique that discovers full sensor-actuator relationships and predictive models under a wide range of conditions (dynamic, temporal, spatial, etc.). The research includes collecting data for dynamic system behavior, extending the datamining algorithms for summarizing temporal rules, developing the rule selection strategy for actuation schema, and developing wind tunnel experiments to validate the approach. This work has the potential to advance the state-of-the-art in data mining substantially, as this problem has many features (real time feedback, spatio-temporal nature) that are not commonly found in other applications. The success of data mining techniques is expected to advance the MEMS sensor and actuation technology in system monitoring and control and in other engineering problems.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">97438</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">97438</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1242" target="n1243">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Keeping Found Things Found On the Web: How Do Users Get Back to Relevant Web Pages?</data>
      <data key="e_abstract">The goal of this project is to understand and improve upon the ways in which people manage information for re-access and re-use. The project focuses on information found on the Word Wide Web and consists of three phases. In Phase 1, the current situation is assessed to determine the nature and severity of problems in managing Web information for re-use and to determine the effectiveness of supporting tools. Participants are first observed in their workplaces as they use the Web to research a work-relevant topic. In a follow-on session, participants try to return to each in a collection of web sites they have previously visited and found useful. Special attention is given to the choice of tools, their success or failure and to the overall time required to reach a site. In Phase 2, practices of re-use and supporting tools, current and potential, are modeled using Phase 1 data. In Phase 3, a select number of tools and practices of re-use are prototyped and evaluated as guided by the modeling of Phase 2. Results of this project will be improved tools and practices to help manage Web information for personal use and re-use. The project will also provide a framework for the assessment of various tools and practices, current and potential. Possible applications extend beyond the Web to other information types including email, the files of a personal computer and even the printed documents in an individual&apos;s office. The results will be disseminated through scholarly publications and through the Internet.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">97855</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">97855</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1242" target="n1244">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Keeping Found Things Found On the Web: How Do Users Get Back to Relevant Web Pages?</data>
      <data key="e_abstract">The goal of this project is to understand and improve upon the ways in which people manage information for re-access and re-use. The project focuses on information found on the Word Wide Web and consists of three phases. In Phase 1, the current situation is assessed to determine the nature and severity of problems in managing Web information for re-use and to determine the effectiveness of supporting tools. Participants are first observed in their workplaces as they use the Web to research a work-relevant topic. In a follow-on session, participants try to return to each in a collection of web sites they have previously visited and found useful. Special attention is given to the choice of tools, their success or failure and to the overall time required to reach a site. In Phase 2, practices of re-use and supporting tools, current and potential, are modeled using Phase 1 data. In Phase 3, a select number of tools and practices of re-use are prototyped and evaluated as guided by the modeling of Phase 2. Results of this project will be improved tools and practices to help manage Web information for personal use and re-use. The project will also provide a framework for the assessment of various tools and practices, current and potential. Possible applications extend beyond the Web to other information types including email, the files of a personal computer and even the printed documents in an individual&apos;s office. The results will be disseminated through scholarly publications and through the Internet.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">97855</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">97855</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1243" target="n1244">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Keeping Found Things Found On the Web: How Do Users Get Back to Relevant Web Pages?</data>
      <data key="e_abstract">The goal of this project is to understand and improve upon the ways in which people manage information for re-access and re-use. The project focuses on information found on the Word Wide Web and consists of three phases. In Phase 1, the current situation is assessed to determine the nature and severity of problems in managing Web information for re-use and to determine the effectiveness of supporting tools. Participants are first observed in their workplaces as they use the Web to research a work-relevant topic. In a follow-on session, participants try to return to each in a collection of web sites they have previously visited and found useful. Special attention is given to the choice of tools, their success or failure and to the overall time required to reach a site. In Phase 2, practices of re-use and supporting tools, current and potential, are modeled using Phase 1 data. In Phase 3, a select number of tools and practices of re-use are prototyped and evaluated as guided by the modeling of Phase 2. Results of this project will be improved tools and practices to help manage Web information for personal use and re-use. The project will also provide a framework for the assessment of various tools and practices, current and potential. Possible applications extend beyond the Web to other information types including email, the files of a personal computer and even the printed documents in an individual&apos;s office. The results will be disseminated through scholarly publications and through the Internet.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">97855</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">97855</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1246" target="n1247">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Digital Government: FedStats Secure Collaborative Environment (FSCE)</data>
      <data key="e_abstract">EIA-0120256&lt;br/&gt;Herbert Schorr&lt;br/&gt;University of Southern California&lt;br/&gt;&lt;br/&gt;SGER: Digital Government: Fedstats Secure Collaborative Environment&lt;br/&gt;&lt;br/&gt;This grant will support preliminary explorations of the needs of statistical researchers and Federal statistical agencies to collaborate at a distance, using the Internet, over potentially confidential information. Issues such as security, privacy and authenticated data arise, particularly when some of the collaborators are connected from behind Federal agency firewalls Data analysis, visualization, Internet-based teleconferencing and other collaboration tools will be explored, as will the applicability of new Internet protocols.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">120256</data>
      <data key="e_expirationDate">2002-10-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">120256</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1246" target="n1248">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Digital Government: FedStats Secure Collaborative Environment (FSCE)</data>
      <data key="e_abstract">EIA-0120256&lt;br/&gt;Herbert Schorr&lt;br/&gt;University of Southern California&lt;br/&gt;&lt;br/&gt;SGER: Digital Government: Fedstats Secure Collaborative Environment&lt;br/&gt;&lt;br/&gt;This grant will support preliminary explorations of the needs of statistical researchers and Federal statistical agencies to collaborate at a distance, using the Internet, over potentially confidential information. Issues such as security, privacy and authenticated data arise, particularly when some of the collaborators are connected from behind Federal agency firewalls Data analysis, visualization, Internet-based teleconferencing and other collaboration tools will be explored, as will the applicability of new Internet protocols.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">120256</data>
      <data key="e_expirationDate">2002-10-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">120256</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1247" target="n1248">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Digital Government: FedStats Secure Collaborative Environment (FSCE)</data>
      <data key="e_abstract">EIA-0120256&lt;br/&gt;Herbert Schorr&lt;br/&gt;University of Southern California&lt;br/&gt;&lt;br/&gt;SGER: Digital Government: Fedstats Secure Collaborative Environment&lt;br/&gt;&lt;br/&gt;This grant will support preliminary explorations of the needs of statistical researchers and Federal statistical agencies to collaborate at a distance, using the Internet, over potentially confidential information. Issues such as security, privacy and authenticated data arise, particularly when some of the collaborators are connected from behind Federal agency firewalls Data analysis, visualization, Internet-based teleconferencing and other collaboration tools will be explored, as will the applicability of new Internet protocols.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">120256</data>
      <data key="e_expirationDate">2002-10-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">120256</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n716" target="n719">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Research for Mixed Signal Electronic Technologies: A Joint Initiative Between NSF and SRC: Scalable Design and Test Methods for Single-Chip Multi-Link Radio-Frequency Transceivers</data>
      <data key="e_abstract">Recent developments in mixed-signal systems, especially those integrating computing and&lt;br/&gt;communication in a system-on-a-chip, have focused on the goal to communicate information via wireless devices and networks. While digital system design to process baseband information is moving into the low gigahertz (GHz) frequency range, the mixed-signal transceivers have to operate in the ISM bands (2.5 GHz up to 5.8 GHz) with even higher frequencies in the near future to satisfy bandwidth demands. Analog design advances have produced several transceiver designs up to 5 GHz, using CMOS, BiCMOS, and other technologies. To reduce noise, these designs tend to separate the transmitter and receiver, and so far, have provided only a single physical link (one transmitter and one receiver) in a wireless device. In the design area, this proposal addresses the creation and verification of scalable systematic design methods to integrate two or more physical links on one single chip to provide more bandwidth and flexibility in communication applications. A methodology to incorporate multi-links is scalable in the sense that more links can be added by application demands. To create this methodology, we propose the following design approaches:&lt;br/&gt;&lt;br/&gt;1. Noise cancellation techniques and circuits to deal with digital switching noise.&lt;br/&gt;&lt;br/&gt;2. Noise cancellation techniques and circuits to deal with RF noise interference between different transceiver links and circuits.&lt;br/&gt;&lt;br/&gt;These circuits will be validated using case studies from industry with whom we have had close&lt;br/&gt;collaborations: Texas Instruments, Motorola, and National Semiconductors, who will provide&lt;br/&gt;advanced fabrication technologies and simulation models for this study.&lt;br/&gt;The designs will be fully tested and the development of scalable test methods is the second focus of this proposal. Mixed-signal test advances, despite intense activities, have been rather slow, especially in high-frequency (GHz) test. We propose to investigate the following approaches and distill the results into a test methodology that can be scaled with respect to operating frequencies and process advances:&lt;br/&gt;&lt;br/&gt;1. End-to-end digital test methods using one transmit link and one receive link on the same chip&lt;br/&gt;to verify correct information transmission.&lt;br/&gt;&lt;br/&gt;2. Designs of on-chip delay and phase measurement circuits, operating at the same frequency as&lt;br/&gt;the transceivers.&lt;br/&gt;&lt;br/&gt;3. Interface between ATE and on-chip test circuits to use test resources efficiently.&lt;br/&gt;During the validation of these test methodologies, we will need access to advance test equipment for comparison purposes, and these equipment will be provided by our collaborator at Teradyne (Tualatin, OR) and Wavecrest (San Jose, CA).&lt;br/&gt;&lt;br/&gt;Another level of integration involves the curriculum - research aspects of the proposed work,&lt;br/&gt;which is being implemented in our current curriculum revision. Dissemination approaches re-used the distance learning methods and assessment supported by NSF, FIPSE, and our own&lt;br/&gt;university. The proposal will deliver fundamental methodologies and techniques, and train the&lt;br/&gt;first-generation system architects in high-frequency mixed-signal design and test.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">120255</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">120255</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n531" target="n1254">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">NGS: Scalable I/O Management and Access Optimizations for Scientific Applications for High-Performance Computing</data>
      <data key="e_abstract">EIA-0103023&lt;br/&gt;Alok Choudhary&lt;br/&gt;Northwestern University&lt;br/&gt;&lt;br/&gt;Scalable I/O Management &amp; Access Optimizations for Scientific Applications for High-Performance Computing&lt;br/&gt;&lt;br/&gt;The main objective of this proposal is to address the problem of large-scale storage, performance management of I/O, automatic performance optimizations of I/O using historical information and access patterns, data management, analysis, and access using simple interfaces which permit flow of access information to lower levels software for exploiting higher level information. Furthermore, since analysis at such a scale is simply not feasible if done manually (e.g., visualization alone or off-line analysis), integration of on-line analysis and feature extraction while simulations and experiments are executing is very important. Our observation is that neither parallel file systems nor runtime systems and database management systems (DBMS) fully-address the large-scale data management problem, as they lack global information about the applications access patterns and most of them are not effective in handling storage hierarchies.&lt;br/&gt;&lt;br/&gt;We believe that the results from the proposed research will enable scientists to address one of the most important bottlenecks in computational simulation cycles; namely, the bottleneck of analyzing and managing massive data in high-performance distributed computing environment (such as Grid).</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103023</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103023</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1256" target="n1256">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ROLE: Cognitive Styles and Individual Differences in Imagery</data>
      <data key="e_abstract">Proposal number: 0106760&lt;br/&gt;PI: Stephen Kosslyn&lt;br/&gt;Institution: Harvard University &lt;br/&gt;Title: Cognitive Styles and Individual Differences in Imagery &lt;br/&gt;&lt;br/&gt;Award Abstract&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This is a study of cognitive styles (such as the visualizer-verbalizer distinction) people (both students and professionals) use in thinking through and learning science and mathematics (specifically in the topic area of kinematics). The project has two goals: (1) To investigate and revise the traditional verbalizer-visualizer cognitive style dimension, including the possibility that more than these two basic styles exist (such as object and spatial visual styles). This approach is based on findings and methods from cognitive psychology and cognitive neuroscience to conceptualize the nature of cognitive style and its neural underpinnings. (2) To examine the instructional implications of these cognitive styles and their relevance to educational practice. The research will proceed along three lines: behavioral studies, classroom-based studies, and functional magnetic resonance imaging (fMRI). Based on evidence from this research, the investigators will develop theoretically-based guidelines for teaching students to process visually/spatially and materials which build on the specific strengths of each type of thinker.</data>
      <data key="e_pgm">T821</data>
      <data key="e_label">106760</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">106760</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1256" target="n1258">
      <data key="e_effectiveDate">None</data>
      <data key="e_title">None</data>
      <data key="e_abstract">None</data>
      <data key="e_pgm">None</data>
      <data key="e_expirationDate">None</data>
      <data key="e_div">None</data>
      <data key="e_dir">None</data>
    </edge>
    <edge source="n1256" target="n1258">
      <data key="e_effectiveDate">None</data>
      <data key="e_title">None</data>
      <data key="e_abstract">None</data>
      <data key="e_pgm">None</data>
      <data key="e_expirationDate">None</data>
      <data key="e_div">None</data>
      <data key="e_dir">None</data>
    </edge>
    <edge source="n1256" target="n1258">
      <data key="e_effectiveDate">None</data>
      <data key="e_title">None</data>
      <data key="e_abstract">None</data>
      <data key="e_pgm">None</data>
      <data key="e_expirationDate">None</data>
      <data key="e_div">None</data>
      <data key="e_dir">None</data>
    </edge>
    <edge source="n1256" target="n1258">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ROLE: Cognitive Styles and Individual Differences in Imagery</data>
      <data key="e_abstract">Proposal number: 0106760&lt;br/&gt;PI: Stephen Kosslyn&lt;br/&gt;Institution: Harvard University &lt;br/&gt;Title: Cognitive Styles and Individual Differences in Imagery &lt;br/&gt;&lt;br/&gt;Award Abstract&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This is a study of cognitive styles (such as the visualizer-verbalizer distinction) people (both students and professionals) use in thinking through and learning science and mathematics (specifically in the topic area of kinematics). The project has two goals: (1) To investigate and revise the traditional verbalizer-visualizer cognitive style dimension, including the possibility that more than these two basic styles exist (such as object and spatial visual styles). This approach is based on findings and methods from cognitive psychology and cognitive neuroscience to conceptualize the nature of cognitive style and its neural underpinnings. (2) To examine the instructional implications of these cognitive styles and their relevance to educational practice. The research will proceed along three lines: behavioral studies, classroom-based studies, and functional magnetic resonance imaging (fMRI). Based on evidence from this research, the investigators will develop theoretically-based guidelines for teaching students to process visually/spatially and materials which build on the specific strengths of each type of thinker.</data>
      <data key="e_pgm">T821</data>
      <data key="e_label">106760</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">106760</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1258" target="n1258">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ROLE: Cognitive Styles and Individual Differences in Imagery</data>
      <data key="e_abstract">Proposal number: 0106760&lt;br/&gt;PI: Stephen Kosslyn&lt;br/&gt;Institution: Harvard University &lt;br/&gt;Title: Cognitive Styles and Individual Differences in Imagery &lt;br/&gt;&lt;br/&gt;Award Abstract&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This is a study of cognitive styles (such as the visualizer-verbalizer distinction) people (both students and professionals) use in thinking through and learning science and mathematics (specifically in the topic area of kinematics). The project has two goals: (1) To investigate and revise the traditional verbalizer-visualizer cognitive style dimension, including the possibility that more than these two basic styles exist (such as object and spatial visual styles). This approach is based on findings and methods from cognitive psychology and cognitive neuroscience to conceptualize the nature of cognitive style and its neural underpinnings. (2) To examine the instructional implications of these cognitive styles and their relevance to educational practice. The research will proceed along three lines: behavioral studies, classroom-based studies, and functional magnetic resonance imaging (fMRI). Based on evidence from this research, the investigators will develop theoretically-based guidelines for teaching students to process visually/spatially and materials which build on the specific strengths of each type of thinker.</data>
      <data key="e_pgm">T821</data>
      <data key="e_label">106760</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">106760</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n604" target="n1262">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/API: Collaborative Research: Cancer Treatment Plan Optimization</data>
      <data key="e_abstract">This project will use new techniques from computational optimization to design radiation therapy planning for cancer treatment. &lt;br/&gt;&lt;br/&gt;Radiation therapy applies ionizing radiation to cancerous tissue, damaging the DNA and interfering with the ability of the cancerous cells to grow and divide. This also damages healthy cells, but they are more able to repair the damage and return to normal function. The therapy planning problem is to specify the shapes of the applied radiation beams, times of exposure, etc., to deliver a specified dose to the tumor but not an excessive dose to the surrounding healthy tissue. New medical devices allow much control over the characteristics of the radiation, thus allowing much scope for the therapy planning. However, the full potential of these devices to deliver optimal treatment plans has yet to be realized due to the complexity of the treatment design process. By using advanced modeling techniques, state-of-the-art optimization algorithms, and implementations on parallel computing platforms this project will provide radiation oncologists with important new computational tools for treatment planning. These tools will be flexible enough to adapt to the varying priorities of different planners and different patients and robust enough to give good solutions to the most difficult planning problems. The project involves collaboration between three researchers whose collective expertise encompasses radiation oncology modeling, optimization algorithms, and parallel implementations. It builds on previous collaborations of these researchers on treatment planning and on NSF-funded work on algorithms for solving large optimization problems.&lt;br/&gt;&lt;br/&gt;The institutions involved in the project are the University of Wisconsin and the University of&lt;br/&gt;Maryland School of Medicine.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113051</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113051</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n724" target="n1264">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Small-Scale Dynamic Reconfigurability for Large-Scale Benefits</data>
      <data key="e_abstract">General purpose processors (GPPs) are designed to implement on fixed configuration that is good on average but may not be well suited for individual applications. In fact, applications can have drastically different execution characteristics (e.g. branch prediction technique preference, cache configuration and policy preference, low-power opportunities). This suggests the use of device reconfigurability, but generic reconfigurable logic of any substantial scale (e.g. most field programmable gate array (FPGA) technology) is slow, lacks density, and is power-hungry. Yet many processor structures are easily adaptable to a wide variety of configurations. This research will develop dynamic, small-scale, partial reconfigurability for such structures. This &quot;Dynaptable&quot; approach has the further benefit that it integrates work at the architectural, logic, and circuit levels.&lt;br/&gt;&lt;br/&gt;The Dynaptable approach consists of three key elements:&lt;br/&gt;&lt;br/&gt; 1. Flexible structures: designing key processor structures with judicious amounts of reconfigurable hardware to provide flexibility in a low-cost, non-invasive way.&lt;br/&gt;&lt;br/&gt; 2. Run-time monitoring: determining the current configuration&apos;s performance or effectiveness compared to other possible competing configurations.&lt;br/&gt;&lt;br/&gt; 3. Dynamic reconfiguration: using the results of run-time monitoring to adapt to a new configuration that improves the chosen figure of merit (e.g. performance, energy-delay product, fault tolerance).&lt;br/&gt;&lt;br/&gt;The research will identify the most profitable places for adding small-scale reconfigurability, design the requisite reconfigurable elements, and develop the most effective and lowest-cost techniques for dynamic monitoring and adaptation. This work will have an impact on the design of a variety of processor components (branch predictor, cache, datapath, etc.) for a range of processing environments (embedded systems, superscalar, SMT, etc.). The final goal is to develop a consistent methodology for dynamically adapting GPP microarchitectures for improved performance, lower power, and increased fault tolerance.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">105626</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105626</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1264" target="n1266">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Small-Scale Dynamic Reconfigurability for Large-Scale Benefits</data>
      <data key="e_abstract">General purpose processors (GPPs) are designed to implement on fixed configuration that is good on average but may not be well suited for individual applications. In fact, applications can have drastically different execution characteristics (e.g. branch prediction technique preference, cache configuration and policy preference, low-power opportunities). This suggests the use of device reconfigurability, but generic reconfigurable logic of any substantial scale (e.g. most field programmable gate array (FPGA) technology) is slow, lacks density, and is power-hungry. Yet many processor structures are easily adaptable to a wide variety of configurations. This research will develop dynamic, small-scale, partial reconfigurability for such structures. This &quot;Dynaptable&quot; approach has the further benefit that it integrates work at the architectural, logic, and circuit levels.&lt;br/&gt;&lt;br/&gt;The Dynaptable approach consists of three key elements:&lt;br/&gt;&lt;br/&gt; 1. Flexible structures: designing key processor structures with judicious amounts of reconfigurable hardware to provide flexibility in a low-cost, non-invasive way.&lt;br/&gt;&lt;br/&gt; 2. Run-time monitoring: determining the current configuration&apos;s performance or effectiveness compared to other possible competing configurations.&lt;br/&gt;&lt;br/&gt; 3. Dynamic reconfiguration: using the results of run-time monitoring to adapt to a new configuration that improves the chosen figure of merit (e.g. performance, energy-delay product, fault tolerance).&lt;br/&gt;&lt;br/&gt;The research will identify the most profitable places for adding small-scale reconfigurability, design the requisite reconfigurable elements, and develop the most effective and lowest-cost techniques for dynamic monitoring and adaptation. This work will have an impact on the design of a variety of processor components (branch predictor, cache, datapath, etc.) for a range of processing environments (embedded systems, superscalar, SMT, etc.). The final goal is to develop a consistent methodology for dynamically adapting GPP microarchitectures for improved performance, lower power, and increased fault tolerance.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">105626</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105626</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n724" target="n1266">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Small-Scale Dynamic Reconfigurability for Large-Scale Benefits</data>
      <data key="e_abstract">General purpose processors (GPPs) are designed to implement on fixed configuration that is good on average but may not be well suited for individual applications. In fact, applications can have drastically different execution characteristics (e.g. branch prediction technique preference, cache configuration and policy preference, low-power opportunities). This suggests the use of device reconfigurability, but generic reconfigurable logic of any substantial scale (e.g. most field programmable gate array (FPGA) technology) is slow, lacks density, and is power-hungry. Yet many processor structures are easily adaptable to a wide variety of configurations. This research will develop dynamic, small-scale, partial reconfigurability for such structures. This &quot;Dynaptable&quot; approach has the further benefit that it integrates work at the architectural, logic, and circuit levels.&lt;br/&gt;&lt;br/&gt;The Dynaptable approach consists of three key elements:&lt;br/&gt;&lt;br/&gt; 1. Flexible structures: designing key processor structures with judicious amounts of reconfigurable hardware to provide flexibility in a low-cost, non-invasive way.&lt;br/&gt;&lt;br/&gt; 2. Run-time monitoring: determining the current configuration&apos;s performance or effectiveness compared to other possible competing configurations.&lt;br/&gt;&lt;br/&gt; 3. Dynamic reconfiguration: using the results of run-time monitoring to adapt to a new configuration that improves the chosen figure of merit (e.g. performance, energy-delay product, fault tolerance).&lt;br/&gt;&lt;br/&gt;The research will identify the most profitable places for adding small-scale reconfigurability, design the requisite reconfigurable elements, and develop the most effective and lowest-cost techniques for dynamic monitoring and adaptation. This work will have an impact on the design of a variety of processor components (branch predictor, cache, datapath, etc.) for a range of processing environments (embedded systems, superscalar, SMT, etc.). The final goal is to develop a consistent methodology for dynamically adapting GPP microarchitectures for improved performance, lower power, and increased fault tolerance.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">105626</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105626</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1269" target="n1270">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Collaborative Research: Data Mining Meets I/O Performance Evaluation: Advanced Statistical Tools for Analyzing Bursty Traffic</data>
      <data key="e_abstract">The goal of this collaborative research project involving Christos Faloutsos and Ngai Hang Chang at Carnegie Mellon (award 0083148) and Tara Madhyastha at U of Cal Santa Cruz (award 0083130) is to develop and apply statistical and datamining tools to analyze bursty time sequences, with emphasis on I/O traffic optimization. The interdisciplinary team includes researchers in computer science, computer engineering and statistics, and industry collaborators. The approach has three parts: (1) advanced statistical tools using the ``ARFIMA&apos;&apos; method; (2) wavelets and the related ``80-20 law&apos;&apos; to model disk traffic; and (3) incorporation of these models&lt;br/&gt;inside the so-called ``Active Disks&apos;&apos;, with the goal to build self-tuning, adaptive disk subsystems.&lt;br/&gt;The results will advance data mining and statistics as well as disk design. An easy-to-use toolkit &quot;T-REX&quot; will aid in I/O and systems design, handling bursty traffic, and better buffering and prefetching. The theory behind the T-REX toolkit will be based on new data mining algorithms and statistical methods that model self-similar time sequences (like web and network traffic,&lt;br/&gt;in addition to I/O traffic). The research team has strong ties with database, data mining and disk manufacturing industrial groups, and this will aid in testing the research toolkit and its technology transfer. It can be expected that the T-REX system will significantly aid the design of disk sub-systems with beneficial impact on the storage industry.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">83148</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83148</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1273" target="n1274">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE+SY: Collaborative Research: Foundations of Electronic Marketplaces: Game Theory, Algorithms and Systems</data>
      <data key="e_abstract">Electronic markets are emerging as a primary medium of trade in business-to-business, business-to-consumer, and consumer-to-consumer settings. In order to design viable electronic marketplaces, a host of novel interrelated game-theoretic and computational issues must be&lt;br/&gt;addressed. With a team of interdisciplinary researchers from multiple institutions, this project will develop a unified theory of games and computing to guide and facilitate the growth of such markets. Specific research directions of the project include the following: (1) Market designs will be generalized to incorporate combinatorial bidding, multi-attribute preferences, multi-stage mechanisms, continuous mechanisms, and multi-unit sale; (2) New algorithms for clearing, quoting, incentive-compatible pricing as well as new incentive-compatible tractable mechanisms will be designed with particular emphasis on online and incremental updating of market states; (3) Bounded rationality of the agents will be investigated under a wide spectrum of models of computations, equilibrium concepts of game theory, and trade-offs between centers and agents; and (4) Novel approaches to relaxing the classic common prior assumption will be explored in order to develop practically useful models for ecommerce. The successful completion of this project will make significant contributions to both theory and practice in the areas of electronic commerce, multi-agent systems, algorithms, computational complexity theory, and game theory.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121541</data>
      <data key="e_expirationDate">2008-05-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121541</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1275" target="n1276">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Multimedia Stream Modeling, Relationships, and Querying</data>
      <data key="e_abstract">This research project introduces needed advances in multimedia data management and access for applications dealing with streams of multimedia data. In particular, medical patient information involves streams of sequences of multimedia data, including streams of images (X-rays, MRIs, etc.), medical reports, treatment and medication information, and other alphanumeric data taken at various points in time, any part of which must be readily accessible for patient care. The goal of this effort is to enable doctors and the patient to store and easily retrieve and visualize the variety of such medical information, and various related data in the web. Data modeling constructs to support the needed substreams, aggregated streams, derived streams, and relationships among and within these constructs is expected to result from this research. A stream algebra for expressing queries is introduced to explore methods for effective query processing, as well as a corresponding visual language for users to easily express the queries. A Time Line end-user interface is extended to incorporate the new constructs introduced for a variety of representative medical user queries. A prototype system will be developed and tested at the UCLA Medical Center to determine the potential impact of these advances through the Time Line paradigm vs current practice in the actual medical domain. Demos will be developed for various computer science and medical domain forums, and will be accessible via Internet, to highlight the technical and medical application innovations. It is expected that the resulting multimedia stream data modeling and data accessing advances will have significant applicability in the medical domain and other areas.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">82817</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82817</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1277" target="n1278">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Spatial and Spatio-Temporal Aggregation</data>
      <data key="e_abstract">Aggregate computation is expensive, especially when time-varying attributes are involved in it. The task of computing aggregates becomes more challenging for spatial and spatio-temporal databases, as the spatial and temporal extent over which the aggregate value holds must be computed. For example, multi-attribute images are often the result of a query in the Earth Observing System (EOS) environments, which retrieves all of the measurements incident on the pixels that satisfy the spatio-temporal bounds of the query. Each pixel of multi-attribute images has several values associated with it. This project develops a suite of techniques for computing spatio-temporal aggregates. While there has been significant work done in temporal aggregation, little is known about how to evaluate spatial aggregates. The existing temporal aggregation and spatial join algorithms are generalized to create efficient algorithms for computing spatial aggregates, and then further generalized these algorithms to accommodate spatio-temporal aggregates. In addition, scalable techniques will be developed by parallelizing the aggregation algorithms on a shared-nothing architecture. The project team includes a hydrologists at the United States Geological Survey on the USGS Death Valley Regional Flow System (DVRFS) Project, which is investigating ground-water flow in the Nevada Test Site, proposed as a repository for high-level nuclear waste. The results from this research has a direct impact on many large-scale spatio-temporal database applications such as EOS, cadastral databases, atmospheric databases and hydrologic databases.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">100436</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">100436</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1280" target="n1281">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">INBOUNDS: The Integrated Network-Based Ohio University Network Detective Service</data>
      <data key="e_abstract">This document describes a proposed software system, called INBOUNDS (Integrated&lt;br/&gt;Network-Based Ohio University Network Detective Service), that will address the&lt;br/&gt;difficult research problem of security in the dynamic real-time Internet environment&lt;br/&gt;populated by both legitimate users and hostile intruders.&lt;br/&gt; Internet security is becoming more critical by the day. Successful attacks on banks,&lt;br/&gt;schools, government agencies, and corporations that do business online are becoming&lt;br/&gt;more and more common, and the frequency of these attacks and the amount of damage&lt;br/&gt;done is rising rapidly. Commercially available firewalls and intrusion detection systems&lt;br/&gt;are currently the only weapons with which to defend against the threat, but they are&lt;br/&gt;obviously not capable of keeping up with the ever-changing attack strategies of hackers.&lt;br/&gt; Thus, we propose INBOUNDS a real-time network based intrusion detection and&lt;br/&gt;response system under development at Ohio University&apos;s Laboratory for Real-Time,&lt;br/&gt;Secure Systems and Applications. INBOUNDS detects and responds to suspicious&lt;br/&gt;behavior by using TCPTrace (a network traffic analysis tool) and DeSiDeRaTa (dynamic,&lt;br/&gt;real-time resource management middleware). INBOUNDS is intended to function in a&lt;br/&gt;heterogeneous environment with fault tolerance, very low overhead, and a high degree of&lt;br/&gt;scalability. A prototype of INBOUNDS is currently being used for around-the-clock&lt;br/&gt;intrusion detection and response at Ohio University and we propose to add functionality&lt;br/&gt;that will enable INBOUNDS to deal with the following important types of attacks:&lt;br/&gt; Large-scale, distributed denial-of-service attacks&lt;br/&gt; Abnormal network protocol behavior including SYN and RESET attacks&lt;br/&gt; Suspicious keywords in interactive sessions/email&lt;br/&gt; Suspicious patterns of data, such as the fan-out patterns commonly seen with&lt;br/&gt; email viruses&lt;br/&gt; Communication over unusual network ports, which are common when attackers&lt;br/&gt; target seldom used and insecure servers&lt;br/&gt; Connections from unknown/unusual hosts&lt;br/&gt; Abnormal data patterns for a particular time of day&lt;br/&gt; Unusual data patterns on known ports, such as would be seen when at attacker&lt;br/&gt; installs programs using the fingerd port as in the Morris Worm</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">86642</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86642</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1280" target="n1282">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">INBOUNDS: The Integrated Network-Based Ohio University Network Detective Service</data>
      <data key="e_abstract">This document describes a proposed software system, called INBOUNDS (Integrated&lt;br/&gt;Network-Based Ohio University Network Detective Service), that will address the&lt;br/&gt;difficult research problem of security in the dynamic real-time Internet environment&lt;br/&gt;populated by both legitimate users and hostile intruders.&lt;br/&gt; Internet security is becoming more critical by the day. Successful attacks on banks,&lt;br/&gt;schools, government agencies, and corporations that do business online are becoming&lt;br/&gt;more and more common, and the frequency of these attacks and the amount of damage&lt;br/&gt;done is rising rapidly. Commercially available firewalls and intrusion detection systems&lt;br/&gt;are currently the only weapons with which to defend against the threat, but they are&lt;br/&gt;obviously not capable of keeping up with the ever-changing attack strategies of hackers.&lt;br/&gt; Thus, we propose INBOUNDS a real-time network based intrusion detection and&lt;br/&gt;response system under development at Ohio University&apos;s Laboratory for Real-Time,&lt;br/&gt;Secure Systems and Applications. INBOUNDS detects and responds to suspicious&lt;br/&gt;behavior by using TCPTrace (a network traffic analysis tool) and DeSiDeRaTa (dynamic,&lt;br/&gt;real-time resource management middleware). INBOUNDS is intended to function in a&lt;br/&gt;heterogeneous environment with fault tolerance, very low overhead, and a high degree of&lt;br/&gt;scalability. A prototype of INBOUNDS is currently being used for around-the-clock&lt;br/&gt;intrusion detection and response at Ohio University and we propose to add functionality&lt;br/&gt;that will enable INBOUNDS to deal with the following important types of attacks:&lt;br/&gt; Large-scale, distributed denial-of-service attacks&lt;br/&gt; Abnormal network protocol behavior including SYN and RESET attacks&lt;br/&gt; Suspicious keywords in interactive sessions/email&lt;br/&gt; Suspicious patterns of data, such as the fan-out patterns commonly seen with&lt;br/&gt; email viruses&lt;br/&gt; Communication over unusual network ports, which are common when attackers&lt;br/&gt; target seldom used and insecure servers&lt;br/&gt; Connections from unknown/unusual hosts&lt;br/&gt; Abnormal data patterns for a particular time of day&lt;br/&gt; Unusual data patterns on known ports, such as would be seen when at attacker&lt;br/&gt; installs programs using the fingerd port as in the Morris Worm</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">86642</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86642</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1281" target="n1282">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">INBOUNDS: The Integrated Network-Based Ohio University Network Detective Service</data>
      <data key="e_abstract">This document describes a proposed software system, called INBOUNDS (Integrated&lt;br/&gt;Network-Based Ohio University Network Detective Service), that will address the&lt;br/&gt;difficult research problem of security in the dynamic real-time Internet environment&lt;br/&gt;populated by both legitimate users and hostile intruders.&lt;br/&gt; Internet security is becoming more critical by the day. Successful attacks on banks,&lt;br/&gt;schools, government agencies, and corporations that do business online are becoming&lt;br/&gt;more and more common, and the frequency of these attacks and the amount of damage&lt;br/&gt;done is rising rapidly. Commercially available firewalls and intrusion detection systems&lt;br/&gt;are currently the only weapons with which to defend against the threat, but they are&lt;br/&gt;obviously not capable of keeping up with the ever-changing attack strategies of hackers.&lt;br/&gt; Thus, we propose INBOUNDS a real-time network based intrusion detection and&lt;br/&gt;response system under development at Ohio University&apos;s Laboratory for Real-Time,&lt;br/&gt;Secure Systems and Applications. INBOUNDS detects and responds to suspicious&lt;br/&gt;behavior by using TCPTrace (a network traffic analysis tool) and DeSiDeRaTa (dynamic,&lt;br/&gt;real-time resource management middleware). INBOUNDS is intended to function in a&lt;br/&gt;heterogeneous environment with fault tolerance, very low overhead, and a high degree of&lt;br/&gt;scalability. A prototype of INBOUNDS is currently being used for around-the-clock&lt;br/&gt;intrusion detection and response at Ohio University and we propose to add functionality&lt;br/&gt;that will enable INBOUNDS to deal with the following important types of attacks:&lt;br/&gt; Large-scale, distributed denial-of-service attacks&lt;br/&gt; Abnormal network protocol behavior including SYN and RESET attacks&lt;br/&gt; Suspicious keywords in interactive sessions/email&lt;br/&gt; Suspicious patterns of data, such as the fan-out patterns commonly seen with&lt;br/&gt; email viruses&lt;br/&gt; Communication over unusual network ports, which are common when attackers&lt;br/&gt; target seldom used and insecure servers&lt;br/&gt; Connections from unknown/unusual hosts&lt;br/&gt; Abnormal data patterns for a particular time of day&lt;br/&gt; Unusual data patterns on known ports, such as would be seen when at attacker&lt;br/&gt; installs programs using the fingerd port as in the Morris Worm</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">86642</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86642</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1285" target="n1286">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+AP: Automated Compilation and Computational Analysis of Regulatory Networks</data>
      <data key="e_abstract">EIA-0121687&lt;br/&gt;Rzhetsky, Andrey&lt;br/&gt;Columbia University&lt;br/&gt;&lt;br/&gt;ITR/IM+AP: Automated compilation and computational analysis of regulatory networks&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project will provide advanced information management systems as well as software applications relevant to the fields of bioinformaticis, molecular biology, and medicine. It will contribute to fundamental Information Management by developing advanced algorithms for the efficient retrieval of information about biological pathways from research articles (using natural language processing techniques), machine-learning-assisted data mining in biological texts, and information management via a uniquely designed database with two levels of representation of biological information.&lt;br/&gt;&lt;br/&gt;The specific aims of the project are the following: to implement and test natural language processing techniques for the automatic retrieval of signal-transduction pathways from the research literature; to develop general and portable text-mining techniques for the automatic recognition of genes, proteins, and other domain terms, and of relationships between them; and to develop a mathematical framework for the statistical analysis of heterogeneous data on regulatory pathways.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121687</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121687</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1285" target="n1287">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+AP: Automated Compilation and Computational Analysis of Regulatory Networks</data>
      <data key="e_abstract">EIA-0121687&lt;br/&gt;Rzhetsky, Andrey&lt;br/&gt;Columbia University&lt;br/&gt;&lt;br/&gt;ITR/IM+AP: Automated compilation and computational analysis of regulatory networks&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project will provide advanced information management systems as well as software applications relevant to the fields of bioinformaticis, molecular biology, and medicine. It will contribute to fundamental Information Management by developing advanced algorithms for the efficient retrieval of information about biological pathways from research articles (using natural language processing techniques), machine-learning-assisted data mining in biological texts, and information management via a uniquely designed database with two levels of representation of biological information.&lt;br/&gt;&lt;br/&gt;The specific aims of the project are the following: to implement and test natural language processing techniques for the automatic retrieval of signal-transduction pathways from the research literature; to develop general and portable text-mining techniques for the automatic recognition of genes, proteins, and other domain terms, and of relationships between them; and to develop a mathematical framework for the statistical analysis of heterogeneous data on regulatory pathways.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121687</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121687</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1285" target="n1288">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+AP: Automated Compilation and Computational Analysis of Regulatory Networks</data>
      <data key="e_abstract">EIA-0121687&lt;br/&gt;Rzhetsky, Andrey&lt;br/&gt;Columbia University&lt;br/&gt;&lt;br/&gt;ITR/IM+AP: Automated compilation and computational analysis of regulatory networks&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project will provide advanced information management systems as well as software applications relevant to the fields of bioinformaticis, molecular biology, and medicine. It will contribute to fundamental Information Management by developing advanced algorithms for the efficient retrieval of information about biological pathways from research articles (using natural language processing techniques), machine-learning-assisted data mining in biological texts, and information management via a uniquely designed database with two levels of representation of biological information.&lt;br/&gt;&lt;br/&gt;The specific aims of the project are the following: to implement and test natural language processing techniques for the automatic retrieval of signal-transduction pathways from the research literature; to develop general and portable text-mining techniques for the automatic recognition of genes, proteins, and other domain terms, and of relationships between them; and to develop a mathematical framework for the statistical analysis of heterogeneous data on regulatory pathways.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121687</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121687</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1285" target="n1289">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+AP: Automated Compilation and Computational Analysis of Regulatory Networks</data>
      <data key="e_abstract">EIA-0121687&lt;br/&gt;Rzhetsky, Andrey&lt;br/&gt;Columbia University&lt;br/&gt;&lt;br/&gt;ITR/IM+AP: Automated compilation and computational analysis of regulatory networks&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project will provide advanced information management systems as well as software applications relevant to the fields of bioinformaticis, molecular biology, and medicine. It will contribute to fundamental Information Management by developing advanced algorithms for the efficient retrieval of information about biological pathways from research articles (using natural language processing techniques), machine-learning-assisted data mining in biological texts, and information management via a uniquely designed database with two levels of representation of biological information.&lt;br/&gt;&lt;br/&gt;The specific aims of the project are the following: to implement and test natural language processing techniques for the automatic retrieval of signal-transduction pathways from the research literature; to develop general and portable text-mining techniques for the automatic recognition of genes, proteins, and other domain terms, and of relationships between them; and to develop a mathematical framework for the statistical analysis of heterogeneous data on regulatory pathways.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121687</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121687</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1286" target="n1287">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+AP: Automated Compilation and Computational Analysis of Regulatory Networks</data>
      <data key="e_abstract">EIA-0121687&lt;br/&gt;Rzhetsky, Andrey&lt;br/&gt;Columbia University&lt;br/&gt;&lt;br/&gt;ITR/IM+AP: Automated compilation and computational analysis of regulatory networks&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project will provide advanced information management systems as well as software applications relevant to the fields of bioinformaticis, molecular biology, and medicine. It will contribute to fundamental Information Management by developing advanced algorithms for the efficient retrieval of information about biological pathways from research articles (using natural language processing techniques), machine-learning-assisted data mining in biological texts, and information management via a uniquely designed database with two levels of representation of biological information.&lt;br/&gt;&lt;br/&gt;The specific aims of the project are the following: to implement and test natural language processing techniques for the automatic retrieval of signal-transduction pathways from the research literature; to develop general and portable text-mining techniques for the automatic recognition of genes, proteins, and other domain terms, and of relationships between them; and to develop a mathematical framework for the statistical analysis of heterogeneous data on regulatory pathways.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121687</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121687</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1286" target="n1288">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+AP: Automated Compilation and Computational Analysis of Regulatory Networks</data>
      <data key="e_abstract">EIA-0121687&lt;br/&gt;Rzhetsky, Andrey&lt;br/&gt;Columbia University&lt;br/&gt;&lt;br/&gt;ITR/IM+AP: Automated compilation and computational analysis of regulatory networks&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project will provide advanced information management systems as well as software applications relevant to the fields of bioinformaticis, molecular biology, and medicine. It will contribute to fundamental Information Management by developing advanced algorithms for the efficient retrieval of information about biological pathways from research articles (using natural language processing techniques), machine-learning-assisted data mining in biological texts, and information management via a uniquely designed database with two levels of representation of biological information.&lt;br/&gt;&lt;br/&gt;The specific aims of the project are the following: to implement and test natural language processing techniques for the automatic retrieval of signal-transduction pathways from the research literature; to develop general and portable text-mining techniques for the automatic recognition of genes, proteins, and other domain terms, and of relationships between them; and to develop a mathematical framework for the statistical analysis of heterogeneous data on regulatory pathways.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121687</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121687</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1286" target="n1289">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+AP: Automated Compilation and Computational Analysis of Regulatory Networks</data>
      <data key="e_abstract">EIA-0121687&lt;br/&gt;Rzhetsky, Andrey&lt;br/&gt;Columbia University&lt;br/&gt;&lt;br/&gt;ITR/IM+AP: Automated compilation and computational analysis of regulatory networks&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project will provide advanced information management systems as well as software applications relevant to the fields of bioinformaticis, molecular biology, and medicine. It will contribute to fundamental Information Management by developing advanced algorithms for the efficient retrieval of information about biological pathways from research articles (using natural language processing techniques), machine-learning-assisted data mining in biological texts, and information management via a uniquely designed database with two levels of representation of biological information.&lt;br/&gt;&lt;br/&gt;The specific aims of the project are the following: to implement and test natural language processing techniques for the automatic retrieval of signal-transduction pathways from the research literature; to develop general and portable text-mining techniques for the automatic recognition of genes, proteins, and other domain terms, and of relationships between them; and to develop a mathematical framework for the statistical analysis of heterogeneous data on regulatory pathways.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121687</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121687</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1287" target="n1288">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+AP: Automated Compilation and Computational Analysis of Regulatory Networks</data>
      <data key="e_abstract">EIA-0121687&lt;br/&gt;Rzhetsky, Andrey&lt;br/&gt;Columbia University&lt;br/&gt;&lt;br/&gt;ITR/IM+AP: Automated compilation and computational analysis of regulatory networks&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project will provide advanced information management systems as well as software applications relevant to the fields of bioinformaticis, molecular biology, and medicine. It will contribute to fundamental Information Management by developing advanced algorithms for the efficient retrieval of information about biological pathways from research articles (using natural language processing techniques), machine-learning-assisted data mining in biological texts, and information management via a uniquely designed database with two levels of representation of biological information.&lt;br/&gt;&lt;br/&gt;The specific aims of the project are the following: to implement and test natural language processing techniques for the automatic retrieval of signal-transduction pathways from the research literature; to develop general and portable text-mining techniques for the automatic recognition of genes, proteins, and other domain terms, and of relationships between them; and to develop a mathematical framework for the statistical analysis of heterogeneous data on regulatory pathways.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121687</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121687</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1287" target="n1289">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+AP: Automated Compilation and Computational Analysis of Regulatory Networks</data>
      <data key="e_abstract">EIA-0121687&lt;br/&gt;Rzhetsky, Andrey&lt;br/&gt;Columbia University&lt;br/&gt;&lt;br/&gt;ITR/IM+AP: Automated compilation and computational analysis of regulatory networks&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project will provide advanced information management systems as well as software applications relevant to the fields of bioinformaticis, molecular biology, and medicine. It will contribute to fundamental Information Management by developing advanced algorithms for the efficient retrieval of information about biological pathways from research articles (using natural language processing techniques), machine-learning-assisted data mining in biological texts, and information management via a uniquely designed database with two levels of representation of biological information.&lt;br/&gt;&lt;br/&gt;The specific aims of the project are the following: to implement and test natural language processing techniques for the automatic retrieval of signal-transduction pathways from the research literature; to develop general and portable text-mining techniques for the automatic recognition of genes, proteins, and other domain terms, and of relationships between them; and to develop a mathematical framework for the statistical analysis of heterogeneous data on regulatory pathways.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121687</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121687</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1288" target="n1289">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+AP: Automated Compilation and Computational Analysis of Regulatory Networks</data>
      <data key="e_abstract">EIA-0121687&lt;br/&gt;Rzhetsky, Andrey&lt;br/&gt;Columbia University&lt;br/&gt;&lt;br/&gt;ITR/IM+AP: Automated compilation and computational analysis of regulatory networks&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project will provide advanced information management systems as well as software applications relevant to the fields of bioinformaticis, molecular biology, and medicine. It will contribute to fundamental Information Management by developing advanced algorithms for the efficient retrieval of information about biological pathways from research articles (using natural language processing techniques), machine-learning-assisted data mining in biological texts, and information management via a uniquely designed database with two levels of representation of biological information.&lt;br/&gt;&lt;br/&gt;The specific aims of the project are the following: to implement and test natural language processing techniques for the automatic retrieval of signal-transduction pathways from the research literature; to develop general and portable text-mining techniques for the automatic recognition of genes, proteins, and other domain terms, and of relationships between them; and to develop a mathematical framework for the statistical analysis of heterogeneous data on regulatory pathways.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121687</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121687</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1291" target="n1292">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Collaborative Research: ITR/AP Reconstructing Complex Evolutionary Histories</data>
      <data key="e_abstract">EIA-0121680&lt;br/&gt;Warnow, Tandy J&lt;br/&gt;University of Texas at Austin&lt;br/&gt;&lt;br/&gt;Collaborative Research: ITR/AP: Reconstructing Complex Evolutionary&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Reconstruction of the evolutionary history of a group of organisms has changed the face of biology and is being used increasingly in drug discovery, epidemiology, and genetic engineering. Unfortunately, such reconstructions typically involve solving difficult optimization problems, so that even moderately large datasets can require months to years of computation. In addition, almost all evolutionary reconstructions presently assume that the historical pattern is one of strict divergence that can be represented by a binary tree. This assumption is frequently violated, especially by plants which often hybridize readily and thus produce networks of relationships.&lt;br/&gt;&lt;br/&gt;This project brings together computer scientists and biologists from two institutions to develop new models and algorithms to address these two problems. Successful completion of this project will have an enormous impact by providing tools for reconstructing phylogenies of large datasets, and the first tools for inferring network models of evolution appropriate to hybridizing speciation. Such network models will alter how biologists think about speciation, while the development of methods for large-scale analyses will strongly benefit medical and pharmaceutical practice. &lt;br/&gt;Information technology will be advanced in fundamental ways as well, as the project will demonstrate how algorithm design and high-performance algorithm engineering can jointly solve very difficult discrete optimization problems.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121680</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121680</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1291" target="n1293">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Collaborative Research: ITR/AP Reconstructing Complex Evolutionary Histories</data>
      <data key="e_abstract">EIA-0121680&lt;br/&gt;Warnow, Tandy J&lt;br/&gt;University of Texas at Austin&lt;br/&gt;&lt;br/&gt;Collaborative Research: ITR/AP: Reconstructing Complex Evolutionary&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Reconstruction of the evolutionary history of a group of organisms has changed the face of biology and is being used increasingly in drug discovery, epidemiology, and genetic engineering. Unfortunately, such reconstructions typically involve solving difficult optimization problems, so that even moderately large datasets can require months to years of computation. In addition, almost all evolutionary reconstructions presently assume that the historical pattern is one of strict divergence that can be represented by a binary tree. This assumption is frequently violated, especially by plants which often hybridize readily and thus produce networks of relationships.&lt;br/&gt;&lt;br/&gt;This project brings together computer scientists and biologists from two institutions to develop new models and algorithms to address these two problems. Successful completion of this project will have an enormous impact by providing tools for reconstructing phylogenies of large datasets, and the first tools for inferring network models of evolution appropriate to hybridizing speciation. Such network models will alter how biologists think about speciation, while the development of methods for large-scale analyses will strongly benefit medical and pharmaceutical practice. &lt;br/&gt;Information technology will be advanced in fundamental ways as well, as the project will demonstrate how algorithm design and high-performance algorithm engineering can jointly solve very difficult discrete optimization problems.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121680</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121680</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1291" target="n1294">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Collaborative Research: ITR/AP Reconstructing Complex Evolutionary Histories</data>
      <data key="e_abstract">EIA-0121680&lt;br/&gt;Warnow, Tandy J&lt;br/&gt;University of Texas at Austin&lt;br/&gt;&lt;br/&gt;Collaborative Research: ITR/AP: Reconstructing Complex Evolutionary&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Reconstruction of the evolutionary history of a group of organisms has changed the face of biology and is being used increasingly in drug discovery, epidemiology, and genetic engineering. Unfortunately, such reconstructions typically involve solving difficult optimization problems, so that even moderately large datasets can require months to years of computation. In addition, almost all evolutionary reconstructions presently assume that the historical pattern is one of strict divergence that can be represented by a binary tree. This assumption is frequently violated, especially by plants which often hybridize readily and thus produce networks of relationships.&lt;br/&gt;&lt;br/&gt;This project brings together computer scientists and biologists from two institutions to develop new models and algorithms to address these two problems. Successful completion of this project will have an enormous impact by providing tools for reconstructing phylogenies of large datasets, and the first tools for inferring network models of evolution appropriate to hybridizing speciation. Such network models will alter how biologists think about speciation, while the development of methods for large-scale analyses will strongly benefit medical and pharmaceutical practice. &lt;br/&gt;Information technology will be advanced in fundamental ways as well, as the project will demonstrate how algorithm design and high-performance algorithm engineering can jointly solve very difficult discrete optimization problems.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121680</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121680</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1291" target="n1295">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Collaborative Research: ITR/AP Reconstructing Complex Evolutionary Histories</data>
      <data key="e_abstract">EIA-0121680&lt;br/&gt;Warnow, Tandy J&lt;br/&gt;University of Texas at Austin&lt;br/&gt;&lt;br/&gt;Collaborative Research: ITR/AP: Reconstructing Complex Evolutionary&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Reconstruction of the evolutionary history of a group of organisms has changed the face of biology and is being used increasingly in drug discovery, epidemiology, and genetic engineering. Unfortunately, such reconstructions typically involve solving difficult optimization problems, so that even moderately large datasets can require months to years of computation. In addition, almost all evolutionary reconstructions presently assume that the historical pattern is one of strict divergence that can be represented by a binary tree. This assumption is frequently violated, especially by plants which often hybridize readily and thus produce networks of relationships.&lt;br/&gt;&lt;br/&gt;This project brings together computer scientists and biologists from two institutions to develop new models and algorithms to address these two problems. Successful completion of this project will have an enormous impact by providing tools for reconstructing phylogenies of large datasets, and the first tools for inferring network models of evolution appropriate to hybridizing speciation. Such network models will alter how biologists think about speciation, while the development of methods for large-scale analyses will strongly benefit medical and pharmaceutical practice. &lt;br/&gt;Information technology will be advanced in fundamental ways as well, as the project will demonstrate how algorithm design and high-performance algorithm engineering can jointly solve very difficult discrete optimization problems.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121680</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121680</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1292" target="n1293">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Collaborative Research: ITR/AP Reconstructing Complex Evolutionary Histories</data>
      <data key="e_abstract">EIA-0121680&lt;br/&gt;Warnow, Tandy J&lt;br/&gt;University of Texas at Austin&lt;br/&gt;&lt;br/&gt;Collaborative Research: ITR/AP: Reconstructing Complex Evolutionary&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Reconstruction of the evolutionary history of a group of organisms has changed the face of biology and is being used increasingly in drug discovery, epidemiology, and genetic engineering. Unfortunately, such reconstructions typically involve solving difficult optimization problems, so that even moderately large datasets can require months to years of computation. In addition, almost all evolutionary reconstructions presently assume that the historical pattern is one of strict divergence that can be represented by a binary tree. This assumption is frequently violated, especially by plants which often hybridize readily and thus produce networks of relationships.&lt;br/&gt;&lt;br/&gt;This project brings together computer scientists and biologists from two institutions to develop new models and algorithms to address these two problems. Successful completion of this project will have an enormous impact by providing tools for reconstructing phylogenies of large datasets, and the first tools for inferring network models of evolution appropriate to hybridizing speciation. Such network models will alter how biologists think about speciation, while the development of methods for large-scale analyses will strongly benefit medical and pharmaceutical practice. &lt;br/&gt;Information technology will be advanced in fundamental ways as well, as the project will demonstrate how algorithm design and high-performance algorithm engineering can jointly solve very difficult discrete optimization problems.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121680</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121680</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1292" target="n1294">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Collaborative Research: ITR/AP Reconstructing Complex Evolutionary Histories</data>
      <data key="e_abstract">EIA-0121680&lt;br/&gt;Warnow, Tandy J&lt;br/&gt;University of Texas at Austin&lt;br/&gt;&lt;br/&gt;Collaborative Research: ITR/AP: Reconstructing Complex Evolutionary&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Reconstruction of the evolutionary history of a group of organisms has changed the face of biology and is being used increasingly in drug discovery, epidemiology, and genetic engineering. Unfortunately, such reconstructions typically involve solving difficult optimization problems, so that even moderately large datasets can require months to years of computation. In addition, almost all evolutionary reconstructions presently assume that the historical pattern is one of strict divergence that can be represented by a binary tree. This assumption is frequently violated, especially by plants which often hybridize readily and thus produce networks of relationships.&lt;br/&gt;&lt;br/&gt;This project brings together computer scientists and biologists from two institutions to develop new models and algorithms to address these two problems. Successful completion of this project will have an enormous impact by providing tools for reconstructing phylogenies of large datasets, and the first tools for inferring network models of evolution appropriate to hybridizing speciation. Such network models will alter how biologists think about speciation, while the development of methods for large-scale analyses will strongly benefit medical and pharmaceutical practice. &lt;br/&gt;Information technology will be advanced in fundamental ways as well, as the project will demonstrate how algorithm design and high-performance algorithm engineering can jointly solve very difficult discrete optimization problems.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121680</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121680</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1292" target="n1295">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Collaborative Research: ITR/AP Reconstructing Complex Evolutionary Histories</data>
      <data key="e_abstract">EIA-0121680&lt;br/&gt;Warnow, Tandy J&lt;br/&gt;University of Texas at Austin&lt;br/&gt;&lt;br/&gt;Collaborative Research: ITR/AP: Reconstructing Complex Evolutionary&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Reconstruction of the evolutionary history of a group of organisms has changed the face of biology and is being used increasingly in drug discovery, epidemiology, and genetic engineering. Unfortunately, such reconstructions typically involve solving difficult optimization problems, so that even moderately large datasets can require months to years of computation. In addition, almost all evolutionary reconstructions presently assume that the historical pattern is one of strict divergence that can be represented by a binary tree. This assumption is frequently violated, especially by plants which often hybridize readily and thus produce networks of relationships.&lt;br/&gt;&lt;br/&gt;This project brings together computer scientists and biologists from two institutions to develop new models and algorithms to address these two problems. Successful completion of this project will have an enormous impact by providing tools for reconstructing phylogenies of large datasets, and the first tools for inferring network models of evolution appropriate to hybridizing speciation. Such network models will alter how biologists think about speciation, while the development of methods for large-scale analyses will strongly benefit medical and pharmaceutical practice. &lt;br/&gt;Information technology will be advanced in fundamental ways as well, as the project will demonstrate how algorithm design and high-performance algorithm engineering can jointly solve very difficult discrete optimization problems.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121680</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121680</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1293" target="n1294">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Collaborative Research: ITR/AP Reconstructing Complex Evolutionary Histories</data>
      <data key="e_abstract">EIA-0121680&lt;br/&gt;Warnow, Tandy J&lt;br/&gt;University of Texas at Austin&lt;br/&gt;&lt;br/&gt;Collaborative Research: ITR/AP: Reconstructing Complex Evolutionary&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Reconstruction of the evolutionary history of a group of organisms has changed the face of biology and is being used increasingly in drug discovery, epidemiology, and genetic engineering. Unfortunately, such reconstructions typically involve solving difficult optimization problems, so that even moderately large datasets can require months to years of computation. In addition, almost all evolutionary reconstructions presently assume that the historical pattern is one of strict divergence that can be represented by a binary tree. This assumption is frequently violated, especially by plants which often hybridize readily and thus produce networks of relationships.&lt;br/&gt;&lt;br/&gt;This project brings together computer scientists and biologists from two institutions to develop new models and algorithms to address these two problems. Successful completion of this project will have an enormous impact by providing tools for reconstructing phylogenies of large datasets, and the first tools for inferring network models of evolution appropriate to hybridizing speciation. Such network models will alter how biologists think about speciation, while the development of methods for large-scale analyses will strongly benefit medical and pharmaceutical practice. &lt;br/&gt;Information technology will be advanced in fundamental ways as well, as the project will demonstrate how algorithm design and high-performance algorithm engineering can jointly solve very difficult discrete optimization problems.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121680</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121680</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1293" target="n1295">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Collaborative Research: ITR/AP Reconstructing Complex Evolutionary Histories</data>
      <data key="e_abstract">EIA-0121680&lt;br/&gt;Warnow, Tandy J&lt;br/&gt;University of Texas at Austin&lt;br/&gt;&lt;br/&gt;Collaborative Research: ITR/AP: Reconstructing Complex Evolutionary&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Reconstruction of the evolutionary history of a group of organisms has changed the face of biology and is being used increasingly in drug discovery, epidemiology, and genetic engineering. Unfortunately, such reconstructions typically involve solving difficult optimization problems, so that even moderately large datasets can require months to years of computation. In addition, almost all evolutionary reconstructions presently assume that the historical pattern is one of strict divergence that can be represented by a binary tree. This assumption is frequently violated, especially by plants which often hybridize readily and thus produce networks of relationships.&lt;br/&gt;&lt;br/&gt;This project brings together computer scientists and biologists from two institutions to develop new models and algorithms to address these two problems. Successful completion of this project will have an enormous impact by providing tools for reconstructing phylogenies of large datasets, and the first tools for inferring network models of evolution appropriate to hybridizing speciation. Such network models will alter how biologists think about speciation, while the development of methods for large-scale analyses will strongly benefit medical and pharmaceutical practice. &lt;br/&gt;Information technology will be advanced in fundamental ways as well, as the project will demonstrate how algorithm design and high-performance algorithm engineering can jointly solve very difficult discrete optimization problems.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121680</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121680</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1294" target="n1295">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Collaborative Research: ITR/AP Reconstructing Complex Evolutionary Histories</data>
      <data key="e_abstract">EIA-0121680&lt;br/&gt;Warnow, Tandy J&lt;br/&gt;University of Texas at Austin&lt;br/&gt;&lt;br/&gt;Collaborative Research: ITR/AP: Reconstructing Complex Evolutionary&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Reconstruction of the evolutionary history of a group of organisms has changed the face of biology and is being used increasingly in drug discovery, epidemiology, and genetic engineering. Unfortunately, such reconstructions typically involve solving difficult optimization problems, so that even moderately large datasets can require months to years of computation. In addition, almost all evolutionary reconstructions presently assume that the historical pattern is one of strict divergence that can be represented by a binary tree. This assumption is frequently violated, especially by plants which often hybridize readily and thus produce networks of relationships.&lt;br/&gt;&lt;br/&gt;This project brings together computer scientists and biologists from two institutions to develop new models and algorithms to address these two problems. Successful completion of this project will have an enormous impact by providing tools for reconstructing phylogenies of large datasets, and the first tools for inferring network models of evolution appropriate to hybridizing speciation. Such network models will alter how biologists think about speciation, while the development of methods for large-scale analyses will strongly benefit medical and pharmaceutical practice. &lt;br/&gt;Information technology will be advanced in fundamental ways as well, as the project will demonstrate how algorithm design and high-performance algorithm engineering can jointly solve very difficult discrete optimization problems.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121680</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121680</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1296" target="n1297">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM: Statistical Data Mining for Cosmology</data>
      <data key="e_abstract">Scientists are now confronted with many very large high-quality data sets. The potential scientific benefits of these data are offset by the laborious process of analyzing them to answer questions and test theories. This project will develop new data mining algorithms in pursuit of the goal of computer assisted discovery. Two key issues in achieving this are computational efficiency and autonomy. If scientists are to focus their energy on understanding, answers must arrive in minutes rather than days, hence the need for efficiency. Autonomy is important both from the data mining and the statistical perspective. Detailed searches for relationships, models, and parameters are too large for humans to undertake manually. New statistical methods will have to autonomously and quickly select models, test their significance, and report the results to search algorithms looking for new discoveries.&lt;br/&gt;&lt;br/&gt;The National Virtual Observatory (NVO) currently under construction is a model of the future of science. The NVO will assemble petabytes of data from many multi-wavelength sky surveys into a single repository. The new methods to be developed will be implemented in the domain of cosmology, but they will be applicable to all other sciences. &lt;br/&gt;&lt;br/&gt;The members of this project are computer scientists, physicists and statisticians who have a track record of collaborating closely. Working together they have produced: new algorithmic theory, new statistical theory, and publicly fielded software packages resulting from the theory, while developing new courseware and training students.&lt;br/&gt;&lt;br/&gt;This proposal involves research and education in the following areas:&lt;br/&gt;&lt;br/&gt;Nonparametric data analysis. Nonparametric statistical models enable powerful analysis techniques that make minimal assumptions, which is critical for scientific accuracy.&lt;br/&gt;&lt;br/&gt;Automated discovery. Statistical models can be used directly for discovery. Individual objects are compared to models to identify anomalies and data generated models are compared to theoretical models to refute or confirm hypotheses.&lt;br/&gt;&lt;br/&gt;Computational methods for fast analysis. The project will build on past successes of getting orders of magnitude speedups on operations such as Expectation Maximization based clustering and n-point correlations to make the new methods fast.&lt;br/&gt;&lt;br/&gt;Automated simulation parameter searching. Using all of the above methods, a system will be developed that starts with a parameterized simulation and some observational data. The system will search the space of parameters, testing the resulting simulation against the real data using nonparametric methods to determine the best settings.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121671</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121671</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1296" target="n1298">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM: Statistical Data Mining for Cosmology</data>
      <data key="e_abstract">Scientists are now confronted with many very large high-quality data sets. The potential scientific benefits of these data are offset by the laborious process of analyzing them to answer questions and test theories. This project will develop new data mining algorithms in pursuit of the goal of computer assisted discovery. Two key issues in achieving this are computational efficiency and autonomy. If scientists are to focus their energy on understanding, answers must arrive in minutes rather than days, hence the need for efficiency. Autonomy is important both from the data mining and the statistical perspective. Detailed searches for relationships, models, and parameters are too large for humans to undertake manually. New statistical methods will have to autonomously and quickly select models, test their significance, and report the results to search algorithms looking for new discoveries.&lt;br/&gt;&lt;br/&gt;The National Virtual Observatory (NVO) currently under construction is a model of the future of science. The NVO will assemble petabytes of data from many multi-wavelength sky surveys into a single repository. The new methods to be developed will be implemented in the domain of cosmology, but they will be applicable to all other sciences. &lt;br/&gt;&lt;br/&gt;The members of this project are computer scientists, physicists and statisticians who have a track record of collaborating closely. Working together they have produced: new algorithmic theory, new statistical theory, and publicly fielded software packages resulting from the theory, while developing new courseware and training students.&lt;br/&gt;&lt;br/&gt;This proposal involves research and education in the following areas:&lt;br/&gt;&lt;br/&gt;Nonparametric data analysis. Nonparametric statistical models enable powerful analysis techniques that make minimal assumptions, which is critical for scientific accuracy.&lt;br/&gt;&lt;br/&gt;Automated discovery. Statistical models can be used directly for discovery. Individual objects are compared to models to identify anomalies and data generated models are compared to theoretical models to refute or confirm hypotheses.&lt;br/&gt;&lt;br/&gt;Computational methods for fast analysis. The project will build on past successes of getting orders of magnitude speedups on operations such as Expectation Maximization based clustering and n-point correlations to make the new methods fast.&lt;br/&gt;&lt;br/&gt;Automated simulation parameter searching. Using all of the above methods, a system will be developed that starts with a parameterized simulation and some observational data. The system will search the space of parameters, testing the resulting simulation against the real data using nonparametric methods to determine the best settings.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121671</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121671</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1296" target="n1299">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM: Statistical Data Mining for Cosmology</data>
      <data key="e_abstract">Scientists are now confronted with many very large high-quality data sets. The potential scientific benefits of these data are offset by the laborious process of analyzing them to answer questions and test theories. This project will develop new data mining algorithms in pursuit of the goal of computer assisted discovery. Two key issues in achieving this are computational efficiency and autonomy. If scientists are to focus their energy on understanding, answers must arrive in minutes rather than days, hence the need for efficiency. Autonomy is important both from the data mining and the statistical perspective. Detailed searches for relationships, models, and parameters are too large for humans to undertake manually. New statistical methods will have to autonomously and quickly select models, test their significance, and report the results to search algorithms looking for new discoveries.&lt;br/&gt;&lt;br/&gt;The National Virtual Observatory (NVO) currently under construction is a model of the future of science. The NVO will assemble petabytes of data from many multi-wavelength sky surveys into a single repository. The new methods to be developed will be implemented in the domain of cosmology, but they will be applicable to all other sciences. &lt;br/&gt;&lt;br/&gt;The members of this project are computer scientists, physicists and statisticians who have a track record of collaborating closely. Working together they have produced: new algorithmic theory, new statistical theory, and publicly fielded software packages resulting from the theory, while developing new courseware and training students.&lt;br/&gt;&lt;br/&gt;This proposal involves research and education in the following areas:&lt;br/&gt;&lt;br/&gt;Nonparametric data analysis. Nonparametric statistical models enable powerful analysis techniques that make minimal assumptions, which is critical for scientific accuracy.&lt;br/&gt;&lt;br/&gt;Automated discovery. Statistical models can be used directly for discovery. Individual objects are compared to models to identify anomalies and data generated models are compared to theoretical models to refute or confirm hypotheses.&lt;br/&gt;&lt;br/&gt;Computational methods for fast analysis. The project will build on past successes of getting orders of magnitude speedups on operations such as Expectation Maximization based clustering and n-point correlations to make the new methods fast.&lt;br/&gt;&lt;br/&gt;Automated simulation parameter searching. Using all of the above methods, a system will be developed that starts with a parameterized simulation and some observational data. The system will search the space of parameters, testing the resulting simulation against the real data using nonparametric methods to determine the best settings.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121671</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121671</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1296" target="n1300">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM: Statistical Data Mining for Cosmology</data>
      <data key="e_abstract">Scientists are now confronted with many very large high-quality data sets. The potential scientific benefits of these data are offset by the laborious process of analyzing them to answer questions and test theories. This project will develop new data mining algorithms in pursuit of the goal of computer assisted discovery. Two key issues in achieving this are computational efficiency and autonomy. If scientists are to focus their energy on understanding, answers must arrive in minutes rather than days, hence the need for efficiency. Autonomy is important both from the data mining and the statistical perspective. Detailed searches for relationships, models, and parameters are too large for humans to undertake manually. New statistical methods will have to autonomously and quickly select models, test their significance, and report the results to search algorithms looking for new discoveries.&lt;br/&gt;&lt;br/&gt;The National Virtual Observatory (NVO) currently under construction is a model of the future of science. The NVO will assemble petabytes of data from many multi-wavelength sky surveys into a single repository. The new methods to be developed will be implemented in the domain of cosmology, but they will be applicable to all other sciences. &lt;br/&gt;&lt;br/&gt;The members of this project are computer scientists, physicists and statisticians who have a track record of collaborating closely. Working together they have produced: new algorithmic theory, new statistical theory, and publicly fielded software packages resulting from the theory, while developing new courseware and training students.&lt;br/&gt;&lt;br/&gt;This proposal involves research and education in the following areas:&lt;br/&gt;&lt;br/&gt;Nonparametric data analysis. Nonparametric statistical models enable powerful analysis techniques that make minimal assumptions, which is critical for scientific accuracy.&lt;br/&gt;&lt;br/&gt;Automated discovery. Statistical models can be used directly for discovery. Individual objects are compared to models to identify anomalies and data generated models are compared to theoretical models to refute or confirm hypotheses.&lt;br/&gt;&lt;br/&gt;Computational methods for fast analysis. The project will build on past successes of getting orders of magnitude speedups on operations such as Expectation Maximization based clustering and n-point correlations to make the new methods fast.&lt;br/&gt;&lt;br/&gt;Automated simulation parameter searching. Using all of the above methods, a system will be developed that starts with a parameterized simulation and some observational data. The system will search the space of parameters, testing the resulting simulation against the real data using nonparametric methods to determine the best settings.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121671</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121671</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1296" target="n1301">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM: Statistical Data Mining for Cosmology</data>
      <data key="e_abstract">Scientists are now confronted with many very large high-quality data sets. The potential scientific benefits of these data are offset by the laborious process of analyzing them to answer questions and test theories. This project will develop new data mining algorithms in pursuit of the goal of computer assisted discovery. Two key issues in achieving this are computational efficiency and autonomy. If scientists are to focus their energy on understanding, answers must arrive in minutes rather than days, hence the need for efficiency. Autonomy is important both from the data mining and the statistical perspective. Detailed searches for relationships, models, and parameters are too large for humans to undertake manually. New statistical methods will have to autonomously and quickly select models, test their significance, and report the results to search algorithms looking for new discoveries.&lt;br/&gt;&lt;br/&gt;The National Virtual Observatory (NVO) currently under construction is a model of the future of science. The NVO will assemble petabytes of data from many multi-wavelength sky surveys into a single repository. The new methods to be developed will be implemented in the domain of cosmology, but they will be applicable to all other sciences. &lt;br/&gt;&lt;br/&gt;The members of this project are computer scientists, physicists and statisticians who have a track record of collaborating closely. Working together they have produced: new algorithmic theory, new statistical theory, and publicly fielded software packages resulting from the theory, while developing new courseware and training students.&lt;br/&gt;&lt;br/&gt;This proposal involves research and education in the following areas:&lt;br/&gt;&lt;br/&gt;Nonparametric data analysis. Nonparametric statistical models enable powerful analysis techniques that make minimal assumptions, which is critical for scientific accuracy.&lt;br/&gt;&lt;br/&gt;Automated discovery. Statistical models can be used directly for discovery. Individual objects are compared to models to identify anomalies and data generated models are compared to theoretical models to refute or confirm hypotheses.&lt;br/&gt;&lt;br/&gt;Computational methods for fast analysis. The project will build on past successes of getting orders of magnitude speedups on operations such as Expectation Maximization based clustering and n-point correlations to make the new methods fast.&lt;br/&gt;&lt;br/&gt;Automated simulation parameter searching. Using all of the above methods, a system will be developed that starts with a parameterized simulation and some observational data. The system will search the space of parameters, testing the resulting simulation against the real data using nonparametric methods to determine the best settings.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121671</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121671</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1297" target="n1298">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM: Statistical Data Mining for Cosmology</data>
      <data key="e_abstract">Scientists are now confronted with many very large high-quality data sets. The potential scientific benefits of these data are offset by the laborious process of analyzing them to answer questions and test theories. This project will develop new data mining algorithms in pursuit of the goal of computer assisted discovery. Two key issues in achieving this are computational efficiency and autonomy. If scientists are to focus their energy on understanding, answers must arrive in minutes rather than days, hence the need for efficiency. Autonomy is important both from the data mining and the statistical perspective. Detailed searches for relationships, models, and parameters are too large for humans to undertake manually. New statistical methods will have to autonomously and quickly select models, test their significance, and report the results to search algorithms looking for new discoveries.&lt;br/&gt;&lt;br/&gt;The National Virtual Observatory (NVO) currently under construction is a model of the future of science. The NVO will assemble petabytes of data from many multi-wavelength sky surveys into a single repository. The new methods to be developed will be implemented in the domain of cosmology, but they will be applicable to all other sciences. &lt;br/&gt;&lt;br/&gt;The members of this project are computer scientists, physicists and statisticians who have a track record of collaborating closely. Working together they have produced: new algorithmic theory, new statistical theory, and publicly fielded software packages resulting from the theory, while developing new courseware and training students.&lt;br/&gt;&lt;br/&gt;This proposal involves research and education in the following areas:&lt;br/&gt;&lt;br/&gt;Nonparametric data analysis. Nonparametric statistical models enable powerful analysis techniques that make minimal assumptions, which is critical for scientific accuracy.&lt;br/&gt;&lt;br/&gt;Automated discovery. Statistical models can be used directly for discovery. Individual objects are compared to models to identify anomalies and data generated models are compared to theoretical models to refute or confirm hypotheses.&lt;br/&gt;&lt;br/&gt;Computational methods for fast analysis. The project will build on past successes of getting orders of magnitude speedups on operations such as Expectation Maximization based clustering and n-point correlations to make the new methods fast.&lt;br/&gt;&lt;br/&gt;Automated simulation parameter searching. Using all of the above methods, a system will be developed that starts with a parameterized simulation and some observational data. The system will search the space of parameters, testing the resulting simulation against the real data using nonparametric methods to determine the best settings.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121671</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121671</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1297" target="n1299">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM: Statistical Data Mining for Cosmology</data>
      <data key="e_abstract">Scientists are now confronted with many very large high-quality data sets. The potential scientific benefits of these data are offset by the laborious process of analyzing them to answer questions and test theories. This project will develop new data mining algorithms in pursuit of the goal of computer assisted discovery. Two key issues in achieving this are computational efficiency and autonomy. If scientists are to focus their energy on understanding, answers must arrive in minutes rather than days, hence the need for efficiency. Autonomy is important both from the data mining and the statistical perspective. Detailed searches for relationships, models, and parameters are too large for humans to undertake manually. New statistical methods will have to autonomously and quickly select models, test their significance, and report the results to search algorithms looking for new discoveries.&lt;br/&gt;&lt;br/&gt;The National Virtual Observatory (NVO) currently under construction is a model of the future of science. The NVO will assemble petabytes of data from many multi-wavelength sky surveys into a single repository. The new methods to be developed will be implemented in the domain of cosmology, but they will be applicable to all other sciences. &lt;br/&gt;&lt;br/&gt;The members of this project are computer scientists, physicists and statisticians who have a track record of collaborating closely. Working together they have produced: new algorithmic theory, new statistical theory, and publicly fielded software packages resulting from the theory, while developing new courseware and training students.&lt;br/&gt;&lt;br/&gt;This proposal involves research and education in the following areas:&lt;br/&gt;&lt;br/&gt;Nonparametric data analysis. Nonparametric statistical models enable powerful analysis techniques that make minimal assumptions, which is critical for scientific accuracy.&lt;br/&gt;&lt;br/&gt;Automated discovery. Statistical models can be used directly for discovery. Individual objects are compared to models to identify anomalies and data generated models are compared to theoretical models to refute or confirm hypotheses.&lt;br/&gt;&lt;br/&gt;Computational methods for fast analysis. The project will build on past successes of getting orders of magnitude speedups on operations such as Expectation Maximization based clustering and n-point correlations to make the new methods fast.&lt;br/&gt;&lt;br/&gt;Automated simulation parameter searching. Using all of the above methods, a system will be developed that starts with a parameterized simulation and some observational data. The system will search the space of parameters, testing the resulting simulation against the real data using nonparametric methods to determine the best settings.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121671</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121671</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1297" target="n1300">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM: Statistical Data Mining for Cosmology</data>
      <data key="e_abstract">Scientists are now confronted with many very large high-quality data sets. The potential scientific benefits of these data are offset by the laborious process of analyzing them to answer questions and test theories. This project will develop new data mining algorithms in pursuit of the goal of computer assisted discovery. Two key issues in achieving this are computational efficiency and autonomy. If scientists are to focus their energy on understanding, answers must arrive in minutes rather than days, hence the need for efficiency. Autonomy is important both from the data mining and the statistical perspective. Detailed searches for relationships, models, and parameters are too large for humans to undertake manually. New statistical methods will have to autonomously and quickly select models, test their significance, and report the results to search algorithms looking for new discoveries.&lt;br/&gt;&lt;br/&gt;The National Virtual Observatory (NVO) currently under construction is a model of the future of science. The NVO will assemble petabytes of data from many multi-wavelength sky surveys into a single repository. The new methods to be developed will be implemented in the domain of cosmology, but they will be applicable to all other sciences. &lt;br/&gt;&lt;br/&gt;The members of this project are computer scientists, physicists and statisticians who have a track record of collaborating closely. Working together they have produced: new algorithmic theory, new statistical theory, and publicly fielded software packages resulting from the theory, while developing new courseware and training students.&lt;br/&gt;&lt;br/&gt;This proposal involves research and education in the following areas:&lt;br/&gt;&lt;br/&gt;Nonparametric data analysis. Nonparametric statistical models enable powerful analysis techniques that make minimal assumptions, which is critical for scientific accuracy.&lt;br/&gt;&lt;br/&gt;Automated discovery. Statistical models can be used directly for discovery. Individual objects are compared to models to identify anomalies and data generated models are compared to theoretical models to refute or confirm hypotheses.&lt;br/&gt;&lt;br/&gt;Computational methods for fast analysis. The project will build on past successes of getting orders of magnitude speedups on operations such as Expectation Maximization based clustering and n-point correlations to make the new methods fast.&lt;br/&gt;&lt;br/&gt;Automated simulation parameter searching. Using all of the above methods, a system will be developed that starts with a parameterized simulation and some observational data. The system will search the space of parameters, testing the resulting simulation against the real data using nonparametric methods to determine the best settings.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121671</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121671</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1297" target="n1301">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM: Statistical Data Mining for Cosmology</data>
      <data key="e_abstract">Scientists are now confronted with many very large high-quality data sets. The potential scientific benefits of these data are offset by the laborious process of analyzing them to answer questions and test theories. This project will develop new data mining algorithms in pursuit of the goal of computer assisted discovery. Two key issues in achieving this are computational efficiency and autonomy. If scientists are to focus their energy on understanding, answers must arrive in minutes rather than days, hence the need for efficiency. Autonomy is important both from the data mining and the statistical perspective. Detailed searches for relationships, models, and parameters are too large for humans to undertake manually. New statistical methods will have to autonomously and quickly select models, test their significance, and report the results to search algorithms looking for new discoveries.&lt;br/&gt;&lt;br/&gt;The National Virtual Observatory (NVO) currently under construction is a model of the future of science. The NVO will assemble petabytes of data from many multi-wavelength sky surveys into a single repository. The new methods to be developed will be implemented in the domain of cosmology, but they will be applicable to all other sciences. &lt;br/&gt;&lt;br/&gt;The members of this project are computer scientists, physicists and statisticians who have a track record of collaborating closely. Working together they have produced: new algorithmic theory, new statistical theory, and publicly fielded software packages resulting from the theory, while developing new courseware and training students.&lt;br/&gt;&lt;br/&gt;This proposal involves research and education in the following areas:&lt;br/&gt;&lt;br/&gt;Nonparametric data analysis. Nonparametric statistical models enable powerful analysis techniques that make minimal assumptions, which is critical for scientific accuracy.&lt;br/&gt;&lt;br/&gt;Automated discovery. Statistical models can be used directly for discovery. Individual objects are compared to models to identify anomalies and data generated models are compared to theoretical models to refute or confirm hypotheses.&lt;br/&gt;&lt;br/&gt;Computational methods for fast analysis. The project will build on past successes of getting orders of magnitude speedups on operations such as Expectation Maximization based clustering and n-point correlations to make the new methods fast.&lt;br/&gt;&lt;br/&gt;Automated simulation parameter searching. Using all of the above methods, a system will be developed that starts with a parameterized simulation and some observational data. The system will search the space of parameters, testing the resulting simulation against the real data using nonparametric methods to determine the best settings.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121671</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121671</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1298" target="n1299">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM: Statistical Data Mining for Cosmology</data>
      <data key="e_abstract">Scientists are now confronted with many very large high-quality data sets. The potential scientific benefits of these data are offset by the laborious process of analyzing them to answer questions and test theories. This project will develop new data mining algorithms in pursuit of the goal of computer assisted discovery. Two key issues in achieving this are computational efficiency and autonomy. If scientists are to focus their energy on understanding, answers must arrive in minutes rather than days, hence the need for efficiency. Autonomy is important both from the data mining and the statistical perspective. Detailed searches for relationships, models, and parameters are too large for humans to undertake manually. New statistical methods will have to autonomously and quickly select models, test their significance, and report the results to search algorithms looking for new discoveries.&lt;br/&gt;&lt;br/&gt;The National Virtual Observatory (NVO) currently under construction is a model of the future of science. The NVO will assemble petabytes of data from many multi-wavelength sky surveys into a single repository. The new methods to be developed will be implemented in the domain of cosmology, but they will be applicable to all other sciences. &lt;br/&gt;&lt;br/&gt;The members of this project are computer scientists, physicists and statisticians who have a track record of collaborating closely. Working together they have produced: new algorithmic theory, new statistical theory, and publicly fielded software packages resulting from the theory, while developing new courseware and training students.&lt;br/&gt;&lt;br/&gt;This proposal involves research and education in the following areas:&lt;br/&gt;&lt;br/&gt;Nonparametric data analysis. Nonparametric statistical models enable powerful analysis techniques that make minimal assumptions, which is critical for scientific accuracy.&lt;br/&gt;&lt;br/&gt;Automated discovery. Statistical models can be used directly for discovery. Individual objects are compared to models to identify anomalies and data generated models are compared to theoretical models to refute or confirm hypotheses.&lt;br/&gt;&lt;br/&gt;Computational methods for fast analysis. The project will build on past successes of getting orders of magnitude speedups on operations such as Expectation Maximization based clustering and n-point correlations to make the new methods fast.&lt;br/&gt;&lt;br/&gt;Automated simulation parameter searching. Using all of the above methods, a system will be developed that starts with a parameterized simulation and some observational data. The system will search the space of parameters, testing the resulting simulation against the real data using nonparametric methods to determine the best settings.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121671</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121671</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1298" target="n1300">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM: Statistical Data Mining for Cosmology</data>
      <data key="e_abstract">Scientists are now confronted with many very large high-quality data sets. The potential scientific benefits of these data are offset by the laborious process of analyzing them to answer questions and test theories. This project will develop new data mining algorithms in pursuit of the goal of computer assisted discovery. Two key issues in achieving this are computational efficiency and autonomy. If scientists are to focus their energy on understanding, answers must arrive in minutes rather than days, hence the need for efficiency. Autonomy is important both from the data mining and the statistical perspective. Detailed searches for relationships, models, and parameters are too large for humans to undertake manually. New statistical methods will have to autonomously and quickly select models, test their significance, and report the results to search algorithms looking for new discoveries.&lt;br/&gt;&lt;br/&gt;The National Virtual Observatory (NVO) currently under construction is a model of the future of science. The NVO will assemble petabytes of data from many multi-wavelength sky surveys into a single repository. The new methods to be developed will be implemented in the domain of cosmology, but they will be applicable to all other sciences. &lt;br/&gt;&lt;br/&gt;The members of this project are computer scientists, physicists and statisticians who have a track record of collaborating closely. Working together they have produced: new algorithmic theory, new statistical theory, and publicly fielded software packages resulting from the theory, while developing new courseware and training students.&lt;br/&gt;&lt;br/&gt;This proposal involves research and education in the following areas:&lt;br/&gt;&lt;br/&gt;Nonparametric data analysis. Nonparametric statistical models enable powerful analysis techniques that make minimal assumptions, which is critical for scientific accuracy.&lt;br/&gt;&lt;br/&gt;Automated discovery. Statistical models can be used directly for discovery. Individual objects are compared to models to identify anomalies and data generated models are compared to theoretical models to refute or confirm hypotheses.&lt;br/&gt;&lt;br/&gt;Computational methods for fast analysis. The project will build on past successes of getting orders of magnitude speedups on operations such as Expectation Maximization based clustering and n-point correlations to make the new methods fast.&lt;br/&gt;&lt;br/&gt;Automated simulation parameter searching. Using all of the above methods, a system will be developed that starts with a parameterized simulation and some observational data. The system will search the space of parameters, testing the resulting simulation against the real data using nonparametric methods to determine the best settings.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121671</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121671</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1298" target="n1301">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM: Statistical Data Mining for Cosmology</data>
      <data key="e_abstract">Scientists are now confronted with many very large high-quality data sets. The potential scientific benefits of these data are offset by the laborious process of analyzing them to answer questions and test theories. This project will develop new data mining algorithms in pursuit of the goal of computer assisted discovery. Two key issues in achieving this are computational efficiency and autonomy. If scientists are to focus their energy on understanding, answers must arrive in minutes rather than days, hence the need for efficiency. Autonomy is important both from the data mining and the statistical perspective. Detailed searches for relationships, models, and parameters are too large for humans to undertake manually. New statistical methods will have to autonomously and quickly select models, test their significance, and report the results to search algorithms looking for new discoveries.&lt;br/&gt;&lt;br/&gt;The National Virtual Observatory (NVO) currently under construction is a model of the future of science. The NVO will assemble petabytes of data from many multi-wavelength sky surveys into a single repository. The new methods to be developed will be implemented in the domain of cosmology, but they will be applicable to all other sciences. &lt;br/&gt;&lt;br/&gt;The members of this project are computer scientists, physicists and statisticians who have a track record of collaborating closely. Working together they have produced: new algorithmic theory, new statistical theory, and publicly fielded software packages resulting from the theory, while developing new courseware and training students.&lt;br/&gt;&lt;br/&gt;This proposal involves research and education in the following areas:&lt;br/&gt;&lt;br/&gt;Nonparametric data analysis. Nonparametric statistical models enable powerful analysis techniques that make minimal assumptions, which is critical for scientific accuracy.&lt;br/&gt;&lt;br/&gt;Automated discovery. Statistical models can be used directly for discovery. Individual objects are compared to models to identify anomalies and data generated models are compared to theoretical models to refute or confirm hypotheses.&lt;br/&gt;&lt;br/&gt;Computational methods for fast analysis. The project will build on past successes of getting orders of magnitude speedups on operations such as Expectation Maximization based clustering and n-point correlations to make the new methods fast.&lt;br/&gt;&lt;br/&gt;Automated simulation parameter searching. Using all of the above methods, a system will be developed that starts with a parameterized simulation and some observational data. The system will search the space of parameters, testing the resulting simulation against the real data using nonparametric methods to determine the best settings.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121671</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121671</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1299" target="n1300">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM: Statistical Data Mining for Cosmology</data>
      <data key="e_abstract">Scientists are now confronted with many very large high-quality data sets. The potential scientific benefits of these data are offset by the laborious process of analyzing them to answer questions and test theories. This project will develop new data mining algorithms in pursuit of the goal of computer assisted discovery. Two key issues in achieving this are computational efficiency and autonomy. If scientists are to focus their energy on understanding, answers must arrive in minutes rather than days, hence the need for efficiency. Autonomy is important both from the data mining and the statistical perspective. Detailed searches for relationships, models, and parameters are too large for humans to undertake manually. New statistical methods will have to autonomously and quickly select models, test their significance, and report the results to search algorithms looking for new discoveries.&lt;br/&gt;&lt;br/&gt;The National Virtual Observatory (NVO) currently under construction is a model of the future of science. The NVO will assemble petabytes of data from many multi-wavelength sky surveys into a single repository. The new methods to be developed will be implemented in the domain of cosmology, but they will be applicable to all other sciences. &lt;br/&gt;&lt;br/&gt;The members of this project are computer scientists, physicists and statisticians who have a track record of collaborating closely. Working together they have produced: new algorithmic theory, new statistical theory, and publicly fielded software packages resulting from the theory, while developing new courseware and training students.&lt;br/&gt;&lt;br/&gt;This proposal involves research and education in the following areas:&lt;br/&gt;&lt;br/&gt;Nonparametric data analysis. Nonparametric statistical models enable powerful analysis techniques that make minimal assumptions, which is critical for scientific accuracy.&lt;br/&gt;&lt;br/&gt;Automated discovery. Statistical models can be used directly for discovery. Individual objects are compared to models to identify anomalies and data generated models are compared to theoretical models to refute or confirm hypotheses.&lt;br/&gt;&lt;br/&gt;Computational methods for fast analysis. The project will build on past successes of getting orders of magnitude speedups on operations such as Expectation Maximization based clustering and n-point correlations to make the new methods fast.&lt;br/&gt;&lt;br/&gt;Automated simulation parameter searching. Using all of the above methods, a system will be developed that starts with a parameterized simulation and some observational data. The system will search the space of parameters, testing the resulting simulation against the real data using nonparametric methods to determine the best settings.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121671</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121671</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1299" target="n1301">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM: Statistical Data Mining for Cosmology</data>
      <data key="e_abstract">Scientists are now confronted with many very large high-quality data sets. The potential scientific benefits of these data are offset by the laborious process of analyzing them to answer questions and test theories. This project will develop new data mining algorithms in pursuit of the goal of computer assisted discovery. Two key issues in achieving this are computational efficiency and autonomy. If scientists are to focus their energy on understanding, answers must arrive in minutes rather than days, hence the need for efficiency. Autonomy is important both from the data mining and the statistical perspective. Detailed searches for relationships, models, and parameters are too large for humans to undertake manually. New statistical methods will have to autonomously and quickly select models, test their significance, and report the results to search algorithms looking for new discoveries.&lt;br/&gt;&lt;br/&gt;The National Virtual Observatory (NVO) currently under construction is a model of the future of science. The NVO will assemble petabytes of data from many multi-wavelength sky surveys into a single repository. The new methods to be developed will be implemented in the domain of cosmology, but they will be applicable to all other sciences. &lt;br/&gt;&lt;br/&gt;The members of this project are computer scientists, physicists and statisticians who have a track record of collaborating closely. Working together they have produced: new algorithmic theory, new statistical theory, and publicly fielded software packages resulting from the theory, while developing new courseware and training students.&lt;br/&gt;&lt;br/&gt;This proposal involves research and education in the following areas:&lt;br/&gt;&lt;br/&gt;Nonparametric data analysis. Nonparametric statistical models enable powerful analysis techniques that make minimal assumptions, which is critical for scientific accuracy.&lt;br/&gt;&lt;br/&gt;Automated discovery. Statistical models can be used directly for discovery. Individual objects are compared to models to identify anomalies and data generated models are compared to theoretical models to refute or confirm hypotheses.&lt;br/&gt;&lt;br/&gt;Computational methods for fast analysis. The project will build on past successes of getting orders of magnitude speedups on operations such as Expectation Maximization based clustering and n-point correlations to make the new methods fast.&lt;br/&gt;&lt;br/&gt;Automated simulation parameter searching. Using all of the above methods, a system will be developed that starts with a parameterized simulation and some observational data. The system will search the space of parameters, testing the resulting simulation against the real data using nonparametric methods to determine the best settings.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121671</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121671</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1300" target="n1301">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM: Statistical Data Mining for Cosmology</data>
      <data key="e_abstract">Scientists are now confronted with many very large high-quality data sets. The potential scientific benefits of these data are offset by the laborious process of analyzing them to answer questions and test theories. This project will develop new data mining algorithms in pursuit of the goal of computer assisted discovery. Two key issues in achieving this are computational efficiency and autonomy. If scientists are to focus their energy on understanding, answers must arrive in minutes rather than days, hence the need for efficiency. Autonomy is important both from the data mining and the statistical perspective. Detailed searches for relationships, models, and parameters are too large for humans to undertake manually. New statistical methods will have to autonomously and quickly select models, test their significance, and report the results to search algorithms looking for new discoveries.&lt;br/&gt;&lt;br/&gt;The National Virtual Observatory (NVO) currently under construction is a model of the future of science. The NVO will assemble petabytes of data from many multi-wavelength sky surveys into a single repository. The new methods to be developed will be implemented in the domain of cosmology, but they will be applicable to all other sciences. &lt;br/&gt;&lt;br/&gt;The members of this project are computer scientists, physicists and statisticians who have a track record of collaborating closely. Working together they have produced: new algorithmic theory, new statistical theory, and publicly fielded software packages resulting from the theory, while developing new courseware and training students.&lt;br/&gt;&lt;br/&gt;This proposal involves research and education in the following areas:&lt;br/&gt;&lt;br/&gt;Nonparametric data analysis. Nonparametric statistical models enable powerful analysis techniques that make minimal assumptions, which is critical for scientific accuracy.&lt;br/&gt;&lt;br/&gt;Automated discovery. Statistical models can be used directly for discovery. Individual objects are compared to models to identify anomalies and data generated models are compared to theoretical models to refute or confirm hypotheses.&lt;br/&gt;&lt;br/&gt;Computational methods for fast analysis. The project will build on past successes of getting orders of magnitude speedups on operations such as Expectation Maximization based clustering and n-point correlations to make the new methods fast.&lt;br/&gt;&lt;br/&gt;Automated simulation parameter searching. Using all of the above methods, a system will be developed that starts with a parameterized simulation and some observational data. The system will search the space of parameters, testing the resulting simulation against the real data using nonparametric methods to determine the best settings.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121671</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121671</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1302" target="n1303">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Creation of the AVIDD Data Facility: A Distributed Facility for Managing, Analyzing and Visualizing Instrument-Driven Data</data>
      <data key="e_abstract">EIA-0116050&lt;br/&gt;Michael A. McRobbie&lt;br/&gt;Indiana University - Bloomington&lt;br/&gt;&lt;br/&gt;MRI: Creation of the AVIDD Data Facility: a Distributed Facility for Managing, Analyzing, and Visualizing Instrument-Driven Data &lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a broad range of instrument-driven data-intensive science areas. The proposed distributed facility for managing, analyzing, and visualizing instrument-driven data would address the data life cycle consisting of data capture and remote data reduction; high speed data transfer; real time data analysis and processing; data storage; data retreival; data analysis and postprocessing; data visualization; and the use of remote data stores. Among the research projects enhanced and enabled by the proposed facility are both computer science and applications area projects, for example work on end-to-end real time data management for remote control and use of beam-line systems by X-ray crystallographers</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116050</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116050</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1134" target="n1302">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Creation of the AVIDD Data Facility: A Distributed Facility for Managing, Analyzing and Visualizing Instrument-Driven Data</data>
      <data key="e_abstract">EIA-0116050&lt;br/&gt;Michael A. McRobbie&lt;br/&gt;Indiana University - Bloomington&lt;br/&gt;&lt;br/&gt;MRI: Creation of the AVIDD Data Facility: a Distributed Facility for Managing, Analyzing, and Visualizing Instrument-Driven Data &lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a broad range of instrument-driven data-intensive science areas. The proposed distributed facility for managing, analyzing, and visualizing instrument-driven data would address the data life cycle consisting of data capture and remote data reduction; high speed data transfer; real time data analysis and processing; data storage; data retreival; data analysis and postprocessing; data visualization; and the use of remote data stores. Among the research projects enhanced and enabled by the proposed facility are both computer science and applications area projects, for example work on end-to-end real time data management for remote control and use of beam-line systems by X-ray crystallographers</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116050</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116050</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1302" target="n1305">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Creation of the AVIDD Data Facility: A Distributed Facility for Managing, Analyzing and Visualizing Instrument-Driven Data</data>
      <data key="e_abstract">EIA-0116050&lt;br/&gt;Michael A. McRobbie&lt;br/&gt;Indiana University - Bloomington&lt;br/&gt;&lt;br/&gt;MRI: Creation of the AVIDD Data Facility: a Distributed Facility for Managing, Analyzing, and Visualizing Instrument-Driven Data &lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a broad range of instrument-driven data-intensive science areas. The proposed distributed facility for managing, analyzing, and visualizing instrument-driven data would address the data life cycle consisting of data capture and remote data reduction; high speed data transfer; real time data analysis and processing; data storage; data retreival; data analysis and postprocessing; data visualization; and the use of remote data stores. Among the research projects enhanced and enabled by the proposed facility are both computer science and applications area projects, for example work on end-to-end real time data management for remote control and use of beam-line systems by X-ray crystallographers</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116050</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116050</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1134" target="n1303">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Creation of the AVIDD Data Facility: A Distributed Facility for Managing, Analyzing and Visualizing Instrument-Driven Data</data>
      <data key="e_abstract">EIA-0116050&lt;br/&gt;Michael A. McRobbie&lt;br/&gt;Indiana University - Bloomington&lt;br/&gt;&lt;br/&gt;MRI: Creation of the AVIDD Data Facility: a Distributed Facility for Managing, Analyzing, and Visualizing Instrument-Driven Data &lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a broad range of instrument-driven data-intensive science areas. The proposed distributed facility for managing, analyzing, and visualizing instrument-driven data would address the data life cycle consisting of data capture and remote data reduction; high speed data transfer; real time data analysis and processing; data storage; data retreival; data analysis and postprocessing; data visualization; and the use of remote data stores. Among the research projects enhanced and enabled by the proposed facility are both computer science and applications area projects, for example work on end-to-end real time data management for remote control and use of beam-line systems by X-ray crystallographers</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116050</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116050</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1303" target="n1305">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Creation of the AVIDD Data Facility: A Distributed Facility for Managing, Analyzing and Visualizing Instrument-Driven Data</data>
      <data key="e_abstract">EIA-0116050&lt;br/&gt;Michael A. McRobbie&lt;br/&gt;Indiana University - Bloomington&lt;br/&gt;&lt;br/&gt;MRI: Creation of the AVIDD Data Facility: a Distributed Facility for Managing, Analyzing, and Visualizing Instrument-Driven Data &lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a broad range of instrument-driven data-intensive science areas. The proposed distributed facility for managing, analyzing, and visualizing instrument-driven data would address the data life cycle consisting of data capture and remote data reduction; high speed data transfer; real time data analysis and processing; data storage; data retreival; data analysis and postprocessing; data visualization; and the use of remote data stores. Among the research projects enhanced and enabled by the proposed facility are both computer science and applications area projects, for example work on end-to-end real time data management for remote control and use of beam-line systems by X-ray crystallographers</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116050</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116050</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1134" target="n1305">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Creation of the AVIDD Data Facility: A Distributed Facility for Managing, Analyzing and Visualizing Instrument-Driven Data</data>
      <data key="e_abstract">EIA-0116050&lt;br/&gt;Michael A. McRobbie&lt;br/&gt;Indiana University - Bloomington&lt;br/&gt;&lt;br/&gt;MRI: Creation of the AVIDD Data Facility: a Distributed Facility for Managing, Analyzing, and Visualizing Instrument-Driven Data &lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a broad range of instrument-driven data-intensive science areas. The proposed distributed facility for managing, analyzing, and visualizing instrument-driven data would address the data life cycle consisting of data capture and remote data reduction; high speed data transfer; real time data analysis and processing; data storage; data retreival; data analysis and postprocessing; data visualization; and the use of remote data stores. Among the research projects enhanced and enabled by the proposed facility are both computer science and applications area projects, for example work on end-to-end real time data management for remote control and use of beam-line systems by X-ray crystallographers</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116050</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116050</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1279" target="n1307">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">EI: A Concentration Track in Embedded Systems</data>
      <data key="e_abstract">EIA- 0122600&lt;br/&gt;Panchanathan, Sethuraman&lt;br/&gt;Arizona State University&lt;br/&gt;&lt;br/&gt;Educational Innovation: A Concentration Track in Embedded Systems&lt;br/&gt;&lt;br/&gt;This project presents a novel structure for a concentration track in embedded systems. The new curriculum combines important aspects of content, synthesized from the latest research in academia, with industry research aspects channeled through a capstone project (implemented as internships in industry). The curriculum spans a spectrum of activities related to the design and delivery of educational and research efforts and is characterized by three main innovative components namely; a new industry-university collaborative model for integrating basic and applied research, creation and delivery of state-of-the-art course content and appropriate laboratories, delivery of a capstone project through internships. The embedded systems curriculum emphasizes fundamental issues such as the balance between hardware and software and the respective trade-offs in building embedded systems. Practical design experience is fostered through the courses, laboratory experiments, and capstone projects that expose the students to the state-of-the-art design methodologies. In addition, internships, funded by industry, expose the students to work on applied projects mentored by the industrial supervisors and faculty fellows.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">122600</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">122600</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1279" target="n1308">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">EI: A Concentration Track in Embedded Systems</data>
      <data key="e_abstract">EIA- 0122600&lt;br/&gt;Panchanathan, Sethuraman&lt;br/&gt;Arizona State University&lt;br/&gt;&lt;br/&gt;Educational Innovation: A Concentration Track in Embedded Systems&lt;br/&gt;&lt;br/&gt;This project presents a novel structure for a concentration track in embedded systems. The new curriculum combines important aspects of content, synthesized from the latest research in academia, with industry research aspects channeled through a capstone project (implemented as internships in industry). The curriculum spans a spectrum of activities related to the design and delivery of educational and research efforts and is characterized by three main innovative components namely; a new industry-university collaborative model for integrating basic and applied research, creation and delivery of state-of-the-art course content and appropriate laboratories, delivery of a capstone project through internships. The embedded systems curriculum emphasizes fundamental issues such as the balance between hardware and software and the respective trade-offs in building embedded systems. Practical design experience is fostered through the courses, laboratory experiments, and capstone projects that expose the students to the state-of-the-art design methodologies. In addition, internships, funded by industry, expose the students to work on applied projects mentored by the industrial supervisors and faculty fellows.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">122600</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">122600</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1279" target="n1309">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">EI: A Concentration Track in Embedded Systems</data>
      <data key="e_abstract">EIA- 0122600&lt;br/&gt;Panchanathan, Sethuraman&lt;br/&gt;Arizona State University&lt;br/&gt;&lt;br/&gt;Educational Innovation: A Concentration Track in Embedded Systems&lt;br/&gt;&lt;br/&gt;This project presents a novel structure for a concentration track in embedded systems. The new curriculum combines important aspects of content, synthesized from the latest research in academia, with industry research aspects channeled through a capstone project (implemented as internships in industry). The curriculum spans a spectrum of activities related to the design and delivery of educational and research efforts and is characterized by three main innovative components namely; a new industry-university collaborative model for integrating basic and applied research, creation and delivery of state-of-the-art course content and appropriate laboratories, delivery of a capstone project through internships. The embedded systems curriculum emphasizes fundamental issues such as the balance between hardware and software and the respective trade-offs in building embedded systems. Practical design experience is fostered through the courses, laboratory experiments, and capstone projects that expose the students to the state-of-the-art design methodologies. In addition, internships, funded by industry, expose the students to work on applied projects mentored by the industrial supervisors and faculty fellows.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">122600</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">122600</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1279" target="n1310">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">EI: A Concentration Track in Embedded Systems</data>
      <data key="e_abstract">EIA- 0122600&lt;br/&gt;Panchanathan, Sethuraman&lt;br/&gt;Arizona State University&lt;br/&gt;&lt;br/&gt;Educational Innovation: A Concentration Track in Embedded Systems&lt;br/&gt;&lt;br/&gt;This project presents a novel structure for a concentration track in embedded systems. The new curriculum combines important aspects of content, synthesized from the latest research in academia, with industry research aspects channeled through a capstone project (implemented as internships in industry). The curriculum spans a spectrum of activities related to the design and delivery of educational and research efforts and is characterized by three main innovative components namely; a new industry-university collaborative model for integrating basic and applied research, creation and delivery of state-of-the-art course content and appropriate laboratories, delivery of a capstone project through internships. The embedded systems curriculum emphasizes fundamental issues such as the balance between hardware and software and the respective trade-offs in building embedded systems. Practical design experience is fostered through the courses, laboratory experiments, and capstone projects that expose the students to the state-of-the-art design methodologies. In addition, internships, funded by industry, expose the students to work on applied projects mentored by the industrial supervisors and faculty fellows.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">122600</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">122600</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1307" target="n1308">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">EI: A Concentration Track in Embedded Systems</data>
      <data key="e_abstract">EIA- 0122600&lt;br/&gt;Panchanathan, Sethuraman&lt;br/&gt;Arizona State University&lt;br/&gt;&lt;br/&gt;Educational Innovation: A Concentration Track in Embedded Systems&lt;br/&gt;&lt;br/&gt;This project presents a novel structure for a concentration track in embedded systems. The new curriculum combines important aspects of content, synthesized from the latest research in academia, with industry research aspects channeled through a capstone project (implemented as internships in industry). The curriculum spans a spectrum of activities related to the design and delivery of educational and research efforts and is characterized by three main innovative components namely; a new industry-university collaborative model for integrating basic and applied research, creation and delivery of state-of-the-art course content and appropriate laboratories, delivery of a capstone project through internships. The embedded systems curriculum emphasizes fundamental issues such as the balance between hardware and software and the respective trade-offs in building embedded systems. Practical design experience is fostered through the courses, laboratory experiments, and capstone projects that expose the students to the state-of-the-art design methodologies. In addition, internships, funded by industry, expose the students to work on applied projects mentored by the industrial supervisors and faculty fellows.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">122600</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">122600</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1307" target="n1309">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">EI: A Concentration Track in Embedded Systems</data>
      <data key="e_abstract">EIA- 0122600&lt;br/&gt;Panchanathan, Sethuraman&lt;br/&gt;Arizona State University&lt;br/&gt;&lt;br/&gt;Educational Innovation: A Concentration Track in Embedded Systems&lt;br/&gt;&lt;br/&gt;This project presents a novel structure for a concentration track in embedded systems. The new curriculum combines important aspects of content, synthesized from the latest research in academia, with industry research aspects channeled through a capstone project (implemented as internships in industry). The curriculum spans a spectrum of activities related to the design and delivery of educational and research efforts and is characterized by three main innovative components namely; a new industry-university collaborative model for integrating basic and applied research, creation and delivery of state-of-the-art course content and appropriate laboratories, delivery of a capstone project through internships. The embedded systems curriculum emphasizes fundamental issues such as the balance between hardware and software and the respective trade-offs in building embedded systems. Practical design experience is fostered through the courses, laboratory experiments, and capstone projects that expose the students to the state-of-the-art design methodologies. In addition, internships, funded by industry, expose the students to work on applied projects mentored by the industrial supervisors and faculty fellows.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">122600</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">122600</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1307" target="n1310">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">EI: A Concentration Track in Embedded Systems</data>
      <data key="e_abstract">EIA- 0122600&lt;br/&gt;Panchanathan, Sethuraman&lt;br/&gt;Arizona State University&lt;br/&gt;&lt;br/&gt;Educational Innovation: A Concentration Track in Embedded Systems&lt;br/&gt;&lt;br/&gt;This project presents a novel structure for a concentration track in embedded systems. The new curriculum combines important aspects of content, synthesized from the latest research in academia, with industry research aspects channeled through a capstone project (implemented as internships in industry). The curriculum spans a spectrum of activities related to the design and delivery of educational and research efforts and is characterized by three main innovative components namely; a new industry-university collaborative model for integrating basic and applied research, creation and delivery of state-of-the-art course content and appropriate laboratories, delivery of a capstone project through internships. The embedded systems curriculum emphasizes fundamental issues such as the balance between hardware and software and the respective trade-offs in building embedded systems. Practical design experience is fostered through the courses, laboratory experiments, and capstone projects that expose the students to the state-of-the-art design methodologies. In addition, internships, funded by industry, expose the students to work on applied projects mentored by the industrial supervisors and faculty fellows.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">122600</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">122600</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1308" target="n1309">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">EI: A Concentration Track in Embedded Systems</data>
      <data key="e_abstract">EIA- 0122600&lt;br/&gt;Panchanathan, Sethuraman&lt;br/&gt;Arizona State University&lt;br/&gt;&lt;br/&gt;Educational Innovation: A Concentration Track in Embedded Systems&lt;br/&gt;&lt;br/&gt;This project presents a novel structure for a concentration track in embedded systems. The new curriculum combines important aspects of content, synthesized from the latest research in academia, with industry research aspects channeled through a capstone project (implemented as internships in industry). The curriculum spans a spectrum of activities related to the design and delivery of educational and research efforts and is characterized by three main innovative components namely; a new industry-university collaborative model for integrating basic and applied research, creation and delivery of state-of-the-art course content and appropriate laboratories, delivery of a capstone project through internships. The embedded systems curriculum emphasizes fundamental issues such as the balance between hardware and software and the respective trade-offs in building embedded systems. Practical design experience is fostered through the courses, laboratory experiments, and capstone projects that expose the students to the state-of-the-art design methodologies. In addition, internships, funded by industry, expose the students to work on applied projects mentored by the industrial supervisors and faculty fellows.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">122600</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">122600</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1308" target="n1310">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">EI: A Concentration Track in Embedded Systems</data>
      <data key="e_abstract">EIA- 0122600&lt;br/&gt;Panchanathan, Sethuraman&lt;br/&gt;Arizona State University&lt;br/&gt;&lt;br/&gt;Educational Innovation: A Concentration Track in Embedded Systems&lt;br/&gt;&lt;br/&gt;This project presents a novel structure for a concentration track in embedded systems. The new curriculum combines important aspects of content, synthesized from the latest research in academia, with industry research aspects channeled through a capstone project (implemented as internships in industry). The curriculum spans a spectrum of activities related to the design and delivery of educational and research efforts and is characterized by three main innovative components namely; a new industry-university collaborative model for integrating basic and applied research, creation and delivery of state-of-the-art course content and appropriate laboratories, delivery of a capstone project through internships. The embedded systems curriculum emphasizes fundamental issues such as the balance between hardware and software and the respective trade-offs in building embedded systems. Practical design experience is fostered through the courses, laboratory experiments, and capstone projects that expose the students to the state-of-the-art design methodologies. In addition, internships, funded by industry, expose the students to work on applied projects mentored by the industrial supervisors and faculty fellows.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">122600</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">122600</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1309" target="n1310">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">EI: A Concentration Track in Embedded Systems</data>
      <data key="e_abstract">EIA- 0122600&lt;br/&gt;Panchanathan, Sethuraman&lt;br/&gt;Arizona State University&lt;br/&gt;&lt;br/&gt;Educational Innovation: A Concentration Track in Embedded Systems&lt;br/&gt;&lt;br/&gt;This project presents a novel structure for a concentration track in embedded systems. The new curriculum combines important aspects of content, synthesized from the latest research in academia, with industry research aspects channeled through a capstone project (implemented as internships in industry). The curriculum spans a spectrum of activities related to the design and delivery of educational and research efforts and is characterized by three main innovative components namely; a new industry-university collaborative model for integrating basic and applied research, creation and delivery of state-of-the-art course content and appropriate laboratories, delivery of a capstone project through internships. The embedded systems curriculum emphasizes fundamental issues such as the balance between hardware and software and the respective trade-offs in building embedded systems. Practical design experience is fostered through the courses, laboratory experiments, and capstone projects that expose the students to the state-of-the-art design methodologies. In addition, internships, funded by industry, expose the students to work on applied projects mentored by the industrial supervisors and faculty fellows.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">122600</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">122600</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n163" target="n368">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Computer cluster to support computational biology and other nonlinear signal reconstruction and system design problems</data>
      <data key="e_abstract">0130538 &lt;br/&gt;Peter Doerschuk&lt;br/&gt;Purdue University&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Computer Cluster to Support Computational Biology and Other Nonlinear Signal Reconstruction and System Design Problems&lt;br/&gt;&lt;br/&gt;The grant, an equipment grant, will purchase two computers, a compute server and a 3-D visualization workstation, in support of the investigators&apos; NSF CISE funded work in computational mathematics and its application to the structural biology of viruses and other nonlinear signal reconstruction and system design problems. The compute server (Microway, Inc.) is a cluster computer containing 64 Intel Pentium III microprocessors and 16 GB of memory organized into 32 nodes connected by 100 Mb Ethernet. The visualization workstation (Sun Microsystems) is a workstation with one 900 MHz UltraSPARC-III microprocessor, 1 GB of memory, and the intermediate level of Sun&apos;s three levels of 3-D graphics hardware accelerators.&lt;br/&gt;&lt;br/&gt;This equipment will support 3 projects which concern computational mathematics including multi-dimensional quadratures, transforms, and numerical optimization. The unifying theme is numerical optimization, especially global optimization. The projects are divided according to application and include 3-D reconstruction problems in structural biology and systems design problems for nonlinear communication channels. The main application is structural biology, in particular, the structural biology of viruses. The problems of particular interest are 3-D signal reconstruction problems for computing the dynamical behavior of viruses from a variety of biophysical measurements.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130538</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">130538</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n33" target="n34">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Instrumentation for Intelligent Agent and Wireless Computing Research</data>
      <data key="e_abstract">EIA-0115885&lt;br/&gt;Diane J. Cook&lt;br/&gt;University of Texas at Arlington&lt;br/&gt;&lt;br/&gt;MRI: Instrumentation for Intelligent Agent and Wireless Computing Research &lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training on intelligent agents in a mobile environment. The Wireless Intelligent Simulator Environment being established will integrate software agents, human agents, and robot agents, so that physically distributed interacting agents can perform a variety of tasks cooperatively or competitively.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">115885</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">115885</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n33" target="n35">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Instrumentation for Intelligent Agent and Wireless Computing Research</data>
      <data key="e_abstract">EIA-0115885&lt;br/&gt;Diane J. Cook&lt;br/&gt;University of Texas at Arlington&lt;br/&gt;&lt;br/&gt;MRI: Instrumentation for Intelligent Agent and Wireless Computing Research &lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training on intelligent agents in a mobile environment. The Wireless Intelligent Simulator Environment being established will integrate software agents, human agents, and robot agents, so that physically distributed interacting agents can perform a variety of tasks cooperatively or competitively.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">115885</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">115885</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n33" target="n37">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Instrumentation for Intelligent Agent and Wireless Computing Research</data>
      <data key="e_abstract">EIA-0115885&lt;br/&gt;Diane J. Cook&lt;br/&gt;University of Texas at Arlington&lt;br/&gt;&lt;br/&gt;MRI: Instrumentation for Intelligent Agent and Wireless Computing Research &lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training on intelligent agents in a mobile environment. The Wireless Intelligent Simulator Environment being established will integrate software agents, human agents, and robot agents, so that physically distributed interacting agents can perform a variety of tasks cooperatively or competitively.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">115885</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">115885</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n33" target="n1324">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Instrumentation for Intelligent Agent and Wireless Computing Research</data>
      <data key="e_abstract">EIA-0115885&lt;br/&gt;Diane J. Cook&lt;br/&gt;University of Texas at Arlington&lt;br/&gt;&lt;br/&gt;MRI: Instrumentation for Intelligent Agent and Wireless Computing Research &lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training on intelligent agents in a mobile environment. The Wireless Intelligent Simulator Environment being established will integrate software agents, human agents, and robot agents, so that physically distributed interacting agents can perform a variety of tasks cooperatively or competitively.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">115885</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">115885</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n34" target="n35">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Instrumentation for Intelligent Agent and Wireless Computing Research</data>
      <data key="e_abstract">EIA-0115885&lt;br/&gt;Diane J. Cook&lt;br/&gt;University of Texas at Arlington&lt;br/&gt;&lt;br/&gt;MRI: Instrumentation for Intelligent Agent and Wireless Computing Research &lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training on intelligent agents in a mobile environment. The Wireless Intelligent Simulator Environment being established will integrate software agents, human agents, and robot agents, so that physically distributed interacting agents can perform a variety of tasks cooperatively or competitively.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">115885</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">115885</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n34" target="n37">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Instrumentation for Intelligent Agent and Wireless Computing Research</data>
      <data key="e_abstract">EIA-0115885&lt;br/&gt;Diane J. Cook&lt;br/&gt;University of Texas at Arlington&lt;br/&gt;&lt;br/&gt;MRI: Instrumentation for Intelligent Agent and Wireless Computing Research &lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training on intelligent agents in a mobile environment. The Wireless Intelligent Simulator Environment being established will integrate software agents, human agents, and robot agents, so that physically distributed interacting agents can perform a variety of tasks cooperatively or competitively.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">115885</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">115885</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n34" target="n1324">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Instrumentation for Intelligent Agent and Wireless Computing Research</data>
      <data key="e_abstract">EIA-0115885&lt;br/&gt;Diane J. Cook&lt;br/&gt;University of Texas at Arlington&lt;br/&gt;&lt;br/&gt;MRI: Instrumentation for Intelligent Agent and Wireless Computing Research &lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training on intelligent agents in a mobile environment. The Wireless Intelligent Simulator Environment being established will integrate software agents, human agents, and robot agents, so that physically distributed interacting agents can perform a variety of tasks cooperatively or competitively.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">115885</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">115885</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n35" target="n37">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Instrumentation for Intelligent Agent and Wireless Computing Research</data>
      <data key="e_abstract">EIA-0115885&lt;br/&gt;Diane J. Cook&lt;br/&gt;University of Texas at Arlington&lt;br/&gt;&lt;br/&gt;MRI: Instrumentation for Intelligent Agent and Wireless Computing Research &lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training on intelligent agents in a mobile environment. The Wireless Intelligent Simulator Environment being established will integrate software agents, human agents, and robot agents, so that physically distributed interacting agents can perform a variety of tasks cooperatively or competitively.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">115885</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">115885</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n35" target="n1324">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Instrumentation for Intelligent Agent and Wireless Computing Research</data>
      <data key="e_abstract">EIA-0115885&lt;br/&gt;Diane J. Cook&lt;br/&gt;University of Texas at Arlington&lt;br/&gt;&lt;br/&gt;MRI: Instrumentation for Intelligent Agent and Wireless Computing Research &lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training on intelligent agents in a mobile environment. The Wireless Intelligent Simulator Environment being established will integrate software agents, human agents, and robot agents, so that physically distributed interacting agents can perform a variety of tasks cooperatively or competitively.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">115885</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">115885</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n37" target="n1324">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Instrumentation for Intelligent Agent and Wireless Computing Research</data>
      <data key="e_abstract">EIA-0115885&lt;br/&gt;Diane J. Cook&lt;br/&gt;University of Texas at Arlington&lt;br/&gt;&lt;br/&gt;MRI: Instrumentation for Intelligent Agent and Wireless Computing Research &lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training on intelligent agents in a mobile environment. The Wireless Intelligent Simulator Environment being established will integrate software agents, human agents, and robot agents, so that physically distributed interacting agents can perform a variety of tasks cooperatively or competitively.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">115885</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">115885</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1326" target="n1327">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">VISUALIZATION: High Fidelity Virtual Touch: Algorithms, Applications and Evaluation</data>
      <data key="e_abstract">Force feedback devices, or haptic interfaces, have the potential to increase the qualityof human-computer interaction by adding the sense of touch. However, there are still few practical force feedback applications, due in large part to the stringent computational requirements of haptic rendering. In order to maintain a high fidelity system, haptic update rates must be as high as 1000 Hz, rather than the 30 Hz updates for graphical displays. This is especially challenging for 6-degree of freedom (DOF) haptic devices which are used to display forces and torques for arbitrary pairs of objects. This requires accurate contact determination and contact force and torque computation of all collision points in less than a millisecond.&lt;br/&gt;&lt;br/&gt;This project focuses on three aspects of high fidelity haptic display or &apos;&apos;virtual touch&apos;&apos;. The first goal includes developing new geometric and physically-based algorithms that can improve the state of the art by more than an order of magnitude, in addition to the expected improvements in processor speed and computing power over that time. This will be based on hybrid spatial data structures, simplification hierarchies, multi-resolution representations, bounded error approximations, and massively parallel rasterization hardware. The second goal is to pursue applications that can benefit significantly from the use of high-fidelity 6-DOF haptic displays. This includes virtual prototyping of nano-structures, haptic visualization of biological interaction between molecules, maintenance analysis and interactive modeling and painting. The third goal is the evaluation of 6-DOF haptic rendering systems as a tool for human-computer interface. This will be done in collaboration with Boeing, Sandia Labs, and Sensable Technologies. If successful, the proposed research will provide enabling algorithms and a prototype software system for designing a high-fidelity virtual touch system.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">118743</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">118743</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1330" target="n1331">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Memory-Centric Resource Management for Data-Intensive Workloads on Clusters</data>
      <data key="e_abstract">Clusters of workstations have become standard system platforms for many scientific, commercial, and educational applications. This research focuses on effective usage of global memory resources to deal with dynamic job demands in large cluster systems. The targeted workloads are data-intensive scientific applications, Internet web accesses, and data processing for commercial databases. The first objective is to develop analytical/experimental performance models/tools to quantitatively examine the impact of the technology changes and data-intensive workloads to resource management policies, and to provide resource management guidance. The second objective is to design several memory-centric load sharing schemes by comprehensively considering dynamic job interactions and global cluster system resources. Finally, these schemes will be implemented and tested in a large cluster system. The impact and contributions of the proposed projects will be: (1) providing insights into memory systems performance and understanding potentials of memory-centric load sharing in clusters; (2) providing effective system solutions to adapt rapid changes of technology and workloads in cluster computing; and (3) making low-cost clusters more accessible for both scientists and business users to effectively run their large and demanding applications.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">98055</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">98055</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1334" target="n1335">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SI: A Flexible Framework for Secure Information Sharing Among Collaborating Organizations</data>
      <data key="e_abstract">Proposal No: 0113515&lt;br/&gt;ITR/SY: A Flexible Framework for Secure Information Sharing Among Collaborating Organizations&lt;br/&gt;PI: Jajodia Sushil&lt;br/&gt;&lt;br/&gt;This proposal seeks to develop a exible framework for secure information &lt;br/&gt;sharing among collaborating organizations. The approach will be to extend the Flexible &lt;br/&gt;Authorization Framework (FAF) designed by the PI and his colleagues. FAF is based on an &lt;br/&gt;authorization specication language that has rule based syntax and sound, non-monotonic logic based &lt;br/&gt;semantics. The rst goal of this proposal is to enhance FAF to include information ow &lt;br/&gt;controls, provisional authorizations, delegation of authority, and a rather broad sense of &lt;br/&gt;revocation capabilities of granted permissions. The second undertaking is to investigate ecient &lt;br/&gt;implementation of the resulting framework that is based on the best practices of the security &lt;br/&gt;community. The third goal of this proposal is to investigate the properties of basic constructs &lt;br/&gt;that are used to compose security policies.&lt;br/&gt;The main deliverables of this proposal are a reference architecture, &lt;br/&gt;mathematics and algorithms for security specications of information access and ow, their ecient &lt;br/&gt;implementation techniques, and an an algebra for policy compositions.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113515</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113515</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1336" target="n1337">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Integration of Stochastic and Dynamical Methods for Speech Technology</data>
      <data key="e_abstract">This project focuses on the creation of a stochastic representation for the phase-space embeddings of dynamical systems, for application to the task of speech classification and recognition. The research team will develop a general stochastic model for such signal embeddings, test the model through classification simulations, then apply the technique to both isolated and continuous speech recognition. The goal of the research is to discover time-domain analysis techniques using dynamical systems methods that will lead to improved analysis of speech signals and to improvements in speech recognition accuracy. This approach represents the integration of two traditionally distinct research fields: statistical signal processing and chaotic systems. Since signal processing is fundamentally based on linear systems theory and the study of chaos is inherently non-linear, these fields have little or no overlap outside of the fact that both attempt to model the behavior of physical systems. This research integrates these very different viewpoints by applying stochastic analysis and modeling tools from the signal processing field to the problem of analyzing embedded phase spaces obtained from chaotic systems analysis of time-series signals. The results will lead to a significant gain in our fundamental understanding of the characteristics and analysis of speech signals, with potential long-term application to other areas of speech processing such as speech coding and synthesis. The impact of developing these new technologies and applying them to the speech recognition task extends into both the machine learning and signal processing communities; specifically, the development of time-domain characterization methods is directly applicable to many problems of interest in the chaos and non-linear modeling domain, and will demonstrate an ability to concretely measure differences between the phase-space attractors of chaotic systems.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">113508</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">113508</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1340" target="n1341">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Fast Algorithms for Wave Scattering in Layered Media for Electronic Packaging and Geophysical Exploration</data>
      <data key="e_abstract">Proposal #0098140&lt;br/&gt;Duke University&lt;br/&gt;Liu, Qing Huo&lt;br/&gt;&lt;br/&gt;In this interdisciplinary project, we propose to develop fast algorithms for electromagnetic and elastic wave scattering in layered media. The impetus for such a joint effort is the ever increasing demand for efficient and accurate numerical simulation tools for electronic packaging and geophysical exploration where wave phenomenon plays an important role for design, evaluation, prediction and production. In both applications, there is a pressing need for fast solution techniques for full wave equations in layered media, namely, Maxwell&apos;s equations for electronic packaging and both electromagnetic and elastic wave equations for geophysical exploration. As the numerical issues involved in the solution of both wave equations share many common features, a concerted effort to develop fast algorithms for wave scattering in layered media will have a significant impact in both areas.&lt;br/&gt;&lt;br/&gt;In a high-speed electronic package, interconnects are one of the determining factors for the speed performance of the system. Such a high order effect is not easily captured in either equations or tables, rendering conventional timing driven layout techniques inaccurate and obsolete. One must fully characterize&lt;br/&gt;the interconnect structures to ensure on-chip signal integrity and to achieve the expected high-speed system performance. Therefore, there is a strong need for faster and more accurate full-wave electromagnetic analysis tools to extract parasitic parameters such as resistance, capacitance, and inductance.&lt;br/&gt;&lt;br/&gt;On the other hand, in geophysical exploration for oil and gas, electromagnetic and acoustic sensors are widely used to probe complex geologic structures. The goal of electromagnetic and acoustic subsurface sensing is to infer from these measurements the electromagnetic and mechanical properties of the formation,&lt;br/&gt;and to combine with other, such as nuclear, measurements to determine the petrophysical characteristics of the reservoir. The interpretation of these easurements, however, remains a challenging problem because of the complicated interaction of waves with the complex geologic structures and wellbore. The interpretation and processing of these measurements depend on fast and accurate forward and inverse solutions of lectromagnetic and acoustic waves in large-scale, highly heterogeneous media.&lt;br/&gt;&lt;br/&gt;The main emphasis of this proposal is on numerical algorithm development relevant to direct problems for electromagnetic and elastic waves propagation in layered media. A frequency domain integral equation formulation will be used. Major tasks include fast calculation of dyadic Green&apos;s functions for general&lt;br/&gt;layered media; fast matrix-vector multiplication and robust preconditioner for matrix solver; construction and study of high order basis functions for large targets; application of the obtained numerical algorithms in electronic packaging and geophysical exploration.&lt;br/&gt;&lt;br/&gt;Both PI&apos;s have extensive experience in the proposed application areas---parameter extraction for VLSI and RF component design (Cai) and geophysical subsurface sensing and electronic packaging (Liu). The&lt;br/&gt;collaborated research will greatly benefit the electronics and oil exploration industry, and our research and educational programs in electrical engineering and applied mathematics and scientific computation.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">98140</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98140</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1342" target="n1343">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Information Technology Workforce - ITWF: Multiple Pathways toward Gender Equity in the Information Technology Workforce</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;Information Technology Workforce (ITWF)&lt;br/&gt;&lt;br/&gt;Proposal Id: EIA-0119839&lt;br/&gt;PI: Paula G. Leventman, Thomas P. Cullinane and Ronald F. Perry&lt;br/&gt;Institution: Northeastern U.&lt;br/&gt;Title: Multiple Pathways towards Gender Equity in the Information Technology Workforce&lt;br/&gt;&lt;br/&gt;This ITWF award provides support for a study of graduates of an information systems graduate program (MSIS) at Northeastern University, as well as men and women currently working in Information Technology (IT) positions, to understand the factors that influence entry and persistence of women in IT positions. The MSIS program was designed for adult learners who wished to make a career transition to the IT field. The research team will use the longitudinal data collected in the study to develop and validate a model that will predict women&apos;s IT career pathways.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">119839</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">119839</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1342" target="n1344">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Information Technology Workforce - ITWF: Multiple Pathways toward Gender Equity in the Information Technology Workforce</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;Information Technology Workforce (ITWF)&lt;br/&gt;&lt;br/&gt;Proposal Id: EIA-0119839&lt;br/&gt;PI: Paula G. Leventman, Thomas P. Cullinane and Ronald F. Perry&lt;br/&gt;Institution: Northeastern U.&lt;br/&gt;Title: Multiple Pathways towards Gender Equity in the Information Technology Workforce&lt;br/&gt;&lt;br/&gt;This ITWF award provides support for a study of graduates of an information systems graduate program (MSIS) at Northeastern University, as well as men and women currently working in Information Technology (IT) positions, to understand the factors that influence entry and persistence of women in IT positions. The MSIS program was designed for adult learners who wished to make a career transition to the IT field. The research team will use the longitudinal data collected in the study to develop and validate a model that will predict women&apos;s IT career pathways.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">119839</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">119839</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1342" target="n1345">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Information Technology Workforce - ITWF: Multiple Pathways toward Gender Equity in the Information Technology Workforce</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;Information Technology Workforce (ITWF)&lt;br/&gt;&lt;br/&gt;Proposal Id: EIA-0119839&lt;br/&gt;PI: Paula G. Leventman, Thomas P. Cullinane and Ronald F. Perry&lt;br/&gt;Institution: Northeastern U.&lt;br/&gt;Title: Multiple Pathways towards Gender Equity in the Information Technology Workforce&lt;br/&gt;&lt;br/&gt;This ITWF award provides support for a study of graduates of an information systems graduate program (MSIS) at Northeastern University, as well as men and women currently working in Information Technology (IT) positions, to understand the factors that influence entry and persistence of women in IT positions. The MSIS program was designed for adult learners who wished to make a career transition to the IT field. The research team will use the longitudinal data collected in the study to develop and validate a model that will predict women&apos;s IT career pathways.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">119839</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">119839</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1343" target="n1344">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Information Technology Workforce - ITWF: Multiple Pathways toward Gender Equity in the Information Technology Workforce</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;Information Technology Workforce (ITWF)&lt;br/&gt;&lt;br/&gt;Proposal Id: EIA-0119839&lt;br/&gt;PI: Paula G. Leventman, Thomas P. Cullinane and Ronald F. Perry&lt;br/&gt;Institution: Northeastern U.&lt;br/&gt;Title: Multiple Pathways towards Gender Equity in the Information Technology Workforce&lt;br/&gt;&lt;br/&gt;This ITWF award provides support for a study of graduates of an information systems graduate program (MSIS) at Northeastern University, as well as men and women currently working in Information Technology (IT) positions, to understand the factors that influence entry and persistence of women in IT positions. The MSIS program was designed for adult learners who wished to make a career transition to the IT field. The research team will use the longitudinal data collected in the study to develop and validate a model that will predict women&apos;s IT career pathways.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">119839</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">119839</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1343" target="n1345">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Information Technology Workforce - ITWF: Multiple Pathways toward Gender Equity in the Information Technology Workforce</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;Information Technology Workforce (ITWF)&lt;br/&gt;&lt;br/&gt;Proposal Id: EIA-0119839&lt;br/&gt;PI: Paula G. Leventman, Thomas P. Cullinane and Ronald F. Perry&lt;br/&gt;Institution: Northeastern U.&lt;br/&gt;Title: Multiple Pathways towards Gender Equity in the Information Technology Workforce&lt;br/&gt;&lt;br/&gt;This ITWF award provides support for a study of graduates of an information systems graduate program (MSIS) at Northeastern University, as well as men and women currently working in Information Technology (IT) positions, to understand the factors that influence entry and persistence of women in IT positions. The MSIS program was designed for adult learners who wished to make a career transition to the IT field. The research team will use the longitudinal data collected in the study to develop and validate a model that will predict women&apos;s IT career pathways.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">119839</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">119839</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1344" target="n1345">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Information Technology Workforce - ITWF: Multiple Pathways toward Gender Equity in the Information Technology Workforce</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;Information Technology Workforce (ITWF)&lt;br/&gt;&lt;br/&gt;Proposal Id: EIA-0119839&lt;br/&gt;PI: Paula G. Leventman, Thomas P. Cullinane and Ronald F. Perry&lt;br/&gt;Institution: Northeastern U.&lt;br/&gt;Title: Multiple Pathways towards Gender Equity in the Information Technology Workforce&lt;br/&gt;&lt;br/&gt;This ITWF award provides support for a study of graduates of an information systems graduate program (MSIS) at Northeastern University, as well as men and women currently working in Information Technology (IT) positions, to understand the factors that influence entry and persistence of women in IT positions. The MSIS program was designed for adult learners who wished to make a career transition to the IT field. The research team will use the longitudinal data collected in the study to develop and validate a model that will predict women&apos;s IT career pathways.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">119839</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">119839</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1347" target="n1348">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE(CISE): Tutoring Explanation and Discovery Learning: Achieving Deep Understanding through Tutuorial Dialog</data>
      <data key="e_abstract">EIA-0113864&lt;br/&gt;Vincent Aleven&lt;br/&gt;Carnegie Mellon University&lt;br/&gt;&lt;br/&gt;ITR/PE(CISE): Tutoring Explanation and Discovery Learning: Achieving Deep Understanding through Tutorial Dialog&lt;br/&gt;&lt;br/&gt;This multidisciplinary research project will develop new instructional software, which will be evaluated in actual classrooms. The project will yield research advances in computer science, education, and cognitive psychology. With respect to education, the project will develop and test new methods of instruction, namely, &quot;tutoring at the explanation level&quot;. These methods aim to achieve deeper student understanding, resulting in better memory (retention) of the acquired knowledge, as well as the ability to apply what was learned to unfamiliar problems (transfer). With respect to computer science, this project will advance the state-of-the-art in intelligent tutoring systems through the incorporation of natural language processing techniques for assessing student explanations and improving them through dialog. With respect to cognitive psychology, this project will create and test cognitive models of how student understanding emerges from an integration of explicit-verbal and implicit-perceptual learning processes.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">113864</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113864</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1349" target="n1350">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY (CCR): Implementing Modular Program Analysis via Intersection and Union Types</data>
      <data key="e_abstract">Proposal Number: ITR Proposal 0113193&lt;br/&gt;&lt;br/&gt;Title: Implementing Modular Program Analysis via Intersection&lt;br/&gt;and Union Types.&lt;br/&gt;&lt;br/&gt;PI: Assaf J. Kfoury&lt;br/&gt;&lt;br/&gt;The proposed research will investigate real-world relevance of a &lt;br/&gt;new framework for modular program-analysis, which uses &quot;intersection&quot; &lt;br/&gt;and &quot;union&quot; types. The starting point of this investigation is a &lt;br/&gt;recently designed polymorphic type system, called System I, for a &lt;br/&gt;foundational functional language, the lambda-calculus. The chief &lt;br/&gt;feature of System I is the use of &quot;intersection&quot; types together &lt;br/&gt;with the new technology of &quot;expansion variables&quot;, which allow &lt;br/&gt;System I to satisfy a substitution-based principal-typings property. &lt;br/&gt;Although fully modular, the resulting program analysis is now &lt;br/&gt;restricted to a foundational language (the lambda-calculus) missing &lt;br/&gt;many standard high-level programming features such as conditionals,&lt;br/&gt;recursive definitions, exceptions, assignments, input/output, etc.&lt;br/&gt;Considerable work is necessary in order to turn System I into a &lt;br/&gt;type system for a full-fledged programming language such as Scheme &lt;br/&gt;(now considered to be the initial target language of proposed research).&lt;br/&gt;&lt;br/&gt;The proposed research is largely engineering work, aimed at producing &lt;br/&gt;an efficient prototype implementation, based on appropriate extensions &lt;br/&gt;of System I. The implementation will be evaluated --- or re-designed &lt;br/&gt;in parts --- by the extent to which it produces demonstrably better &lt;br/&gt;results in handling large software systems (enforcing larger classes &lt;br/&gt;of safety properties, statically detecting and ruling out larger classes &lt;br/&gt;of run-time errors).</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113193</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113193</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1351" target="n1352">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">NIRT: Biologically Based Assemblies of Electronic Materials at the Nanoscale; Improving on Nature</data>
      <data key="e_abstract">Abstract&lt;br/&gt;CTS-0103473&lt;br/&gt;Angela Belcher, University of Texas Austin&lt;br/&gt;&lt;br/&gt;This proposal was received in response to Nanoscale Science and Engineering (NSE) solicitation, NSF-00119, in the category Nanoscale Interdisciplinary Research Teams (NIRT). &lt;br/&gt;&lt;br/&gt;Biological systems efficiently and accurately assemble nanoscale building blocks into complex&lt;br/&gt;and functionally sophisticated structures with high perfection, controlled size and compositional&lt;br/&gt;uniformity. The self-organizing processes found in these systems rely largely on non-covalent interactions that enable elegant rearrangement between usable architectural forms and self-correction. The research will take advantage of the atomic composition and plane specific recognition that a biomolecule can exhibit for an inorganic phase, and the nanostructural control and regularity that biomolecules typically impose on crystal phases and crystallographic orientations to control nanostructure formation. Furthermore, RNA templates will be used to direct the parallel self-assembly of multiple electronic components with high precision. Using combinatorial peptide evolution, peptide sequences will be identified that select for and bind to specific nanocrystal and nanowire substrates, such as magnetic and semiconductor quantum dots and silicon nanowires synthesized in solution. The peptides provide recognition specificity between the biological molecules and the inorganic substrate. The peptides couple the inorganic electronic &quot;building blocks&quot; to the biological machinery that directs the architectural &quot;blueprints&quot; for organization. In essence, genetically encoding biological-electronic interactions are selecting the mRNA sequences that code for specific amino acid sequences, but beyond that, specific secondary and ultimately tertiary structures can be achieved; thus, leading to supermolecular architectures. &lt;br/&gt;&lt;br/&gt;An interdisciplinary effort will include synthetic chemistry, electrical and materials engineering, and molecular biology, which targets the development of specific recognition chemistries between biological and inorganic substrates for the creation of nanostructured materials and devices with novel applications. The proposed project offers highly interdisciplinary educational and research opportunities for graduate students.</data>
      <data key="e_pgm">1762</data>
      <data key="e_label">103473</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0307</data>
      <data key="e_awardID">103473</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1351" target="n1353">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">NIRT: Biologically Based Assemblies of Electronic Materials at the Nanoscale; Improving on Nature</data>
      <data key="e_abstract">Abstract&lt;br/&gt;CTS-0103473&lt;br/&gt;Angela Belcher, University of Texas Austin&lt;br/&gt;&lt;br/&gt;This proposal was received in response to Nanoscale Science and Engineering (NSE) solicitation, NSF-00119, in the category Nanoscale Interdisciplinary Research Teams (NIRT). &lt;br/&gt;&lt;br/&gt;Biological systems efficiently and accurately assemble nanoscale building blocks into complex&lt;br/&gt;and functionally sophisticated structures with high perfection, controlled size and compositional&lt;br/&gt;uniformity. The self-organizing processes found in these systems rely largely on non-covalent interactions that enable elegant rearrangement between usable architectural forms and self-correction. The research will take advantage of the atomic composition and plane specific recognition that a biomolecule can exhibit for an inorganic phase, and the nanostructural control and regularity that biomolecules typically impose on crystal phases and crystallographic orientations to control nanostructure formation. Furthermore, RNA templates will be used to direct the parallel self-assembly of multiple electronic components with high precision. Using combinatorial peptide evolution, peptide sequences will be identified that select for and bind to specific nanocrystal and nanowire substrates, such as magnetic and semiconductor quantum dots and silicon nanowires synthesized in solution. The peptides provide recognition specificity between the biological molecules and the inorganic substrate. The peptides couple the inorganic electronic &quot;building blocks&quot; to the biological machinery that directs the architectural &quot;blueprints&quot; for organization. In essence, genetically encoding biological-electronic interactions are selecting the mRNA sequences that code for specific amino acid sequences, but beyond that, specific secondary and ultimately tertiary structures can be achieved; thus, leading to supermolecular architectures. &lt;br/&gt;&lt;br/&gt;An interdisciplinary effort will include synthetic chemistry, electrical and materials engineering, and molecular biology, which targets the development of specific recognition chemistries between biological and inorganic substrates for the creation of nanostructured materials and devices with novel applications. The proposed project offers highly interdisciplinary educational and research opportunities for graduate students.</data>
      <data key="e_pgm">1762</data>
      <data key="e_label">103473</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0307</data>
      <data key="e_awardID">103473</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1352" target="n1353">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">NIRT: Biologically Based Assemblies of Electronic Materials at the Nanoscale; Improving on Nature</data>
      <data key="e_abstract">Abstract&lt;br/&gt;CTS-0103473&lt;br/&gt;Angela Belcher, University of Texas Austin&lt;br/&gt;&lt;br/&gt;This proposal was received in response to Nanoscale Science and Engineering (NSE) solicitation, NSF-00119, in the category Nanoscale Interdisciplinary Research Teams (NIRT). &lt;br/&gt;&lt;br/&gt;Biological systems efficiently and accurately assemble nanoscale building blocks into complex&lt;br/&gt;and functionally sophisticated structures with high perfection, controlled size and compositional&lt;br/&gt;uniformity. The self-organizing processes found in these systems rely largely on non-covalent interactions that enable elegant rearrangement between usable architectural forms and self-correction. The research will take advantage of the atomic composition and plane specific recognition that a biomolecule can exhibit for an inorganic phase, and the nanostructural control and regularity that biomolecules typically impose on crystal phases and crystallographic orientations to control nanostructure formation. Furthermore, RNA templates will be used to direct the parallel self-assembly of multiple electronic components with high precision. Using combinatorial peptide evolution, peptide sequences will be identified that select for and bind to specific nanocrystal and nanowire substrates, such as magnetic and semiconductor quantum dots and silicon nanowires synthesized in solution. The peptides provide recognition specificity between the biological molecules and the inorganic substrate. The peptides couple the inorganic electronic &quot;building blocks&quot; to the biological machinery that directs the architectural &quot;blueprints&quot; for organization. In essence, genetically encoding biological-electronic interactions are selecting the mRNA sequences that code for specific amino acid sequences, but beyond that, specific secondary and ultimately tertiary structures can be achieved; thus, leading to supermolecular architectures. &lt;br/&gt;&lt;br/&gt;An interdisciplinary effort will include synthetic chemistry, electrical and materials engineering, and molecular biology, which targets the development of specific recognition chemistries between biological and inorganic substrates for the creation of nanostructured materials and devices with novel applications. The proposed project offers highly interdisciplinary educational and research opportunities for graduate students.</data>
      <data key="e_pgm">1762</data>
      <data key="e_label">103473</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0307</data>
      <data key="e_awardID">103473</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1354" target="n1355">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Parallel Logic and Constraint Programming, with Applications to Planning and Web Accessibility</data>
      <data key="e_abstract">EIA-0130887 &lt;br/&gt;Enrico Pontelli&lt;br/&gt;New Mexico State University&lt;br/&gt;&lt;br/&gt;The Laboratory for Logic, Databases, and Advanced Programming (LLDAP) plans to acquire modern computing equipment to create an infrastructure for parallel and distributed computing and for software development for autonomous agents. The equipment requested consists of four Pentium III shared memory machines (with four CPUs each) to provide shared memory programming support. The four machines will be connected using fast gigabit Ethernet, creating the testbed for experimenting with distributed memory programming. The Project will also involve the acquisition of a set&lt;br/&gt;of five autonomous robots. The equipment will be used to support research in three inter-related projects: (1) The goal of the first project is to pursue the development of technology to support exploitation of different forms of parallelism from traditional logic programming and from constraint programming languages, (2) The second project develops technology for the creation of planners capable of dealing with incomplete knowledge, dynamic domains, and sensing actions, (3) The goal of the third project is to develop technology to promote accessibility of Web documents in the context of Web-based course-ware engineering.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130887</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130887</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1354" target="n1356">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Parallel Logic and Constraint Programming, with Applications to Planning and Web Accessibility</data>
      <data key="e_abstract">EIA-0130887 &lt;br/&gt;Enrico Pontelli&lt;br/&gt;New Mexico State University&lt;br/&gt;&lt;br/&gt;The Laboratory for Logic, Databases, and Advanced Programming (LLDAP) plans to acquire modern computing equipment to create an infrastructure for parallel and distributed computing and for software development for autonomous agents. The equipment requested consists of four Pentium III shared memory machines (with four CPUs each) to provide shared memory programming support. The four machines will be connected using fast gigabit Ethernet, creating the testbed for experimenting with distributed memory programming. The Project will also involve the acquisition of a set&lt;br/&gt;of five autonomous robots. The equipment will be used to support research in three inter-related projects: (1) The goal of the first project is to pursue the development of technology to support exploitation of different forms of parallelism from traditional logic programming and from constraint programming languages, (2) The second project develops technology for the creation of planners capable of dealing with incomplete knowledge, dynamic domains, and sensing actions, (3) The goal of the third project is to develop technology to promote accessibility of Web documents in the context of Web-based course-ware engineering.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130887</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130887</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1354" target="n1357">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Parallel Logic and Constraint Programming, with Applications to Planning and Web Accessibility</data>
      <data key="e_abstract">EIA-0130887 &lt;br/&gt;Enrico Pontelli&lt;br/&gt;New Mexico State University&lt;br/&gt;&lt;br/&gt;The Laboratory for Logic, Databases, and Advanced Programming (LLDAP) plans to acquire modern computing equipment to create an infrastructure for parallel and distributed computing and for software development for autonomous agents. The equipment requested consists of four Pentium III shared memory machines (with four CPUs each) to provide shared memory programming support. The four machines will be connected using fast gigabit Ethernet, creating the testbed for experimenting with distributed memory programming. The Project will also involve the acquisition of a set&lt;br/&gt;of five autonomous robots. The equipment will be used to support research in three inter-related projects: (1) The goal of the first project is to pursue the development of technology to support exploitation of different forms of parallelism from traditional logic programming and from constraint programming languages, (2) The second project develops technology for the creation of planners capable of dealing with incomplete knowledge, dynamic domains, and sensing actions, (3) The goal of the third project is to develop technology to promote accessibility of Web documents in the context of Web-based course-ware engineering.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130887</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130887</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1355" target="n1356">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Parallel Logic and Constraint Programming, with Applications to Planning and Web Accessibility</data>
      <data key="e_abstract">EIA-0130887 &lt;br/&gt;Enrico Pontelli&lt;br/&gt;New Mexico State University&lt;br/&gt;&lt;br/&gt;The Laboratory for Logic, Databases, and Advanced Programming (LLDAP) plans to acquire modern computing equipment to create an infrastructure for parallel and distributed computing and for software development for autonomous agents. The equipment requested consists of four Pentium III shared memory machines (with four CPUs each) to provide shared memory programming support. The four machines will be connected using fast gigabit Ethernet, creating the testbed for experimenting with distributed memory programming. The Project will also involve the acquisition of a set&lt;br/&gt;of five autonomous robots. The equipment will be used to support research in three inter-related projects: (1) The goal of the first project is to pursue the development of technology to support exploitation of different forms of parallelism from traditional logic programming and from constraint programming languages, (2) The second project develops technology for the creation of planners capable of dealing with incomplete knowledge, dynamic domains, and sensing actions, (3) The goal of the third project is to develop technology to promote accessibility of Web documents in the context of Web-based course-ware engineering.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130887</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130887</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1355" target="n1357">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Parallel Logic and Constraint Programming, with Applications to Planning and Web Accessibility</data>
      <data key="e_abstract">EIA-0130887 &lt;br/&gt;Enrico Pontelli&lt;br/&gt;New Mexico State University&lt;br/&gt;&lt;br/&gt;The Laboratory for Logic, Databases, and Advanced Programming (LLDAP) plans to acquire modern computing equipment to create an infrastructure for parallel and distributed computing and for software development for autonomous agents. The equipment requested consists of four Pentium III shared memory machines (with four CPUs each) to provide shared memory programming support. The four machines will be connected using fast gigabit Ethernet, creating the testbed for experimenting with distributed memory programming. The Project will also involve the acquisition of a set&lt;br/&gt;of five autonomous robots. The equipment will be used to support research in three inter-related projects: (1) The goal of the first project is to pursue the development of technology to support exploitation of different forms of parallelism from traditional logic programming and from constraint programming languages, (2) The second project develops technology for the creation of planners capable of dealing with incomplete knowledge, dynamic domains, and sensing actions, (3) The goal of the third project is to develop technology to promote accessibility of Web documents in the context of Web-based course-ware engineering.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130887</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130887</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1356" target="n1357">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: Parallel Logic and Constraint Programming, with Applications to Planning and Web Accessibility</data>
      <data key="e_abstract">EIA-0130887 &lt;br/&gt;Enrico Pontelli&lt;br/&gt;New Mexico State University&lt;br/&gt;&lt;br/&gt;The Laboratory for Logic, Databases, and Advanced Programming (LLDAP) plans to acquire modern computing equipment to create an infrastructure for parallel and distributed computing and for software development for autonomous agents. The equipment requested consists of four Pentium III shared memory machines (with four CPUs each) to provide shared memory programming support. The four machines will be connected using fast gigabit Ethernet, creating the testbed for experimenting with distributed memory programming. The Project will also involve the acquisition of a set&lt;br/&gt;of five autonomous robots. The equipment will be used to support research in three inter-related projects: (1) The goal of the first project is to pursue the development of technology to support exploitation of different forms of parallelism from traditional logic programming and from constraint programming languages, (2) The second project develops technology for the creation of planners capable of dealing with incomplete knowledge, dynamic domains, and sensing actions, (3) The goal of the third project is to develop technology to promote accessibility of Web documents in the context of Web-based course-ware engineering.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130887</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130887</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1358" target="n1359">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Human Computer Interaction for Direct Brain-Computer Interfaces</data>
      <data key="e_abstract">Human-Computer Interaction for Direct Brain-Computer Interfaces&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This is a the first year funding of a 3-year continuing award. Recent developments in medical technology have brought closer the possibility of direct control of a computer by the human brain, e.g. using recordings from electrodes placed on the scalp or a neurotrophic electrode that is implanted in the brain. Although advances in these biometric interface device technologies hold much promise, there are many aspects yet to be researched. For the half-million people with locked-in syndrome (completely paralyzed and unable to speak), and for many others with severe and progressive disabilities such as quadriplegia or ALS, the potential impact of BCI technology are staggering; preliminary results achieved by the PI encourage vigorous pursuit of refinements and improvements in BCI technology. In this project the PI&apos;s objectives will be: to establish a theoretical framework for the field of Brain-Computer Interface (BCI) research; to investigate the effectiveness of various user interaction styles for existing brain-computer interface device technologies; and to validate various interaction styles in two quality-of-life application domains, a basic communications application and an environmental control application.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">118917</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">118917</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n173" target="n1358">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Human Computer Interaction for Direct Brain-Computer Interfaces</data>
      <data key="e_abstract">Human-Computer Interaction for Direct Brain-Computer Interfaces&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This is a the first year funding of a 3-year continuing award. Recent developments in medical technology have brought closer the possibility of direct control of a computer by the human brain, e.g. using recordings from electrodes placed on the scalp or a neurotrophic electrode that is implanted in the brain. Although advances in these biometric interface device technologies hold much promise, there are many aspects yet to be researched. For the half-million people with locked-in syndrome (completely paralyzed and unable to speak), and for many others with severe and progressive disabilities such as quadriplegia or ALS, the potential impact of BCI technology are staggering; preliminary results achieved by the PI encourage vigorous pursuit of refinements and improvements in BCI technology. In this project the PI&apos;s objectives will be: to establish a theoretical framework for the field of Brain-Computer Interface (BCI) research; to investigate the effectiveness of various user interaction styles for existing brain-computer interface device technologies; and to validate various interaction styles in two quality-of-life application domains, a basic communications application and an environmental control application.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">118917</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">118917</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1358" target="n1361">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Human Computer Interaction for Direct Brain-Computer Interfaces</data>
      <data key="e_abstract">Human-Computer Interaction for Direct Brain-Computer Interfaces&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This is a the first year funding of a 3-year continuing award. Recent developments in medical technology have brought closer the possibility of direct control of a computer by the human brain, e.g. using recordings from electrodes placed on the scalp or a neurotrophic electrode that is implanted in the brain. Although advances in these biometric interface device technologies hold much promise, there are many aspects yet to be researched. For the half-million people with locked-in syndrome (completely paralyzed and unable to speak), and for many others with severe and progressive disabilities such as quadriplegia or ALS, the potential impact of BCI technology are staggering; preliminary results achieved by the PI encourage vigorous pursuit of refinements and improvements in BCI technology. In this project the PI&apos;s objectives will be: to establish a theoretical framework for the field of Brain-Computer Interface (BCI) research; to investigate the effectiveness of various user interaction styles for existing brain-computer interface device technologies; and to validate various interaction styles in two quality-of-life application domains, a basic communications application and an environmental control application.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">118917</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">118917</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n173" target="n1359">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Human Computer Interaction for Direct Brain-Computer Interfaces</data>
      <data key="e_abstract">Human-Computer Interaction for Direct Brain-Computer Interfaces&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This is a the first year funding of a 3-year continuing award. Recent developments in medical technology have brought closer the possibility of direct control of a computer by the human brain, e.g. using recordings from electrodes placed on the scalp or a neurotrophic electrode that is implanted in the brain. Although advances in these biometric interface device technologies hold much promise, there are many aspects yet to be researched. For the half-million people with locked-in syndrome (completely paralyzed and unable to speak), and for many others with severe and progressive disabilities such as quadriplegia or ALS, the potential impact of BCI technology are staggering; preliminary results achieved by the PI encourage vigorous pursuit of refinements and improvements in BCI technology. In this project the PI&apos;s objectives will be: to establish a theoretical framework for the field of Brain-Computer Interface (BCI) research; to investigate the effectiveness of various user interaction styles for existing brain-computer interface device technologies; and to validate various interaction styles in two quality-of-life application domains, a basic communications application and an environmental control application.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">118917</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">118917</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1359" target="n1361">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Human Computer Interaction for Direct Brain-Computer Interfaces</data>
      <data key="e_abstract">Human-Computer Interaction for Direct Brain-Computer Interfaces&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This is a the first year funding of a 3-year continuing award. Recent developments in medical technology have brought closer the possibility of direct control of a computer by the human brain, e.g. using recordings from electrodes placed on the scalp or a neurotrophic electrode that is implanted in the brain. Although advances in these biometric interface device technologies hold much promise, there are many aspects yet to be researched. For the half-million people with locked-in syndrome (completely paralyzed and unable to speak), and for many others with severe and progressive disabilities such as quadriplegia or ALS, the potential impact of BCI technology are staggering; preliminary results achieved by the PI encourage vigorous pursuit of refinements and improvements in BCI technology. In this project the PI&apos;s objectives will be: to establish a theoretical framework for the field of Brain-Computer Interface (BCI) research; to investigate the effectiveness of various user interaction styles for existing brain-computer interface device technologies; and to validate various interaction styles in two quality-of-life application domains, a basic communications application and an environmental control application.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">118917</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">118917</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n173" target="n1361">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Human Computer Interaction for Direct Brain-Computer Interfaces</data>
      <data key="e_abstract">Human-Computer Interaction for Direct Brain-Computer Interfaces&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This is a the first year funding of a 3-year continuing award. Recent developments in medical technology have brought closer the possibility of direct control of a computer by the human brain, e.g. using recordings from electrodes placed on the scalp or a neurotrophic electrode that is implanted in the brain. Although advances in these biometric interface device technologies hold much promise, there are many aspects yet to be researched. For the half-million people with locked-in syndrome (completely paralyzed and unable to speak), and for many others with severe and progressive disabilities such as quadriplegia or ALS, the potential impact of BCI technology are staggering; preliminary results achieved by the PI encourage vigorous pursuit of refinements and improvements in BCI technology. In this project the PI&apos;s objectives will be: to establish a theoretical framework for the field of Brain-Computer Interface (BCI) research; to investigate the effectiveness of various user interaction styles for existing brain-computer interface device technologies; and to validate various interaction styles in two quality-of-life application domains, a basic communications application and an environmental control application.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">118917</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">118917</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1365" target="n1366">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Acquisition of Computing and Peripheral Hardware to Support Collaborative Research and Undergraduate Research Education in Virtual Reality</data>
      <data key="e_abstract">EIA-0116295&lt;br/&gt;Sharon Stansfield&lt;br/&gt;Ithaca College&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of Computing and Peripheral Hardware to Support Collaborative Research and Undergraduate Research Education in Virtual Reality&lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training in virtual reality. The virtual reality effort will provide a platform for the investigation of human motion planning and the development of computational models to enable high fidelity object manipulation within a virtual environment.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116295</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116295</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1365" target="n1367">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Acquisition of Computing and Peripheral Hardware to Support Collaborative Research and Undergraduate Research Education in Virtual Reality</data>
      <data key="e_abstract">EIA-0116295&lt;br/&gt;Sharon Stansfield&lt;br/&gt;Ithaca College&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of Computing and Peripheral Hardware to Support Collaborative Research and Undergraduate Research Education in Virtual Reality&lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training in virtual reality. The virtual reality effort will provide a platform for the investigation of human motion planning and the development of computational models to enable high fidelity object manipulation within a virtual environment.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116295</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116295</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1366" target="n1367">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">MRI: Acquisition of Computing and Peripheral Hardware to Support Collaborative Research and Undergraduate Research Education in Virtual Reality</data>
      <data key="e_abstract">EIA-0116295&lt;br/&gt;Sharon Stansfield&lt;br/&gt;Ithaca College&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of Computing and Peripheral Hardware to Support Collaborative Research and Undergraduate Research Education in Virtual Reality&lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training in virtual reality. The virtual reality effort will provide a platform for the investigation of human motion planning and the development of computational models to enable high fidelity object manipulation within a virtual environment.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116295</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116295</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n729" target="n1368">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Design and Control of Next Generation Networks:A Measurement-Analytic Approach</data>
      <data key="e_abstract">Next generation high-speed networks are expected to support exciting new applications involv-&lt;br/&gt;ing multi-media traffic, such as video-conferencing, tele-medicine, video-on-demand, and web&lt;br/&gt;TV, in addition to countless other yet-to-be-conceived network applications. The viability and&lt;br/&gt;the continuing success of these networks will depend crucially upon their ability to offer high&lt;br/&gt;performance with regard to latency, delay variation, and bandwidths they can provide to these&lt;br/&gt;myriad of applications. Customers will demand high-quality multimedia services, and not be&lt;br/&gt;satisfied with the kind of high-latency, variable delay that is characteristic of the current Inter-&lt;br/&gt;net. The various performance requirements from the users and the sheer size of these networks&lt;br/&gt;make it imperative that we properly understand at a fundamental level how to design, engi-&lt;br/&gt;neer, and control these networks, and develop appropriate methodologies. It is also essential&lt;br/&gt;that these tools be evaluated in terms of complexity and accuracy using experimentation (i.e.,&lt;br/&gt;testbed) and simulation.&lt;br/&gt; To address the challenges described above, we propose to develop an innovative approach&lt;br/&gt;that harnesses the power of combined measurement and analysis to create design and control&lt;br/&gt;tools for next generation networks. We plan to elaborate on the approach in the context of:&lt;br/&gt;performance evaluation (i.e., QoS estimation), and QoS-sensitivity estimation.&lt;br/&gt; These issues form the foundation required to solve key network design and control problems.&lt;br/&gt;Building upon our approach|combining analysis with measurements|we will then focus on&lt;br/&gt;on a variety of problems that have to be addressed in order for networks to support QoS:&lt;br/&gt; admission control, congestion control,&lt;br/&gt; QoS-based routing, and&lt;br/&gt; network design and dimensioning.&lt;br/&gt;Our proposed solutions to the above problems will be supported and refined by extensive&lt;br/&gt;empirical studies on our experimental platform. This platform is capable of supporting a&lt;br/&gt;variety of network technologies and traffic characteristics.&lt;br/&gt; Our research team has been at the forefront of the development of important results in traffic&lt;br/&gt;analysis, network design, and control, and is committed to creating the necessary synergy for&lt;br/&gt;addressing the key problems outlined above. Our team already has a significantly productive&lt;br/&gt;track record in previous collaborative efforts.&lt;br/&gt; We are planning to address a set of problems that are critically important for deploying&lt;br/&gt;next generation networks with the capability of offering high bandwidth and stringent QoS to&lt;br/&gt;users. Creating a future broadband network that is exible, efficient, robust, and controllable&lt;br/&gt;is essential to the viability of our economy and to the different communities within it. Hence, if&lt;br/&gt;our research is successful, it will have a significant impact on the delivery of services necessary&lt;br/&gt;to meet the diverse needs of education, business, and entertainment.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">99137</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">99137</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n999" target="n1368">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Design and Control of Next Generation Networks:A Measurement-Analytic Approach</data>
      <data key="e_abstract">Next generation high-speed networks are expected to support exciting new applications involv-&lt;br/&gt;ing multi-media traffic, such as video-conferencing, tele-medicine, video-on-demand, and web&lt;br/&gt;TV, in addition to countless other yet-to-be-conceived network applications. The viability and&lt;br/&gt;the continuing success of these networks will depend crucially upon their ability to offer high&lt;br/&gt;performance with regard to latency, delay variation, and bandwidths they can provide to these&lt;br/&gt;myriad of applications. Customers will demand high-quality multimedia services, and not be&lt;br/&gt;satisfied with the kind of high-latency, variable delay that is characteristic of the current Inter-&lt;br/&gt;net. The various performance requirements from the users and the sheer size of these networks&lt;br/&gt;make it imperative that we properly understand at a fundamental level how to design, engi-&lt;br/&gt;neer, and control these networks, and develop appropriate methodologies. It is also essential&lt;br/&gt;that these tools be evaluated in terms of complexity and accuracy using experimentation (i.e.,&lt;br/&gt;testbed) and simulation.&lt;br/&gt; To address the challenges described above, we propose to develop an innovative approach&lt;br/&gt;that harnesses the power of combined measurement and analysis to create design and control&lt;br/&gt;tools for next generation networks. We plan to elaborate on the approach in the context of:&lt;br/&gt;performance evaluation (i.e., QoS estimation), and QoS-sensitivity estimation.&lt;br/&gt; These issues form the foundation required to solve key network design and control problems.&lt;br/&gt;Building upon our approach|combining analysis with measurements|we will then focus on&lt;br/&gt;on a variety of problems that have to be addressed in order for networks to support QoS:&lt;br/&gt; admission control, congestion control,&lt;br/&gt; QoS-based routing, and&lt;br/&gt; network design and dimensioning.&lt;br/&gt;Our proposed solutions to the above problems will be supported and refined by extensive&lt;br/&gt;empirical studies on our experimental platform. This platform is capable of supporting a&lt;br/&gt;variety of network technologies and traffic characteristics.&lt;br/&gt; Our research team has been at the forefront of the development of important results in traffic&lt;br/&gt;analysis, network design, and control, and is committed to creating the necessary synergy for&lt;br/&gt;addressing the key problems outlined above. Our team already has a significantly productive&lt;br/&gt;track record in previous collaborative efforts.&lt;br/&gt; We are planning to address a set of problems that are critically important for deploying&lt;br/&gt;next generation networks with the capability of offering high bandwidth and stringent QoS to&lt;br/&gt;users. Creating a future broadband network that is exible, efficient, robust, and controllable&lt;br/&gt;is essential to the viability of our economy and to the different communities within it. Hence, if&lt;br/&gt;our research is successful, it will have a significant impact on the delivery of services necessary&lt;br/&gt;to meet the diverse needs of education, business, and entertainment.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">99137</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">99137</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1368" target="n1371">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Design and Control of Next Generation Networks:A Measurement-Analytic Approach</data>
      <data key="e_abstract">Next generation high-speed networks are expected to support exciting new applications involv-&lt;br/&gt;ing multi-media traffic, such as video-conferencing, tele-medicine, video-on-demand, and web&lt;br/&gt;TV, in addition to countless other yet-to-be-conceived network applications. The viability and&lt;br/&gt;the continuing success of these networks will depend crucially upon their ability to offer high&lt;br/&gt;performance with regard to latency, delay variation, and bandwidths they can provide to these&lt;br/&gt;myriad of applications. Customers will demand high-quality multimedia services, and not be&lt;br/&gt;satisfied with the kind of high-latency, variable delay that is characteristic of the current Inter-&lt;br/&gt;net. The various performance requirements from the users and the sheer size of these networks&lt;br/&gt;make it imperative that we properly understand at a fundamental level how to design, engi-&lt;br/&gt;neer, and control these networks, and develop appropriate methodologies. It is also essential&lt;br/&gt;that these tools be evaluated in terms of complexity and accuracy using experimentation (i.e.,&lt;br/&gt;testbed) and simulation.&lt;br/&gt; To address the challenges described above, we propose to develop an innovative approach&lt;br/&gt;that harnesses the power of combined measurement and analysis to create design and control&lt;br/&gt;tools for next generation networks. We plan to elaborate on the approach in the context of:&lt;br/&gt;performance evaluation (i.e., QoS estimation), and QoS-sensitivity estimation.&lt;br/&gt; These issues form the foundation required to solve key network design and control problems.&lt;br/&gt;Building upon our approach|combining analysis with measurements|we will then focus on&lt;br/&gt;on a variety of problems that have to be addressed in order for networks to support QoS:&lt;br/&gt; admission control, congestion control,&lt;br/&gt; QoS-based routing, and&lt;br/&gt; network design and dimensioning.&lt;br/&gt;Our proposed solutions to the above problems will be supported and refined by extensive&lt;br/&gt;empirical studies on our experimental platform. This platform is capable of supporting a&lt;br/&gt;variety of network technologies and traffic characteristics.&lt;br/&gt; Our research team has been at the forefront of the development of important results in traffic&lt;br/&gt;analysis, network design, and control, and is committed to creating the necessary synergy for&lt;br/&gt;addressing the key problems outlined above. Our team already has a significantly productive&lt;br/&gt;track record in previous collaborative efforts.&lt;br/&gt; We are planning to address a set of problems that are critically important for deploying&lt;br/&gt;next generation networks with the capability of offering high bandwidth and stringent QoS to&lt;br/&gt;users. Creating a future broadband network that is exible, efficient, robust, and controllable&lt;br/&gt;is essential to the viability of our economy and to the different communities within it. Hence, if&lt;br/&gt;our research is successful, it will have a significant impact on the delivery of services necessary&lt;br/&gt;to meet the diverse needs of education, business, and entertainment.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">99137</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">99137</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n729" target="n999">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Design and Control of Next Generation Networks:A Measurement-Analytic Approach</data>
      <data key="e_abstract">Next generation high-speed networks are expected to support exciting new applications involv-&lt;br/&gt;ing multi-media traffic, such as video-conferencing, tele-medicine, video-on-demand, and web&lt;br/&gt;TV, in addition to countless other yet-to-be-conceived network applications. The viability and&lt;br/&gt;the continuing success of these networks will depend crucially upon their ability to offer high&lt;br/&gt;performance with regard to latency, delay variation, and bandwidths they can provide to these&lt;br/&gt;myriad of applications. Customers will demand high-quality multimedia services, and not be&lt;br/&gt;satisfied with the kind of high-latency, variable delay that is characteristic of the current Inter-&lt;br/&gt;net. The various performance requirements from the users and the sheer size of these networks&lt;br/&gt;make it imperative that we properly understand at a fundamental level how to design, engi-&lt;br/&gt;neer, and control these networks, and develop appropriate methodologies. It is also essential&lt;br/&gt;that these tools be evaluated in terms of complexity and accuracy using experimentation (i.e.,&lt;br/&gt;testbed) and simulation.&lt;br/&gt; To address the challenges described above, we propose to develop an innovative approach&lt;br/&gt;that harnesses the power of combined measurement and analysis to create design and control&lt;br/&gt;tools for next generation networks. We plan to elaborate on the approach in the context of:&lt;br/&gt;performance evaluation (i.e., QoS estimation), and QoS-sensitivity estimation.&lt;br/&gt; These issues form the foundation required to solve key network design and control problems.&lt;br/&gt;Building upon our approach|combining analysis with measurements|we will then focus on&lt;br/&gt;on a variety of problems that have to be addressed in order for networks to support QoS:&lt;br/&gt; admission control, congestion control,&lt;br/&gt; QoS-based routing, and&lt;br/&gt; network design and dimensioning.&lt;br/&gt;Our proposed solutions to the above problems will be supported and refined by extensive&lt;br/&gt;empirical studies on our experimental platform. This platform is capable of supporting a&lt;br/&gt;variety of network technologies and traffic characteristics.&lt;br/&gt; Our research team has been at the forefront of the development of important results in traffic&lt;br/&gt;analysis, network design, and control, and is committed to creating the necessary synergy for&lt;br/&gt;addressing the key problems outlined above. Our team already has a significantly productive&lt;br/&gt;track record in previous collaborative efforts.&lt;br/&gt; We are planning to address a set of problems that are critically important for deploying&lt;br/&gt;next generation networks with the capability of offering high bandwidth and stringent QoS to&lt;br/&gt;users. Creating a future broadband network that is exible, efficient, robust, and controllable&lt;br/&gt;is essential to the viability of our economy and to the different communities within it. Hence, if&lt;br/&gt;our research is successful, it will have a significant impact on the delivery of services necessary&lt;br/&gt;to meet the diverse needs of education, business, and entertainment.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">99137</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">99137</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n729" target="n1371">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Design and Control of Next Generation Networks:A Measurement-Analytic Approach</data>
      <data key="e_abstract">Next generation high-speed networks are expected to support exciting new applications involv-&lt;br/&gt;ing multi-media traffic, such as video-conferencing, tele-medicine, video-on-demand, and web&lt;br/&gt;TV, in addition to countless other yet-to-be-conceived network applications. The viability and&lt;br/&gt;the continuing success of these networks will depend crucially upon their ability to offer high&lt;br/&gt;performance with regard to latency, delay variation, and bandwidths they can provide to these&lt;br/&gt;myriad of applications. Customers will demand high-quality multimedia services, and not be&lt;br/&gt;satisfied with the kind of high-latency, variable delay that is characteristic of the current Inter-&lt;br/&gt;net. The various performance requirements from the users and the sheer size of these networks&lt;br/&gt;make it imperative that we properly understand at a fundamental level how to design, engi-&lt;br/&gt;neer, and control these networks, and develop appropriate methodologies. It is also essential&lt;br/&gt;that these tools be evaluated in terms of complexity and accuracy using experimentation (i.e.,&lt;br/&gt;testbed) and simulation.&lt;br/&gt; To address the challenges described above, we propose to develop an innovative approach&lt;br/&gt;that harnesses the power of combined measurement and analysis to create design and control&lt;br/&gt;tools for next generation networks. We plan to elaborate on the approach in the context of:&lt;br/&gt;performance evaluation (i.e., QoS estimation), and QoS-sensitivity estimation.&lt;br/&gt; These issues form the foundation required to solve key network design and control problems.&lt;br/&gt;Building upon our approach|combining analysis with measurements|we will then focus on&lt;br/&gt;on a variety of problems that have to be addressed in order for networks to support QoS:&lt;br/&gt; admission control, congestion control,&lt;br/&gt; QoS-based routing, and&lt;br/&gt; network design and dimensioning.&lt;br/&gt;Our proposed solutions to the above problems will be supported and refined by extensive&lt;br/&gt;empirical studies on our experimental platform. This platform is capable of supporting a&lt;br/&gt;variety of network technologies and traffic characteristics.&lt;br/&gt; Our research team has been at the forefront of the development of important results in traffic&lt;br/&gt;analysis, network design, and control, and is committed to creating the necessary synergy for&lt;br/&gt;addressing the key problems outlined above. Our team already has a significantly productive&lt;br/&gt;track record in previous collaborative efforts.&lt;br/&gt; We are planning to address a set of problems that are critically important for deploying&lt;br/&gt;next generation networks with the capability of offering high bandwidth and stringent QoS to&lt;br/&gt;users. Creating a future broadband network that is exible, efficient, robust, and controllable&lt;br/&gt;is essential to the viability of our economy and to the different communities within it. Hence, if&lt;br/&gt;our research is successful, it will have a significant impact on the delivery of services necessary&lt;br/&gt;to meet the diverse needs of education, business, and entertainment.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">99137</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">99137</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n999" target="n1371">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Design and Control of Next Generation Networks:A Measurement-Analytic Approach</data>
      <data key="e_abstract">Next generation high-speed networks are expected to support exciting new applications involv-&lt;br/&gt;ing multi-media traffic, such as video-conferencing, tele-medicine, video-on-demand, and web&lt;br/&gt;TV, in addition to countless other yet-to-be-conceived network applications. The viability and&lt;br/&gt;the continuing success of these networks will depend crucially upon their ability to offer high&lt;br/&gt;performance with regard to latency, delay variation, and bandwidths they can provide to these&lt;br/&gt;myriad of applications. Customers will demand high-quality multimedia services, and not be&lt;br/&gt;satisfied with the kind of high-latency, variable delay that is characteristic of the current Inter-&lt;br/&gt;net. The various performance requirements from the users and the sheer size of these networks&lt;br/&gt;make it imperative that we properly understand at a fundamental level how to design, engi-&lt;br/&gt;neer, and control these networks, and develop appropriate methodologies. It is also essential&lt;br/&gt;that these tools be evaluated in terms of complexity and accuracy using experimentation (i.e.,&lt;br/&gt;testbed) and simulation.&lt;br/&gt; To address the challenges described above, we propose to develop an innovative approach&lt;br/&gt;that harnesses the power of combined measurement and analysis to create design and control&lt;br/&gt;tools for next generation networks. We plan to elaborate on the approach in the context of:&lt;br/&gt;performance evaluation (i.e., QoS estimation), and QoS-sensitivity estimation.&lt;br/&gt; These issues form the foundation required to solve key network design and control problems.&lt;br/&gt;Building upon our approach|combining analysis with measurements|we will then focus on&lt;br/&gt;on a variety of problems that have to be addressed in order for networks to support QoS:&lt;br/&gt; admission control, congestion control,&lt;br/&gt; QoS-based routing, and&lt;br/&gt; network design and dimensioning.&lt;br/&gt;Our proposed solutions to the above problems will be supported and refined by extensive&lt;br/&gt;empirical studies on our experimental platform. This platform is capable of supporting a&lt;br/&gt;variety of network technologies and traffic characteristics.&lt;br/&gt; Our research team has been at the forefront of the development of important results in traffic&lt;br/&gt;analysis, network design, and control, and is committed to creating the necessary synergy for&lt;br/&gt;addressing the key problems outlined above. Our team already has a significantly productive&lt;br/&gt;track record in previous collaborative efforts.&lt;br/&gt; We are planning to address a set of problems that are critically important for deploying&lt;br/&gt;next generation networks with the capability of offering high bandwidth and stringent QoS to&lt;br/&gt;users. Creating a future broadband network that is exible, efficient, robust, and controllable&lt;br/&gt;is essential to the viability of our economy and to the different communities within it. Hence, if&lt;br/&gt;our research is successful, it will have a significant impact on the delivery of services necessary&lt;br/&gt;to meet the diverse needs of education, business, and entertainment.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">99137</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">99137</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1375" target="n1376">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY (CISE) Geometrical Image Representation</data>
      <data key="e_abstract">The proposed research is aimed at constructing an image representation that improves performances of current image compression algorithms and organizes the information in a form suitable for fast patter discrimination and search algorithms. The approach relies on recently discovered bandlelet orthogonal bases, which can be adapted to curves and efficiently represent image profiles. The work will also build on and incorporate previous work in such areas as graphs matching. Developing new techniques in this area can greatly improve the efficiency of future image search engines.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">114391</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">114391</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1292" target="n1293">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">IGERT: Computational Phylogenetics and Applications to Biology</data>
      <data key="e_abstract">Phylogenetics , the study of the relationships among genes, individuals, populations, and species, forms the basis for all of comparative biology. This IGERT grant will support a comprehensive, interdisciplinary graduate training program in Computational Phylogenetics and Applications to Biology. The program involves 27 faculty participants from the computational and biological sciences at the University of Texas at Austin, and it will support 12 graduate trainees each year for five years. Two major research areas will be emphasized: computational phylogenetics and applied phylogenetics. Phylogenies provide a fundamental framework for all of biology, and present the computational scientist with many technical challenges. Computational phylogenetics is concerned with the computational aspects of phylogenetic inference, and applied phylogenetics uses estimated phylogenies to address a wide diversity of biological questions. The training program will involve a series of new and existing courses and seminars, a summer training program for students from underrepresented areas of science, co-advisement of each graduate student by one computational and one biological faculty participant, placement of students into well-established research groups in biology and computer science, participation in spring recruitment conferences and fall phylogenetics retreats, and opportunities for internships in the bioinformatics industry, national laboratories, and non-government organizations. The goals of this project are: (i) design and implement an interdisciplinary training curriculum for graduate students across computational and biological sciences that prepares students to understand and contribute to both sides of computational biology; (ii) stimulate interdisciplinary graduate research and interdisciplinary interactions in general between computational scientists and biological scientists that will lead to development and testing of novel approaches to unsolved problems in phylogenetics and their application to problems in biology; (iii) prepare trainees for their careers beyond graduate school and help them achieve visibility in the larger research community; and (iv) evaluate and improve the program in computational and applied phylogenetics to ensure its success beyond the proposed IGERT project. This program will create a unique collaborative environment for graduate students and faculty from the computational and biological sciences.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Biological Sciences; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114387</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114387</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1292" target="n1294">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">IGERT: Computational Phylogenetics and Applications to Biology</data>
      <data key="e_abstract">Phylogenetics , the study of the relationships among genes, individuals, populations, and species, forms the basis for all of comparative biology. This IGERT grant will support a comprehensive, interdisciplinary graduate training program in Computational Phylogenetics and Applications to Biology. The program involves 27 faculty participants from the computational and biological sciences at the University of Texas at Austin, and it will support 12 graduate trainees each year for five years. Two major research areas will be emphasized: computational phylogenetics and applied phylogenetics. Phylogenies provide a fundamental framework for all of biology, and present the computational scientist with many technical challenges. Computational phylogenetics is concerned with the computational aspects of phylogenetic inference, and applied phylogenetics uses estimated phylogenies to address a wide diversity of biological questions. The training program will involve a series of new and existing courses and seminars, a summer training program for students from underrepresented areas of science, co-advisement of each graduate student by one computational and one biological faculty participant, placement of students into well-established research groups in biology and computer science, participation in spring recruitment conferences and fall phylogenetics retreats, and opportunities for internships in the bioinformatics industry, national laboratories, and non-government organizations. The goals of this project are: (i) design and implement an interdisciplinary training curriculum for graduate students across computational and biological sciences that prepares students to understand and contribute to both sides of computational biology; (ii) stimulate interdisciplinary graduate research and interdisciplinary interactions in general between computational scientists and biological scientists that will lead to development and testing of novel approaches to unsolved problems in phylogenetics and their application to problems in biology; (iii) prepare trainees for their careers beyond graduate school and help them achieve visibility in the larger research community; and (iv) evaluate and improve the program in computational and applied phylogenetics to ensure its success beyond the proposed IGERT project. This program will create a unique collaborative environment for graduate students and faculty from the computational and biological sciences.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Biological Sciences; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114387</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114387</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1292" target="n1380">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">IGERT: Computational Phylogenetics and Applications to Biology</data>
      <data key="e_abstract">Phylogenetics , the study of the relationships among genes, individuals, populations, and species, forms the basis for all of comparative biology. This IGERT grant will support a comprehensive, interdisciplinary graduate training program in Computational Phylogenetics and Applications to Biology. The program involves 27 faculty participants from the computational and biological sciences at the University of Texas at Austin, and it will support 12 graduate trainees each year for five years. Two major research areas will be emphasized: computational phylogenetics and applied phylogenetics. Phylogenies provide a fundamental framework for all of biology, and present the computational scientist with many technical challenges. Computational phylogenetics is concerned with the computational aspects of phylogenetic inference, and applied phylogenetics uses estimated phylogenies to address a wide diversity of biological questions. The training program will involve a series of new and existing courses and seminars, a summer training program for students from underrepresented areas of science, co-advisement of each graduate student by one computational and one biological faculty participant, placement of students into well-established research groups in biology and computer science, participation in spring recruitment conferences and fall phylogenetics retreats, and opportunities for internships in the bioinformatics industry, national laboratories, and non-government organizations. The goals of this project are: (i) design and implement an interdisciplinary training curriculum for graduate students across computational and biological sciences that prepares students to understand and contribute to both sides of computational biology; (ii) stimulate interdisciplinary graduate research and interdisciplinary interactions in general between computational scientists and biological scientists that will lead to development and testing of novel approaches to unsolved problems in phylogenetics and their application to problems in biology; (iii) prepare trainees for their careers beyond graduate school and help them achieve visibility in the larger research community; and (iv) evaluate and improve the program in computational and applied phylogenetics to ensure its success beyond the proposed IGERT project. This program will create a unique collaborative environment for graduate students and faculty from the computational and biological sciences.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Biological Sciences; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114387</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114387</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1292" target="n1295">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">IGERT: Computational Phylogenetics and Applications to Biology</data>
      <data key="e_abstract">Phylogenetics , the study of the relationships among genes, individuals, populations, and species, forms the basis for all of comparative biology. This IGERT grant will support a comprehensive, interdisciplinary graduate training program in Computational Phylogenetics and Applications to Biology. The program involves 27 faculty participants from the computational and biological sciences at the University of Texas at Austin, and it will support 12 graduate trainees each year for five years. Two major research areas will be emphasized: computational phylogenetics and applied phylogenetics. Phylogenies provide a fundamental framework for all of biology, and present the computational scientist with many technical challenges. Computational phylogenetics is concerned with the computational aspects of phylogenetic inference, and applied phylogenetics uses estimated phylogenies to address a wide diversity of biological questions. The training program will involve a series of new and existing courses and seminars, a summer training program for students from underrepresented areas of science, co-advisement of each graduate student by one computational and one biological faculty participant, placement of students into well-established research groups in biology and computer science, participation in spring recruitment conferences and fall phylogenetics retreats, and opportunities for internships in the bioinformatics industry, national laboratories, and non-government organizations. The goals of this project are: (i) design and implement an interdisciplinary training curriculum for graduate students across computational and biological sciences that prepares students to understand and contribute to both sides of computational biology; (ii) stimulate interdisciplinary graduate research and interdisciplinary interactions in general between computational scientists and biological scientists that will lead to development and testing of novel approaches to unsolved problems in phylogenetics and their application to problems in biology; (iii) prepare trainees for their careers beyond graduate school and help them achieve visibility in the larger research community; and (iv) evaluate and improve the program in computational and applied phylogenetics to ensure its success beyond the proposed IGERT project. This program will create a unique collaborative environment for graduate students and faculty from the computational and biological sciences.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Biological Sciences; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114387</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114387</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1293" target="n1294">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">IGERT: Computational Phylogenetics and Applications to Biology</data>
      <data key="e_abstract">Phylogenetics , the study of the relationships among genes, individuals, populations, and species, forms the basis for all of comparative biology. This IGERT grant will support a comprehensive, interdisciplinary graduate training program in Computational Phylogenetics and Applications to Biology. The program involves 27 faculty participants from the computational and biological sciences at the University of Texas at Austin, and it will support 12 graduate trainees each year for five years. Two major research areas will be emphasized: computational phylogenetics and applied phylogenetics. Phylogenies provide a fundamental framework for all of biology, and present the computational scientist with many technical challenges. Computational phylogenetics is concerned with the computational aspects of phylogenetic inference, and applied phylogenetics uses estimated phylogenies to address a wide diversity of biological questions. The training program will involve a series of new and existing courses and seminars, a summer training program for students from underrepresented areas of science, co-advisement of each graduate student by one computational and one biological faculty participant, placement of students into well-established research groups in biology and computer science, participation in spring recruitment conferences and fall phylogenetics retreats, and opportunities for internships in the bioinformatics industry, national laboratories, and non-government organizations. The goals of this project are: (i) design and implement an interdisciplinary training curriculum for graduate students across computational and biological sciences that prepares students to understand and contribute to both sides of computational biology; (ii) stimulate interdisciplinary graduate research and interdisciplinary interactions in general between computational scientists and biological scientists that will lead to development and testing of novel approaches to unsolved problems in phylogenetics and their application to problems in biology; (iii) prepare trainees for their careers beyond graduate school and help them achieve visibility in the larger research community; and (iv) evaluate and improve the program in computational and applied phylogenetics to ensure its success beyond the proposed IGERT project. This program will create a unique collaborative environment for graduate students and faculty from the computational and biological sciences.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Biological Sciences; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114387</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114387</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1293" target="n1380">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">IGERT: Computational Phylogenetics and Applications to Biology</data>
      <data key="e_abstract">Phylogenetics , the study of the relationships among genes, individuals, populations, and species, forms the basis for all of comparative biology. This IGERT grant will support a comprehensive, interdisciplinary graduate training program in Computational Phylogenetics and Applications to Biology. The program involves 27 faculty participants from the computational and biological sciences at the University of Texas at Austin, and it will support 12 graduate trainees each year for five years. Two major research areas will be emphasized: computational phylogenetics and applied phylogenetics. Phylogenies provide a fundamental framework for all of biology, and present the computational scientist with many technical challenges. Computational phylogenetics is concerned with the computational aspects of phylogenetic inference, and applied phylogenetics uses estimated phylogenies to address a wide diversity of biological questions. The training program will involve a series of new and existing courses and seminars, a summer training program for students from underrepresented areas of science, co-advisement of each graduate student by one computational and one biological faculty participant, placement of students into well-established research groups in biology and computer science, participation in spring recruitment conferences and fall phylogenetics retreats, and opportunities for internships in the bioinformatics industry, national laboratories, and non-government organizations. The goals of this project are: (i) design and implement an interdisciplinary training curriculum for graduate students across computational and biological sciences that prepares students to understand and contribute to both sides of computational biology; (ii) stimulate interdisciplinary graduate research and interdisciplinary interactions in general between computational scientists and biological scientists that will lead to development and testing of novel approaches to unsolved problems in phylogenetics and their application to problems in biology; (iii) prepare trainees for their careers beyond graduate school and help them achieve visibility in the larger research community; and (iv) evaluate and improve the program in computational and applied phylogenetics to ensure its success beyond the proposed IGERT project. This program will create a unique collaborative environment for graduate students and faculty from the computational and biological sciences.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Biological Sciences; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114387</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114387</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1293" target="n1295">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">IGERT: Computational Phylogenetics and Applications to Biology</data>
      <data key="e_abstract">Phylogenetics , the study of the relationships among genes, individuals, populations, and species, forms the basis for all of comparative biology. This IGERT grant will support a comprehensive, interdisciplinary graduate training program in Computational Phylogenetics and Applications to Biology. The program involves 27 faculty participants from the computational and biological sciences at the University of Texas at Austin, and it will support 12 graduate trainees each year for five years. Two major research areas will be emphasized: computational phylogenetics and applied phylogenetics. Phylogenies provide a fundamental framework for all of biology, and present the computational scientist with many technical challenges. Computational phylogenetics is concerned with the computational aspects of phylogenetic inference, and applied phylogenetics uses estimated phylogenies to address a wide diversity of biological questions. The training program will involve a series of new and existing courses and seminars, a summer training program for students from underrepresented areas of science, co-advisement of each graduate student by one computational and one biological faculty participant, placement of students into well-established research groups in biology and computer science, participation in spring recruitment conferences and fall phylogenetics retreats, and opportunities for internships in the bioinformatics industry, national laboratories, and non-government organizations. The goals of this project are: (i) design and implement an interdisciplinary training curriculum for graduate students across computational and biological sciences that prepares students to understand and contribute to both sides of computational biology; (ii) stimulate interdisciplinary graduate research and interdisciplinary interactions in general between computational scientists and biological scientists that will lead to development and testing of novel approaches to unsolved problems in phylogenetics and their application to problems in biology; (iii) prepare trainees for their careers beyond graduate school and help them achieve visibility in the larger research community; and (iv) evaluate and improve the program in computational and applied phylogenetics to ensure its success beyond the proposed IGERT project. This program will create a unique collaborative environment for graduate students and faculty from the computational and biological sciences.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Biological Sciences; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114387</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114387</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1294" target="n1380">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">IGERT: Computational Phylogenetics and Applications to Biology</data>
      <data key="e_abstract">Phylogenetics , the study of the relationships among genes, individuals, populations, and species, forms the basis for all of comparative biology. This IGERT grant will support a comprehensive, interdisciplinary graduate training program in Computational Phylogenetics and Applications to Biology. The program involves 27 faculty participants from the computational and biological sciences at the University of Texas at Austin, and it will support 12 graduate trainees each year for five years. Two major research areas will be emphasized: computational phylogenetics and applied phylogenetics. Phylogenies provide a fundamental framework for all of biology, and present the computational scientist with many technical challenges. Computational phylogenetics is concerned with the computational aspects of phylogenetic inference, and applied phylogenetics uses estimated phylogenies to address a wide diversity of biological questions. The training program will involve a series of new and existing courses and seminars, a summer training program for students from underrepresented areas of science, co-advisement of each graduate student by one computational and one biological faculty participant, placement of students into well-established research groups in biology and computer science, participation in spring recruitment conferences and fall phylogenetics retreats, and opportunities for internships in the bioinformatics industry, national laboratories, and non-government organizations. The goals of this project are: (i) design and implement an interdisciplinary training curriculum for graduate students across computational and biological sciences that prepares students to understand and contribute to both sides of computational biology; (ii) stimulate interdisciplinary graduate research and interdisciplinary interactions in general between computational scientists and biological scientists that will lead to development and testing of novel approaches to unsolved problems in phylogenetics and their application to problems in biology; (iii) prepare trainees for their careers beyond graduate school and help them achieve visibility in the larger research community; and (iv) evaluate and improve the program in computational and applied phylogenetics to ensure its success beyond the proposed IGERT project. This program will create a unique collaborative environment for graduate students and faculty from the computational and biological sciences.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Biological Sciences; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114387</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114387</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1294" target="n1295">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">IGERT: Computational Phylogenetics and Applications to Biology</data>
      <data key="e_abstract">Phylogenetics , the study of the relationships among genes, individuals, populations, and species, forms the basis for all of comparative biology. This IGERT grant will support a comprehensive, interdisciplinary graduate training program in Computational Phylogenetics and Applications to Biology. The program involves 27 faculty participants from the computational and biological sciences at the University of Texas at Austin, and it will support 12 graduate trainees each year for five years. Two major research areas will be emphasized: computational phylogenetics and applied phylogenetics. Phylogenies provide a fundamental framework for all of biology, and present the computational scientist with many technical challenges. Computational phylogenetics is concerned with the computational aspects of phylogenetic inference, and applied phylogenetics uses estimated phylogenies to address a wide diversity of biological questions. The training program will involve a series of new and existing courses and seminars, a summer training program for students from underrepresented areas of science, co-advisement of each graduate student by one computational and one biological faculty participant, placement of students into well-established research groups in biology and computer science, participation in spring recruitment conferences and fall phylogenetics retreats, and opportunities for internships in the bioinformatics industry, national laboratories, and non-government organizations. The goals of this project are: (i) design and implement an interdisciplinary training curriculum for graduate students across computational and biological sciences that prepares students to understand and contribute to both sides of computational biology; (ii) stimulate interdisciplinary graduate research and interdisciplinary interactions in general between computational scientists and biological scientists that will lead to development and testing of novel approaches to unsolved problems in phylogenetics and their application to problems in biology; (iii) prepare trainees for their careers beyond graduate school and help them achieve visibility in the larger research community; and (iv) evaluate and improve the program in computational and applied phylogenetics to ensure its success beyond the proposed IGERT project. This program will create a unique collaborative environment for graduate students and faculty from the computational and biological sciences.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Biological Sciences; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114387</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114387</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1295" target="n1380">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">IGERT: Computational Phylogenetics and Applications to Biology</data>
      <data key="e_abstract">Phylogenetics , the study of the relationships among genes, individuals, populations, and species, forms the basis for all of comparative biology. This IGERT grant will support a comprehensive, interdisciplinary graduate training program in Computational Phylogenetics and Applications to Biology. The program involves 27 faculty participants from the computational and biological sciences at the University of Texas at Austin, and it will support 12 graduate trainees each year for five years. Two major research areas will be emphasized: computational phylogenetics and applied phylogenetics. Phylogenies provide a fundamental framework for all of biology, and present the computational scientist with many technical challenges. Computational phylogenetics is concerned with the computational aspects of phylogenetic inference, and applied phylogenetics uses estimated phylogenies to address a wide diversity of biological questions. The training program will involve a series of new and existing courses and seminars, a summer training program for students from underrepresented areas of science, co-advisement of each graduate student by one computational and one biological faculty participant, placement of students into well-established research groups in biology and computer science, participation in spring recruitment conferences and fall phylogenetics retreats, and opportunities for internships in the bioinformatics industry, national laboratories, and non-government organizations. The goals of this project are: (i) design and implement an interdisciplinary training curriculum for graduate students across computational and biological sciences that prepares students to understand and contribute to both sides of computational biology; (ii) stimulate interdisciplinary graduate research and interdisciplinary interactions in general between computational scientists and biological scientists that will lead to development and testing of novel approaches to unsolved problems in phylogenetics and their application to problems in biology; (iii) prepare trainees for their careers beyond graduate school and help them achieve visibility in the larger research community; and (iv) evaluate and improve the program in computational and applied phylogenetics to ensure its success beyond the proposed IGERT project. This program will create a unique collaborative environment for graduate students and faculty from the computational and biological sciences.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Biological Sciences; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114387</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114387</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1383" target="n1384">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY: Situationally Appropriate Interaction</data>
      <data key="e_abstract">We are poised at the threshold of an information rich world with devices and services able to deliver that information to nearly anyone, at any place, and at any time. Humans have evolved social mechanisms for smoothly and flexibly managing interpersonal communications; however, current computational and communications devices are, almost without exception, utterly unaware of the social and attentional state of the user. They know little or nothing of the personal, social, and task situations in which they are used, and they do little or nothing to account for, and minimize, the human costs they induce. In this project, the PI and his team will explore situationally appropriate interfaces that retrieve, generate, and deliver information in a manner that is sensitive to the situation of the user. These interfaces will allow for communication and information systems that maneuver, rather than blunder, through the social world. To accomplish this ambitious goal, the team will pursue a three-part research plan. First, they will use behavioral theory and research to model social mechanisms for managing interpersonal communications. The comparatively unexploited research we will draw on examines the affordances of situations and consistent patterns of human nonverbal social behavior within situations. Second, they will extract key situational and user behavior data from these models via input from new sensing technologies, using noninvasive (e.g., vision-based) sensing technology to provide information about situations and users. Third, leveraging knowledge from sensory, perceptual, and cognitive psychology, as well as from the fields of visual and interaction design, the team will create displays and interaction designs that are far more situationally appropriate than today&apos;s interfaces. To address the substantial challenges that this breadth of work presents, the PI has assembled a strong multidisciplinary team that brings expertise from computer science, social, sensory, perceptual, and cognitive psychology, and the field of design.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121560</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121560</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1383" target="n1385">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY: Situationally Appropriate Interaction</data>
      <data key="e_abstract">We are poised at the threshold of an information rich world with devices and services able to deliver that information to nearly anyone, at any place, and at any time. Humans have evolved social mechanisms for smoothly and flexibly managing interpersonal communications; however, current computational and communications devices are, almost without exception, utterly unaware of the social and attentional state of the user. They know little or nothing of the personal, social, and task situations in which they are used, and they do little or nothing to account for, and minimize, the human costs they induce. In this project, the PI and his team will explore situationally appropriate interfaces that retrieve, generate, and deliver information in a manner that is sensitive to the situation of the user. These interfaces will allow for communication and information systems that maneuver, rather than blunder, through the social world. To accomplish this ambitious goal, the team will pursue a three-part research plan. First, they will use behavioral theory and research to model social mechanisms for managing interpersonal communications. The comparatively unexploited research we will draw on examines the affordances of situations and consistent patterns of human nonverbal social behavior within situations. Second, they will extract key situational and user behavior data from these models via input from new sensing technologies, using noninvasive (e.g., vision-based) sensing technology to provide information about situations and users. Third, leveraging knowledge from sensory, perceptual, and cognitive psychology, as well as from the fields of visual and interaction design, the team will create displays and interaction designs that are far more situationally appropriate than today&apos;s interfaces. To address the substantial challenges that this breadth of work presents, the PI has assembled a strong multidisciplinary team that brings expertise from computer science, social, sensory, perceptual, and cognitive psychology, and the field of design.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121560</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121560</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1383" target="n1386">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY: Situationally Appropriate Interaction</data>
      <data key="e_abstract">We are poised at the threshold of an information rich world with devices and services able to deliver that information to nearly anyone, at any place, and at any time. Humans have evolved social mechanisms for smoothly and flexibly managing interpersonal communications; however, current computational and communications devices are, almost without exception, utterly unaware of the social and attentional state of the user. They know little or nothing of the personal, social, and task situations in which they are used, and they do little or nothing to account for, and minimize, the human costs they induce. In this project, the PI and his team will explore situationally appropriate interfaces that retrieve, generate, and deliver information in a manner that is sensitive to the situation of the user. These interfaces will allow for communication and information systems that maneuver, rather than blunder, through the social world. To accomplish this ambitious goal, the team will pursue a three-part research plan. First, they will use behavioral theory and research to model social mechanisms for managing interpersonal communications. The comparatively unexploited research we will draw on examines the affordances of situations and consistent patterns of human nonverbal social behavior within situations. Second, they will extract key situational and user behavior data from these models via input from new sensing technologies, using noninvasive (e.g., vision-based) sensing technology to provide information about situations and users. Third, leveraging knowledge from sensory, perceptual, and cognitive psychology, as well as from the fields of visual and interaction design, the team will create displays and interaction designs that are far more situationally appropriate than today&apos;s interfaces. To address the substantial challenges that this breadth of work presents, the PI has assembled a strong multidisciplinary team that brings expertise from computer science, social, sensory, perceptual, and cognitive psychology, and the field of design.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121560</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121560</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1384" target="n1385">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY: Situationally Appropriate Interaction</data>
      <data key="e_abstract">We are poised at the threshold of an information rich world with devices and services able to deliver that information to nearly anyone, at any place, and at any time. Humans have evolved social mechanisms for smoothly and flexibly managing interpersonal communications; however, current computational and communications devices are, almost without exception, utterly unaware of the social and attentional state of the user. They know little or nothing of the personal, social, and task situations in which they are used, and they do little or nothing to account for, and minimize, the human costs they induce. In this project, the PI and his team will explore situationally appropriate interfaces that retrieve, generate, and deliver information in a manner that is sensitive to the situation of the user. These interfaces will allow for communication and information systems that maneuver, rather than blunder, through the social world. To accomplish this ambitious goal, the team will pursue a three-part research plan. First, they will use behavioral theory and research to model social mechanisms for managing interpersonal communications. The comparatively unexploited research we will draw on examines the affordances of situations and consistent patterns of human nonverbal social behavior within situations. Second, they will extract key situational and user behavior data from these models via input from new sensing technologies, using noninvasive (e.g., vision-based) sensing technology to provide information about situations and users. Third, leveraging knowledge from sensory, perceptual, and cognitive psychology, as well as from the fields of visual and interaction design, the team will create displays and interaction designs that are far more situationally appropriate than today&apos;s interfaces. To address the substantial challenges that this breadth of work presents, the PI has assembled a strong multidisciplinary team that brings expertise from computer science, social, sensory, perceptual, and cognitive psychology, and the field of design.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121560</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121560</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1384" target="n1386">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY: Situationally Appropriate Interaction</data>
      <data key="e_abstract">We are poised at the threshold of an information rich world with devices and services able to deliver that information to nearly anyone, at any place, and at any time. Humans have evolved social mechanisms for smoothly and flexibly managing interpersonal communications; however, current computational and communications devices are, almost without exception, utterly unaware of the social and attentional state of the user. They know little or nothing of the personal, social, and task situations in which they are used, and they do little or nothing to account for, and minimize, the human costs they induce. In this project, the PI and his team will explore situationally appropriate interfaces that retrieve, generate, and deliver information in a manner that is sensitive to the situation of the user. These interfaces will allow for communication and information systems that maneuver, rather than blunder, through the social world. To accomplish this ambitious goal, the team will pursue a three-part research plan. First, they will use behavioral theory and research to model social mechanisms for managing interpersonal communications. The comparatively unexploited research we will draw on examines the affordances of situations and consistent patterns of human nonverbal social behavior within situations. Second, they will extract key situational and user behavior data from these models via input from new sensing technologies, using noninvasive (e.g., vision-based) sensing technology to provide information about situations and users. Third, leveraging knowledge from sensory, perceptual, and cognitive psychology, as well as from the fields of visual and interaction design, the team will create displays and interaction designs that are far more situationally appropriate than today&apos;s interfaces. To address the substantial challenges that this breadth of work presents, the PI has assembled a strong multidisciplinary team that brings expertise from computer science, social, sensory, perceptual, and cognitive psychology, and the field of design.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121560</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121560</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1385" target="n1386">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY: Situationally Appropriate Interaction</data>
      <data key="e_abstract">We are poised at the threshold of an information rich world with devices and services able to deliver that information to nearly anyone, at any place, and at any time. Humans have evolved social mechanisms for smoothly and flexibly managing interpersonal communications; however, current computational and communications devices are, almost without exception, utterly unaware of the social and attentional state of the user. They know little or nothing of the personal, social, and task situations in which they are used, and they do little or nothing to account for, and minimize, the human costs they induce. In this project, the PI and his team will explore situationally appropriate interfaces that retrieve, generate, and deliver information in a manner that is sensitive to the situation of the user. These interfaces will allow for communication and information systems that maneuver, rather than blunder, through the social world. To accomplish this ambitious goal, the team will pursue a three-part research plan. First, they will use behavioral theory and research to model social mechanisms for managing interpersonal communications. The comparatively unexploited research we will draw on examines the affordances of situations and consistent patterns of human nonverbal social behavior within situations. Second, they will extract key situational and user behavior data from these models via input from new sensing technologies, using noninvasive (e.g., vision-based) sensing technology to provide information about situations and users. Third, leveraging knowledge from sensory, perceptual, and cognitive psychology, as well as from the fields of visual and interaction design, the team will create displays and interaction designs that are far more situationally appropriate than today&apos;s interfaces. To address the substantial challenges that this breadth of work presents, the PI has assembled a strong multidisciplinary team that brings expertise from computer science, social, sensory, perceptual, and cognitive psychology, and the field of design.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121560</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121560</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1387" target="n1388">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: Optical Commuications and Networking</data>
      <data key="e_abstract">The University of Central Florida will establish an IGERT program with a multidisciplinary research theme in Optical Communications and Networking to train 30 Ph.D. students over the next five years. This program is a joint effort of twenty scientists, engineers and educators from the Departments/Schools of Mathematics, Statistics, Optics, Physics, Material science, Electrical Engineering and Computer Science, and Education at UCF. Optical communications and networking is a particularly well-suited IGERT theme because the diverse multidisciplinary technologies that need to be developed to enable next-generation information infrastructure. Major research efforts are grouped in four areas: advanced components, transport, switching, and networking and network management. Each IGERT thesis project is designed to build upon expertise from at least two different groups in realizing an enhanced functionality that is greater than the sum of the parts, over and above what the two groups would pursue independently. The advising team of each IGERT student, consisting of two or more faculty members from different departments, an industrial advisor and a non-technical advisor, is designed to ensure the successful integration of education, research and training. Industry, the users of technology considered in this effort, will be involved at the onset of the thesis research for each student. The research framework is complemented by the existing multidisciplinary courses, new courses in optical communications and networking to be developed under this IGERT program and being developed under an existing NSF CRCD grant, on-site training on state-of-the-art equipment at UCF, off-site training in industry, and non-technical training including business, communication/ interpersonal skills and ethics. The objective of this IGERT program is not only to train the participating Ph.D. students to become leaders in industry and/or academia but also for this program to serve as a national model for training scientists and engineers in today&apos;s globally competitive and technology-driven market economy. &lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114418</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114418</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1387" target="n1389">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: Optical Commuications and Networking</data>
      <data key="e_abstract">The University of Central Florida will establish an IGERT program with a multidisciplinary research theme in Optical Communications and Networking to train 30 Ph.D. students over the next five years. This program is a joint effort of twenty scientists, engineers and educators from the Departments/Schools of Mathematics, Statistics, Optics, Physics, Material science, Electrical Engineering and Computer Science, and Education at UCF. Optical communications and networking is a particularly well-suited IGERT theme because the diverse multidisciplinary technologies that need to be developed to enable next-generation information infrastructure. Major research efforts are grouped in four areas: advanced components, transport, switching, and networking and network management. Each IGERT thesis project is designed to build upon expertise from at least two different groups in realizing an enhanced functionality that is greater than the sum of the parts, over and above what the two groups would pursue independently. The advising team of each IGERT student, consisting of two or more faculty members from different departments, an industrial advisor and a non-technical advisor, is designed to ensure the successful integration of education, research and training. Industry, the users of technology considered in this effort, will be involved at the onset of the thesis research for each student. The research framework is complemented by the existing multidisciplinary courses, new courses in optical communications and networking to be developed under this IGERT program and being developed under an existing NSF CRCD grant, on-site training on state-of-the-art equipment at UCF, off-site training in industry, and non-technical training including business, communication/ interpersonal skills and ethics. The objective of this IGERT program is not only to train the participating Ph.D. students to become leaders in industry and/or academia but also for this program to serve as a national model for training scientists and engineers in today&apos;s globally competitive and technology-driven market economy. &lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114418</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114418</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1387" target="n1390">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: Optical Commuications and Networking</data>
      <data key="e_abstract">The University of Central Florida will establish an IGERT program with a multidisciplinary research theme in Optical Communications and Networking to train 30 Ph.D. students over the next five years. This program is a joint effort of twenty scientists, engineers and educators from the Departments/Schools of Mathematics, Statistics, Optics, Physics, Material science, Electrical Engineering and Computer Science, and Education at UCF. Optical communications and networking is a particularly well-suited IGERT theme because the diverse multidisciplinary technologies that need to be developed to enable next-generation information infrastructure. Major research efforts are grouped in four areas: advanced components, transport, switching, and networking and network management. Each IGERT thesis project is designed to build upon expertise from at least two different groups in realizing an enhanced functionality that is greater than the sum of the parts, over and above what the two groups would pursue independently. The advising team of each IGERT student, consisting of two or more faculty members from different departments, an industrial advisor and a non-technical advisor, is designed to ensure the successful integration of education, research and training. Industry, the users of technology considered in this effort, will be involved at the onset of the thesis research for each student. The research framework is complemented by the existing multidisciplinary courses, new courses in optical communications and networking to be developed under this IGERT program and being developed under an existing NSF CRCD grant, on-site training on state-of-the-art equipment at UCF, off-site training in industry, and non-technical training including business, communication/ interpersonal skills and ethics. The objective of this IGERT program is not only to train the participating Ph.D. students to become leaders in industry and/or academia but also for this program to serve as a national model for training scientists and engineers in today&apos;s globally competitive and technology-driven market economy. &lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114418</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114418</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1387" target="n1391">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: Optical Commuications and Networking</data>
      <data key="e_abstract">The University of Central Florida will establish an IGERT program with a multidisciplinary research theme in Optical Communications and Networking to train 30 Ph.D. students over the next five years. This program is a joint effort of twenty scientists, engineers and educators from the Departments/Schools of Mathematics, Statistics, Optics, Physics, Material science, Electrical Engineering and Computer Science, and Education at UCF. Optical communications and networking is a particularly well-suited IGERT theme because the diverse multidisciplinary technologies that need to be developed to enable next-generation information infrastructure. Major research efforts are grouped in four areas: advanced components, transport, switching, and networking and network management. Each IGERT thesis project is designed to build upon expertise from at least two different groups in realizing an enhanced functionality that is greater than the sum of the parts, over and above what the two groups would pursue independently. The advising team of each IGERT student, consisting of two or more faculty members from different departments, an industrial advisor and a non-technical advisor, is designed to ensure the successful integration of education, research and training. Industry, the users of technology considered in this effort, will be involved at the onset of the thesis research for each student. The research framework is complemented by the existing multidisciplinary courses, new courses in optical communications and networking to be developed under this IGERT program and being developed under an existing NSF CRCD grant, on-site training on state-of-the-art equipment at UCF, off-site training in industry, and non-technical training including business, communication/ interpersonal skills and ethics. The objective of this IGERT program is not only to train the participating Ph.D. students to become leaders in industry and/or academia but also for this program to serve as a national model for training scientists and engineers in today&apos;s globally competitive and technology-driven market economy. &lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114418</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114418</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1388" target="n1389">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: Optical Commuications and Networking</data>
      <data key="e_abstract">The University of Central Florida will establish an IGERT program with a multidisciplinary research theme in Optical Communications and Networking to train 30 Ph.D. students over the next five years. This program is a joint effort of twenty scientists, engineers and educators from the Departments/Schools of Mathematics, Statistics, Optics, Physics, Material science, Electrical Engineering and Computer Science, and Education at UCF. Optical communications and networking is a particularly well-suited IGERT theme because the diverse multidisciplinary technologies that need to be developed to enable next-generation information infrastructure. Major research efforts are grouped in four areas: advanced components, transport, switching, and networking and network management. Each IGERT thesis project is designed to build upon expertise from at least two different groups in realizing an enhanced functionality that is greater than the sum of the parts, over and above what the two groups would pursue independently. The advising team of each IGERT student, consisting of two or more faculty members from different departments, an industrial advisor and a non-technical advisor, is designed to ensure the successful integration of education, research and training. Industry, the users of technology considered in this effort, will be involved at the onset of the thesis research for each student. The research framework is complemented by the existing multidisciplinary courses, new courses in optical communications and networking to be developed under this IGERT program and being developed under an existing NSF CRCD grant, on-site training on state-of-the-art equipment at UCF, off-site training in industry, and non-technical training including business, communication/ interpersonal skills and ethics. The objective of this IGERT program is not only to train the participating Ph.D. students to become leaders in industry and/or academia but also for this program to serve as a national model for training scientists and engineers in today&apos;s globally competitive and technology-driven market economy. &lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114418</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114418</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1388" target="n1390">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: Optical Commuications and Networking</data>
      <data key="e_abstract">The University of Central Florida will establish an IGERT program with a multidisciplinary research theme in Optical Communications and Networking to train 30 Ph.D. students over the next five years. This program is a joint effort of twenty scientists, engineers and educators from the Departments/Schools of Mathematics, Statistics, Optics, Physics, Material science, Electrical Engineering and Computer Science, and Education at UCF. Optical communications and networking is a particularly well-suited IGERT theme because the diverse multidisciplinary technologies that need to be developed to enable next-generation information infrastructure. Major research efforts are grouped in four areas: advanced components, transport, switching, and networking and network management. Each IGERT thesis project is designed to build upon expertise from at least two different groups in realizing an enhanced functionality that is greater than the sum of the parts, over and above what the two groups would pursue independently. The advising team of each IGERT student, consisting of two or more faculty members from different departments, an industrial advisor and a non-technical advisor, is designed to ensure the successful integration of education, research and training. Industry, the users of technology considered in this effort, will be involved at the onset of the thesis research for each student. The research framework is complemented by the existing multidisciplinary courses, new courses in optical communications and networking to be developed under this IGERT program and being developed under an existing NSF CRCD grant, on-site training on state-of-the-art equipment at UCF, off-site training in industry, and non-technical training including business, communication/ interpersonal skills and ethics. The objective of this IGERT program is not only to train the participating Ph.D. students to become leaders in industry and/or academia but also for this program to serve as a national model for training scientists and engineers in today&apos;s globally competitive and technology-driven market economy. &lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114418</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114418</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1388" target="n1391">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: Optical Commuications and Networking</data>
      <data key="e_abstract">The University of Central Florida will establish an IGERT program with a multidisciplinary research theme in Optical Communications and Networking to train 30 Ph.D. students over the next five years. This program is a joint effort of twenty scientists, engineers and educators from the Departments/Schools of Mathematics, Statistics, Optics, Physics, Material science, Electrical Engineering and Computer Science, and Education at UCF. Optical communications and networking is a particularly well-suited IGERT theme because the diverse multidisciplinary technologies that need to be developed to enable next-generation information infrastructure. Major research efforts are grouped in four areas: advanced components, transport, switching, and networking and network management. Each IGERT thesis project is designed to build upon expertise from at least two different groups in realizing an enhanced functionality that is greater than the sum of the parts, over and above what the two groups would pursue independently. The advising team of each IGERT student, consisting of two or more faculty members from different departments, an industrial advisor and a non-technical advisor, is designed to ensure the successful integration of education, research and training. Industry, the users of technology considered in this effort, will be involved at the onset of the thesis research for each student. The research framework is complemented by the existing multidisciplinary courses, new courses in optical communications and networking to be developed under this IGERT program and being developed under an existing NSF CRCD grant, on-site training on state-of-the-art equipment at UCF, off-site training in industry, and non-technical training including business, communication/ interpersonal skills and ethics. The objective of this IGERT program is not only to train the participating Ph.D. students to become leaders in industry and/or academia but also for this program to serve as a national model for training scientists and engineers in today&apos;s globally competitive and technology-driven market economy. &lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114418</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114418</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1389" target="n1390">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: Optical Commuications and Networking</data>
      <data key="e_abstract">The University of Central Florida will establish an IGERT program with a multidisciplinary research theme in Optical Communications and Networking to train 30 Ph.D. students over the next five years. This program is a joint effort of twenty scientists, engineers and educators from the Departments/Schools of Mathematics, Statistics, Optics, Physics, Material science, Electrical Engineering and Computer Science, and Education at UCF. Optical communications and networking is a particularly well-suited IGERT theme because the diverse multidisciplinary technologies that need to be developed to enable next-generation information infrastructure. Major research efforts are grouped in four areas: advanced components, transport, switching, and networking and network management. Each IGERT thesis project is designed to build upon expertise from at least two different groups in realizing an enhanced functionality that is greater than the sum of the parts, over and above what the two groups would pursue independently. The advising team of each IGERT student, consisting of two or more faculty members from different departments, an industrial advisor and a non-technical advisor, is designed to ensure the successful integration of education, research and training. Industry, the users of technology considered in this effort, will be involved at the onset of the thesis research for each student. The research framework is complemented by the existing multidisciplinary courses, new courses in optical communications and networking to be developed under this IGERT program and being developed under an existing NSF CRCD grant, on-site training on state-of-the-art equipment at UCF, off-site training in industry, and non-technical training including business, communication/ interpersonal skills and ethics. The objective of this IGERT program is not only to train the participating Ph.D. students to become leaders in industry and/or academia but also for this program to serve as a national model for training scientists and engineers in today&apos;s globally competitive and technology-driven market economy. &lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114418</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114418</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1389" target="n1391">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: Optical Commuications and Networking</data>
      <data key="e_abstract">The University of Central Florida will establish an IGERT program with a multidisciplinary research theme in Optical Communications and Networking to train 30 Ph.D. students over the next five years. This program is a joint effort of twenty scientists, engineers and educators from the Departments/Schools of Mathematics, Statistics, Optics, Physics, Material science, Electrical Engineering and Computer Science, and Education at UCF. Optical communications and networking is a particularly well-suited IGERT theme because the diverse multidisciplinary technologies that need to be developed to enable next-generation information infrastructure. Major research efforts are grouped in four areas: advanced components, transport, switching, and networking and network management. Each IGERT thesis project is designed to build upon expertise from at least two different groups in realizing an enhanced functionality that is greater than the sum of the parts, over and above what the two groups would pursue independently. The advising team of each IGERT student, consisting of two or more faculty members from different departments, an industrial advisor and a non-technical advisor, is designed to ensure the successful integration of education, research and training. Industry, the users of technology considered in this effort, will be involved at the onset of the thesis research for each student. The research framework is complemented by the existing multidisciplinary courses, new courses in optical communications and networking to be developed under this IGERT program and being developed under an existing NSF CRCD grant, on-site training on state-of-the-art equipment at UCF, off-site training in industry, and non-technical training including business, communication/ interpersonal skills and ethics. The objective of this IGERT program is not only to train the participating Ph.D. students to become leaders in industry and/or academia but also for this program to serve as a national model for training scientists and engineers in today&apos;s globally competitive and technology-driven market economy. &lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114418</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114418</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1390" target="n1391">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">IGERT: Optical Commuications and Networking</data>
      <data key="e_abstract">The University of Central Florida will establish an IGERT program with a multidisciplinary research theme in Optical Communications and Networking to train 30 Ph.D. students over the next five years. This program is a joint effort of twenty scientists, engineers and educators from the Departments/Schools of Mathematics, Statistics, Optics, Physics, Material science, Electrical Engineering and Computer Science, and Education at UCF. Optical communications and networking is a particularly well-suited IGERT theme because the diverse multidisciplinary technologies that need to be developed to enable next-generation information infrastructure. Major research efforts are grouped in four areas: advanced components, transport, switching, and networking and network management. Each IGERT thesis project is designed to build upon expertise from at least two different groups in realizing an enhanced functionality that is greater than the sum of the parts, over and above what the two groups would pursue independently. The advising team of each IGERT student, consisting of two or more faculty members from different departments, an industrial advisor and a non-technical advisor, is designed to ensure the successful integration of education, research and training. Industry, the users of technology considered in this effort, will be involved at the onset of the thesis research for each student. The research framework is complemented by the existing multidisciplinary courses, new courses in optical communications and networking to be developed under this IGERT program and being developed under an existing NSF CRCD grant, on-site training on state-of-the-art equipment at UCF, off-site training in industry, and non-technical training including business, communication/ interpersonal skills and ethics. The objective of this IGERT program is not only to train the participating Ph.D. students to become leaders in industry and/or academia but also for this program to serve as a national model for training scientists and engineers in today&apos;s globally competitive and technology-driven market economy. &lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Engineering; Computer and Information Science and Engineering; Mathematical and Physical Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">114418</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">114418</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1392" target="n1393">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SI+IM (CISE):Distributed Data Compression and Dissemination for Wireless Sensor Networks</data>
      <data key="e_abstract">This project will develop new methods for the efficient dissemination of the data collected by wireless&lt;br/&gt;sensor networks. With recent and projected advances in small, low cost microelectronic and micro-electromechanical sensors, it is easy to envision that a large array of sensors, distributed over an&lt;br/&gt;appropriate region, will be able to measure the spatial and temporal variations of important attribute&lt;br/&gt;field such as temperature, moisture, sound, light, gas concentrations, etc.. However, to realize the&lt;br/&gt;benefits of such arrays, wireless communication networks must be devised that with low power encode&lt;br/&gt;and disseminate the large amounts of data they generate. With this as the goal, the project will&lt;br/&gt;develop new methods of distributed data compression and data dissemination for dense sensor arrays,&lt;br/&gt;that is, for arrays whose sensors are so close that their measurements are highly correlated. One thesis&lt;br/&gt;of this project is that the correlations in such arrays can be exploited in order to make the network&lt;br/&gt;operate essentially as efficiently as a sparse sensor network, while having the additional advantages of&lt;br/&gt;being resilient to sensor failures and permitting the attribute field to be measured adaptively or with&lt;br/&gt;higher spatial resolution. Another thesis is that the data compression and dissemination issues for&lt;br/&gt;such networks are deeply intertwined. Accordingly, the project focuses on the joint design of such.&lt;br/&gt;For example, it seeks methodology for tailoring distributed data compression methods to partcular&lt;br/&gt;dissemination strategies, and vice versa. In the process, it proposed to learn how the performance of&lt;br/&gt;sensor networks depends on a variety of issues such as the number and placement of sensors.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112801</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">112801</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n699" target="n1395">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Query Processing in Pervasive Location Aware Computing Environments</data>
      <data key="e_abstract">Abstract:&lt;br/&gt;This proposal centers on research challenges arising from the realization of Pervasive Location-Aware Computing Environments (PLACE). In PLACE, mobile objects are aware of their own locations as well as&lt;br/&gt;those of surrounding objects. Such an environment enables (i) the navigation of moving objects, (ii) the execution of continuous queries about moving objects, and (iii) services for groups of moving &lt;br/&gt;objects. The pervasive nature of location-aware objects, the need for timely responses to numerous concurrent continuous queries dependent upon continuously arriving data pose new challenges for scalable query processing.&lt;br/&gt;&lt;br/&gt;The proposed research investigates query processing, data management, and broadcasting techniques for the efficient execution of continuous queries. The proposed techniques include (i) the use of data filters and indexing techniques to reduce the amount of data the queries need to analyze, and (ii) the development of techniques for executing continuous queries in an environment characterized by differences in bandwidth, communication costs, and computational capabilities of the objects. The proposal includes building a prototype system to validate and evaluate the proposed techniques and solutions.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">10044</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">10044</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n699" target="n1396">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Query Processing in Pervasive Location Aware Computing Environments</data>
      <data key="e_abstract">Abstract:&lt;br/&gt;This proposal centers on research challenges arising from the realization of Pervasive Location-Aware Computing Environments (PLACE). In PLACE, mobile objects are aware of their own locations as well as&lt;br/&gt;those of surrounding objects. Such an environment enables (i) the navigation of moving objects, (ii) the execution of continuous queries about moving objects, and (iii) services for groups of moving &lt;br/&gt;objects. The pervasive nature of location-aware objects, the need for timely responses to numerous concurrent continuous queries dependent upon continuously arriving data pose new challenges for scalable query processing.&lt;br/&gt;&lt;br/&gt;The proposed research investigates query processing, data management, and broadcasting techniques for the efficient execution of continuous queries. The proposed techniques include (i) the use of data filters and indexing techniques to reduce the amount of data the queries need to analyze, and (ii) the development of techniques for executing continuous queries in an environment characterized by differences in bandwidth, communication costs, and computational capabilities of the objects. The proposal includes building a prototype system to validate and evaluate the proposed techniques and solutions.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">10044</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">10044</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1395" target="n1396">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Query Processing in Pervasive Location Aware Computing Environments</data>
      <data key="e_abstract">Abstract:&lt;br/&gt;This proposal centers on research challenges arising from the realization of Pervasive Location-Aware Computing Environments (PLACE). In PLACE, mobile objects are aware of their own locations as well as&lt;br/&gt;those of surrounding objects. Such an environment enables (i) the navigation of moving objects, (ii) the execution of continuous queries about moving objects, and (iii) services for groups of moving &lt;br/&gt;objects. The pervasive nature of location-aware objects, the need for timely responses to numerous concurrent continuous queries dependent upon continuously arriving data pose new challenges for scalable query processing.&lt;br/&gt;&lt;br/&gt;The proposed research investigates query processing, data management, and broadcasting techniques for the efficient execution of continuous queries. The proposed techniques include (i) the use of data filters and indexing techniques to reduce the amount of data the queries need to analyze, and (ii) the development of techniques for executing continuous queries in an environment characterized by differences in bandwidth, communication costs, and computational capabilities of the objects. The proposal includes building a prototype system to validate and evaluate the proposed techniques and solutions.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">10044</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">10044</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n550" target="n551">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">A Semantic Caching Service for Wireless Network Centric Environments</data>
      <data key="e_abstract">The rapid development of wireless communications technologies and their impact on everyday life has greatly increased over the last couple of years. But, the traditional client/server architecture, where the client relies completely on the server for information, is not useful in the wireless environment, as the server is often not reachable because it is not connected.&lt;br/&gt;&lt;br/&gt;The proposal focuses on the development of wireless communications technologies and plans to design, develop and implement a &quot;client/proxy/server system&quot; in which a client-side proxy assumes the role of a local server during the disconnections. &lt;br/&gt;&lt;br/&gt;Preliminary work shows that the approach has great potential to improve data access, computation and service. The targeted application areas include distance learning, fast mobile Internet as well as 3rd and 4th generation wireless cellular networks.</data>
      <data key="e_pgm">4089</data>
      <data key="e_label">123950</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">123950</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n550" target="n595">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">A Semantic Caching Service for Wireless Network Centric Environments</data>
      <data key="e_abstract">The rapid development of wireless communications technologies and their impact on everyday life has greatly increased over the last couple of years. But, the traditional client/server architecture, where the client relies completely on the server for information, is not useful in the wireless environment, as the server is often not reachable because it is not connected.&lt;br/&gt;&lt;br/&gt;The proposal focuses on the development of wireless communications technologies and plans to design, develop and implement a &quot;client/proxy/server system&quot; in which a client-side proxy assumes the role of a local server during the disconnections. &lt;br/&gt;&lt;br/&gt;Preliminary work shows that the approach has great potential to improve data access, computation and service. The targeted application areas include distance learning, fast mobile Internet as well as 3rd and 4th generation wireless cellular networks.</data>
      <data key="e_pgm">4089</data>
      <data key="e_label">123950</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">123950</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n551" target="n595">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">A Semantic Caching Service for Wireless Network Centric Environments</data>
      <data key="e_abstract">The rapid development of wireless communications technologies and their impact on everyday life has greatly increased over the last couple of years. But, the traditional client/server architecture, where the client relies completely on the server for information, is not useful in the wireless environment, as the server is often not reachable because it is not connected.&lt;br/&gt;&lt;br/&gt;The proposal focuses on the development of wireless communications technologies and plans to design, develop and implement a &quot;client/proxy/server system&quot; in which a client-side proxy assumes the role of a local server during the disconnections. &lt;br/&gt;&lt;br/&gt;Preliminary work shows that the approach has great potential to improve data access, computation and service. The targeted application areas include distance learning, fast mobile Internet as well as 3rd and 4th generation wireless cellular networks.</data>
      <data key="e_pgm">4089</data>
      <data key="e_label">123950</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">123950</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1401" target="n1402">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">MWIR(CISE/ANIR): A Secure and Scalable Bandwidth Management Platform for Open Market-based Dynamic and Distributed Provisioning DiffServ Interconnection</data>
      <data key="e_abstract">The convergence of telecommunications networks rely heavily on the interconnection of networks and related Quality of Service (QoS). The Internet is evolving into a multi-class service, integrated network, which is trying to support various applications, which require QoS in addition to the traditional best-effort applications. &lt;br/&gt;&lt;br/&gt;The goal of the proposal is to design, evaluate, develop and test new dynamic Quality of Service and bandwidth management mechanisms and systems that allow secure and scalable dynamic open provisioning for the interconnection of Differentiated services. The proposal will develop a Bandwidth Management Point to manage scalable and secure control, and performs the functions of service assignment, admission control, traffic conditioning and class routing.&lt;br/&gt;&lt;br/&gt;The Bandwidth Management Point will support intra-domain communications, which is useful for applications within the same domain. It will also support inter-domain applications including policy information such as reservation process, edge-to-edge admission control and peer-to-peer access control. The deliverables include the design, analysis and implementation of software.</data>
      <data key="e_pgm">4089</data>
      <data key="e_label">123939</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">123939</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1403" target="n1404">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">New Directions in Cluster Computing: A Conference at the American Museum of Natural History &quot;New Directions in Cluster Supercomputing&quot;</data>
      <data key="e_abstract">This proposal is for partial support of the conference, &quot;New Directions in Cluster Supercomputing&quot;, being held June 12-14, 2001 in New York City. The conference will examine the impact of low-cost distributed &quot;supercomputing&quot; in areas of science and technology and other application areas. A goal of the event is to stimulate the exchange of ideas and examine general approaches and techniques, and encourage increased interaction among members of the systems, algorithms and natural science applications communities.&lt;br/&gt;The conference will run concurrently with the Museum&apos;s first major public exhibition on the human genome, &quot;The Genomic Revolution.&quot; Support for the conference is also being provided by NASA and public sector technology corporations..</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">123975</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">123975</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n16" target="n926">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Procedural Representation and Visualization Enabling Personalized Computational Fluid Dynamics</data>
      <data key="e_abstract">Computer power has increased dramatically over the past decade and has allowed computational fluid dynamics (CFD) researchers to more accurately simulate many types of complex flow. These simulations have enabled great leaps forward in the design and safety of ships, airplanes, automobiles, and other vehicles. However, this new power has also yielded terabytes of data, and CFD researchers now face a very difficult task in trying to find, extract, and analyze important flow features (e.g., time varying vortices, shock waves) buried within these monstrous datasets. Unlike the explosive growth in computer power, visualization tools for very large datasets have evolved modestly and cannot yet help with these tasks significantly. In particular, since detailed visualization of such large datasets is impractical, CFD researchers must work at a very cumbersome, low level to dice their datasets into workable pieces.&lt;br/&gt;&lt;br/&gt;CFD researchers desperately need new techniques that simplify and automate the iterative process of finding the appropriate portion of their data set. They need a system that will allow the user to articulate appropriate types of features of interest, provide a compact representation of those features, and effectively visualize the feature information locally. The system will have to overcome the challenges of loading a sufficient portion of the data set over a network connection into a desktop machine, mapping the entire data set to a visual representation, and rendering the results at interactive rates.&lt;br/&gt;&lt;br/&gt;This project will attack these CFD visualization problems by developing techniques for creating and using a procedural abstraction for a dataset. The major research objectives are to:&lt;br/&gt;1. Detect features (e.g. shocks) in complex flows using topological operators.&lt;br/&gt;2. Characterize the data relative to these features using a procedural representation consisting of implicit models and free-form deformations.&lt;br/&gt;3. Adapt the procedural representation to the appropriate level of detail using multi-resolution techniques.&lt;br/&gt;4. Encapsulate domain-specific knowledge as metadata to explore these extremely large datasets.&lt;br/&gt;5. Visualize the data directly from the procedural representation.&lt;br/&gt;6. Verify the accuracy of the procedural representation by tracking approximation error.&lt;br/&gt;7. Apply these techniques to the large-scale computational flow simulation problems currently studied at Stanford and Mississippi State University. &lt;br/&gt;The resulting system will allow CFD researchers to work more effectively by interactively exploring their data to pinpoint the features of interest. Moreover, the results of this project will provide solutions not only for CFD researchers, but also for a wide variety of other visualization challenges and applications. The project&apos;s main goal is to develop techniques that allow visualization exploration, feature detection, extraction, and analysis at a higher, more effective level through the use of procedural data abstraction and representation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121288</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121288</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n926" target="n1408">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Procedural Representation and Visualization Enabling Personalized Computational Fluid Dynamics</data>
      <data key="e_abstract">Computer power has increased dramatically over the past decade and has allowed computational fluid dynamics (CFD) researchers to more accurately simulate many types of complex flow. These simulations have enabled great leaps forward in the design and safety of ships, airplanes, automobiles, and other vehicles. However, this new power has also yielded terabytes of data, and CFD researchers now face a very difficult task in trying to find, extract, and analyze important flow features (e.g., time varying vortices, shock waves) buried within these monstrous datasets. Unlike the explosive growth in computer power, visualization tools for very large datasets have evolved modestly and cannot yet help with these tasks significantly. In particular, since detailed visualization of such large datasets is impractical, CFD researchers must work at a very cumbersome, low level to dice their datasets into workable pieces.&lt;br/&gt;&lt;br/&gt;CFD researchers desperately need new techniques that simplify and automate the iterative process of finding the appropriate portion of their data set. They need a system that will allow the user to articulate appropriate types of features of interest, provide a compact representation of those features, and effectively visualize the feature information locally. The system will have to overcome the challenges of loading a sufficient portion of the data set over a network connection into a desktop machine, mapping the entire data set to a visual representation, and rendering the results at interactive rates.&lt;br/&gt;&lt;br/&gt;This project will attack these CFD visualization problems by developing techniques for creating and using a procedural abstraction for a dataset. The major research objectives are to:&lt;br/&gt;1. Detect features (e.g. shocks) in complex flows using topological operators.&lt;br/&gt;2. Characterize the data relative to these features using a procedural representation consisting of implicit models and free-form deformations.&lt;br/&gt;3. Adapt the procedural representation to the appropriate level of detail using multi-resolution techniques.&lt;br/&gt;4. Encapsulate domain-specific knowledge as metadata to explore these extremely large datasets.&lt;br/&gt;5. Visualize the data directly from the procedural representation.&lt;br/&gt;6. Verify the accuracy of the procedural representation by tracking approximation error.&lt;br/&gt;7. Apply these techniques to the large-scale computational flow simulation problems currently studied at Stanford and Mississippi State University. &lt;br/&gt;The resulting system will allow CFD researchers to work more effectively by interactively exploring their data to pinpoint the features of interest. Moreover, the results of this project will provide solutions not only for CFD researchers, but also for a wide variety of other visualization challenges and applications. The project&apos;s main goal is to develop techniques that allow visualization exploration, feature detection, extraction, and analysis at a higher, more effective level through the use of procedural data abstraction and representation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121288</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121288</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n926" target="n1409">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Procedural Representation and Visualization Enabling Personalized Computational Fluid Dynamics</data>
      <data key="e_abstract">Computer power has increased dramatically over the past decade and has allowed computational fluid dynamics (CFD) researchers to more accurately simulate many types of complex flow. These simulations have enabled great leaps forward in the design and safety of ships, airplanes, automobiles, and other vehicles. However, this new power has also yielded terabytes of data, and CFD researchers now face a very difficult task in trying to find, extract, and analyze important flow features (e.g., time varying vortices, shock waves) buried within these monstrous datasets. Unlike the explosive growth in computer power, visualization tools for very large datasets have evolved modestly and cannot yet help with these tasks significantly. In particular, since detailed visualization of such large datasets is impractical, CFD researchers must work at a very cumbersome, low level to dice their datasets into workable pieces.&lt;br/&gt;&lt;br/&gt;CFD researchers desperately need new techniques that simplify and automate the iterative process of finding the appropriate portion of their data set. They need a system that will allow the user to articulate appropriate types of features of interest, provide a compact representation of those features, and effectively visualize the feature information locally. The system will have to overcome the challenges of loading a sufficient portion of the data set over a network connection into a desktop machine, mapping the entire data set to a visual representation, and rendering the results at interactive rates.&lt;br/&gt;&lt;br/&gt;This project will attack these CFD visualization problems by developing techniques for creating and using a procedural abstraction for a dataset. The major research objectives are to:&lt;br/&gt;1. Detect features (e.g. shocks) in complex flows using topological operators.&lt;br/&gt;2. Characterize the data relative to these features using a procedural representation consisting of implicit models and free-form deformations.&lt;br/&gt;3. Adapt the procedural representation to the appropriate level of detail using multi-resolution techniques.&lt;br/&gt;4. Encapsulate domain-specific knowledge as metadata to explore these extremely large datasets.&lt;br/&gt;5. Visualize the data directly from the procedural representation.&lt;br/&gt;6. Verify the accuracy of the procedural representation by tracking approximation error.&lt;br/&gt;7. Apply these techniques to the large-scale computational flow simulation problems currently studied at Stanford and Mississippi State University. &lt;br/&gt;The resulting system will allow CFD researchers to work more effectively by interactively exploring their data to pinpoint the features of interest. Moreover, the results of this project will provide solutions not only for CFD researchers, but also for a wide variety of other visualization challenges and applications. The project&apos;s main goal is to develop techniques that allow visualization exploration, feature detection, extraction, and analysis at a higher, more effective level through the use of procedural data abstraction and representation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121288</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121288</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n926" target="n1410">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Procedural Representation and Visualization Enabling Personalized Computational Fluid Dynamics</data>
      <data key="e_abstract">Computer power has increased dramatically over the past decade and has allowed computational fluid dynamics (CFD) researchers to more accurately simulate many types of complex flow. These simulations have enabled great leaps forward in the design and safety of ships, airplanes, automobiles, and other vehicles. However, this new power has also yielded terabytes of data, and CFD researchers now face a very difficult task in trying to find, extract, and analyze important flow features (e.g., time varying vortices, shock waves) buried within these monstrous datasets. Unlike the explosive growth in computer power, visualization tools for very large datasets have evolved modestly and cannot yet help with these tasks significantly. In particular, since detailed visualization of such large datasets is impractical, CFD researchers must work at a very cumbersome, low level to dice their datasets into workable pieces.&lt;br/&gt;&lt;br/&gt;CFD researchers desperately need new techniques that simplify and automate the iterative process of finding the appropriate portion of their data set. They need a system that will allow the user to articulate appropriate types of features of interest, provide a compact representation of those features, and effectively visualize the feature information locally. The system will have to overcome the challenges of loading a sufficient portion of the data set over a network connection into a desktop machine, mapping the entire data set to a visual representation, and rendering the results at interactive rates.&lt;br/&gt;&lt;br/&gt;This project will attack these CFD visualization problems by developing techniques for creating and using a procedural abstraction for a dataset. The major research objectives are to:&lt;br/&gt;1. Detect features (e.g. shocks) in complex flows using topological operators.&lt;br/&gt;2. Characterize the data relative to these features using a procedural representation consisting of implicit models and free-form deformations.&lt;br/&gt;3. Adapt the procedural representation to the appropriate level of detail using multi-resolution techniques.&lt;br/&gt;4. Encapsulate domain-specific knowledge as metadata to explore these extremely large datasets.&lt;br/&gt;5. Visualize the data directly from the procedural representation.&lt;br/&gt;6. Verify the accuracy of the procedural representation by tracking approximation error.&lt;br/&gt;7. Apply these techniques to the large-scale computational flow simulation problems currently studied at Stanford and Mississippi State University. &lt;br/&gt;The resulting system will allow CFD researchers to work more effectively by interactively exploring their data to pinpoint the features of interest. Moreover, the results of this project will provide solutions not only for CFD researchers, but also for a wide variety of other visualization challenges and applications. The project&apos;s main goal is to develop techniques that allow visualization exploration, feature detection, extraction, and analysis at a higher, more effective level through the use of procedural data abstraction and representation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121288</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121288</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n16" target="n1408">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Procedural Representation and Visualization Enabling Personalized Computational Fluid Dynamics</data>
      <data key="e_abstract">Computer power has increased dramatically over the past decade and has allowed computational fluid dynamics (CFD) researchers to more accurately simulate many types of complex flow. These simulations have enabled great leaps forward in the design and safety of ships, airplanes, automobiles, and other vehicles. However, this new power has also yielded terabytes of data, and CFD researchers now face a very difficult task in trying to find, extract, and analyze important flow features (e.g., time varying vortices, shock waves) buried within these monstrous datasets. Unlike the explosive growth in computer power, visualization tools for very large datasets have evolved modestly and cannot yet help with these tasks significantly. In particular, since detailed visualization of such large datasets is impractical, CFD researchers must work at a very cumbersome, low level to dice their datasets into workable pieces.&lt;br/&gt;&lt;br/&gt;CFD researchers desperately need new techniques that simplify and automate the iterative process of finding the appropriate portion of their data set. They need a system that will allow the user to articulate appropriate types of features of interest, provide a compact representation of those features, and effectively visualize the feature information locally. The system will have to overcome the challenges of loading a sufficient portion of the data set over a network connection into a desktop machine, mapping the entire data set to a visual representation, and rendering the results at interactive rates.&lt;br/&gt;&lt;br/&gt;This project will attack these CFD visualization problems by developing techniques for creating and using a procedural abstraction for a dataset. The major research objectives are to:&lt;br/&gt;1. Detect features (e.g. shocks) in complex flows using topological operators.&lt;br/&gt;2. Characterize the data relative to these features using a procedural representation consisting of implicit models and free-form deformations.&lt;br/&gt;3. Adapt the procedural representation to the appropriate level of detail using multi-resolution techniques.&lt;br/&gt;4. Encapsulate domain-specific knowledge as metadata to explore these extremely large datasets.&lt;br/&gt;5. Visualize the data directly from the procedural representation.&lt;br/&gt;6. Verify the accuracy of the procedural representation by tracking approximation error.&lt;br/&gt;7. Apply these techniques to the large-scale computational flow simulation problems currently studied at Stanford and Mississippi State University. &lt;br/&gt;The resulting system will allow CFD researchers to work more effectively by interactively exploring their data to pinpoint the features of interest. Moreover, the results of this project will provide solutions not only for CFD researchers, but also for a wide variety of other visualization challenges and applications. The project&apos;s main goal is to develop techniques that allow visualization exploration, feature detection, extraction, and analysis at a higher, more effective level through the use of procedural data abstraction and representation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121288</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121288</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n16" target="n1409">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Procedural Representation and Visualization Enabling Personalized Computational Fluid Dynamics</data>
      <data key="e_abstract">Computer power has increased dramatically over the past decade and has allowed computational fluid dynamics (CFD) researchers to more accurately simulate many types of complex flow. These simulations have enabled great leaps forward in the design and safety of ships, airplanes, automobiles, and other vehicles. However, this new power has also yielded terabytes of data, and CFD researchers now face a very difficult task in trying to find, extract, and analyze important flow features (e.g., time varying vortices, shock waves) buried within these monstrous datasets. Unlike the explosive growth in computer power, visualization tools for very large datasets have evolved modestly and cannot yet help with these tasks significantly. In particular, since detailed visualization of such large datasets is impractical, CFD researchers must work at a very cumbersome, low level to dice their datasets into workable pieces.&lt;br/&gt;&lt;br/&gt;CFD researchers desperately need new techniques that simplify and automate the iterative process of finding the appropriate portion of their data set. They need a system that will allow the user to articulate appropriate types of features of interest, provide a compact representation of those features, and effectively visualize the feature information locally. The system will have to overcome the challenges of loading a sufficient portion of the data set over a network connection into a desktop machine, mapping the entire data set to a visual representation, and rendering the results at interactive rates.&lt;br/&gt;&lt;br/&gt;This project will attack these CFD visualization problems by developing techniques for creating and using a procedural abstraction for a dataset. The major research objectives are to:&lt;br/&gt;1. Detect features (e.g. shocks) in complex flows using topological operators.&lt;br/&gt;2. Characterize the data relative to these features using a procedural representation consisting of implicit models and free-form deformations.&lt;br/&gt;3. Adapt the procedural representation to the appropriate level of detail using multi-resolution techniques.&lt;br/&gt;4. Encapsulate domain-specific knowledge as metadata to explore these extremely large datasets.&lt;br/&gt;5. Visualize the data directly from the procedural representation.&lt;br/&gt;6. Verify the accuracy of the procedural representation by tracking approximation error.&lt;br/&gt;7. Apply these techniques to the large-scale computational flow simulation problems currently studied at Stanford and Mississippi State University. &lt;br/&gt;The resulting system will allow CFD researchers to work more effectively by interactively exploring their data to pinpoint the features of interest. Moreover, the results of this project will provide solutions not only for CFD researchers, but also for a wide variety of other visualization challenges and applications. The project&apos;s main goal is to develop techniques that allow visualization exploration, feature detection, extraction, and analysis at a higher, more effective level through the use of procedural data abstraction and representation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121288</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121288</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n16" target="n1410">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Procedural Representation and Visualization Enabling Personalized Computational Fluid Dynamics</data>
      <data key="e_abstract">Computer power has increased dramatically over the past decade and has allowed computational fluid dynamics (CFD) researchers to more accurately simulate many types of complex flow. These simulations have enabled great leaps forward in the design and safety of ships, airplanes, automobiles, and other vehicles. However, this new power has also yielded terabytes of data, and CFD researchers now face a very difficult task in trying to find, extract, and analyze important flow features (e.g., time varying vortices, shock waves) buried within these monstrous datasets. Unlike the explosive growth in computer power, visualization tools for very large datasets have evolved modestly and cannot yet help with these tasks significantly. In particular, since detailed visualization of such large datasets is impractical, CFD researchers must work at a very cumbersome, low level to dice their datasets into workable pieces.&lt;br/&gt;&lt;br/&gt;CFD researchers desperately need new techniques that simplify and automate the iterative process of finding the appropriate portion of their data set. They need a system that will allow the user to articulate appropriate types of features of interest, provide a compact representation of those features, and effectively visualize the feature information locally. The system will have to overcome the challenges of loading a sufficient portion of the data set over a network connection into a desktop machine, mapping the entire data set to a visual representation, and rendering the results at interactive rates.&lt;br/&gt;&lt;br/&gt;This project will attack these CFD visualization problems by developing techniques for creating and using a procedural abstraction for a dataset. The major research objectives are to:&lt;br/&gt;1. Detect features (e.g. shocks) in complex flows using topological operators.&lt;br/&gt;2. Characterize the data relative to these features using a procedural representation consisting of implicit models and free-form deformations.&lt;br/&gt;3. Adapt the procedural representation to the appropriate level of detail using multi-resolution techniques.&lt;br/&gt;4. Encapsulate domain-specific knowledge as metadata to explore these extremely large datasets.&lt;br/&gt;5. Visualize the data directly from the procedural representation.&lt;br/&gt;6. Verify the accuracy of the procedural representation by tracking approximation error.&lt;br/&gt;7. Apply these techniques to the large-scale computational flow simulation problems currently studied at Stanford and Mississippi State University. &lt;br/&gt;The resulting system will allow CFD researchers to work more effectively by interactively exploring their data to pinpoint the features of interest. Moreover, the results of this project will provide solutions not only for CFD researchers, but also for a wide variety of other visualization challenges and applications. The project&apos;s main goal is to develop techniques that allow visualization exploration, feature detection, extraction, and analysis at a higher, more effective level through the use of procedural data abstraction and representation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121288</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121288</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1408" target="n1409">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Procedural Representation and Visualization Enabling Personalized Computational Fluid Dynamics</data>
      <data key="e_abstract">Computer power has increased dramatically over the past decade and has allowed computational fluid dynamics (CFD) researchers to more accurately simulate many types of complex flow. These simulations have enabled great leaps forward in the design and safety of ships, airplanes, automobiles, and other vehicles. However, this new power has also yielded terabytes of data, and CFD researchers now face a very difficult task in trying to find, extract, and analyze important flow features (e.g., time varying vortices, shock waves) buried within these monstrous datasets. Unlike the explosive growth in computer power, visualization tools for very large datasets have evolved modestly and cannot yet help with these tasks significantly. In particular, since detailed visualization of such large datasets is impractical, CFD researchers must work at a very cumbersome, low level to dice their datasets into workable pieces.&lt;br/&gt;&lt;br/&gt;CFD researchers desperately need new techniques that simplify and automate the iterative process of finding the appropriate portion of their data set. They need a system that will allow the user to articulate appropriate types of features of interest, provide a compact representation of those features, and effectively visualize the feature information locally. The system will have to overcome the challenges of loading a sufficient portion of the data set over a network connection into a desktop machine, mapping the entire data set to a visual representation, and rendering the results at interactive rates.&lt;br/&gt;&lt;br/&gt;This project will attack these CFD visualization problems by developing techniques for creating and using a procedural abstraction for a dataset. The major research objectives are to:&lt;br/&gt;1. Detect features (e.g. shocks) in complex flows using topological operators.&lt;br/&gt;2. Characterize the data relative to these features using a procedural representation consisting of implicit models and free-form deformations.&lt;br/&gt;3. Adapt the procedural representation to the appropriate level of detail using multi-resolution techniques.&lt;br/&gt;4. Encapsulate domain-specific knowledge as metadata to explore these extremely large datasets.&lt;br/&gt;5. Visualize the data directly from the procedural representation.&lt;br/&gt;6. Verify the accuracy of the procedural representation by tracking approximation error.&lt;br/&gt;7. Apply these techniques to the large-scale computational flow simulation problems currently studied at Stanford and Mississippi State University. &lt;br/&gt;The resulting system will allow CFD researchers to work more effectively by interactively exploring their data to pinpoint the features of interest. Moreover, the results of this project will provide solutions not only for CFD researchers, but also for a wide variety of other visualization challenges and applications. The project&apos;s main goal is to develop techniques that allow visualization exploration, feature detection, extraction, and analysis at a higher, more effective level through the use of procedural data abstraction and representation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121288</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121288</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1408" target="n1410">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Procedural Representation and Visualization Enabling Personalized Computational Fluid Dynamics</data>
      <data key="e_abstract">Computer power has increased dramatically over the past decade and has allowed computational fluid dynamics (CFD) researchers to more accurately simulate many types of complex flow. These simulations have enabled great leaps forward in the design and safety of ships, airplanes, automobiles, and other vehicles. However, this new power has also yielded terabytes of data, and CFD researchers now face a very difficult task in trying to find, extract, and analyze important flow features (e.g., time varying vortices, shock waves) buried within these monstrous datasets. Unlike the explosive growth in computer power, visualization tools for very large datasets have evolved modestly and cannot yet help with these tasks significantly. In particular, since detailed visualization of such large datasets is impractical, CFD researchers must work at a very cumbersome, low level to dice their datasets into workable pieces.&lt;br/&gt;&lt;br/&gt;CFD researchers desperately need new techniques that simplify and automate the iterative process of finding the appropriate portion of their data set. They need a system that will allow the user to articulate appropriate types of features of interest, provide a compact representation of those features, and effectively visualize the feature information locally. The system will have to overcome the challenges of loading a sufficient portion of the data set over a network connection into a desktop machine, mapping the entire data set to a visual representation, and rendering the results at interactive rates.&lt;br/&gt;&lt;br/&gt;This project will attack these CFD visualization problems by developing techniques for creating and using a procedural abstraction for a dataset. The major research objectives are to:&lt;br/&gt;1. Detect features (e.g. shocks) in complex flows using topological operators.&lt;br/&gt;2. Characterize the data relative to these features using a procedural representation consisting of implicit models and free-form deformations.&lt;br/&gt;3. Adapt the procedural representation to the appropriate level of detail using multi-resolution techniques.&lt;br/&gt;4. Encapsulate domain-specific knowledge as metadata to explore these extremely large datasets.&lt;br/&gt;5. Visualize the data directly from the procedural representation.&lt;br/&gt;6. Verify the accuracy of the procedural representation by tracking approximation error.&lt;br/&gt;7. Apply these techniques to the large-scale computational flow simulation problems currently studied at Stanford and Mississippi State University. &lt;br/&gt;The resulting system will allow CFD researchers to work more effectively by interactively exploring their data to pinpoint the features of interest. Moreover, the results of this project will provide solutions not only for CFD researchers, but also for a wide variety of other visualization challenges and applications. The project&apos;s main goal is to develop techniques that allow visualization exploration, feature detection, extraction, and analysis at a higher, more effective level through the use of procedural data abstraction and representation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121288</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121288</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1409" target="n1410">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/AP+IM: Procedural Representation and Visualization Enabling Personalized Computational Fluid Dynamics</data>
      <data key="e_abstract">Computer power has increased dramatically over the past decade and has allowed computational fluid dynamics (CFD) researchers to more accurately simulate many types of complex flow. These simulations have enabled great leaps forward in the design and safety of ships, airplanes, automobiles, and other vehicles. However, this new power has also yielded terabytes of data, and CFD researchers now face a very difficult task in trying to find, extract, and analyze important flow features (e.g., time varying vortices, shock waves) buried within these monstrous datasets. Unlike the explosive growth in computer power, visualization tools for very large datasets have evolved modestly and cannot yet help with these tasks significantly. In particular, since detailed visualization of such large datasets is impractical, CFD researchers must work at a very cumbersome, low level to dice their datasets into workable pieces.&lt;br/&gt;&lt;br/&gt;CFD researchers desperately need new techniques that simplify and automate the iterative process of finding the appropriate portion of their data set. They need a system that will allow the user to articulate appropriate types of features of interest, provide a compact representation of those features, and effectively visualize the feature information locally. The system will have to overcome the challenges of loading a sufficient portion of the data set over a network connection into a desktop machine, mapping the entire data set to a visual representation, and rendering the results at interactive rates.&lt;br/&gt;&lt;br/&gt;This project will attack these CFD visualization problems by developing techniques for creating and using a procedural abstraction for a dataset. The major research objectives are to:&lt;br/&gt;1. Detect features (e.g. shocks) in complex flows using topological operators.&lt;br/&gt;2. Characterize the data relative to these features using a procedural representation consisting of implicit models and free-form deformations.&lt;br/&gt;3. Adapt the procedural representation to the appropriate level of detail using multi-resolution techniques.&lt;br/&gt;4. Encapsulate domain-specific knowledge as metadata to explore these extremely large datasets.&lt;br/&gt;5. Visualize the data directly from the procedural representation.&lt;br/&gt;6. Verify the accuracy of the procedural representation by tracking approximation error.&lt;br/&gt;7. Apply these techniques to the large-scale computational flow simulation problems currently studied at Stanford and Mississippi State University. &lt;br/&gt;The resulting system will allow CFD researchers to work more effectively by interactively exploring their data to pinpoint the features of interest. Moreover, the results of this project will provide solutions not only for CFD researchers, but also for a wide variety of other visualization challenges and applications. The project&apos;s main goal is to develop techniques that allow visualization exploration, feature detection, extraction, and analysis at a higher, more effective level through the use of procedural data abstraction and representation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121288</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121288</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1412" target="n1413">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/SY: Communication and Data Sharing Services for Dynamic Distributed Systems</data>
      <data key="e_abstract">This project is developing and analyzing algorithms to solve problems of communication and data sharing in highly dynamic distributed environments such as found in networks of mobile and embedded devices. The term dynamic here encompasses many types of changes, including changing network topology, processor mobility, changing sets of participating client processes, a wide range of types of processor and network failures, and timing variations. The properties being studied include ordering and reliability guarantees for communication and coherence guarantees for data sharing. The algorithmic results are accompanied by lower bound and impossibility results, which describe inherent limitations on what problems can be solved, and at what cost. This is particulary important in the networks of embedded devices that need to operate subject to the resource constraints such limited battery power, storage capacity, communication bandwidth and computation power. &lt;br/&gt;&lt;br/&gt;The communication and data-sharing problems to be solved are viewed as high level global services, which span network locations. These services generally provide performance and fault-tolerance guarantees, conditioned on assumptions about the behavior of the environment and of the underlying network substrate. Traditionally, research on distributed services has emphasized specification and correctness, while research on distributed algorithms has emphasized complexity and performance. This project combines and synthesizes these two concerns: It yields algorithms that perform efficiently and degrade gracefully in dynamic distributed systems, and whose correctness, performance, and fault-tolerance guarantees are expressed by precisely-defined global services. Because the setting is so complex, the algorithms are also be very complex, which means that it it is necessary to decompose them into smaller, more manageable pieces. In this project, many of those smaller pieces are being viewed as lower-level, auxiliary global services. These auxiliary services provide lower-level communication and data-sharing capabilities, plus other capabilities such as failure detection, progress detection, consensus, group membership, leader election, reconfiguration, resource allocation, workload distribution, location determination, and routing. This work is being carried out in terms of a mathematical framework based on interacting state machines. The state machines include features to express issues of timing, continuous behavior, and probabilistic behavior. &lt;br/&gt;&lt;br/&gt;The theoretical work in this project is guided by the requirements of systems that include networks of mobile and embedded devices and examples chosen from several prototype applications, including distributed file management, information collection and dissemination, computer-supported cooperative work, distributed games, and multimedia transmission.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121277</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121277</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n141" target="n1415">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Routing, Wavelength Assignment, Dimensioning and Performance of Optical Networks with Multicast Service</data>
      <data key="e_abstract">Optical communication networks employing wavelength division multiplexing can support several&lt;br/&gt;communication sessions using a limited number of wavelength channels. The employment ofwave-&lt;br/&gt;length reuse in different parts of the network contributes to the increase in the number of sessions&lt;br/&gt;that can be supported. As such, these networks can provide a means to serve those applica-&lt;br/&gt;tions which individually or collectively require large amounts of bandwidth. An important class&lt;br/&gt;of services with such property is the class of multicast, or multipoint traffic, which requires the&lt;br/&gt;delivery of data from the source to a group of destinations. Although a few studies have addressed&lt;br/&gt;packet-switched multicast services in broadcast-and-select optical networks, the topics of routing,&lt;br/&gt;wavelength assignment, dimensioning, and performance of circuit-switched multicast services in&lt;br/&gt;wavelength routing optical networks have received little attention.&lt;br/&gt; This project deals with multicast services in optical networks, with and without wavelength&lt;br/&gt;converters. Such applications can be served using wavelength routing networks. The project has&lt;br/&gt;several objectives. First, we would like to develop multicast tree construction algorithms for optical&lt;br/&gt;networks which take into account the optical network constraints into account, such as thepower&lt;br/&gt;budget, wavelength collisions and wavelength continuity, aswell as the cost and type of splitters and&lt;br/&gt;wavelength converters. Optimal network provisioning and dimensioning under multicast service,&lt;br/&gt;given the traffic demands and cost constraints, is another objective. We also plan to investigate the&lt;br/&gt;possibility of formulating a joint problem for the optimal provisioning, and the optimal wavelength&lt;br/&gt;converter and splitter placement. It is also planned that routing, and wavelength selection algo-&lt;br/&gt;rithms will be developed for multicast services in optical networks. It is also planned to study the&lt;br/&gt;effect of these algorithms on the dimensioning and performance of the network. All this requires&lt;br/&gt;construction of accurate analytical models for the evaluation of call blocking probabilities under&lt;br/&gt;different routing and wavelength selection algorithms. As the efficient support of multicast traffic&lt;br/&gt;requires the branching of the traffic at several points in the network, these models are expected to&lt;br/&gt;be very involved, especially that several standard independence assumptions cannot be used in this&lt;br/&gt;case. In addition to general performance models, bounds on performance will be established, which&lt;br/&gt;will provide a guide to the provisioning and dimensioning of networks for multicast service. Such&lt;br/&gt;bounds will be established through the identification of extreme network topologies which serve to&lt;br/&gt;provide these bounds.&lt;br/&gt; The results of this work are expected to advance the understanding of the behavior of optical&lt;br/&gt;networks under futuristic, yet more involved, traffic conditions. At the same time, these results&lt;br/&gt;will contribute to the design and dimensioning of such networks.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">87746</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">87746</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n33" target="n97">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+SI - MavHome: Development of an Intelligent Home Environment</data>
      <data key="e_abstract">The MavHome project views a smart home as an intelligent agent, which is able to perceive its environment through the use of sensors and act upon the environment through the use of actuators. The home has overall goals, such as minimizing the cost of maintaining the home and maximizing the comfort and productivity of its inhabitants. In order to meet these goals, the house must be able to reason about and adapt to information using knowledge about databases, machine learning, multiagent systems, robotics, smart sensors, wireless mobile computing, and multimedia computing. Smart homes can potentially minimize home operating costs in a time when utilities fees are becoming prohibitive, and can assist individuals with disabilities to lead independent lives. Through the university visitor program, minority recruitment, course development, and dissemination of results, the project will impact research and education in the area of &quot;smart space technologies.&quot;</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121297</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121297</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n33" target="n34">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+SI - MavHome: Development of an Intelligent Home Environment</data>
      <data key="e_abstract">The MavHome project views a smart home as an intelligent agent, which is able to perceive its environment through the use of sensors and act upon the environment through the use of actuators. The home has overall goals, such as minimizing the cost of maintaining the home and maximizing the comfort and productivity of its inhabitants. In order to meet these goals, the house must be able to reason about and adapt to information using knowledge about databases, machine learning, multiagent systems, robotics, smart sensors, wireless mobile computing, and multimedia computing. Smart homes can potentially minimize home operating costs in a time when utilities fees are becoming prohibitive, and can assist individuals with disabilities to lead independent lives. Through the university visitor program, minority recruitment, course development, and dissemination of results, the project will impact research and education in the area of &quot;smart space technologies.&quot;</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121297</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121297</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n33" target="n35">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+SI - MavHome: Development of an Intelligent Home Environment</data>
      <data key="e_abstract">The MavHome project views a smart home as an intelligent agent, which is able to perceive its environment through the use of sensors and act upon the environment through the use of actuators. The home has overall goals, such as minimizing the cost of maintaining the home and maximizing the comfort and productivity of its inhabitants. In order to meet these goals, the house must be able to reason about and adapt to information using knowledge about databases, machine learning, multiagent systems, robotics, smart sensors, wireless mobile computing, and multimedia computing. Smart homes can potentially minimize home operating costs in a time when utilities fees are becoming prohibitive, and can assist individuals with disabilities to lead independent lives. Through the university visitor program, minority recruitment, course development, and dissemination of results, the project will impact research and education in the area of &quot;smart space technologies.&quot;</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121297</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121297</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n33" target="n1420">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+SI - MavHome: Development of an Intelligent Home Environment</data>
      <data key="e_abstract">The MavHome project views a smart home as an intelligent agent, which is able to perceive its environment through the use of sensors and act upon the environment through the use of actuators. The home has overall goals, such as minimizing the cost of maintaining the home and maximizing the comfort and productivity of its inhabitants. In order to meet these goals, the house must be able to reason about and adapt to information using knowledge about databases, machine learning, multiagent systems, robotics, smart sensors, wireless mobile computing, and multimedia computing. Smart homes can potentially minimize home operating costs in a time when utilities fees are becoming prohibitive, and can assist individuals with disabilities to lead independent lives. Through the university visitor program, minority recruitment, course development, and dissemination of results, the project will impact research and education in the area of &quot;smart space technologies.&quot;</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121297</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121297</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n34" target="n97">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+SI - MavHome: Development of an Intelligent Home Environment</data>
      <data key="e_abstract">The MavHome project views a smart home as an intelligent agent, which is able to perceive its environment through the use of sensors and act upon the environment through the use of actuators. The home has overall goals, such as minimizing the cost of maintaining the home and maximizing the comfort and productivity of its inhabitants. In order to meet these goals, the house must be able to reason about and adapt to information using knowledge about databases, machine learning, multiagent systems, robotics, smart sensors, wireless mobile computing, and multimedia computing. Smart homes can potentially minimize home operating costs in a time when utilities fees are becoming prohibitive, and can assist individuals with disabilities to lead independent lives. Through the university visitor program, minority recruitment, course development, and dissemination of results, the project will impact research and education in the area of &quot;smart space technologies.&quot;</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121297</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121297</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n35" target="n97">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+SI - MavHome: Development of an Intelligent Home Environment</data>
      <data key="e_abstract">The MavHome project views a smart home as an intelligent agent, which is able to perceive its environment through the use of sensors and act upon the environment through the use of actuators. The home has overall goals, such as minimizing the cost of maintaining the home and maximizing the comfort and productivity of its inhabitants. In order to meet these goals, the house must be able to reason about and adapt to information using knowledge about databases, machine learning, multiagent systems, robotics, smart sensors, wireless mobile computing, and multimedia computing. Smart homes can potentially minimize home operating costs in a time when utilities fees are becoming prohibitive, and can assist individuals with disabilities to lead independent lives. Through the university visitor program, minority recruitment, course development, and dissemination of results, the project will impact research and education in the area of &quot;smart space technologies.&quot;</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121297</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121297</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n97" target="n1420">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+SI - MavHome: Development of an Intelligent Home Environment</data>
      <data key="e_abstract">The MavHome project views a smart home as an intelligent agent, which is able to perceive its environment through the use of sensors and act upon the environment through the use of actuators. The home has overall goals, such as minimizing the cost of maintaining the home and maximizing the comfort and productivity of its inhabitants. In order to meet these goals, the house must be able to reason about and adapt to information using knowledge about databases, machine learning, multiagent systems, robotics, smart sensors, wireless mobile computing, and multimedia computing. Smart homes can potentially minimize home operating costs in a time when utilities fees are becoming prohibitive, and can assist individuals with disabilities to lead independent lives. Through the university visitor program, minority recruitment, course development, and dissemination of results, the project will impact research and education in the area of &quot;smart space technologies.&quot;</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121297</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121297</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n34" target="n35">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+SI - MavHome: Development of an Intelligent Home Environment</data>
      <data key="e_abstract">The MavHome project views a smart home as an intelligent agent, which is able to perceive its environment through the use of sensors and act upon the environment through the use of actuators. The home has overall goals, such as minimizing the cost of maintaining the home and maximizing the comfort and productivity of its inhabitants. In order to meet these goals, the house must be able to reason about and adapt to information using knowledge about databases, machine learning, multiagent systems, robotics, smart sensors, wireless mobile computing, and multimedia computing. Smart homes can potentially minimize home operating costs in a time when utilities fees are becoming prohibitive, and can assist individuals with disabilities to lead independent lives. Through the university visitor program, minority recruitment, course development, and dissemination of results, the project will impact research and education in the area of &quot;smart space technologies.&quot;</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121297</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121297</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n34" target="n1420">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+SI - MavHome: Development of an Intelligent Home Environment</data>
      <data key="e_abstract">The MavHome project views a smart home as an intelligent agent, which is able to perceive its environment through the use of sensors and act upon the environment through the use of actuators. The home has overall goals, such as minimizing the cost of maintaining the home and maximizing the comfort and productivity of its inhabitants. In order to meet these goals, the house must be able to reason about and adapt to information using knowledge about databases, machine learning, multiagent systems, robotics, smart sensors, wireless mobile computing, and multimedia computing. Smart homes can potentially minimize home operating costs in a time when utilities fees are becoming prohibitive, and can assist individuals with disabilities to lead independent lives. Through the university visitor program, minority recruitment, course development, and dissemination of results, the project will impact research and education in the area of &quot;smart space technologies.&quot;</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121297</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121297</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n35" target="n1420">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/IM+SI - MavHome: Development of an Intelligent Home Environment</data>
      <data key="e_abstract">The MavHome project views a smart home as an intelligent agent, which is able to perceive its environment through the use of sensors and act upon the environment through the use of actuators. The home has overall goals, such as minimizing the cost of maintaining the home and maximizing the comfort and productivity of its inhabitants. In order to meet these goals, the house must be able to reason about and adapt to information using knowledge about databases, machine learning, multiagent systems, robotics, smart sensors, wireless mobile computing, and multimedia computing. Smart homes can potentially minimize home operating costs in a time when utilities fees are becoming prohibitive, and can assist individuals with disabilities to lead independent lives. Through the university visitor program, minority recruitment, course development, and dissemination of results, the project will impact research and education in the area of &quot;smart space technologies.&quot;</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121297</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121297</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1421" target="n1422">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: Collaborative Research: Performance-Driven Adaptive Software Design and Control</data>
      <data key="e_abstract">EIA-0103688&lt;br/&gt;Purdue University&lt;br/&gt;John R. Rice&lt;br/&gt;&lt;br/&gt;The goal of this project is to develop the tools needed for performance-directed integrated design and control of complex applications running on distributed computational systems. Its target computational systems are complex, incorporating the difficult heterogeneity, latency, and adaptive properties of computational grids. Its target applications are at the cutting edge of computational science: very large, complex applications with adaptive characteristics that do not allow their optimal system configurations or computational requirements to be estimated prior to run time. Each application will be viewed as a composition of components, with a formal, high fidelity model of performance to be designed for each component. The approach is to use model-based adaptive run-time control, based on these composed performance models, to control the execution of the application to meet specified performance goals. The control strategy will make real-time changes to parameters that modify the behavior of both application and computational platform.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103688</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103688</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1426" target="n1427">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/Comp Bio: The Computer Science of Biologically Embedded Systems</data>
      <data key="e_abstract">EIA- 0113679&lt;br/&gt; Black, Michael&lt;br/&gt;Brown University&lt;br/&gt;&lt;br/&gt;ITR/SY: The Computer Science of Biologically-Embedded Systems&lt;br/&gt;&lt;br/&gt;Biologically-embedded systems that directly couple artificial computational devices with neural systems are emerging as a new area of information technology research. The physical structure and adaptability of the human brain make these biologically-embedded systems quite different from computational systems typically studied in Computer Science.&lt;br/&gt;&lt;br/&gt;Fundamentally, biologically-embedded systems must make inferences about the behavior of a biological system based on measurements of neural activity that are indirect, ambiguous, and uncertain. Moreover these systems must adapt to short- and long-term changes in neural activity of the brain. These problems are addressed by a multi-disciplinary team in the context of developing a robot arm that is controlled by simultaneous recordings from neurons in the motor cortex of awake behaving monkeys. The goal is to probabilistically model the behavior of these neurons as a function of arm motion and then reconstruct continuous arm trajectories based on the neural activity. To do so, the project will exploit mathematical and computational techniques from computer vision, image processing, and machine learning.&lt;br/&gt;&lt;br/&gt;This work will enhance scientific knowledge about how to design and build new types of hybrid human/computer systems, will explore new devices to assist the severely disabled, will address the information technology questions raised by these biologically-embedded systems, and will contribute to the understanding of neural coding.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113679</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113679</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1426" target="n1428">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/Comp Bio: The Computer Science of Biologically Embedded Systems</data>
      <data key="e_abstract">EIA- 0113679&lt;br/&gt; Black, Michael&lt;br/&gt;Brown University&lt;br/&gt;&lt;br/&gt;ITR/SY: The Computer Science of Biologically-Embedded Systems&lt;br/&gt;&lt;br/&gt;Biologically-embedded systems that directly couple artificial computational devices with neural systems are emerging as a new area of information technology research. The physical structure and adaptability of the human brain make these biologically-embedded systems quite different from computational systems typically studied in Computer Science.&lt;br/&gt;&lt;br/&gt;Fundamentally, biologically-embedded systems must make inferences about the behavior of a biological system based on measurements of neural activity that are indirect, ambiguous, and uncertain. Moreover these systems must adapt to short- and long-term changes in neural activity of the brain. These problems are addressed by a multi-disciplinary team in the context of developing a robot arm that is controlled by simultaneous recordings from neurons in the motor cortex of awake behaving monkeys. The goal is to probabilistically model the behavior of these neurons as a function of arm motion and then reconstruct continuous arm trajectories based on the neural activity. To do so, the project will exploit mathematical and computational techniques from computer vision, image processing, and machine learning.&lt;br/&gt;&lt;br/&gt;This work will enhance scientific knowledge about how to design and build new types of hybrid human/computer systems, will explore new devices to assist the severely disabled, will address the information technology questions raised by these biologically-embedded systems, and will contribute to the understanding of neural coding.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113679</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113679</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1427" target="n1428">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR/Comp Bio: The Computer Science of Biologically Embedded Systems</data>
      <data key="e_abstract">EIA- 0113679&lt;br/&gt; Black, Michael&lt;br/&gt;Brown University&lt;br/&gt;&lt;br/&gt;ITR/SY: The Computer Science of Biologically-Embedded Systems&lt;br/&gt;&lt;br/&gt;Biologically-embedded systems that directly couple artificial computational devices with neural systems are emerging as a new area of information technology research. The physical structure and adaptability of the human brain make these biologically-embedded systems quite different from computational systems typically studied in Computer Science.&lt;br/&gt;&lt;br/&gt;Fundamentally, biologically-embedded systems must make inferences about the behavior of a biological system based on measurements of neural activity that are indirect, ambiguous, and uncertain. Moreover these systems must adapt to short- and long-term changes in neural activity of the brain. These problems are addressed by a multi-disciplinary team in the context of developing a robot arm that is controlled by simultaneous recordings from neurons in the motor cortex of awake behaving monkeys. The goal is to probabilistically model the behavior of these neurons as a function of arm motion and then reconstruct continuous arm trajectories based on the neural activity. To do so, the project will exploit mathematical and computational techniques from computer vision, image processing, and machine learning.&lt;br/&gt;&lt;br/&gt;This work will enhance scientific knowledge about how to design and build new types of hybrid human/computer systems, will explore new devices to assist the severely disabled, will address the information technology questions raised by these biologically-embedded systems, and will contribute to the understanding of neural coding.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113679</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">113679</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1430" target="n1431">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">ITR: Architectural Level Software Metrics</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">1640</data>
      <data key="e_label">296082</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">296082</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1432" target="n1433">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Studies of Geometric Arrangements and their Algorithmic Applications</data>
      <data key="e_abstract">Computational geometry covers a wide range of applications, such as motion planning and computer graphics, &lt;br/&gt;and its contribution to their solution involves the use of sophisticated techniques drawn from many branches of mathematics and computer science. The investigators extensively study many basic and applied problems in the area, including motion planning, Voronoi diagrams, combinatorial and algebraic analysis of arrangements of curves and algebraic surfaces, graph drawing, randomized algorithms, and geometric optimization.&lt;br/&gt;&lt;br/&gt;A major portion of this research involves the study of arrangements of curves and surfaces. The significant progress made by the PI&apos;s on these problems during the past 15 years has opened up many new challenging research directions, including:&lt;br/&gt;Combinatorial and algorithmic problems related to substructures (lower envelopes, single cells, zones, levels, vertical decompositions) in arrangements of surfaces in higher dimensions. Related algorithms in real algebraic geometry for computing connected components, stratifications, the dimension and other&lt;br/&gt;topological parameters of real semi-algebraic sets. Graph drawing and other algorithmic, combinatorial, and topological problems involving planar arrangements of segments or curves. Applications of these results to numerous areas, including motion planning in robotics, rendering and modeling problems in computer graphics, generalized Voronoi diagrams and geometric optimization problems, including problems in metrology and facility location. &lt;br/&gt;&lt;br/&gt;An important feature of this research is the cross-fertilization between basic research in computational and combinatorial geometry and various application areas. Another theme is the strong connection between the combinatorial analysis of arrangements and the design of efficient algorithms for constructing and utilizing these structures. The efficiency of the algorithms often crucially depends on the size of the structure to be computed, and most of the work is devoted to bounding this quantity.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">98246</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98246</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1432" target="n1434">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Studies of Geometric Arrangements and their Algorithmic Applications</data>
      <data key="e_abstract">Computational geometry covers a wide range of applications, such as motion planning and computer graphics, &lt;br/&gt;and its contribution to their solution involves the use of sophisticated techniques drawn from many branches of mathematics and computer science. The investigators extensively study many basic and applied problems in the area, including motion planning, Voronoi diagrams, combinatorial and algebraic analysis of arrangements of curves and algebraic surfaces, graph drawing, randomized algorithms, and geometric optimization.&lt;br/&gt;&lt;br/&gt;A major portion of this research involves the study of arrangements of curves and surfaces. The significant progress made by the PI&apos;s on these problems during the past 15 years has opened up many new challenging research directions, including:&lt;br/&gt;Combinatorial and algorithmic problems related to substructures (lower envelopes, single cells, zones, levels, vertical decompositions) in arrangements of surfaces in higher dimensions. Related algorithms in real algebraic geometry for computing connected components, stratifications, the dimension and other&lt;br/&gt;topological parameters of real semi-algebraic sets. Graph drawing and other algorithmic, combinatorial, and topological problems involving planar arrangements of segments or curves. Applications of these results to numerous areas, including motion planning in robotics, rendering and modeling problems in computer graphics, generalized Voronoi diagrams and geometric optimization problems, including problems in metrology and facility location. &lt;br/&gt;&lt;br/&gt;An important feature of this research is the cross-fertilization between basic research in computational and combinatorial geometry and various application areas. Another theme is the strong connection between the combinatorial analysis of arrangements and the design of efficient algorithms for constructing and utilizing these structures. The efficiency of the algorithms often crucially depends on the size of the structure to be computed, and most of the work is devoted to bounding this quantity.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">98246</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98246</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1433" target="n1434">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Studies of Geometric Arrangements and their Algorithmic Applications</data>
      <data key="e_abstract">Computational geometry covers a wide range of applications, such as motion planning and computer graphics, &lt;br/&gt;and its contribution to their solution involves the use of sophisticated techniques drawn from many branches of mathematics and computer science. The investigators extensively study many basic and applied problems in the area, including motion planning, Voronoi diagrams, combinatorial and algebraic analysis of arrangements of curves and algebraic surfaces, graph drawing, randomized algorithms, and geometric optimization.&lt;br/&gt;&lt;br/&gt;A major portion of this research involves the study of arrangements of curves and surfaces. The significant progress made by the PI&apos;s on these problems during the past 15 years has opened up many new challenging research directions, including:&lt;br/&gt;Combinatorial and algorithmic problems related to substructures (lower envelopes, single cells, zones, levels, vertical decompositions) in arrangements of surfaces in higher dimensions. Related algorithms in real algebraic geometry for computing connected components, stratifications, the dimension and other&lt;br/&gt;topological parameters of real semi-algebraic sets. Graph drawing and other algorithmic, combinatorial, and topological problems involving planar arrangements of segments or curves. Applications of these results to numerous areas, including motion planning in robotics, rendering and modeling problems in computer graphics, generalized Voronoi diagrams and geometric optimization problems, including problems in metrology and facility location. &lt;br/&gt;&lt;br/&gt;An important feature of this research is the cross-fertilization between basic research in computational and combinatorial geometry and various application areas. Another theme is the strong connection between the combinatorial analysis of arrangements and the design of efficient algorithms for constructing and utilizing these structures. The efficiency of the algorithms often crucially depends on the size of the structure to be computed, and most of the work is devoted to bounding this quantity.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">98246</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98246</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n664" target="n1435">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: POWERful Software for Power Constrained Systems</data>
      <data key="e_abstract">EIA-0103583&lt;br/&gt;Anand Sivasubramaniam&lt;br/&gt;Pennsylvania State University&lt;br/&gt;&lt;br/&gt;NGS: Powerful Software for Power Constrained Systems&lt;br/&gt;&lt;br/&gt;The objective of this proposal is to develop new monitoring and simulation tools for resource monitoring at runtime, and compiler assisted optimization to enable execution under energy evaluation and optimization conditions. The proposal will implement these methods into an integrated framework to evaluate optimizations at the application, compiler and operating system levels. This project will specifically focus on a spatial database application called PocketGIS, an important mobile application, to explore these issues.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103583</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103583</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n531" target="n1435">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: POWERful Software for Power Constrained Systems</data>
      <data key="e_abstract">EIA-0103583&lt;br/&gt;Anand Sivasubramaniam&lt;br/&gt;Pennsylvania State University&lt;br/&gt;&lt;br/&gt;NGS: Powerful Software for Power Constrained Systems&lt;br/&gt;&lt;br/&gt;The objective of this proposal is to develop new monitoring and simulation tools for resource monitoring at runtime, and compiler assisted optimization to enable execution under energy evaluation and optimization conditions. The proposal will implement these methods into an integrated framework to evaluate optimizations at the application, compiler and operating system levels. This project will specifically focus on a spatial database application called PocketGIS, an important mobile application, to explore these issues.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103583</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103583</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n540" target="n1435">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: POWERful Software for Power Constrained Systems</data>
      <data key="e_abstract">EIA-0103583&lt;br/&gt;Anand Sivasubramaniam&lt;br/&gt;Pennsylvania State University&lt;br/&gt;&lt;br/&gt;NGS: Powerful Software for Power Constrained Systems&lt;br/&gt;&lt;br/&gt;The objective of this proposal is to develop new monitoring and simulation tools for resource monitoring at runtime, and compiler assisted optimization to enable execution under energy evaluation and optimization conditions. The proposal will implement these methods into an integrated framework to evaluate optimizations at the application, compiler and operating system levels. This project will specifically focus on a spatial database application called PocketGIS, an important mobile application, to explore these issues.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103583</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103583</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n531" target="n664">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: POWERful Software for Power Constrained Systems</data>
      <data key="e_abstract">EIA-0103583&lt;br/&gt;Anand Sivasubramaniam&lt;br/&gt;Pennsylvania State University&lt;br/&gt;&lt;br/&gt;NGS: Powerful Software for Power Constrained Systems&lt;br/&gt;&lt;br/&gt;The objective of this proposal is to develop new monitoring and simulation tools for resource monitoring at runtime, and compiler assisted optimization to enable execution under energy evaluation and optimization conditions. The proposal will implement these methods into an integrated framework to evaluate optimizations at the application, compiler and operating system levels. This project will specifically focus on a spatial database application called PocketGIS, an important mobile application, to explore these issues.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103583</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103583</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n540" target="n664">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: POWERful Software for Power Constrained Systems</data>
      <data key="e_abstract">EIA-0103583&lt;br/&gt;Anand Sivasubramaniam&lt;br/&gt;Pennsylvania State University&lt;br/&gt;&lt;br/&gt;NGS: Powerful Software for Power Constrained Systems&lt;br/&gt;&lt;br/&gt;The objective of this proposal is to develop new monitoring and simulation tools for resource monitoring at runtime, and compiler assisted optimization to enable execution under energy evaluation and optimization conditions. The proposal will implement these methods into an integrated framework to evaluate optimizations at the application, compiler and operating system levels. This project will specifically focus on a spatial database application called PocketGIS, an important mobile application, to explore these issues.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103583</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103583</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n531" target="n540">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: POWERful Software for Power Constrained Systems</data>
      <data key="e_abstract">EIA-0103583&lt;br/&gt;Anand Sivasubramaniam&lt;br/&gt;Pennsylvania State University&lt;br/&gt;&lt;br/&gt;NGS: Powerful Software for Power Constrained Systems&lt;br/&gt;&lt;br/&gt;The objective of this proposal is to develop new monitoring and simulation tools for resource monitoring at runtime, and compiler assisted optimization to enable execution under energy evaluation and optimization conditions. The proposal will implement these methods into an integrated framework to evaluate optimizations at the application, compiler and operating system levels. This project will specifically focus on a spatial database application called PocketGIS, an important mobile application, to explore these issues.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103583</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103583</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1439" target="n1440">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE - A Live Performance Simulation System: Virtual Vaudeville</data>
      <data key="e_abstract">This project brings together a diverse array of scholars from around&lt;br/&gt;the country, including computer scientists, 3D modellers and animators,&lt;br/&gt;theater practioners, and theater and music historians. The objective is&lt;br/&gt;to use digital technology to address a problem fundamental to performance&lt;br/&gt;scholarship and pedagogy: how to represent and communicate the phenomenon&lt;br/&gt;of live performance using media. This problem becomes especially pressing&lt;br/&gt;when the objective is to represent a performance tradition from the past.&lt;br/&gt;Neither a written description nor a filmed recreation is capable of conveying&lt;br/&gt;the experience of attending a live performance, an experience that encompasses&lt;br/&gt;not only the way the performance on stage looks and sounds from the perspective&lt;br/&gt;of spectators in different parts of the theatre, but also spectator&apos;s&lt;br/&gt;perceptions of and interactions with one another.&lt;br/&gt;&lt;br/&gt;Our proposed solution to this problem is to recreate historical performances&lt;br/&gt;in a virtual reality environment. The central objective is to simulate&lt;br/&gt;a feeling of &quot;liveness&quot; in this environment: the sensation of being&lt;br/&gt;surrounded by human activity onstage, in the audience and backstage,&lt;br/&gt;and the ability to choose where to look at any given time (onstage&lt;br/&gt;or off) and to move within the environment. With respect to the performers&lt;br/&gt;themselves, a critical concern is to find a way to bring the nuances&lt;br/&gt;of great stage performances into the virtual environment. To this end,&lt;br/&gt;we propose to use motion capture technology to capture real-world performances&lt;br/&gt;by professional, highly skilled actors, singers, dancers, acrobats&lt;br/&gt;and musicians.&lt;br/&gt;&lt;br/&gt;Key to our project is the depth of the proposed collaboration between&lt;br/&gt;technology, scholarship, pedagogy and art. This project is conceived&lt;br/&gt;to make a significant contribution to all four domains simultaneously,&lt;br/&gt;rather than merely using any one in the service of the others. The end&lt;br/&gt;result will represent an important advance in the design and implementation&lt;br/&gt;of virtual environments, building on recent successes in creating&lt;br/&gt;photo-realistic simulations of real 3D environments. The scale of this&lt;br/&gt;simulation, and in particular the complexity and precision of the character&lt;br/&gt;animation, pose an important technical challenge: how to integrate the complex&lt;br/&gt;pre-defined motion capture-generated animations of the onstage performances&lt;br/&gt;with the autonomous behaviors of characters in the audience and backstage.The&lt;br/&gt;project also constitute an invaluable work of applied scholarship,&lt;br/&gt;an unprecedented resource for visualizing past performances and testing&lt;br/&gt;hypotheses about historical performance practices. It will provide&lt;br/&gt;an unprecedented resource for students to engage with historical performance&lt;br/&gt;traditions as performance (and not as literature or film). Finally,&lt;br/&gt;from an artistic perspective, the Virtual Vaudeville project will test&lt;br/&gt;the potential of virtual environments to provide truly high-quality&lt;br/&gt;theater experiences to remote audiences.&lt;br/&gt;~</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121764</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121764</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1439" target="n1441">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE - A Live Performance Simulation System: Virtual Vaudeville</data>
      <data key="e_abstract">This project brings together a diverse array of scholars from around&lt;br/&gt;the country, including computer scientists, 3D modellers and animators,&lt;br/&gt;theater practioners, and theater and music historians. The objective is&lt;br/&gt;to use digital technology to address a problem fundamental to performance&lt;br/&gt;scholarship and pedagogy: how to represent and communicate the phenomenon&lt;br/&gt;of live performance using media. This problem becomes especially pressing&lt;br/&gt;when the objective is to represent a performance tradition from the past.&lt;br/&gt;Neither a written description nor a filmed recreation is capable of conveying&lt;br/&gt;the experience of attending a live performance, an experience that encompasses&lt;br/&gt;not only the way the performance on stage looks and sounds from the perspective&lt;br/&gt;of spectators in different parts of the theatre, but also spectator&apos;s&lt;br/&gt;perceptions of and interactions with one another.&lt;br/&gt;&lt;br/&gt;Our proposed solution to this problem is to recreate historical performances&lt;br/&gt;in a virtual reality environment. The central objective is to simulate&lt;br/&gt;a feeling of &quot;liveness&quot; in this environment: the sensation of being&lt;br/&gt;surrounded by human activity onstage, in the audience and backstage,&lt;br/&gt;and the ability to choose where to look at any given time (onstage&lt;br/&gt;or off) and to move within the environment. With respect to the performers&lt;br/&gt;themselves, a critical concern is to find a way to bring the nuances&lt;br/&gt;of great stage performances into the virtual environment. To this end,&lt;br/&gt;we propose to use motion capture technology to capture real-world performances&lt;br/&gt;by professional, highly skilled actors, singers, dancers, acrobats&lt;br/&gt;and musicians.&lt;br/&gt;&lt;br/&gt;Key to our project is the depth of the proposed collaboration between&lt;br/&gt;technology, scholarship, pedagogy and art. This project is conceived&lt;br/&gt;to make a significant contribution to all four domains simultaneously,&lt;br/&gt;rather than merely using any one in the service of the others. The end&lt;br/&gt;result will represent an important advance in the design and implementation&lt;br/&gt;of virtual environments, building on recent successes in creating&lt;br/&gt;photo-realistic simulations of real 3D environments. The scale of this&lt;br/&gt;simulation, and in particular the complexity and precision of the character&lt;br/&gt;animation, pose an important technical challenge: how to integrate the complex&lt;br/&gt;pre-defined motion capture-generated animations of the onstage performances&lt;br/&gt;with the autonomous behaviors of characters in the audience and backstage.The&lt;br/&gt;project also constitute an invaluable work of applied scholarship,&lt;br/&gt;an unprecedented resource for visualizing past performances and testing&lt;br/&gt;hypotheses about historical performance practices. It will provide&lt;br/&gt;an unprecedented resource for students to engage with historical performance&lt;br/&gt;traditions as performance (and not as literature or film). Finally,&lt;br/&gt;from an artistic perspective, the Virtual Vaudeville project will test&lt;br/&gt;the potential of virtual environments to provide truly high-quality&lt;br/&gt;theater experiences to remote audiences.&lt;br/&gt;~</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121764</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121764</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1439" target="n1442">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE - A Live Performance Simulation System: Virtual Vaudeville</data>
      <data key="e_abstract">This project brings together a diverse array of scholars from around&lt;br/&gt;the country, including computer scientists, 3D modellers and animators,&lt;br/&gt;theater practioners, and theater and music historians. The objective is&lt;br/&gt;to use digital technology to address a problem fundamental to performance&lt;br/&gt;scholarship and pedagogy: how to represent and communicate the phenomenon&lt;br/&gt;of live performance using media. This problem becomes especially pressing&lt;br/&gt;when the objective is to represent a performance tradition from the past.&lt;br/&gt;Neither a written description nor a filmed recreation is capable of conveying&lt;br/&gt;the experience of attending a live performance, an experience that encompasses&lt;br/&gt;not only the way the performance on stage looks and sounds from the perspective&lt;br/&gt;of spectators in different parts of the theatre, but also spectator&apos;s&lt;br/&gt;perceptions of and interactions with one another.&lt;br/&gt;&lt;br/&gt;Our proposed solution to this problem is to recreate historical performances&lt;br/&gt;in a virtual reality environment. The central objective is to simulate&lt;br/&gt;a feeling of &quot;liveness&quot; in this environment: the sensation of being&lt;br/&gt;surrounded by human activity onstage, in the audience and backstage,&lt;br/&gt;and the ability to choose where to look at any given time (onstage&lt;br/&gt;or off) and to move within the environment. With respect to the performers&lt;br/&gt;themselves, a critical concern is to find a way to bring the nuances&lt;br/&gt;of great stage performances into the virtual environment. To this end,&lt;br/&gt;we propose to use motion capture technology to capture real-world performances&lt;br/&gt;by professional, highly skilled actors, singers, dancers, acrobats&lt;br/&gt;and musicians.&lt;br/&gt;&lt;br/&gt;Key to our project is the depth of the proposed collaboration between&lt;br/&gt;technology, scholarship, pedagogy and art. This project is conceived&lt;br/&gt;to make a significant contribution to all four domains simultaneously,&lt;br/&gt;rather than merely using any one in the service of the others. The end&lt;br/&gt;result will represent an important advance in the design and implementation&lt;br/&gt;of virtual environments, building on recent successes in creating&lt;br/&gt;photo-realistic simulations of real 3D environments. The scale of this&lt;br/&gt;simulation, and in particular the complexity and precision of the character&lt;br/&gt;animation, pose an important technical challenge: how to integrate the complex&lt;br/&gt;pre-defined motion capture-generated animations of the onstage performances&lt;br/&gt;with the autonomous behaviors of characters in the audience and backstage.The&lt;br/&gt;project also constitute an invaluable work of applied scholarship,&lt;br/&gt;an unprecedented resource for visualizing past performances and testing&lt;br/&gt;hypotheses about historical performance practices. It will provide&lt;br/&gt;an unprecedented resource for students to engage with historical performance&lt;br/&gt;traditions as performance (and not as literature or film). Finally,&lt;br/&gt;from an artistic perspective, the Virtual Vaudeville project will test&lt;br/&gt;the potential of virtual environments to provide truly high-quality&lt;br/&gt;theater experiences to remote audiences.&lt;br/&gt;~</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121764</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121764</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1439" target="n1443">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE - A Live Performance Simulation System: Virtual Vaudeville</data>
      <data key="e_abstract">This project brings together a diverse array of scholars from around&lt;br/&gt;the country, including computer scientists, 3D modellers and animators,&lt;br/&gt;theater practioners, and theater and music historians. The objective is&lt;br/&gt;to use digital technology to address a problem fundamental to performance&lt;br/&gt;scholarship and pedagogy: how to represent and communicate the phenomenon&lt;br/&gt;of live performance using media. This problem becomes especially pressing&lt;br/&gt;when the objective is to represent a performance tradition from the past.&lt;br/&gt;Neither a written description nor a filmed recreation is capable of conveying&lt;br/&gt;the experience of attending a live performance, an experience that encompasses&lt;br/&gt;not only the way the performance on stage looks and sounds from the perspective&lt;br/&gt;of spectators in different parts of the theatre, but also spectator&apos;s&lt;br/&gt;perceptions of and interactions with one another.&lt;br/&gt;&lt;br/&gt;Our proposed solution to this problem is to recreate historical performances&lt;br/&gt;in a virtual reality environment. The central objective is to simulate&lt;br/&gt;a feeling of &quot;liveness&quot; in this environment: the sensation of being&lt;br/&gt;surrounded by human activity onstage, in the audience and backstage,&lt;br/&gt;and the ability to choose where to look at any given time (onstage&lt;br/&gt;or off) and to move within the environment. With respect to the performers&lt;br/&gt;themselves, a critical concern is to find a way to bring the nuances&lt;br/&gt;of great stage performances into the virtual environment. To this end,&lt;br/&gt;we propose to use motion capture technology to capture real-world performances&lt;br/&gt;by professional, highly skilled actors, singers, dancers, acrobats&lt;br/&gt;and musicians.&lt;br/&gt;&lt;br/&gt;Key to our project is the depth of the proposed collaboration between&lt;br/&gt;technology, scholarship, pedagogy and art. This project is conceived&lt;br/&gt;to make a significant contribution to all four domains simultaneously,&lt;br/&gt;rather than merely using any one in the service of the others. The end&lt;br/&gt;result will represent an important advance in the design and implementation&lt;br/&gt;of virtual environments, building on recent successes in creating&lt;br/&gt;photo-realistic simulations of real 3D environments. The scale of this&lt;br/&gt;simulation, and in particular the complexity and precision of the character&lt;br/&gt;animation, pose an important technical challenge: how to integrate the complex&lt;br/&gt;pre-defined motion capture-generated animations of the onstage performances&lt;br/&gt;with the autonomous behaviors of characters in the audience and backstage.The&lt;br/&gt;project also constitute an invaluable work of applied scholarship,&lt;br/&gt;an unprecedented resource for visualizing past performances and testing&lt;br/&gt;hypotheses about historical performance practices. It will provide&lt;br/&gt;an unprecedented resource for students to engage with historical performance&lt;br/&gt;traditions as performance (and not as literature or film). Finally,&lt;br/&gt;from an artistic perspective, the Virtual Vaudeville project will test&lt;br/&gt;the potential of virtual environments to provide truly high-quality&lt;br/&gt;theater experiences to remote audiences.&lt;br/&gt;~</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121764</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121764</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1440" target="n1441">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE - A Live Performance Simulation System: Virtual Vaudeville</data>
      <data key="e_abstract">This project brings together a diverse array of scholars from around&lt;br/&gt;the country, including computer scientists, 3D modellers and animators,&lt;br/&gt;theater practioners, and theater and music historians. The objective is&lt;br/&gt;to use digital technology to address a problem fundamental to performance&lt;br/&gt;scholarship and pedagogy: how to represent and communicate the phenomenon&lt;br/&gt;of live performance using media. This problem becomes especially pressing&lt;br/&gt;when the objective is to represent a performance tradition from the past.&lt;br/&gt;Neither a written description nor a filmed recreation is capable of conveying&lt;br/&gt;the experience of attending a live performance, an experience that encompasses&lt;br/&gt;not only the way the performance on stage looks and sounds from the perspective&lt;br/&gt;of spectators in different parts of the theatre, but also spectator&apos;s&lt;br/&gt;perceptions of and interactions with one another.&lt;br/&gt;&lt;br/&gt;Our proposed solution to this problem is to recreate historical performances&lt;br/&gt;in a virtual reality environment. The central objective is to simulate&lt;br/&gt;a feeling of &quot;liveness&quot; in this environment: the sensation of being&lt;br/&gt;surrounded by human activity onstage, in the audience and backstage,&lt;br/&gt;and the ability to choose where to look at any given time (onstage&lt;br/&gt;or off) and to move within the environment. With respect to the performers&lt;br/&gt;themselves, a critical concern is to find a way to bring the nuances&lt;br/&gt;of great stage performances into the virtual environment. To this end,&lt;br/&gt;we propose to use motion capture technology to capture real-world performances&lt;br/&gt;by professional, highly skilled actors, singers, dancers, acrobats&lt;br/&gt;and musicians.&lt;br/&gt;&lt;br/&gt;Key to our project is the depth of the proposed collaboration between&lt;br/&gt;technology, scholarship, pedagogy and art. This project is conceived&lt;br/&gt;to make a significant contribution to all four domains simultaneously,&lt;br/&gt;rather than merely using any one in the service of the others. The end&lt;br/&gt;result will represent an important advance in the design and implementation&lt;br/&gt;of virtual environments, building on recent successes in creating&lt;br/&gt;photo-realistic simulations of real 3D environments. The scale of this&lt;br/&gt;simulation, and in particular the complexity and precision of the character&lt;br/&gt;animation, pose an important technical challenge: how to integrate the complex&lt;br/&gt;pre-defined motion capture-generated animations of the onstage performances&lt;br/&gt;with the autonomous behaviors of characters in the audience and backstage.The&lt;br/&gt;project also constitute an invaluable work of applied scholarship,&lt;br/&gt;an unprecedented resource for visualizing past performances and testing&lt;br/&gt;hypotheses about historical performance practices. It will provide&lt;br/&gt;an unprecedented resource for students to engage with historical performance&lt;br/&gt;traditions as performance (and not as literature or film). Finally,&lt;br/&gt;from an artistic perspective, the Virtual Vaudeville project will test&lt;br/&gt;the potential of virtual environments to provide truly high-quality&lt;br/&gt;theater experiences to remote audiences.&lt;br/&gt;~</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121764</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121764</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1440" target="n1442">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE - A Live Performance Simulation System: Virtual Vaudeville</data>
      <data key="e_abstract">This project brings together a diverse array of scholars from around&lt;br/&gt;the country, including computer scientists, 3D modellers and animators,&lt;br/&gt;theater practioners, and theater and music historians. The objective is&lt;br/&gt;to use digital technology to address a problem fundamental to performance&lt;br/&gt;scholarship and pedagogy: how to represent and communicate the phenomenon&lt;br/&gt;of live performance using media. This problem becomes especially pressing&lt;br/&gt;when the objective is to represent a performance tradition from the past.&lt;br/&gt;Neither a written description nor a filmed recreation is capable of conveying&lt;br/&gt;the experience of attending a live performance, an experience that encompasses&lt;br/&gt;not only the way the performance on stage looks and sounds from the perspective&lt;br/&gt;of spectators in different parts of the theatre, but also spectator&apos;s&lt;br/&gt;perceptions of and interactions with one another.&lt;br/&gt;&lt;br/&gt;Our proposed solution to this problem is to recreate historical performances&lt;br/&gt;in a virtual reality environment. The central objective is to simulate&lt;br/&gt;a feeling of &quot;liveness&quot; in this environment: the sensation of being&lt;br/&gt;surrounded by human activity onstage, in the audience and backstage,&lt;br/&gt;and the ability to choose where to look at any given time (onstage&lt;br/&gt;or off) and to move within the environment. With respect to the performers&lt;br/&gt;themselves, a critical concern is to find a way to bring the nuances&lt;br/&gt;of great stage performances into the virtual environment. To this end,&lt;br/&gt;we propose to use motion capture technology to capture real-world performances&lt;br/&gt;by professional, highly skilled actors, singers, dancers, acrobats&lt;br/&gt;and musicians.&lt;br/&gt;&lt;br/&gt;Key to our project is the depth of the proposed collaboration between&lt;br/&gt;technology, scholarship, pedagogy and art. This project is conceived&lt;br/&gt;to make a significant contribution to all four domains simultaneously,&lt;br/&gt;rather than merely using any one in the service of the others. The end&lt;br/&gt;result will represent an important advance in the design and implementation&lt;br/&gt;of virtual environments, building on recent successes in creating&lt;br/&gt;photo-realistic simulations of real 3D environments. The scale of this&lt;br/&gt;simulation, and in particular the complexity and precision of the character&lt;br/&gt;animation, pose an important technical challenge: how to integrate the complex&lt;br/&gt;pre-defined motion capture-generated animations of the onstage performances&lt;br/&gt;with the autonomous behaviors of characters in the audience and backstage.The&lt;br/&gt;project also constitute an invaluable work of applied scholarship,&lt;br/&gt;an unprecedented resource for visualizing past performances and testing&lt;br/&gt;hypotheses about historical performance practices. It will provide&lt;br/&gt;an unprecedented resource for students to engage with historical performance&lt;br/&gt;traditions as performance (and not as literature or film). Finally,&lt;br/&gt;from an artistic perspective, the Virtual Vaudeville project will test&lt;br/&gt;the potential of virtual environments to provide truly high-quality&lt;br/&gt;theater experiences to remote audiences.&lt;br/&gt;~</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121764</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121764</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1440" target="n1443">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE - A Live Performance Simulation System: Virtual Vaudeville</data>
      <data key="e_abstract">This project brings together a diverse array of scholars from around&lt;br/&gt;the country, including computer scientists, 3D modellers and animators,&lt;br/&gt;theater practioners, and theater and music historians. The objective is&lt;br/&gt;to use digital technology to address a problem fundamental to performance&lt;br/&gt;scholarship and pedagogy: how to represent and communicate the phenomenon&lt;br/&gt;of live performance using media. This problem becomes especially pressing&lt;br/&gt;when the objective is to represent a performance tradition from the past.&lt;br/&gt;Neither a written description nor a filmed recreation is capable of conveying&lt;br/&gt;the experience of attending a live performance, an experience that encompasses&lt;br/&gt;not only the way the performance on stage looks and sounds from the perspective&lt;br/&gt;of spectators in different parts of the theatre, but also spectator&apos;s&lt;br/&gt;perceptions of and interactions with one another.&lt;br/&gt;&lt;br/&gt;Our proposed solution to this problem is to recreate historical performances&lt;br/&gt;in a virtual reality environment. The central objective is to simulate&lt;br/&gt;a feeling of &quot;liveness&quot; in this environment: the sensation of being&lt;br/&gt;surrounded by human activity onstage, in the audience and backstage,&lt;br/&gt;and the ability to choose where to look at any given time (onstage&lt;br/&gt;or off) and to move within the environment. With respect to the performers&lt;br/&gt;themselves, a critical concern is to find a way to bring the nuances&lt;br/&gt;of great stage performances into the virtual environment. To this end,&lt;br/&gt;we propose to use motion capture technology to capture real-world performances&lt;br/&gt;by professional, highly skilled actors, singers, dancers, acrobats&lt;br/&gt;and musicians.&lt;br/&gt;&lt;br/&gt;Key to our project is the depth of the proposed collaboration between&lt;br/&gt;technology, scholarship, pedagogy and art. This project is conceived&lt;br/&gt;to make a significant contribution to all four domains simultaneously,&lt;br/&gt;rather than merely using any one in the service of the others. The end&lt;br/&gt;result will represent an important advance in the design and implementation&lt;br/&gt;of virtual environments, building on recent successes in creating&lt;br/&gt;photo-realistic simulations of real 3D environments. The scale of this&lt;br/&gt;simulation, and in particular the complexity and precision of the character&lt;br/&gt;animation, pose an important technical challenge: how to integrate the complex&lt;br/&gt;pre-defined motion capture-generated animations of the onstage performances&lt;br/&gt;with the autonomous behaviors of characters in the audience and backstage.The&lt;br/&gt;project also constitute an invaluable work of applied scholarship,&lt;br/&gt;an unprecedented resource for visualizing past performances and testing&lt;br/&gt;hypotheses about historical performance practices. It will provide&lt;br/&gt;an unprecedented resource for students to engage with historical performance&lt;br/&gt;traditions as performance (and not as literature or film). Finally,&lt;br/&gt;from an artistic perspective, the Virtual Vaudeville project will test&lt;br/&gt;the potential of virtual environments to provide truly high-quality&lt;br/&gt;theater experiences to remote audiences.&lt;br/&gt;~</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121764</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121764</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1441" target="n1442">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE - A Live Performance Simulation System: Virtual Vaudeville</data>
      <data key="e_abstract">This project brings together a diverse array of scholars from around&lt;br/&gt;the country, including computer scientists, 3D modellers and animators,&lt;br/&gt;theater practioners, and theater and music historians. The objective is&lt;br/&gt;to use digital technology to address a problem fundamental to performance&lt;br/&gt;scholarship and pedagogy: how to represent and communicate the phenomenon&lt;br/&gt;of live performance using media. This problem becomes especially pressing&lt;br/&gt;when the objective is to represent a performance tradition from the past.&lt;br/&gt;Neither a written description nor a filmed recreation is capable of conveying&lt;br/&gt;the experience of attending a live performance, an experience that encompasses&lt;br/&gt;not only the way the performance on stage looks and sounds from the perspective&lt;br/&gt;of spectators in different parts of the theatre, but also spectator&apos;s&lt;br/&gt;perceptions of and interactions with one another.&lt;br/&gt;&lt;br/&gt;Our proposed solution to this problem is to recreate historical performances&lt;br/&gt;in a virtual reality environment. The central objective is to simulate&lt;br/&gt;a feeling of &quot;liveness&quot; in this environment: the sensation of being&lt;br/&gt;surrounded by human activity onstage, in the audience and backstage,&lt;br/&gt;and the ability to choose where to look at any given time (onstage&lt;br/&gt;or off) and to move within the environment. With respect to the performers&lt;br/&gt;themselves, a critical concern is to find a way to bring the nuances&lt;br/&gt;of great stage performances into the virtual environment. To this end,&lt;br/&gt;we propose to use motion capture technology to capture real-world performances&lt;br/&gt;by professional, highly skilled actors, singers, dancers, acrobats&lt;br/&gt;and musicians.&lt;br/&gt;&lt;br/&gt;Key to our project is the depth of the proposed collaboration between&lt;br/&gt;technology, scholarship, pedagogy and art. This project is conceived&lt;br/&gt;to make a significant contribution to all four domains simultaneously,&lt;br/&gt;rather than merely using any one in the service of the others. The end&lt;br/&gt;result will represent an important advance in the design and implementation&lt;br/&gt;of virtual environments, building on recent successes in creating&lt;br/&gt;photo-realistic simulations of real 3D environments. The scale of this&lt;br/&gt;simulation, and in particular the complexity and precision of the character&lt;br/&gt;animation, pose an important technical challenge: how to integrate the complex&lt;br/&gt;pre-defined motion capture-generated animations of the onstage performances&lt;br/&gt;with the autonomous behaviors of characters in the audience and backstage.The&lt;br/&gt;project also constitute an invaluable work of applied scholarship,&lt;br/&gt;an unprecedented resource for visualizing past performances and testing&lt;br/&gt;hypotheses about historical performance practices. It will provide&lt;br/&gt;an unprecedented resource for students to engage with historical performance&lt;br/&gt;traditions as performance (and not as literature or film). Finally,&lt;br/&gt;from an artistic perspective, the Virtual Vaudeville project will test&lt;br/&gt;the potential of virtual environments to provide truly high-quality&lt;br/&gt;theater experiences to remote audiences.&lt;br/&gt;~</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121764</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121764</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1441" target="n1443">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE - A Live Performance Simulation System: Virtual Vaudeville</data>
      <data key="e_abstract">This project brings together a diverse array of scholars from around&lt;br/&gt;the country, including computer scientists, 3D modellers and animators,&lt;br/&gt;theater practioners, and theater and music historians. The objective is&lt;br/&gt;to use digital technology to address a problem fundamental to performance&lt;br/&gt;scholarship and pedagogy: how to represent and communicate the phenomenon&lt;br/&gt;of live performance using media. This problem becomes especially pressing&lt;br/&gt;when the objective is to represent a performance tradition from the past.&lt;br/&gt;Neither a written description nor a filmed recreation is capable of conveying&lt;br/&gt;the experience of attending a live performance, an experience that encompasses&lt;br/&gt;not only the way the performance on stage looks and sounds from the perspective&lt;br/&gt;of spectators in different parts of the theatre, but also spectator&apos;s&lt;br/&gt;perceptions of and interactions with one another.&lt;br/&gt;&lt;br/&gt;Our proposed solution to this problem is to recreate historical performances&lt;br/&gt;in a virtual reality environment. The central objective is to simulate&lt;br/&gt;a feeling of &quot;liveness&quot; in this environment: the sensation of being&lt;br/&gt;surrounded by human activity onstage, in the audience and backstage,&lt;br/&gt;and the ability to choose where to look at any given time (onstage&lt;br/&gt;or off) and to move within the environment. With respect to the performers&lt;br/&gt;themselves, a critical concern is to find a way to bring the nuances&lt;br/&gt;of great stage performances into the virtual environment. To this end,&lt;br/&gt;we propose to use motion capture technology to capture real-world performances&lt;br/&gt;by professional, highly skilled actors, singers, dancers, acrobats&lt;br/&gt;and musicians.&lt;br/&gt;&lt;br/&gt;Key to our project is the depth of the proposed collaboration between&lt;br/&gt;technology, scholarship, pedagogy and art. This project is conceived&lt;br/&gt;to make a significant contribution to all four domains simultaneously,&lt;br/&gt;rather than merely using any one in the service of the others. The end&lt;br/&gt;result will represent an important advance in the design and implementation&lt;br/&gt;of virtual environments, building on recent successes in creating&lt;br/&gt;photo-realistic simulations of real 3D environments. The scale of this&lt;br/&gt;simulation, and in particular the complexity and precision of the character&lt;br/&gt;animation, pose an important technical challenge: how to integrate the complex&lt;br/&gt;pre-defined motion capture-generated animations of the onstage performances&lt;br/&gt;with the autonomous behaviors of characters in the audience and backstage.The&lt;br/&gt;project also constitute an invaluable work of applied scholarship,&lt;br/&gt;an unprecedented resource for visualizing past performances and testing&lt;br/&gt;hypotheses about historical performance practices. It will provide&lt;br/&gt;an unprecedented resource for students to engage with historical performance&lt;br/&gt;traditions as performance (and not as literature or film). Finally,&lt;br/&gt;from an artistic perspective, the Virtual Vaudeville project will test&lt;br/&gt;the potential of virtual environments to provide truly high-quality&lt;br/&gt;theater experiences to remote audiences.&lt;br/&gt;~</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121764</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121764</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1442" target="n1443">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE - A Live Performance Simulation System: Virtual Vaudeville</data>
      <data key="e_abstract">This project brings together a diverse array of scholars from around&lt;br/&gt;the country, including computer scientists, 3D modellers and animators,&lt;br/&gt;theater practioners, and theater and music historians. The objective is&lt;br/&gt;to use digital technology to address a problem fundamental to performance&lt;br/&gt;scholarship and pedagogy: how to represent and communicate the phenomenon&lt;br/&gt;of live performance using media. This problem becomes especially pressing&lt;br/&gt;when the objective is to represent a performance tradition from the past.&lt;br/&gt;Neither a written description nor a filmed recreation is capable of conveying&lt;br/&gt;the experience of attending a live performance, an experience that encompasses&lt;br/&gt;not only the way the performance on stage looks and sounds from the perspective&lt;br/&gt;of spectators in different parts of the theatre, but also spectator&apos;s&lt;br/&gt;perceptions of and interactions with one another.&lt;br/&gt;&lt;br/&gt;Our proposed solution to this problem is to recreate historical performances&lt;br/&gt;in a virtual reality environment. The central objective is to simulate&lt;br/&gt;a feeling of &quot;liveness&quot; in this environment: the sensation of being&lt;br/&gt;surrounded by human activity onstage, in the audience and backstage,&lt;br/&gt;and the ability to choose where to look at any given time (onstage&lt;br/&gt;or off) and to move within the environment. With respect to the performers&lt;br/&gt;themselves, a critical concern is to find a way to bring the nuances&lt;br/&gt;of great stage performances into the virtual environment. To this end,&lt;br/&gt;we propose to use motion capture technology to capture real-world performances&lt;br/&gt;by professional, highly skilled actors, singers, dancers, acrobats&lt;br/&gt;and musicians.&lt;br/&gt;&lt;br/&gt;Key to our project is the depth of the proposed collaboration between&lt;br/&gt;technology, scholarship, pedagogy and art. This project is conceived&lt;br/&gt;to make a significant contribution to all four domains simultaneously,&lt;br/&gt;rather than merely using any one in the service of the others. The end&lt;br/&gt;result will represent an important advance in the design and implementation&lt;br/&gt;of virtual environments, building on recent successes in creating&lt;br/&gt;photo-realistic simulations of real 3D environments. The scale of this&lt;br/&gt;simulation, and in particular the complexity and precision of the character&lt;br/&gt;animation, pose an important technical challenge: how to integrate the complex&lt;br/&gt;pre-defined motion capture-generated animations of the onstage performances&lt;br/&gt;with the autonomous behaviors of characters in the audience and backstage.The&lt;br/&gt;project also constitute an invaluable work of applied scholarship,&lt;br/&gt;an unprecedented resource for visualizing past performances and testing&lt;br/&gt;hypotheses about historical performance practices. It will provide&lt;br/&gt;an unprecedented resource for students to engage with historical performance&lt;br/&gt;traditions as performance (and not as literature or film). Finally,&lt;br/&gt;from an artistic perspective, the Virtual Vaudeville project will test&lt;br/&gt;the potential of virtual environments to provide truly high-quality&lt;br/&gt;theater experiences to remote audiences.&lt;br/&gt;~</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121764</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121764</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1444" target="n1445">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Latent Semantic Analysis: Theory and Technology</data>
      <data key="e_abstract">EIA-0121201 &lt;br/&gt;Kintsch, Walter&lt;br/&gt;University of Colorado Boulder&lt;br/&gt;&lt;br/&gt;ITR/PE: Latent Semantic Analysis: Theory and Technology&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This research program is designed to motivate, create and evaluate a new theory and methodology of learning, in which computer analysis of writing and speech is used to teach students to process and comprehend information more effectively. This work will inform educators on instructional and assessment strategies for improved reading comprehension to a degree not yet present in schools today. The methodology will teach students to read more effectively and to comprehend and learn more from what they read. Since the methodology can be applied to any text, it can be incorporated into reading programs to improve reading achievement, and be used as an effective tool to improve achievement in science and mathematics.&lt;br/&gt;&lt;br/&gt;The work is intended to achieve both theoretical and research breakthroughs in four areas: (1) extend LSA to incorporate syntactic as well as semantic information; (2) extend the power and scope of LSA by advancing its conceptual and mathematical framework to include other psychological process models that handle phenomena such as metaphor and causal inferences; and (3) extend LSA to process transcriptions of natural continuous speech, enabling LSA to be used to teach comprehension of both read and spoken text by children who cannot write or type well enough to produce textual responses; and (4) demonstrate that these theoretical advances improve comprehension of speech and text by students in different grades, ethic backgrounds, and in different subjects.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121201</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121201</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1444" target="n1446">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Latent Semantic Analysis: Theory and Technology</data>
      <data key="e_abstract">EIA-0121201 &lt;br/&gt;Kintsch, Walter&lt;br/&gt;University of Colorado Boulder&lt;br/&gt;&lt;br/&gt;ITR/PE: Latent Semantic Analysis: Theory and Technology&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This research program is designed to motivate, create and evaluate a new theory and methodology of learning, in which computer analysis of writing and speech is used to teach students to process and comprehend information more effectively. This work will inform educators on instructional and assessment strategies for improved reading comprehension to a degree not yet present in schools today. The methodology will teach students to read more effectively and to comprehend and learn more from what they read. Since the methodology can be applied to any text, it can be incorporated into reading programs to improve reading achievement, and be used as an effective tool to improve achievement in science and mathematics.&lt;br/&gt;&lt;br/&gt;The work is intended to achieve both theoretical and research breakthroughs in four areas: (1) extend LSA to incorporate syntactic as well as semantic information; (2) extend the power and scope of LSA by advancing its conceptual and mathematical framework to include other psychological process models that handle phenomena such as metaphor and causal inferences; and (3) extend LSA to process transcriptions of natural continuous speech, enabling LSA to be used to teach comprehension of both read and spoken text by children who cannot write or type well enough to produce textual responses; and (4) demonstrate that these theoretical advances improve comprehension of speech and text by students in different grades, ethic backgrounds, and in different subjects.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121201</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121201</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1444" target="n1447">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Latent Semantic Analysis: Theory and Technology</data>
      <data key="e_abstract">EIA-0121201 &lt;br/&gt;Kintsch, Walter&lt;br/&gt;University of Colorado Boulder&lt;br/&gt;&lt;br/&gt;ITR/PE: Latent Semantic Analysis: Theory and Technology&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This research program is designed to motivate, create and evaluate a new theory and methodology of learning, in which computer analysis of writing and speech is used to teach students to process and comprehend information more effectively. This work will inform educators on instructional and assessment strategies for improved reading comprehension to a degree not yet present in schools today. The methodology will teach students to read more effectively and to comprehend and learn more from what they read. Since the methodology can be applied to any text, it can be incorporated into reading programs to improve reading achievement, and be used as an effective tool to improve achievement in science and mathematics.&lt;br/&gt;&lt;br/&gt;The work is intended to achieve both theoretical and research breakthroughs in four areas: (1) extend LSA to incorporate syntactic as well as semantic information; (2) extend the power and scope of LSA by advancing its conceptual and mathematical framework to include other psychological process models that handle phenomena such as metaphor and causal inferences; and (3) extend LSA to process transcriptions of natural continuous speech, enabling LSA to be used to teach comprehension of both read and spoken text by children who cannot write or type well enough to produce textual responses; and (4) demonstrate that these theoretical advances improve comprehension of speech and text by students in different grades, ethic backgrounds, and in different subjects.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121201</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121201</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1445" target="n1446">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Latent Semantic Analysis: Theory and Technology</data>
      <data key="e_abstract">EIA-0121201 &lt;br/&gt;Kintsch, Walter&lt;br/&gt;University of Colorado Boulder&lt;br/&gt;&lt;br/&gt;ITR/PE: Latent Semantic Analysis: Theory and Technology&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This research program is designed to motivate, create and evaluate a new theory and methodology of learning, in which computer analysis of writing and speech is used to teach students to process and comprehend information more effectively. This work will inform educators on instructional and assessment strategies for improved reading comprehension to a degree not yet present in schools today. The methodology will teach students to read more effectively and to comprehend and learn more from what they read. Since the methodology can be applied to any text, it can be incorporated into reading programs to improve reading achievement, and be used as an effective tool to improve achievement in science and mathematics.&lt;br/&gt;&lt;br/&gt;The work is intended to achieve both theoretical and research breakthroughs in four areas: (1) extend LSA to incorporate syntactic as well as semantic information; (2) extend the power and scope of LSA by advancing its conceptual and mathematical framework to include other psychological process models that handle phenomena such as metaphor and causal inferences; and (3) extend LSA to process transcriptions of natural continuous speech, enabling LSA to be used to teach comprehension of both read and spoken text by children who cannot write or type well enough to produce textual responses; and (4) demonstrate that these theoretical advances improve comprehension of speech and text by students in different grades, ethic backgrounds, and in different subjects.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121201</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121201</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1445" target="n1447">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Latent Semantic Analysis: Theory and Technology</data>
      <data key="e_abstract">EIA-0121201 &lt;br/&gt;Kintsch, Walter&lt;br/&gt;University of Colorado Boulder&lt;br/&gt;&lt;br/&gt;ITR/PE: Latent Semantic Analysis: Theory and Technology&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This research program is designed to motivate, create and evaluate a new theory and methodology of learning, in which computer analysis of writing and speech is used to teach students to process and comprehend information more effectively. This work will inform educators on instructional and assessment strategies for improved reading comprehension to a degree not yet present in schools today. The methodology will teach students to read more effectively and to comprehend and learn more from what they read. Since the methodology can be applied to any text, it can be incorporated into reading programs to improve reading achievement, and be used as an effective tool to improve achievement in science and mathematics.&lt;br/&gt;&lt;br/&gt;The work is intended to achieve both theoretical and research breakthroughs in four areas: (1) extend LSA to incorporate syntactic as well as semantic information; (2) extend the power and scope of LSA by advancing its conceptual and mathematical framework to include other psychological process models that handle phenomena such as metaphor and causal inferences; and (3) extend LSA to process transcriptions of natural continuous speech, enabling LSA to be used to teach comprehension of both read and spoken text by children who cannot write or type well enough to produce textual responses; and (4) demonstrate that these theoretical advances improve comprehension of speech and text by students in different grades, ethic backgrounds, and in different subjects.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121201</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121201</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1446" target="n1447">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">ITR/PE: Latent Semantic Analysis: Theory and Technology</data>
      <data key="e_abstract">EIA-0121201 &lt;br/&gt;Kintsch, Walter&lt;br/&gt;University of Colorado Boulder&lt;br/&gt;&lt;br/&gt;ITR/PE: Latent Semantic Analysis: Theory and Technology&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This research program is designed to motivate, create and evaluate a new theory and methodology of learning, in which computer analysis of writing and speech is used to teach students to process and comprehend information more effectively. This work will inform educators on instructional and assessment strategies for improved reading comprehension to a degree not yet present in schools today. The methodology will teach students to read more effectively and to comprehend and learn more from what they read. Since the methodology can be applied to any text, it can be incorporated into reading programs to improve reading achievement, and be used as an effective tool to improve achievement in science and mathematics.&lt;br/&gt;&lt;br/&gt;The work is intended to achieve both theoretical and research breakthroughs in four areas: (1) extend LSA to incorporate syntactic as well as semantic information; (2) extend the power and scope of LSA by advancing its conceptual and mathematical framework to include other psychological process models that handle phenomena such as metaphor and causal inferences; and (3) extend LSA to process transcriptions of natural continuous speech, enabling LSA to be used to teach comprehension of both read and spoken text by children who cannot write or type well enough to produce textual responses; and (4) demonstrate that these theoretical advances improve comprehension of speech and text by students in different grades, ethic backgrounds, and in different subjects.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121201</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121201</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n175" target="n1450">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Anonymity: Securing User Privacy on the Internet</data>
      <data key="e_abstract">Today the World Wide Web is the foundation of e-commerce (including, e-advertisement, e-banking, etc.) Unfortunately, the World Wide Web does not offer good support for anonymity (called &quot;privacy&quot; in the popular press). While in a mechanical society window shoppers do not need to identify themselves, they implicitly or explicitly do so (to a certain extend) when using the World Wide Web. &quot;Privacy&quot; concerns may limit the growth and the use of the internet.&lt;br/&gt;&lt;br/&gt;The state of the art techniques to guarantee anonymity only provide weak security, or are too expensive to be used, or are completely impractical. Moreover, some protocols have been broken. The goals are:&lt;br/&gt;&lt;br/&gt;1. to revisit and to (crypt) analyze existing protocols,&lt;br/&gt;2. to study more efficient protocols to achieve robust anonymous communication,&lt;br/&gt;3. to develop protocols to guarantee anonymity even when insiders are trying to break the anonymity, or trying to disrupt the anonymous communication,&lt;br/&gt;4. to address the user-friendly aspect of cookies, without endangering the anonymity of the communication,&lt;br/&gt;5. to enhance very secure (called unconditionally secure) protocols for anonymous communication,&lt;br/&gt;6. to study limited anonymity.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">87641</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">87641</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1269" target="n1451">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">A Knowledge Discovery Framework for Civil Infrastructure Contexts</data>
      <data key="e_abstract">Proposal: CMS-9987871&lt;br/&gt;PI: James Garrett&lt;br/&gt;Institution: Carnegie Mellon University&lt;br/&gt;Date: April 20, 2001&lt;br/&gt;&lt;br/&gt;ABSTRACT CMS-9987871 &quot; A Knowledge Discovery Framework for Civil Infrastructure Contexts&quot; PI: Garrett, James, Carnegie Mellon University; Christos Faloutsos, Carnegie Mellon University; and Sue McNeil, University of Illinois at Chicago.&lt;br/&gt;&lt;br/&gt;The primary objective of this research is to specialize the abstract CRISP-DM (Cross-Industry Standard Process for Data Mining) process model being developed within the Knowledge Discovery in Databases (KDD) community, into a framework for use in civil infrastructure problem domains. With the recent accumulation of domain data, civil infrastructrue researchers have turned to data-intensive techniques to aid their understanding of deterioration mechanisms and usage patterns. During roughly the same time period, researchers in the machine learning, database, and statistical communities began to develop a set of new tools and techniques, known as CRISP-DM, to analyze very large databases. This framework will assist civil infrastructure researchers in systematically applying the CRISP-DM process for their data analysis needs. Such a framework will become vital for civil infrastructure researchers as they begin to analyze the enormous amounts of infrastructure data they have collected. The research team will identify the analyze preliminary case studies in civil infrastructure using the CRISP-DM process. Case studies will be chosen based on the uniqueness of their KDD problem characteristics. Special attention will be paid to those case studies that present challenging issues in data quality and data preparation, the most time-consuming and difficult stages of the process as well as the least-studied. The research team will classify civil infrastructure data analysis needs in terms of CRISP-DM process characteristics. All phases of the process will be addressed, but the data understanding (including data quality), data preparation, and modeling phases will be treated in-depth. The research team will then develop a more specific framework for applying the CRISP-DM process to civil infrastructure analysis needs. The research team will also identify and conduct case studies with which to validate the framework. Finally, the development and deployment of a web-based course and a web repository based on this research will be completed during the final year.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">9.98787e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98787e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1451" target="n1453">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">A Knowledge Discovery Framework for Civil Infrastructure Contexts</data>
      <data key="e_abstract">Proposal: CMS-9987871&lt;br/&gt;PI: James Garrett&lt;br/&gt;Institution: Carnegie Mellon University&lt;br/&gt;Date: April 20, 2001&lt;br/&gt;&lt;br/&gt;ABSTRACT CMS-9987871 &quot; A Knowledge Discovery Framework for Civil Infrastructure Contexts&quot; PI: Garrett, James, Carnegie Mellon University; Christos Faloutsos, Carnegie Mellon University; and Sue McNeil, University of Illinois at Chicago.&lt;br/&gt;&lt;br/&gt;The primary objective of this research is to specialize the abstract CRISP-DM (Cross-Industry Standard Process for Data Mining) process model being developed within the Knowledge Discovery in Databases (KDD) community, into a framework for use in civil infrastructure problem domains. With the recent accumulation of domain data, civil infrastructrue researchers have turned to data-intensive techniques to aid their understanding of deterioration mechanisms and usage patterns. During roughly the same time period, researchers in the machine learning, database, and statistical communities began to develop a set of new tools and techniques, known as CRISP-DM, to analyze very large databases. This framework will assist civil infrastructure researchers in systematically applying the CRISP-DM process for their data analysis needs. Such a framework will become vital for civil infrastructure researchers as they begin to analyze the enormous amounts of infrastructure data they have collected. The research team will identify the analyze preliminary case studies in civil infrastructure using the CRISP-DM process. Case studies will be chosen based on the uniqueness of their KDD problem characteristics. Special attention will be paid to those case studies that present challenging issues in data quality and data preparation, the most time-consuming and difficult stages of the process as well as the least-studied. The research team will classify civil infrastructure data analysis needs in terms of CRISP-DM process characteristics. All phases of the process will be addressed, but the data understanding (including data quality), data preparation, and modeling phases will be treated in-depth. The research team will then develop a more specific framework for applying the CRISP-DM process to civil infrastructure analysis needs. The research team will also identify and conduct case studies with which to validate the framework. Finally, the development and deployment of a web-based course and a web repository based on this research will be completed during the final year.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">9.98787e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98787e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1269" target="n1453">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">A Knowledge Discovery Framework for Civil Infrastructure Contexts</data>
      <data key="e_abstract">Proposal: CMS-9987871&lt;br/&gt;PI: James Garrett&lt;br/&gt;Institution: Carnegie Mellon University&lt;br/&gt;Date: April 20, 2001&lt;br/&gt;&lt;br/&gt;ABSTRACT CMS-9987871 &quot; A Knowledge Discovery Framework for Civil Infrastructure Contexts&quot; PI: Garrett, James, Carnegie Mellon University; Christos Faloutsos, Carnegie Mellon University; and Sue McNeil, University of Illinois at Chicago.&lt;br/&gt;&lt;br/&gt;The primary objective of this research is to specialize the abstract CRISP-DM (Cross-Industry Standard Process for Data Mining) process model being developed within the Knowledge Discovery in Databases (KDD) community, into a framework for use in civil infrastructure problem domains. With the recent accumulation of domain data, civil infrastructrue researchers have turned to data-intensive techniques to aid their understanding of deterioration mechanisms and usage patterns. During roughly the same time period, researchers in the machine learning, database, and statistical communities began to develop a set of new tools and techniques, known as CRISP-DM, to analyze very large databases. This framework will assist civil infrastructure researchers in systematically applying the CRISP-DM process for their data analysis needs. Such a framework will become vital for civil infrastructure researchers as they begin to analyze the enormous amounts of infrastructure data they have collected. The research team will identify the analyze preliminary case studies in civil infrastructure using the CRISP-DM process. Case studies will be chosen based on the uniqueness of their KDD problem characteristics. Special attention will be paid to those case studies that present challenging issues in data quality and data preparation, the most time-consuming and difficult stages of the process as well as the least-studied. The research team will classify civil infrastructure data analysis needs in terms of CRISP-DM process characteristics. All phases of the process will be addressed, but the data understanding (including data quality), data preparation, and modeling phases will be treated in-depth. The research team will then develop a more specific framework for applying the CRISP-DM process to civil infrastructure analysis needs. The research team will also identify and conduct case studies with which to validate the framework. Finally, the development and deployment of a web-based course and a web repository based on this research will be completed during the final year.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">9.98787e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98787e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n604" target="n1081">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">Collaborative Research: Performance-Driven Adaptive Software Design and Control</data>
      <data key="e_abstract">EIA-0127857&lt;br/&gt;Mary K. Vernon&lt;br/&gt;University of Wisconsin&lt;br/&gt;&lt;br/&gt;The goal of this project is to develop the tools needed for performance-directed integrated design and control of complex applications running on distributed computational systems. Its target computational systems are complex, incorporating the difficult heterogeneity, latency, and adaptive properties of computational grids. Its target applications are at the cutting edge of computational science: very large, complex applications with adaptive characteristics that do not allow their optimal system configurations or computational requirements to be estimated prior to run time. Each application will be viewed as a composition of components, with a formal, high fidelity model of performance to be designed for each component. The approach is to use model-based adaptive run-time control, based on these composed performance models, to control the execution of the application to meet specified performance goals. The control strategy will make real-time changes to parameters that modify the behavior of both application and computational platform.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">127857</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">127857</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n979" target="n980">
      <data key="e_effectiveDate">2001-09-01</data>
      <data key="e_title">Workshop on Ultra-Large Networks: Challenges and New Research Directions for Modeling and Simulation</data>
      <data key="e_abstract">We propose to hold a two-day workshop in Tucson, Arizona during fall 2001 on emerging ultra-large&lt;br/&gt;networks, such as the Internet. Hosted by Arizona Center for Integrative Modeling and Simulation, the&lt;br/&gt;meeting will bring together some of the world&apos;s leading researchers in the networking area to meet with&lt;br/&gt;counterparts with expertise in modeling and simulation of networks and of systems more generally. The&lt;br/&gt;workshop will provide these researchers with the task of coming up with the unknowns of ultra-large&lt;br/&gt;networks and with new directions of research in modeling and simulation that can address these unknowns.&lt;br/&gt; The Internet is increasing in connectivity, toward 1 billion nodes in 2005 and node capability providing a&lt;br/&gt;highly interconnected and computationally powerful medium. Such a globally and ubiquitously dispersed&lt;br/&gt;network will provide a new frontier for new kinds of educational, commerce and entertainment activities.&lt;br/&gt;However, there are many issues that arise in the emergence of such a large, highly decentralized, collection&lt;br/&gt;of interaction parts. The increased connectivity and capability creates new complexity and dynamics that&lt;br/&gt;we are only on the verge of appreciating. Difficulties in dealing with large-scale software systems are well&lt;br/&gt;documented in a recent report by the National Research Council. Techniques that work for small software&lt;br/&gt;systems fail markedly when the scale is increased by one million fold. Computer-based modeling and&lt;br/&gt;simulation (M&amp;S) methodology is required to address these issues since the scale is well beyond what&lt;br/&gt;analytical tools alone can handle and there is limited ability to do controlled experiments on the always&lt;br/&gt;on Internet. Traditional M&amp;S approaches have focussed on the micro-level components rather than the&lt;br/&gt;macro level integration of these components. However, with the advent of ultra-large scale systems such as&lt;br/&gt;the Internet of the future, it is necessary to develop M&amp;S approaches for understanding the behaviors of&lt;br/&gt;very large inter-connected networks with very few loci of control and many interacting and varied sources&lt;br/&gt;of input and services demand.&lt;br/&gt; The results of this workshop are expected to be a set of specific finding of gaps in our knowledge of the&lt;br/&gt;behavior of ultra-large networks and how to deal with their design, management, and control. Participants&lt;br/&gt;may assess whether current approaches can be evolved to deal with the large increases in scale or whether&lt;br/&gt;different, revolutionary paradigms are required. Participants will address the need for new techniques and&lt;br/&gt;approaches for building models of ultra-large networks and developing simulation environments for&lt;br/&gt;studying their behaviors. Suggestions for borrowing points of view form other areas such as complex&lt;br/&gt;adaptive systems and from basic theory of modeling and simulation will be encouraged. The proceedings&lt;br/&gt;will be compiled in a form that will provide a useable and significant guide for new NSF funding initiatives&lt;br/&gt;for future network infrastructure research.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">135530</data>
      <data key="e_expirationDate">2003-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">135530</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1460" target="n1461">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: A Compute-Intensive Sensor-Based Environment for Research in Computer Vision and Artificial Intelligence</data>
      <data key="e_abstract">EIA-0130768 &lt;br/&gt;Sudeep Sarkar&lt;br/&gt;University of South Florida&lt;br/&gt;&lt;br/&gt;CISE Research Resources: A Compute-Intensive Sensor-Based Environment for Research in Computer Vision and Artificial Intelligence&lt;br/&gt;&lt;br/&gt;Automated learning of grouping parameters for perceptual organization of complex images, modeling and reconstruction of elastic objects from image sequences, real-time matching of buyers and sellers for E-commerce, and learning models from extremely large databases, all require large data storage and a computing environment that supports exploring extremely large parameter spaces along with the ability to process huge quantities of data. A multiprocessor computing environment with substantial memory and disk storage is requested for high-performance computing associated with these four research projects in the general areas of computer vision and artificial intelligence. The compute server will increase the present capabilities by an order of magnitude.&lt;br/&gt;&lt;br/&gt;In addition, image acquisition devices, including high-resolution color cameras, digital video cameras, stereo cameras, and laser range scanners are requested for gathering color, motion, and range data. The ability to acquire fast range images and motion sequences will enable the consideration of the problem of integrating motion and range into the perceptual organization process. Also, the ability to acquire fast and high-resolution range, with registered color, will facilitate development of physics-based non-rigid algorithms and models that incorporate true material properties, which have, heretofore, not been possible.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130768</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130768</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1460" target="n1462">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: A Compute-Intensive Sensor-Based Environment for Research in Computer Vision and Artificial Intelligence</data>
      <data key="e_abstract">EIA-0130768 &lt;br/&gt;Sudeep Sarkar&lt;br/&gt;University of South Florida&lt;br/&gt;&lt;br/&gt;CISE Research Resources: A Compute-Intensive Sensor-Based Environment for Research in Computer Vision and Artificial Intelligence&lt;br/&gt;&lt;br/&gt;Automated learning of grouping parameters for perceptual organization of complex images, modeling and reconstruction of elastic objects from image sequences, real-time matching of buyers and sellers for E-commerce, and learning models from extremely large databases, all require large data storage and a computing environment that supports exploring extremely large parameter spaces along with the ability to process huge quantities of data. A multiprocessor computing environment with substantial memory and disk storage is requested for high-performance computing associated with these four research projects in the general areas of computer vision and artificial intelligence. The compute server will increase the present capabilities by an order of magnitude.&lt;br/&gt;&lt;br/&gt;In addition, image acquisition devices, including high-resolution color cameras, digital video cameras, stereo cameras, and laser range scanners are requested for gathering color, motion, and range data. The ability to acquire fast range images and motion sequences will enable the consideration of the problem of integrating motion and range into the perceptual organization process. Also, the ability to acquire fast and high-resolution range, with registered color, will facilitate development of physics-based non-rigid algorithms and models that incorporate true material properties, which have, heretofore, not been possible.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130768</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130768</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1460" target="n1463">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: A Compute-Intensive Sensor-Based Environment for Research in Computer Vision and Artificial Intelligence</data>
      <data key="e_abstract">EIA-0130768 &lt;br/&gt;Sudeep Sarkar&lt;br/&gt;University of South Florida&lt;br/&gt;&lt;br/&gt;CISE Research Resources: A Compute-Intensive Sensor-Based Environment for Research in Computer Vision and Artificial Intelligence&lt;br/&gt;&lt;br/&gt;Automated learning of grouping parameters for perceptual organization of complex images, modeling and reconstruction of elastic objects from image sequences, real-time matching of buyers and sellers for E-commerce, and learning models from extremely large databases, all require large data storage and a computing environment that supports exploring extremely large parameter spaces along with the ability to process huge quantities of data. A multiprocessor computing environment with substantial memory and disk storage is requested for high-performance computing associated with these four research projects in the general areas of computer vision and artificial intelligence. The compute server will increase the present capabilities by an order of magnitude.&lt;br/&gt;&lt;br/&gt;In addition, image acquisition devices, including high-resolution color cameras, digital video cameras, stereo cameras, and laser range scanners are requested for gathering color, motion, and range data. The ability to acquire fast range images and motion sequences will enable the consideration of the problem of integrating motion and range into the perceptual organization process. Also, the ability to acquire fast and high-resolution range, with registered color, will facilitate development of physics-based non-rigid algorithms and models that incorporate true material properties, which have, heretofore, not been possible.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130768</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130768</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1461" target="n1462">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: A Compute-Intensive Sensor-Based Environment for Research in Computer Vision and Artificial Intelligence</data>
      <data key="e_abstract">EIA-0130768 &lt;br/&gt;Sudeep Sarkar&lt;br/&gt;University of South Florida&lt;br/&gt;&lt;br/&gt;CISE Research Resources: A Compute-Intensive Sensor-Based Environment for Research in Computer Vision and Artificial Intelligence&lt;br/&gt;&lt;br/&gt;Automated learning of grouping parameters for perceptual organization of complex images, modeling and reconstruction of elastic objects from image sequences, real-time matching of buyers and sellers for E-commerce, and learning models from extremely large databases, all require large data storage and a computing environment that supports exploring extremely large parameter spaces along with the ability to process huge quantities of data. A multiprocessor computing environment with substantial memory and disk storage is requested for high-performance computing associated with these four research projects in the general areas of computer vision and artificial intelligence. The compute server will increase the present capabilities by an order of magnitude.&lt;br/&gt;&lt;br/&gt;In addition, image acquisition devices, including high-resolution color cameras, digital video cameras, stereo cameras, and laser range scanners are requested for gathering color, motion, and range data. The ability to acquire fast range images and motion sequences will enable the consideration of the problem of integrating motion and range into the perceptual organization process. Also, the ability to acquire fast and high-resolution range, with registered color, will facilitate development of physics-based non-rigid algorithms and models that incorporate true material properties, which have, heretofore, not been possible.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130768</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130768</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1461" target="n1463">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: A Compute-Intensive Sensor-Based Environment for Research in Computer Vision and Artificial Intelligence</data>
      <data key="e_abstract">EIA-0130768 &lt;br/&gt;Sudeep Sarkar&lt;br/&gt;University of South Florida&lt;br/&gt;&lt;br/&gt;CISE Research Resources: A Compute-Intensive Sensor-Based Environment for Research in Computer Vision and Artificial Intelligence&lt;br/&gt;&lt;br/&gt;Automated learning of grouping parameters for perceptual organization of complex images, modeling and reconstruction of elastic objects from image sequences, real-time matching of buyers and sellers for E-commerce, and learning models from extremely large databases, all require large data storage and a computing environment that supports exploring extremely large parameter spaces along with the ability to process huge quantities of data. A multiprocessor computing environment with substantial memory and disk storage is requested for high-performance computing associated with these four research projects in the general areas of computer vision and artificial intelligence. The compute server will increase the present capabilities by an order of magnitude.&lt;br/&gt;&lt;br/&gt;In addition, image acquisition devices, including high-resolution color cameras, digital video cameras, stereo cameras, and laser range scanners are requested for gathering color, motion, and range data. The ability to acquire fast range images and motion sequences will enable the consideration of the problem of integrating motion and range into the perceptual organization process. Also, the ability to acquire fast and high-resolution range, with registered color, will facilitate development of physics-based non-rigid algorithms and models that incorporate true material properties, which have, heretofore, not been possible.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130768</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130768</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1462" target="n1463">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">CISE Research Resources: A Compute-Intensive Sensor-Based Environment for Research in Computer Vision and Artificial Intelligence</data>
      <data key="e_abstract">EIA-0130768 &lt;br/&gt;Sudeep Sarkar&lt;br/&gt;University of South Florida&lt;br/&gt;&lt;br/&gt;CISE Research Resources: A Compute-Intensive Sensor-Based Environment for Research in Computer Vision and Artificial Intelligence&lt;br/&gt;&lt;br/&gt;Automated learning of grouping parameters for perceptual organization of complex images, modeling and reconstruction of elastic objects from image sequences, real-time matching of buyers and sellers for E-commerce, and learning models from extremely large databases, all require large data storage and a computing environment that supports exploring extremely large parameter spaces along with the ability to process huge quantities of data. A multiprocessor computing environment with substantial memory and disk storage is requested for high-performance computing associated with these four research projects in the general areas of computer vision and artificial intelligence. The compute server will increase the present capabilities by an order of magnitude.&lt;br/&gt;&lt;br/&gt;In addition, image acquisition devices, including high-resolution color cameras, digital video cameras, stereo cameras, and laser range scanners are requested for gathering color, motion, and range data. The ability to acquire fast range images and motion sequences will enable the consideration of the problem of integrating motion and range into the perceptual organization process. Also, the ability to acquire fast and high-resolution range, with registered color, will facilitate development of physics-based non-rigid algorithms and models that incorporate true material properties, which have, heretofore, not been possible.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">130768</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130768</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1465" target="n1466">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Microarray Experiment Management System</data>
      <data key="e_abstract">EIA-0103660&lt;br/&gt;Naren Ramakrishnan&lt;br/&gt;Virginia Polytechnic Institute&lt;br/&gt;&lt;br/&gt;NGS: A Microarray Experiment Management System&lt;br/&gt;&lt;br/&gt;The objective of this proposal is to develop systems software relating to the management of microarray experiments for studying hundreds of genes in given organism simultaneously. This is a multidisciplinary collaboration between computer scientists who will perform the computer science research part of this project and will develop the computer systems software, and biologist who will use the microarray experiment as a driving case for the systems software to be developed under the project, and then they will be the users of the developed management infrastructure.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103660</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103660</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1465" target="n1467">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Microarray Experiment Management System</data>
      <data key="e_abstract">EIA-0103660&lt;br/&gt;Naren Ramakrishnan&lt;br/&gt;Virginia Polytechnic Institute&lt;br/&gt;&lt;br/&gt;NGS: A Microarray Experiment Management System&lt;br/&gt;&lt;br/&gt;The objective of this proposal is to develop systems software relating to the management of microarray experiments for studying hundreds of genes in given organism simultaneously. This is a multidisciplinary collaboration between computer scientists who will perform the computer science research part of this project and will develop the computer systems software, and biologist who will use the microarray experiment as a driving case for the systems software to be developed under the project, and then they will be the users of the developed management infrastructure.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103660</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103660</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1465" target="n1468">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Microarray Experiment Management System</data>
      <data key="e_abstract">EIA-0103660&lt;br/&gt;Naren Ramakrishnan&lt;br/&gt;Virginia Polytechnic Institute&lt;br/&gt;&lt;br/&gt;NGS: A Microarray Experiment Management System&lt;br/&gt;&lt;br/&gt;The objective of this proposal is to develop systems software relating to the management of microarray experiments for studying hundreds of genes in given organism simultaneously. This is a multidisciplinary collaboration between computer scientists who will perform the computer science research part of this project and will develop the computer systems software, and biologist who will use the microarray experiment as a driving case for the systems software to be developed under the project, and then they will be the users of the developed management infrastructure.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103660</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103660</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1465" target="n1469">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Microarray Experiment Management System</data>
      <data key="e_abstract">EIA-0103660&lt;br/&gt;Naren Ramakrishnan&lt;br/&gt;Virginia Polytechnic Institute&lt;br/&gt;&lt;br/&gt;NGS: A Microarray Experiment Management System&lt;br/&gt;&lt;br/&gt;The objective of this proposal is to develop systems software relating to the management of microarray experiments for studying hundreds of genes in given organism simultaneously. This is a multidisciplinary collaboration between computer scientists who will perform the computer science research part of this project and will develop the computer systems software, and biologist who will use the microarray experiment as a driving case for the systems software to be developed under the project, and then they will be the users of the developed management infrastructure.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103660</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103660</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1466" target="n1467">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Microarray Experiment Management System</data>
      <data key="e_abstract">EIA-0103660&lt;br/&gt;Naren Ramakrishnan&lt;br/&gt;Virginia Polytechnic Institute&lt;br/&gt;&lt;br/&gt;NGS: A Microarray Experiment Management System&lt;br/&gt;&lt;br/&gt;The objective of this proposal is to develop systems software relating to the management of microarray experiments for studying hundreds of genes in given organism simultaneously. This is a multidisciplinary collaboration between computer scientists who will perform the computer science research part of this project and will develop the computer systems software, and biologist who will use the microarray experiment as a driving case for the systems software to be developed under the project, and then they will be the users of the developed management infrastructure.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103660</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103660</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1466" target="n1468">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Microarray Experiment Management System</data>
      <data key="e_abstract">EIA-0103660&lt;br/&gt;Naren Ramakrishnan&lt;br/&gt;Virginia Polytechnic Institute&lt;br/&gt;&lt;br/&gt;NGS: A Microarray Experiment Management System&lt;br/&gt;&lt;br/&gt;The objective of this proposal is to develop systems software relating to the management of microarray experiments for studying hundreds of genes in given organism simultaneously. This is a multidisciplinary collaboration between computer scientists who will perform the computer science research part of this project and will develop the computer systems software, and biologist who will use the microarray experiment as a driving case for the systems software to be developed under the project, and then they will be the users of the developed management infrastructure.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103660</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103660</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1466" target="n1469">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Microarray Experiment Management System</data>
      <data key="e_abstract">EIA-0103660&lt;br/&gt;Naren Ramakrishnan&lt;br/&gt;Virginia Polytechnic Institute&lt;br/&gt;&lt;br/&gt;NGS: A Microarray Experiment Management System&lt;br/&gt;&lt;br/&gt;The objective of this proposal is to develop systems software relating to the management of microarray experiments for studying hundreds of genes in given organism simultaneously. This is a multidisciplinary collaboration between computer scientists who will perform the computer science research part of this project and will develop the computer systems software, and biologist who will use the microarray experiment as a driving case for the systems software to be developed under the project, and then they will be the users of the developed management infrastructure.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103660</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103660</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1467" target="n1468">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Microarray Experiment Management System</data>
      <data key="e_abstract">EIA-0103660&lt;br/&gt;Naren Ramakrishnan&lt;br/&gt;Virginia Polytechnic Institute&lt;br/&gt;&lt;br/&gt;NGS: A Microarray Experiment Management System&lt;br/&gt;&lt;br/&gt;The objective of this proposal is to develop systems software relating to the management of microarray experiments for studying hundreds of genes in given organism simultaneously. This is a multidisciplinary collaboration between computer scientists who will perform the computer science research part of this project and will develop the computer systems software, and biologist who will use the microarray experiment as a driving case for the systems software to be developed under the project, and then they will be the users of the developed management infrastructure.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103660</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103660</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1467" target="n1469">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Microarray Experiment Management System</data>
      <data key="e_abstract">EIA-0103660&lt;br/&gt;Naren Ramakrishnan&lt;br/&gt;Virginia Polytechnic Institute&lt;br/&gt;&lt;br/&gt;NGS: A Microarray Experiment Management System&lt;br/&gt;&lt;br/&gt;The objective of this proposal is to develop systems software relating to the management of microarray experiments for studying hundreds of genes in given organism simultaneously. This is a multidisciplinary collaboration between computer scientists who will perform the computer science research part of this project and will develop the computer systems software, and biologist who will use the microarray experiment as a driving case for the systems software to be developed under the project, and then they will be the users of the developed management infrastructure.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103660</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103660</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1468" target="n1469">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: A Microarray Experiment Management System</data>
      <data key="e_abstract">EIA-0103660&lt;br/&gt;Naren Ramakrishnan&lt;br/&gt;Virginia Polytechnic Institute&lt;br/&gt;&lt;br/&gt;NGS: A Microarray Experiment Management System&lt;br/&gt;&lt;br/&gt;The objective of this proposal is to develop systems software relating to the management of microarray experiments for studying hundreds of genes in given organism simultaneously. This is a multidisciplinary collaboration between computer scientists who will perform the computer science research part of this project and will develop the computer systems software, and biologist who will use the microarray experiment as a driving case for the systems software to be developed under the project, and then they will be the users of the developed management infrastructure.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103660</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103660</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n408" target="n1470">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: Wisconsin DOVE: Distributed Optimizing Virtual Environment</data>
      <data key="e_abstract">EIA-0103670&lt;br/&gt;Rastislav Bodik&lt;br/&gt;University of Wisconsin&lt;br/&gt;&lt;br/&gt;NGS: Wisconsin DOVE: Distributed Optimizing Virtual Environment&lt;br/&gt;&lt;br/&gt;Wisconsin DOVE proposes to improve the performance of modern distributed and parallel applications as well as increase the productivity of application developers and system maintainers. Building on recent advances in distributed control, dynamic program optimization, and hardware-supported performance monitoring, our project will build a Distributed Optimizing Virtual Environment (DOVE) whose power will stem from two primary innovations.&lt;br/&gt;&lt;br/&gt;The VM-in-OS paradigm. Dove will implant into the operating system an optimizing virtual machine (VM), whose ability to analyze a running program and correlate the analysis with hardware-based performance monitoring will achieve vertical integration, spanning the application, the kernel, and the hardware. The VM-enabled operating system will be intimately aware of both the application above and the hardware and network below, and hence it will be able to schedule resources more intelligently and adaptively.&lt;br/&gt;&lt;br/&gt;A clan of optimizing virtual machines. Expecting that future distributed applications will be assembled from distributed components written in Java (or a similar mobile language), we propose to organize the VMs underlying the individual distributed components into a clan, in which the VMs exchange profiling and program-analysis information about their clients. By supporting &quot;gossip&quot; among the distributed VMs in a clan, we will be able to compute a run-time communication and dependence profile of the distributed application and, in response, perform a dynamic repartitioning of the application.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103670</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103670</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1223" target="n1470">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: Wisconsin DOVE: Distributed Optimizing Virtual Environment</data>
      <data key="e_abstract">EIA-0103670&lt;br/&gt;Rastislav Bodik&lt;br/&gt;University of Wisconsin&lt;br/&gt;&lt;br/&gt;NGS: Wisconsin DOVE: Distributed Optimizing Virtual Environment&lt;br/&gt;&lt;br/&gt;Wisconsin DOVE proposes to improve the performance of modern distributed and parallel applications as well as increase the productivity of application developers and system maintainers. Building on recent advances in distributed control, dynamic program optimization, and hardware-supported performance monitoring, our project will build a Distributed Optimizing Virtual Environment (DOVE) whose power will stem from two primary innovations.&lt;br/&gt;&lt;br/&gt;The VM-in-OS paradigm. Dove will implant into the operating system an optimizing virtual machine (VM), whose ability to analyze a running program and correlate the analysis with hardware-based performance monitoring will achieve vertical integration, spanning the application, the kernel, and the hardware. The VM-enabled operating system will be intimately aware of both the application above and the hardware and network below, and hence it will be able to schedule resources more intelligently and adaptively.&lt;br/&gt;&lt;br/&gt;A clan of optimizing virtual machines. Expecting that future distributed applications will be assembled from distributed components written in Java (or a similar mobile language), we propose to organize the VMs underlying the individual distributed components into a clan, in which the VMs exchange profiling and program-analysis information about their clients. By supporting &quot;gossip&quot; among the distributed VMs in a clan, we will be able to compute a run-time communication and dependence profile of the distributed application and, in response, perform a dynamic repartitioning of the application.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103670</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103670</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n408" target="n1223">
      <data key="e_effectiveDate">2001-09-15</data>
      <data key="e_title">NGS: Wisconsin DOVE: Distributed Optimizing Virtual Environment</data>
      <data key="e_abstract">EIA-0103670&lt;br/&gt;Rastislav Bodik&lt;br/&gt;University of Wisconsin&lt;br/&gt;&lt;br/&gt;NGS: Wisconsin DOVE: Distributed Optimizing Virtual Environment&lt;br/&gt;&lt;br/&gt;Wisconsin DOVE proposes to improve the performance of modern distributed and parallel applications as well as increase the productivity of application developers and system maintainers. Building on recent advances in distributed control, dynamic program optimization, and hardware-supported performance monitoring, our project will build a Distributed Optimizing Virtual Environment (DOVE) whose power will stem from two primary innovations.&lt;br/&gt;&lt;br/&gt;The VM-in-OS paradigm. Dove will implant into the operating system an optimizing virtual machine (VM), whose ability to analyze a running program and correlate the analysis with hardware-based performance monitoring will achieve vertical integration, spanning the application, the kernel, and the hardware. The VM-enabled operating system will be intimately aware of both the application above and the hardware and network below, and hence it will be able to schedule resources more intelligently and adaptively.&lt;br/&gt;&lt;br/&gt;A clan of optimizing virtual machines. Expecting that future distributed applications will be assembled from distributed components written in Java (or a similar mobile language), we propose to organize the VMs underlying the individual distributed components into a clan, in which the VMs exchange profiling and program-analysis information about their clients. By supporting &quot;gossip&quot; among the distributed VMs in a clan, we will be able to compute a run-time communication and dependence profile of the distributed application and, in response, perform a dynamic repartitioning of the application.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103670</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103670</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1477" target="n1478">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Computational Logic Tools for Research and Education</data>
      <data key="e_abstract">Formal logic is an extremely powerful and general conceptual tool that&lt;br/&gt;can be made even more useful if it can be manipulated computationally.&lt;br/&gt;Since computational logic is often difficult to implement and&lt;br/&gt;computationally expensive, progress on automating it has lagged&lt;br/&gt;somewhat. However, the problems and their solutions are becoming&lt;br/&gt;better understood and computational power has continued to advance&lt;br/&gt;exponentially, so the possibility of developing practical&lt;br/&gt;computational solutions to many interesting logic problems is now&lt;br/&gt;potentially solvable.&lt;br/&gt;&lt;br/&gt;There are three themes to the project: computational logic components,&lt;br/&gt;educational applications, and systems applications. The first theme&lt;br/&gt;encompasses both the theoretical research and implementation work&lt;br/&gt;necessary to develop practical software components for manipulating&lt;br/&gt;logical representations at several levels of abstraction, including&lt;br/&gt;decision procedures for fragments of first order logic and automated&lt;br/&gt;deduction for first-order logic. The second theme involves the use of&lt;br/&gt;computational logic for computer-assisted analysis of designs in three&lt;br/&gt;application areas of critical importance: networking, hybrid systems,&lt;br/&gt;and computer security. The third theme is the development and use in&lt;br/&gt;the classroom of instructional courseware for mathematical concepts&lt;br/&gt;relevant to computer science. This work is in collaboration with&lt;br/&gt;Stanford&apos;s Educational Program for Gifted Children (EPGY).&lt;br/&gt;&lt;br/&gt;The impact of this work follows from making computational logic&lt;br/&gt;accessible and available. Computational logic components will make it&lt;br/&gt;easier to construct sophisticated software applications, by providing&lt;br/&gt;powerful off-the-shelf building blocks for solving problems that would&lt;br/&gt;not otherwise be tractable. Improved analysis tools and techniques&lt;br/&gt;will result in superior system designs achieved faster and at lower&lt;br/&gt;cost. Instructional software will enhance the sophistication and&lt;br/&gt;problem-solving abilities of computer science students.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121403</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121403</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1477" target="n1479">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Computational Logic Tools for Research and Education</data>
      <data key="e_abstract">Formal logic is an extremely powerful and general conceptual tool that&lt;br/&gt;can be made even more useful if it can be manipulated computationally.&lt;br/&gt;Since computational logic is often difficult to implement and&lt;br/&gt;computationally expensive, progress on automating it has lagged&lt;br/&gt;somewhat. However, the problems and their solutions are becoming&lt;br/&gt;better understood and computational power has continued to advance&lt;br/&gt;exponentially, so the possibility of developing practical&lt;br/&gt;computational solutions to many interesting logic problems is now&lt;br/&gt;potentially solvable.&lt;br/&gt;&lt;br/&gt;There are three themes to the project: computational logic components,&lt;br/&gt;educational applications, and systems applications. The first theme&lt;br/&gt;encompasses both the theoretical research and implementation work&lt;br/&gt;necessary to develop practical software components for manipulating&lt;br/&gt;logical representations at several levels of abstraction, including&lt;br/&gt;decision procedures for fragments of first order logic and automated&lt;br/&gt;deduction for first-order logic. The second theme involves the use of&lt;br/&gt;computational logic for computer-assisted analysis of designs in three&lt;br/&gt;application areas of critical importance: networking, hybrid systems,&lt;br/&gt;and computer security. The third theme is the development and use in&lt;br/&gt;the classroom of instructional courseware for mathematical concepts&lt;br/&gt;relevant to computer science. This work is in collaboration with&lt;br/&gt;Stanford&apos;s Educational Program for Gifted Children (EPGY).&lt;br/&gt;&lt;br/&gt;The impact of this work follows from making computational logic&lt;br/&gt;accessible and available. Computational logic components will make it&lt;br/&gt;easier to construct sophisticated software applications, by providing&lt;br/&gt;powerful off-the-shelf building blocks for solving problems that would&lt;br/&gt;not otherwise be tractable. Improved analysis tools and techniques&lt;br/&gt;will result in superior system designs achieved faster and at lower&lt;br/&gt;cost. Instructional software will enhance the sophistication and&lt;br/&gt;problem-solving abilities of computer science students.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121403</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121403</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1478" target="n1479">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Computational Logic Tools for Research and Education</data>
      <data key="e_abstract">Formal logic is an extremely powerful and general conceptual tool that&lt;br/&gt;can be made even more useful if it can be manipulated computationally.&lt;br/&gt;Since computational logic is often difficult to implement and&lt;br/&gt;computationally expensive, progress on automating it has lagged&lt;br/&gt;somewhat. However, the problems and their solutions are becoming&lt;br/&gt;better understood and computational power has continued to advance&lt;br/&gt;exponentially, so the possibility of developing practical&lt;br/&gt;computational solutions to many interesting logic problems is now&lt;br/&gt;potentially solvable.&lt;br/&gt;&lt;br/&gt;There are three themes to the project: computational logic components,&lt;br/&gt;educational applications, and systems applications. The first theme&lt;br/&gt;encompasses both the theoretical research and implementation work&lt;br/&gt;necessary to develop practical software components for manipulating&lt;br/&gt;logical representations at several levels of abstraction, including&lt;br/&gt;decision procedures for fragments of first order logic and automated&lt;br/&gt;deduction for first-order logic. The second theme involves the use of&lt;br/&gt;computational logic for computer-assisted analysis of designs in three&lt;br/&gt;application areas of critical importance: networking, hybrid systems,&lt;br/&gt;and computer security. The third theme is the development and use in&lt;br/&gt;the classroom of instructional courseware for mathematical concepts&lt;br/&gt;relevant to computer science. This work is in collaboration with&lt;br/&gt;Stanford&apos;s Educational Program for Gifted Children (EPGY).&lt;br/&gt;&lt;br/&gt;The impact of this work follows from making computational logic&lt;br/&gt;accessible and available. Computational logic components will make it&lt;br/&gt;easier to construct sophisticated software applications, by providing&lt;br/&gt;powerful off-the-shelf building blocks for solving problems that would&lt;br/&gt;not otherwise be tractable. Improved analysis tools and techniques&lt;br/&gt;will result in superior system designs achieved faster and at lower&lt;br/&gt;cost. Instructional software will enhance the sophistication and&lt;br/&gt;problem-solving abilities of computer science students.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121403</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121403</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1260" target="n1481">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Collaborative Proposal-ITR/SY: Molecular Computation with Automated Microfluidic Sensors (MCAMS)</data>
      <data key="e_abstract">EIA-0121405&lt;br/&gt;Sohn, Lydia L&lt;br/&gt;Princeton University&lt;br/&gt;&lt;br/&gt;Collaborative Proposal-ITR/SY: Molecular Computation with Automated Microfluidic Sensors (MCAMS)&lt;br/&gt;&lt;br/&gt; The main objective of this cross-disciplinary project between Princeton University, Stanford University and UC Berkeley is to combine microfluidic technology with recently-developed algorithms of RNA-based computing to create a compact, automated nucelotide-based computational device capable of rapid detection of the computational output. Realization of such a device would greatly impact not only the field of molecular computing but also the general field of molecular biology, as the proposed platform technologies would provide novel, advanced tools for biological research.&lt;br/&gt;The project is addressing three main questions:&lt;br/&gt;&lt;br/&gt; - Can a microfluidic device be used to automate RNA-based computation?&lt;br/&gt; - Can alternative detection methods be used to avoid labor-intensive readout &lt;br/&gt; steps in RNA-based computation?&lt;br/&gt; -Can these detection methods be applied to other problems encountered in general biological v research?&lt;br/&gt;&lt;br/&gt; The success of this project will represent a leap forward in the direction of &quot;hands-free&quot; molecular computation. In addition, it will provide the necessary platform technologies to accelerate biological research, particularly in the areas of rapid DNA sequencing and fingerprinting.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121405</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121405</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1482" target="n1483">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Cooperative Computing for Distributed Embedded Systems</data>
      <data key="e_abstract">The objective of the proposed work is to develop an integrated computational-communication a chitecture&lt;br/&gt;for large scale netwo ks of embedded systems,called cooperative computin .Unlike distributed comput-&lt;br/&gt;ing,coope ative computing is pe formed ove a dynamic ad-hoc network with volatile nodes and connections,&lt;br/&gt;and expects statistically correct executions.E .ective esource management,in pa ticula power and energy&lt;br/&gt;management,will be c ucial since most devices in the netwo k will be mobile and will ely on batte y.The&lt;br/&gt;basic idea in coope ative computing is that nodes coope ate in pe forming a global task in an ad-hoc manne&lt;br/&gt;using thei local esou ces and state information to att act task &apos;s computation o communication.Caching,&lt;br/&gt;routing,topology discove y,o any speci .c use -de .ned application,can be implemented as a global task&lt;br/&gt;over the cooperative computing model.&lt;br/&gt; To effectively cooperate,the nodes that are involved need to know the esources and constraints they face&lt;br/&gt;in successfully accomplishing the ta get task.In such a power and esource constrained environment,the&lt;br/&gt;coope ating nodes must intelligently make the best decisions to ensu e the quality of esult (QoR).&lt;br/&gt;With these goals in mind,the p oposed esea ch is to develop a system a chitectu e to support coope ative&lt;br/&gt;computing ove dynamic netwo ks of embedded systems.Our resea ch will address the following issues:&lt;br/&gt;How to de .ne a global task at the application and ope ating system levels?&lt;br/&gt;How to prog am very large networks of embedded systems to execute a global task without handling&lt;br/&gt;each node individually?&lt;br/&gt;How to implement self-outing using peer-to-peer communication only?&lt;br/&gt;How to measure a pa tially successful execution?&lt;br/&gt;How to predict the powe dissipation and energy consumption of individual nodes,and nodes cooper-&lt;br/&gt;ating ac oss the network?&lt;br/&gt;How to optimize a task to reduce its powe and ene gy budget running on individual nodes,and across&lt;br/&gt;nodes in network?&lt;br/&gt; The system a chitectu e for coope ative computing is based on Smart Messages (SM),which can be&lt;br/&gt;viewed as intelligent ca ie s of data in a netwo k.Sma t Messages a e collections of code and mobile data&lt;br/&gt;that migrate th ough the netwo k,a single network hop at a time,executing at each step.Smart Messages&lt;br/&gt;a e self-outing,namely they a e esponsible fo dete mining thei own paths th ough the netwo k,utilizing a&lt;br/&gt;minimal set of facilities p ovided by nodes in the netwo k.An SM may be denied a equest to oute th ough&lt;br/&gt;a node due to the node &apos;s esou ce limitations,in pa ticula its emaining ene gy level.We conjectu e that&lt;br/&gt;Smart Messages p ovide a .exible support for a wide variety of applications,anging from data collection&lt;br/&gt;and dissemination,content-based outing and object t acking,to more t aditional dist ibuted computing&lt;br/&gt;applications in which execution of a task is sp ead across a collection of devices.&lt;br/&gt; As pa t of this p oposal we plan to implement and extensively evaluate seve al p ototype implementations&lt;br/&gt;of the Smart Message a chitectu e.Ou testbed will consist of networks of embedded multicontrollers and&lt;br/&gt;PDAs based on Bluetooth sho t-range wi eless communication.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121416</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121416</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n222" target="n1482">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Cooperative Computing for Distributed Embedded Systems</data>
      <data key="e_abstract">The objective of the proposed work is to develop an integrated computational-communication a chitecture&lt;br/&gt;for large scale netwo ks of embedded systems,called cooperative computin .Unlike distributed comput-&lt;br/&gt;ing,coope ative computing is pe formed ove a dynamic ad-hoc network with volatile nodes and connections,&lt;br/&gt;and expects statistically correct executions.E .ective esource management,in pa ticula power and energy&lt;br/&gt;management,will be c ucial since most devices in the netwo k will be mobile and will ely on batte y.The&lt;br/&gt;basic idea in coope ative computing is that nodes coope ate in pe forming a global task in an ad-hoc manne&lt;br/&gt;using thei local esou ces and state information to att act task &apos;s computation o communication.Caching,&lt;br/&gt;routing,topology discove y,o any speci .c use -de .ned application,can be implemented as a global task&lt;br/&gt;over the cooperative computing model.&lt;br/&gt; To effectively cooperate,the nodes that are involved need to know the esources and constraints they face&lt;br/&gt;in successfully accomplishing the ta get task.In such a power and esource constrained environment,the&lt;br/&gt;coope ating nodes must intelligently make the best decisions to ensu e the quality of esult (QoR).&lt;br/&gt;With these goals in mind,the p oposed esea ch is to develop a system a chitectu e to support coope ative&lt;br/&gt;computing ove dynamic netwo ks of embedded systems.Our resea ch will address the following issues:&lt;br/&gt;How to de .ne a global task at the application and ope ating system levels?&lt;br/&gt;How to prog am very large networks of embedded systems to execute a global task without handling&lt;br/&gt;each node individually?&lt;br/&gt;How to implement self-outing using peer-to-peer communication only?&lt;br/&gt;How to measure a pa tially successful execution?&lt;br/&gt;How to predict the powe dissipation and energy consumption of individual nodes,and nodes cooper-&lt;br/&gt;ating ac oss the network?&lt;br/&gt;How to optimize a task to reduce its powe and ene gy budget running on individual nodes,and across&lt;br/&gt;nodes in network?&lt;br/&gt; The system a chitectu e for coope ative computing is based on Smart Messages (SM),which can be&lt;br/&gt;viewed as intelligent ca ie s of data in a netwo k.Sma t Messages a e collections of code and mobile data&lt;br/&gt;that migrate th ough the netwo k,a single network hop at a time,executing at each step.Smart Messages&lt;br/&gt;a e self-outing,namely they a e esponsible fo dete mining thei own paths th ough the netwo k,utilizing a&lt;br/&gt;minimal set of facilities p ovided by nodes in the netwo k.An SM may be denied a equest to oute th ough&lt;br/&gt;a node due to the node &apos;s esou ce limitations,in pa ticula its emaining ene gy level.We conjectu e that&lt;br/&gt;Smart Messages p ovide a .exible support for a wide variety of applications,anging from data collection&lt;br/&gt;and dissemination,content-based outing and object t acking,to more t aditional dist ibuted computing&lt;br/&gt;applications in which execution of a task is sp ead across a collection of devices.&lt;br/&gt; As pa t of this p oposal we plan to implement and extensively evaluate seve al p ototype implementations&lt;br/&gt;of the Smart Message a chitectu e.Ou testbed will consist of networks of embedded multicontrollers and&lt;br/&gt;PDAs based on Bluetooth sho t-range wi eless communication.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121416</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121416</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n222" target="n1483">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Cooperative Computing for Distributed Embedded Systems</data>
      <data key="e_abstract">The objective of the proposed work is to develop an integrated computational-communication a chitecture&lt;br/&gt;for large scale netwo ks of embedded systems,called cooperative computin .Unlike distributed comput-&lt;br/&gt;ing,coope ative computing is pe formed ove a dynamic ad-hoc network with volatile nodes and connections,&lt;br/&gt;and expects statistically correct executions.E .ective esource management,in pa ticula power and energy&lt;br/&gt;management,will be c ucial since most devices in the netwo k will be mobile and will ely on batte y.The&lt;br/&gt;basic idea in coope ative computing is that nodes coope ate in pe forming a global task in an ad-hoc manne&lt;br/&gt;using thei local esou ces and state information to att act task &apos;s computation o communication.Caching,&lt;br/&gt;routing,topology discove y,o any speci .c use -de .ned application,can be implemented as a global task&lt;br/&gt;over the cooperative computing model.&lt;br/&gt; To effectively cooperate,the nodes that are involved need to know the esources and constraints they face&lt;br/&gt;in successfully accomplishing the ta get task.In such a power and esource constrained environment,the&lt;br/&gt;coope ating nodes must intelligently make the best decisions to ensu e the quality of esult (QoR).&lt;br/&gt;With these goals in mind,the p oposed esea ch is to develop a system a chitectu e to support coope ative&lt;br/&gt;computing ove dynamic netwo ks of embedded systems.Our resea ch will address the following issues:&lt;br/&gt;How to de .ne a global task at the application and ope ating system levels?&lt;br/&gt;How to prog am very large networks of embedded systems to execute a global task without handling&lt;br/&gt;each node individually?&lt;br/&gt;How to implement self-outing using peer-to-peer communication only?&lt;br/&gt;How to measure a pa tially successful execution?&lt;br/&gt;How to predict the powe dissipation and energy consumption of individual nodes,and nodes cooper-&lt;br/&gt;ating ac oss the network?&lt;br/&gt;How to optimize a task to reduce its powe and ene gy budget running on individual nodes,and across&lt;br/&gt;nodes in network?&lt;br/&gt; The system a chitectu e for coope ative computing is based on Smart Messages (SM),which can be&lt;br/&gt;viewed as intelligent ca ie s of data in a netwo k.Sma t Messages a e collections of code and mobile data&lt;br/&gt;that migrate th ough the netwo k,a single network hop at a time,executing at each step.Smart Messages&lt;br/&gt;a e self-outing,namely they a e esponsible fo dete mining thei own paths th ough the netwo k,utilizing a&lt;br/&gt;minimal set of facilities p ovided by nodes in the netwo k.An SM may be denied a equest to oute th ough&lt;br/&gt;a node due to the node &apos;s esou ce limitations,in pa ticula its emaining ene gy level.We conjectu e that&lt;br/&gt;Smart Messages p ovide a .exible support for a wide variety of applications,anging from data collection&lt;br/&gt;and dissemination,content-based outing and object t acking,to more t aditional dist ibuted computing&lt;br/&gt;applications in which execution of a task is sp ead across a collection of devices.&lt;br/&gt; As pa t of this p oposal we plan to implement and extensively evaluate seve al p ototype implementations&lt;br/&gt;of the Smart Message a chitectu e.Ou testbed will consist of networks of embedded multicontrollers and&lt;br/&gt;PDAs based on Bluetooth sho t-range wi eless communication.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121416</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121416</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1481" target="n1485">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY Molecular Computation in Ciliates</data>
      <data key="e_abstract">This research will focus on the interplay between molecular biology, computer science, and evolution to probe a complex form of biological information processing: gene unscrambling in hypotrichous ciliates. The long term goal is to develop a multidisciplinary approach to this problem that will enable us to tap into this biological process as a computational tool. The proposed research combines three approaches that first improve our understanding of the function, mechanism and logic of this phenomenon. Specifically, this proposal uses gene unscrambling as a model system to explore the mechanisms underlying complex gene rearrangements in ciliates the steps through which these processes have evolved, and their capacity to solve hard computational problems in vivo. The impact of the award would be an influx of new ideas and direction from computational sciences into the study of a complex biological system involving programmed genome rearrangements in ciliated protozoa, with the ultimate goal of harnessing the process for the purpose of performing in vivo computation.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121422</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121422</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1487" target="n1488">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/PE: Cognitive and Social Design of Robotic Assistants</data>
      <data key="e_abstract">Soon, autonomous mobile robots will be used in settings such as elderly communities, schools, and hospitals to carry out intelligent but &quot;unskilled&quot; tasks such as delivering food or teaching calisthenics. Humans are integral in these systems-as operators, as users, and as people who live and work where robots are employed. This proposal targets critical questions of human-robot interaction, and of robotic assistants in personal and work settings. The research involves experiments on mental models of robots and robotic assistants, ethnographic design research, and organizational field research. The initial research will aim at design of appropriate roles, tasks, and interactions of robotic assistants in elderly environments. Later research will address other domains for using assistive robots. This research builds on cognitive and social psychology, and design. The research will contribute to theory on people&apos;s interactions with robots, facilitate useful and graceful interactions between people and robotic assistants, and advance robotic technology and dialogue on ethical issues surrounding deployment of life-like robots. The research team is highly multidisciplinary. This project will address major NSF concerns including: bringing technology to bear for special populations, extending human capabilities, using technology for collective work, and integrating technology into social contexts.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121426</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121426</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1487" target="n1489">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/PE: Cognitive and Social Design of Robotic Assistants</data>
      <data key="e_abstract">Soon, autonomous mobile robots will be used in settings such as elderly communities, schools, and hospitals to carry out intelligent but &quot;unskilled&quot; tasks such as delivering food or teaching calisthenics. Humans are integral in these systems-as operators, as users, and as people who live and work where robots are employed. This proposal targets critical questions of human-robot interaction, and of robotic assistants in personal and work settings. The research involves experiments on mental models of robots and robotic assistants, ethnographic design research, and organizational field research. The initial research will aim at design of appropriate roles, tasks, and interactions of robotic assistants in elderly environments. Later research will address other domains for using assistive robots. This research builds on cognitive and social psychology, and design. The research will contribute to theory on people&apos;s interactions with robots, facilitate useful and graceful interactions between people and robotic assistants, and advance robotic technology and dialogue on ethical issues surrounding deployment of life-like robots. The research team is highly multidisciplinary. This project will address major NSF concerns including: bringing technology to bear for special populations, extending human capabilities, using technology for collective work, and integrating technology into social contexts.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121426</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121426</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1385" target="n1487">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/PE: Cognitive and Social Design of Robotic Assistants</data>
      <data key="e_abstract">Soon, autonomous mobile robots will be used in settings such as elderly communities, schools, and hospitals to carry out intelligent but &quot;unskilled&quot; tasks such as delivering food or teaching calisthenics. Humans are integral in these systems-as operators, as users, and as people who live and work where robots are employed. This proposal targets critical questions of human-robot interaction, and of robotic assistants in personal and work settings. The research involves experiments on mental models of robots and robotic assistants, ethnographic design research, and organizational field research. The initial research will aim at design of appropriate roles, tasks, and interactions of robotic assistants in elderly environments. Later research will address other domains for using assistive robots. This research builds on cognitive and social psychology, and design. The research will contribute to theory on people&apos;s interactions with robots, facilitate useful and graceful interactions between people and robotic assistants, and advance robotic technology and dialogue on ethical issues surrounding deployment of life-like robots. The research team is highly multidisciplinary. This project will address major NSF concerns including: bringing technology to bear for special populations, extending human capabilities, using technology for collective work, and integrating technology into social contexts.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121426</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121426</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1488" target="n1489">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/PE: Cognitive and Social Design of Robotic Assistants</data>
      <data key="e_abstract">Soon, autonomous mobile robots will be used in settings such as elderly communities, schools, and hospitals to carry out intelligent but &quot;unskilled&quot; tasks such as delivering food or teaching calisthenics. Humans are integral in these systems-as operators, as users, and as people who live and work where robots are employed. This proposal targets critical questions of human-robot interaction, and of robotic assistants in personal and work settings. The research involves experiments on mental models of robots and robotic assistants, ethnographic design research, and organizational field research. The initial research will aim at design of appropriate roles, tasks, and interactions of robotic assistants in elderly environments. Later research will address other domains for using assistive robots. This research builds on cognitive and social psychology, and design. The research will contribute to theory on people&apos;s interactions with robots, facilitate useful and graceful interactions between people and robotic assistants, and advance robotic technology and dialogue on ethical issues surrounding deployment of life-like robots. The research team is highly multidisciplinary. This project will address major NSF concerns including: bringing technology to bear for special populations, extending human capabilities, using technology for collective work, and integrating technology into social contexts.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121426</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121426</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1385" target="n1488">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/PE: Cognitive and Social Design of Robotic Assistants</data>
      <data key="e_abstract">Soon, autonomous mobile robots will be used in settings such as elderly communities, schools, and hospitals to carry out intelligent but &quot;unskilled&quot; tasks such as delivering food or teaching calisthenics. Humans are integral in these systems-as operators, as users, and as people who live and work where robots are employed. This proposal targets critical questions of human-robot interaction, and of robotic assistants in personal and work settings. The research involves experiments on mental models of robots and robotic assistants, ethnographic design research, and organizational field research. The initial research will aim at design of appropriate roles, tasks, and interactions of robotic assistants in elderly environments. Later research will address other domains for using assistive robots. This research builds on cognitive and social psychology, and design. The research will contribute to theory on people&apos;s interactions with robots, facilitate useful and graceful interactions between people and robotic assistants, and advance robotic technology and dialogue on ethical issues surrounding deployment of life-like robots. The research team is highly multidisciplinary. This project will address major NSF concerns including: bringing technology to bear for special populations, extending human capabilities, using technology for collective work, and integrating technology into social contexts.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121426</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121426</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1385" target="n1489">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/PE: Cognitive and Social Design of Robotic Assistants</data>
      <data key="e_abstract">Soon, autonomous mobile robots will be used in settings such as elderly communities, schools, and hospitals to carry out intelligent but &quot;unskilled&quot; tasks such as delivering food or teaching calisthenics. Humans are integral in these systems-as operators, as users, and as people who live and work where robots are employed. This proposal targets critical questions of human-robot interaction, and of robotic assistants in personal and work settings. The research involves experiments on mental models of robots and robotic assistants, ethnographic design research, and organizational field research. The initial research will aim at design of appropriate roles, tasks, and interactions of robotic assistants in elderly environments. Later research will address other domains for using assistive robots. This research builds on cognitive and social psychology, and design. The research will contribute to theory on people&apos;s interactions with robots, facilitate useful and graceful interactions between people and robotic assistants, and advance robotic technology and dialogue on ethical issues surrounding deployment of life-like robots. The research team is highly multidisciplinary. This project will address major NSF concerns including: bringing technology to bear for special populations, extending human capabilities, using technology for collective work, and integrating technology into social contexts.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121426</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121426</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1491" target="n1492">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Reversible Computing Using Superconducting Quantum Devices</data>
      <data key="e_abstract">EIA-0121428&lt;br/&gt;Averin, Dmitri V&lt;br/&gt;SUNY at Stony Brook&lt;br/&gt;&lt;br/&gt;ITR/SY Reversible Computing Using Superconducting Quantum Devices&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project is exploring two options of using the unique macroscopic quantum properties of superconductor devices for implementation of classical and quantum reversible computing.&lt;br/&gt;&lt;br/&gt;Under the first objective of classical reversible computing, experimental demonstration of reversible processing of digital information in a basic integrated circuit using Parametric-Quantuton type devices, with bits coded by single quanta of magnetic field are investigated. The implementation of this type of computing device is opening a way to information processing at a rate of about 1025 bits per second per Centimeter Square.&lt;br/&gt;&lt;br/&gt;Under the second objective of quantum reversible computing, the experimental studies of decoherence in the charge-based qubit (&quot;single-Cooper-Pair Box&quot;), using the radio-frequency single-electron transistor as the readout, is being achieved. Measurement of the decoherence time and investigation of possible ways of suppressing the decoherence is facilitating more rapid development of a scalable quantum computer.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121428</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121428</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1491" target="n1493">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Reversible Computing Using Superconducting Quantum Devices</data>
      <data key="e_abstract">EIA-0121428&lt;br/&gt;Averin, Dmitri V&lt;br/&gt;SUNY at Stony Brook&lt;br/&gt;&lt;br/&gt;ITR/SY Reversible Computing Using Superconducting Quantum Devices&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project is exploring two options of using the unique macroscopic quantum properties of superconductor devices for implementation of classical and quantum reversible computing.&lt;br/&gt;&lt;br/&gt;Under the first objective of classical reversible computing, experimental demonstration of reversible processing of digital information in a basic integrated circuit using Parametric-Quantuton type devices, with bits coded by single quanta of magnetic field are investigated. The implementation of this type of computing device is opening a way to information processing at a rate of about 1025 bits per second per Centimeter Square.&lt;br/&gt;&lt;br/&gt;Under the second objective of quantum reversible computing, the experimental studies of decoherence in the charge-based qubit (&quot;single-Cooper-Pair Box&quot;), using the radio-frequency single-electron transistor as the readout, is being achieved. Measurement of the decoherence time and investigation of possible ways of suppressing the decoherence is facilitating more rapid development of a scalable quantum computer.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121428</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121428</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n465" target="n1491">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Reversible Computing Using Superconducting Quantum Devices</data>
      <data key="e_abstract">EIA-0121428&lt;br/&gt;Averin, Dmitri V&lt;br/&gt;SUNY at Stony Brook&lt;br/&gt;&lt;br/&gt;ITR/SY Reversible Computing Using Superconducting Quantum Devices&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project is exploring two options of using the unique macroscopic quantum properties of superconductor devices for implementation of classical and quantum reversible computing.&lt;br/&gt;&lt;br/&gt;Under the first objective of classical reversible computing, experimental demonstration of reversible processing of digital information in a basic integrated circuit using Parametric-Quantuton type devices, with bits coded by single quanta of magnetic field are investigated. The implementation of this type of computing device is opening a way to information processing at a rate of about 1025 bits per second per Centimeter Square.&lt;br/&gt;&lt;br/&gt;Under the second objective of quantum reversible computing, the experimental studies of decoherence in the charge-based qubit (&quot;single-Cooper-Pair Box&quot;), using the radio-frequency single-electron transistor as the readout, is being achieved. Measurement of the decoherence time and investigation of possible ways of suppressing the decoherence is facilitating more rapid development of a scalable quantum computer.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121428</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121428</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1492" target="n1493">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Reversible Computing Using Superconducting Quantum Devices</data>
      <data key="e_abstract">EIA-0121428&lt;br/&gt;Averin, Dmitri V&lt;br/&gt;SUNY at Stony Brook&lt;br/&gt;&lt;br/&gt;ITR/SY Reversible Computing Using Superconducting Quantum Devices&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project is exploring two options of using the unique macroscopic quantum properties of superconductor devices for implementation of classical and quantum reversible computing.&lt;br/&gt;&lt;br/&gt;Under the first objective of classical reversible computing, experimental demonstration of reversible processing of digital information in a basic integrated circuit using Parametric-Quantuton type devices, with bits coded by single quanta of magnetic field are investigated. The implementation of this type of computing device is opening a way to information processing at a rate of about 1025 bits per second per Centimeter Square.&lt;br/&gt;&lt;br/&gt;Under the second objective of quantum reversible computing, the experimental studies of decoherence in the charge-based qubit (&quot;single-Cooper-Pair Box&quot;), using the radio-frequency single-electron transistor as the readout, is being achieved. Measurement of the decoherence time and investigation of possible ways of suppressing the decoherence is facilitating more rapid development of a scalable quantum computer.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121428</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121428</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n465" target="n1492">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Reversible Computing Using Superconducting Quantum Devices</data>
      <data key="e_abstract">EIA-0121428&lt;br/&gt;Averin, Dmitri V&lt;br/&gt;SUNY at Stony Brook&lt;br/&gt;&lt;br/&gt;ITR/SY Reversible Computing Using Superconducting Quantum Devices&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project is exploring two options of using the unique macroscopic quantum properties of superconductor devices for implementation of classical and quantum reversible computing.&lt;br/&gt;&lt;br/&gt;Under the first objective of classical reversible computing, experimental demonstration of reversible processing of digital information in a basic integrated circuit using Parametric-Quantuton type devices, with bits coded by single quanta of magnetic field are investigated. The implementation of this type of computing device is opening a way to information processing at a rate of about 1025 bits per second per Centimeter Square.&lt;br/&gt;&lt;br/&gt;Under the second objective of quantum reversible computing, the experimental studies of decoherence in the charge-based qubit (&quot;single-Cooper-Pair Box&quot;), using the radio-frequency single-electron transistor as the readout, is being achieved. Measurement of the decoherence time and investigation of possible ways of suppressing the decoherence is facilitating more rapid development of a scalable quantum computer.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121428</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121428</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n465" target="n1493">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Reversible Computing Using Superconducting Quantum Devices</data>
      <data key="e_abstract">EIA-0121428&lt;br/&gt;Averin, Dmitri V&lt;br/&gt;SUNY at Stony Brook&lt;br/&gt;&lt;br/&gt;ITR/SY Reversible Computing Using Superconducting Quantum Devices&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project is exploring two options of using the unique macroscopic quantum properties of superconductor devices for implementation of classical and quantum reversible computing.&lt;br/&gt;&lt;br/&gt;Under the first objective of classical reversible computing, experimental demonstration of reversible processing of digital information in a basic integrated circuit using Parametric-Quantuton type devices, with bits coded by single quanta of magnetic field are investigated. The implementation of this type of computing device is opening a way to information processing at a rate of about 1025 bits per second per Centimeter Square.&lt;br/&gt;&lt;br/&gt;Under the second objective of quantum reversible computing, the experimental studies of decoherence in the charge-based qubit (&quot;single-Cooper-Pair Box&quot;), using the radio-frequency single-electron transistor as the readout, is being achieved. Measurement of the decoherence time and investigation of possible ways of suppressing the decoherence is facilitating more rapid development of a scalable quantum computer.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121428</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121428</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1495" target="n1496">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">RES - Women in Information Technology: Pivotal Transitions from School to Careers</data>
      <data key="e_abstract">The &quot; Women in Information Technology: Pivotal Transitions from School to Careers&quot; project is gathering primary research data for K-12 and university educators, policy makers, and administrators about those pivotal transition points in girls&apos; lives that result in their positive or negative view of information technology as a viable career choice. It is gathering new information about how the total environment - at the high school, community college, and university levels, both inside and outside the school -- helps shape girls&apos; perceptions of technology as friendly or unfriendly to them. It will document longitudinally the impact of family, peers, school, and community on girls&apos; perceptions of IT careers; examine the key transition points in girls&apos; experiences with technology; and determine how the choice of a nontraditional career is associated with the development of self-authorship.&lt;br/&gt;&lt;br/&gt;The project combines standard interview and survey techniques within the theoretical framework of self-authorship. Methods include a pre- and post-survey; individual interviews; small group interviews; a videotape documentary and case studies of the longitudinal development of girls&apos; career transitions and choices; and group activities using computer programs to stimulate girls&apos; interest in and understanding of IT careers. A set of IT careers workshops are planned as an incentive for participating students and parents, as another data collection point, and as a model for IT career exploration.&lt;br/&gt;&lt;br/&gt;The project is an interdisciplinary collaboration among faculty with expertise in the areas of gender and science, quantitative and qualitative research methods in the social sciences, and information technology impacts on children, youth, and families. Dr. Marcia Baxter Magolda, leading expert in the study of how college students&apos; and young adults&apos; self-authorship effects their learning capacity, will act as an advisor for this project. The other advisors include a former school principal and superintendent, evaluation and data analysis expert, educational technology expert, state technology workforce director, and a communications&lt;br/&gt;researcher.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">120458</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">120458</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1495" target="n1497">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">RES - Women in Information Technology: Pivotal Transitions from School to Careers</data>
      <data key="e_abstract">The &quot; Women in Information Technology: Pivotal Transitions from School to Careers&quot; project is gathering primary research data for K-12 and university educators, policy makers, and administrators about those pivotal transition points in girls&apos; lives that result in their positive or negative view of information technology as a viable career choice. It is gathering new information about how the total environment - at the high school, community college, and university levels, both inside and outside the school -- helps shape girls&apos; perceptions of technology as friendly or unfriendly to them. It will document longitudinally the impact of family, peers, school, and community on girls&apos; perceptions of IT careers; examine the key transition points in girls&apos; experiences with technology; and determine how the choice of a nontraditional career is associated with the development of self-authorship.&lt;br/&gt;&lt;br/&gt;The project combines standard interview and survey techniques within the theoretical framework of self-authorship. Methods include a pre- and post-survey; individual interviews; small group interviews; a videotape documentary and case studies of the longitudinal development of girls&apos; career transitions and choices; and group activities using computer programs to stimulate girls&apos; interest in and understanding of IT careers. A set of IT careers workshops are planned as an incentive for participating students and parents, as another data collection point, and as a model for IT career exploration.&lt;br/&gt;&lt;br/&gt;The project is an interdisciplinary collaboration among faculty with expertise in the areas of gender and science, quantitative and qualitative research methods in the social sciences, and information technology impacts on children, youth, and families. Dr. Marcia Baxter Magolda, leading expert in the study of how college students&apos; and young adults&apos; self-authorship effects their learning capacity, will act as an advisor for this project. The other advisors include a former school principal and superintendent, evaluation and data analysis expert, educational technology expert, state technology workforce director, and a communications&lt;br/&gt;researcher.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">120458</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">120458</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1496" target="n1497">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">RES - Women in Information Technology: Pivotal Transitions from School to Careers</data>
      <data key="e_abstract">The &quot; Women in Information Technology: Pivotal Transitions from School to Careers&quot; project is gathering primary research data for K-12 and university educators, policy makers, and administrators about those pivotal transition points in girls&apos; lives that result in their positive or negative view of information technology as a viable career choice. It is gathering new information about how the total environment - at the high school, community college, and university levels, both inside and outside the school -- helps shape girls&apos; perceptions of technology as friendly or unfriendly to them. It will document longitudinally the impact of family, peers, school, and community on girls&apos; perceptions of IT careers; examine the key transition points in girls&apos; experiences with technology; and determine how the choice of a nontraditional career is associated with the development of self-authorship.&lt;br/&gt;&lt;br/&gt;The project combines standard interview and survey techniques within the theoretical framework of self-authorship. Methods include a pre- and post-survey; individual interviews; small group interviews; a videotape documentary and case studies of the longitudinal development of girls&apos; career transitions and choices; and group activities using computer programs to stimulate girls&apos; interest in and understanding of IT careers. A set of IT careers workshops are planned as an incentive for participating students and parents, as another data collection point, and as a model for IT career exploration.&lt;br/&gt;&lt;br/&gt;The project is an interdisciplinary collaboration among faculty with expertise in the areas of gender and science, quantitative and qualitative research methods in the social sciences, and information technology impacts on children, youth, and families. Dr. Marcia Baxter Magolda, leading expert in the study of how college students&apos; and young adults&apos; self-authorship effects their learning capacity, will act as an advisor for this project. The other advisors include a former school principal and superintendent, evaluation and data analysis expert, educational technology expert, state technology workforce director, and a communications&lt;br/&gt;researcher.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">120458</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">120458</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1498" target="n1499">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Biological Information Technology &amp; Systems - BITS: Epigenetic Computers: An in vitro, in abstractio, and in silico Study</data>
      <data key="e_abstract">EIA-0130822&lt;br/&gt;Boahen, Kwabena&lt;br/&gt;Univ. of Pennsylvania&lt;br/&gt;&lt;br/&gt;BITS: Epigenetic Computers: An in vitro, in abstractio, and in silico study&lt;br/&gt;&lt;br/&gt;Ultimately, nanotechnology will enable us to design programmable information processors (computers) from the ground up, growing them from molecules. The brain is built in this fashion through the power of epigenesis multicellular differentiation triggered by environmental signals. Despite impressive progress in our understanding of this process (Sanes, Reh et al. 2000), no one has attempted to design a computer that learns by growing based on insights from neurobiology. Using information derived from in vitro and in abstractio studies of diffusible neurotrophic factor mediated neuronal pathfinding (growth-cone chemotaxis) and synaptogenesis, we will develop a self-configurable computer that reprograms itself at the level of individual connections by merging custom, very large scale integration, in silico microtechnology with growing virtual wires. With the insights gained in this project, we will be poised to take advantage of real anatomical plasticity made possible by rapid progress in nanotechnology.</data>
      <data key="e_pgm">1705</data>
      <data key="e_label">130822</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130822</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1498" target="n1500">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Biological Information Technology &amp; Systems - BITS: Epigenetic Computers: An in vitro, in abstractio, and in silico Study</data>
      <data key="e_abstract">EIA-0130822&lt;br/&gt;Boahen, Kwabena&lt;br/&gt;Univ. of Pennsylvania&lt;br/&gt;&lt;br/&gt;BITS: Epigenetic Computers: An in vitro, in abstractio, and in silico study&lt;br/&gt;&lt;br/&gt;Ultimately, nanotechnology will enable us to design programmable information processors (computers) from the ground up, growing them from molecules. The brain is built in this fashion through the power of epigenesis multicellular differentiation triggered by environmental signals. Despite impressive progress in our understanding of this process (Sanes, Reh et al. 2000), no one has attempted to design a computer that learns by growing based on insights from neurobiology. Using information derived from in vitro and in abstractio studies of diffusible neurotrophic factor mediated neuronal pathfinding (growth-cone chemotaxis) and synaptogenesis, we will develop a self-configurable computer that reprograms itself at the level of individual connections by merging custom, very large scale integration, in silico microtechnology with growing virtual wires. With the insights gained in this project, we will be poised to take advantage of real anatomical plasticity made possible by rapid progress in nanotechnology.</data>
      <data key="e_pgm">1705</data>
      <data key="e_label">130822</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130822</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1498" target="n1501">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Biological Information Technology &amp; Systems - BITS: Epigenetic Computers: An in vitro, in abstractio, and in silico Study</data>
      <data key="e_abstract">EIA-0130822&lt;br/&gt;Boahen, Kwabena&lt;br/&gt;Univ. of Pennsylvania&lt;br/&gt;&lt;br/&gt;BITS: Epigenetic Computers: An in vitro, in abstractio, and in silico study&lt;br/&gt;&lt;br/&gt;Ultimately, nanotechnology will enable us to design programmable information processors (computers) from the ground up, growing them from molecules. The brain is built in this fashion through the power of epigenesis multicellular differentiation triggered by environmental signals. Despite impressive progress in our understanding of this process (Sanes, Reh et al. 2000), no one has attempted to design a computer that learns by growing based on insights from neurobiology. Using information derived from in vitro and in abstractio studies of diffusible neurotrophic factor mediated neuronal pathfinding (growth-cone chemotaxis) and synaptogenesis, we will develop a self-configurable computer that reprograms itself at the level of individual connections by merging custom, very large scale integration, in silico microtechnology with growing virtual wires. With the insights gained in this project, we will be poised to take advantage of real anatomical plasticity made possible by rapid progress in nanotechnology.</data>
      <data key="e_pgm">1705</data>
      <data key="e_label">130822</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130822</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1499" target="n1500">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Biological Information Technology &amp; Systems - BITS: Epigenetic Computers: An in vitro, in abstractio, and in silico Study</data>
      <data key="e_abstract">EIA-0130822&lt;br/&gt;Boahen, Kwabena&lt;br/&gt;Univ. of Pennsylvania&lt;br/&gt;&lt;br/&gt;BITS: Epigenetic Computers: An in vitro, in abstractio, and in silico study&lt;br/&gt;&lt;br/&gt;Ultimately, nanotechnology will enable us to design programmable information processors (computers) from the ground up, growing them from molecules. The brain is built in this fashion through the power of epigenesis multicellular differentiation triggered by environmental signals. Despite impressive progress in our understanding of this process (Sanes, Reh et al. 2000), no one has attempted to design a computer that learns by growing based on insights from neurobiology. Using information derived from in vitro and in abstractio studies of diffusible neurotrophic factor mediated neuronal pathfinding (growth-cone chemotaxis) and synaptogenesis, we will develop a self-configurable computer that reprograms itself at the level of individual connections by merging custom, very large scale integration, in silico microtechnology with growing virtual wires. With the insights gained in this project, we will be poised to take advantage of real anatomical plasticity made possible by rapid progress in nanotechnology.</data>
      <data key="e_pgm">1705</data>
      <data key="e_label">130822</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130822</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1499" target="n1501">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Biological Information Technology &amp; Systems - BITS: Epigenetic Computers: An in vitro, in abstractio, and in silico Study</data>
      <data key="e_abstract">EIA-0130822&lt;br/&gt;Boahen, Kwabena&lt;br/&gt;Univ. of Pennsylvania&lt;br/&gt;&lt;br/&gt;BITS: Epigenetic Computers: An in vitro, in abstractio, and in silico study&lt;br/&gt;&lt;br/&gt;Ultimately, nanotechnology will enable us to design programmable information processors (computers) from the ground up, growing them from molecules. The brain is built in this fashion through the power of epigenesis multicellular differentiation triggered by environmental signals. Despite impressive progress in our understanding of this process (Sanes, Reh et al. 2000), no one has attempted to design a computer that learns by growing based on insights from neurobiology. Using information derived from in vitro and in abstractio studies of diffusible neurotrophic factor mediated neuronal pathfinding (growth-cone chemotaxis) and synaptogenesis, we will develop a self-configurable computer that reprograms itself at the level of individual connections by merging custom, very large scale integration, in silico microtechnology with growing virtual wires. With the insights gained in this project, we will be poised to take advantage of real anatomical plasticity made possible by rapid progress in nanotechnology.</data>
      <data key="e_pgm">1705</data>
      <data key="e_label">130822</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130822</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1500" target="n1501">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Biological Information Technology &amp; Systems - BITS: Epigenetic Computers: An in vitro, in abstractio, and in silico Study</data>
      <data key="e_abstract">EIA-0130822&lt;br/&gt;Boahen, Kwabena&lt;br/&gt;Univ. of Pennsylvania&lt;br/&gt;&lt;br/&gt;BITS: Epigenetic Computers: An in vitro, in abstractio, and in silico study&lt;br/&gt;&lt;br/&gt;Ultimately, nanotechnology will enable us to design programmable information processors (computers) from the ground up, growing them from molecules. The brain is built in this fashion through the power of epigenesis multicellular differentiation triggered by environmental signals. Despite impressive progress in our understanding of this process (Sanes, Reh et al. 2000), no one has attempted to design a computer that learns by growing based on insights from neurobiology. Using information derived from in vitro and in abstractio studies of diffusible neurotrophic factor mediated neuronal pathfinding (growth-cone chemotaxis) and synaptogenesis, we will develop a self-configurable computer that reprograms itself at the level of individual connections by merging custom, very large scale integration, in silico microtechnology with growing virtual wires. With the insights gained in this project, we will be poised to take advantage of real anatomical plasticity made possible by rapid progress in nanotechnology.</data>
      <data key="e_pgm">1705</data>
      <data key="e_label">130822</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130822</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1504" target="n1505">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Emergent Organizational Structures in Multi-Agent Scalable Enterprises</data>
      <data key="e_abstract">This Scalable Enterprise Systems Phase II project&apos;s primary goal is to develop models of emergent enterprises that capture the independent behavior of the organizations involved, as well as the effects of interactions among individual organizations, in order to accurately predict dynamics of the emergent enterprise and performance of the system in the long run. Participants in the enterprise can use these results to select appropriate operational strategies in order to improve local performance measures such as profit or cash flow over a fixed planning horizon. In addition, these models will provide insight into the mechanisms that result in effective alliances and organizational designs. The modeling approach has three steps. First, a representation scheme is developed for capturing characteristics of individual participants that incorporates task information, organizational relationships, local and system level goals, and possible changes in the environment. An enterprise is thus a collection of several of such detailed representations for each organization and the interactions between these organizations. Second, a micro-macro modeling approach is developed, where the details of each agent&apos;s tasks (represented in the micro-level simulation framework) are aggregated into a set of interaction parameters at the macro-level. The macro-level enterprise model is formulated as an interacting particle system. Finally, the third step involves validation of the theories developed using field data. Field data will be collected from different industries that have large number of participants in the supply network, such as the food industry and the automotive industry, to construct practical agent representations and to verify the theoretical performance estimates. Large-scale simulations using a multi-agent simulator will also be used to empirically estimate the performance of a given organizational configuration. &lt;br/&gt;&lt;br/&gt;This research will allow planners to estimate a priori the overall enterprise performance under different operating conditions, and eventually, design optimal emergent societies. Moreover, systems composed of distributed decision-makers are encountered in various application domains, such as traffic on highway networks, air traffic management systems, design teams composed of dispersed designers, distributed sensor networks, multi-agent systems, etc. The fundamental advances made in this research will have significant impact on our ability to predict the performance of such systems, and therefore devise appropriate planning and control strategies.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122173</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">122173</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1504" target="n1506">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Emergent Organizational Structures in Multi-Agent Scalable Enterprises</data>
      <data key="e_abstract">This Scalable Enterprise Systems Phase II project&apos;s primary goal is to develop models of emergent enterprises that capture the independent behavior of the organizations involved, as well as the effects of interactions among individual organizations, in order to accurately predict dynamics of the emergent enterprise and performance of the system in the long run. Participants in the enterprise can use these results to select appropriate operational strategies in order to improve local performance measures such as profit or cash flow over a fixed planning horizon. In addition, these models will provide insight into the mechanisms that result in effective alliances and organizational designs. The modeling approach has three steps. First, a representation scheme is developed for capturing characteristics of individual participants that incorporates task information, organizational relationships, local and system level goals, and possible changes in the environment. An enterprise is thus a collection of several of such detailed representations for each organization and the interactions between these organizations. Second, a micro-macro modeling approach is developed, where the details of each agent&apos;s tasks (represented in the micro-level simulation framework) are aggregated into a set of interaction parameters at the macro-level. The macro-level enterprise model is formulated as an interacting particle system. Finally, the third step involves validation of the theories developed using field data. Field data will be collected from different industries that have large number of participants in the supply network, such as the food industry and the automotive industry, to construct practical agent representations and to verify the theoretical performance estimates. Large-scale simulations using a multi-agent simulator will also be used to empirically estimate the performance of a given organizational configuration. &lt;br/&gt;&lt;br/&gt;This research will allow planners to estimate a priori the overall enterprise performance under different operating conditions, and eventually, design optimal emergent societies. Moreover, systems composed of distributed decision-makers are encountered in various application domains, such as traffic on highway networks, air traffic management systems, design teams composed of dispersed designers, distributed sensor networks, multi-agent systems, etc. The fundamental advances made in this research will have significant impact on our ability to predict the performance of such systems, and therefore devise appropriate planning and control strategies.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122173</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">122173</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1504" target="n1507">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Emergent Organizational Structures in Multi-Agent Scalable Enterprises</data>
      <data key="e_abstract">This Scalable Enterprise Systems Phase II project&apos;s primary goal is to develop models of emergent enterprises that capture the independent behavior of the organizations involved, as well as the effects of interactions among individual organizations, in order to accurately predict dynamics of the emergent enterprise and performance of the system in the long run. Participants in the enterprise can use these results to select appropriate operational strategies in order to improve local performance measures such as profit or cash flow over a fixed planning horizon. In addition, these models will provide insight into the mechanisms that result in effective alliances and organizational designs. The modeling approach has three steps. First, a representation scheme is developed for capturing characteristics of individual participants that incorporates task information, organizational relationships, local and system level goals, and possible changes in the environment. An enterprise is thus a collection of several of such detailed representations for each organization and the interactions between these organizations. Second, a micro-macro modeling approach is developed, where the details of each agent&apos;s tasks (represented in the micro-level simulation framework) are aggregated into a set of interaction parameters at the macro-level. The macro-level enterprise model is formulated as an interacting particle system. Finally, the third step involves validation of the theories developed using field data. Field data will be collected from different industries that have large number of participants in the supply network, such as the food industry and the automotive industry, to construct practical agent representations and to verify the theoretical performance estimates. Large-scale simulations using a multi-agent simulator will also be used to empirically estimate the performance of a given organizational configuration. &lt;br/&gt;&lt;br/&gt;This research will allow planners to estimate a priori the overall enterprise performance under different operating conditions, and eventually, design optimal emergent societies. Moreover, systems composed of distributed decision-makers are encountered in various application domains, such as traffic on highway networks, air traffic management systems, design teams composed of dispersed designers, distributed sensor networks, multi-agent systems, etc. The fundamental advances made in this research will have significant impact on our ability to predict the performance of such systems, and therefore devise appropriate planning and control strategies.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122173</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">122173</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1504" target="n1508">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Emergent Organizational Structures in Multi-Agent Scalable Enterprises</data>
      <data key="e_abstract">This Scalable Enterprise Systems Phase II project&apos;s primary goal is to develop models of emergent enterprises that capture the independent behavior of the organizations involved, as well as the effects of interactions among individual organizations, in order to accurately predict dynamics of the emergent enterprise and performance of the system in the long run. Participants in the enterprise can use these results to select appropriate operational strategies in order to improve local performance measures such as profit or cash flow over a fixed planning horizon. In addition, these models will provide insight into the mechanisms that result in effective alliances and organizational designs. The modeling approach has three steps. First, a representation scheme is developed for capturing characteristics of individual participants that incorporates task information, organizational relationships, local and system level goals, and possible changes in the environment. An enterprise is thus a collection of several of such detailed representations for each organization and the interactions between these organizations. Second, a micro-macro modeling approach is developed, where the details of each agent&apos;s tasks (represented in the micro-level simulation framework) are aggregated into a set of interaction parameters at the macro-level. The macro-level enterprise model is formulated as an interacting particle system. Finally, the third step involves validation of the theories developed using field data. Field data will be collected from different industries that have large number of participants in the supply network, such as the food industry and the automotive industry, to construct practical agent representations and to verify the theoretical performance estimates. Large-scale simulations using a multi-agent simulator will also be used to empirically estimate the performance of a given organizational configuration. &lt;br/&gt;&lt;br/&gt;This research will allow planners to estimate a priori the overall enterprise performance under different operating conditions, and eventually, design optimal emergent societies. Moreover, systems composed of distributed decision-makers are encountered in various application domains, such as traffic on highway networks, air traffic management systems, design teams composed of dispersed designers, distributed sensor networks, multi-agent systems, etc. The fundamental advances made in this research will have significant impact on our ability to predict the performance of such systems, and therefore devise appropriate planning and control strategies.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122173</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">122173</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1504" target="n1509">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Emergent Organizational Structures in Multi-Agent Scalable Enterprises</data>
      <data key="e_abstract">This Scalable Enterprise Systems Phase II project&apos;s primary goal is to develop models of emergent enterprises that capture the independent behavior of the organizations involved, as well as the effects of interactions among individual organizations, in order to accurately predict dynamics of the emergent enterprise and performance of the system in the long run. Participants in the enterprise can use these results to select appropriate operational strategies in order to improve local performance measures such as profit or cash flow over a fixed planning horizon. In addition, these models will provide insight into the mechanisms that result in effective alliances and organizational designs. The modeling approach has three steps. First, a representation scheme is developed for capturing characteristics of individual participants that incorporates task information, organizational relationships, local and system level goals, and possible changes in the environment. An enterprise is thus a collection of several of such detailed representations for each organization and the interactions between these organizations. Second, a micro-macro modeling approach is developed, where the details of each agent&apos;s tasks (represented in the micro-level simulation framework) are aggregated into a set of interaction parameters at the macro-level. The macro-level enterprise model is formulated as an interacting particle system. Finally, the third step involves validation of the theories developed using field data. Field data will be collected from different industries that have large number of participants in the supply network, such as the food industry and the automotive industry, to construct practical agent representations and to verify the theoretical performance estimates. Large-scale simulations using a multi-agent simulator will also be used to empirically estimate the performance of a given organizational configuration. &lt;br/&gt;&lt;br/&gt;This research will allow planners to estimate a priori the overall enterprise performance under different operating conditions, and eventually, design optimal emergent societies. Moreover, systems composed of distributed decision-makers are encountered in various application domains, such as traffic on highway networks, air traffic management systems, design teams composed of dispersed designers, distributed sensor networks, multi-agent systems, etc. The fundamental advances made in this research will have significant impact on our ability to predict the performance of such systems, and therefore devise appropriate planning and control strategies.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122173</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">122173</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1505" target="n1506">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Emergent Organizational Structures in Multi-Agent Scalable Enterprises</data>
      <data key="e_abstract">This Scalable Enterprise Systems Phase II project&apos;s primary goal is to develop models of emergent enterprises that capture the independent behavior of the organizations involved, as well as the effects of interactions among individual organizations, in order to accurately predict dynamics of the emergent enterprise and performance of the system in the long run. Participants in the enterprise can use these results to select appropriate operational strategies in order to improve local performance measures such as profit or cash flow over a fixed planning horizon. In addition, these models will provide insight into the mechanisms that result in effective alliances and organizational designs. The modeling approach has three steps. First, a representation scheme is developed for capturing characteristics of individual participants that incorporates task information, organizational relationships, local and system level goals, and possible changes in the environment. An enterprise is thus a collection of several of such detailed representations for each organization and the interactions between these organizations. Second, a micro-macro modeling approach is developed, where the details of each agent&apos;s tasks (represented in the micro-level simulation framework) are aggregated into a set of interaction parameters at the macro-level. The macro-level enterprise model is formulated as an interacting particle system. Finally, the third step involves validation of the theories developed using field data. Field data will be collected from different industries that have large number of participants in the supply network, such as the food industry and the automotive industry, to construct practical agent representations and to verify the theoretical performance estimates. Large-scale simulations using a multi-agent simulator will also be used to empirically estimate the performance of a given organizational configuration. &lt;br/&gt;&lt;br/&gt;This research will allow planners to estimate a priori the overall enterprise performance under different operating conditions, and eventually, design optimal emergent societies. Moreover, systems composed of distributed decision-makers are encountered in various application domains, such as traffic on highway networks, air traffic management systems, design teams composed of dispersed designers, distributed sensor networks, multi-agent systems, etc. The fundamental advances made in this research will have significant impact on our ability to predict the performance of such systems, and therefore devise appropriate planning and control strategies.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122173</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">122173</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1505" target="n1507">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Emergent Organizational Structures in Multi-Agent Scalable Enterprises</data>
      <data key="e_abstract">This Scalable Enterprise Systems Phase II project&apos;s primary goal is to develop models of emergent enterprises that capture the independent behavior of the organizations involved, as well as the effects of interactions among individual organizations, in order to accurately predict dynamics of the emergent enterprise and performance of the system in the long run. Participants in the enterprise can use these results to select appropriate operational strategies in order to improve local performance measures such as profit or cash flow over a fixed planning horizon. In addition, these models will provide insight into the mechanisms that result in effective alliances and organizational designs. The modeling approach has three steps. First, a representation scheme is developed for capturing characteristics of individual participants that incorporates task information, organizational relationships, local and system level goals, and possible changes in the environment. An enterprise is thus a collection of several of such detailed representations for each organization and the interactions between these organizations. Second, a micro-macro modeling approach is developed, where the details of each agent&apos;s tasks (represented in the micro-level simulation framework) are aggregated into a set of interaction parameters at the macro-level. The macro-level enterprise model is formulated as an interacting particle system. Finally, the third step involves validation of the theories developed using field data. Field data will be collected from different industries that have large number of participants in the supply network, such as the food industry and the automotive industry, to construct practical agent representations and to verify the theoretical performance estimates. Large-scale simulations using a multi-agent simulator will also be used to empirically estimate the performance of a given organizational configuration. &lt;br/&gt;&lt;br/&gt;This research will allow planners to estimate a priori the overall enterprise performance under different operating conditions, and eventually, design optimal emergent societies. Moreover, systems composed of distributed decision-makers are encountered in various application domains, such as traffic on highway networks, air traffic management systems, design teams composed of dispersed designers, distributed sensor networks, multi-agent systems, etc. The fundamental advances made in this research will have significant impact on our ability to predict the performance of such systems, and therefore devise appropriate planning and control strategies.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122173</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">122173</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1505" target="n1508">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Emergent Organizational Structures in Multi-Agent Scalable Enterprises</data>
      <data key="e_abstract">This Scalable Enterprise Systems Phase II project&apos;s primary goal is to develop models of emergent enterprises that capture the independent behavior of the organizations involved, as well as the effects of interactions among individual organizations, in order to accurately predict dynamics of the emergent enterprise and performance of the system in the long run. Participants in the enterprise can use these results to select appropriate operational strategies in order to improve local performance measures such as profit or cash flow over a fixed planning horizon. In addition, these models will provide insight into the mechanisms that result in effective alliances and organizational designs. The modeling approach has three steps. First, a representation scheme is developed for capturing characteristics of individual participants that incorporates task information, organizational relationships, local and system level goals, and possible changes in the environment. An enterprise is thus a collection of several of such detailed representations for each organization and the interactions between these organizations. Second, a micro-macro modeling approach is developed, where the details of each agent&apos;s tasks (represented in the micro-level simulation framework) are aggregated into a set of interaction parameters at the macro-level. The macro-level enterprise model is formulated as an interacting particle system. Finally, the third step involves validation of the theories developed using field data. Field data will be collected from different industries that have large number of participants in the supply network, such as the food industry and the automotive industry, to construct practical agent representations and to verify the theoretical performance estimates. Large-scale simulations using a multi-agent simulator will also be used to empirically estimate the performance of a given organizational configuration. &lt;br/&gt;&lt;br/&gt;This research will allow planners to estimate a priori the overall enterprise performance under different operating conditions, and eventually, design optimal emergent societies. Moreover, systems composed of distributed decision-makers are encountered in various application domains, such as traffic on highway networks, air traffic management systems, design teams composed of dispersed designers, distributed sensor networks, multi-agent systems, etc. The fundamental advances made in this research will have significant impact on our ability to predict the performance of such systems, and therefore devise appropriate planning and control strategies.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122173</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">122173</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1505" target="n1509">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Emergent Organizational Structures in Multi-Agent Scalable Enterprises</data>
      <data key="e_abstract">This Scalable Enterprise Systems Phase II project&apos;s primary goal is to develop models of emergent enterprises that capture the independent behavior of the organizations involved, as well as the effects of interactions among individual organizations, in order to accurately predict dynamics of the emergent enterprise and performance of the system in the long run. Participants in the enterprise can use these results to select appropriate operational strategies in order to improve local performance measures such as profit or cash flow over a fixed planning horizon. In addition, these models will provide insight into the mechanisms that result in effective alliances and organizational designs. The modeling approach has three steps. First, a representation scheme is developed for capturing characteristics of individual participants that incorporates task information, organizational relationships, local and system level goals, and possible changes in the environment. An enterprise is thus a collection of several of such detailed representations for each organization and the interactions between these organizations. Second, a micro-macro modeling approach is developed, where the details of each agent&apos;s tasks (represented in the micro-level simulation framework) are aggregated into a set of interaction parameters at the macro-level. The macro-level enterprise model is formulated as an interacting particle system. Finally, the third step involves validation of the theories developed using field data. Field data will be collected from different industries that have large number of participants in the supply network, such as the food industry and the automotive industry, to construct practical agent representations and to verify the theoretical performance estimates. Large-scale simulations using a multi-agent simulator will also be used to empirically estimate the performance of a given organizational configuration. &lt;br/&gt;&lt;br/&gt;This research will allow planners to estimate a priori the overall enterprise performance under different operating conditions, and eventually, design optimal emergent societies. Moreover, systems composed of distributed decision-makers are encountered in various application domains, such as traffic on highway networks, air traffic management systems, design teams composed of dispersed designers, distributed sensor networks, multi-agent systems, etc. The fundamental advances made in this research will have significant impact on our ability to predict the performance of such systems, and therefore devise appropriate planning and control strategies.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122173</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">122173</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1506" target="n1507">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Emergent Organizational Structures in Multi-Agent Scalable Enterprises</data>
      <data key="e_abstract">This Scalable Enterprise Systems Phase II project&apos;s primary goal is to develop models of emergent enterprises that capture the independent behavior of the organizations involved, as well as the effects of interactions among individual organizations, in order to accurately predict dynamics of the emergent enterprise and performance of the system in the long run. Participants in the enterprise can use these results to select appropriate operational strategies in order to improve local performance measures such as profit or cash flow over a fixed planning horizon. In addition, these models will provide insight into the mechanisms that result in effective alliances and organizational designs. The modeling approach has three steps. First, a representation scheme is developed for capturing characteristics of individual participants that incorporates task information, organizational relationships, local and system level goals, and possible changes in the environment. An enterprise is thus a collection of several of such detailed representations for each organization and the interactions between these organizations. Second, a micro-macro modeling approach is developed, where the details of each agent&apos;s tasks (represented in the micro-level simulation framework) are aggregated into a set of interaction parameters at the macro-level. The macro-level enterprise model is formulated as an interacting particle system. Finally, the third step involves validation of the theories developed using field data. Field data will be collected from different industries that have large number of participants in the supply network, such as the food industry and the automotive industry, to construct practical agent representations and to verify the theoretical performance estimates. Large-scale simulations using a multi-agent simulator will also be used to empirically estimate the performance of a given organizational configuration. &lt;br/&gt;&lt;br/&gt;This research will allow planners to estimate a priori the overall enterprise performance under different operating conditions, and eventually, design optimal emergent societies. Moreover, systems composed of distributed decision-makers are encountered in various application domains, such as traffic on highway networks, air traffic management systems, design teams composed of dispersed designers, distributed sensor networks, multi-agent systems, etc. The fundamental advances made in this research will have significant impact on our ability to predict the performance of such systems, and therefore devise appropriate planning and control strategies.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122173</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">122173</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1506" target="n1508">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Emergent Organizational Structures in Multi-Agent Scalable Enterprises</data>
      <data key="e_abstract">This Scalable Enterprise Systems Phase II project&apos;s primary goal is to develop models of emergent enterprises that capture the independent behavior of the organizations involved, as well as the effects of interactions among individual organizations, in order to accurately predict dynamics of the emergent enterprise and performance of the system in the long run. Participants in the enterprise can use these results to select appropriate operational strategies in order to improve local performance measures such as profit or cash flow over a fixed planning horizon. In addition, these models will provide insight into the mechanisms that result in effective alliances and organizational designs. The modeling approach has three steps. First, a representation scheme is developed for capturing characteristics of individual participants that incorporates task information, organizational relationships, local and system level goals, and possible changes in the environment. An enterprise is thus a collection of several of such detailed representations for each organization and the interactions between these organizations. Second, a micro-macro modeling approach is developed, where the details of each agent&apos;s tasks (represented in the micro-level simulation framework) are aggregated into a set of interaction parameters at the macro-level. The macro-level enterprise model is formulated as an interacting particle system. Finally, the third step involves validation of the theories developed using field data. Field data will be collected from different industries that have large number of participants in the supply network, such as the food industry and the automotive industry, to construct practical agent representations and to verify the theoretical performance estimates. Large-scale simulations using a multi-agent simulator will also be used to empirically estimate the performance of a given organizational configuration. &lt;br/&gt;&lt;br/&gt;This research will allow planners to estimate a priori the overall enterprise performance under different operating conditions, and eventually, design optimal emergent societies. Moreover, systems composed of distributed decision-makers are encountered in various application domains, such as traffic on highway networks, air traffic management systems, design teams composed of dispersed designers, distributed sensor networks, multi-agent systems, etc. The fundamental advances made in this research will have significant impact on our ability to predict the performance of such systems, and therefore devise appropriate planning and control strategies.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122173</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">122173</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1506" target="n1509">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Emergent Organizational Structures in Multi-Agent Scalable Enterprises</data>
      <data key="e_abstract">This Scalable Enterprise Systems Phase II project&apos;s primary goal is to develop models of emergent enterprises that capture the independent behavior of the organizations involved, as well as the effects of interactions among individual organizations, in order to accurately predict dynamics of the emergent enterprise and performance of the system in the long run. Participants in the enterprise can use these results to select appropriate operational strategies in order to improve local performance measures such as profit or cash flow over a fixed planning horizon. In addition, these models will provide insight into the mechanisms that result in effective alliances and organizational designs. The modeling approach has three steps. First, a representation scheme is developed for capturing characteristics of individual participants that incorporates task information, organizational relationships, local and system level goals, and possible changes in the environment. An enterprise is thus a collection of several of such detailed representations for each organization and the interactions between these organizations. Second, a micro-macro modeling approach is developed, where the details of each agent&apos;s tasks (represented in the micro-level simulation framework) are aggregated into a set of interaction parameters at the macro-level. The macro-level enterprise model is formulated as an interacting particle system. Finally, the third step involves validation of the theories developed using field data. Field data will be collected from different industries that have large number of participants in the supply network, such as the food industry and the automotive industry, to construct practical agent representations and to verify the theoretical performance estimates. Large-scale simulations using a multi-agent simulator will also be used to empirically estimate the performance of a given organizational configuration. &lt;br/&gt;&lt;br/&gt;This research will allow planners to estimate a priori the overall enterprise performance under different operating conditions, and eventually, design optimal emergent societies. Moreover, systems composed of distributed decision-makers are encountered in various application domains, such as traffic on highway networks, air traffic management systems, design teams composed of dispersed designers, distributed sensor networks, multi-agent systems, etc. The fundamental advances made in this research will have significant impact on our ability to predict the performance of such systems, and therefore devise appropriate planning and control strategies.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122173</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">122173</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1507" target="n1508">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Emergent Organizational Structures in Multi-Agent Scalable Enterprises</data>
      <data key="e_abstract">This Scalable Enterprise Systems Phase II project&apos;s primary goal is to develop models of emergent enterprises that capture the independent behavior of the organizations involved, as well as the effects of interactions among individual organizations, in order to accurately predict dynamics of the emergent enterprise and performance of the system in the long run. Participants in the enterprise can use these results to select appropriate operational strategies in order to improve local performance measures such as profit or cash flow over a fixed planning horizon. In addition, these models will provide insight into the mechanisms that result in effective alliances and organizational designs. The modeling approach has three steps. First, a representation scheme is developed for capturing characteristics of individual participants that incorporates task information, organizational relationships, local and system level goals, and possible changes in the environment. An enterprise is thus a collection of several of such detailed representations for each organization and the interactions between these organizations. Second, a micro-macro modeling approach is developed, where the details of each agent&apos;s tasks (represented in the micro-level simulation framework) are aggregated into a set of interaction parameters at the macro-level. The macro-level enterprise model is formulated as an interacting particle system. Finally, the third step involves validation of the theories developed using field data. Field data will be collected from different industries that have large number of participants in the supply network, such as the food industry and the automotive industry, to construct practical agent representations and to verify the theoretical performance estimates. Large-scale simulations using a multi-agent simulator will also be used to empirically estimate the performance of a given organizational configuration. &lt;br/&gt;&lt;br/&gt;This research will allow planners to estimate a priori the overall enterprise performance under different operating conditions, and eventually, design optimal emergent societies. Moreover, systems composed of distributed decision-makers are encountered in various application domains, such as traffic on highway networks, air traffic management systems, design teams composed of dispersed designers, distributed sensor networks, multi-agent systems, etc. The fundamental advances made in this research will have significant impact on our ability to predict the performance of such systems, and therefore devise appropriate planning and control strategies.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122173</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">122173</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1507" target="n1509">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Emergent Organizational Structures in Multi-Agent Scalable Enterprises</data>
      <data key="e_abstract">This Scalable Enterprise Systems Phase II project&apos;s primary goal is to develop models of emergent enterprises that capture the independent behavior of the organizations involved, as well as the effects of interactions among individual organizations, in order to accurately predict dynamics of the emergent enterprise and performance of the system in the long run. Participants in the enterprise can use these results to select appropriate operational strategies in order to improve local performance measures such as profit or cash flow over a fixed planning horizon. In addition, these models will provide insight into the mechanisms that result in effective alliances and organizational designs. The modeling approach has three steps. First, a representation scheme is developed for capturing characteristics of individual participants that incorporates task information, organizational relationships, local and system level goals, and possible changes in the environment. An enterprise is thus a collection of several of such detailed representations for each organization and the interactions between these organizations. Second, a micro-macro modeling approach is developed, where the details of each agent&apos;s tasks (represented in the micro-level simulation framework) are aggregated into a set of interaction parameters at the macro-level. The macro-level enterprise model is formulated as an interacting particle system. Finally, the third step involves validation of the theories developed using field data. Field data will be collected from different industries that have large number of participants in the supply network, such as the food industry and the automotive industry, to construct practical agent representations and to verify the theoretical performance estimates. Large-scale simulations using a multi-agent simulator will also be used to empirically estimate the performance of a given organizational configuration. &lt;br/&gt;&lt;br/&gt;This research will allow planners to estimate a priori the overall enterprise performance under different operating conditions, and eventually, design optimal emergent societies. Moreover, systems composed of distributed decision-makers are encountered in various application domains, such as traffic on highway networks, air traffic management systems, design teams composed of dispersed designers, distributed sensor networks, multi-agent systems, etc. The fundamental advances made in this research will have significant impact on our ability to predict the performance of such systems, and therefore devise appropriate planning and control strategies.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122173</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">122173</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1508" target="n1509">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Emergent Organizational Structures in Multi-Agent Scalable Enterprises</data>
      <data key="e_abstract">This Scalable Enterprise Systems Phase II project&apos;s primary goal is to develop models of emergent enterprises that capture the independent behavior of the organizations involved, as well as the effects of interactions among individual organizations, in order to accurately predict dynamics of the emergent enterprise and performance of the system in the long run. Participants in the enterprise can use these results to select appropriate operational strategies in order to improve local performance measures such as profit or cash flow over a fixed planning horizon. In addition, these models will provide insight into the mechanisms that result in effective alliances and organizational designs. The modeling approach has three steps. First, a representation scheme is developed for capturing characteristics of individual participants that incorporates task information, organizational relationships, local and system level goals, and possible changes in the environment. An enterprise is thus a collection of several of such detailed representations for each organization and the interactions between these organizations. Second, a micro-macro modeling approach is developed, where the details of each agent&apos;s tasks (represented in the micro-level simulation framework) are aggregated into a set of interaction parameters at the macro-level. The macro-level enterprise model is formulated as an interacting particle system. Finally, the third step involves validation of the theories developed using field data. Field data will be collected from different industries that have large number of participants in the supply network, such as the food industry and the automotive industry, to construct practical agent representations and to verify the theoretical performance estimates. Large-scale simulations using a multi-agent simulator will also be used to empirically estimate the performance of a given organizational configuration. &lt;br/&gt;&lt;br/&gt;This research will allow planners to estimate a priori the overall enterprise performance under different operating conditions, and eventually, design optimal emergent societies. Moreover, systems composed of distributed decision-makers are encountered in various application domains, such as traffic on highway networks, air traffic management systems, design teams composed of dispersed designers, distributed sensor networks, multi-agent systems, etc. The fundamental advances made in this research will have significant impact on our ability to predict the performance of such systems, and therefore devise appropriate planning and control strategies.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122173</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">122173</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1512" target="n1513">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Center for Bits and Atoms</data>
      <data key="e_abstract">The scaling of information technology has been an active area of inquiry from the outset of its commercial development. Pioneering studies including those of [Moore, 79] and [Keyes, 87] identified the possibility of sustained exponential improvements in key semiconductor device parameters, as well as the significant obstacles that would need to be overcome to maintain such a pace. The enormous intellectual and financial investment into that effort has translated into steady improvements in overall system performance, so that now more and more computer applications are less and less resource-bound. But what&apos;s typically been missed in forecasts such as the Semiconductor Industry Association&apos;s influential roadmap is a projection of the relevance of the whole scaling effort. A narrow focus on improving device performance ignores the importance of the context in which computers are used, which is leading to very real scaling limits that are among the most serious obstacles to further progress. These include the economics of producing both chips and chip fabs, and the difficulty of designing and managing very large-scale systems. Beyond their practical significance, these issues present some of the most profound research questions in all of information technology, but they are questions that crucially cut across traditional discipline boundaries. Most importantly, it is no longer possible to maintain the fiction that developing hardware can be neatly separated from developing software. &lt;br/&gt;&lt;br/&gt;The Center for Bits and Atoms is an ambitious attempt to close this historical divide by bringing together the resources required to simultaneously study the content of information and its physical properties, on length scales from atomic nuclei to global networks. It aims to develop architectures for scaling information technology appropriate to each of these levels of description, and through a network of partnerships deploy these capabilities for the greatest global impact. Along the way, it seeks to fundamentally revisit the notion of what is a computer, and what is a computation. The CBA&apos;s program is based on the belief that the most significant of all the obstacles to progress has been the isolation of the investigation of each these pieces from that of the larger whole that they promise to enable. &lt;br/&gt;&lt;br/&gt;The research agenda is organized into three layers, in order of accessibility and importance. The first of these addresses system-level questions, asking how to extend networks of (relatively) conventional processors up to and beyond billions of interacting entities. Such coming complexity is being driven by countless practical applications, but will break the existing protocols used to operate the Internet as well as the techniques used for managing it. The approach taken here will be to &quot;de-layer&quot; the divisions between physical transport, logical connection, and application implementation, so that when devices are connected they simultaneously create a network, a distributed data structure, and the computer to manipulate it. The algorithms for processing and routing information are crucially assembled as the components are assembled, and autonomously adapt as nodes come and go, so that scalability is literally built in as the system grows. De-layering also beneficially exposes the capabilities of low-level devices to high-level applications (and vice versa), so that rich interfaces such as sensor networks can become the norm rather than the exception. &lt;br/&gt;&lt;br/&gt;The second layer builds on this system-level insight to ask about technologies to meet the demand for embedding billions of computers into everyday objects. Even though the cost per transistor has fallen exponentially for decades, the minimum cost per packaged part has remained relatively unchanged over the whole VLSI scaling era. For such large-scale systems to be compatible with the global GDP, it&apos;s necessary to fundamentally rethink the nature of device fabrication. The approach in the CBA will be to seek to eliminate central chip fabs entirely, using table-top printing technologies to move the production of computers to where and when they are needed. The fundamental enabling insight that makes this possible is the use of nanocrystalline electronically-active inks. Not only does this promise to dramatically reduce the cost per part, it offers a route from mass-production to the customization of the design of computers, as well as a way to grow from 2D to 3D architectures. &lt;br/&gt;&lt;br/&gt;The third (and most speculative) layer asks about the fundamental mechanisms for manipulating information that will be enabled by this agenda. It seeks to apply the insights that will be developed into programming enormous imperfect distributed systems and accessibly fabricating nanoscale structures in order to harness the intrinsic computational capabilities of natural systems. Fundamental to this approach is the conviction that progress towards these long-standing goals</data>
      <data key="e_pgm">1688</data>
      <data key="e_label">122419</data>
      <data key="e_expirationDate">2008-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">122419</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1512" target="n1514">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Center for Bits and Atoms</data>
      <data key="e_abstract">The scaling of information technology has been an active area of inquiry from the outset of its commercial development. Pioneering studies including those of [Moore, 79] and [Keyes, 87] identified the possibility of sustained exponential improvements in key semiconductor device parameters, as well as the significant obstacles that would need to be overcome to maintain such a pace. The enormous intellectual and financial investment into that effort has translated into steady improvements in overall system performance, so that now more and more computer applications are less and less resource-bound. But what&apos;s typically been missed in forecasts such as the Semiconductor Industry Association&apos;s influential roadmap is a projection of the relevance of the whole scaling effort. A narrow focus on improving device performance ignores the importance of the context in which computers are used, which is leading to very real scaling limits that are among the most serious obstacles to further progress. These include the economics of producing both chips and chip fabs, and the difficulty of designing and managing very large-scale systems. Beyond their practical significance, these issues present some of the most profound research questions in all of information technology, but they are questions that crucially cut across traditional discipline boundaries. Most importantly, it is no longer possible to maintain the fiction that developing hardware can be neatly separated from developing software. &lt;br/&gt;&lt;br/&gt;The Center for Bits and Atoms is an ambitious attempt to close this historical divide by bringing together the resources required to simultaneously study the content of information and its physical properties, on length scales from atomic nuclei to global networks. It aims to develop architectures for scaling information technology appropriate to each of these levels of description, and through a network of partnerships deploy these capabilities for the greatest global impact. Along the way, it seeks to fundamentally revisit the notion of what is a computer, and what is a computation. The CBA&apos;s program is based on the belief that the most significant of all the obstacles to progress has been the isolation of the investigation of each these pieces from that of the larger whole that they promise to enable. &lt;br/&gt;&lt;br/&gt;The research agenda is organized into three layers, in order of accessibility and importance. The first of these addresses system-level questions, asking how to extend networks of (relatively) conventional processors up to and beyond billions of interacting entities. Such coming complexity is being driven by countless practical applications, but will break the existing protocols used to operate the Internet as well as the techniques used for managing it. The approach taken here will be to &quot;de-layer&quot; the divisions between physical transport, logical connection, and application implementation, so that when devices are connected they simultaneously create a network, a distributed data structure, and the computer to manipulate it. The algorithms for processing and routing information are crucially assembled as the components are assembled, and autonomously adapt as nodes come and go, so that scalability is literally built in as the system grows. De-layering also beneficially exposes the capabilities of low-level devices to high-level applications (and vice versa), so that rich interfaces such as sensor networks can become the norm rather than the exception. &lt;br/&gt;&lt;br/&gt;The second layer builds on this system-level insight to ask about technologies to meet the demand for embedding billions of computers into everyday objects. Even though the cost per transistor has fallen exponentially for decades, the minimum cost per packaged part has remained relatively unchanged over the whole VLSI scaling era. For such large-scale systems to be compatible with the global GDP, it&apos;s necessary to fundamentally rethink the nature of device fabrication. The approach in the CBA will be to seek to eliminate central chip fabs entirely, using table-top printing technologies to move the production of computers to where and when they are needed. The fundamental enabling insight that makes this possible is the use of nanocrystalline electronically-active inks. Not only does this promise to dramatically reduce the cost per part, it offers a route from mass-production to the customization of the design of computers, as well as a way to grow from 2D to 3D architectures. &lt;br/&gt;&lt;br/&gt;The third (and most speculative) layer asks about the fundamental mechanisms for manipulating information that will be enabled by this agenda. It seeks to apply the insights that will be developed into programming enormous imperfect distributed systems and accessibly fabricating nanoscale structures in order to harness the intrinsic computational capabilities of natural systems. Fundamental to this approach is the conviction that progress towards these long-standing goals</data>
      <data key="e_pgm">1688</data>
      <data key="e_label">122419</data>
      <data key="e_expirationDate">2008-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">122419</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1512" target="n1515">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Center for Bits and Atoms</data>
      <data key="e_abstract">The scaling of information technology has been an active area of inquiry from the outset of its commercial development. Pioneering studies including those of [Moore, 79] and [Keyes, 87] identified the possibility of sustained exponential improvements in key semiconductor device parameters, as well as the significant obstacles that would need to be overcome to maintain such a pace. The enormous intellectual and financial investment into that effort has translated into steady improvements in overall system performance, so that now more and more computer applications are less and less resource-bound. But what&apos;s typically been missed in forecasts such as the Semiconductor Industry Association&apos;s influential roadmap is a projection of the relevance of the whole scaling effort. A narrow focus on improving device performance ignores the importance of the context in which computers are used, which is leading to very real scaling limits that are among the most serious obstacles to further progress. These include the economics of producing both chips and chip fabs, and the difficulty of designing and managing very large-scale systems. Beyond their practical significance, these issues present some of the most profound research questions in all of information technology, but they are questions that crucially cut across traditional discipline boundaries. Most importantly, it is no longer possible to maintain the fiction that developing hardware can be neatly separated from developing software. &lt;br/&gt;&lt;br/&gt;The Center for Bits and Atoms is an ambitious attempt to close this historical divide by bringing together the resources required to simultaneously study the content of information and its physical properties, on length scales from atomic nuclei to global networks. It aims to develop architectures for scaling information technology appropriate to each of these levels of description, and through a network of partnerships deploy these capabilities for the greatest global impact. Along the way, it seeks to fundamentally revisit the notion of what is a computer, and what is a computation. The CBA&apos;s program is based on the belief that the most significant of all the obstacles to progress has been the isolation of the investigation of each these pieces from that of the larger whole that they promise to enable. &lt;br/&gt;&lt;br/&gt;The research agenda is organized into three layers, in order of accessibility and importance. The first of these addresses system-level questions, asking how to extend networks of (relatively) conventional processors up to and beyond billions of interacting entities. Such coming complexity is being driven by countless practical applications, but will break the existing protocols used to operate the Internet as well as the techniques used for managing it. The approach taken here will be to &quot;de-layer&quot; the divisions between physical transport, logical connection, and application implementation, so that when devices are connected they simultaneously create a network, a distributed data structure, and the computer to manipulate it. The algorithms for processing and routing information are crucially assembled as the components are assembled, and autonomously adapt as nodes come and go, so that scalability is literally built in as the system grows. De-layering also beneficially exposes the capabilities of low-level devices to high-level applications (and vice versa), so that rich interfaces such as sensor networks can become the norm rather than the exception. &lt;br/&gt;&lt;br/&gt;The second layer builds on this system-level insight to ask about technologies to meet the demand for embedding billions of computers into everyday objects. Even though the cost per transistor has fallen exponentially for decades, the minimum cost per packaged part has remained relatively unchanged over the whole VLSI scaling era. For such large-scale systems to be compatible with the global GDP, it&apos;s necessary to fundamentally rethink the nature of device fabrication. The approach in the CBA will be to seek to eliminate central chip fabs entirely, using table-top printing technologies to move the production of computers to where and when they are needed. The fundamental enabling insight that makes this possible is the use of nanocrystalline electronically-active inks. Not only does this promise to dramatically reduce the cost per part, it offers a route from mass-production to the customization of the design of computers, as well as a way to grow from 2D to 3D architectures. &lt;br/&gt;&lt;br/&gt;The third (and most speculative) layer asks about the fundamental mechanisms for manipulating information that will be enabled by this agenda. It seeks to apply the insights that will be developed into programming enormous imperfect distributed systems and accessibly fabricating nanoscale structures in order to harness the intrinsic computational capabilities of natural systems. Fundamental to this approach is the conviction that progress towards these long-standing goals</data>
      <data key="e_pgm">1688</data>
      <data key="e_label">122419</data>
      <data key="e_expirationDate">2008-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">122419</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1513" target="n1514">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Center for Bits and Atoms</data>
      <data key="e_abstract">The scaling of information technology has been an active area of inquiry from the outset of its commercial development. Pioneering studies including those of [Moore, 79] and [Keyes, 87] identified the possibility of sustained exponential improvements in key semiconductor device parameters, as well as the significant obstacles that would need to be overcome to maintain such a pace. The enormous intellectual and financial investment into that effort has translated into steady improvements in overall system performance, so that now more and more computer applications are less and less resource-bound. But what&apos;s typically been missed in forecasts such as the Semiconductor Industry Association&apos;s influential roadmap is a projection of the relevance of the whole scaling effort. A narrow focus on improving device performance ignores the importance of the context in which computers are used, which is leading to very real scaling limits that are among the most serious obstacles to further progress. These include the economics of producing both chips and chip fabs, and the difficulty of designing and managing very large-scale systems. Beyond their practical significance, these issues present some of the most profound research questions in all of information technology, but they are questions that crucially cut across traditional discipline boundaries. Most importantly, it is no longer possible to maintain the fiction that developing hardware can be neatly separated from developing software. &lt;br/&gt;&lt;br/&gt;The Center for Bits and Atoms is an ambitious attempt to close this historical divide by bringing together the resources required to simultaneously study the content of information and its physical properties, on length scales from atomic nuclei to global networks. It aims to develop architectures for scaling information technology appropriate to each of these levels of description, and through a network of partnerships deploy these capabilities for the greatest global impact. Along the way, it seeks to fundamentally revisit the notion of what is a computer, and what is a computation. The CBA&apos;s program is based on the belief that the most significant of all the obstacles to progress has been the isolation of the investigation of each these pieces from that of the larger whole that they promise to enable. &lt;br/&gt;&lt;br/&gt;The research agenda is organized into three layers, in order of accessibility and importance. The first of these addresses system-level questions, asking how to extend networks of (relatively) conventional processors up to and beyond billions of interacting entities. Such coming complexity is being driven by countless practical applications, but will break the existing protocols used to operate the Internet as well as the techniques used for managing it. The approach taken here will be to &quot;de-layer&quot; the divisions between physical transport, logical connection, and application implementation, so that when devices are connected they simultaneously create a network, a distributed data structure, and the computer to manipulate it. The algorithms for processing and routing information are crucially assembled as the components are assembled, and autonomously adapt as nodes come and go, so that scalability is literally built in as the system grows. De-layering also beneficially exposes the capabilities of low-level devices to high-level applications (and vice versa), so that rich interfaces such as sensor networks can become the norm rather than the exception. &lt;br/&gt;&lt;br/&gt;The second layer builds on this system-level insight to ask about technologies to meet the demand for embedding billions of computers into everyday objects. Even though the cost per transistor has fallen exponentially for decades, the minimum cost per packaged part has remained relatively unchanged over the whole VLSI scaling era. For such large-scale systems to be compatible with the global GDP, it&apos;s necessary to fundamentally rethink the nature of device fabrication. The approach in the CBA will be to seek to eliminate central chip fabs entirely, using table-top printing technologies to move the production of computers to where and when they are needed. The fundamental enabling insight that makes this possible is the use of nanocrystalline electronically-active inks. Not only does this promise to dramatically reduce the cost per part, it offers a route from mass-production to the customization of the design of computers, as well as a way to grow from 2D to 3D architectures. &lt;br/&gt;&lt;br/&gt;The third (and most speculative) layer asks about the fundamental mechanisms for manipulating information that will be enabled by this agenda. It seeks to apply the insights that will be developed into programming enormous imperfect distributed systems and accessibly fabricating nanoscale structures in order to harness the intrinsic computational capabilities of natural systems. Fundamental to this approach is the conviction that progress towards these long-standing goals</data>
      <data key="e_pgm">1688</data>
      <data key="e_label">122419</data>
      <data key="e_expirationDate">2008-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">122419</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1513" target="n1515">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Center for Bits and Atoms</data>
      <data key="e_abstract">The scaling of information technology has been an active area of inquiry from the outset of its commercial development. Pioneering studies including those of [Moore, 79] and [Keyes, 87] identified the possibility of sustained exponential improvements in key semiconductor device parameters, as well as the significant obstacles that would need to be overcome to maintain such a pace. The enormous intellectual and financial investment into that effort has translated into steady improvements in overall system performance, so that now more and more computer applications are less and less resource-bound. But what&apos;s typically been missed in forecasts such as the Semiconductor Industry Association&apos;s influential roadmap is a projection of the relevance of the whole scaling effort. A narrow focus on improving device performance ignores the importance of the context in which computers are used, which is leading to very real scaling limits that are among the most serious obstacles to further progress. These include the economics of producing both chips and chip fabs, and the difficulty of designing and managing very large-scale systems. Beyond their practical significance, these issues present some of the most profound research questions in all of information technology, but they are questions that crucially cut across traditional discipline boundaries. Most importantly, it is no longer possible to maintain the fiction that developing hardware can be neatly separated from developing software. &lt;br/&gt;&lt;br/&gt;The Center for Bits and Atoms is an ambitious attempt to close this historical divide by bringing together the resources required to simultaneously study the content of information and its physical properties, on length scales from atomic nuclei to global networks. It aims to develop architectures for scaling information technology appropriate to each of these levels of description, and through a network of partnerships deploy these capabilities for the greatest global impact. Along the way, it seeks to fundamentally revisit the notion of what is a computer, and what is a computation. The CBA&apos;s program is based on the belief that the most significant of all the obstacles to progress has been the isolation of the investigation of each these pieces from that of the larger whole that they promise to enable. &lt;br/&gt;&lt;br/&gt;The research agenda is organized into three layers, in order of accessibility and importance. The first of these addresses system-level questions, asking how to extend networks of (relatively) conventional processors up to and beyond billions of interacting entities. Such coming complexity is being driven by countless practical applications, but will break the existing protocols used to operate the Internet as well as the techniques used for managing it. The approach taken here will be to &quot;de-layer&quot; the divisions between physical transport, logical connection, and application implementation, so that when devices are connected they simultaneously create a network, a distributed data structure, and the computer to manipulate it. The algorithms for processing and routing information are crucially assembled as the components are assembled, and autonomously adapt as nodes come and go, so that scalability is literally built in as the system grows. De-layering also beneficially exposes the capabilities of low-level devices to high-level applications (and vice versa), so that rich interfaces such as sensor networks can become the norm rather than the exception. &lt;br/&gt;&lt;br/&gt;The second layer builds on this system-level insight to ask about technologies to meet the demand for embedding billions of computers into everyday objects. Even though the cost per transistor has fallen exponentially for decades, the minimum cost per packaged part has remained relatively unchanged over the whole VLSI scaling era. For such large-scale systems to be compatible with the global GDP, it&apos;s necessary to fundamentally rethink the nature of device fabrication. The approach in the CBA will be to seek to eliminate central chip fabs entirely, using table-top printing technologies to move the production of computers to where and when they are needed. The fundamental enabling insight that makes this possible is the use of nanocrystalline electronically-active inks. Not only does this promise to dramatically reduce the cost per part, it offers a route from mass-production to the customization of the design of computers, as well as a way to grow from 2D to 3D architectures. &lt;br/&gt;&lt;br/&gt;The third (and most speculative) layer asks about the fundamental mechanisms for manipulating information that will be enabled by this agenda. It seeks to apply the insights that will be developed into programming enormous imperfect distributed systems and accessibly fabricating nanoscale structures in order to harness the intrinsic computational capabilities of natural systems. Fundamental to this approach is the conviction that progress towards these long-standing goals</data>
      <data key="e_pgm">1688</data>
      <data key="e_label">122419</data>
      <data key="e_expirationDate">2008-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">122419</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1514" target="n1515">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Center for Bits and Atoms</data>
      <data key="e_abstract">The scaling of information technology has been an active area of inquiry from the outset of its commercial development. Pioneering studies including those of [Moore, 79] and [Keyes, 87] identified the possibility of sustained exponential improvements in key semiconductor device parameters, as well as the significant obstacles that would need to be overcome to maintain such a pace. The enormous intellectual and financial investment into that effort has translated into steady improvements in overall system performance, so that now more and more computer applications are less and less resource-bound. But what&apos;s typically been missed in forecasts such as the Semiconductor Industry Association&apos;s influential roadmap is a projection of the relevance of the whole scaling effort. A narrow focus on improving device performance ignores the importance of the context in which computers are used, which is leading to very real scaling limits that are among the most serious obstacles to further progress. These include the economics of producing both chips and chip fabs, and the difficulty of designing and managing very large-scale systems. Beyond their practical significance, these issues present some of the most profound research questions in all of information technology, but they are questions that crucially cut across traditional discipline boundaries. Most importantly, it is no longer possible to maintain the fiction that developing hardware can be neatly separated from developing software. &lt;br/&gt;&lt;br/&gt;The Center for Bits and Atoms is an ambitious attempt to close this historical divide by bringing together the resources required to simultaneously study the content of information and its physical properties, on length scales from atomic nuclei to global networks. It aims to develop architectures for scaling information technology appropriate to each of these levels of description, and through a network of partnerships deploy these capabilities for the greatest global impact. Along the way, it seeks to fundamentally revisit the notion of what is a computer, and what is a computation. The CBA&apos;s program is based on the belief that the most significant of all the obstacles to progress has been the isolation of the investigation of each these pieces from that of the larger whole that they promise to enable. &lt;br/&gt;&lt;br/&gt;The research agenda is organized into three layers, in order of accessibility and importance. The first of these addresses system-level questions, asking how to extend networks of (relatively) conventional processors up to and beyond billions of interacting entities. Such coming complexity is being driven by countless practical applications, but will break the existing protocols used to operate the Internet as well as the techniques used for managing it. The approach taken here will be to &quot;de-layer&quot; the divisions between physical transport, logical connection, and application implementation, so that when devices are connected they simultaneously create a network, a distributed data structure, and the computer to manipulate it. The algorithms for processing and routing information are crucially assembled as the components are assembled, and autonomously adapt as nodes come and go, so that scalability is literally built in as the system grows. De-layering also beneficially exposes the capabilities of low-level devices to high-level applications (and vice versa), so that rich interfaces such as sensor networks can become the norm rather than the exception. &lt;br/&gt;&lt;br/&gt;The second layer builds on this system-level insight to ask about technologies to meet the demand for embedding billions of computers into everyday objects. Even though the cost per transistor has fallen exponentially for decades, the minimum cost per packaged part has remained relatively unchanged over the whole VLSI scaling era. For such large-scale systems to be compatible with the global GDP, it&apos;s necessary to fundamentally rethink the nature of device fabrication. The approach in the CBA will be to seek to eliminate central chip fabs entirely, using table-top printing technologies to move the production of computers to where and when they are needed. The fundamental enabling insight that makes this possible is the use of nanocrystalline electronically-active inks. Not only does this promise to dramatically reduce the cost per part, it offers a route from mass-production to the customization of the design of computers, as well as a way to grow from 2D to 3D architectures. &lt;br/&gt;&lt;br/&gt;The third (and most speculative) layer asks about the fundamental mechanisms for manipulating information that will be enabled by this agenda. It seeks to apply the insights that will be developed into programming enormous imperfect distributed systems and accessibly fabricating nanoscale structures in order to harness the intrinsic computational capabilities of natural systems. Fundamental to this approach is the conviction that progress towards these long-standing goals</data>
      <data key="e_pgm">1688</data>
      <data key="e_label">122419</data>
      <data key="e_expirationDate">2008-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">122419</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1518" target="n1519">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">CISE Research Resources: Instrumentation for Multidimensional Imaging and Applications</data>
      <data key="e_abstract">0130839 &lt;br/&gt;Patrick Flynn&lt;br/&gt;University of Notre Dame&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Instrumentation for Multidimensional Imaging and Applications&lt;br/&gt;&lt;br/&gt;The award contributes to the purchase of (a) specialized sensors and actuators (range scanners, digital video facilities, 3D fabrication equipment), (b) high-performance computing equipment (a Beowulf cluster and high-performance workstations), and (c) infrastructure (including a network-attached disk array and networking hardware). The proposed equipment will support current and planned research in image processing and understanding. Research in planning and currently underway that will employ the facility includes image and video processing (range image compression techniques, image compression artifact mitigation, and high-resolution video from sensor fusion), image analysis and solid modeling (image segmentation, skeletonization, object representation, and applications in orthopedics and gait analysis), and empirical evaluation of image understanding algorithms (performance of low-level and high-level image understanding tasks as well as 3D modeling techniques). The planned facility will foster the continued development of a nascent image processing and understanding research group at the University of Notre Dame, and the research topics to be supported were carefully examined to validate their ability to take advantage of the proposed equipment acquisition.</data>
      <data key="e_pgm">S075</data>
      <data key="e_label">130839</data>
      <data key="e_expirationDate">2009-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130839</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1518" target="n1520">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">CISE Research Resources: Instrumentation for Multidimensional Imaging and Applications</data>
      <data key="e_abstract">0130839 &lt;br/&gt;Patrick Flynn&lt;br/&gt;University of Notre Dame&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Instrumentation for Multidimensional Imaging and Applications&lt;br/&gt;&lt;br/&gt;The award contributes to the purchase of (a) specialized sensors and actuators (range scanners, digital video facilities, 3D fabrication equipment), (b) high-performance computing equipment (a Beowulf cluster and high-performance workstations), and (c) infrastructure (including a network-attached disk array and networking hardware). The proposed equipment will support current and planned research in image processing and understanding. Research in planning and currently underway that will employ the facility includes image and video processing (range image compression techniques, image compression artifact mitigation, and high-resolution video from sensor fusion), image analysis and solid modeling (image segmentation, skeletonization, object representation, and applications in orthopedics and gait analysis), and empirical evaluation of image understanding algorithms (performance of low-level and high-level image understanding tasks as well as 3D modeling techniques). The planned facility will foster the continued development of a nascent image processing and understanding research group at the University of Notre Dame, and the research topics to be supported were carefully examined to validate their ability to take advantage of the proposed equipment acquisition.</data>
      <data key="e_pgm">S075</data>
      <data key="e_label">130839</data>
      <data key="e_expirationDate">2009-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130839</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1518" target="n1521">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">CISE Research Resources: Instrumentation for Multidimensional Imaging and Applications</data>
      <data key="e_abstract">0130839 &lt;br/&gt;Patrick Flynn&lt;br/&gt;University of Notre Dame&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Instrumentation for Multidimensional Imaging and Applications&lt;br/&gt;&lt;br/&gt;The award contributes to the purchase of (a) specialized sensors and actuators (range scanners, digital video facilities, 3D fabrication equipment), (b) high-performance computing equipment (a Beowulf cluster and high-performance workstations), and (c) infrastructure (including a network-attached disk array and networking hardware). The proposed equipment will support current and planned research in image processing and understanding. Research in planning and currently underway that will employ the facility includes image and video processing (range image compression techniques, image compression artifact mitigation, and high-resolution video from sensor fusion), image analysis and solid modeling (image segmentation, skeletonization, object representation, and applications in orthopedics and gait analysis), and empirical evaluation of image understanding algorithms (performance of low-level and high-level image understanding tasks as well as 3D modeling techniques). The planned facility will foster the continued development of a nascent image processing and understanding research group at the University of Notre Dame, and the research topics to be supported were carefully examined to validate their ability to take advantage of the proposed equipment acquisition.</data>
      <data key="e_pgm">S075</data>
      <data key="e_label">130839</data>
      <data key="e_expirationDate">2009-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130839</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1519" target="n1520">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">CISE Research Resources: Instrumentation for Multidimensional Imaging and Applications</data>
      <data key="e_abstract">0130839 &lt;br/&gt;Patrick Flynn&lt;br/&gt;University of Notre Dame&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Instrumentation for Multidimensional Imaging and Applications&lt;br/&gt;&lt;br/&gt;The award contributes to the purchase of (a) specialized sensors and actuators (range scanners, digital video facilities, 3D fabrication equipment), (b) high-performance computing equipment (a Beowulf cluster and high-performance workstations), and (c) infrastructure (including a network-attached disk array and networking hardware). The proposed equipment will support current and planned research in image processing and understanding. Research in planning and currently underway that will employ the facility includes image and video processing (range image compression techniques, image compression artifact mitigation, and high-resolution video from sensor fusion), image analysis and solid modeling (image segmentation, skeletonization, object representation, and applications in orthopedics and gait analysis), and empirical evaluation of image understanding algorithms (performance of low-level and high-level image understanding tasks as well as 3D modeling techniques). The planned facility will foster the continued development of a nascent image processing and understanding research group at the University of Notre Dame, and the research topics to be supported were carefully examined to validate their ability to take advantage of the proposed equipment acquisition.</data>
      <data key="e_pgm">S075</data>
      <data key="e_label">130839</data>
      <data key="e_expirationDate">2009-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130839</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1519" target="n1521">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">CISE Research Resources: Instrumentation for Multidimensional Imaging and Applications</data>
      <data key="e_abstract">0130839 &lt;br/&gt;Patrick Flynn&lt;br/&gt;University of Notre Dame&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Instrumentation for Multidimensional Imaging and Applications&lt;br/&gt;&lt;br/&gt;The award contributes to the purchase of (a) specialized sensors and actuators (range scanners, digital video facilities, 3D fabrication equipment), (b) high-performance computing equipment (a Beowulf cluster and high-performance workstations), and (c) infrastructure (including a network-attached disk array and networking hardware). The proposed equipment will support current and planned research in image processing and understanding. Research in planning and currently underway that will employ the facility includes image and video processing (range image compression techniques, image compression artifact mitigation, and high-resolution video from sensor fusion), image analysis and solid modeling (image segmentation, skeletonization, object representation, and applications in orthopedics and gait analysis), and empirical evaluation of image understanding algorithms (performance of low-level and high-level image understanding tasks as well as 3D modeling techniques). The planned facility will foster the continued development of a nascent image processing and understanding research group at the University of Notre Dame, and the research topics to be supported were carefully examined to validate their ability to take advantage of the proposed equipment acquisition.</data>
      <data key="e_pgm">S075</data>
      <data key="e_label">130839</data>
      <data key="e_expirationDate">2009-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130839</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1520" target="n1521">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">CISE Research Resources: Instrumentation for Multidimensional Imaging and Applications</data>
      <data key="e_abstract">0130839 &lt;br/&gt;Patrick Flynn&lt;br/&gt;University of Notre Dame&lt;br/&gt;&lt;br/&gt;CISE Research Resources: Instrumentation for Multidimensional Imaging and Applications&lt;br/&gt;&lt;br/&gt;The award contributes to the purchase of (a) specialized sensors and actuators (range scanners, digital video facilities, 3D fabrication equipment), (b) high-performance computing equipment (a Beowulf cluster and high-performance workstations), and (c) infrastructure (including a network-attached disk array and networking hardware). The proposed equipment will support current and planned research in image processing and understanding. Research in planning and currently underway that will employ the facility includes image and video processing (range image compression techniques, image compression artifact mitigation, and high-resolution video from sensor fusion), image analysis and solid modeling (image segmentation, skeletonization, object representation, and applications in orthopedics and gait analysis), and empirical evaluation of image understanding algorithms (performance of low-level and high-level image understanding tasks as well as 3D modeling techniques). The planned facility will foster the continued development of a nascent image processing and understanding research group at the University of Notre Dame, and the research topics to be supported were carefully examined to validate their ability to take advantage of the proposed equipment acquisition.</data>
      <data key="e_pgm">S075</data>
      <data key="e_label">130839</data>
      <data key="e_expirationDate">2009-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130839</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n907" target="n1522">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: A New Framework For Program Optimization</data>
      <data key="e_abstract">The objective of this project is to develop a methodology to design the optimization component of a compiler that learns from experience. The strategy to be studied involves the use of an explanation-based learning (EBL) sub-system that, based on analytical and empirical information, will generate policies to control the optimization component of the compiler. The analytical information will relate program characteristics to performance. The empirical information will be obtained by profiling the program and will be stored in a database containing information from earlier versions of the program and from other programs in the same problem domain.&lt;br/&gt;An experimental compiler will be implemented to evaluate the methodolog,. The core of the compiler will be a translator controlled by parameters that could be selected from a standard collection, in which case the compiler will behave like a conventional compiler, or be generated by the machine learning sub-system.&lt;br/&gt;Specific topics to be studied as part of this project include: compiler organization, program transformations and their interaction, performance prediction based on both static and dynamic information, machine learning techniques and the integration of both prior knowledge (performance abstractions in our case) and empirical data, and context-adaptive computing systems.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121401</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121401</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n263" target="n1522">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: A New Framework For Program Optimization</data>
      <data key="e_abstract">The objective of this project is to develop a methodology to design the optimization component of a compiler that learns from experience. The strategy to be studied involves the use of an explanation-based learning (EBL) sub-system that, based on analytical and empirical information, will generate policies to control the optimization component of the compiler. The analytical information will relate program characteristics to performance. The empirical information will be obtained by profiling the program and will be stored in a database containing information from earlier versions of the program and from other programs in the same problem domain.&lt;br/&gt;An experimental compiler will be implemented to evaluate the methodolog,. The core of the compiler will be a translator controlled by parameters that could be selected from a standard collection, in which case the compiler will behave like a conventional compiler, or be generated by the machine learning sub-system.&lt;br/&gt;Specific topics to be studied as part of this project include: compiler organization, program transformations and their interaction, performance prediction based on both static and dynamic information, machine learning techniques and the integration of both prior knowledge (performance abstractions in our case) and empirical data, and context-adaptive computing systems.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121401</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121401</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n263" target="n907">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: A New Framework For Program Optimization</data>
      <data key="e_abstract">The objective of this project is to develop a methodology to design the optimization component of a compiler that learns from experience. The strategy to be studied involves the use of an explanation-based learning (EBL) sub-system that, based on analytical and empirical information, will generate policies to control the optimization component of the compiler. The analytical information will relate program characteristics to performance. The empirical information will be obtained by profiling the program and will be stored in a database containing information from earlier versions of the program and from other programs in the same problem domain.&lt;br/&gt;An experimental compiler will be implemented to evaluate the methodolog,. The core of the compiler will be a translator controlled by parameters that could be selected from a standard collection, in which case the compiler will behave like a conventional compiler, or be generated by the machine learning sub-system.&lt;br/&gt;Specific topics to be studied as part of this project include: compiler organization, program transformations and their interaction, performance prediction based on both static and dynamic information, machine learning techniques and the integration of both prior knowledge (performance abstractions in our case) and empirical data, and context-adaptive computing systems.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121401</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121401</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1525" target="n1526">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR: Digital Government: Communications Technology and Urban Governance Reform</data>
      <data key="e_abstract">EIA: 0112899&lt;br/&gt;Christopher Weare&lt;br/&gt;University of Southern California&lt;br/&gt;&lt;br/&gt;Title: ITR/IM: Communications Technology and Urban Governance Reform&lt;br/&gt;&lt;br/&gt;The project will analyze the effects of advanced telecommunications technologies on reform efforts seeking to decentralize democratic governance. Analysis will focus on technology use in the implementation of a formal neighborhood council system in the City of Los Angeles. This reform effort represents a significant and well-defined natural experiment in the political uses of information and communications technologies (ICTs). As such, it provides an opportunity to extend the technology and politics literature, which to date has focused on the national level and elections, to investigate effects on local governance, which is the primary sphere for civic engagement in policy making. The research will draw from theories in the area of political communication, organizational theory and urban political economy to develop hypotheses about the factors that influence the successful integration of ICTs in the creation of neighborhood council systems, their effects on patterns of political communication, and the effects of communications networks on political outcomes and attitudes among stakeholders at the local level. &lt;br/&gt;&lt;br/&gt;The implementation of neighborhood councils in Los Angeles is currently in the planning process and will proceed beginning June 2001. The current plan calls for the creation of 100 to 200 advisory councils at the community level, and the implementation of an Internet-based early notification system intended to provide councils early input into city policy making processes. In addition, councils are required to create systems for communication with community stakeholders. The City in turn is mandated to provide councils with a communication system linking all neighborhood councils and training and material support for communications. This reform represents an unprecedented urban experiment with political communication, and is particularly important given the size, ethnic diversity, and global economic position of the City of Los Angeles. The research will employ multiple methods to elucidate the complex linkages between implementation inputs, communication networks, political processes, and policy and attitudinal outcomes. The methods employed will include (1) case study analysis of the design, implementation, and diffusion of neighborhood councils and communications technology; (2) analysis of the vertical communications promulgated and received by elected officials as a part of the mandated early notification system; (3) network analysis of the dynamics of communications within and between neighborhood councils and city government; and (4) preliminary analysis of the political effects of vertical and horizontal communications.&lt;br/&gt;&lt;br/&gt;Data collection will include archival and field research, interviews, and two panel surveys. These data will be employed in diffusion analyses and network analyses producing sociograms of linkages and cliques developing among members of city government and neighborhood councils. The network analysis provides a systematic framework for conceptualizing and analyzing the effects of ICTs on communication patterns and the manner in which changing communication patterns influence democratic outcomes.&lt;br/&gt;This analysis will illuminate the effects of ICTs on the distribution of information and political communications and the resulting effects of changing communication patterns on citizen participation and the responsiveness of political institutions. The research is expected to improve understanding of the technological factors that impede or promote political participation by traditionally underrepresented groups and provide a more detailed understanding of socio-economic and communicative characteristics of citizens impede them from taking advantage of technology-based reforms. This knowledge will help policy makers develop methods by which these differences may be bridged.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112899</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112899</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1525" target="n1527">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR: Digital Government: Communications Technology and Urban Governance Reform</data>
      <data key="e_abstract">EIA: 0112899&lt;br/&gt;Christopher Weare&lt;br/&gt;University of Southern California&lt;br/&gt;&lt;br/&gt;Title: ITR/IM: Communications Technology and Urban Governance Reform&lt;br/&gt;&lt;br/&gt;The project will analyze the effects of advanced telecommunications technologies on reform efforts seeking to decentralize democratic governance. Analysis will focus on technology use in the implementation of a formal neighborhood council system in the City of Los Angeles. This reform effort represents a significant and well-defined natural experiment in the political uses of information and communications technologies (ICTs). As such, it provides an opportunity to extend the technology and politics literature, which to date has focused on the national level and elections, to investigate effects on local governance, which is the primary sphere for civic engagement in policy making. The research will draw from theories in the area of political communication, organizational theory and urban political economy to develop hypotheses about the factors that influence the successful integration of ICTs in the creation of neighborhood council systems, their effects on patterns of political communication, and the effects of communications networks on political outcomes and attitudes among stakeholders at the local level. &lt;br/&gt;&lt;br/&gt;The implementation of neighborhood councils in Los Angeles is currently in the planning process and will proceed beginning June 2001. The current plan calls for the creation of 100 to 200 advisory councils at the community level, and the implementation of an Internet-based early notification system intended to provide councils early input into city policy making processes. In addition, councils are required to create systems for communication with community stakeholders. The City in turn is mandated to provide councils with a communication system linking all neighborhood councils and training and material support for communications. This reform represents an unprecedented urban experiment with political communication, and is particularly important given the size, ethnic diversity, and global economic position of the City of Los Angeles. The research will employ multiple methods to elucidate the complex linkages between implementation inputs, communication networks, political processes, and policy and attitudinal outcomes. The methods employed will include (1) case study analysis of the design, implementation, and diffusion of neighborhood councils and communications technology; (2) analysis of the vertical communications promulgated and received by elected officials as a part of the mandated early notification system; (3) network analysis of the dynamics of communications within and between neighborhood councils and city government; and (4) preliminary analysis of the political effects of vertical and horizontal communications.&lt;br/&gt;&lt;br/&gt;Data collection will include archival and field research, interviews, and two panel surveys. These data will be employed in diffusion analyses and network analyses producing sociograms of linkages and cliques developing among members of city government and neighborhood councils. The network analysis provides a systematic framework for conceptualizing and analyzing the effects of ICTs on communication patterns and the manner in which changing communication patterns influence democratic outcomes.&lt;br/&gt;This analysis will illuminate the effects of ICTs on the distribution of information and political communications and the resulting effects of changing communication patterns on citizen participation and the responsiveness of political institutions. The research is expected to improve understanding of the technological factors that impede or promote political participation by traditionally underrepresented groups and provide a more detailed understanding of socio-economic and communicative characteristics of citizens impede them from taking advantage of technology-based reforms. This knowledge will help policy makers develop methods by which these differences may be bridged.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112899</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112899</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1525" target="n1528">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR: Digital Government: Communications Technology and Urban Governance Reform</data>
      <data key="e_abstract">EIA: 0112899&lt;br/&gt;Christopher Weare&lt;br/&gt;University of Southern California&lt;br/&gt;&lt;br/&gt;Title: ITR/IM: Communications Technology and Urban Governance Reform&lt;br/&gt;&lt;br/&gt;The project will analyze the effects of advanced telecommunications technologies on reform efforts seeking to decentralize democratic governance. Analysis will focus on technology use in the implementation of a formal neighborhood council system in the City of Los Angeles. This reform effort represents a significant and well-defined natural experiment in the political uses of information and communications technologies (ICTs). As such, it provides an opportunity to extend the technology and politics literature, which to date has focused on the national level and elections, to investigate effects on local governance, which is the primary sphere for civic engagement in policy making. The research will draw from theories in the area of political communication, organizational theory and urban political economy to develop hypotheses about the factors that influence the successful integration of ICTs in the creation of neighborhood council systems, their effects on patterns of political communication, and the effects of communications networks on political outcomes and attitudes among stakeholders at the local level. &lt;br/&gt;&lt;br/&gt;The implementation of neighborhood councils in Los Angeles is currently in the planning process and will proceed beginning June 2001. The current plan calls for the creation of 100 to 200 advisory councils at the community level, and the implementation of an Internet-based early notification system intended to provide councils early input into city policy making processes. In addition, councils are required to create systems for communication with community stakeholders. The City in turn is mandated to provide councils with a communication system linking all neighborhood councils and training and material support for communications. This reform represents an unprecedented urban experiment with political communication, and is particularly important given the size, ethnic diversity, and global economic position of the City of Los Angeles. The research will employ multiple methods to elucidate the complex linkages between implementation inputs, communication networks, political processes, and policy and attitudinal outcomes. The methods employed will include (1) case study analysis of the design, implementation, and diffusion of neighborhood councils and communications technology; (2) analysis of the vertical communications promulgated and received by elected officials as a part of the mandated early notification system; (3) network analysis of the dynamics of communications within and between neighborhood councils and city government; and (4) preliminary analysis of the political effects of vertical and horizontal communications.&lt;br/&gt;&lt;br/&gt;Data collection will include archival and field research, interviews, and two panel surveys. These data will be employed in diffusion analyses and network analyses producing sociograms of linkages and cliques developing among members of city government and neighborhood councils. The network analysis provides a systematic framework for conceptualizing and analyzing the effects of ICTs on communication patterns and the manner in which changing communication patterns influence democratic outcomes.&lt;br/&gt;This analysis will illuminate the effects of ICTs on the distribution of information and political communications and the resulting effects of changing communication patterns on citizen participation and the responsiveness of political institutions. The research is expected to improve understanding of the technological factors that impede or promote political participation by traditionally underrepresented groups and provide a more detailed understanding of socio-economic and communicative characteristics of citizens impede them from taking advantage of technology-based reforms. This knowledge will help policy makers develop methods by which these differences may be bridged.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112899</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112899</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1526" target="n1527">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR: Digital Government: Communications Technology and Urban Governance Reform</data>
      <data key="e_abstract">EIA: 0112899&lt;br/&gt;Christopher Weare&lt;br/&gt;University of Southern California&lt;br/&gt;&lt;br/&gt;Title: ITR/IM: Communications Technology and Urban Governance Reform&lt;br/&gt;&lt;br/&gt;The project will analyze the effects of advanced telecommunications technologies on reform efforts seeking to decentralize democratic governance. Analysis will focus on technology use in the implementation of a formal neighborhood council system in the City of Los Angeles. This reform effort represents a significant and well-defined natural experiment in the political uses of information and communications technologies (ICTs). As such, it provides an opportunity to extend the technology and politics literature, which to date has focused on the national level and elections, to investigate effects on local governance, which is the primary sphere for civic engagement in policy making. The research will draw from theories in the area of political communication, organizational theory and urban political economy to develop hypotheses about the factors that influence the successful integration of ICTs in the creation of neighborhood council systems, their effects on patterns of political communication, and the effects of communications networks on political outcomes and attitudes among stakeholders at the local level. &lt;br/&gt;&lt;br/&gt;The implementation of neighborhood councils in Los Angeles is currently in the planning process and will proceed beginning June 2001. The current plan calls for the creation of 100 to 200 advisory councils at the community level, and the implementation of an Internet-based early notification system intended to provide councils early input into city policy making processes. In addition, councils are required to create systems for communication with community stakeholders. The City in turn is mandated to provide councils with a communication system linking all neighborhood councils and training and material support for communications. This reform represents an unprecedented urban experiment with political communication, and is particularly important given the size, ethnic diversity, and global economic position of the City of Los Angeles. The research will employ multiple methods to elucidate the complex linkages between implementation inputs, communication networks, political processes, and policy and attitudinal outcomes. The methods employed will include (1) case study analysis of the design, implementation, and diffusion of neighborhood councils and communications technology; (2) analysis of the vertical communications promulgated and received by elected officials as a part of the mandated early notification system; (3) network analysis of the dynamics of communications within and between neighborhood councils and city government; and (4) preliminary analysis of the political effects of vertical and horizontal communications.&lt;br/&gt;&lt;br/&gt;Data collection will include archival and field research, interviews, and two panel surveys. These data will be employed in diffusion analyses and network analyses producing sociograms of linkages and cliques developing among members of city government and neighborhood councils. The network analysis provides a systematic framework for conceptualizing and analyzing the effects of ICTs on communication patterns and the manner in which changing communication patterns influence democratic outcomes.&lt;br/&gt;This analysis will illuminate the effects of ICTs on the distribution of information and political communications and the resulting effects of changing communication patterns on citizen participation and the responsiveness of political institutions. The research is expected to improve understanding of the technological factors that impede or promote political participation by traditionally underrepresented groups and provide a more detailed understanding of socio-economic and communicative characteristics of citizens impede them from taking advantage of technology-based reforms. This knowledge will help policy makers develop methods by which these differences may be bridged.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112899</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112899</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1526" target="n1528">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR: Digital Government: Communications Technology and Urban Governance Reform</data>
      <data key="e_abstract">EIA: 0112899&lt;br/&gt;Christopher Weare&lt;br/&gt;University of Southern California&lt;br/&gt;&lt;br/&gt;Title: ITR/IM: Communications Technology and Urban Governance Reform&lt;br/&gt;&lt;br/&gt;The project will analyze the effects of advanced telecommunications technologies on reform efforts seeking to decentralize democratic governance. Analysis will focus on technology use in the implementation of a formal neighborhood council system in the City of Los Angeles. This reform effort represents a significant and well-defined natural experiment in the political uses of information and communications technologies (ICTs). As such, it provides an opportunity to extend the technology and politics literature, which to date has focused on the national level and elections, to investigate effects on local governance, which is the primary sphere for civic engagement in policy making. The research will draw from theories in the area of political communication, organizational theory and urban political economy to develop hypotheses about the factors that influence the successful integration of ICTs in the creation of neighborhood council systems, their effects on patterns of political communication, and the effects of communications networks on political outcomes and attitudes among stakeholders at the local level. &lt;br/&gt;&lt;br/&gt;The implementation of neighborhood councils in Los Angeles is currently in the planning process and will proceed beginning June 2001. The current plan calls for the creation of 100 to 200 advisory councils at the community level, and the implementation of an Internet-based early notification system intended to provide councils early input into city policy making processes. In addition, councils are required to create systems for communication with community stakeholders. The City in turn is mandated to provide councils with a communication system linking all neighborhood councils and training and material support for communications. This reform represents an unprecedented urban experiment with political communication, and is particularly important given the size, ethnic diversity, and global economic position of the City of Los Angeles. The research will employ multiple methods to elucidate the complex linkages between implementation inputs, communication networks, political processes, and policy and attitudinal outcomes. The methods employed will include (1) case study analysis of the design, implementation, and diffusion of neighborhood councils and communications technology; (2) analysis of the vertical communications promulgated and received by elected officials as a part of the mandated early notification system; (3) network analysis of the dynamics of communications within and between neighborhood councils and city government; and (4) preliminary analysis of the political effects of vertical and horizontal communications.&lt;br/&gt;&lt;br/&gt;Data collection will include archival and field research, interviews, and two panel surveys. These data will be employed in diffusion analyses and network analyses producing sociograms of linkages and cliques developing among members of city government and neighborhood councils. The network analysis provides a systematic framework for conceptualizing and analyzing the effects of ICTs on communication patterns and the manner in which changing communication patterns influence democratic outcomes.&lt;br/&gt;This analysis will illuminate the effects of ICTs on the distribution of information and political communications and the resulting effects of changing communication patterns on citizen participation and the responsiveness of political institutions. The research is expected to improve understanding of the technological factors that impede or promote political participation by traditionally underrepresented groups and provide a more detailed understanding of socio-economic and communicative characteristics of citizens impede them from taking advantage of technology-based reforms. This knowledge will help policy makers develop methods by which these differences may be bridged.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112899</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112899</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1527" target="n1528">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR: Digital Government: Communications Technology and Urban Governance Reform</data>
      <data key="e_abstract">EIA: 0112899&lt;br/&gt;Christopher Weare&lt;br/&gt;University of Southern California&lt;br/&gt;&lt;br/&gt;Title: ITR/IM: Communications Technology and Urban Governance Reform&lt;br/&gt;&lt;br/&gt;The project will analyze the effects of advanced telecommunications technologies on reform efforts seeking to decentralize democratic governance. Analysis will focus on technology use in the implementation of a formal neighborhood council system in the City of Los Angeles. This reform effort represents a significant and well-defined natural experiment in the political uses of information and communications technologies (ICTs). As such, it provides an opportunity to extend the technology and politics literature, which to date has focused on the national level and elections, to investigate effects on local governance, which is the primary sphere for civic engagement in policy making. The research will draw from theories in the area of political communication, organizational theory and urban political economy to develop hypotheses about the factors that influence the successful integration of ICTs in the creation of neighborhood council systems, their effects on patterns of political communication, and the effects of communications networks on political outcomes and attitudes among stakeholders at the local level. &lt;br/&gt;&lt;br/&gt;The implementation of neighborhood councils in Los Angeles is currently in the planning process and will proceed beginning June 2001. The current plan calls for the creation of 100 to 200 advisory councils at the community level, and the implementation of an Internet-based early notification system intended to provide councils early input into city policy making processes. In addition, councils are required to create systems for communication with community stakeholders. The City in turn is mandated to provide councils with a communication system linking all neighborhood councils and training and material support for communications. This reform represents an unprecedented urban experiment with political communication, and is particularly important given the size, ethnic diversity, and global economic position of the City of Los Angeles. The research will employ multiple methods to elucidate the complex linkages between implementation inputs, communication networks, political processes, and policy and attitudinal outcomes. The methods employed will include (1) case study analysis of the design, implementation, and diffusion of neighborhood councils and communications technology; (2) analysis of the vertical communications promulgated and received by elected officials as a part of the mandated early notification system; (3) network analysis of the dynamics of communications within and between neighborhood councils and city government; and (4) preliminary analysis of the political effects of vertical and horizontal communications.&lt;br/&gt;&lt;br/&gt;Data collection will include archival and field research, interviews, and two panel surveys. These data will be employed in diffusion analyses and network analyses producing sociograms of linkages and cliques developing among members of city government and neighborhood councils. The network analysis provides a systematic framework for conceptualizing and analyzing the effects of ICTs on communication patterns and the manner in which changing communication patterns influence democratic outcomes.&lt;br/&gt;This analysis will illuminate the effects of ICTs on the distribution of information and political communications and the resulting effects of changing communication patterns on citizen participation and the responsiveness of political institutions. The research is expected to improve understanding of the technological factors that impede or promote political participation by traditionally underrepresented groups and provide a more detailed understanding of socio-economic and communicative characteristics of citizens impede them from taking advantage of technology-based reforms. This knowledge will help policy makers develop methods by which these differences may be bridged.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112899</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112899</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1530" target="n1531">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Management and Processing of Data Streams</data>
      <data key="e_abstract">For many recent applications, the concept of a data stream, possibly infinite, is more appropriate than a data set. By nature, a stored data set is appropriate when significant portions of the data are queried again and again, and updates are small and/or relatively infrequent. In contrast, a data stream is appropriate when the data is changing constantly (often exclusively through insertions of new elements), and it is either unnecessary or impractical to operate on large portions of the data multiple times. The goal of this research project is to develop models and techniques for the management and processing of data streams. Sampling, summarization, and online approximation algorithms will be employed to facilitate query processing and data mining over streams. The results of this project will provide efficient data stream techniques for data management, memory management, query processing, data mining, and data analysis. In addition, a software prototype will be developed for experimentation with algorithms and query processing, and as a testbed for some sample applications of significant scope, such as networking monitoring and traffic engineering, and medical monitoring data.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">118173</data>
      <data key="e_expirationDate">2005-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">118173</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1533" target="n1534">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Research for Mixed Signal Electronic Technologies: A Joint Initiative Between NSF and SRC: Inter-Pulse-Interval Based Mixed Signal Representations</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;Although digital signal representation has become almost universal, there are still many areas where an analog representation is required to interface with an analog world or to meet various other objectives such as power dissipation, frequency, or cost. In these domains analog signal representation is essential for many input modalities such as instrumentation, sensor interfaces, and communications. Likewise, there are related output applications, such as biomedical actuation and industrial control. In addition, the needs of wireless and fiber-optical communication have reinvigorated analog design. However, there are serious problems concerning how to keep these analog components on a reasonable scaling curve as Moore&apos;s law continues unabated in the digital domain, and in integrating analog representations into large, complex digital systems (&quot;system on a chip&quot;).&lt;br/&gt;&lt;br/&gt;The purpose of this proposal is to study a new approach to representing analog signals that we believe will integrate more cleanly into these deep submicron, single-chip systems. Today analog signals are almost exclusively represented by current or voltage quantities. Our proposal is to borrow a page from neuroscience and to use the Inter-Pulse-Interval (IPI) between single-bit, asynchronous pulses to represent analog quantities. We are proposing to develop a mixed-mode analog/digital cell library and design methodology based on IPI representation and the associated computation elements, and to engineer a case study to illustrate the outcomes. &lt;br/&gt;&lt;br/&gt;As we move to deep submicron and then on to the nanometer/molecular devices, the problems that digital encounters with scaling, such as threshold inconsistency, subthreshold currents, hot-electron effects, doping variability, substrate coupling, and transmission line and complex cross-talk effects, are even more serious for analog circuitry. IPI representations will provide significantly better immunity to these effects, as well as to the more traditional process, temperature, and reference voltage variations. For most applications, pulse based analog systems will require less power. There are numerous advantages to using pulses or pulses to communicate. They are significantly more immune to noise. An approximate analogy would be that of AM versus FM radio signal representation. The outcomes of the proposed research are&lt;br/&gt;&lt;br/&gt; Create a library of basic communication, computation (arithmetic and logic) and conversion building blocks;&lt;br/&gt;Design, implementation and testing of a case study using the derived building blocks and methodology;&lt;br/&gt;&lt;br/&gt;Document an IPI-based design methodology</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">120369</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">120369</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1533" target="n1535">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Research for Mixed Signal Electronic Technologies: A Joint Initiative Between NSF and SRC: Inter-Pulse-Interval Based Mixed Signal Representations</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;Although digital signal representation has become almost universal, there are still many areas where an analog representation is required to interface with an analog world or to meet various other objectives such as power dissipation, frequency, or cost. In these domains analog signal representation is essential for many input modalities such as instrumentation, sensor interfaces, and communications. Likewise, there are related output applications, such as biomedical actuation and industrial control. In addition, the needs of wireless and fiber-optical communication have reinvigorated analog design. However, there are serious problems concerning how to keep these analog components on a reasonable scaling curve as Moore&apos;s law continues unabated in the digital domain, and in integrating analog representations into large, complex digital systems (&quot;system on a chip&quot;).&lt;br/&gt;&lt;br/&gt;The purpose of this proposal is to study a new approach to representing analog signals that we believe will integrate more cleanly into these deep submicron, single-chip systems. Today analog signals are almost exclusively represented by current or voltage quantities. Our proposal is to borrow a page from neuroscience and to use the Inter-Pulse-Interval (IPI) between single-bit, asynchronous pulses to represent analog quantities. We are proposing to develop a mixed-mode analog/digital cell library and design methodology based on IPI representation and the associated computation elements, and to engineer a case study to illustrate the outcomes. &lt;br/&gt;&lt;br/&gt;As we move to deep submicron and then on to the nanometer/molecular devices, the problems that digital encounters with scaling, such as threshold inconsistency, subthreshold currents, hot-electron effects, doping variability, substrate coupling, and transmission line and complex cross-talk effects, are even more serious for analog circuitry. IPI representations will provide significantly better immunity to these effects, as well as to the more traditional process, temperature, and reference voltage variations. For most applications, pulse based analog systems will require less power. There are numerous advantages to using pulses or pulses to communicate. They are significantly more immune to noise. An approximate analogy would be that of AM versus FM radio signal representation. The outcomes of the proposed research are&lt;br/&gt;&lt;br/&gt; Create a library of basic communication, computation (arithmetic and logic) and conversion building blocks;&lt;br/&gt;Design, implementation and testing of a case study using the derived building blocks and methodology;&lt;br/&gt;&lt;br/&gt;Document an IPI-based design methodology</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">120369</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">120369</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1534" target="n1535">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Research for Mixed Signal Electronic Technologies: A Joint Initiative Between NSF and SRC: Inter-Pulse-Interval Based Mixed Signal Representations</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;Although digital signal representation has become almost universal, there are still many areas where an analog representation is required to interface with an analog world or to meet various other objectives such as power dissipation, frequency, or cost. In these domains analog signal representation is essential for many input modalities such as instrumentation, sensor interfaces, and communications. Likewise, there are related output applications, such as biomedical actuation and industrial control. In addition, the needs of wireless and fiber-optical communication have reinvigorated analog design. However, there are serious problems concerning how to keep these analog components on a reasonable scaling curve as Moore&apos;s law continues unabated in the digital domain, and in integrating analog representations into large, complex digital systems (&quot;system on a chip&quot;).&lt;br/&gt;&lt;br/&gt;The purpose of this proposal is to study a new approach to representing analog signals that we believe will integrate more cleanly into these deep submicron, single-chip systems. Today analog signals are almost exclusively represented by current or voltage quantities. Our proposal is to borrow a page from neuroscience and to use the Inter-Pulse-Interval (IPI) between single-bit, asynchronous pulses to represent analog quantities. We are proposing to develop a mixed-mode analog/digital cell library and design methodology based on IPI representation and the associated computation elements, and to engineer a case study to illustrate the outcomes. &lt;br/&gt;&lt;br/&gt;As we move to deep submicron and then on to the nanometer/molecular devices, the problems that digital encounters with scaling, such as threshold inconsistency, subthreshold currents, hot-electron effects, doping variability, substrate coupling, and transmission line and complex cross-talk effects, are even more serious for analog circuitry. IPI representations will provide significantly better immunity to these effects, as well as to the more traditional process, temperature, and reference voltage variations. For most applications, pulse based analog systems will require less power. There are numerous advantages to using pulses or pulses to communicate. They are significantly more immune to noise. An approximate analogy would be that of AM versus FM radio signal representation. The outcomes of the proposed research are&lt;br/&gt;&lt;br/&gt; Create a library of basic communication, computation (arithmetic and logic) and conversion building blocks;&lt;br/&gt;Design, implementation and testing of a case study using the derived building blocks and methodology;&lt;br/&gt;&lt;br/&gt;Document an IPI-based design methodology</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">120369</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">120369</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1537" target="n1538">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/PE: Digital Imaging Techniques for the Simulation and Enhancement of Low Vision</data>
      <data key="e_abstract">In this project the PIs will carry out a program of research and development to apply digital imaging techniques to the problem of low vision. The work will focus on two related thrusts. First, low vision simulation methods will be developed that accurately show people with normal vision what the world looks like to people with visual deficits. The goal is to develop simulation tools that designers can use to understand the visual limitations (and abilities) of people with low vision and thereby eliminate barriers to access from their designs. Applications in the areas of architectural design, driving safety, and the development of graphical user interfaces will be demonstrated. Second, the PIs will develop low vision image enhancement tools that can be used to transform images from digital cameras or graphics applications to create new images that are more comprehensible to people with low vision. The objective is to use computational models of low vision to develop image enhancement algorithms that can be incorporated into digital imaging systems to give people with low vision better access to the visual world. The PIs plan to use these algorithms to develop two new kinds of assistive technology: image profiling and enhancement tools for Web browsers to enable low vision access to Web-based graphical information; and portable augmented vision systems that can facilitate orientation, mobility and object recognition for people with low vision. The hope is that the methods developed will provide better understanding of the challenges facing people with low vision, and lead to effective assistive technologies that can help people with low vision live easier, fuller and more productive lives.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">113310</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">113310</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1542" target="n1543">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Collaborative: Design of Adaptable Dynamic Wireless Network Architectures Using Intelligent Agents</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">4097</data>
      <data key="e_label">296196</data>
      <data key="e_expirationDate">2004-03-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">296196</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n670" target="n1546">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">SGER: Query Optimization to Meet Performance Targets in a Wide Area Environment</data>
      <data key="e_abstract">The goal of this research project is to develop a performance target (PT) sensitive optimizer. Performance targets are relevant in the noisy wide area environment where access costs to Internet accessible WebSources exhibit transient behavior, and are best characterized by a distribution of access costs. A PT sensitive optimizer will have the ability to differentiate among multiple alternate WebSources, and to choose a combination of WebSources so as to best meet a performance target for some query (and its query evaluation plan). The ability to meet a target is quantified by a utility function. Existing optimizers and their cost model consider either specific values or expected values for access costs, and are not sensitive to performance targets. This approach characterizes each plan with the expected value of the cost of the plan, as well as the delay; delay is the deviation above the expected value. A Cost-Delay measure (CDM) combines these two values using a cost factor and a delay factor. A simulation based study of the optimizer&apos;s aggregate behavior, for a set of queries and a set of remote relations on WebSources will be used to correlate the PT optimizer&apos;s selection of plans and WebSources with its success in maximizing utility or meeting a performance target. The results of this project will provide a tunable optimizer for noisy environments that allow applications to be sensitive to performance targets and to better utilize Internet resources.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">135142</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">135142</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1547" target="n1548">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Digital Government Research Center (DGRC): Bringing Complex Data to Users</data>
      <data key="e_abstract">EIA-0091533&lt;br/&gt;Judith Klavans &lt;br/&gt;Columbia University&lt;br/&gt;&lt;br/&gt;Digital Government Research Center (DGRC): Bringing Complex Data to Users&lt;br/&gt;&lt;br/&gt;In partnership with the Federal Energy Information Agency on the topic of trade data, Columbia University and the Information Sciences Institute of the University of Southern California will work in three areas of relevance to the Agency mission:&lt;br/&gt;&lt;br/&gt;1. Main memory query processing, which provides extremely fast querying of multiple statistical data sets, an area of concern to all statistical agencies which must provide aggregated data which maintains the confidentiality of the citizens and businesses which contributed the data; &lt;br/&gt;2. Multilingual question and answering, which will explore the possibility of providing automated translation and querying from English to Spanish and Chinese, and perhaps one other language. As the US population becomes increasingly multi-lingual, natural language processing as a service of gov&apos;t web sites will become more and more expected.&lt;br/&gt;3. Usability testing of components developed in this and in another grant to this team under the Digital Government program.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">91533</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">91533</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1550" target="n1551">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics - BDEI -Bioinformatic Prediction of Functions of Unculturable Microbes in Ecosystems</data>
      <data key="e_abstract">EIA-0131899&lt;br/&gt;Dickerson, Allan&lt;br/&gt;Virginia Polytechnic Institute and State University&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;SUMMARY&lt;br/&gt;&lt;br/&gt;Cells routinely perform complex computational tasks that enable them to control the orchestration&lt;br/&gt;of thousands of genes and communicate with other cells to manifest emergent properties such as growth&lt;br/&gt;and differentiation.Understanding and engineering the algorithms underlying the complex computational&lt;br/&gt;machinery of cells should have significant impact in science and technology,particularly biotechnology,&lt;br/&gt;biocomputation and medicine.&lt;br/&gt;&lt;br/&gt;Several notable recent reports demonstrate that it is possible to design and construct simple de&lt;br/&gt;novo genetic circuits such as a switch and an oscillator in Escherichia coli .This work also revealed that&lt;br/&gt;implementation of even the most simple circuits in vivo requires tedious optimization of often poorly-&lt;br/&gt;understood protein-DNA interactions and mRNA and protein stabilities,among other parameters.We&lt;br/&gt;propose to develop efficient,evolutionary design strategies for constructing functional de novo genetic&lt;br/&gt;circuits.We will apply methods of molecular evolution,which have proven highly successful for&lt;br/&gt;engineering proteins with improved or altered properties,to complex genetic systems involving multiple&lt;br/&gt;repressors,operators,and promoters.We believe that evolution will prove to be generally applicable for&lt;br/&gt;optimizing individual devices as well as complex genetic circuits,and our goal will be to demonstrate&lt;br/&gt;how evolutionary searches are best performed in order to build libraries of devices and assemble them&lt;br/&gt;into functional circuits.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">131899</data>
      <data key="e_expirationDate">2003-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">131899</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1552" target="n1553">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Diversity in Computing Symposium, October 18-20, 2001; Houston, Texas</data>
      <data key="e_abstract">This award provides funding for scholarships to support students to attend the Richard Tapia Celebration of Diversity in Computing Symposium to be held in Houston, Texas on October 18-20, 2001. The goals of the symposium are to make visible the accomplishments of people of diverse ethnicities in the field of Computing. The symposium will be a technical event focusing on undergraduate and graduate students, who will participate directly in the symposium activities as speakers and panelists and who will also participate in poster board sessions.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">127282</data>
      <data key="e_expirationDate">2002-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">127282</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1552" target="n1554">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Diversity in Computing Symposium, October 18-20, 2001; Houston, Texas</data>
      <data key="e_abstract">This award provides funding for scholarships to support students to attend the Richard Tapia Celebration of Diversity in Computing Symposium to be held in Houston, Texas on October 18-20, 2001. The goals of the symposium are to make visible the accomplishments of people of diverse ethnicities in the field of Computing. The symposium will be a technical event focusing on undergraduate and graduate students, who will participate directly in the symposium activities as speakers and panelists and who will also participate in poster board sessions.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">127282</data>
      <data key="e_expirationDate">2002-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">127282</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1552" target="n1555">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Diversity in Computing Symposium, October 18-20, 2001; Houston, Texas</data>
      <data key="e_abstract">This award provides funding for scholarships to support students to attend the Richard Tapia Celebration of Diversity in Computing Symposium to be held in Houston, Texas on October 18-20, 2001. The goals of the symposium are to make visible the accomplishments of people of diverse ethnicities in the field of Computing. The symposium will be a technical event focusing on undergraduate and graduate students, who will participate directly in the symposium activities as speakers and panelists and who will also participate in poster board sessions.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">127282</data>
      <data key="e_expirationDate">2002-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">127282</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1553" target="n1554">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Diversity in Computing Symposium, October 18-20, 2001; Houston, Texas</data>
      <data key="e_abstract">This award provides funding for scholarships to support students to attend the Richard Tapia Celebration of Diversity in Computing Symposium to be held in Houston, Texas on October 18-20, 2001. The goals of the symposium are to make visible the accomplishments of people of diverse ethnicities in the field of Computing. The symposium will be a technical event focusing on undergraduate and graduate students, who will participate directly in the symposium activities as speakers and panelists and who will also participate in poster board sessions.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">127282</data>
      <data key="e_expirationDate">2002-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">127282</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1553" target="n1555">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Diversity in Computing Symposium, October 18-20, 2001; Houston, Texas</data>
      <data key="e_abstract">This award provides funding for scholarships to support students to attend the Richard Tapia Celebration of Diversity in Computing Symposium to be held in Houston, Texas on October 18-20, 2001. The goals of the symposium are to make visible the accomplishments of people of diverse ethnicities in the field of Computing. The symposium will be a technical event focusing on undergraduate and graduate students, who will participate directly in the symposium activities as speakers and panelists and who will also participate in poster board sessions.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">127282</data>
      <data key="e_expirationDate">2002-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">127282</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1554" target="n1555">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Diversity in Computing Symposium, October 18-20, 2001; Houston, Texas</data>
      <data key="e_abstract">This award provides funding for scholarships to support students to attend the Richard Tapia Celebration of Diversity in Computing Symposium to be held in Houston, Texas on October 18-20, 2001. The goals of the symposium are to make visible the accomplishments of people of diverse ethnicities in the field of Computing. The symposium will be a technical event focusing on undergraduate and graduate students, who will participate directly in the symposium activities as speakers and panelists and who will also participate in poster board sessions.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">127282</data>
      <data key="e_expirationDate">2002-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">127282</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1556" target="n1557">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Computational Design of Mixed-Technology Systems</data>
      <data key="e_abstract">The objective of this research is to develop new computational design tools with rigorous experimental validation to enable design and development of distributed, heterogeneous mixed-technology systems. Mixed-technology system development requires research on two fronts: The first is the component or the device level and the second is the integration of heterogeneous components at the system level. At the component or the device level, the research will focus on four building-blocks: Microelectromechanical Systems (MEMS), Biological Microelectromechanical Systems (Bio-MEMS), Nanoelectromechanical Systems (NEMS) and Biological ion channels integrated with nanoelectronics (nanobioelectronics). Efficient computational design tools integrated with experimental validation will be developed for each of these building blocks. At the system level the research focuses on integration of MEMS and Bio-MEMS with conventional electronics. &lt;br/&gt;Device level modeling research will focus on development of new scattered point computational methods for fast, efficient and flexible analysis of micro and nanoscale devices, development of multiscale approaches combining continuum and molecular approaches, development of enhanced continuum models to capture microscopic phenomena, and development of efficient reduced-order modeling approaches for fast dynamic analysis. System level modeling research will focus on development of new algorithms and techniques to integrate various micro-device partial differential equations solvers with the circuit simulator (SPICE3) and development of efficient time stepping schemes with different numerical devices for simulation of the complete system. The experimental effort will focus on development of new fabrication approaches for realizing nanobioelectronics, NEMS and systems level integration of MEMS and Bio-MEMS with conventional electronics.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121616</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121616</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1251" target="n1556">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Computational Design of Mixed-Technology Systems</data>
      <data key="e_abstract">The objective of this research is to develop new computational design tools with rigorous experimental validation to enable design and development of distributed, heterogeneous mixed-technology systems. Mixed-technology system development requires research on two fronts: The first is the component or the device level and the second is the integration of heterogeneous components at the system level. At the component or the device level, the research will focus on four building-blocks: Microelectromechanical Systems (MEMS), Biological Microelectromechanical Systems (Bio-MEMS), Nanoelectromechanical Systems (NEMS) and Biological ion channels integrated with nanoelectronics (nanobioelectronics). Efficient computational design tools integrated with experimental validation will be developed for each of these building blocks. At the system level the research focuses on integration of MEMS and Bio-MEMS with conventional electronics. &lt;br/&gt;Device level modeling research will focus on development of new scattered point computational methods for fast, efficient and flexible analysis of micro and nanoscale devices, development of multiscale approaches combining continuum and molecular approaches, development of enhanced continuum models to capture microscopic phenomena, and development of efficient reduced-order modeling approaches for fast dynamic analysis. System level modeling research will focus on development of new algorithms and techniques to integrate various micro-device partial differential equations solvers with the circuit simulator (SPICE3) and development of efficient time stepping schemes with different numerical devices for simulation of the complete system. The experimental effort will focus on development of new fabrication approaches for realizing nanobioelectronics, NEMS and systems level integration of MEMS and Bio-MEMS with conventional electronics.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121616</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121616</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1556" target="n1559">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Computational Design of Mixed-Technology Systems</data>
      <data key="e_abstract">The objective of this research is to develop new computational design tools with rigorous experimental validation to enable design and development of distributed, heterogeneous mixed-technology systems. Mixed-technology system development requires research on two fronts: The first is the component or the device level and the second is the integration of heterogeneous components at the system level. At the component or the device level, the research will focus on four building-blocks: Microelectromechanical Systems (MEMS), Biological Microelectromechanical Systems (Bio-MEMS), Nanoelectromechanical Systems (NEMS) and Biological ion channels integrated with nanoelectronics (nanobioelectronics). Efficient computational design tools integrated with experimental validation will be developed for each of these building blocks. At the system level the research focuses on integration of MEMS and Bio-MEMS with conventional electronics. &lt;br/&gt;Device level modeling research will focus on development of new scattered point computational methods for fast, efficient and flexible analysis of micro and nanoscale devices, development of multiscale approaches combining continuum and molecular approaches, development of enhanced continuum models to capture microscopic phenomena, and development of efficient reduced-order modeling approaches for fast dynamic analysis. System level modeling research will focus on development of new algorithms and techniques to integrate various micro-device partial differential equations solvers with the circuit simulator (SPICE3) and development of efficient time stepping schemes with different numerical devices for simulation of the complete system. The experimental effort will focus on development of new fabrication approaches for realizing nanobioelectronics, NEMS and systems level integration of MEMS and Bio-MEMS with conventional electronics.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121616</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121616</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n71" target="n1556">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Computational Design of Mixed-Technology Systems</data>
      <data key="e_abstract">The objective of this research is to develop new computational design tools with rigorous experimental validation to enable design and development of distributed, heterogeneous mixed-technology systems. Mixed-technology system development requires research on two fronts: The first is the component or the device level and the second is the integration of heterogeneous components at the system level. At the component or the device level, the research will focus on four building-blocks: Microelectromechanical Systems (MEMS), Biological Microelectromechanical Systems (Bio-MEMS), Nanoelectromechanical Systems (NEMS) and Biological ion channels integrated with nanoelectronics (nanobioelectronics). Efficient computational design tools integrated with experimental validation will be developed for each of these building blocks. At the system level the research focuses on integration of MEMS and Bio-MEMS with conventional electronics. &lt;br/&gt;Device level modeling research will focus on development of new scattered point computational methods for fast, efficient and flexible analysis of micro and nanoscale devices, development of multiscale approaches combining continuum and molecular approaches, development of enhanced continuum models to capture microscopic phenomena, and development of efficient reduced-order modeling approaches for fast dynamic analysis. System level modeling research will focus on development of new algorithms and techniques to integrate various micro-device partial differential equations solvers with the circuit simulator (SPICE3) and development of efficient time stepping schemes with different numerical devices for simulation of the complete system. The experimental effort will focus on development of new fabrication approaches for realizing nanobioelectronics, NEMS and systems level integration of MEMS and Bio-MEMS with conventional electronics.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121616</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121616</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1251" target="n1557">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Computational Design of Mixed-Technology Systems</data>
      <data key="e_abstract">The objective of this research is to develop new computational design tools with rigorous experimental validation to enable design and development of distributed, heterogeneous mixed-technology systems. Mixed-technology system development requires research on two fronts: The first is the component or the device level and the second is the integration of heterogeneous components at the system level. At the component or the device level, the research will focus on four building-blocks: Microelectromechanical Systems (MEMS), Biological Microelectromechanical Systems (Bio-MEMS), Nanoelectromechanical Systems (NEMS) and Biological ion channels integrated with nanoelectronics (nanobioelectronics). Efficient computational design tools integrated with experimental validation will be developed for each of these building blocks. At the system level the research focuses on integration of MEMS and Bio-MEMS with conventional electronics. &lt;br/&gt;Device level modeling research will focus on development of new scattered point computational methods for fast, efficient and flexible analysis of micro and nanoscale devices, development of multiscale approaches combining continuum and molecular approaches, development of enhanced continuum models to capture microscopic phenomena, and development of efficient reduced-order modeling approaches for fast dynamic analysis. System level modeling research will focus on development of new algorithms and techniques to integrate various micro-device partial differential equations solvers with the circuit simulator (SPICE3) and development of efficient time stepping schemes with different numerical devices for simulation of the complete system. The experimental effort will focus on development of new fabrication approaches for realizing nanobioelectronics, NEMS and systems level integration of MEMS and Bio-MEMS with conventional electronics.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121616</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121616</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1557" target="n1559">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Computational Design of Mixed-Technology Systems</data>
      <data key="e_abstract">The objective of this research is to develop new computational design tools with rigorous experimental validation to enable design and development of distributed, heterogeneous mixed-technology systems. Mixed-technology system development requires research on two fronts: The first is the component or the device level and the second is the integration of heterogeneous components at the system level. At the component or the device level, the research will focus on four building-blocks: Microelectromechanical Systems (MEMS), Biological Microelectromechanical Systems (Bio-MEMS), Nanoelectromechanical Systems (NEMS) and Biological ion channels integrated with nanoelectronics (nanobioelectronics). Efficient computational design tools integrated with experimental validation will be developed for each of these building blocks. At the system level the research focuses on integration of MEMS and Bio-MEMS with conventional electronics. &lt;br/&gt;Device level modeling research will focus on development of new scattered point computational methods for fast, efficient and flexible analysis of micro and nanoscale devices, development of multiscale approaches combining continuum and molecular approaches, development of enhanced continuum models to capture microscopic phenomena, and development of efficient reduced-order modeling approaches for fast dynamic analysis. System level modeling research will focus on development of new algorithms and techniques to integrate various micro-device partial differential equations solvers with the circuit simulator (SPICE3) and development of efficient time stepping schemes with different numerical devices for simulation of the complete system. The experimental effort will focus on development of new fabrication approaches for realizing nanobioelectronics, NEMS and systems level integration of MEMS and Bio-MEMS with conventional electronics.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121616</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121616</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n71" target="n1557">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Computational Design of Mixed-Technology Systems</data>
      <data key="e_abstract">The objective of this research is to develop new computational design tools with rigorous experimental validation to enable design and development of distributed, heterogeneous mixed-technology systems. Mixed-technology system development requires research on two fronts: The first is the component or the device level and the second is the integration of heterogeneous components at the system level. At the component or the device level, the research will focus on four building-blocks: Microelectromechanical Systems (MEMS), Biological Microelectromechanical Systems (Bio-MEMS), Nanoelectromechanical Systems (NEMS) and Biological ion channels integrated with nanoelectronics (nanobioelectronics). Efficient computational design tools integrated with experimental validation will be developed for each of these building blocks. At the system level the research focuses on integration of MEMS and Bio-MEMS with conventional electronics. &lt;br/&gt;Device level modeling research will focus on development of new scattered point computational methods for fast, efficient and flexible analysis of micro and nanoscale devices, development of multiscale approaches combining continuum and molecular approaches, development of enhanced continuum models to capture microscopic phenomena, and development of efficient reduced-order modeling approaches for fast dynamic analysis. System level modeling research will focus on development of new algorithms and techniques to integrate various micro-device partial differential equations solvers with the circuit simulator (SPICE3) and development of efficient time stepping schemes with different numerical devices for simulation of the complete system. The experimental effort will focus on development of new fabrication approaches for realizing nanobioelectronics, NEMS and systems level integration of MEMS and Bio-MEMS with conventional electronics.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121616</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121616</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1251" target="n1559">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Computational Design of Mixed-Technology Systems</data>
      <data key="e_abstract">The objective of this research is to develop new computational design tools with rigorous experimental validation to enable design and development of distributed, heterogeneous mixed-technology systems. Mixed-technology system development requires research on two fronts: The first is the component or the device level and the second is the integration of heterogeneous components at the system level. At the component or the device level, the research will focus on four building-blocks: Microelectromechanical Systems (MEMS), Biological Microelectromechanical Systems (Bio-MEMS), Nanoelectromechanical Systems (NEMS) and Biological ion channels integrated with nanoelectronics (nanobioelectronics). Efficient computational design tools integrated with experimental validation will be developed for each of these building blocks. At the system level the research focuses on integration of MEMS and Bio-MEMS with conventional electronics. &lt;br/&gt;Device level modeling research will focus on development of new scattered point computational methods for fast, efficient and flexible analysis of micro and nanoscale devices, development of multiscale approaches combining continuum and molecular approaches, development of enhanced continuum models to capture microscopic phenomena, and development of efficient reduced-order modeling approaches for fast dynamic analysis. System level modeling research will focus on development of new algorithms and techniques to integrate various micro-device partial differential equations solvers with the circuit simulator (SPICE3) and development of efficient time stepping schemes with different numerical devices for simulation of the complete system. The experimental effort will focus on development of new fabrication approaches for realizing nanobioelectronics, NEMS and systems level integration of MEMS and Bio-MEMS with conventional electronics.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121616</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121616</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n71" target="n1251">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Computational Design of Mixed-Technology Systems</data>
      <data key="e_abstract">The objective of this research is to develop new computational design tools with rigorous experimental validation to enable design and development of distributed, heterogeneous mixed-technology systems. Mixed-technology system development requires research on two fronts: The first is the component or the device level and the second is the integration of heterogeneous components at the system level. At the component or the device level, the research will focus on four building-blocks: Microelectromechanical Systems (MEMS), Biological Microelectromechanical Systems (Bio-MEMS), Nanoelectromechanical Systems (NEMS) and Biological ion channels integrated with nanoelectronics (nanobioelectronics). Efficient computational design tools integrated with experimental validation will be developed for each of these building blocks. At the system level the research focuses on integration of MEMS and Bio-MEMS with conventional electronics. &lt;br/&gt;Device level modeling research will focus on development of new scattered point computational methods for fast, efficient and flexible analysis of micro and nanoscale devices, development of multiscale approaches combining continuum and molecular approaches, development of enhanced continuum models to capture microscopic phenomena, and development of efficient reduced-order modeling approaches for fast dynamic analysis. System level modeling research will focus on development of new algorithms and techniques to integrate various micro-device partial differential equations solvers with the circuit simulator (SPICE3) and development of efficient time stepping schemes with different numerical devices for simulation of the complete system. The experimental effort will focus on development of new fabrication approaches for realizing nanobioelectronics, NEMS and systems level integration of MEMS and Bio-MEMS with conventional electronics.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121616</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121616</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n71" target="n1559">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Computational Design of Mixed-Technology Systems</data>
      <data key="e_abstract">The objective of this research is to develop new computational design tools with rigorous experimental validation to enable design and development of distributed, heterogeneous mixed-technology systems. Mixed-technology system development requires research on two fronts: The first is the component or the device level and the second is the integration of heterogeneous components at the system level. At the component or the device level, the research will focus on four building-blocks: Microelectromechanical Systems (MEMS), Biological Microelectromechanical Systems (Bio-MEMS), Nanoelectromechanical Systems (NEMS) and Biological ion channels integrated with nanoelectronics (nanobioelectronics). Efficient computational design tools integrated with experimental validation will be developed for each of these building blocks. At the system level the research focuses on integration of MEMS and Bio-MEMS with conventional electronics. &lt;br/&gt;Device level modeling research will focus on development of new scattered point computational methods for fast, efficient and flexible analysis of micro and nanoscale devices, development of multiscale approaches combining continuum and molecular approaches, development of enhanced continuum models to capture microscopic phenomena, and development of efficient reduced-order modeling approaches for fast dynamic analysis. System level modeling research will focus on development of new algorithms and techniques to integrate various micro-device partial differential equations solvers with the circuit simulator (SPICE3) and development of efficient time stepping schemes with different numerical devices for simulation of the complete system. The experimental effort will focus on development of new fabrication approaches for realizing nanobioelectronics, NEMS and systems level integration of MEMS and Bio-MEMS with conventional electronics.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121616</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121616</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1563" target="n1564">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Examining the Roles of ICTs in Collaborative Networking between Academia and Industry</data>
      <data key="e_abstract">This study will examine the ways in which Information and Communication Technologies (ICTs) enable the development of collaborative, boundary-spanning networks between academic and industry scientists working in three disciplinary areas, 1) oceanography, 2) marine biology and 3) astronomy. The methodology combines survey techniques with semi-structured interviews, in-depth case studies, and network analysis to provide a detailed view of how these scientists use ICTs to gather data, store and share information, and communicate with one another. The results of this research will provide a better understanding of the use of ICTs in inter-organizational scientific collaborations, and the ways in which these collaborations may enable economic development, technological innovation, and advances in science.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">118728</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">118728</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1069" target="n1565">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Adaptive Wide-Area Information Delivery</data>
      <data key="e_abstract">The explosive popularity and exponentially increasing scale of the World Wide Web has severely stressed the Internet&apos;s content-delivery infrastructure. This stress has begun to expose the inefficiencies and limitations of the Web&apos;s traditional client-server architecture. A progression of new mechanisms and alternative architectures have recently appeared, including content delivery networks (CDNs), such as Akamai, and peer-to-peer overlay networks, such as Napster. Despite the rapid deployment and global use of these architectures, however, little is understood about their behavior in many cases. &lt;br/&gt;&lt;br/&gt;The goals of this research project are threefold. First, the effectiveness of existing Internet content-delivery architectures will be quantitatively analyzed, specifically focusing on (1) cache infrastructure, (2) content-delivery networks, and (3) peer-to-peer networks. Second, based upon these measurements, the design of new or enhanced architectures for Web content delivery will be explored. Third, new designs will be implemented, deployed, and measured. One particular focus will be the design and implementation of adaptive systems that make heavy use of self-measurement and on-line algorithms, e.g., to dynamically optimize request routing, content placement, or the topology of the peer-to-peer overlay network. Overall, the hope is to increase the understanding of content-delivery architectures and to develop new content-delivery mechanisms that greatly improve the behavior of the Internet.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121341</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121341</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1565" target="n1567">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Adaptive Wide-Area Information Delivery</data>
      <data key="e_abstract">The explosive popularity and exponentially increasing scale of the World Wide Web has severely stressed the Internet&apos;s content-delivery infrastructure. This stress has begun to expose the inefficiencies and limitations of the Web&apos;s traditional client-server architecture. A progression of new mechanisms and alternative architectures have recently appeared, including content delivery networks (CDNs), such as Akamai, and peer-to-peer overlay networks, such as Napster. Despite the rapid deployment and global use of these architectures, however, little is understood about their behavior in many cases. &lt;br/&gt;&lt;br/&gt;The goals of this research project are threefold. First, the effectiveness of existing Internet content-delivery architectures will be quantitatively analyzed, specifically focusing on (1) cache infrastructure, (2) content-delivery networks, and (3) peer-to-peer networks. Second, based upon these measurements, the design of new or enhanced architectures for Web content delivery will be explored. Third, new designs will be implemented, deployed, and measured. One particular focus will be the design and implementation of adaptive systems that make heavy use of self-measurement and on-line algorithms, e.g., to dynamically optimize request routing, content placement, or the topology of the peer-to-peer overlay network. Overall, the hope is to increase the understanding of content-delivery architectures and to develop new content-delivery mechanisms that greatly improve the behavior of the Internet.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121341</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121341</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1069" target="n1567">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Adaptive Wide-Area Information Delivery</data>
      <data key="e_abstract">The explosive popularity and exponentially increasing scale of the World Wide Web has severely stressed the Internet&apos;s content-delivery infrastructure. This stress has begun to expose the inefficiencies and limitations of the Web&apos;s traditional client-server architecture. A progression of new mechanisms and alternative architectures have recently appeared, including content delivery networks (CDNs), such as Akamai, and peer-to-peer overlay networks, such as Napster. Despite the rapid deployment and global use of these architectures, however, little is understood about their behavior in many cases. &lt;br/&gt;&lt;br/&gt;The goals of this research project are threefold. First, the effectiveness of existing Internet content-delivery architectures will be quantitatively analyzed, specifically focusing on (1) cache infrastructure, (2) content-delivery networks, and (3) peer-to-peer networks. Second, based upon these measurements, the design of new or enhanced architectures for Web content delivery will be explored. Third, new designs will be implemented, deployed, and measured. One particular focus will be the design and implementation of adaptive systems that make heavy use of self-measurement and on-line algorithms, e.g., to dynamically optimize request routing, content placement, or the topology of the peer-to-peer overlay network. Overall, the hope is to increase the understanding of content-delivery architectures and to develop new content-delivery mechanisms that greatly improve the behavior of the Internet.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121341</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121341</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1571" target="n1572">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Research in Computer-Aided Design of VLSI Circuits</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">4710</data>
      <data key="e_label">296185</data>
      <data key="e_expirationDate">2005-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">296185</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1571" target="n1573">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Research in Computer-Aided Design of VLSI Circuits</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">4710</data>
      <data key="e_label">296185</data>
      <data key="e_expirationDate">2005-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">296185</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1572" target="n1573">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Research in Computer-Aided Design of VLSI Circuits</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">4710</data>
      <data key="e_label">296185</data>
      <data key="e_expirationDate">2005-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">296185</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1575" target="n1576">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Fast Capture and Understanding of Dynamic 3-D Shapes</data>
      <data key="e_abstract">The project focuses on sensing, and proposes to develop new ways for fast shape and appearance sensing, 3D tracking, recognition, and compression. The sensing approach proposed is based on dynamic active triangulation, with structured light illumination swept across the scene., allowing 100-200 range maps per second.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">102272</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">102272</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1579" target="n1580">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Guarding the Next Internet Frontier: Countering Denial of Information</data>
      <data key="e_abstract">As applications enabled by the Internet become information rich, ensuring access to quality information in the presence of potentially malicious entities will be a major challenge. The goal of this research project is to develop defensive techniques to counter denial-of-information (DoI) attacks. Such attacks attempt to confuse an information system by deliberately introducing noise that appears to be useful information. The mere availability of information is insufficient if the user must find a needle in a haystack of noise that is created by an adversary to hide critical information. The research focuses on the characterization of information quality metrics that are relevant in the presence of DoI attacks. In particular, two complementary metrics are explored. Information regularity captures predictability in the patterns of information creation and access. The second metric, information quality trust, captures the known ability of an information source to meet the needs of its clients. The development of techniques to derive the values of these metrics for information sources is a key goal of the research. Other planned research activities include the building of a distributed information infrastructure and experimental evaluation of defensive techniques against DoI attacks.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121643</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121643</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1579" target="n1581">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Guarding the Next Internet Frontier: Countering Denial of Information</data>
      <data key="e_abstract">As applications enabled by the Internet become information rich, ensuring access to quality information in the presence of potentially malicious entities will be a major challenge. The goal of this research project is to develop defensive techniques to counter denial-of-information (DoI) attacks. Such attacks attempt to confuse an information system by deliberately introducing noise that appears to be useful information. The mere availability of information is insufficient if the user must find a needle in a haystack of noise that is created by an adversary to hide critical information. The research focuses on the characterization of information quality metrics that are relevant in the presence of DoI attacks. In particular, two complementary metrics are explored. Information regularity captures predictability in the patterns of information creation and access. The second metric, information quality trust, captures the known ability of an information source to meet the needs of its clients. The development of techniques to derive the values of these metrics for information sources is a key goal of the research. Other planned research activities include the building of a distributed information infrastructure and experimental evaluation of defensive techniques against DoI attacks.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121643</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121643</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1579" target="n1582">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Guarding the Next Internet Frontier: Countering Denial of Information</data>
      <data key="e_abstract">As applications enabled by the Internet become information rich, ensuring access to quality information in the presence of potentially malicious entities will be a major challenge. The goal of this research project is to develop defensive techniques to counter denial-of-information (DoI) attacks. Such attacks attempt to confuse an information system by deliberately introducing noise that appears to be useful information. The mere availability of information is insufficient if the user must find a needle in a haystack of noise that is created by an adversary to hide critical information. The research focuses on the characterization of information quality metrics that are relevant in the presence of DoI attacks. In particular, two complementary metrics are explored. Information regularity captures predictability in the patterns of information creation and access. The second metric, information quality trust, captures the known ability of an information source to meet the needs of its clients. The development of techniques to derive the values of these metrics for information sources is a key goal of the research. Other planned research activities include the building of a distributed information infrastructure and experimental evaluation of defensive techniques against DoI attacks.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121643</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121643</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1579" target="n1583">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Guarding the Next Internet Frontier: Countering Denial of Information</data>
      <data key="e_abstract">As applications enabled by the Internet become information rich, ensuring access to quality information in the presence of potentially malicious entities will be a major challenge. The goal of this research project is to develop defensive techniques to counter denial-of-information (DoI) attacks. Such attacks attempt to confuse an information system by deliberately introducing noise that appears to be useful information. The mere availability of information is insufficient if the user must find a needle in a haystack of noise that is created by an adversary to hide critical information. The research focuses on the characterization of information quality metrics that are relevant in the presence of DoI attacks. In particular, two complementary metrics are explored. Information regularity captures predictability in the patterns of information creation and access. The second metric, information quality trust, captures the known ability of an information source to meet the needs of its clients. The development of techniques to derive the values of these metrics for information sources is a key goal of the research. Other planned research activities include the building of a distributed information infrastructure and experimental evaluation of defensive techniques against DoI attacks.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121643</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121643</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1580" target="n1581">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Guarding the Next Internet Frontier: Countering Denial of Information</data>
      <data key="e_abstract">As applications enabled by the Internet become information rich, ensuring access to quality information in the presence of potentially malicious entities will be a major challenge. The goal of this research project is to develop defensive techniques to counter denial-of-information (DoI) attacks. Such attacks attempt to confuse an information system by deliberately introducing noise that appears to be useful information. The mere availability of information is insufficient if the user must find a needle in a haystack of noise that is created by an adversary to hide critical information. The research focuses on the characterization of information quality metrics that are relevant in the presence of DoI attacks. In particular, two complementary metrics are explored. Information regularity captures predictability in the patterns of information creation and access. The second metric, information quality trust, captures the known ability of an information source to meet the needs of its clients. The development of techniques to derive the values of these metrics for information sources is a key goal of the research. Other planned research activities include the building of a distributed information infrastructure and experimental evaluation of defensive techniques against DoI attacks.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121643</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121643</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1580" target="n1582">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Guarding the Next Internet Frontier: Countering Denial of Information</data>
      <data key="e_abstract">As applications enabled by the Internet become information rich, ensuring access to quality information in the presence of potentially malicious entities will be a major challenge. The goal of this research project is to develop defensive techniques to counter denial-of-information (DoI) attacks. Such attacks attempt to confuse an information system by deliberately introducing noise that appears to be useful information. The mere availability of information is insufficient if the user must find a needle in a haystack of noise that is created by an adversary to hide critical information. The research focuses on the characterization of information quality metrics that are relevant in the presence of DoI attacks. In particular, two complementary metrics are explored. Information regularity captures predictability in the patterns of information creation and access. The second metric, information quality trust, captures the known ability of an information source to meet the needs of its clients. The development of techniques to derive the values of these metrics for information sources is a key goal of the research. Other planned research activities include the building of a distributed information infrastructure and experimental evaluation of defensive techniques against DoI attacks.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121643</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121643</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1580" target="n1583">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Guarding the Next Internet Frontier: Countering Denial of Information</data>
      <data key="e_abstract">As applications enabled by the Internet become information rich, ensuring access to quality information in the presence of potentially malicious entities will be a major challenge. The goal of this research project is to develop defensive techniques to counter denial-of-information (DoI) attacks. Such attacks attempt to confuse an information system by deliberately introducing noise that appears to be useful information. The mere availability of information is insufficient if the user must find a needle in a haystack of noise that is created by an adversary to hide critical information. The research focuses on the characterization of information quality metrics that are relevant in the presence of DoI attacks. In particular, two complementary metrics are explored. Information regularity captures predictability in the patterns of information creation and access. The second metric, information quality trust, captures the known ability of an information source to meet the needs of its clients. The development of techniques to derive the values of these metrics for information sources is a key goal of the research. Other planned research activities include the building of a distributed information infrastructure and experimental evaluation of defensive techniques against DoI attacks.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121643</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121643</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1581" target="n1582">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Guarding the Next Internet Frontier: Countering Denial of Information</data>
      <data key="e_abstract">As applications enabled by the Internet become information rich, ensuring access to quality information in the presence of potentially malicious entities will be a major challenge. The goal of this research project is to develop defensive techniques to counter denial-of-information (DoI) attacks. Such attacks attempt to confuse an information system by deliberately introducing noise that appears to be useful information. The mere availability of information is insufficient if the user must find a needle in a haystack of noise that is created by an adversary to hide critical information. The research focuses on the characterization of information quality metrics that are relevant in the presence of DoI attacks. In particular, two complementary metrics are explored. Information regularity captures predictability in the patterns of information creation and access. The second metric, information quality trust, captures the known ability of an information source to meet the needs of its clients. The development of techniques to derive the values of these metrics for information sources is a key goal of the research. Other planned research activities include the building of a distributed information infrastructure and experimental evaluation of defensive techniques against DoI attacks.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121643</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121643</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1581" target="n1583">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Guarding the Next Internet Frontier: Countering Denial of Information</data>
      <data key="e_abstract">As applications enabled by the Internet become information rich, ensuring access to quality information in the presence of potentially malicious entities will be a major challenge. The goal of this research project is to develop defensive techniques to counter denial-of-information (DoI) attacks. Such attacks attempt to confuse an information system by deliberately introducing noise that appears to be useful information. The mere availability of information is insufficient if the user must find a needle in a haystack of noise that is created by an adversary to hide critical information. The research focuses on the characterization of information quality metrics that are relevant in the presence of DoI attacks. In particular, two complementary metrics are explored. Information regularity captures predictability in the patterns of information creation and access. The second metric, information quality trust, captures the known ability of an information source to meet the needs of its clients. The development of techniques to derive the values of these metrics for information sources is a key goal of the research. Other planned research activities include the building of a distributed information infrastructure and experimental evaluation of defensive techniques against DoI attacks.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121643</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121643</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1582" target="n1583">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Guarding the Next Internet Frontier: Countering Denial of Information</data>
      <data key="e_abstract">As applications enabled by the Internet become information rich, ensuring access to quality information in the presence of potentially malicious entities will be a major challenge. The goal of this research project is to develop defensive techniques to counter denial-of-information (DoI) attacks. Such attacks attempt to confuse an information system by deliberately introducing noise that appears to be useful information. The mere availability of information is insufficient if the user must find a needle in a haystack of noise that is created by an adversary to hide critical information. The research focuses on the characterization of information quality metrics that are relevant in the presence of DoI attacks. In particular, two complementary metrics are explored. Information regularity captures predictability in the patterns of information creation and access. The second metric, information quality trust, captures the known ability of an information source to meet the needs of its clients. The development of techniques to derive the values of these metrics for information sources is a key goal of the research. Other planned research activities include the building of a distributed information infrastructure and experimental evaluation of defensive techniques against DoI attacks.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121643</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121643</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1584" target="n1585">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/IM: Capturing, Coordinating and Remembering Human Experience</data>
      <data key="e_abstract">This work will develop algorithms and systems enabling people&lt;br/&gt;to query and communicate a synthesized record of&lt;br/&gt;human experiences derived from individual perspectives captured&lt;br/&gt;during selected personal and group activities. For this research,&lt;br/&gt;an experience is defined through what you see, what you hear,&lt;br/&gt;where you are, and associated sensor data and electronic communications.&lt;br/&gt;The research will transform this record into a meaningful, accessible&lt;br/&gt;information resource, available contemporaneously and retrospectively.&lt;br/&gt;We will validate our vision with two societally relevant applications:&lt;br/&gt;(1) providing memory aids as a personal prosthetic or behavioral&lt;br/&gt;monitor for the elderly; and (2) coordinating emergency response&lt;br/&gt;activity in disaster scenarios.&lt;br/&gt;&lt;br/&gt;This project assumes that within ten years technology will be capable&lt;br/&gt;of creating a continuously recorded, digital, high fidelity record of&lt;br/&gt;a person&apos;s activities and observations in video form. This research&lt;br/&gt;will prototype personal experience capture units to record audio, video,&lt;br/&gt;location and sensory data, and electronic communications. Each constituent&lt;br/&gt;unit captures, manages, secures and associates information from&lt;br/&gt;its unique point of view. Each operates as a portable, interoperable,&lt;br/&gt;information system, allowing search and retrieval by both its&lt;br/&gt;human operator and remote collaborating systems. An individual&lt;br/&gt;cannot see everything, nor remember everything that was seen or&lt;br/&gt;heard. The integration of multiple points of view provides more&lt;br/&gt;comprehensive coverage of an event, especially when coupled with support&lt;br/&gt;for vastly improving the memory from each perspective. The research thus&lt;br/&gt;enables the following technological advances:&lt;br/&gt;&lt;br/&gt;* Enhanced memory for individuals from an intelligent assistant using an&lt;br/&gt;automatically analyzed and fully indexed archive of captured personal&lt;br/&gt;experiences.&lt;br/&gt;&lt;br/&gt;* Coordination of distributed group activity, such as management of an&lt;br/&gt;emergency response team in a disaster relief situation, utilizing multiple&lt;br/&gt;synchronized streams of incoming observation data to construct a &quot;collective&lt;br/&gt;experience.&quot;&lt;br/&gt;&lt;br/&gt;* Expertise synthesized across individuals and maintained over generations,&lt;br/&gt;retrieved and summarized on demand to enable example-based training and&lt;br/&gt;retrospective analysis.&lt;br/&gt;&lt;br/&gt;* Understanding of privacy, security and other societal implications of&lt;br/&gt;ubiquitous experience collection.&lt;br/&gt;&lt;br/&gt;The foundation for this work, the Informedia Digital Video Library,&lt;br/&gt;has demonstrated the successful application of speech, image, and&lt;br/&gt;natural language processing in automatically creating a rich, indexed,&lt;br/&gt;searchable multimedia information resource for broadcast-quality video.&lt;br/&gt;The proposed work builds from these technologies, moving well beyond&lt;br/&gt;a digital video library into new information spaces composed of&lt;br/&gt;unedited personal experience video augmented with additional sensory&lt;br/&gt;and position data. Tools will be created to analyze large amounts&lt;br/&gt;of continuously captured digital experience data in order to extract&lt;br/&gt;salient features, describe scenes and characterize events. The&lt;br/&gt;research will address summarization and collaboration of multiple&lt;br/&gt;simultaneous experiences integrated across time, space and people.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121641</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121641</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1584" target="n1586">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/IM: Capturing, Coordinating and Remembering Human Experience</data>
      <data key="e_abstract">This work will develop algorithms and systems enabling people&lt;br/&gt;to query and communicate a synthesized record of&lt;br/&gt;human experiences derived from individual perspectives captured&lt;br/&gt;during selected personal and group activities. For this research,&lt;br/&gt;an experience is defined through what you see, what you hear,&lt;br/&gt;where you are, and associated sensor data and electronic communications.&lt;br/&gt;The research will transform this record into a meaningful, accessible&lt;br/&gt;information resource, available contemporaneously and retrospectively.&lt;br/&gt;We will validate our vision with two societally relevant applications:&lt;br/&gt;(1) providing memory aids as a personal prosthetic or behavioral&lt;br/&gt;monitor for the elderly; and (2) coordinating emergency response&lt;br/&gt;activity in disaster scenarios.&lt;br/&gt;&lt;br/&gt;This project assumes that within ten years technology will be capable&lt;br/&gt;of creating a continuously recorded, digital, high fidelity record of&lt;br/&gt;a person&apos;s activities and observations in video form. This research&lt;br/&gt;will prototype personal experience capture units to record audio, video,&lt;br/&gt;location and sensory data, and electronic communications. Each constituent&lt;br/&gt;unit captures, manages, secures and associates information from&lt;br/&gt;its unique point of view. Each operates as a portable, interoperable,&lt;br/&gt;information system, allowing search and retrieval by both its&lt;br/&gt;human operator and remote collaborating systems. An individual&lt;br/&gt;cannot see everything, nor remember everything that was seen or&lt;br/&gt;heard. The integration of multiple points of view provides more&lt;br/&gt;comprehensive coverage of an event, especially when coupled with support&lt;br/&gt;for vastly improving the memory from each perspective. The research thus&lt;br/&gt;enables the following technological advances:&lt;br/&gt;&lt;br/&gt;* Enhanced memory for individuals from an intelligent assistant using an&lt;br/&gt;automatically analyzed and fully indexed archive of captured personal&lt;br/&gt;experiences.&lt;br/&gt;&lt;br/&gt;* Coordination of distributed group activity, such as management of an&lt;br/&gt;emergency response team in a disaster relief situation, utilizing multiple&lt;br/&gt;synchronized streams of incoming observation data to construct a &quot;collective&lt;br/&gt;experience.&quot;&lt;br/&gt;&lt;br/&gt;* Expertise synthesized across individuals and maintained over generations,&lt;br/&gt;retrieved and summarized on demand to enable example-based training and&lt;br/&gt;retrospective analysis.&lt;br/&gt;&lt;br/&gt;* Understanding of privacy, security and other societal implications of&lt;br/&gt;ubiquitous experience collection.&lt;br/&gt;&lt;br/&gt;The foundation for this work, the Informedia Digital Video Library,&lt;br/&gt;has demonstrated the successful application of speech, image, and&lt;br/&gt;natural language processing in automatically creating a rich, indexed,&lt;br/&gt;searchable multimedia information resource for broadcast-quality video.&lt;br/&gt;The proposed work builds from these technologies, moving well beyond&lt;br/&gt;a digital video library into new information spaces composed of&lt;br/&gt;unedited personal experience video augmented with additional sensory&lt;br/&gt;and position data. Tools will be created to analyze large amounts&lt;br/&gt;of continuously captured digital experience data in order to extract&lt;br/&gt;salient features, describe scenes and characterize events. The&lt;br/&gt;research will address summarization and collaboration of multiple&lt;br/&gt;simultaneous experiences integrated across time, space and people.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121641</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121641</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1584" target="n1587">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/IM: Capturing, Coordinating and Remembering Human Experience</data>
      <data key="e_abstract">This work will develop algorithms and systems enabling people&lt;br/&gt;to query and communicate a synthesized record of&lt;br/&gt;human experiences derived from individual perspectives captured&lt;br/&gt;during selected personal and group activities. For this research,&lt;br/&gt;an experience is defined through what you see, what you hear,&lt;br/&gt;where you are, and associated sensor data and electronic communications.&lt;br/&gt;The research will transform this record into a meaningful, accessible&lt;br/&gt;information resource, available contemporaneously and retrospectively.&lt;br/&gt;We will validate our vision with two societally relevant applications:&lt;br/&gt;(1) providing memory aids as a personal prosthetic or behavioral&lt;br/&gt;monitor for the elderly; and (2) coordinating emergency response&lt;br/&gt;activity in disaster scenarios.&lt;br/&gt;&lt;br/&gt;This project assumes that within ten years technology will be capable&lt;br/&gt;of creating a continuously recorded, digital, high fidelity record of&lt;br/&gt;a person&apos;s activities and observations in video form. This research&lt;br/&gt;will prototype personal experience capture units to record audio, video,&lt;br/&gt;location and sensory data, and electronic communications. Each constituent&lt;br/&gt;unit captures, manages, secures and associates information from&lt;br/&gt;its unique point of view. Each operates as a portable, interoperable,&lt;br/&gt;information system, allowing search and retrieval by both its&lt;br/&gt;human operator and remote collaborating systems. An individual&lt;br/&gt;cannot see everything, nor remember everything that was seen or&lt;br/&gt;heard. The integration of multiple points of view provides more&lt;br/&gt;comprehensive coverage of an event, especially when coupled with support&lt;br/&gt;for vastly improving the memory from each perspective. The research thus&lt;br/&gt;enables the following technological advances:&lt;br/&gt;&lt;br/&gt;* Enhanced memory for individuals from an intelligent assistant using an&lt;br/&gt;automatically analyzed and fully indexed archive of captured personal&lt;br/&gt;experiences.&lt;br/&gt;&lt;br/&gt;* Coordination of distributed group activity, such as management of an&lt;br/&gt;emergency response team in a disaster relief situation, utilizing multiple&lt;br/&gt;synchronized streams of incoming observation data to construct a &quot;collective&lt;br/&gt;experience.&quot;&lt;br/&gt;&lt;br/&gt;* Expertise synthesized across individuals and maintained over generations,&lt;br/&gt;retrieved and summarized on demand to enable example-based training and&lt;br/&gt;retrospective analysis.&lt;br/&gt;&lt;br/&gt;* Understanding of privacy, security and other societal implications of&lt;br/&gt;ubiquitous experience collection.&lt;br/&gt;&lt;br/&gt;The foundation for this work, the Informedia Digital Video Library,&lt;br/&gt;has demonstrated the successful application of speech, image, and&lt;br/&gt;natural language processing in automatically creating a rich, indexed,&lt;br/&gt;searchable multimedia information resource for broadcast-quality video.&lt;br/&gt;The proposed work builds from these technologies, moving well beyond&lt;br/&gt;a digital video library into new information spaces composed of&lt;br/&gt;unedited personal experience video augmented with additional sensory&lt;br/&gt;and position data. Tools will be created to analyze large amounts&lt;br/&gt;of continuously captured digital experience data in order to extract&lt;br/&gt;salient features, describe scenes and characterize events. The&lt;br/&gt;research will address summarization and collaboration of multiple&lt;br/&gt;simultaneous experiences integrated across time, space and people.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121641</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121641</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1584" target="n1588">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/IM: Capturing, Coordinating and Remembering Human Experience</data>
      <data key="e_abstract">This work will develop algorithms and systems enabling people&lt;br/&gt;to query and communicate a synthesized record of&lt;br/&gt;human experiences derived from individual perspectives captured&lt;br/&gt;during selected personal and group activities. For this research,&lt;br/&gt;an experience is defined through what you see, what you hear,&lt;br/&gt;where you are, and associated sensor data and electronic communications.&lt;br/&gt;The research will transform this record into a meaningful, accessible&lt;br/&gt;information resource, available contemporaneously and retrospectively.&lt;br/&gt;We will validate our vision with two societally relevant applications:&lt;br/&gt;(1) providing memory aids as a personal prosthetic or behavioral&lt;br/&gt;monitor for the elderly; and (2) coordinating emergency response&lt;br/&gt;activity in disaster scenarios.&lt;br/&gt;&lt;br/&gt;This project assumes that within ten years technology will be capable&lt;br/&gt;of creating a continuously recorded, digital, high fidelity record of&lt;br/&gt;a person&apos;s activities and observations in video form. This research&lt;br/&gt;will prototype personal experience capture units to record audio, video,&lt;br/&gt;location and sensory data, and electronic communications. Each constituent&lt;br/&gt;unit captures, manages, secures and associates information from&lt;br/&gt;its unique point of view. Each operates as a portable, interoperable,&lt;br/&gt;information system, allowing search and retrieval by both its&lt;br/&gt;human operator and remote collaborating systems. An individual&lt;br/&gt;cannot see everything, nor remember everything that was seen or&lt;br/&gt;heard. The integration of multiple points of view provides more&lt;br/&gt;comprehensive coverage of an event, especially when coupled with support&lt;br/&gt;for vastly improving the memory from each perspective. The research thus&lt;br/&gt;enables the following technological advances:&lt;br/&gt;&lt;br/&gt;* Enhanced memory for individuals from an intelligent assistant using an&lt;br/&gt;automatically analyzed and fully indexed archive of captured personal&lt;br/&gt;experiences.&lt;br/&gt;&lt;br/&gt;* Coordination of distributed group activity, such as management of an&lt;br/&gt;emergency response team in a disaster relief situation, utilizing multiple&lt;br/&gt;synchronized streams of incoming observation data to construct a &quot;collective&lt;br/&gt;experience.&quot;&lt;br/&gt;&lt;br/&gt;* Expertise synthesized across individuals and maintained over generations,&lt;br/&gt;retrieved and summarized on demand to enable example-based training and&lt;br/&gt;retrospective analysis.&lt;br/&gt;&lt;br/&gt;* Understanding of privacy, security and other societal implications of&lt;br/&gt;ubiquitous experience collection.&lt;br/&gt;&lt;br/&gt;The foundation for this work, the Informedia Digital Video Library,&lt;br/&gt;has demonstrated the successful application of speech, image, and&lt;br/&gt;natural language processing in automatically creating a rich, indexed,&lt;br/&gt;searchable multimedia information resource for broadcast-quality video.&lt;br/&gt;The proposed work builds from these technologies, moving well beyond&lt;br/&gt;a digital video library into new information spaces composed of&lt;br/&gt;unedited personal experience video augmented with additional sensory&lt;br/&gt;and position data. Tools will be created to analyze large amounts&lt;br/&gt;of continuously captured digital experience data in order to extract&lt;br/&gt;salient features, describe scenes and characterize events. The&lt;br/&gt;research will address summarization and collaboration of multiple&lt;br/&gt;simultaneous experiences integrated across time, space and people.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121641</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121641</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1585" target="n1586">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/IM: Capturing, Coordinating and Remembering Human Experience</data>
      <data key="e_abstract">This work will develop algorithms and systems enabling people&lt;br/&gt;to query and communicate a synthesized record of&lt;br/&gt;human experiences derived from individual perspectives captured&lt;br/&gt;during selected personal and group activities. For this research,&lt;br/&gt;an experience is defined through what you see, what you hear,&lt;br/&gt;where you are, and associated sensor data and electronic communications.&lt;br/&gt;The research will transform this record into a meaningful, accessible&lt;br/&gt;information resource, available contemporaneously and retrospectively.&lt;br/&gt;We will validate our vision with two societally relevant applications:&lt;br/&gt;(1) providing memory aids as a personal prosthetic or behavioral&lt;br/&gt;monitor for the elderly; and (2) coordinating emergency response&lt;br/&gt;activity in disaster scenarios.&lt;br/&gt;&lt;br/&gt;This project assumes that within ten years technology will be capable&lt;br/&gt;of creating a continuously recorded, digital, high fidelity record of&lt;br/&gt;a person&apos;s activities and observations in video form. This research&lt;br/&gt;will prototype personal experience capture units to record audio, video,&lt;br/&gt;location and sensory data, and electronic communications. Each constituent&lt;br/&gt;unit captures, manages, secures and associates information from&lt;br/&gt;its unique point of view. Each operates as a portable, interoperable,&lt;br/&gt;information system, allowing search and retrieval by both its&lt;br/&gt;human operator and remote collaborating systems. An individual&lt;br/&gt;cannot see everything, nor remember everything that was seen or&lt;br/&gt;heard. The integration of multiple points of view provides more&lt;br/&gt;comprehensive coverage of an event, especially when coupled with support&lt;br/&gt;for vastly improving the memory from each perspective. The research thus&lt;br/&gt;enables the following technological advances:&lt;br/&gt;&lt;br/&gt;* Enhanced memory for individuals from an intelligent assistant using an&lt;br/&gt;automatically analyzed and fully indexed archive of captured personal&lt;br/&gt;experiences.&lt;br/&gt;&lt;br/&gt;* Coordination of distributed group activity, such as management of an&lt;br/&gt;emergency response team in a disaster relief situation, utilizing multiple&lt;br/&gt;synchronized streams of incoming observation data to construct a &quot;collective&lt;br/&gt;experience.&quot;&lt;br/&gt;&lt;br/&gt;* Expertise synthesized across individuals and maintained over generations,&lt;br/&gt;retrieved and summarized on demand to enable example-based training and&lt;br/&gt;retrospective analysis.&lt;br/&gt;&lt;br/&gt;* Understanding of privacy, security and other societal implications of&lt;br/&gt;ubiquitous experience collection.&lt;br/&gt;&lt;br/&gt;The foundation for this work, the Informedia Digital Video Library,&lt;br/&gt;has demonstrated the successful application of speech, image, and&lt;br/&gt;natural language processing in automatically creating a rich, indexed,&lt;br/&gt;searchable multimedia information resource for broadcast-quality video.&lt;br/&gt;The proposed work builds from these technologies, moving well beyond&lt;br/&gt;a digital video library into new information spaces composed of&lt;br/&gt;unedited personal experience video augmented with additional sensory&lt;br/&gt;and position data. Tools will be created to analyze large amounts&lt;br/&gt;of continuously captured digital experience data in order to extract&lt;br/&gt;salient features, describe scenes and characterize events. The&lt;br/&gt;research will address summarization and collaboration of multiple&lt;br/&gt;simultaneous experiences integrated across time, space and people.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121641</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121641</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1585" target="n1587">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/IM: Capturing, Coordinating and Remembering Human Experience</data>
      <data key="e_abstract">This work will develop algorithms and systems enabling people&lt;br/&gt;to query and communicate a synthesized record of&lt;br/&gt;human experiences derived from individual perspectives captured&lt;br/&gt;during selected personal and group activities. For this research,&lt;br/&gt;an experience is defined through what you see, what you hear,&lt;br/&gt;where you are, and associated sensor data and electronic communications.&lt;br/&gt;The research will transform this record into a meaningful, accessible&lt;br/&gt;information resource, available contemporaneously and retrospectively.&lt;br/&gt;We will validate our vision with two societally relevant applications:&lt;br/&gt;(1) providing memory aids as a personal prosthetic or behavioral&lt;br/&gt;monitor for the elderly; and (2) coordinating emergency response&lt;br/&gt;activity in disaster scenarios.&lt;br/&gt;&lt;br/&gt;This project assumes that within ten years technology will be capable&lt;br/&gt;of creating a continuously recorded, digital, high fidelity record of&lt;br/&gt;a person&apos;s activities and observations in video form. This research&lt;br/&gt;will prototype personal experience capture units to record audio, video,&lt;br/&gt;location and sensory data, and electronic communications. Each constituent&lt;br/&gt;unit captures, manages, secures and associates information from&lt;br/&gt;its unique point of view. Each operates as a portable, interoperable,&lt;br/&gt;information system, allowing search and retrieval by both its&lt;br/&gt;human operator and remote collaborating systems. An individual&lt;br/&gt;cannot see everything, nor remember everything that was seen or&lt;br/&gt;heard. The integration of multiple points of view provides more&lt;br/&gt;comprehensive coverage of an event, especially when coupled with support&lt;br/&gt;for vastly improving the memory from each perspective. The research thus&lt;br/&gt;enables the following technological advances:&lt;br/&gt;&lt;br/&gt;* Enhanced memory for individuals from an intelligent assistant using an&lt;br/&gt;automatically analyzed and fully indexed archive of captured personal&lt;br/&gt;experiences.&lt;br/&gt;&lt;br/&gt;* Coordination of distributed group activity, such as management of an&lt;br/&gt;emergency response team in a disaster relief situation, utilizing multiple&lt;br/&gt;synchronized streams of incoming observation data to construct a &quot;collective&lt;br/&gt;experience.&quot;&lt;br/&gt;&lt;br/&gt;* Expertise synthesized across individuals and maintained over generations,&lt;br/&gt;retrieved and summarized on demand to enable example-based training and&lt;br/&gt;retrospective analysis.&lt;br/&gt;&lt;br/&gt;* Understanding of privacy, security and other societal implications of&lt;br/&gt;ubiquitous experience collection.&lt;br/&gt;&lt;br/&gt;The foundation for this work, the Informedia Digital Video Library,&lt;br/&gt;has demonstrated the successful application of speech, image, and&lt;br/&gt;natural language processing in automatically creating a rich, indexed,&lt;br/&gt;searchable multimedia information resource for broadcast-quality video.&lt;br/&gt;The proposed work builds from these technologies, moving well beyond&lt;br/&gt;a digital video library into new information spaces composed of&lt;br/&gt;unedited personal experience video augmented with additional sensory&lt;br/&gt;and position data. Tools will be created to analyze large amounts&lt;br/&gt;of continuously captured digital experience data in order to extract&lt;br/&gt;salient features, describe scenes and characterize events. The&lt;br/&gt;research will address summarization and collaboration of multiple&lt;br/&gt;simultaneous experiences integrated across time, space and people.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121641</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121641</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1585" target="n1588">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/IM: Capturing, Coordinating and Remembering Human Experience</data>
      <data key="e_abstract">This work will develop algorithms and systems enabling people&lt;br/&gt;to query and communicate a synthesized record of&lt;br/&gt;human experiences derived from individual perspectives captured&lt;br/&gt;during selected personal and group activities. For this research,&lt;br/&gt;an experience is defined through what you see, what you hear,&lt;br/&gt;where you are, and associated sensor data and electronic communications.&lt;br/&gt;The research will transform this record into a meaningful, accessible&lt;br/&gt;information resource, available contemporaneously and retrospectively.&lt;br/&gt;We will validate our vision with two societally relevant applications:&lt;br/&gt;(1) providing memory aids as a personal prosthetic or behavioral&lt;br/&gt;monitor for the elderly; and (2) coordinating emergency response&lt;br/&gt;activity in disaster scenarios.&lt;br/&gt;&lt;br/&gt;This project assumes that within ten years technology will be capable&lt;br/&gt;of creating a continuously recorded, digital, high fidelity record of&lt;br/&gt;a person&apos;s activities and observations in video form. This research&lt;br/&gt;will prototype personal experience capture units to record audio, video,&lt;br/&gt;location and sensory data, and electronic communications. Each constituent&lt;br/&gt;unit captures, manages, secures and associates information from&lt;br/&gt;its unique point of view. Each operates as a portable, interoperable,&lt;br/&gt;information system, allowing search and retrieval by both its&lt;br/&gt;human operator and remote collaborating systems. An individual&lt;br/&gt;cannot see everything, nor remember everything that was seen or&lt;br/&gt;heard. The integration of multiple points of view provides more&lt;br/&gt;comprehensive coverage of an event, especially when coupled with support&lt;br/&gt;for vastly improving the memory from each perspective. The research thus&lt;br/&gt;enables the following technological advances:&lt;br/&gt;&lt;br/&gt;* Enhanced memory for individuals from an intelligent assistant using an&lt;br/&gt;automatically analyzed and fully indexed archive of captured personal&lt;br/&gt;experiences.&lt;br/&gt;&lt;br/&gt;* Coordination of distributed group activity, such as management of an&lt;br/&gt;emergency response team in a disaster relief situation, utilizing multiple&lt;br/&gt;synchronized streams of incoming observation data to construct a &quot;collective&lt;br/&gt;experience.&quot;&lt;br/&gt;&lt;br/&gt;* Expertise synthesized across individuals and maintained over generations,&lt;br/&gt;retrieved and summarized on demand to enable example-based training and&lt;br/&gt;retrospective analysis.&lt;br/&gt;&lt;br/&gt;* Understanding of privacy, security and other societal implications of&lt;br/&gt;ubiquitous experience collection.&lt;br/&gt;&lt;br/&gt;The foundation for this work, the Informedia Digital Video Library,&lt;br/&gt;has demonstrated the successful application of speech, image, and&lt;br/&gt;natural language processing in automatically creating a rich, indexed,&lt;br/&gt;searchable multimedia information resource for broadcast-quality video.&lt;br/&gt;The proposed work builds from these technologies, moving well beyond&lt;br/&gt;a digital video library into new information spaces composed of&lt;br/&gt;unedited personal experience video augmented with additional sensory&lt;br/&gt;and position data. Tools will be created to analyze large amounts&lt;br/&gt;of continuously captured digital experience data in order to extract&lt;br/&gt;salient features, describe scenes and characterize events. The&lt;br/&gt;research will address summarization and collaboration of multiple&lt;br/&gt;simultaneous experiences integrated across time, space and people.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121641</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121641</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1586" target="n1587">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/IM: Capturing, Coordinating and Remembering Human Experience</data>
      <data key="e_abstract">This work will develop algorithms and systems enabling people&lt;br/&gt;to query and communicate a synthesized record of&lt;br/&gt;human experiences derived from individual perspectives captured&lt;br/&gt;during selected personal and group activities. For this research,&lt;br/&gt;an experience is defined through what you see, what you hear,&lt;br/&gt;where you are, and associated sensor data and electronic communications.&lt;br/&gt;The research will transform this record into a meaningful, accessible&lt;br/&gt;information resource, available contemporaneously and retrospectively.&lt;br/&gt;We will validate our vision with two societally relevant applications:&lt;br/&gt;(1) providing memory aids as a personal prosthetic or behavioral&lt;br/&gt;monitor for the elderly; and (2) coordinating emergency response&lt;br/&gt;activity in disaster scenarios.&lt;br/&gt;&lt;br/&gt;This project assumes that within ten years technology will be capable&lt;br/&gt;of creating a continuously recorded, digital, high fidelity record of&lt;br/&gt;a person&apos;s activities and observations in video form. This research&lt;br/&gt;will prototype personal experience capture units to record audio, video,&lt;br/&gt;location and sensory data, and electronic communications. Each constituent&lt;br/&gt;unit captures, manages, secures and associates information from&lt;br/&gt;its unique point of view. Each operates as a portable, interoperable,&lt;br/&gt;information system, allowing search and retrieval by both its&lt;br/&gt;human operator and remote collaborating systems. An individual&lt;br/&gt;cannot see everything, nor remember everything that was seen or&lt;br/&gt;heard. The integration of multiple points of view provides more&lt;br/&gt;comprehensive coverage of an event, especially when coupled with support&lt;br/&gt;for vastly improving the memory from each perspective. The research thus&lt;br/&gt;enables the following technological advances:&lt;br/&gt;&lt;br/&gt;* Enhanced memory for individuals from an intelligent assistant using an&lt;br/&gt;automatically analyzed and fully indexed archive of captured personal&lt;br/&gt;experiences.&lt;br/&gt;&lt;br/&gt;* Coordination of distributed group activity, such as management of an&lt;br/&gt;emergency response team in a disaster relief situation, utilizing multiple&lt;br/&gt;synchronized streams of incoming observation data to construct a &quot;collective&lt;br/&gt;experience.&quot;&lt;br/&gt;&lt;br/&gt;* Expertise synthesized across individuals and maintained over generations,&lt;br/&gt;retrieved and summarized on demand to enable example-based training and&lt;br/&gt;retrospective analysis.&lt;br/&gt;&lt;br/&gt;* Understanding of privacy, security and other societal implications of&lt;br/&gt;ubiquitous experience collection.&lt;br/&gt;&lt;br/&gt;The foundation for this work, the Informedia Digital Video Library,&lt;br/&gt;has demonstrated the successful application of speech, image, and&lt;br/&gt;natural language processing in automatically creating a rich, indexed,&lt;br/&gt;searchable multimedia information resource for broadcast-quality video.&lt;br/&gt;The proposed work builds from these technologies, moving well beyond&lt;br/&gt;a digital video library into new information spaces composed of&lt;br/&gt;unedited personal experience video augmented with additional sensory&lt;br/&gt;and position data. Tools will be created to analyze large amounts&lt;br/&gt;of continuously captured digital experience data in order to extract&lt;br/&gt;salient features, describe scenes and characterize events. The&lt;br/&gt;research will address summarization and collaboration of multiple&lt;br/&gt;simultaneous experiences integrated across time, space and people.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121641</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121641</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1586" target="n1588">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/IM: Capturing, Coordinating and Remembering Human Experience</data>
      <data key="e_abstract">This work will develop algorithms and systems enabling people&lt;br/&gt;to query and communicate a synthesized record of&lt;br/&gt;human experiences derived from individual perspectives captured&lt;br/&gt;during selected personal and group activities. For this research,&lt;br/&gt;an experience is defined through what you see, what you hear,&lt;br/&gt;where you are, and associated sensor data and electronic communications.&lt;br/&gt;The research will transform this record into a meaningful, accessible&lt;br/&gt;information resource, available contemporaneously and retrospectively.&lt;br/&gt;We will validate our vision with two societally relevant applications:&lt;br/&gt;(1) providing memory aids as a personal prosthetic or behavioral&lt;br/&gt;monitor for the elderly; and (2) coordinating emergency response&lt;br/&gt;activity in disaster scenarios.&lt;br/&gt;&lt;br/&gt;This project assumes that within ten years technology will be capable&lt;br/&gt;of creating a continuously recorded, digital, high fidelity record of&lt;br/&gt;a person&apos;s activities and observations in video form. This research&lt;br/&gt;will prototype personal experience capture units to record audio, video,&lt;br/&gt;location and sensory data, and electronic communications. Each constituent&lt;br/&gt;unit captures, manages, secures and associates information from&lt;br/&gt;its unique point of view. Each operates as a portable, interoperable,&lt;br/&gt;information system, allowing search and retrieval by both its&lt;br/&gt;human operator and remote collaborating systems. An individual&lt;br/&gt;cannot see everything, nor remember everything that was seen or&lt;br/&gt;heard. The integration of multiple points of view provides more&lt;br/&gt;comprehensive coverage of an event, especially when coupled with support&lt;br/&gt;for vastly improving the memory from each perspective. The research thus&lt;br/&gt;enables the following technological advances:&lt;br/&gt;&lt;br/&gt;* Enhanced memory for individuals from an intelligent assistant using an&lt;br/&gt;automatically analyzed and fully indexed archive of captured personal&lt;br/&gt;experiences.&lt;br/&gt;&lt;br/&gt;* Coordination of distributed group activity, such as management of an&lt;br/&gt;emergency response team in a disaster relief situation, utilizing multiple&lt;br/&gt;synchronized streams of incoming observation data to construct a &quot;collective&lt;br/&gt;experience.&quot;&lt;br/&gt;&lt;br/&gt;* Expertise synthesized across individuals and maintained over generations,&lt;br/&gt;retrieved and summarized on demand to enable example-based training and&lt;br/&gt;retrospective analysis.&lt;br/&gt;&lt;br/&gt;* Understanding of privacy, security and other societal implications of&lt;br/&gt;ubiquitous experience collection.&lt;br/&gt;&lt;br/&gt;The foundation for this work, the Informedia Digital Video Library,&lt;br/&gt;has demonstrated the successful application of speech, image, and&lt;br/&gt;natural language processing in automatically creating a rich, indexed,&lt;br/&gt;searchable multimedia information resource for broadcast-quality video.&lt;br/&gt;The proposed work builds from these technologies, moving well beyond&lt;br/&gt;a digital video library into new information spaces composed of&lt;br/&gt;unedited personal experience video augmented with additional sensory&lt;br/&gt;and position data. Tools will be created to analyze large amounts&lt;br/&gt;of continuously captured digital experience data in order to extract&lt;br/&gt;salient features, describe scenes and characterize events. The&lt;br/&gt;research will address summarization and collaboration of multiple&lt;br/&gt;simultaneous experiences integrated across time, space and people.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121641</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121641</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1587" target="n1588">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/IM: Capturing, Coordinating and Remembering Human Experience</data>
      <data key="e_abstract">This work will develop algorithms and systems enabling people&lt;br/&gt;to query and communicate a synthesized record of&lt;br/&gt;human experiences derived from individual perspectives captured&lt;br/&gt;during selected personal and group activities. For this research,&lt;br/&gt;an experience is defined through what you see, what you hear,&lt;br/&gt;where you are, and associated sensor data and electronic communications.&lt;br/&gt;The research will transform this record into a meaningful, accessible&lt;br/&gt;information resource, available contemporaneously and retrospectively.&lt;br/&gt;We will validate our vision with two societally relevant applications:&lt;br/&gt;(1) providing memory aids as a personal prosthetic or behavioral&lt;br/&gt;monitor for the elderly; and (2) coordinating emergency response&lt;br/&gt;activity in disaster scenarios.&lt;br/&gt;&lt;br/&gt;This project assumes that within ten years technology will be capable&lt;br/&gt;of creating a continuously recorded, digital, high fidelity record of&lt;br/&gt;a person&apos;s activities and observations in video form. This research&lt;br/&gt;will prototype personal experience capture units to record audio, video,&lt;br/&gt;location and sensory data, and electronic communications. Each constituent&lt;br/&gt;unit captures, manages, secures and associates information from&lt;br/&gt;its unique point of view. Each operates as a portable, interoperable,&lt;br/&gt;information system, allowing search and retrieval by both its&lt;br/&gt;human operator and remote collaborating systems. An individual&lt;br/&gt;cannot see everything, nor remember everything that was seen or&lt;br/&gt;heard. The integration of multiple points of view provides more&lt;br/&gt;comprehensive coverage of an event, especially when coupled with support&lt;br/&gt;for vastly improving the memory from each perspective. The research thus&lt;br/&gt;enables the following technological advances:&lt;br/&gt;&lt;br/&gt;* Enhanced memory for individuals from an intelligent assistant using an&lt;br/&gt;automatically analyzed and fully indexed archive of captured personal&lt;br/&gt;experiences.&lt;br/&gt;&lt;br/&gt;* Coordination of distributed group activity, such as management of an&lt;br/&gt;emergency response team in a disaster relief situation, utilizing multiple&lt;br/&gt;synchronized streams of incoming observation data to construct a &quot;collective&lt;br/&gt;experience.&quot;&lt;br/&gt;&lt;br/&gt;* Expertise synthesized across individuals and maintained over generations,&lt;br/&gt;retrieved and summarized on demand to enable example-based training and&lt;br/&gt;retrospective analysis.&lt;br/&gt;&lt;br/&gt;* Understanding of privacy, security and other societal implications of&lt;br/&gt;ubiquitous experience collection.&lt;br/&gt;&lt;br/&gt;The foundation for this work, the Informedia Digital Video Library,&lt;br/&gt;has demonstrated the successful application of speech, image, and&lt;br/&gt;natural language processing in automatically creating a rich, indexed,&lt;br/&gt;searchable multimedia information resource for broadcast-quality video.&lt;br/&gt;The proposed work builds from these technologies, moving well beyond&lt;br/&gt;a digital video library into new information spaces composed of&lt;br/&gt;unedited personal experience video augmented with additional sensory&lt;br/&gt;and position data. Tools will be created to analyze large amounts&lt;br/&gt;of continuously captured digital experience data in order to extract&lt;br/&gt;salient features, describe scenes and characterize events. The&lt;br/&gt;research will address summarization and collaboration of multiple&lt;br/&gt;simultaneous experiences integrated across time, space and people.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121641</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">121641</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1595" target="n1596">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Making Dysarthric Speech Intelligible</data>
      <data key="e_abstract">Making Dysarthric Speech Intelligible&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This is the first year funding of a three year continuing award. Of the 2.5 million or more adult Americans with significant disability due to chronic neurologic impairment, a large percentage present with dysarthria, or speech impairment, as one of their disabling conditions, and there are no known cures. Dysarthric individuals report loss of employment, educational opportunities, social integration, and quality of life. Despite some strategies for compensating, the isolation caused by communication impairment is pervasive. In this project, the PI will develop new algorithms that, when implemented in wearable devices, will enable dysarthric individuals to be more easily understood. Currently available devices are essentially (digital or analog) spectral filters and amplifiers that enhance certain parts of the spectrum. While these can help certain types of dysarthria, many dysarthric persons suffer from speech problems that require forms of speech modification that are much more profound and complex such as: irregular sub-glottal pressure, resulting in loudness bursts that can be difficult to adjust to; absence, or poor control, of voicing; systematic mispronunciation of certain phoneme groups, resulting in certain sounds becoming indistinguishable or unrecognizable; variable mispronunciation; and poor prosody (pitch control, timing, and loudness). For these difficult problems, new approaches are needed that do not merely filter the speech signal but analyze it at acoustic, articulatory, phonetic, and linguistic levels. These approaches can be combined to generate an output speech signal that, while preserving certain features of the input speech, modifies the input speech along as many dimensions as is needed to achieve intelligibility. The past decade has seen a revolution in speech technology that can be applied to these problems; while little of the currently developed technologies are in their present form applicable to dysarthria, the underlying algorithms can form a basis for the creation of innovative techniques that are specifically targeted to address these more difficult speech problems. The PI will create these technologies in a diagnostic framework, so that the appropriate technology is used for a given type of dysarthria. The results will be of great value for dysarthric individuals; the scientific challenges are formidable, and meeting them will produce insights that will be broadly useful for other speech technologies as well.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">117911</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">117911</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1598" target="n1599">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Information Technology Workforce - ITWF: Information Technology Educational Pathways of African Americans: A Synthetic Study with NELS 1988-2000 Data and Case Studies</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;Proposal Id: EIA- 0120096&lt;br/&gt;PI: Gary G. Huang, Patricia C. Thompson and Young-Hee Yoon&lt;br/&gt;Institution: CSR, Inc&lt;br/&gt;Title: Information Technology Educational Pathways of African-Americans: A Synthetic Study with NELS 1988-2000 Data and Case Studies.&lt;br/&gt;&lt;br/&gt;This award to CSR, Incorporated provides support for a 3-year study to examine the secondary and postsecondary educational pathways that African-American youth follow to pursue information technology(IT) careers. The study will distinguish specific factors linked to the successful pursuit of IT education from other generic predictors of educational attainment. The finding are intended to guide sound policy and program decisions that will support an equitable participation of African -Americans in the IT education and workforce. The study will compare the educational pathways taken by African-Americans to pursue careers in IT with similar pursuits in other&quot;pre-professional&quot; careers in law, medicine, and business administration, and the fields of science, mathematics, engineering, and technology</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">120096</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">120096</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1598" target="n1600">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Information Technology Workforce - ITWF: Information Technology Educational Pathways of African Americans: A Synthetic Study with NELS 1988-2000 Data and Case Studies</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;Proposal Id: EIA- 0120096&lt;br/&gt;PI: Gary G. Huang, Patricia C. Thompson and Young-Hee Yoon&lt;br/&gt;Institution: CSR, Inc&lt;br/&gt;Title: Information Technology Educational Pathways of African-Americans: A Synthetic Study with NELS 1988-2000 Data and Case Studies.&lt;br/&gt;&lt;br/&gt;This award to CSR, Incorporated provides support for a 3-year study to examine the secondary and postsecondary educational pathways that African-American youth follow to pursue information technology(IT) careers. The study will distinguish specific factors linked to the successful pursuit of IT education from other generic predictors of educational attainment. The finding are intended to guide sound policy and program decisions that will support an equitable participation of African -Americans in the IT education and workforce. The study will compare the educational pathways taken by African-Americans to pursue careers in IT with similar pursuits in other&quot;pre-professional&quot; careers in law, medicine, and business administration, and the fields of science, mathematics, engineering, and technology</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">120096</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">120096</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1599" target="n1600">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Information Technology Workforce - ITWF: Information Technology Educational Pathways of African Americans: A Synthetic Study with NELS 1988-2000 Data and Case Studies</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;Proposal Id: EIA- 0120096&lt;br/&gt;PI: Gary G. Huang, Patricia C. Thompson and Young-Hee Yoon&lt;br/&gt;Institution: CSR, Inc&lt;br/&gt;Title: Information Technology Educational Pathways of African-Americans: A Synthetic Study with NELS 1988-2000 Data and Case Studies.&lt;br/&gt;&lt;br/&gt;This award to CSR, Incorporated provides support for a 3-year study to examine the secondary and postsecondary educational pathways that African-American youth follow to pursue information technology(IT) careers. The study will distinguish specific factors linked to the successful pursuit of IT education from other generic predictors of educational attainment. The finding are intended to guide sound policy and program decisions that will support an equitable participation of African -Americans in the IT education and workforce. The study will compare the educational pathways taken by African-Americans to pursue careers in IT with similar pursuits in other&quot;pre-professional&quot; careers in law, medicine, and business administration, and the fields of science, mathematics, engineering, and technology</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">120096</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">120096</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1601" target="n1602">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Algebraic and Geometric Constructions of Shannon Limit Approaching Codes and Turbo Decoding of Reed-Solomon Codes</data>
      <data key="e_abstract">As the demand for error-free data transmission and data storage increases, &lt;br/&gt;error control becomes increasingly important in data communication and data&lt;br/&gt;storage systems. It has become an integral part in almost every data &lt;br/&gt;communication or storage system design. Today very sophisticated error &lt;br/&gt;control schemes are being used in a broad range of communication and data&lt;br/&gt;storage systems to achieve reliable data transmission and storage, such as&lt;br/&gt;wireless communication, satellite communication, optical communication, &lt;br/&gt;hard disc drives, compact disks and many others. The objective of this &lt;br/&gt;research is to devise methods for constructing good error control codes and &lt;br/&gt;to develop efficient error control schemes which have great potential to &lt;br/&gt;achieve error-free communication and data storage for the future generation&lt;br/&gt;of data communication and storage systems.&lt;br/&gt;&lt;br/&gt;Recently, there have been dramatic developments in error control codes and&lt;br/&gt;decoding algorithms. Two families of powerful codes, known as turbo and&lt;br/&gt;low density parity check (lDPC) codes, have been discovered and rediscovered.&lt;br/&gt;These two families of codes with iterative decoding have been shown to &lt;br/&gt;perform close to Shannon&apos;s theoretical limits with reasonable implementation&lt;br/&gt;complexity. As a result of their amazing error performance and practical&lt;br/&gt;implementation, it is anticipated that these two classes of codes will have&lt;br/&gt;an enormous impact on virtually all applications of error control coding over&lt;br/&gt;the next 10 years or so. This research involves in two important aspects&lt;br/&gt;of these two classes of Shannon limit approaching codes: construction of LDPC &lt;br/&gt;codes and turbo decoding of Reed-Solomon (RS) codes. The construction of &lt;br/&gt;LDPC codes is based on combinatoric appraches, such as finite geometries over&lt;br/&gt;finite fields, statistical experimental designs, permutation groups and&lt;br/&gt;graphs. In these approaches, points, lines, hyperplanes in finite geometries, &lt;br/&gt;balanced incomplete block designs, affine permutation groups, and paths&lt;br/&gt;and independent sets of graphs are used for constructing LDPC codes whose&lt;br/&gt;Tanner graphs do not contain short cycles. All the construction methods&lt;br/&gt;are systematic and codes constructed have good structural properties which&lt;br/&gt;simplify encoding and decoding implementations. Turbo decoding of a RS code &lt;br/&gt;is based on binary decomposition of the code into a set of simple binary &lt;br/&gt;component codes and formulating the code as a self concatenated code with the &lt;br/&gt;RS code itself as the outer code and the component codes as inner codes in a &lt;br/&gt;turbo coding arrangement. The decoding is carried out in two stages, turbo &lt;br/&gt;inner decoding and outer algebraic soft-decision decoding.</data>
      <data key="e_pgm">4096</data>
      <data key="e_label">117891</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">117891</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1603" target="n1604">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITWF: Stretching Horizons: Upward Bound Programs in Stimulating Information Technology Education and Career Aspirations among Underrepresented Minorities</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;Information Technology Workforce (ITWF)&lt;br/&gt;&lt;br/&gt;Proposal Id: EIA-0120138&lt;br/&gt;Investigator: Russel Stockard, Ali Akbari and Myungsook Klassen&lt;br/&gt;Institution: California Lutheran U.&lt;br/&gt;Title: Stretching Horizons: Upward Bound Programs in Stimulating Information Technology Education and Career Aspirations among Underrepresented Minorities &lt;br/&gt;&lt;br/&gt;This proposal provides support for California Lutheran University to conduct a research project to examine why so few male and female African American and Latino students are studying computer science at the college level. The three cohorts, Upward Bound students, Math Upward Bound students and non-Upward Bound students, will be studied as to their experiences, opportunities, attitudes and aspirations with respect to information technology (the federally-funded Upward Bound program is made up of two different programs, the traditional Upward Bound program and the Math/Science Upward Bound program). The sites of the study are schools in the three-county Southern California region IX of Upward Bound, Los Angeles, Ventura and Orange Counties.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">120138</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">120138</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1603" target="n1605">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITWF: Stretching Horizons: Upward Bound Programs in Stimulating Information Technology Education and Career Aspirations among Underrepresented Minorities</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;Information Technology Workforce (ITWF)&lt;br/&gt;&lt;br/&gt;Proposal Id: EIA-0120138&lt;br/&gt;Investigator: Russel Stockard, Ali Akbari and Myungsook Klassen&lt;br/&gt;Institution: California Lutheran U.&lt;br/&gt;Title: Stretching Horizons: Upward Bound Programs in Stimulating Information Technology Education and Career Aspirations among Underrepresented Minorities &lt;br/&gt;&lt;br/&gt;This proposal provides support for California Lutheran University to conduct a research project to examine why so few male and female African American and Latino students are studying computer science at the college level. The three cohorts, Upward Bound students, Math Upward Bound students and non-Upward Bound students, will be studied as to their experiences, opportunities, attitudes and aspirations with respect to information technology (the federally-funded Upward Bound program is made up of two different programs, the traditional Upward Bound program and the Math/Science Upward Bound program). The sites of the study are schools in the three-county Southern California region IX of Upward Bound, Los Angeles, Ventura and Orange Counties.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">120138</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">120138</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1604" target="n1605">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITWF: Stretching Horizons: Upward Bound Programs in Stimulating Information Technology Education and Career Aspirations among Underrepresented Minorities</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;Information Technology Workforce (ITWF)&lt;br/&gt;&lt;br/&gt;Proposal Id: EIA-0120138&lt;br/&gt;Investigator: Russel Stockard, Ali Akbari and Myungsook Klassen&lt;br/&gt;Institution: California Lutheran U.&lt;br/&gt;Title: Stretching Horizons: Upward Bound Programs in Stimulating Information Technology Education and Career Aspirations among Underrepresented Minorities &lt;br/&gt;&lt;br/&gt;This proposal provides support for California Lutheran University to conduct a research project to examine why so few male and female African American and Latino students are studying computer science at the college level. The three cohorts, Upward Bound students, Math Upward Bound students and non-Upward Bound students, will be studied as to their experiences, opportunities, attitudes and aspirations with respect to information technology (the federally-funded Upward Bound program is made up of two different programs, the traditional Upward Bound program and the Math/Science Upward Bound program). The sites of the study are schools in the three-county Southern California region IX of Upward Bound, Los Angeles, Ventura and Orange Counties.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">120138</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">120138</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1607" target="n1608">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Graph Visualization and Geometric Algorithm Design</data>
      <data key="e_abstract">Proposal #0098068&lt;br/&gt;Michael T. Goodrich&lt;br/&gt;Johns Hopkins University&lt;br/&gt;&lt;br/&gt;This project is focused on the development of data structures and algorithms for performing computations involving the representation, transformation, and visualization of collections of objects that can be modeled geometrically, such as vertices, edges, lines, planes, polygons, curves, and spheres. In particular, it addresses the following topics:&lt;br/&gt;&lt;br/&gt;1. Graph visualization: the design of efficient data structures and algorithms for efficiently representing and visualizing relational information.&lt;br/&gt;&lt;br/&gt;2. Geometric algorithms: the development of efficient algorithms for solving problems involving geometric data, particularly for problems related to graph visualization.&lt;br/&gt;&lt;br/&gt;3. Geometric data structures: the design of fast and efficient data structures for representing graphs and sets of geometric objects.&lt;br/&gt;&lt;br/&gt;4. JDSLviz: the implementation of a prototype of a Java library of algorithms for graph visualization. Building on our existing JDSL library of fundamental data structures in Java, we shall identify algorithm engineering design patterns for the geometric computations performed in graph drawing, and implement fundamental graph visualization algorithms as reusable software components.&lt;br/&gt;&lt;br/&gt;5. GraphNet: the implementation of an Internet computing infrastructure for graph visualization. Extending previous work on the GeomNet system for geometric computing over the Internet, we plan to build a prototype of a novel Web-based graph layout service in the ASP (application service provider) model.&lt;br/&gt;&lt;br/&gt;The main objective of this work is the development of general methodologies for the design of geometric data structures and algorithms, as well as their incorporation into software libraries and internet computing services. This work has potential impact in areas of science and engineering that model physical objects or that use geometry for information visualization.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">98068</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98068</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1611" target="n1612">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/AP Methodologies and Tools for Designing and Implementing Large Scale Real-Time Systems</data>
      <data key="e_abstract">This proposal requests support for research to develop methodologies and tools for designing and implementing very large-scale real-time embedded computer systems that (a) achieve ultra high computational performance through use of parallel hardware architectures, (b) achieve and maintain functional integrity via distributed, hierarchical monitoring and control, (c) are required to be highly available, and (d) are dynamically reconfigurable, maintainable, and evolvable. The specific application that will drive this research and provide a test platform for it is the trigger and data acquisition system for BTeV, an accelerator-based High Energy Physics (HEP) experiment to study matter-antimatter asymmetries (also known as Charge-Parity violation) in the decays of particles containing the b-quark. BTeV has been approved by Fermilab management and is expected to be constructed over the next 5-6 years to run in conjunction with the Fermilab Tevatron Collider. The data-taking phase of the experiment is expected to be at least five years. It requires a massively parallel, heterogeneous cluster of computing elements to reconstruct 15 million particle interactions (events) per second and to use the reconstruction data to decide which events to retain for further data analysis. &lt;br/&gt;&lt;br/&gt;Creating usable software for this type of real-time embedded system will require research into solutions of general problems in the fields of computer science and engineering. The proponents plan to approach these problems in a way that is general, and to produce methodologies and tools that can be applied to many scientific and commercial problems. During this project, the research results will be carried into the high-school system through projects, which enhance existing infrastructure for attracting students into science and engineering disciplines. &lt;br/&gt;&lt;br/&gt;The classes of systems targeted by this research include those imbedded in environments, like BTeV, that produce very large data streams which must be processed in real-time using data-dependent computation strategies. These systems require ultra high performance (~1012 operations per second), necessitating parallel hardware architectures, which in the case of BTeV is composed of a mix of thousands of commodity processors, special purpose processors such as Digital Signal Processors (DSPs), and specialized hardware such as Field Programmable Gate Arrays (FPGAs), all connected by very high-speed networks. The systems must be dynamically reconfigurable to allow maximum performance from the available and potentially changing resources. The systems must be highly available, since the environments produce the data streams continuously over a long period of time, and interesting phenomena important to the analysis are rare and could occur at any time. To achieve the high availability, the systems must be fault tolerant, self-aware, and fault adaptive, since any malfunction of processing elements, the interconnection switches, or the front-end sensors (which provide the input stream) can result in an unrecoverable loss of data. Faults must be corrected in the shortest possible time, and corrected semi-autonomously (i.e., with as little human intervention as possible). Hence, distributed and hierarchical monitoring and control are vital.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121658</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121658</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1611" target="n1613">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/AP Methodologies and Tools for Designing and Implementing Large Scale Real-Time Systems</data>
      <data key="e_abstract">This proposal requests support for research to develop methodologies and tools for designing and implementing very large-scale real-time embedded computer systems that (a) achieve ultra high computational performance through use of parallel hardware architectures, (b) achieve and maintain functional integrity via distributed, hierarchical monitoring and control, (c) are required to be highly available, and (d) are dynamically reconfigurable, maintainable, and evolvable. The specific application that will drive this research and provide a test platform for it is the trigger and data acquisition system for BTeV, an accelerator-based High Energy Physics (HEP) experiment to study matter-antimatter asymmetries (also known as Charge-Parity violation) in the decays of particles containing the b-quark. BTeV has been approved by Fermilab management and is expected to be constructed over the next 5-6 years to run in conjunction with the Fermilab Tevatron Collider. The data-taking phase of the experiment is expected to be at least five years. It requires a massively parallel, heterogeneous cluster of computing elements to reconstruct 15 million particle interactions (events) per second and to use the reconstruction data to decide which events to retain for further data analysis. &lt;br/&gt;&lt;br/&gt;Creating usable software for this type of real-time embedded system will require research into solutions of general problems in the fields of computer science and engineering. The proponents plan to approach these problems in a way that is general, and to produce methodologies and tools that can be applied to many scientific and commercial problems. During this project, the research results will be carried into the high-school system through projects, which enhance existing infrastructure for attracting students into science and engineering disciplines. &lt;br/&gt;&lt;br/&gt;The classes of systems targeted by this research include those imbedded in environments, like BTeV, that produce very large data streams which must be processed in real-time using data-dependent computation strategies. These systems require ultra high performance (~1012 operations per second), necessitating parallel hardware architectures, which in the case of BTeV is composed of a mix of thousands of commodity processors, special purpose processors such as Digital Signal Processors (DSPs), and specialized hardware such as Field Programmable Gate Arrays (FPGAs), all connected by very high-speed networks. The systems must be dynamically reconfigurable to allow maximum performance from the available and potentially changing resources. The systems must be highly available, since the environments produce the data streams continuously over a long period of time, and interesting phenomena important to the analysis are rare and could occur at any time. To achieve the high availability, the systems must be fault tolerant, self-aware, and fault adaptive, since any malfunction of processing elements, the interconnection switches, or the front-end sensors (which provide the input stream) can result in an unrecoverable loss of data. Faults must be corrected in the shortest possible time, and corrected semi-autonomously (i.e., with as little human intervention as possible). Hence, distributed and hierarchical monitoring and control are vital.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121658</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121658</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1611" target="n1614">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/AP Methodologies and Tools for Designing and Implementing Large Scale Real-Time Systems</data>
      <data key="e_abstract">This proposal requests support for research to develop methodologies and tools for designing and implementing very large-scale real-time embedded computer systems that (a) achieve ultra high computational performance through use of parallel hardware architectures, (b) achieve and maintain functional integrity via distributed, hierarchical monitoring and control, (c) are required to be highly available, and (d) are dynamically reconfigurable, maintainable, and evolvable. The specific application that will drive this research and provide a test platform for it is the trigger and data acquisition system for BTeV, an accelerator-based High Energy Physics (HEP) experiment to study matter-antimatter asymmetries (also known as Charge-Parity violation) in the decays of particles containing the b-quark. BTeV has been approved by Fermilab management and is expected to be constructed over the next 5-6 years to run in conjunction with the Fermilab Tevatron Collider. The data-taking phase of the experiment is expected to be at least five years. It requires a massively parallel, heterogeneous cluster of computing elements to reconstruct 15 million particle interactions (events) per second and to use the reconstruction data to decide which events to retain for further data analysis. &lt;br/&gt;&lt;br/&gt;Creating usable software for this type of real-time embedded system will require research into solutions of general problems in the fields of computer science and engineering. The proponents plan to approach these problems in a way that is general, and to produce methodologies and tools that can be applied to many scientific and commercial problems. During this project, the research results will be carried into the high-school system through projects, which enhance existing infrastructure for attracting students into science and engineering disciplines. &lt;br/&gt;&lt;br/&gt;The classes of systems targeted by this research include those imbedded in environments, like BTeV, that produce very large data streams which must be processed in real-time using data-dependent computation strategies. These systems require ultra high performance (~1012 operations per second), necessitating parallel hardware architectures, which in the case of BTeV is composed of a mix of thousands of commodity processors, special purpose processors such as Digital Signal Processors (DSPs), and specialized hardware such as Field Programmable Gate Arrays (FPGAs), all connected by very high-speed networks. The systems must be dynamically reconfigurable to allow maximum performance from the available and potentially changing resources. The systems must be highly available, since the environments produce the data streams continuously over a long period of time, and interesting phenomena important to the analysis are rare and could occur at any time. To achieve the high availability, the systems must be fault tolerant, self-aware, and fault adaptive, since any malfunction of processing elements, the interconnection switches, or the front-end sensors (which provide the input stream) can result in an unrecoverable loss of data. Faults must be corrected in the shortest possible time, and corrected semi-autonomously (i.e., with as little human intervention as possible). Hence, distributed and hierarchical monitoring and control are vital.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121658</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121658</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1611" target="n1615">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/AP Methodologies and Tools for Designing and Implementing Large Scale Real-Time Systems</data>
      <data key="e_abstract">This proposal requests support for research to develop methodologies and tools for designing and implementing very large-scale real-time embedded computer systems that (a) achieve ultra high computational performance through use of parallel hardware architectures, (b) achieve and maintain functional integrity via distributed, hierarchical monitoring and control, (c) are required to be highly available, and (d) are dynamically reconfigurable, maintainable, and evolvable. The specific application that will drive this research and provide a test platform for it is the trigger and data acquisition system for BTeV, an accelerator-based High Energy Physics (HEP) experiment to study matter-antimatter asymmetries (also known as Charge-Parity violation) in the decays of particles containing the b-quark. BTeV has been approved by Fermilab management and is expected to be constructed over the next 5-6 years to run in conjunction with the Fermilab Tevatron Collider. The data-taking phase of the experiment is expected to be at least five years. It requires a massively parallel, heterogeneous cluster of computing elements to reconstruct 15 million particle interactions (events) per second and to use the reconstruction data to decide which events to retain for further data analysis. &lt;br/&gt;&lt;br/&gt;Creating usable software for this type of real-time embedded system will require research into solutions of general problems in the fields of computer science and engineering. The proponents plan to approach these problems in a way that is general, and to produce methodologies and tools that can be applied to many scientific and commercial problems. During this project, the research results will be carried into the high-school system through projects, which enhance existing infrastructure for attracting students into science and engineering disciplines. &lt;br/&gt;&lt;br/&gt;The classes of systems targeted by this research include those imbedded in environments, like BTeV, that produce very large data streams which must be processed in real-time using data-dependent computation strategies. These systems require ultra high performance (~1012 operations per second), necessitating parallel hardware architectures, which in the case of BTeV is composed of a mix of thousands of commodity processors, special purpose processors such as Digital Signal Processors (DSPs), and specialized hardware such as Field Programmable Gate Arrays (FPGAs), all connected by very high-speed networks. The systems must be dynamically reconfigurable to allow maximum performance from the available and potentially changing resources. The systems must be highly available, since the environments produce the data streams continuously over a long period of time, and interesting phenomena important to the analysis are rare and could occur at any time. To achieve the high availability, the systems must be fault tolerant, self-aware, and fault adaptive, since any malfunction of processing elements, the interconnection switches, or the front-end sensors (which provide the input stream) can result in an unrecoverable loss of data. Faults must be corrected in the shortest possible time, and corrected semi-autonomously (i.e., with as little human intervention as possible). Hence, distributed and hierarchical monitoring and control are vital.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121658</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121658</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1612" target="n1613">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/AP Methodologies and Tools for Designing and Implementing Large Scale Real-Time Systems</data>
      <data key="e_abstract">This proposal requests support for research to develop methodologies and tools for designing and implementing very large-scale real-time embedded computer systems that (a) achieve ultra high computational performance through use of parallel hardware architectures, (b) achieve and maintain functional integrity via distributed, hierarchical monitoring and control, (c) are required to be highly available, and (d) are dynamically reconfigurable, maintainable, and evolvable. The specific application that will drive this research and provide a test platform for it is the trigger and data acquisition system for BTeV, an accelerator-based High Energy Physics (HEP) experiment to study matter-antimatter asymmetries (also known as Charge-Parity violation) in the decays of particles containing the b-quark. BTeV has been approved by Fermilab management and is expected to be constructed over the next 5-6 years to run in conjunction with the Fermilab Tevatron Collider. The data-taking phase of the experiment is expected to be at least five years. It requires a massively parallel, heterogeneous cluster of computing elements to reconstruct 15 million particle interactions (events) per second and to use the reconstruction data to decide which events to retain for further data analysis. &lt;br/&gt;&lt;br/&gt;Creating usable software for this type of real-time embedded system will require research into solutions of general problems in the fields of computer science and engineering. The proponents plan to approach these problems in a way that is general, and to produce methodologies and tools that can be applied to many scientific and commercial problems. During this project, the research results will be carried into the high-school system through projects, which enhance existing infrastructure for attracting students into science and engineering disciplines. &lt;br/&gt;&lt;br/&gt;The classes of systems targeted by this research include those imbedded in environments, like BTeV, that produce very large data streams which must be processed in real-time using data-dependent computation strategies. These systems require ultra high performance (~1012 operations per second), necessitating parallel hardware architectures, which in the case of BTeV is composed of a mix of thousands of commodity processors, special purpose processors such as Digital Signal Processors (DSPs), and specialized hardware such as Field Programmable Gate Arrays (FPGAs), all connected by very high-speed networks. The systems must be dynamically reconfigurable to allow maximum performance from the available and potentially changing resources. The systems must be highly available, since the environments produce the data streams continuously over a long period of time, and interesting phenomena important to the analysis are rare and could occur at any time. To achieve the high availability, the systems must be fault tolerant, self-aware, and fault adaptive, since any malfunction of processing elements, the interconnection switches, or the front-end sensors (which provide the input stream) can result in an unrecoverable loss of data. Faults must be corrected in the shortest possible time, and corrected semi-autonomously (i.e., with as little human intervention as possible). Hence, distributed and hierarchical monitoring and control are vital.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121658</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121658</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1612" target="n1614">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/AP Methodologies and Tools for Designing and Implementing Large Scale Real-Time Systems</data>
      <data key="e_abstract">This proposal requests support for research to develop methodologies and tools for designing and implementing very large-scale real-time embedded computer systems that (a) achieve ultra high computational performance through use of parallel hardware architectures, (b) achieve and maintain functional integrity via distributed, hierarchical monitoring and control, (c) are required to be highly available, and (d) are dynamically reconfigurable, maintainable, and evolvable. The specific application that will drive this research and provide a test platform for it is the trigger and data acquisition system for BTeV, an accelerator-based High Energy Physics (HEP) experiment to study matter-antimatter asymmetries (also known as Charge-Parity violation) in the decays of particles containing the b-quark. BTeV has been approved by Fermilab management and is expected to be constructed over the next 5-6 years to run in conjunction with the Fermilab Tevatron Collider. The data-taking phase of the experiment is expected to be at least five years. It requires a massively parallel, heterogeneous cluster of computing elements to reconstruct 15 million particle interactions (events) per second and to use the reconstruction data to decide which events to retain for further data analysis. &lt;br/&gt;&lt;br/&gt;Creating usable software for this type of real-time embedded system will require research into solutions of general problems in the fields of computer science and engineering. The proponents plan to approach these problems in a way that is general, and to produce methodologies and tools that can be applied to many scientific and commercial problems. During this project, the research results will be carried into the high-school system through projects, which enhance existing infrastructure for attracting students into science and engineering disciplines. &lt;br/&gt;&lt;br/&gt;The classes of systems targeted by this research include those imbedded in environments, like BTeV, that produce very large data streams which must be processed in real-time using data-dependent computation strategies. These systems require ultra high performance (~1012 operations per second), necessitating parallel hardware architectures, which in the case of BTeV is composed of a mix of thousands of commodity processors, special purpose processors such as Digital Signal Processors (DSPs), and specialized hardware such as Field Programmable Gate Arrays (FPGAs), all connected by very high-speed networks. The systems must be dynamically reconfigurable to allow maximum performance from the available and potentially changing resources. The systems must be highly available, since the environments produce the data streams continuously over a long period of time, and interesting phenomena important to the analysis are rare and could occur at any time. To achieve the high availability, the systems must be fault tolerant, self-aware, and fault adaptive, since any malfunction of processing elements, the interconnection switches, or the front-end sensors (which provide the input stream) can result in an unrecoverable loss of data. Faults must be corrected in the shortest possible time, and corrected semi-autonomously (i.e., with as little human intervention as possible). Hence, distributed and hierarchical monitoring and control are vital.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121658</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121658</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1612" target="n1615">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/AP Methodologies and Tools for Designing and Implementing Large Scale Real-Time Systems</data>
      <data key="e_abstract">This proposal requests support for research to develop methodologies and tools for designing and implementing very large-scale real-time embedded computer systems that (a) achieve ultra high computational performance through use of parallel hardware architectures, (b) achieve and maintain functional integrity via distributed, hierarchical monitoring and control, (c) are required to be highly available, and (d) are dynamically reconfigurable, maintainable, and evolvable. The specific application that will drive this research and provide a test platform for it is the trigger and data acquisition system for BTeV, an accelerator-based High Energy Physics (HEP) experiment to study matter-antimatter asymmetries (also known as Charge-Parity violation) in the decays of particles containing the b-quark. BTeV has been approved by Fermilab management and is expected to be constructed over the next 5-6 years to run in conjunction with the Fermilab Tevatron Collider. The data-taking phase of the experiment is expected to be at least five years. It requires a massively parallel, heterogeneous cluster of computing elements to reconstruct 15 million particle interactions (events) per second and to use the reconstruction data to decide which events to retain for further data analysis. &lt;br/&gt;&lt;br/&gt;Creating usable software for this type of real-time embedded system will require research into solutions of general problems in the fields of computer science and engineering. The proponents plan to approach these problems in a way that is general, and to produce methodologies and tools that can be applied to many scientific and commercial problems. During this project, the research results will be carried into the high-school system through projects, which enhance existing infrastructure for attracting students into science and engineering disciplines. &lt;br/&gt;&lt;br/&gt;The classes of systems targeted by this research include those imbedded in environments, like BTeV, that produce very large data streams which must be processed in real-time using data-dependent computation strategies. These systems require ultra high performance (~1012 operations per second), necessitating parallel hardware architectures, which in the case of BTeV is composed of a mix of thousands of commodity processors, special purpose processors such as Digital Signal Processors (DSPs), and specialized hardware such as Field Programmable Gate Arrays (FPGAs), all connected by very high-speed networks. The systems must be dynamically reconfigurable to allow maximum performance from the available and potentially changing resources. The systems must be highly available, since the environments produce the data streams continuously over a long period of time, and interesting phenomena important to the analysis are rare and could occur at any time. To achieve the high availability, the systems must be fault tolerant, self-aware, and fault adaptive, since any malfunction of processing elements, the interconnection switches, or the front-end sensors (which provide the input stream) can result in an unrecoverable loss of data. Faults must be corrected in the shortest possible time, and corrected semi-autonomously (i.e., with as little human intervention as possible). Hence, distributed and hierarchical monitoring and control are vital.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121658</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121658</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1613" target="n1614">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/AP Methodologies and Tools for Designing and Implementing Large Scale Real-Time Systems</data>
      <data key="e_abstract">This proposal requests support for research to develop methodologies and tools for designing and implementing very large-scale real-time embedded computer systems that (a) achieve ultra high computational performance through use of parallel hardware architectures, (b) achieve and maintain functional integrity via distributed, hierarchical monitoring and control, (c) are required to be highly available, and (d) are dynamically reconfigurable, maintainable, and evolvable. The specific application that will drive this research and provide a test platform for it is the trigger and data acquisition system for BTeV, an accelerator-based High Energy Physics (HEP) experiment to study matter-antimatter asymmetries (also known as Charge-Parity violation) in the decays of particles containing the b-quark. BTeV has been approved by Fermilab management and is expected to be constructed over the next 5-6 years to run in conjunction with the Fermilab Tevatron Collider. The data-taking phase of the experiment is expected to be at least five years. It requires a massively parallel, heterogeneous cluster of computing elements to reconstruct 15 million particle interactions (events) per second and to use the reconstruction data to decide which events to retain for further data analysis. &lt;br/&gt;&lt;br/&gt;Creating usable software for this type of real-time embedded system will require research into solutions of general problems in the fields of computer science and engineering. The proponents plan to approach these problems in a way that is general, and to produce methodologies and tools that can be applied to many scientific and commercial problems. During this project, the research results will be carried into the high-school system through projects, which enhance existing infrastructure for attracting students into science and engineering disciplines. &lt;br/&gt;&lt;br/&gt;The classes of systems targeted by this research include those imbedded in environments, like BTeV, that produce very large data streams which must be processed in real-time using data-dependent computation strategies. These systems require ultra high performance (~1012 operations per second), necessitating parallel hardware architectures, which in the case of BTeV is composed of a mix of thousands of commodity processors, special purpose processors such as Digital Signal Processors (DSPs), and specialized hardware such as Field Programmable Gate Arrays (FPGAs), all connected by very high-speed networks. The systems must be dynamically reconfigurable to allow maximum performance from the available and potentially changing resources. The systems must be highly available, since the environments produce the data streams continuously over a long period of time, and interesting phenomena important to the analysis are rare and could occur at any time. To achieve the high availability, the systems must be fault tolerant, self-aware, and fault adaptive, since any malfunction of processing elements, the interconnection switches, or the front-end sensors (which provide the input stream) can result in an unrecoverable loss of data. Faults must be corrected in the shortest possible time, and corrected semi-autonomously (i.e., with as little human intervention as possible). Hence, distributed and hierarchical monitoring and control are vital.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121658</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121658</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1613" target="n1615">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/AP Methodologies and Tools for Designing and Implementing Large Scale Real-Time Systems</data>
      <data key="e_abstract">This proposal requests support for research to develop methodologies and tools for designing and implementing very large-scale real-time embedded computer systems that (a) achieve ultra high computational performance through use of parallel hardware architectures, (b) achieve and maintain functional integrity via distributed, hierarchical monitoring and control, (c) are required to be highly available, and (d) are dynamically reconfigurable, maintainable, and evolvable. The specific application that will drive this research and provide a test platform for it is the trigger and data acquisition system for BTeV, an accelerator-based High Energy Physics (HEP) experiment to study matter-antimatter asymmetries (also known as Charge-Parity violation) in the decays of particles containing the b-quark. BTeV has been approved by Fermilab management and is expected to be constructed over the next 5-6 years to run in conjunction with the Fermilab Tevatron Collider. The data-taking phase of the experiment is expected to be at least five years. It requires a massively parallel, heterogeneous cluster of computing elements to reconstruct 15 million particle interactions (events) per second and to use the reconstruction data to decide which events to retain for further data analysis. &lt;br/&gt;&lt;br/&gt;Creating usable software for this type of real-time embedded system will require research into solutions of general problems in the fields of computer science and engineering. The proponents plan to approach these problems in a way that is general, and to produce methodologies and tools that can be applied to many scientific and commercial problems. During this project, the research results will be carried into the high-school system through projects, which enhance existing infrastructure for attracting students into science and engineering disciplines. &lt;br/&gt;&lt;br/&gt;The classes of systems targeted by this research include those imbedded in environments, like BTeV, that produce very large data streams which must be processed in real-time using data-dependent computation strategies. These systems require ultra high performance (~1012 operations per second), necessitating parallel hardware architectures, which in the case of BTeV is composed of a mix of thousands of commodity processors, special purpose processors such as Digital Signal Processors (DSPs), and specialized hardware such as Field Programmable Gate Arrays (FPGAs), all connected by very high-speed networks. The systems must be dynamically reconfigurable to allow maximum performance from the available and potentially changing resources. The systems must be highly available, since the environments produce the data streams continuously over a long period of time, and interesting phenomena important to the analysis are rare and could occur at any time. To achieve the high availability, the systems must be fault tolerant, self-aware, and fault adaptive, since any malfunction of processing elements, the interconnection switches, or the front-end sensors (which provide the input stream) can result in an unrecoverable loss of data. Faults must be corrected in the shortest possible time, and corrected semi-autonomously (i.e., with as little human intervention as possible). Hence, distributed and hierarchical monitoring and control are vital.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121658</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121658</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1614" target="n1615">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/AP Methodologies and Tools for Designing and Implementing Large Scale Real-Time Systems</data>
      <data key="e_abstract">This proposal requests support for research to develop methodologies and tools for designing and implementing very large-scale real-time embedded computer systems that (a) achieve ultra high computational performance through use of parallel hardware architectures, (b) achieve and maintain functional integrity via distributed, hierarchical monitoring and control, (c) are required to be highly available, and (d) are dynamically reconfigurable, maintainable, and evolvable. The specific application that will drive this research and provide a test platform for it is the trigger and data acquisition system for BTeV, an accelerator-based High Energy Physics (HEP) experiment to study matter-antimatter asymmetries (also known as Charge-Parity violation) in the decays of particles containing the b-quark. BTeV has been approved by Fermilab management and is expected to be constructed over the next 5-6 years to run in conjunction with the Fermilab Tevatron Collider. The data-taking phase of the experiment is expected to be at least five years. It requires a massively parallel, heterogeneous cluster of computing elements to reconstruct 15 million particle interactions (events) per second and to use the reconstruction data to decide which events to retain for further data analysis. &lt;br/&gt;&lt;br/&gt;Creating usable software for this type of real-time embedded system will require research into solutions of general problems in the fields of computer science and engineering. The proponents plan to approach these problems in a way that is general, and to produce methodologies and tools that can be applied to many scientific and commercial problems. During this project, the research results will be carried into the high-school system through projects, which enhance existing infrastructure for attracting students into science and engineering disciplines. &lt;br/&gt;&lt;br/&gt;The classes of systems targeted by this research include those imbedded in environments, like BTeV, that produce very large data streams which must be processed in real-time using data-dependent computation strategies. These systems require ultra high performance (~1012 operations per second), necessitating parallel hardware architectures, which in the case of BTeV is composed of a mix of thousands of commodity processors, special purpose processors such as Digital Signal Processors (DSPs), and specialized hardware such as Field Programmable Gate Arrays (FPGAs), all connected by very high-speed networks. The systems must be dynamically reconfigurable to allow maximum performance from the available and potentially changing resources. The systems must be highly available, since the environments produce the data streams continuously over a long period of time, and interesting phenomena important to the analysis are rare and could occur at any time. To achieve the high availability, the systems must be fault tolerant, self-aware, and fault adaptive, since any malfunction of processing elements, the interconnection switches, or the front-end sensors (which provide the input stream) can result in an unrecoverable loss of data. Faults must be corrected in the shortest possible time, and corrected semi-autonomously (i.e., with as little human intervention as possible). Hence, distributed and hierarchical monitoring and control are vital.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121658</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121658</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n768" target="n1616">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: The Aware Home: Sustaining the Quality of Life for an Aging Population</data>
      <data key="e_abstract">This is a standard award. The focus of this project is on development of a domestic environment that is cognizant of the whereabouts and activities of its occupants and can support them in their everyday life. While the technology is applicable to a range of domestic situations, the emphasis in this work will be on support for aging in place; through collaboration with experts in assistive care and cognitive aging, the PI and his team will design, demonstrate, and evaluate a series of domestic services that aim to maintain the quality of life for an aging population, with the goal of increasing the likelihood of a &quot;stay at home&quot; alternative to assisted living that satisfies the needs of an aging individual and his/her distributed family. In particular, the PI will explore two areas that are key to sustaining quality of life for an independent senior adult: maintaining familial vigilance, and supporting daily routines. The intention is to serve as an active partner, aiding the senior occupant without taking control. This research will lead to advances in three research areas: human-computer interaction; computational perception; and software engineering. To achieve the desired goals, the PI will conduct the research and experimentation in an authentic domestic setting, a novel research facility called the Residential Laboratory recently completed next to the Georgia Tech campus. Together with experts in theoretical and practical aspects of aging, the PI will establish a pattern of research in which informed design of ubiquitous computing technology can be rapidly deployed, evaluated and evolved in an authentic setting. Special attention will be paid throughout to issues relating to privacy and trust implications. The PI will transition the products of this project to researchers and practitioners interested in performing more large-scale observations of the social and economic impact of Aware Home technologies.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121661</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121661</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1616" target="n1618">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: The Aware Home: Sustaining the Quality of Life for an Aging Population</data>
      <data key="e_abstract">This is a standard award. The focus of this project is on development of a domestic environment that is cognizant of the whereabouts and activities of its occupants and can support them in their everyday life. While the technology is applicable to a range of domestic situations, the emphasis in this work will be on support for aging in place; through collaboration with experts in assistive care and cognitive aging, the PI and his team will design, demonstrate, and evaluate a series of domestic services that aim to maintain the quality of life for an aging population, with the goal of increasing the likelihood of a &quot;stay at home&quot; alternative to assisted living that satisfies the needs of an aging individual and his/her distributed family. In particular, the PI will explore two areas that are key to sustaining quality of life for an independent senior adult: maintaining familial vigilance, and supporting daily routines. The intention is to serve as an active partner, aiding the senior occupant without taking control. This research will lead to advances in three research areas: human-computer interaction; computational perception; and software engineering. To achieve the desired goals, the PI will conduct the research and experimentation in an authentic domestic setting, a novel research facility called the Residential Laboratory recently completed next to the Georgia Tech campus. Together with experts in theoretical and practical aspects of aging, the PI will establish a pattern of research in which informed design of ubiquitous computing technology can be rapidly deployed, evaluated and evolved in an authentic setting. Special attention will be paid throughout to issues relating to privacy and trust implications. The PI will transition the products of this project to researchers and practitioners interested in performing more large-scale observations of the social and economic impact of Aware Home technologies.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121661</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121661</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n173" target="n1616">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: The Aware Home: Sustaining the Quality of Life for an Aging Population</data>
      <data key="e_abstract">This is a standard award. The focus of this project is on development of a domestic environment that is cognizant of the whereabouts and activities of its occupants and can support them in their everyday life. While the technology is applicable to a range of domestic situations, the emphasis in this work will be on support for aging in place; through collaboration with experts in assistive care and cognitive aging, the PI and his team will design, demonstrate, and evaluate a series of domestic services that aim to maintain the quality of life for an aging population, with the goal of increasing the likelihood of a &quot;stay at home&quot; alternative to assisted living that satisfies the needs of an aging individual and his/her distributed family. In particular, the PI will explore two areas that are key to sustaining quality of life for an independent senior adult: maintaining familial vigilance, and supporting daily routines. The intention is to serve as an active partner, aiding the senior occupant without taking control. This research will lead to advances in three research areas: human-computer interaction; computational perception; and software engineering. To achieve the desired goals, the PI will conduct the research and experimentation in an authentic domestic setting, a novel research facility called the Residential Laboratory recently completed next to the Georgia Tech campus. Together with experts in theoretical and practical aspects of aging, the PI will establish a pattern of research in which informed design of ubiquitous computing technology can be rapidly deployed, evaluated and evolved in an authentic setting. Special attention will be paid throughout to issues relating to privacy and trust implications. The PI will transition the products of this project to researchers and practitioners interested in performing more large-scale observations of the social and economic impact of Aware Home technologies.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121661</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121661</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1616" target="n1620">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: The Aware Home: Sustaining the Quality of Life for an Aging Population</data>
      <data key="e_abstract">This is a standard award. The focus of this project is on development of a domestic environment that is cognizant of the whereabouts and activities of its occupants and can support them in their everyday life. While the technology is applicable to a range of domestic situations, the emphasis in this work will be on support for aging in place; through collaboration with experts in assistive care and cognitive aging, the PI and his team will design, demonstrate, and evaluate a series of domestic services that aim to maintain the quality of life for an aging population, with the goal of increasing the likelihood of a &quot;stay at home&quot; alternative to assisted living that satisfies the needs of an aging individual and his/her distributed family. In particular, the PI will explore two areas that are key to sustaining quality of life for an independent senior adult: maintaining familial vigilance, and supporting daily routines. The intention is to serve as an active partner, aiding the senior occupant without taking control. This research will lead to advances in three research areas: human-computer interaction; computational perception; and software engineering. To achieve the desired goals, the PI will conduct the research and experimentation in an authentic domestic setting, a novel research facility called the Residential Laboratory recently completed next to the Georgia Tech campus. Together with experts in theoretical and practical aspects of aging, the PI will establish a pattern of research in which informed design of ubiquitous computing technology can be rapidly deployed, evaluated and evolved in an authentic setting. Special attention will be paid throughout to issues relating to privacy and trust implications. The PI will transition the products of this project to researchers and practitioners interested in performing more large-scale observations of the social and economic impact of Aware Home technologies.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121661</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121661</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n768" target="n1618">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: The Aware Home: Sustaining the Quality of Life for an Aging Population</data>
      <data key="e_abstract">This is a standard award. The focus of this project is on development of a domestic environment that is cognizant of the whereabouts and activities of its occupants and can support them in their everyday life. While the technology is applicable to a range of domestic situations, the emphasis in this work will be on support for aging in place; through collaboration with experts in assistive care and cognitive aging, the PI and his team will design, demonstrate, and evaluate a series of domestic services that aim to maintain the quality of life for an aging population, with the goal of increasing the likelihood of a &quot;stay at home&quot; alternative to assisted living that satisfies the needs of an aging individual and his/her distributed family. In particular, the PI will explore two areas that are key to sustaining quality of life for an independent senior adult: maintaining familial vigilance, and supporting daily routines. The intention is to serve as an active partner, aiding the senior occupant without taking control. This research will lead to advances in three research areas: human-computer interaction; computational perception; and software engineering. To achieve the desired goals, the PI will conduct the research and experimentation in an authentic domestic setting, a novel research facility called the Residential Laboratory recently completed next to the Georgia Tech campus. Together with experts in theoretical and practical aspects of aging, the PI will establish a pattern of research in which informed design of ubiquitous computing technology can be rapidly deployed, evaluated and evolved in an authentic setting. Special attention will be paid throughout to issues relating to privacy and trust implications. The PI will transition the products of this project to researchers and practitioners interested in performing more large-scale observations of the social and economic impact of Aware Home technologies.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121661</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121661</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n173" target="n768">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: The Aware Home: Sustaining the Quality of Life for an Aging Population</data>
      <data key="e_abstract">This is a standard award. The focus of this project is on development of a domestic environment that is cognizant of the whereabouts and activities of its occupants and can support them in their everyday life. While the technology is applicable to a range of domestic situations, the emphasis in this work will be on support for aging in place; through collaboration with experts in assistive care and cognitive aging, the PI and his team will design, demonstrate, and evaluate a series of domestic services that aim to maintain the quality of life for an aging population, with the goal of increasing the likelihood of a &quot;stay at home&quot; alternative to assisted living that satisfies the needs of an aging individual and his/her distributed family. In particular, the PI will explore two areas that are key to sustaining quality of life for an independent senior adult: maintaining familial vigilance, and supporting daily routines. The intention is to serve as an active partner, aiding the senior occupant without taking control. This research will lead to advances in three research areas: human-computer interaction; computational perception; and software engineering. To achieve the desired goals, the PI will conduct the research and experimentation in an authentic domestic setting, a novel research facility called the Residential Laboratory recently completed next to the Georgia Tech campus. Together with experts in theoretical and practical aspects of aging, the PI will establish a pattern of research in which informed design of ubiquitous computing technology can be rapidly deployed, evaluated and evolved in an authentic setting. Special attention will be paid throughout to issues relating to privacy and trust implications. The PI will transition the products of this project to researchers and practitioners interested in performing more large-scale observations of the social and economic impact of Aware Home technologies.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121661</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121661</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n768" target="n1620">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: The Aware Home: Sustaining the Quality of Life for an Aging Population</data>
      <data key="e_abstract">This is a standard award. The focus of this project is on development of a domestic environment that is cognizant of the whereabouts and activities of its occupants and can support them in their everyday life. While the technology is applicable to a range of domestic situations, the emphasis in this work will be on support for aging in place; through collaboration with experts in assistive care and cognitive aging, the PI and his team will design, demonstrate, and evaluate a series of domestic services that aim to maintain the quality of life for an aging population, with the goal of increasing the likelihood of a &quot;stay at home&quot; alternative to assisted living that satisfies the needs of an aging individual and his/her distributed family. In particular, the PI will explore two areas that are key to sustaining quality of life for an independent senior adult: maintaining familial vigilance, and supporting daily routines. The intention is to serve as an active partner, aiding the senior occupant without taking control. This research will lead to advances in three research areas: human-computer interaction; computational perception; and software engineering. To achieve the desired goals, the PI will conduct the research and experimentation in an authentic domestic setting, a novel research facility called the Residential Laboratory recently completed next to the Georgia Tech campus. Together with experts in theoretical and practical aspects of aging, the PI will establish a pattern of research in which informed design of ubiquitous computing technology can be rapidly deployed, evaluated and evolved in an authentic setting. Special attention will be paid throughout to issues relating to privacy and trust implications. The PI will transition the products of this project to researchers and practitioners interested in performing more large-scale observations of the social and economic impact of Aware Home technologies.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121661</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121661</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n173" target="n1618">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: The Aware Home: Sustaining the Quality of Life for an Aging Population</data>
      <data key="e_abstract">This is a standard award. The focus of this project is on development of a domestic environment that is cognizant of the whereabouts and activities of its occupants and can support them in their everyday life. While the technology is applicable to a range of domestic situations, the emphasis in this work will be on support for aging in place; through collaboration with experts in assistive care and cognitive aging, the PI and his team will design, demonstrate, and evaluate a series of domestic services that aim to maintain the quality of life for an aging population, with the goal of increasing the likelihood of a &quot;stay at home&quot; alternative to assisted living that satisfies the needs of an aging individual and his/her distributed family. In particular, the PI will explore two areas that are key to sustaining quality of life for an independent senior adult: maintaining familial vigilance, and supporting daily routines. The intention is to serve as an active partner, aiding the senior occupant without taking control. This research will lead to advances in three research areas: human-computer interaction; computational perception; and software engineering. To achieve the desired goals, the PI will conduct the research and experimentation in an authentic domestic setting, a novel research facility called the Residential Laboratory recently completed next to the Georgia Tech campus. Together with experts in theoretical and practical aspects of aging, the PI will establish a pattern of research in which informed design of ubiquitous computing technology can be rapidly deployed, evaluated and evolved in an authentic setting. Special attention will be paid throughout to issues relating to privacy and trust implications. The PI will transition the products of this project to researchers and practitioners interested in performing more large-scale observations of the social and economic impact of Aware Home technologies.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121661</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121661</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1618" target="n1620">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: The Aware Home: Sustaining the Quality of Life for an Aging Population</data>
      <data key="e_abstract">This is a standard award. The focus of this project is on development of a domestic environment that is cognizant of the whereabouts and activities of its occupants and can support them in their everyday life. While the technology is applicable to a range of domestic situations, the emphasis in this work will be on support for aging in place; through collaboration with experts in assistive care and cognitive aging, the PI and his team will design, demonstrate, and evaluate a series of domestic services that aim to maintain the quality of life for an aging population, with the goal of increasing the likelihood of a &quot;stay at home&quot; alternative to assisted living that satisfies the needs of an aging individual and his/her distributed family. In particular, the PI will explore two areas that are key to sustaining quality of life for an independent senior adult: maintaining familial vigilance, and supporting daily routines. The intention is to serve as an active partner, aiding the senior occupant without taking control. This research will lead to advances in three research areas: human-computer interaction; computational perception; and software engineering. To achieve the desired goals, the PI will conduct the research and experimentation in an authentic domestic setting, a novel research facility called the Residential Laboratory recently completed next to the Georgia Tech campus. Together with experts in theoretical and practical aspects of aging, the PI will establish a pattern of research in which informed design of ubiquitous computing technology can be rapidly deployed, evaluated and evolved in an authentic setting. Special attention will be paid throughout to issues relating to privacy and trust implications. The PI will transition the products of this project to researchers and practitioners interested in performing more large-scale observations of the social and economic impact of Aware Home technologies.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121661</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121661</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n173" target="n1620">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: The Aware Home: Sustaining the Quality of Life for an Aging Population</data>
      <data key="e_abstract">This is a standard award. The focus of this project is on development of a domestic environment that is cognizant of the whereabouts and activities of its occupants and can support them in their everyday life. While the technology is applicable to a range of domestic situations, the emphasis in this work will be on support for aging in place; through collaboration with experts in assistive care and cognitive aging, the PI and his team will design, demonstrate, and evaluate a series of domestic services that aim to maintain the quality of life for an aging population, with the goal of increasing the likelihood of a &quot;stay at home&quot; alternative to assisted living that satisfies the needs of an aging individual and his/her distributed family. In particular, the PI will explore two areas that are key to sustaining quality of life for an independent senior adult: maintaining familial vigilance, and supporting daily routines. The intention is to serve as an active partner, aiding the senior occupant without taking control. This research will lead to advances in three research areas: human-computer interaction; computational perception; and software engineering. To achieve the desired goals, the PI will conduct the research and experimentation in an authentic domestic setting, a novel research facility called the Residential Laboratory recently completed next to the Georgia Tech campus. Together with experts in theoretical and practical aspects of aging, the PI will establish a pattern of research in which informed design of ubiquitous computing technology can be rapidly deployed, evaluated and evolved in an authentic setting. Special attention will be paid throughout to issues relating to privacy and trust implications. The PI will transition the products of this project to researchers and practitioners interested in performing more large-scale observations of the social and economic impact of Aware Home technologies.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121661</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121661</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1621" target="n1622">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: High-Speed Wavelength-Agile Optical Networks</data>
      <data key="e_abstract">We propose to explore the architecture and device development issues necessary to develop optical LAN&apos;s that are ready to interface with optical MAN&apos;s. Our goal is to develop a clear plan for integration of LAN&apos;s and MAN&apos;s in order to improve the degree to which the benefits of high bandwidth in the MAN&apos;s are delivered to end users on the LAN&apos;s. The application of architectural techniques to this problem cannot be effectively pursued without understanding the capabilities of proposed and available devices, yet the needs of architecture should strongly guide device development to ensure usefulness and relevance. To address these issues, we have formed a synergistic partnership between network architecture and hardware technology groups.&lt;br/&gt; At the architectural level, we will explore issues in the design and evaluation of robust optical LAN architectures with explicit focus on the means of access to MAN&apos;s and on the capabilities necessary for both LAN nodes and the MAN-LAN interfaces (MLI&apos;s). The MLI&apos;s will serve as both hub and head-end for the LAN and will provide a simple interface between the LAN and the MAN. We will explore the robustness of these architectures to faults and will quantify the benefits of exploiting wavelength conversion and tunable sources to improve a network&apos;s robustness to failures with a range of automatic protection algorithms. Using wavelength conversion at MLI&apos;s, we will study the impact of wavelength conversion on robustness and network routing. Using wavelength conversion to enhance robustness has received very little attention, whereas routing is the focus of substantial previous work. We will also explore the effectiveness of optically transparent paths as limited by noise and insertion loss in the devices, and will study the tradeoffs between network capacity and robustness given these routing limitations.&lt;br/&gt; At the device level, we will explore devices and subsystems that trade some functionality for increased simplicity or improved cost-effectiveness. The first of these elements is a multi-cavity VCSEL, which exploits the underlying physics to produce wavelength-tunable transmitters at a fraction of the cost of current tunable sources. These VC-SEL&apos;s will fill the gap between inexpensive, non-tunable VCSEL&apos;s available in a small range of wavelengths, and high-end, carrier-grade lasers. Our proposed tunable VCSEL&apos;s are intended to fulfill the requirements of MAN/LAN environments, which are a hybrid of current core networks and current LAN&apos;s.&lt;br/&gt; We also plan to develop optical wavelength converters based on dual-pump, four-wave mixing. The dual-pump design enhances both the efficiency and the range of the converter, and can also provide polarization independence. High efficiency is necessary to reduce or to eliminate the need for regeneration of optical signals within the MAN-LAN environment. Only a single wavelength enters the wavelength converter, thus the filter eliminates the effects of coherent and incoherent crosstalk between wavelengths that arises through four-wave mixing. SOA-based wavelength conversion is typically cheaper than other approaches, but suffers from poorer noise figures stemming from the use of the SOA, which make them less attractive for long-distance applications in core networks. For the MAN-LAN environment, SOA&apos;s present an attractive and economical alternative that allows ubiquitous availability of wavelength converters at switching nodes.&lt;br/&gt;Third, we will develop high-speed photodetectors based on indium-phosphide materials. The development of&lt;br/&gt;high-speed photodetectors helps to maintain fairly lean wavelength requirements in local and metropolitan areas, avoiding the challenges of dense WDM. While more aggressive scalability is attractive in many ways, a single 80 Gbps wavelength can move a terabyte of data in less than two minutes. A single wavelength with effective access mecha-nisms can be used as a virtual private network if appropriately deployed over a LAN/MAN infrastructure.&lt;br/&gt; Finally, we propose to develop tunable 2x2 switch elements based on indium phosphide ring resonators. Such devices allow a single wavelength to be selected and exchanged by the switch while all other wavelengths pass through untouched. These switches serve as building blocks for several important components, including tunable receivers and low insertion loss, low-crosstalk, high-speed optical crossconnects.&lt;br/&gt; The development of these network elements will guide the types of systems that the network architecture group examines. At the same time, results from the network architecture group in terms of maximum system gain for a given approach (i.e., wavelength conversion, add-drop capability) will influence the direction of the hardware technology development. We believe that this approach is the best method for optimizing the architecture of next-generation fiber-optic WDM systems within the framework of the network element technology.&lt;br/&gt; The researc</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121662</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121662</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1621" target="n1623">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: High-Speed Wavelength-Agile Optical Networks</data>
      <data key="e_abstract">We propose to explore the architecture and device development issues necessary to develop optical LAN&apos;s that are ready to interface with optical MAN&apos;s. Our goal is to develop a clear plan for integration of LAN&apos;s and MAN&apos;s in order to improve the degree to which the benefits of high bandwidth in the MAN&apos;s are delivered to end users on the LAN&apos;s. The application of architectural techniques to this problem cannot be effectively pursued without understanding the capabilities of proposed and available devices, yet the needs of architecture should strongly guide device development to ensure usefulness and relevance. To address these issues, we have formed a synergistic partnership between network architecture and hardware technology groups.&lt;br/&gt; At the architectural level, we will explore issues in the design and evaluation of robust optical LAN architectures with explicit focus on the means of access to MAN&apos;s and on the capabilities necessary for both LAN nodes and the MAN-LAN interfaces (MLI&apos;s). The MLI&apos;s will serve as both hub and head-end for the LAN and will provide a simple interface between the LAN and the MAN. We will explore the robustness of these architectures to faults and will quantify the benefits of exploiting wavelength conversion and tunable sources to improve a network&apos;s robustness to failures with a range of automatic protection algorithms. Using wavelength conversion at MLI&apos;s, we will study the impact of wavelength conversion on robustness and network routing. Using wavelength conversion to enhance robustness has received very little attention, whereas routing is the focus of substantial previous work. We will also explore the effectiveness of optically transparent paths as limited by noise and insertion loss in the devices, and will study the tradeoffs between network capacity and robustness given these routing limitations.&lt;br/&gt; At the device level, we will explore devices and subsystems that trade some functionality for increased simplicity or improved cost-effectiveness. The first of these elements is a multi-cavity VCSEL, which exploits the underlying physics to produce wavelength-tunable transmitters at a fraction of the cost of current tunable sources. These VC-SEL&apos;s will fill the gap between inexpensive, non-tunable VCSEL&apos;s available in a small range of wavelengths, and high-end, carrier-grade lasers. Our proposed tunable VCSEL&apos;s are intended to fulfill the requirements of MAN/LAN environments, which are a hybrid of current core networks and current LAN&apos;s.&lt;br/&gt; We also plan to develop optical wavelength converters based on dual-pump, four-wave mixing. The dual-pump design enhances both the efficiency and the range of the converter, and can also provide polarization independence. High efficiency is necessary to reduce or to eliminate the need for regeneration of optical signals within the MAN-LAN environment. Only a single wavelength enters the wavelength converter, thus the filter eliminates the effects of coherent and incoherent crosstalk between wavelengths that arises through four-wave mixing. SOA-based wavelength conversion is typically cheaper than other approaches, but suffers from poorer noise figures stemming from the use of the SOA, which make them less attractive for long-distance applications in core networks. For the MAN-LAN environment, SOA&apos;s present an attractive and economical alternative that allows ubiquitous availability of wavelength converters at switching nodes.&lt;br/&gt;Third, we will develop high-speed photodetectors based on indium-phosphide materials. The development of&lt;br/&gt;high-speed photodetectors helps to maintain fairly lean wavelength requirements in local and metropolitan areas, avoiding the challenges of dense WDM. While more aggressive scalability is attractive in many ways, a single 80 Gbps wavelength can move a terabyte of data in less than two minutes. A single wavelength with effective access mecha-nisms can be used as a virtual private network if appropriately deployed over a LAN/MAN infrastructure.&lt;br/&gt; Finally, we propose to develop tunable 2x2 switch elements based on indium phosphide ring resonators. Such devices allow a single wavelength to be selected and exchanged by the switch while all other wavelengths pass through untouched. These switches serve as building blocks for several important components, including tunable receivers and low insertion loss, low-crosstalk, high-speed optical crossconnects.&lt;br/&gt; The development of these network elements will guide the types of systems that the network architecture group examines. At the same time, results from the network architecture group in terms of maximum system gain for a given approach (i.e., wavelength conversion, add-drop capability) will influence the direction of the hardware technology development. We believe that this approach is the best method for optimizing the architecture of next-generation fiber-optic WDM systems within the framework of the network element technology.&lt;br/&gt; The researc</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121662</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121662</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1621" target="n1624">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: High-Speed Wavelength-Agile Optical Networks</data>
      <data key="e_abstract">We propose to explore the architecture and device development issues necessary to develop optical LAN&apos;s that are ready to interface with optical MAN&apos;s. Our goal is to develop a clear plan for integration of LAN&apos;s and MAN&apos;s in order to improve the degree to which the benefits of high bandwidth in the MAN&apos;s are delivered to end users on the LAN&apos;s. The application of architectural techniques to this problem cannot be effectively pursued without understanding the capabilities of proposed and available devices, yet the needs of architecture should strongly guide device development to ensure usefulness and relevance. To address these issues, we have formed a synergistic partnership between network architecture and hardware technology groups.&lt;br/&gt; At the architectural level, we will explore issues in the design and evaluation of robust optical LAN architectures with explicit focus on the means of access to MAN&apos;s and on the capabilities necessary for both LAN nodes and the MAN-LAN interfaces (MLI&apos;s). The MLI&apos;s will serve as both hub and head-end for the LAN and will provide a simple interface between the LAN and the MAN. We will explore the robustness of these architectures to faults and will quantify the benefits of exploiting wavelength conversion and tunable sources to improve a network&apos;s robustness to failures with a range of automatic protection algorithms. Using wavelength conversion at MLI&apos;s, we will study the impact of wavelength conversion on robustness and network routing. Using wavelength conversion to enhance robustness has received very little attention, whereas routing is the focus of substantial previous work. We will also explore the effectiveness of optically transparent paths as limited by noise and insertion loss in the devices, and will study the tradeoffs between network capacity and robustness given these routing limitations.&lt;br/&gt; At the device level, we will explore devices and subsystems that trade some functionality for increased simplicity or improved cost-effectiveness. The first of these elements is a multi-cavity VCSEL, which exploits the underlying physics to produce wavelength-tunable transmitters at a fraction of the cost of current tunable sources. These VC-SEL&apos;s will fill the gap between inexpensive, non-tunable VCSEL&apos;s available in a small range of wavelengths, and high-end, carrier-grade lasers. Our proposed tunable VCSEL&apos;s are intended to fulfill the requirements of MAN/LAN environments, which are a hybrid of current core networks and current LAN&apos;s.&lt;br/&gt; We also plan to develop optical wavelength converters based on dual-pump, four-wave mixing. The dual-pump design enhances both the efficiency and the range of the converter, and can also provide polarization independence. High efficiency is necessary to reduce or to eliminate the need for regeneration of optical signals within the MAN-LAN environment. Only a single wavelength enters the wavelength converter, thus the filter eliminates the effects of coherent and incoherent crosstalk between wavelengths that arises through four-wave mixing. SOA-based wavelength conversion is typically cheaper than other approaches, but suffers from poorer noise figures stemming from the use of the SOA, which make them less attractive for long-distance applications in core networks. For the MAN-LAN environment, SOA&apos;s present an attractive and economical alternative that allows ubiquitous availability of wavelength converters at switching nodes.&lt;br/&gt;Third, we will develop high-speed photodetectors based on indium-phosphide materials. The development of&lt;br/&gt;high-speed photodetectors helps to maintain fairly lean wavelength requirements in local and metropolitan areas, avoiding the challenges of dense WDM. While more aggressive scalability is attractive in many ways, a single 80 Gbps wavelength can move a terabyte of data in less than two minutes. A single wavelength with effective access mecha-nisms can be used as a virtual private network if appropriately deployed over a LAN/MAN infrastructure.&lt;br/&gt; Finally, we propose to develop tunable 2x2 switch elements based on indium phosphide ring resonators. Such devices allow a single wavelength to be selected and exchanged by the switch while all other wavelengths pass through untouched. These switches serve as building blocks for several important components, including tunable receivers and low insertion loss, low-crosstalk, high-speed optical crossconnects.&lt;br/&gt; The development of these network elements will guide the types of systems that the network architecture group examines. At the same time, results from the network architecture group in terms of maximum system gain for a given approach (i.e., wavelength conversion, add-drop capability) will influence the direction of the hardware technology development. We believe that this approach is the best method for optimizing the architecture of next-generation fiber-optic WDM systems within the framework of the network element technology.&lt;br/&gt; The researc</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121662</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121662</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1622" target="n1623">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: High-Speed Wavelength-Agile Optical Networks</data>
      <data key="e_abstract">We propose to explore the architecture and device development issues necessary to develop optical LAN&apos;s that are ready to interface with optical MAN&apos;s. Our goal is to develop a clear plan for integration of LAN&apos;s and MAN&apos;s in order to improve the degree to which the benefits of high bandwidth in the MAN&apos;s are delivered to end users on the LAN&apos;s. The application of architectural techniques to this problem cannot be effectively pursued without understanding the capabilities of proposed and available devices, yet the needs of architecture should strongly guide device development to ensure usefulness and relevance. To address these issues, we have formed a synergistic partnership between network architecture and hardware technology groups.&lt;br/&gt; At the architectural level, we will explore issues in the design and evaluation of robust optical LAN architectures with explicit focus on the means of access to MAN&apos;s and on the capabilities necessary for both LAN nodes and the MAN-LAN interfaces (MLI&apos;s). The MLI&apos;s will serve as both hub and head-end for the LAN and will provide a simple interface between the LAN and the MAN. We will explore the robustness of these architectures to faults and will quantify the benefits of exploiting wavelength conversion and tunable sources to improve a network&apos;s robustness to failures with a range of automatic protection algorithms. Using wavelength conversion at MLI&apos;s, we will study the impact of wavelength conversion on robustness and network routing. Using wavelength conversion to enhance robustness has received very little attention, whereas routing is the focus of substantial previous work. We will also explore the effectiveness of optically transparent paths as limited by noise and insertion loss in the devices, and will study the tradeoffs between network capacity and robustness given these routing limitations.&lt;br/&gt; At the device level, we will explore devices and subsystems that trade some functionality for increased simplicity or improved cost-effectiveness. The first of these elements is a multi-cavity VCSEL, which exploits the underlying physics to produce wavelength-tunable transmitters at a fraction of the cost of current tunable sources. These VC-SEL&apos;s will fill the gap between inexpensive, non-tunable VCSEL&apos;s available in a small range of wavelengths, and high-end, carrier-grade lasers. Our proposed tunable VCSEL&apos;s are intended to fulfill the requirements of MAN/LAN environments, which are a hybrid of current core networks and current LAN&apos;s.&lt;br/&gt; We also plan to develop optical wavelength converters based on dual-pump, four-wave mixing. The dual-pump design enhances both the efficiency and the range of the converter, and can also provide polarization independence. High efficiency is necessary to reduce or to eliminate the need for regeneration of optical signals within the MAN-LAN environment. Only a single wavelength enters the wavelength converter, thus the filter eliminates the effects of coherent and incoherent crosstalk between wavelengths that arises through four-wave mixing. SOA-based wavelength conversion is typically cheaper than other approaches, but suffers from poorer noise figures stemming from the use of the SOA, which make them less attractive for long-distance applications in core networks. For the MAN-LAN environment, SOA&apos;s present an attractive and economical alternative that allows ubiquitous availability of wavelength converters at switching nodes.&lt;br/&gt;Third, we will develop high-speed photodetectors based on indium-phosphide materials. The development of&lt;br/&gt;high-speed photodetectors helps to maintain fairly lean wavelength requirements in local and metropolitan areas, avoiding the challenges of dense WDM. While more aggressive scalability is attractive in many ways, a single 80 Gbps wavelength can move a terabyte of data in less than two minutes. A single wavelength with effective access mecha-nisms can be used as a virtual private network if appropriately deployed over a LAN/MAN infrastructure.&lt;br/&gt; Finally, we propose to develop tunable 2x2 switch elements based on indium phosphide ring resonators. Such devices allow a single wavelength to be selected and exchanged by the switch while all other wavelengths pass through untouched. These switches serve as building blocks for several important components, including tunable receivers and low insertion loss, low-crosstalk, high-speed optical crossconnects.&lt;br/&gt; The development of these network elements will guide the types of systems that the network architecture group examines. At the same time, results from the network architecture group in terms of maximum system gain for a given approach (i.e., wavelength conversion, add-drop capability) will influence the direction of the hardware technology development. We believe that this approach is the best method for optimizing the architecture of next-generation fiber-optic WDM systems within the framework of the network element technology.&lt;br/&gt; The researc</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121662</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121662</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1622" target="n1624">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: High-Speed Wavelength-Agile Optical Networks</data>
      <data key="e_abstract">We propose to explore the architecture and device development issues necessary to develop optical LAN&apos;s that are ready to interface with optical MAN&apos;s. Our goal is to develop a clear plan for integration of LAN&apos;s and MAN&apos;s in order to improve the degree to which the benefits of high bandwidth in the MAN&apos;s are delivered to end users on the LAN&apos;s. The application of architectural techniques to this problem cannot be effectively pursued without understanding the capabilities of proposed and available devices, yet the needs of architecture should strongly guide device development to ensure usefulness and relevance. To address these issues, we have formed a synergistic partnership between network architecture and hardware technology groups.&lt;br/&gt; At the architectural level, we will explore issues in the design and evaluation of robust optical LAN architectures with explicit focus on the means of access to MAN&apos;s and on the capabilities necessary for both LAN nodes and the MAN-LAN interfaces (MLI&apos;s). The MLI&apos;s will serve as both hub and head-end for the LAN and will provide a simple interface between the LAN and the MAN. We will explore the robustness of these architectures to faults and will quantify the benefits of exploiting wavelength conversion and tunable sources to improve a network&apos;s robustness to failures with a range of automatic protection algorithms. Using wavelength conversion at MLI&apos;s, we will study the impact of wavelength conversion on robustness and network routing. Using wavelength conversion to enhance robustness has received very little attention, whereas routing is the focus of substantial previous work. We will also explore the effectiveness of optically transparent paths as limited by noise and insertion loss in the devices, and will study the tradeoffs between network capacity and robustness given these routing limitations.&lt;br/&gt; At the device level, we will explore devices and subsystems that trade some functionality for increased simplicity or improved cost-effectiveness. The first of these elements is a multi-cavity VCSEL, which exploits the underlying physics to produce wavelength-tunable transmitters at a fraction of the cost of current tunable sources. These VC-SEL&apos;s will fill the gap between inexpensive, non-tunable VCSEL&apos;s available in a small range of wavelengths, and high-end, carrier-grade lasers. Our proposed tunable VCSEL&apos;s are intended to fulfill the requirements of MAN/LAN environments, which are a hybrid of current core networks and current LAN&apos;s.&lt;br/&gt; We also plan to develop optical wavelength converters based on dual-pump, four-wave mixing. The dual-pump design enhances both the efficiency and the range of the converter, and can also provide polarization independence. High efficiency is necessary to reduce or to eliminate the need for regeneration of optical signals within the MAN-LAN environment. Only a single wavelength enters the wavelength converter, thus the filter eliminates the effects of coherent and incoherent crosstalk between wavelengths that arises through four-wave mixing. SOA-based wavelength conversion is typically cheaper than other approaches, but suffers from poorer noise figures stemming from the use of the SOA, which make them less attractive for long-distance applications in core networks. For the MAN-LAN environment, SOA&apos;s present an attractive and economical alternative that allows ubiquitous availability of wavelength converters at switching nodes.&lt;br/&gt;Third, we will develop high-speed photodetectors based on indium-phosphide materials. The development of&lt;br/&gt;high-speed photodetectors helps to maintain fairly lean wavelength requirements in local and metropolitan areas, avoiding the challenges of dense WDM. While more aggressive scalability is attractive in many ways, a single 80 Gbps wavelength can move a terabyte of data in less than two minutes. A single wavelength with effective access mecha-nisms can be used as a virtual private network if appropriately deployed over a LAN/MAN infrastructure.&lt;br/&gt; Finally, we propose to develop tunable 2x2 switch elements based on indium phosphide ring resonators. Such devices allow a single wavelength to be selected and exchanged by the switch while all other wavelengths pass through untouched. These switches serve as building blocks for several important components, including tunable receivers and low insertion loss, low-crosstalk, high-speed optical crossconnects.&lt;br/&gt; The development of these network elements will guide the types of systems that the network architecture group examines. At the same time, results from the network architecture group in terms of maximum system gain for a given approach (i.e., wavelength conversion, add-drop capability) will influence the direction of the hardware technology development. We believe that this approach is the best method for optimizing the architecture of next-generation fiber-optic WDM systems within the framework of the network element technology.&lt;br/&gt; The researc</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121662</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121662</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1623" target="n1624">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: High-Speed Wavelength-Agile Optical Networks</data>
      <data key="e_abstract">We propose to explore the architecture and device development issues necessary to develop optical LAN&apos;s that are ready to interface with optical MAN&apos;s. Our goal is to develop a clear plan for integration of LAN&apos;s and MAN&apos;s in order to improve the degree to which the benefits of high bandwidth in the MAN&apos;s are delivered to end users on the LAN&apos;s. The application of architectural techniques to this problem cannot be effectively pursued without understanding the capabilities of proposed and available devices, yet the needs of architecture should strongly guide device development to ensure usefulness and relevance. To address these issues, we have formed a synergistic partnership between network architecture and hardware technology groups.&lt;br/&gt; At the architectural level, we will explore issues in the design and evaluation of robust optical LAN architectures with explicit focus on the means of access to MAN&apos;s and on the capabilities necessary for both LAN nodes and the MAN-LAN interfaces (MLI&apos;s). The MLI&apos;s will serve as both hub and head-end for the LAN and will provide a simple interface between the LAN and the MAN. We will explore the robustness of these architectures to faults and will quantify the benefits of exploiting wavelength conversion and tunable sources to improve a network&apos;s robustness to failures with a range of automatic protection algorithms. Using wavelength conversion at MLI&apos;s, we will study the impact of wavelength conversion on robustness and network routing. Using wavelength conversion to enhance robustness has received very little attention, whereas routing is the focus of substantial previous work. We will also explore the effectiveness of optically transparent paths as limited by noise and insertion loss in the devices, and will study the tradeoffs between network capacity and robustness given these routing limitations.&lt;br/&gt; At the device level, we will explore devices and subsystems that trade some functionality for increased simplicity or improved cost-effectiveness. The first of these elements is a multi-cavity VCSEL, which exploits the underlying physics to produce wavelength-tunable transmitters at a fraction of the cost of current tunable sources. These VC-SEL&apos;s will fill the gap between inexpensive, non-tunable VCSEL&apos;s available in a small range of wavelengths, and high-end, carrier-grade lasers. Our proposed tunable VCSEL&apos;s are intended to fulfill the requirements of MAN/LAN environments, which are a hybrid of current core networks and current LAN&apos;s.&lt;br/&gt; We also plan to develop optical wavelength converters based on dual-pump, four-wave mixing. The dual-pump design enhances both the efficiency and the range of the converter, and can also provide polarization independence. High efficiency is necessary to reduce or to eliminate the need for regeneration of optical signals within the MAN-LAN environment. Only a single wavelength enters the wavelength converter, thus the filter eliminates the effects of coherent and incoherent crosstalk between wavelengths that arises through four-wave mixing. SOA-based wavelength conversion is typically cheaper than other approaches, but suffers from poorer noise figures stemming from the use of the SOA, which make them less attractive for long-distance applications in core networks. For the MAN-LAN environment, SOA&apos;s present an attractive and economical alternative that allows ubiquitous availability of wavelength converters at switching nodes.&lt;br/&gt;Third, we will develop high-speed photodetectors based on indium-phosphide materials. The development of&lt;br/&gt;high-speed photodetectors helps to maintain fairly lean wavelength requirements in local and metropolitan areas, avoiding the challenges of dense WDM. While more aggressive scalability is attractive in many ways, a single 80 Gbps wavelength can move a terabyte of data in less than two minutes. A single wavelength with effective access mecha-nisms can be used as a virtual private network if appropriately deployed over a LAN/MAN infrastructure.&lt;br/&gt; Finally, we propose to develop tunable 2x2 switch elements based on indium phosphide ring resonators. Such devices allow a single wavelength to be selected and exchanged by the switch while all other wavelengths pass through untouched. These switches serve as building blocks for several important components, including tunable receivers and low insertion loss, low-crosstalk, high-speed optical crossconnects.&lt;br/&gt; The development of these network elements will guide the types of systems that the network architecture group examines. At the same time, results from the network architecture group in terms of maximum system gain for a given approach (i.e., wavelength conversion, add-drop capability) will influence the direction of the hardware technology development. We believe that this approach is the best method for optimizing the architecture of next-generation fiber-optic WDM systems within the framework of the network element technology.&lt;br/&gt; The researc</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121662</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121662</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1625" target="n1626">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">IGERT: Evolutionary, Computational, and Molecular Approaches to Genome Structure and Function</data>
      <data key="e_abstract">This multidisciplinary IGERT program will train graduate students in areas at the interface of evolutionary biology, functional genomics, and computational biology in a way that will enable them to collaborate productively across traditional disciplinary boundaries. The emerging fields of comparative and functional genomics will reshape biological research in the next twenty years and have a profound impact on medicine and human health, agriculture, engineering, and our understanding of the origin of life and the relationships among living organisms. Research in these emerging disciplines requires the coordinated interaction of scientists with diverse backgrounds in evolution, molecular biology, and computer science, yet current departmental boundaries at most universities do not foster interactions among these areas. The comprehensive IGERT training plan at the University of Arizona is intended to meet these challenges. This program includes advising, research rotations and colloquia, two novel multidisciplinary courses that engage students in hands-on problem solving, additional lecture and laboratory courses, training in ethics, a monthly discussion group, opportunities to interact with visiting scientists, bi-annual mini-symposia, opportunities to mentor undergraduates in research, and offsite internships at other academic institutions or at leading genomics companies. Finally, the training plan includes a strategy to recruit both minority undergraduate and graduate students into science, drawing on the Hispanic and Native American populations in Arizona. Some of the proposed training initiatives build on existing structures; others are entirely new. The University of Arizona is particularly well suited for this training program because of existing strengths in evolution, functional genomics, and computation, a strong graduate program, and an institutional commitment to interdisciplinary programs. Moreover, the individual participants in this program have a long track record of success in training students and a commitment to multidisciplinary collaborations. The impact of the proposed training program uniting three research areas will be to equip the next generation of biologists with the tools to tackle the challenges of genome-scale research.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Biological Sciences; Engineering; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114420</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114420</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1625" target="n1627">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">IGERT: Evolutionary, Computational, and Molecular Approaches to Genome Structure and Function</data>
      <data key="e_abstract">This multidisciplinary IGERT program will train graduate students in areas at the interface of evolutionary biology, functional genomics, and computational biology in a way that will enable them to collaborate productively across traditional disciplinary boundaries. The emerging fields of comparative and functional genomics will reshape biological research in the next twenty years and have a profound impact on medicine and human health, agriculture, engineering, and our understanding of the origin of life and the relationships among living organisms. Research in these emerging disciplines requires the coordinated interaction of scientists with diverse backgrounds in evolution, molecular biology, and computer science, yet current departmental boundaries at most universities do not foster interactions among these areas. The comprehensive IGERT training plan at the University of Arizona is intended to meet these challenges. This program includes advising, research rotations and colloquia, two novel multidisciplinary courses that engage students in hands-on problem solving, additional lecture and laboratory courses, training in ethics, a monthly discussion group, opportunities to interact with visiting scientists, bi-annual mini-symposia, opportunities to mentor undergraduates in research, and offsite internships at other academic institutions or at leading genomics companies. Finally, the training plan includes a strategy to recruit both minority undergraduate and graduate students into science, drawing on the Hispanic and Native American populations in Arizona. Some of the proposed training initiatives build on existing structures; others are entirely new. The University of Arizona is particularly well suited for this training program because of existing strengths in evolution, functional genomics, and computation, a strong graduate program, and an institutional commitment to interdisciplinary programs. Moreover, the individual participants in this program have a long track record of success in training students and a commitment to multidisciplinary collaborations. The impact of the proposed training program uniting three research areas will be to equip the next generation of biologists with the tools to tackle the challenges of genome-scale research.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Biological Sciences; Engineering; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114420</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114420</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1625" target="n1628">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">IGERT: Evolutionary, Computational, and Molecular Approaches to Genome Structure and Function</data>
      <data key="e_abstract">This multidisciplinary IGERT program will train graduate students in areas at the interface of evolutionary biology, functional genomics, and computational biology in a way that will enable them to collaborate productively across traditional disciplinary boundaries. The emerging fields of comparative and functional genomics will reshape biological research in the next twenty years and have a profound impact on medicine and human health, agriculture, engineering, and our understanding of the origin of life and the relationships among living organisms. Research in these emerging disciplines requires the coordinated interaction of scientists with diverse backgrounds in evolution, molecular biology, and computer science, yet current departmental boundaries at most universities do not foster interactions among these areas. The comprehensive IGERT training plan at the University of Arizona is intended to meet these challenges. This program includes advising, research rotations and colloquia, two novel multidisciplinary courses that engage students in hands-on problem solving, additional lecture and laboratory courses, training in ethics, a monthly discussion group, opportunities to interact with visiting scientists, bi-annual mini-symposia, opportunities to mentor undergraduates in research, and offsite internships at other academic institutions or at leading genomics companies. Finally, the training plan includes a strategy to recruit both minority undergraduate and graduate students into science, drawing on the Hispanic and Native American populations in Arizona. Some of the proposed training initiatives build on existing structures; others are entirely new. The University of Arizona is particularly well suited for this training program because of existing strengths in evolution, functional genomics, and computation, a strong graduate program, and an institutional commitment to interdisciplinary programs. Moreover, the individual participants in this program have a long track record of success in training students and a commitment to multidisciplinary collaborations. The impact of the proposed training program uniting three research areas will be to equip the next generation of biologists with the tools to tackle the challenges of genome-scale research.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Biological Sciences; Engineering; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114420</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114420</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1625" target="n1629">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">IGERT: Evolutionary, Computational, and Molecular Approaches to Genome Structure and Function</data>
      <data key="e_abstract">This multidisciplinary IGERT program will train graduate students in areas at the interface of evolutionary biology, functional genomics, and computational biology in a way that will enable them to collaborate productively across traditional disciplinary boundaries. The emerging fields of comparative and functional genomics will reshape biological research in the next twenty years and have a profound impact on medicine and human health, agriculture, engineering, and our understanding of the origin of life and the relationships among living organisms. Research in these emerging disciplines requires the coordinated interaction of scientists with diverse backgrounds in evolution, molecular biology, and computer science, yet current departmental boundaries at most universities do not foster interactions among these areas. The comprehensive IGERT training plan at the University of Arizona is intended to meet these challenges. This program includes advising, research rotations and colloquia, two novel multidisciplinary courses that engage students in hands-on problem solving, additional lecture and laboratory courses, training in ethics, a monthly discussion group, opportunities to interact with visiting scientists, bi-annual mini-symposia, opportunities to mentor undergraduates in research, and offsite internships at other academic institutions or at leading genomics companies. Finally, the training plan includes a strategy to recruit both minority undergraduate and graduate students into science, drawing on the Hispanic and Native American populations in Arizona. Some of the proposed training initiatives build on existing structures; others are entirely new. The University of Arizona is particularly well suited for this training program because of existing strengths in evolution, functional genomics, and computation, a strong graduate program, and an institutional commitment to interdisciplinary programs. Moreover, the individual participants in this program have a long track record of success in training students and a commitment to multidisciplinary collaborations. The impact of the proposed training program uniting three research areas will be to equip the next generation of biologists with the tools to tackle the challenges of genome-scale research.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Biological Sciences; Engineering; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114420</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114420</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1626" target="n1627">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">IGERT: Evolutionary, Computational, and Molecular Approaches to Genome Structure and Function</data>
      <data key="e_abstract">This multidisciplinary IGERT program will train graduate students in areas at the interface of evolutionary biology, functional genomics, and computational biology in a way that will enable them to collaborate productively across traditional disciplinary boundaries. The emerging fields of comparative and functional genomics will reshape biological research in the next twenty years and have a profound impact on medicine and human health, agriculture, engineering, and our understanding of the origin of life and the relationships among living organisms. Research in these emerging disciplines requires the coordinated interaction of scientists with diverse backgrounds in evolution, molecular biology, and computer science, yet current departmental boundaries at most universities do not foster interactions among these areas. The comprehensive IGERT training plan at the University of Arizona is intended to meet these challenges. This program includes advising, research rotations and colloquia, two novel multidisciplinary courses that engage students in hands-on problem solving, additional lecture and laboratory courses, training in ethics, a monthly discussion group, opportunities to interact with visiting scientists, bi-annual mini-symposia, opportunities to mentor undergraduates in research, and offsite internships at other academic institutions or at leading genomics companies. Finally, the training plan includes a strategy to recruit both minority undergraduate and graduate students into science, drawing on the Hispanic and Native American populations in Arizona. Some of the proposed training initiatives build on existing structures; others are entirely new. The University of Arizona is particularly well suited for this training program because of existing strengths in evolution, functional genomics, and computation, a strong graduate program, and an institutional commitment to interdisciplinary programs. Moreover, the individual participants in this program have a long track record of success in training students and a commitment to multidisciplinary collaborations. The impact of the proposed training program uniting three research areas will be to equip the next generation of biologists with the tools to tackle the challenges of genome-scale research.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Biological Sciences; Engineering; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114420</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114420</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1626" target="n1628">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">IGERT: Evolutionary, Computational, and Molecular Approaches to Genome Structure and Function</data>
      <data key="e_abstract">This multidisciplinary IGERT program will train graduate students in areas at the interface of evolutionary biology, functional genomics, and computational biology in a way that will enable them to collaborate productively across traditional disciplinary boundaries. The emerging fields of comparative and functional genomics will reshape biological research in the next twenty years and have a profound impact on medicine and human health, agriculture, engineering, and our understanding of the origin of life and the relationships among living organisms. Research in these emerging disciplines requires the coordinated interaction of scientists with diverse backgrounds in evolution, molecular biology, and computer science, yet current departmental boundaries at most universities do not foster interactions among these areas. The comprehensive IGERT training plan at the University of Arizona is intended to meet these challenges. This program includes advising, research rotations and colloquia, two novel multidisciplinary courses that engage students in hands-on problem solving, additional lecture and laboratory courses, training in ethics, a monthly discussion group, opportunities to interact with visiting scientists, bi-annual mini-symposia, opportunities to mentor undergraduates in research, and offsite internships at other academic institutions or at leading genomics companies. Finally, the training plan includes a strategy to recruit both minority undergraduate and graduate students into science, drawing on the Hispanic and Native American populations in Arizona. Some of the proposed training initiatives build on existing structures; others are entirely new. The University of Arizona is particularly well suited for this training program because of existing strengths in evolution, functional genomics, and computation, a strong graduate program, and an institutional commitment to interdisciplinary programs. Moreover, the individual participants in this program have a long track record of success in training students and a commitment to multidisciplinary collaborations. The impact of the proposed training program uniting three research areas will be to equip the next generation of biologists with the tools to tackle the challenges of genome-scale research.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Biological Sciences; Engineering; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114420</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114420</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1626" target="n1629">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">IGERT: Evolutionary, Computational, and Molecular Approaches to Genome Structure and Function</data>
      <data key="e_abstract">This multidisciplinary IGERT program will train graduate students in areas at the interface of evolutionary biology, functional genomics, and computational biology in a way that will enable them to collaborate productively across traditional disciplinary boundaries. The emerging fields of comparative and functional genomics will reshape biological research in the next twenty years and have a profound impact on medicine and human health, agriculture, engineering, and our understanding of the origin of life and the relationships among living organisms. Research in these emerging disciplines requires the coordinated interaction of scientists with diverse backgrounds in evolution, molecular biology, and computer science, yet current departmental boundaries at most universities do not foster interactions among these areas. The comprehensive IGERT training plan at the University of Arizona is intended to meet these challenges. This program includes advising, research rotations and colloquia, two novel multidisciplinary courses that engage students in hands-on problem solving, additional lecture and laboratory courses, training in ethics, a monthly discussion group, opportunities to interact with visiting scientists, bi-annual mini-symposia, opportunities to mentor undergraduates in research, and offsite internships at other academic institutions or at leading genomics companies. Finally, the training plan includes a strategy to recruit both minority undergraduate and graduate students into science, drawing on the Hispanic and Native American populations in Arizona. Some of the proposed training initiatives build on existing structures; others are entirely new. The University of Arizona is particularly well suited for this training program because of existing strengths in evolution, functional genomics, and computation, a strong graduate program, and an institutional commitment to interdisciplinary programs. Moreover, the individual participants in this program have a long track record of success in training students and a commitment to multidisciplinary collaborations. The impact of the proposed training program uniting three research areas will be to equip the next generation of biologists with the tools to tackle the challenges of genome-scale research.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Biological Sciences; Engineering; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114420</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114420</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1627" target="n1628">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">IGERT: Evolutionary, Computational, and Molecular Approaches to Genome Structure and Function</data>
      <data key="e_abstract">This multidisciplinary IGERT program will train graduate students in areas at the interface of evolutionary biology, functional genomics, and computational biology in a way that will enable them to collaborate productively across traditional disciplinary boundaries. The emerging fields of comparative and functional genomics will reshape biological research in the next twenty years and have a profound impact on medicine and human health, agriculture, engineering, and our understanding of the origin of life and the relationships among living organisms. Research in these emerging disciplines requires the coordinated interaction of scientists with diverse backgrounds in evolution, molecular biology, and computer science, yet current departmental boundaries at most universities do not foster interactions among these areas. The comprehensive IGERT training plan at the University of Arizona is intended to meet these challenges. This program includes advising, research rotations and colloquia, two novel multidisciplinary courses that engage students in hands-on problem solving, additional lecture and laboratory courses, training in ethics, a monthly discussion group, opportunities to interact with visiting scientists, bi-annual mini-symposia, opportunities to mentor undergraduates in research, and offsite internships at other academic institutions or at leading genomics companies. Finally, the training plan includes a strategy to recruit both minority undergraduate and graduate students into science, drawing on the Hispanic and Native American populations in Arizona. Some of the proposed training initiatives build on existing structures; others are entirely new. The University of Arizona is particularly well suited for this training program because of existing strengths in evolution, functional genomics, and computation, a strong graduate program, and an institutional commitment to interdisciplinary programs. Moreover, the individual participants in this program have a long track record of success in training students and a commitment to multidisciplinary collaborations. The impact of the proposed training program uniting three research areas will be to equip the next generation of biologists with the tools to tackle the challenges of genome-scale research.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Biological Sciences; Engineering; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114420</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114420</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1627" target="n1629">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">IGERT: Evolutionary, Computational, and Molecular Approaches to Genome Structure and Function</data>
      <data key="e_abstract">This multidisciplinary IGERT program will train graduate students in areas at the interface of evolutionary biology, functional genomics, and computational biology in a way that will enable them to collaborate productively across traditional disciplinary boundaries. The emerging fields of comparative and functional genomics will reshape biological research in the next twenty years and have a profound impact on medicine and human health, agriculture, engineering, and our understanding of the origin of life and the relationships among living organisms. Research in these emerging disciplines requires the coordinated interaction of scientists with diverse backgrounds in evolution, molecular biology, and computer science, yet current departmental boundaries at most universities do not foster interactions among these areas. The comprehensive IGERT training plan at the University of Arizona is intended to meet these challenges. This program includes advising, research rotations and colloquia, two novel multidisciplinary courses that engage students in hands-on problem solving, additional lecture and laboratory courses, training in ethics, a monthly discussion group, opportunities to interact with visiting scientists, bi-annual mini-symposia, opportunities to mentor undergraduates in research, and offsite internships at other academic institutions or at leading genomics companies. Finally, the training plan includes a strategy to recruit both minority undergraduate and graduate students into science, drawing on the Hispanic and Native American populations in Arizona. Some of the proposed training initiatives build on existing structures; others are entirely new. The University of Arizona is particularly well suited for this training program because of existing strengths in evolution, functional genomics, and computation, a strong graduate program, and an institutional commitment to interdisciplinary programs. Moreover, the individual participants in this program have a long track record of success in training students and a commitment to multidisciplinary collaborations. The impact of the proposed training program uniting three research areas will be to equip the next generation of biologists with the tools to tackle the challenges of genome-scale research.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Biological Sciences; Engineering; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114420</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114420</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1628" target="n1629">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">IGERT: Evolutionary, Computational, and Molecular Approaches to Genome Structure and Function</data>
      <data key="e_abstract">This multidisciplinary IGERT program will train graduate students in areas at the interface of evolutionary biology, functional genomics, and computational biology in a way that will enable them to collaborate productively across traditional disciplinary boundaries. The emerging fields of comparative and functional genomics will reshape biological research in the next twenty years and have a profound impact on medicine and human health, agriculture, engineering, and our understanding of the origin of life and the relationships among living organisms. Research in these emerging disciplines requires the coordinated interaction of scientists with diverse backgrounds in evolution, molecular biology, and computer science, yet current departmental boundaries at most universities do not foster interactions among these areas. The comprehensive IGERT training plan at the University of Arizona is intended to meet these challenges. This program includes advising, research rotations and colloquia, two novel multidisciplinary courses that engage students in hands-on problem solving, additional lecture and laboratory courses, training in ethics, a monthly discussion group, opportunities to interact with visiting scientists, bi-annual mini-symposia, opportunities to mentor undergraduates in research, and offsite internships at other academic institutions or at leading genomics companies. Finally, the training plan includes a strategy to recruit both minority undergraduate and graduate students into science, drawing on the Hispanic and Native American populations in Arizona. Some of the proposed training initiatives build on existing structures; others are entirely new. The University of Arizona is particularly well suited for this training program because of existing strengths in evolution, functional genomics, and computation, a strong graduate program, and an institutional commitment to interdisciplinary programs. Moreover, the individual participants in this program have a long track record of success in training students and a commitment to multidisciplinary collaborations. The impact of the proposed training program uniting three research areas will be to equip the next generation of biologists with the tools to tackle the challenges of genome-scale research.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Biological Sciences; Engineering; Computer and Information Science and Engineering; and Education and Human Resources.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">114420</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">114420</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1630" target="n1631">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Postdoc: Integrating Soft Segmentation With Intensity-Based Matching for 2D/3D Image Data Registration</data>
      <data key="e_abstract">0104114&lt;br/&gt;Shahidi, Ramin&lt;br/&gt;Stanford University&lt;br/&gt;&lt;br/&gt;CISE Postdoctoral Associates in Experimental Computer Science: Integrating Soft Segmentation with Intensity-Based Matching for 2D/3D Image Data Registration&lt;br/&gt;&lt;br/&gt;&apos;Soft&apos; segmentation is a recent innovation in image processing which attempts to preserve the maximum amount of information possible in an image while classifying various image elements. Soft segmentation produces flexible (e.g., fuzzy or probabilistic) labels for individual pixels in an image as opposed to forcing a decision about each pixel. Soft segmentation is an effective method for noisy images, such as intra-operative fluoroscopic x-ray images, where preservation of information is critical. In recent years, a variety of promising voxel-property or intensity-based matching algorithms have been developed for three-dimensional (3D) medical image registration, but these algorithms are inadequate in the presence of significant noise. Spine images contain rigid elements (vertebrae) within a deformable structure (spine). Registration is performed on a single vertebra, therefore, spine images contain both structured and unstructured noise. Research is proposed to apply soft labels to the segmentation of two-dimensional (2D) images of the spine for 2D/3D image registration. The postdoctoral associate will assist in 1) adapting an existing fiducial-based clinical spinal navigation system to use image-based fine registration using pre-segmented images, 2) developing a semi-automated segmentation of fluoroscopic images growing a bounding-box around the region of interest, 3) developing both fuzzy and probabilistic &apos;soft&apos; labels for segmenting the fluoroscopic images, and 4) the convolution of soft labels into gradient and mutual information-based intensity-based matching algorithms.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">104114</data>
      <data key="e_expirationDate">2003-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">104114</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1630" target="n1632">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Postdoc: Integrating Soft Segmentation With Intensity-Based Matching for 2D/3D Image Data Registration</data>
      <data key="e_abstract">0104114&lt;br/&gt;Shahidi, Ramin&lt;br/&gt;Stanford University&lt;br/&gt;&lt;br/&gt;CISE Postdoctoral Associates in Experimental Computer Science: Integrating Soft Segmentation with Intensity-Based Matching for 2D/3D Image Data Registration&lt;br/&gt;&lt;br/&gt;&apos;Soft&apos; segmentation is a recent innovation in image processing which attempts to preserve the maximum amount of information possible in an image while classifying various image elements. Soft segmentation produces flexible (e.g., fuzzy or probabilistic) labels for individual pixels in an image as opposed to forcing a decision about each pixel. Soft segmentation is an effective method for noisy images, such as intra-operative fluoroscopic x-ray images, where preservation of information is critical. In recent years, a variety of promising voxel-property or intensity-based matching algorithms have been developed for three-dimensional (3D) medical image registration, but these algorithms are inadequate in the presence of significant noise. Spine images contain rigid elements (vertebrae) within a deformable structure (spine). Registration is performed on a single vertebra, therefore, spine images contain both structured and unstructured noise. Research is proposed to apply soft labels to the segmentation of two-dimensional (2D) images of the spine for 2D/3D image registration. The postdoctoral associate will assist in 1) adapting an existing fiducial-based clinical spinal navigation system to use image-based fine registration using pre-segmented images, 2) developing a semi-automated segmentation of fluoroscopic images growing a bounding-box around the region of interest, 3) developing both fuzzy and probabilistic &apos;soft&apos; labels for segmenting the fluoroscopic images, and 4) the convolution of soft labels into gradient and mutual information-based intensity-based matching algorithms.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">104114</data>
      <data key="e_expirationDate">2003-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">104114</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1631" target="n1632">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Postdoc: Integrating Soft Segmentation With Intensity-Based Matching for 2D/3D Image Data Registration</data>
      <data key="e_abstract">0104114&lt;br/&gt;Shahidi, Ramin&lt;br/&gt;Stanford University&lt;br/&gt;&lt;br/&gt;CISE Postdoctoral Associates in Experimental Computer Science: Integrating Soft Segmentation with Intensity-Based Matching for 2D/3D Image Data Registration&lt;br/&gt;&lt;br/&gt;&apos;Soft&apos; segmentation is a recent innovation in image processing which attempts to preserve the maximum amount of information possible in an image while classifying various image elements. Soft segmentation produces flexible (e.g., fuzzy or probabilistic) labels for individual pixels in an image as opposed to forcing a decision about each pixel. Soft segmentation is an effective method for noisy images, such as intra-operative fluoroscopic x-ray images, where preservation of information is critical. In recent years, a variety of promising voxel-property or intensity-based matching algorithms have been developed for three-dimensional (3D) medical image registration, but these algorithms are inadequate in the presence of significant noise. Spine images contain rigid elements (vertebrae) within a deformable structure (spine). Registration is performed on a single vertebra, therefore, spine images contain both structured and unstructured noise. Research is proposed to apply soft labels to the segmentation of two-dimensional (2D) images of the spine for 2D/3D image registration. The postdoctoral associate will assist in 1) adapting an existing fiducial-based clinical spinal navigation system to use image-based fine registration using pre-segmented images, 2) developing a semi-automated segmentation of fluoroscopic images growing a bounding-box around the region of interest, 3) developing both fuzzy and probabilistic &apos;soft&apos; labels for segmenting the fluoroscopic images, and 4) the convolution of soft labels into gradient and mutual information-based intensity-based matching algorithms.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">104114</data>
      <data key="e_expirationDate">2003-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">104114</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1421" target="n1634">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Agent Based Scalable Enterprise System for Enterprise Co-design</data>
      <data key="e_abstract">The objective of this Scalable Enterprise Systems award is the creation of an agent-based framework for the simulation of product design, marketing and supply-chain as collaborative processes. This project will develop the enabling theories and technologies needed to support its realization. The framework will use computational agents modeling the human behavior, artificial agents modeling synthetic economies, and constructive agents to address strategic, tactical and operational decision-making problems in simulating product life cycle. These agents use both analytic models and adaptive algorithms such as generic algorithms, fuzzy logic, and neural networks. The agents will compete to provide solutions in these simulations using innovative market metaphors such as auctions and recommendation systems. The research tasks include requirements analysis, preliminary functional analysis, methodology development, initial prototyping, testing, and specifications for the framework. &lt;br/&gt;&lt;br/&gt;If successful, this research will allow organizations to become aligned to and focused on to changing markets and changing customer values. The proposed collaborative design framework will test strategies necessary to ensure that the product delivery process, the product development process, and marketing initiatives are aligned to address the multiple dimensions of customer values simultaneously. The results will apply in the modeling of the supply chain for convergence technologies and the development of business models for electronic bandwidth exchange. The prototype will be used to support teaching in several courses in Economics and Management.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122214</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">122214</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1421" target="n1635">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Agent Based Scalable Enterprise System for Enterprise Co-design</data>
      <data key="e_abstract">The objective of this Scalable Enterprise Systems award is the creation of an agent-based framework for the simulation of product design, marketing and supply-chain as collaborative processes. This project will develop the enabling theories and technologies needed to support its realization. The framework will use computational agents modeling the human behavior, artificial agents modeling synthetic economies, and constructive agents to address strategic, tactical and operational decision-making problems in simulating product life cycle. These agents use both analytic models and adaptive algorithms such as generic algorithms, fuzzy logic, and neural networks. The agents will compete to provide solutions in these simulations using innovative market metaphors such as auctions and recommendation systems. The research tasks include requirements analysis, preliminary functional analysis, methodology development, initial prototyping, testing, and specifications for the framework. &lt;br/&gt;&lt;br/&gt;If successful, this research will allow organizations to become aligned to and focused on to changing markets and changing customer values. The proposed collaborative design framework will test strategies necessary to ensure that the product delivery process, the product development process, and marketing initiatives are aligned to address the multiple dimensions of customer values simultaneously. The results will apply in the modeling of the supply chain for convergence technologies and the development of business models for electronic bandwidth exchange. The prototype will be used to support teaching in several courses in Economics and Management.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122214</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">122214</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1421" target="n1636">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Agent Based Scalable Enterprise System for Enterprise Co-design</data>
      <data key="e_abstract">The objective of this Scalable Enterprise Systems award is the creation of an agent-based framework for the simulation of product design, marketing and supply-chain as collaborative processes. This project will develop the enabling theories and technologies needed to support its realization. The framework will use computational agents modeling the human behavior, artificial agents modeling synthetic economies, and constructive agents to address strategic, tactical and operational decision-making problems in simulating product life cycle. These agents use both analytic models and adaptive algorithms such as generic algorithms, fuzzy logic, and neural networks. The agents will compete to provide solutions in these simulations using innovative market metaphors such as auctions and recommendation systems. The research tasks include requirements analysis, preliminary functional analysis, methodology development, initial prototyping, testing, and specifications for the framework. &lt;br/&gt;&lt;br/&gt;If successful, this research will allow organizations to become aligned to and focused on to changing markets and changing customer values. The proposed collaborative design framework will test strategies necessary to ensure that the product delivery process, the product development process, and marketing initiatives are aligned to address the multiple dimensions of customer values simultaneously. The results will apply in the modeling of the supply chain for convergence technologies and the development of business models for electronic bandwidth exchange. The prototype will be used to support teaching in several courses in Economics and Management.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122214</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">122214</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1634" target="n1635">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Agent Based Scalable Enterprise System for Enterprise Co-design</data>
      <data key="e_abstract">The objective of this Scalable Enterprise Systems award is the creation of an agent-based framework for the simulation of product design, marketing and supply-chain as collaborative processes. This project will develop the enabling theories and technologies needed to support its realization. The framework will use computational agents modeling the human behavior, artificial agents modeling synthetic economies, and constructive agents to address strategic, tactical and operational decision-making problems in simulating product life cycle. These agents use both analytic models and adaptive algorithms such as generic algorithms, fuzzy logic, and neural networks. The agents will compete to provide solutions in these simulations using innovative market metaphors such as auctions and recommendation systems. The research tasks include requirements analysis, preliminary functional analysis, methodology development, initial prototyping, testing, and specifications for the framework. &lt;br/&gt;&lt;br/&gt;If successful, this research will allow organizations to become aligned to and focused on to changing markets and changing customer values. The proposed collaborative design framework will test strategies necessary to ensure that the product delivery process, the product development process, and marketing initiatives are aligned to address the multiple dimensions of customer values simultaneously. The results will apply in the modeling of the supply chain for convergence technologies and the development of business models for electronic bandwidth exchange. The prototype will be used to support teaching in several courses in Economics and Management.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122214</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">122214</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1634" target="n1636">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Agent Based Scalable Enterprise System for Enterprise Co-design</data>
      <data key="e_abstract">The objective of this Scalable Enterprise Systems award is the creation of an agent-based framework for the simulation of product design, marketing and supply-chain as collaborative processes. This project will develop the enabling theories and technologies needed to support its realization. The framework will use computational agents modeling the human behavior, artificial agents modeling synthetic economies, and constructive agents to address strategic, tactical and operational decision-making problems in simulating product life cycle. These agents use both analytic models and adaptive algorithms such as generic algorithms, fuzzy logic, and neural networks. The agents will compete to provide solutions in these simulations using innovative market metaphors such as auctions and recommendation systems. The research tasks include requirements analysis, preliminary functional analysis, methodology development, initial prototyping, testing, and specifications for the framework. &lt;br/&gt;&lt;br/&gt;If successful, this research will allow organizations to become aligned to and focused on to changing markets and changing customer values. The proposed collaborative design framework will test strategies necessary to ensure that the product delivery process, the product development process, and marketing initiatives are aligned to address the multiple dimensions of customer values simultaneously. The results will apply in the modeling of the supply chain for convergence technologies and the development of business models for electronic bandwidth exchange. The prototype will be used to support teaching in several courses in Economics and Management.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122214</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">122214</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1635" target="n1636">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Scalable Enterprise Systems Phase II: Agent Based Scalable Enterprise System for Enterprise Co-design</data>
      <data key="e_abstract">The objective of this Scalable Enterprise Systems award is the creation of an agent-based framework for the simulation of product design, marketing and supply-chain as collaborative processes. This project will develop the enabling theories and technologies needed to support its realization. The framework will use computational agents modeling the human behavior, artificial agents modeling synthetic economies, and constructive agents to address strategic, tactical and operational decision-making problems in simulating product life cycle. These agents use both analytic models and adaptive algorithms such as generic algorithms, fuzzy logic, and neural networks. The agents will compete to provide solutions in these simulations using innovative market metaphors such as auctions and recommendation systems. The research tasks include requirements analysis, preliminary functional analysis, methodology development, initial prototyping, testing, and specifications for the framework. &lt;br/&gt;&lt;br/&gt;If successful, this research will allow organizations to become aligned to and focused on to changing markets and changing customer values. The proposed collaborative design framework will test strategies necessary to ensure that the product delivery process, the product development process, and marketing initiatives are aligned to address the multiple dimensions of customer values simultaneously. The results will apply in the modeling of the supply chain for convergence technologies and the development of business models for electronic bandwidth exchange. The prototype will be used to support teaching in several courses in Economics and Management.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">122214</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">122214</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1637" target="n1638">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Information Technology Workforce - ITWF: Catching Up: A Longitudinal Study on Latino Participation in the Information Economy</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;Information Technology Workforce (ITWF)&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Proposal Id: EIA-0119858&lt;br/&gt;Investigator: Louis Tornatzky&lt;br/&gt;Institution: Tomas Rivera Policy Institute&lt;br/&gt;Title: Catching Up: A Longitudinal Study in Latino Participation in the Information Economy &lt;br/&gt;&lt;br/&gt;This award provides support for a 3-year study to better understand the factors relevant to Latino&apos;s involvement in Information Technology (IT) careers. The study will examine various factors influencing the attenuated representation of Latinos in IT jobs, career paths, and appropriate educational experiences. The research approach will be longitudinal using a cross-sectional approach to gather quantifiable as well as qualitative data that encompasses the many decisions and influences that occur between late-adolescence and early IT careers. The research approach will also be multi-level examining the decisions and behaviors of Latinos as affected by: (1) family dynamics including socioeconomic standing, role models, and immigration history; (2) school structure, policies, student population, and programs, at three levels within K-16; (3) community characteristics and dynamics; and (4) regional economies and markets. Data will be gathered from four metropolitan areas with high concentrations of Latino residents: Los Angeles, Houston, Chicago and New York.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">119858</data>
      <data key="e_expirationDate">2005-03-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">119858</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1641" target="n1642">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/AP (CISE) Collaborative Research: Best-First Search Algorithms for Sequence Alignment Problems in Computational Biology</data>
      <data key="e_abstract">Abstract&lt;br/&gt;Zhang 0113618&lt;br/&gt;Korf 0113313&lt;br/&gt;&lt;br/&gt;Best-First Search Algorithms for Sequence Alignment Problems in Computational&lt;br/&gt;Biology&lt;br/&gt;&lt;br/&gt;Molecular biologists are currently faced with very challenging computational problems. For example, a draft of the human genome has been completed, a sequence of about three billion base pairs. A draft of the mouse genome soon will be completed. We know that mice and men share over 90% of their genetic material. What we don&apos;t know is exactly which parts of the human and mouse genomes are common to both species. This information can be used to identify human genes, and to translate results from mouse studies to studies of human health and disease. The problem of identifying the common elements between these two DNA sequences is an example of sequence alignment, which is a computational problem. Other examples of sequence alignment problems include gene identification, and RNA and protean structure prediction. Current computer algorithms are either too slow, or require too much memory, to directly solve a problem as large as the human-mouse genomic sequence alignment. We propose to develop new algorithms for various sequence-alignment problems, based on heuristic search algorithms in artificial intelligence. Our goal is to provide much more efficient sequence-alignment algorithms for use by molecular biologists.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">113618</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">113618</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n284" target="n1644">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Secure Virtually Isolated Networks to Avoid and Tolerate Denial of Service</data>
      <data key="e_abstract">The widespread need and ability to connect machines across the Internet, in a world where intelligent objects rather than documents are exchanged, has caused the network to be more vulnerable to intrusions and has facilitated break-ins of a variety of types. Most of the methods currently available to deal with network&lt;br/&gt;vulnerability to abuse and attacks are either inadequate, inefficient oroverly restrictive. Compounding the&lt;br/&gt;problem is the need to maintain an acceptable level of quality of service (QoS).&lt;br/&gt; The proposed research project considers a heterogeneous network environment where servers, which&lt;br/&gt;provide different levels of QoS support to clients through a contract protocol, are prone to faults and denial&lt;br/&gt;of service attacks. The research project assumes the existence of intrusion detection mechanisms, and aims&lt;br/&gt;at investigating new and potentially revolutionary approaches for the development of scalable and efficient&lt;br/&gt;service deployment strategies and network resource management schemes to maintain acceptable levels of&lt;br/&gt;QoS and security, despite faults. Two types of faults, namely, benign malfunctions and malicious intrusions,&lt;br/&gt;will be considered. The former can be caused by a faulty, yet legitimate client that accidentally loses control&lt;br/&gt;over its behavior, while the latter occurs with the intent to cause damage, such asDenial of Service (DoS).&lt;br/&gt;Both types of faults can severely affect the performance of the network and compromise the integrity and&lt;br/&gt;security of its services.&lt;br/&gt; These faults can manifest themselves in the form of a protocol breach or a contract violation. The former&lt;br/&gt;is exemplified by an authorized clients (impersonation may take place) who attempt to deliberately breach&lt;br/&gt;the contract protocol and impact the behavior of the server to eventually cause its failure. Contract violation&lt;br/&gt;occurs when a client attempts to acquire a level of service beyond what has been agreed upon in the service&lt;br/&gt;contract. In order to protect the servers and the network, we propose two new techniques: fault avoidance,&lt;br/&gt;based on the concept of replicated elusive servers, and fault tolerance, based on resource management schemes through the creation of a Virtually Isolated Network (VIN).&lt;br/&gt; The concept of replicated elusive servers espouses ideas such as roaming addresses and frequent frequency&lt;br/&gt;changes in wireless networks. Replication is coordinated through group communication supported by an&lt;br/&gt;underlying multicast mechanism. VINs, on the other hand, provide the basis to achieve both physical and&lt;br/&gt;logical separation (in space and time) of the resources reserved for each service, client, or class of clients.&lt;br/&gt; Efficient management of network resources is achieved based on an integrative approach which considers&lt;br/&gt;network performance, fault tolerance and security asintegral components of a multi-dimensional QoS space.&lt;br/&gt;QoS support can then be perceived as a multi-layered optimization process which considers security, fault-&lt;br/&gt;tolerance, resource allocation, communication protocol optimization and user level application management&lt;br/&gt;as inhabitants of the same QoS spectrum and seeks to exploit tradeoffs in order to reach anoptimal operating&lt;br/&gt;point. The techniques developed will be designed to handle multiple coordinated intrusions, clustered in&lt;br/&gt;both space and time. A coordinated/clustered fault model will be developed and a study of its effect on the&lt;br/&gt;developed techniques and algorithms will be conducted.&lt;br/&gt; The proposed research will build on a foundation of prior work developed by the PIs which have a&lt;br/&gt;strong track record of success in a wide range of research topics related to fault-tolerance, operating system&lt;br/&gt;development and resource management for QoS support in wired and wireless networks. It is anticipated&lt;br/&gt;that through algorithms development and analysis, simulation and testbed implementations, the results of&lt;br/&gt;this project will lead to a better understanding of how to provide efficient support to QoS performance,&lt;br/&gt;fault-tolerance and securityin an integrated manner, both in wired and wireless environments. An equally&lt;br/&gt;important contribution of this project will be the training of high quality students in a field where expertise&lt;br/&gt;is scarce.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">87609</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">87609</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1647" target="n1648">
      <data key="e_effectiveDate">2001-10-15</data>
      <data key="e_title">First Virtual Conference on Genomics and Bioinformatics</data>
      <data key="e_abstract">North Dakota State University (NDSU) is organizing what it refers to as &quot;The First Virtual Conference on Genomics and Bioinformatics&quot; to be held on October 15-16, 2001. NDSU will be using technology jointly funded, originally, by ANIR and EPSCoR.&lt;br/&gt;&lt;br/&gt;The goals of the conference are to &quot;1) transcend geographical and economical factors limiting researchers around the world, 2) increase the exchange of ideas and establish new ways of collaborating with others and 3) establish new ways to excite the K-20 community about today&apos;s multidisciplinary science.&quot; &lt;br/&gt;&lt;br/&gt;Some speakers will participate from the NDSU campus while others will &quot;telecommute&quot; via the Access Grid. Individuals will be able to participate from Access Grid Node sites around the world and via real-time video-streaming technology from their computer desktops.</data>
      <data key="e_pgm">4090</data>
      <data key="e_label">139651</data>
      <data key="e_expirationDate">2004-03-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">139651</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1649" target="n1650">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">IGERT: A Unified Approach to Sequential Decision-Making in Cognitive Science</data>
      <data key="e_abstract">This IGERT project examines the problem of sequential decision-making as a unifying framework for the study of several central topics in cognitive science: selective attention, navigation, language processing, and the coordination of action in multiple-agent groups. The overarching question our students are trained to investigate is the following: how is it possible for an agent to decide what actions to take to achieve long-term goals? We recognize that decision-making in complex environments is a sequential process, involving a series of episodes in which an agent, based on information available through its senses and stored in memory, selects the action appropriate for its goals. The problem is made difficult by perceptual uncertainty arising from sensory limitations and environmental complexity, by the challenge of sorting through the large space of actions available, and by inherent delays in feedback about the long-term consequences of actions. A wide variety of fundamental cognitive tasks can be cast as sequential decision-making problems. Understanding how such problems may be solved will be a critical component of a general theory of intelligent behavior in organisms, and will be essential for the design of truly intelligent machines. To study these problems, we adopt a comparative approach, combining insights from a range of model systems, including humans, non-human animals, robots, and intelligent software agents. This multidisciplinary framework will enable students to integrate ideas and methods from different fields that have been concerned with the study of sequential decision-making (psychology, behavioral biology, linguistics, and computer science), but that have so far remained largely separate. The training program is designed to create a new generation of scientists trained in this innovative, multidisciplinary approach. Graduate training will be focused on fundamental disciplinary education, a common set of courses focused on the sequential decision-making framework, and a strong emphasis on mentored, interdisciplinary research activities that span each student&apos;s entire graduate program.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; Engineering; Biological Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">6856</data>
      <data key="e_label">114378</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">114378</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1649" target="n1651">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">IGERT: A Unified Approach to Sequential Decision-Making in Cognitive Science</data>
      <data key="e_abstract">This IGERT project examines the problem of sequential decision-making as a unifying framework for the study of several central topics in cognitive science: selective attention, navigation, language processing, and the coordination of action in multiple-agent groups. The overarching question our students are trained to investigate is the following: how is it possible for an agent to decide what actions to take to achieve long-term goals? We recognize that decision-making in complex environments is a sequential process, involving a series of episodes in which an agent, based on information available through its senses and stored in memory, selects the action appropriate for its goals. The problem is made difficult by perceptual uncertainty arising from sensory limitations and environmental complexity, by the challenge of sorting through the large space of actions available, and by inherent delays in feedback about the long-term consequences of actions. A wide variety of fundamental cognitive tasks can be cast as sequential decision-making problems. Understanding how such problems may be solved will be a critical component of a general theory of intelligent behavior in organisms, and will be essential for the design of truly intelligent machines. To study these problems, we adopt a comparative approach, combining insights from a range of model systems, including humans, non-human animals, robots, and intelligent software agents. This multidisciplinary framework will enable students to integrate ideas and methods from different fields that have been concerned with the study of sequential decision-making (psychology, behavioral biology, linguistics, and computer science), but that have so far remained largely separate. The training program is designed to create a new generation of scientists trained in this innovative, multidisciplinary approach. Graduate training will be focused on fundamental disciplinary education, a common set of courses focused on the sequential decision-making framework, and a strong emphasis on mentored, interdisciplinary research activities that span each student&apos;s entire graduate program.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; Engineering; Biological Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">6856</data>
      <data key="e_label">114378</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">114378</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1649" target="n1652">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">IGERT: A Unified Approach to Sequential Decision-Making in Cognitive Science</data>
      <data key="e_abstract">This IGERT project examines the problem of sequential decision-making as a unifying framework for the study of several central topics in cognitive science: selective attention, navigation, language processing, and the coordination of action in multiple-agent groups. The overarching question our students are trained to investigate is the following: how is it possible for an agent to decide what actions to take to achieve long-term goals? We recognize that decision-making in complex environments is a sequential process, involving a series of episodes in which an agent, based on information available through its senses and stored in memory, selects the action appropriate for its goals. The problem is made difficult by perceptual uncertainty arising from sensory limitations and environmental complexity, by the challenge of sorting through the large space of actions available, and by inherent delays in feedback about the long-term consequences of actions. A wide variety of fundamental cognitive tasks can be cast as sequential decision-making problems. Understanding how such problems may be solved will be a critical component of a general theory of intelligent behavior in organisms, and will be essential for the design of truly intelligent machines. To study these problems, we adopt a comparative approach, combining insights from a range of model systems, including humans, non-human animals, robots, and intelligent software agents. This multidisciplinary framework will enable students to integrate ideas and methods from different fields that have been concerned with the study of sequential decision-making (psychology, behavioral biology, linguistics, and computer science), but that have so far remained largely separate. The training program is designed to create a new generation of scientists trained in this innovative, multidisciplinary approach. Graduate training will be focused on fundamental disciplinary education, a common set of courses focused on the sequential decision-making framework, and a strong emphasis on mentored, interdisciplinary research activities that span each student&apos;s entire graduate program.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; Engineering; Biological Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">6856</data>
      <data key="e_label">114378</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">114378</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1649" target="n1653">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">IGERT: A Unified Approach to Sequential Decision-Making in Cognitive Science</data>
      <data key="e_abstract">This IGERT project examines the problem of sequential decision-making as a unifying framework for the study of several central topics in cognitive science: selective attention, navigation, language processing, and the coordination of action in multiple-agent groups. The overarching question our students are trained to investigate is the following: how is it possible for an agent to decide what actions to take to achieve long-term goals? We recognize that decision-making in complex environments is a sequential process, involving a series of episodes in which an agent, based on information available through its senses and stored in memory, selects the action appropriate for its goals. The problem is made difficult by perceptual uncertainty arising from sensory limitations and environmental complexity, by the challenge of sorting through the large space of actions available, and by inherent delays in feedback about the long-term consequences of actions. A wide variety of fundamental cognitive tasks can be cast as sequential decision-making problems. Understanding how such problems may be solved will be a critical component of a general theory of intelligent behavior in organisms, and will be essential for the design of truly intelligent machines. To study these problems, we adopt a comparative approach, combining insights from a range of model systems, including humans, non-human animals, robots, and intelligent software agents. This multidisciplinary framework will enable students to integrate ideas and methods from different fields that have been concerned with the study of sequential decision-making (psychology, behavioral biology, linguistics, and computer science), but that have so far remained largely separate. The training program is designed to create a new generation of scientists trained in this innovative, multidisciplinary approach. Graduate training will be focused on fundamental disciplinary education, a common set of courses focused on the sequential decision-making framework, and a strong emphasis on mentored, interdisciplinary research activities that span each student&apos;s entire graduate program.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; Engineering; Biological Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">6856</data>
      <data key="e_label">114378</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">114378</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1650" target="n1651">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">IGERT: A Unified Approach to Sequential Decision-Making in Cognitive Science</data>
      <data key="e_abstract">This IGERT project examines the problem of sequential decision-making as a unifying framework for the study of several central topics in cognitive science: selective attention, navigation, language processing, and the coordination of action in multiple-agent groups. The overarching question our students are trained to investigate is the following: how is it possible for an agent to decide what actions to take to achieve long-term goals? We recognize that decision-making in complex environments is a sequential process, involving a series of episodes in which an agent, based on information available through its senses and stored in memory, selects the action appropriate for its goals. The problem is made difficult by perceptual uncertainty arising from sensory limitations and environmental complexity, by the challenge of sorting through the large space of actions available, and by inherent delays in feedback about the long-term consequences of actions. A wide variety of fundamental cognitive tasks can be cast as sequential decision-making problems. Understanding how such problems may be solved will be a critical component of a general theory of intelligent behavior in organisms, and will be essential for the design of truly intelligent machines. To study these problems, we adopt a comparative approach, combining insights from a range of model systems, including humans, non-human animals, robots, and intelligent software agents. This multidisciplinary framework will enable students to integrate ideas and methods from different fields that have been concerned with the study of sequential decision-making (psychology, behavioral biology, linguistics, and computer science), but that have so far remained largely separate. The training program is designed to create a new generation of scientists trained in this innovative, multidisciplinary approach. Graduate training will be focused on fundamental disciplinary education, a common set of courses focused on the sequential decision-making framework, and a strong emphasis on mentored, interdisciplinary research activities that span each student&apos;s entire graduate program.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; Engineering; Biological Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">6856</data>
      <data key="e_label">114378</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">114378</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1650" target="n1652">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">IGERT: A Unified Approach to Sequential Decision-Making in Cognitive Science</data>
      <data key="e_abstract">This IGERT project examines the problem of sequential decision-making as a unifying framework for the study of several central topics in cognitive science: selective attention, navigation, language processing, and the coordination of action in multiple-agent groups. The overarching question our students are trained to investigate is the following: how is it possible for an agent to decide what actions to take to achieve long-term goals? We recognize that decision-making in complex environments is a sequential process, involving a series of episodes in which an agent, based on information available through its senses and stored in memory, selects the action appropriate for its goals. The problem is made difficult by perceptual uncertainty arising from sensory limitations and environmental complexity, by the challenge of sorting through the large space of actions available, and by inherent delays in feedback about the long-term consequences of actions. A wide variety of fundamental cognitive tasks can be cast as sequential decision-making problems. Understanding how such problems may be solved will be a critical component of a general theory of intelligent behavior in organisms, and will be essential for the design of truly intelligent machines. To study these problems, we adopt a comparative approach, combining insights from a range of model systems, including humans, non-human animals, robots, and intelligent software agents. This multidisciplinary framework will enable students to integrate ideas and methods from different fields that have been concerned with the study of sequential decision-making (psychology, behavioral biology, linguistics, and computer science), but that have so far remained largely separate. The training program is designed to create a new generation of scientists trained in this innovative, multidisciplinary approach. Graduate training will be focused on fundamental disciplinary education, a common set of courses focused on the sequential decision-making framework, and a strong emphasis on mentored, interdisciplinary research activities that span each student&apos;s entire graduate program.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; Engineering; Biological Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">6856</data>
      <data key="e_label">114378</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">114378</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1650" target="n1653">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">IGERT: A Unified Approach to Sequential Decision-Making in Cognitive Science</data>
      <data key="e_abstract">This IGERT project examines the problem of sequential decision-making as a unifying framework for the study of several central topics in cognitive science: selective attention, navigation, language processing, and the coordination of action in multiple-agent groups. The overarching question our students are trained to investigate is the following: how is it possible for an agent to decide what actions to take to achieve long-term goals? We recognize that decision-making in complex environments is a sequential process, involving a series of episodes in which an agent, based on information available through its senses and stored in memory, selects the action appropriate for its goals. The problem is made difficult by perceptual uncertainty arising from sensory limitations and environmental complexity, by the challenge of sorting through the large space of actions available, and by inherent delays in feedback about the long-term consequences of actions. A wide variety of fundamental cognitive tasks can be cast as sequential decision-making problems. Understanding how such problems may be solved will be a critical component of a general theory of intelligent behavior in organisms, and will be essential for the design of truly intelligent machines. To study these problems, we adopt a comparative approach, combining insights from a range of model systems, including humans, non-human animals, robots, and intelligent software agents. This multidisciplinary framework will enable students to integrate ideas and methods from different fields that have been concerned with the study of sequential decision-making (psychology, behavioral biology, linguistics, and computer science), but that have so far remained largely separate. The training program is designed to create a new generation of scientists trained in this innovative, multidisciplinary approach. Graduate training will be focused on fundamental disciplinary education, a common set of courses focused on the sequential decision-making framework, and a strong emphasis on mentored, interdisciplinary research activities that span each student&apos;s entire graduate program.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; Engineering; Biological Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">6856</data>
      <data key="e_label">114378</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">114378</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1651" target="n1652">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">IGERT: A Unified Approach to Sequential Decision-Making in Cognitive Science</data>
      <data key="e_abstract">This IGERT project examines the problem of sequential decision-making as a unifying framework for the study of several central topics in cognitive science: selective attention, navigation, language processing, and the coordination of action in multiple-agent groups. The overarching question our students are trained to investigate is the following: how is it possible for an agent to decide what actions to take to achieve long-term goals? We recognize that decision-making in complex environments is a sequential process, involving a series of episodes in which an agent, based on information available through its senses and stored in memory, selects the action appropriate for its goals. The problem is made difficult by perceptual uncertainty arising from sensory limitations and environmental complexity, by the challenge of sorting through the large space of actions available, and by inherent delays in feedback about the long-term consequences of actions. A wide variety of fundamental cognitive tasks can be cast as sequential decision-making problems. Understanding how such problems may be solved will be a critical component of a general theory of intelligent behavior in organisms, and will be essential for the design of truly intelligent machines. To study these problems, we adopt a comparative approach, combining insights from a range of model systems, including humans, non-human animals, robots, and intelligent software agents. This multidisciplinary framework will enable students to integrate ideas and methods from different fields that have been concerned with the study of sequential decision-making (psychology, behavioral biology, linguistics, and computer science), but that have so far remained largely separate. The training program is designed to create a new generation of scientists trained in this innovative, multidisciplinary approach. Graduate training will be focused on fundamental disciplinary education, a common set of courses focused on the sequential decision-making framework, and a strong emphasis on mentored, interdisciplinary research activities that span each student&apos;s entire graduate program.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; Engineering; Biological Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">6856</data>
      <data key="e_label">114378</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">114378</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1651" target="n1653">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">IGERT: A Unified Approach to Sequential Decision-Making in Cognitive Science</data>
      <data key="e_abstract">This IGERT project examines the problem of sequential decision-making as a unifying framework for the study of several central topics in cognitive science: selective attention, navigation, language processing, and the coordination of action in multiple-agent groups. The overarching question our students are trained to investigate is the following: how is it possible for an agent to decide what actions to take to achieve long-term goals? We recognize that decision-making in complex environments is a sequential process, involving a series of episodes in which an agent, based on information available through its senses and stored in memory, selects the action appropriate for its goals. The problem is made difficult by perceptual uncertainty arising from sensory limitations and environmental complexity, by the challenge of sorting through the large space of actions available, and by inherent delays in feedback about the long-term consequences of actions. A wide variety of fundamental cognitive tasks can be cast as sequential decision-making problems. Understanding how such problems may be solved will be a critical component of a general theory of intelligent behavior in organisms, and will be essential for the design of truly intelligent machines. To study these problems, we adopt a comparative approach, combining insights from a range of model systems, including humans, non-human animals, robots, and intelligent software agents. This multidisciplinary framework will enable students to integrate ideas and methods from different fields that have been concerned with the study of sequential decision-making (psychology, behavioral biology, linguistics, and computer science), but that have so far remained largely separate. The training program is designed to create a new generation of scientists trained in this innovative, multidisciplinary approach. Graduate training will be focused on fundamental disciplinary education, a common set of courses focused on the sequential decision-making framework, and a strong emphasis on mentored, interdisciplinary research activities that span each student&apos;s entire graduate program.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; Engineering; Biological Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">6856</data>
      <data key="e_label">114378</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">114378</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1652" target="n1653">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">IGERT: A Unified Approach to Sequential Decision-Making in Cognitive Science</data>
      <data key="e_abstract">This IGERT project examines the problem of sequential decision-making as a unifying framework for the study of several central topics in cognitive science: selective attention, navigation, language processing, and the coordination of action in multiple-agent groups. The overarching question our students are trained to investigate is the following: how is it possible for an agent to decide what actions to take to achieve long-term goals? We recognize that decision-making in complex environments is a sequential process, involving a series of episodes in which an agent, based on information available through its senses and stored in memory, selects the action appropriate for its goals. The problem is made difficult by perceptual uncertainty arising from sensory limitations and environmental complexity, by the challenge of sorting through the large space of actions available, and by inherent delays in feedback about the long-term consequences of actions. A wide variety of fundamental cognitive tasks can be cast as sequential decision-making problems. Understanding how such problems may be solved will be a critical component of a general theory of intelligent behavior in organisms, and will be essential for the design of truly intelligent machines. To study these problems, we adopt a comparative approach, combining insights from a range of model systems, including humans, non-human animals, robots, and intelligent software agents. This multidisciplinary framework will enable students to integrate ideas and methods from different fields that have been concerned with the study of sequential decision-making (psychology, behavioral biology, linguistics, and computer science), but that have so far remained largely separate. The training program is designed to create a new generation of scientists trained in this innovative, multidisciplinary approach. Graduate training will be focused on fundamental disciplinary education, a common set of courses focused on the sequential decision-making framework, and a strong emphasis on mentored, interdisciplinary research activities that span each student&apos;s entire graduate program.&lt;br/&gt;&lt;br/&gt;IGERT is an NSF-wide program intended to meet the challenges of educating Ph.D. scientists and engineers with the multidisciplinary backgrounds and the technical, professional, and personal skills needed for the career demands of the future. The program is intended to catalyze a cultural change in graduate education by establishing new, innovative models for graduate education and training in a fertile environment for collaborative research that transcends traditional disciplinary boundaries. In the fourth year of the program, awards are being made to twenty-two institutions for programs that collectively span all areas of science and engineering supported by NSF. The intellectual foci of this specific award reside in the Directorates for Social, Behavioral, and Economic Sciences; Computer and Information Science and Engineering; Engineering; Biological Sciences; and Education and Human Resources.</data>
      <data key="e_pgm">6856</data>
      <data key="e_label">114378</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">114378</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1654" target="n1655">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Discrete Models &amp; Algorithms in the Sciences</data>
      <data key="e_abstract">The research is centered on four major computational themes which &lt;br/&gt;are linked in various ways:&lt;br/&gt;&lt;br/&gt;1. Quantum Computation: &lt;br/&gt;A study of novel quantum algorithms, of entanglement as a computational &lt;br/&gt;resource, and of connections to fundamental issues in quantum physics, &lt;br/&gt;such as the transition from classical to quantum. &lt;br/&gt;&lt;br/&gt;2. Modeling the Regulatory Processes of the Cell:&lt;br/&gt;In the post-genomic era, the computational modeling of the operation &lt;br/&gt;of an entire cell at the level of interactions among genes, proteins &lt;br/&gt;and environmental conditions.&lt;br/&gt;&lt;br/&gt;3. Statistical Physics and Computational Complexity: &lt;br/&gt;A study of central concepts of statistical physics, such as phase &lt;br/&gt;transitions and critical exponents, with emphasis on their computational &lt;br/&gt;manifestations and their relevance to the analysis of large systems &lt;br/&gt;with local interactions.&lt;br/&gt;&lt;br/&gt;4. Mathematical Economics and the Internet: &lt;br/&gt;A study of the Internet as a novel computational artifact and a complex &lt;br/&gt;economic arena, as well as of the algorithmic adaptations of Game Theory &lt;br/&gt;and Mechanism Design necessary for such a study.&lt;br/&gt;&lt;br/&gt;The four PIs --- Richard Karp, Christos Papadimitriou, Alistair Sinclair &lt;br/&gt;and Umesh Vazirani --- are all based in the Computer Science Division at &lt;br/&gt;UC Berkeley. Each of them has a track record of research in at least one &lt;br/&gt;of the above areas, and a substantial interest in at least one other. &lt;br/&gt;The project also includes one senior scientist from each of the four areas: &lt;br/&gt;Birgitta Whaley (Quantum Physics), Adam Arkin (Quantitative Biology), &lt;br/&gt;Yuval Peres (Probability and Statistical Physics), and Scott Shenker &lt;br/&gt;(Economics and the Internet). &lt;br/&gt;&lt;br/&gt;The research is rooted in a realization that a computational perspective &lt;br/&gt;is becoming increasingly important in the Natural and Mathematical Sciences,&lt;br/&gt;and conversely that the Sciences are posing new challenges for the theory &lt;br/&gt;of computation, many of which are related. It aims to foster this connection &lt;br/&gt;within a dedicated program of research and graduate education.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121555</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121555</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1654" target="n1656">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Discrete Models &amp; Algorithms in the Sciences</data>
      <data key="e_abstract">The research is centered on four major computational themes which &lt;br/&gt;are linked in various ways:&lt;br/&gt;&lt;br/&gt;1. Quantum Computation: &lt;br/&gt;A study of novel quantum algorithms, of entanglement as a computational &lt;br/&gt;resource, and of connections to fundamental issues in quantum physics, &lt;br/&gt;such as the transition from classical to quantum. &lt;br/&gt;&lt;br/&gt;2. Modeling the Regulatory Processes of the Cell:&lt;br/&gt;In the post-genomic era, the computational modeling of the operation &lt;br/&gt;of an entire cell at the level of interactions among genes, proteins &lt;br/&gt;and environmental conditions.&lt;br/&gt;&lt;br/&gt;3. Statistical Physics and Computational Complexity: &lt;br/&gt;A study of central concepts of statistical physics, such as phase &lt;br/&gt;transitions and critical exponents, with emphasis on their computational &lt;br/&gt;manifestations and their relevance to the analysis of large systems &lt;br/&gt;with local interactions.&lt;br/&gt;&lt;br/&gt;4. Mathematical Economics and the Internet: &lt;br/&gt;A study of the Internet as a novel computational artifact and a complex &lt;br/&gt;economic arena, as well as of the algorithmic adaptations of Game Theory &lt;br/&gt;and Mechanism Design necessary for such a study.&lt;br/&gt;&lt;br/&gt;The four PIs --- Richard Karp, Christos Papadimitriou, Alistair Sinclair &lt;br/&gt;and Umesh Vazirani --- are all based in the Computer Science Division at &lt;br/&gt;UC Berkeley. Each of them has a track record of research in at least one &lt;br/&gt;of the above areas, and a substantial interest in at least one other. &lt;br/&gt;The project also includes one senior scientist from each of the four areas: &lt;br/&gt;Birgitta Whaley (Quantum Physics), Adam Arkin (Quantitative Biology), &lt;br/&gt;Yuval Peres (Probability and Statistical Physics), and Scott Shenker &lt;br/&gt;(Economics and the Internet). &lt;br/&gt;&lt;br/&gt;The research is rooted in a realization that a computational perspective &lt;br/&gt;is becoming increasingly important in the Natural and Mathematical Sciences,&lt;br/&gt;and conversely that the Sciences are posing new challenges for the theory &lt;br/&gt;of computation, many of which are related. It aims to foster this connection &lt;br/&gt;within a dedicated program of research and graduate education.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121555</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121555</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1654" target="n1657">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Discrete Models &amp; Algorithms in the Sciences</data>
      <data key="e_abstract">The research is centered on four major computational themes which &lt;br/&gt;are linked in various ways:&lt;br/&gt;&lt;br/&gt;1. Quantum Computation: &lt;br/&gt;A study of novel quantum algorithms, of entanglement as a computational &lt;br/&gt;resource, and of connections to fundamental issues in quantum physics, &lt;br/&gt;such as the transition from classical to quantum. &lt;br/&gt;&lt;br/&gt;2. Modeling the Regulatory Processes of the Cell:&lt;br/&gt;In the post-genomic era, the computational modeling of the operation &lt;br/&gt;of an entire cell at the level of interactions among genes, proteins &lt;br/&gt;and environmental conditions.&lt;br/&gt;&lt;br/&gt;3. Statistical Physics and Computational Complexity: &lt;br/&gt;A study of central concepts of statistical physics, such as phase &lt;br/&gt;transitions and critical exponents, with emphasis on their computational &lt;br/&gt;manifestations and their relevance to the analysis of large systems &lt;br/&gt;with local interactions.&lt;br/&gt;&lt;br/&gt;4. Mathematical Economics and the Internet: &lt;br/&gt;A study of the Internet as a novel computational artifact and a complex &lt;br/&gt;economic arena, as well as of the algorithmic adaptations of Game Theory &lt;br/&gt;and Mechanism Design necessary for such a study.&lt;br/&gt;&lt;br/&gt;The four PIs --- Richard Karp, Christos Papadimitriou, Alistair Sinclair &lt;br/&gt;and Umesh Vazirani --- are all based in the Computer Science Division at &lt;br/&gt;UC Berkeley. Each of them has a track record of research in at least one &lt;br/&gt;of the above areas, and a substantial interest in at least one other. &lt;br/&gt;The project also includes one senior scientist from each of the four areas: &lt;br/&gt;Birgitta Whaley (Quantum Physics), Adam Arkin (Quantitative Biology), &lt;br/&gt;Yuval Peres (Probability and Statistical Physics), and Scott Shenker &lt;br/&gt;(Economics and the Internet). &lt;br/&gt;&lt;br/&gt;The research is rooted in a realization that a computational perspective &lt;br/&gt;is becoming increasingly important in the Natural and Mathematical Sciences,&lt;br/&gt;and conversely that the Sciences are posing new challenges for the theory &lt;br/&gt;of computation, many of which are related. It aims to foster this connection &lt;br/&gt;within a dedicated program of research and graduate education.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121555</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121555</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1655" target="n1656">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Discrete Models &amp; Algorithms in the Sciences</data>
      <data key="e_abstract">The research is centered on four major computational themes which &lt;br/&gt;are linked in various ways:&lt;br/&gt;&lt;br/&gt;1. Quantum Computation: &lt;br/&gt;A study of novel quantum algorithms, of entanglement as a computational &lt;br/&gt;resource, and of connections to fundamental issues in quantum physics, &lt;br/&gt;such as the transition from classical to quantum. &lt;br/&gt;&lt;br/&gt;2. Modeling the Regulatory Processes of the Cell:&lt;br/&gt;In the post-genomic era, the computational modeling of the operation &lt;br/&gt;of an entire cell at the level of interactions among genes, proteins &lt;br/&gt;and environmental conditions.&lt;br/&gt;&lt;br/&gt;3. Statistical Physics and Computational Complexity: &lt;br/&gt;A study of central concepts of statistical physics, such as phase &lt;br/&gt;transitions and critical exponents, with emphasis on their computational &lt;br/&gt;manifestations and their relevance to the analysis of large systems &lt;br/&gt;with local interactions.&lt;br/&gt;&lt;br/&gt;4. Mathematical Economics and the Internet: &lt;br/&gt;A study of the Internet as a novel computational artifact and a complex &lt;br/&gt;economic arena, as well as of the algorithmic adaptations of Game Theory &lt;br/&gt;and Mechanism Design necessary for such a study.&lt;br/&gt;&lt;br/&gt;The four PIs --- Richard Karp, Christos Papadimitriou, Alistair Sinclair &lt;br/&gt;and Umesh Vazirani --- are all based in the Computer Science Division at &lt;br/&gt;UC Berkeley. Each of them has a track record of research in at least one &lt;br/&gt;of the above areas, and a substantial interest in at least one other. &lt;br/&gt;The project also includes one senior scientist from each of the four areas: &lt;br/&gt;Birgitta Whaley (Quantum Physics), Adam Arkin (Quantitative Biology), &lt;br/&gt;Yuval Peres (Probability and Statistical Physics), and Scott Shenker &lt;br/&gt;(Economics and the Internet). &lt;br/&gt;&lt;br/&gt;The research is rooted in a realization that a computational perspective &lt;br/&gt;is becoming increasingly important in the Natural and Mathematical Sciences,&lt;br/&gt;and conversely that the Sciences are posing new challenges for the theory &lt;br/&gt;of computation, many of which are related. It aims to foster this connection &lt;br/&gt;within a dedicated program of research and graduate education.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121555</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121555</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1655" target="n1657">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Discrete Models &amp; Algorithms in the Sciences</data>
      <data key="e_abstract">The research is centered on four major computational themes which &lt;br/&gt;are linked in various ways:&lt;br/&gt;&lt;br/&gt;1. Quantum Computation: &lt;br/&gt;A study of novel quantum algorithms, of entanglement as a computational &lt;br/&gt;resource, and of connections to fundamental issues in quantum physics, &lt;br/&gt;such as the transition from classical to quantum. &lt;br/&gt;&lt;br/&gt;2. Modeling the Regulatory Processes of the Cell:&lt;br/&gt;In the post-genomic era, the computational modeling of the operation &lt;br/&gt;of an entire cell at the level of interactions among genes, proteins &lt;br/&gt;and environmental conditions.&lt;br/&gt;&lt;br/&gt;3. Statistical Physics and Computational Complexity: &lt;br/&gt;A study of central concepts of statistical physics, such as phase &lt;br/&gt;transitions and critical exponents, with emphasis on their computational &lt;br/&gt;manifestations and their relevance to the analysis of large systems &lt;br/&gt;with local interactions.&lt;br/&gt;&lt;br/&gt;4. Mathematical Economics and the Internet: &lt;br/&gt;A study of the Internet as a novel computational artifact and a complex &lt;br/&gt;economic arena, as well as of the algorithmic adaptations of Game Theory &lt;br/&gt;and Mechanism Design necessary for such a study.&lt;br/&gt;&lt;br/&gt;The four PIs --- Richard Karp, Christos Papadimitriou, Alistair Sinclair &lt;br/&gt;and Umesh Vazirani --- are all based in the Computer Science Division at &lt;br/&gt;UC Berkeley. Each of them has a track record of research in at least one &lt;br/&gt;of the above areas, and a substantial interest in at least one other. &lt;br/&gt;The project also includes one senior scientist from each of the four areas: &lt;br/&gt;Birgitta Whaley (Quantum Physics), Adam Arkin (Quantitative Biology), &lt;br/&gt;Yuval Peres (Probability and Statistical Physics), and Scott Shenker &lt;br/&gt;(Economics and the Internet). &lt;br/&gt;&lt;br/&gt;The research is rooted in a realization that a computational perspective &lt;br/&gt;is becoming increasingly important in the Natural and Mathematical Sciences,&lt;br/&gt;and conversely that the Sciences are posing new challenges for the theory &lt;br/&gt;of computation, many of which are related. It aims to foster this connection &lt;br/&gt;within a dedicated program of research and graduate education.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121555</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121555</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1656" target="n1657">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY: Discrete Models &amp; Algorithms in the Sciences</data>
      <data key="e_abstract">The research is centered on four major computational themes which &lt;br/&gt;are linked in various ways:&lt;br/&gt;&lt;br/&gt;1. Quantum Computation: &lt;br/&gt;A study of novel quantum algorithms, of entanglement as a computational &lt;br/&gt;resource, and of connections to fundamental issues in quantum physics, &lt;br/&gt;such as the transition from classical to quantum. &lt;br/&gt;&lt;br/&gt;2. Modeling the Regulatory Processes of the Cell:&lt;br/&gt;In the post-genomic era, the computational modeling of the operation &lt;br/&gt;of an entire cell at the level of interactions among genes, proteins &lt;br/&gt;and environmental conditions.&lt;br/&gt;&lt;br/&gt;3. Statistical Physics and Computational Complexity: &lt;br/&gt;A study of central concepts of statistical physics, such as phase &lt;br/&gt;transitions and critical exponents, with emphasis on their computational &lt;br/&gt;manifestations and their relevance to the analysis of large systems &lt;br/&gt;with local interactions.&lt;br/&gt;&lt;br/&gt;4. Mathematical Economics and the Internet: &lt;br/&gt;A study of the Internet as a novel computational artifact and a complex &lt;br/&gt;economic arena, as well as of the algorithmic adaptations of Game Theory &lt;br/&gt;and Mechanism Design necessary for such a study.&lt;br/&gt;&lt;br/&gt;The four PIs --- Richard Karp, Christos Papadimitriou, Alistair Sinclair &lt;br/&gt;and Umesh Vazirani --- are all based in the Computer Science Division at &lt;br/&gt;UC Berkeley. Each of them has a track record of research in at least one &lt;br/&gt;of the above areas, and a substantial interest in at least one other. &lt;br/&gt;The project also includes one senior scientist from each of the four areas: &lt;br/&gt;Birgitta Whaley (Quantum Physics), Adam Arkin (Quantitative Biology), &lt;br/&gt;Yuval Peres (Probability and Statistical Physics), and Scott Shenker &lt;br/&gt;(Economics and the Internet). &lt;br/&gt;&lt;br/&gt;The research is rooted in a realization that a computational perspective &lt;br/&gt;is becoming increasingly important in the Natural and Mathematical Sciences,&lt;br/&gt;and conversely that the Sciences are posing new challenges for the theory &lt;br/&gt;of computation, many of which are related. It aims to foster this connection &lt;br/&gt;within a dedicated program of research and graduate education.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121555</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121555</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n307" target="n1049">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">CISE Research Resources: A Flexible Video Networking Infrastructure to Support Massively Scalable Information Flows</data>
      <data key="e_abstract">EIA-0130344&lt;br/&gt;Wu-chi Feng&lt;br/&gt;Oregon Graduate Institute&lt;br/&gt;&lt;br/&gt;CISE Research Resources: A Flexible Video Networking Infrastructure to Support Massively Scalable Information Flows&lt;br/&gt;&lt;br/&gt;This research project proposes the establishment of a flexible video networking testbed in the Department of Computer Science and Engineering at the Oregon Graduate Institute. The main purpose of this architecture is to facilitate experimental research in systems that support massively scalable information flows. Focus will be on four main tasks: (1) Scalable Video Distillation Architectures - providing mechanisms to support the aggregation of large numbers of video streams in a video sensor network; (2) Scalable Video Multicast Distribution Protocols - providing scalable mechanisms that support quality and rate adaptive video for multicast over overlay networks; (3) An Integrated Approach to Resource Management for Internet Applications - combining application-level knowledge with low-level networking to provide scalable Internet application architectures, and (4) A Video Archive - creating an extensive testset of video data for use within our experiments as well as for distribution of statistical and frame size information to the rest of the research community.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130344</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130344</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1049" target="n1662">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">CISE Research Resources: A Flexible Video Networking Infrastructure to Support Massively Scalable Information Flows</data>
      <data key="e_abstract">EIA-0130344&lt;br/&gt;Wu-chi Feng&lt;br/&gt;Oregon Graduate Institute&lt;br/&gt;&lt;br/&gt;CISE Research Resources: A Flexible Video Networking Infrastructure to Support Massively Scalable Information Flows&lt;br/&gt;&lt;br/&gt;This research project proposes the establishment of a flexible video networking testbed in the Department of Computer Science and Engineering at the Oregon Graduate Institute. The main purpose of this architecture is to facilitate experimental research in systems that support massively scalable information flows. Focus will be on four main tasks: (1) Scalable Video Distillation Architectures - providing mechanisms to support the aggregation of large numbers of video streams in a video sensor network; (2) Scalable Video Multicast Distribution Protocols - providing scalable mechanisms that support quality and rate adaptive video for multicast over overlay networks; (3) An Integrated Approach to Resource Management for Internet Applications - combining application-level knowledge with low-level networking to provide scalable Internet application architectures, and (4) A Video Archive - creating an extensive testset of video data for use within our experiments as well as for distribution of statistical and frame size information to the rest of the research community.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130344</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130344</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n307" target="n1662">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">CISE Research Resources: A Flexible Video Networking Infrastructure to Support Massively Scalable Information Flows</data>
      <data key="e_abstract">EIA-0130344&lt;br/&gt;Wu-chi Feng&lt;br/&gt;Oregon Graduate Institute&lt;br/&gt;&lt;br/&gt;CISE Research Resources: A Flexible Video Networking Infrastructure to Support Massively Scalable Information Flows&lt;br/&gt;&lt;br/&gt;This research project proposes the establishment of a flexible video networking testbed in the Department of Computer Science and Engineering at the Oregon Graduate Institute. The main purpose of this architecture is to facilitate experimental research in systems that support massively scalable information flows. Focus will be on four main tasks: (1) Scalable Video Distillation Architectures - providing mechanisms to support the aggregation of large numbers of video streams in a video sensor network; (2) Scalable Video Multicast Distribution Protocols - providing scalable mechanisms that support quality and rate adaptive video for multicast over overlay networks; (3) An Integrated Approach to Resource Management for Internet Applications - combining application-level knowledge with low-level networking to provide scalable Internet application architectures, and (4) A Video Archive - creating an extensive testset of video data for use within our experiments as well as for distribution of statistical and frame size information to the rest of the research community.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">130344</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">130344</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1664" target="n1665">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Broadband MIMO OFDM Wireless Access</data>
      <data key="e_abstract">The convenience of wireless communications and the ever-increasing demand for higher data rates has motivated the proposed study of more efficient methods of transmission. One highly spectrally efficient link architecture is the multiple-input multiple-output (MIMO) architecture, which uses multiple transmit antennas and multiple receive antennas. Under ideal conditions, the theoretical capacity of a MIMO channel increases linearly in the number of transmit/receive antennas. In other words, the theory promises that more bits can be conveyed through the same bandwidth by simply adding more antennas at each end of the link. Orthogonal frequency division multiplexing (OFDM) is a high-data rate modulation technique that is already part of some single-transmitter communications standards and is known for its scalability and its convenient and cost-effective implementation using standard digital signal processing architectures. This study seeks to determine how to combine OFDM with MIMO architectures over real channels. A challenging goal of this research is to arrive at a flexible, scaleable, wireless modem with bandwidth efficiency on the order of 4 to 10 bits per second per Hertz.&lt;br/&gt;&lt;br/&gt;The study will include analysis of the major functions of the physical (PHY) and medium access control (MAC) layers. Hardware experimentation will include MIMO channel measurements, including characterization of MIMO interference, and verification of the MIMO OFDM link in real-time on a software radio test-bed. Methods for performing MIMO OFDM channel estimation and synchronization jointly over the spatial channels are being investigated, taking into account noise and channel estimation and synchronization errors. An adaptive transmitter is also being studied that combines space-time processing based on singular-value decomposition (SVD) with adaptive modulation. Solutions to the major transmission impairments in OFDM systems, which include intersymbol interference (ISI) due to insufficient guard interval, interference from co-channel systems, and the effects of amplifier non-linearities on OFDM will be sought for MIMO OFDM systems. To provide data reliability, the combination of space-time coding, Turbo coding with iterative receivers, and hybrid ARQ strategies for error control across the PHY and MAC layers are being considered. Finally, media access protocols will be optimized to work with the parameters of the OFDM air interface, such as the OFDM symbol period.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121565</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121565</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1664" target="n1666">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Broadband MIMO OFDM Wireless Access</data>
      <data key="e_abstract">The convenience of wireless communications and the ever-increasing demand for higher data rates has motivated the proposed study of more efficient methods of transmission. One highly spectrally efficient link architecture is the multiple-input multiple-output (MIMO) architecture, which uses multiple transmit antennas and multiple receive antennas. Under ideal conditions, the theoretical capacity of a MIMO channel increases linearly in the number of transmit/receive antennas. In other words, the theory promises that more bits can be conveyed through the same bandwidth by simply adding more antennas at each end of the link. Orthogonal frequency division multiplexing (OFDM) is a high-data rate modulation technique that is already part of some single-transmitter communications standards and is known for its scalability and its convenient and cost-effective implementation using standard digital signal processing architectures. This study seeks to determine how to combine OFDM with MIMO architectures over real channels. A challenging goal of this research is to arrive at a flexible, scaleable, wireless modem with bandwidth efficiency on the order of 4 to 10 bits per second per Hertz.&lt;br/&gt;&lt;br/&gt;The study will include analysis of the major functions of the physical (PHY) and medium access control (MAC) layers. Hardware experimentation will include MIMO channel measurements, including characterization of MIMO interference, and verification of the MIMO OFDM link in real-time on a software radio test-bed. Methods for performing MIMO OFDM channel estimation and synchronization jointly over the spatial channels are being investigated, taking into account noise and channel estimation and synchronization errors. An adaptive transmitter is also being studied that combines space-time processing based on singular-value decomposition (SVD) with adaptive modulation. Solutions to the major transmission impairments in OFDM systems, which include intersymbol interference (ISI) due to insufficient guard interval, interference from co-channel systems, and the effects of amplifier non-linearities on OFDM will be sought for MIMO OFDM systems. To provide data reliability, the combination of space-time coding, Turbo coding with iterative receivers, and hybrid ARQ strategies for error control across the PHY and MAC layers are being considered. Finally, media access protocols will be optimized to work with the parameters of the OFDM air interface, such as the OFDM symbol period.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121565</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121565</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1664" target="n1667">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Broadband MIMO OFDM Wireless Access</data>
      <data key="e_abstract">The convenience of wireless communications and the ever-increasing demand for higher data rates has motivated the proposed study of more efficient methods of transmission. One highly spectrally efficient link architecture is the multiple-input multiple-output (MIMO) architecture, which uses multiple transmit antennas and multiple receive antennas. Under ideal conditions, the theoretical capacity of a MIMO channel increases linearly in the number of transmit/receive antennas. In other words, the theory promises that more bits can be conveyed through the same bandwidth by simply adding more antennas at each end of the link. Orthogonal frequency division multiplexing (OFDM) is a high-data rate modulation technique that is already part of some single-transmitter communications standards and is known for its scalability and its convenient and cost-effective implementation using standard digital signal processing architectures. This study seeks to determine how to combine OFDM with MIMO architectures over real channels. A challenging goal of this research is to arrive at a flexible, scaleable, wireless modem with bandwidth efficiency on the order of 4 to 10 bits per second per Hertz.&lt;br/&gt;&lt;br/&gt;The study will include analysis of the major functions of the physical (PHY) and medium access control (MAC) layers. Hardware experimentation will include MIMO channel measurements, including characterization of MIMO interference, and verification of the MIMO OFDM link in real-time on a software radio test-bed. Methods for performing MIMO OFDM channel estimation and synchronization jointly over the spatial channels are being investigated, taking into account noise and channel estimation and synchronization errors. An adaptive transmitter is also being studied that combines space-time processing based on singular-value decomposition (SVD) with adaptive modulation. Solutions to the major transmission impairments in OFDM systems, which include intersymbol interference (ISI) due to insufficient guard interval, interference from co-channel systems, and the effects of amplifier non-linearities on OFDM will be sought for MIMO OFDM systems. To provide data reliability, the combination of space-time coding, Turbo coding with iterative receivers, and hybrid ARQ strategies for error control across the PHY and MAC layers are being considered. Finally, media access protocols will be optimized to work with the parameters of the OFDM air interface, such as the OFDM symbol period.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121565</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121565</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1664" target="n1668">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Broadband MIMO OFDM Wireless Access</data>
      <data key="e_abstract">The convenience of wireless communications and the ever-increasing demand for higher data rates has motivated the proposed study of more efficient methods of transmission. One highly spectrally efficient link architecture is the multiple-input multiple-output (MIMO) architecture, which uses multiple transmit antennas and multiple receive antennas. Under ideal conditions, the theoretical capacity of a MIMO channel increases linearly in the number of transmit/receive antennas. In other words, the theory promises that more bits can be conveyed through the same bandwidth by simply adding more antennas at each end of the link. Orthogonal frequency division multiplexing (OFDM) is a high-data rate modulation technique that is already part of some single-transmitter communications standards and is known for its scalability and its convenient and cost-effective implementation using standard digital signal processing architectures. This study seeks to determine how to combine OFDM with MIMO architectures over real channels. A challenging goal of this research is to arrive at a flexible, scaleable, wireless modem with bandwidth efficiency on the order of 4 to 10 bits per second per Hertz.&lt;br/&gt;&lt;br/&gt;The study will include analysis of the major functions of the physical (PHY) and medium access control (MAC) layers. Hardware experimentation will include MIMO channel measurements, including characterization of MIMO interference, and verification of the MIMO OFDM link in real-time on a software radio test-bed. Methods for performing MIMO OFDM channel estimation and synchronization jointly over the spatial channels are being investigated, taking into account noise and channel estimation and synchronization errors. An adaptive transmitter is also being studied that combines space-time processing based on singular-value decomposition (SVD) with adaptive modulation. Solutions to the major transmission impairments in OFDM systems, which include intersymbol interference (ISI) due to insufficient guard interval, interference from co-channel systems, and the effects of amplifier non-linearities on OFDM will be sought for MIMO OFDM systems. To provide data reliability, the combination of space-time coding, Turbo coding with iterative receivers, and hybrid ARQ strategies for error control across the PHY and MAC layers are being considered. Finally, media access protocols will be optimized to work with the parameters of the OFDM air interface, such as the OFDM symbol period.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121565</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121565</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1665" target="n1666">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Broadband MIMO OFDM Wireless Access</data>
      <data key="e_abstract">The convenience of wireless communications and the ever-increasing demand for higher data rates has motivated the proposed study of more efficient methods of transmission. One highly spectrally efficient link architecture is the multiple-input multiple-output (MIMO) architecture, which uses multiple transmit antennas and multiple receive antennas. Under ideal conditions, the theoretical capacity of a MIMO channel increases linearly in the number of transmit/receive antennas. In other words, the theory promises that more bits can be conveyed through the same bandwidth by simply adding more antennas at each end of the link. Orthogonal frequency division multiplexing (OFDM) is a high-data rate modulation technique that is already part of some single-transmitter communications standards and is known for its scalability and its convenient and cost-effective implementation using standard digital signal processing architectures. This study seeks to determine how to combine OFDM with MIMO architectures over real channels. A challenging goal of this research is to arrive at a flexible, scaleable, wireless modem with bandwidth efficiency on the order of 4 to 10 bits per second per Hertz.&lt;br/&gt;&lt;br/&gt;The study will include analysis of the major functions of the physical (PHY) and medium access control (MAC) layers. Hardware experimentation will include MIMO channel measurements, including characterization of MIMO interference, and verification of the MIMO OFDM link in real-time on a software radio test-bed. Methods for performing MIMO OFDM channel estimation and synchronization jointly over the spatial channels are being investigated, taking into account noise and channel estimation and synchronization errors. An adaptive transmitter is also being studied that combines space-time processing based on singular-value decomposition (SVD) with adaptive modulation. Solutions to the major transmission impairments in OFDM systems, which include intersymbol interference (ISI) due to insufficient guard interval, interference from co-channel systems, and the effects of amplifier non-linearities on OFDM will be sought for MIMO OFDM systems. To provide data reliability, the combination of space-time coding, Turbo coding with iterative receivers, and hybrid ARQ strategies for error control across the PHY and MAC layers are being considered. Finally, media access protocols will be optimized to work with the parameters of the OFDM air interface, such as the OFDM symbol period.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121565</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121565</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1665" target="n1667">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Broadband MIMO OFDM Wireless Access</data>
      <data key="e_abstract">The convenience of wireless communications and the ever-increasing demand for higher data rates has motivated the proposed study of more efficient methods of transmission. One highly spectrally efficient link architecture is the multiple-input multiple-output (MIMO) architecture, which uses multiple transmit antennas and multiple receive antennas. Under ideal conditions, the theoretical capacity of a MIMO channel increases linearly in the number of transmit/receive antennas. In other words, the theory promises that more bits can be conveyed through the same bandwidth by simply adding more antennas at each end of the link. Orthogonal frequency division multiplexing (OFDM) is a high-data rate modulation technique that is already part of some single-transmitter communications standards and is known for its scalability and its convenient and cost-effective implementation using standard digital signal processing architectures. This study seeks to determine how to combine OFDM with MIMO architectures over real channels. A challenging goal of this research is to arrive at a flexible, scaleable, wireless modem with bandwidth efficiency on the order of 4 to 10 bits per second per Hertz.&lt;br/&gt;&lt;br/&gt;The study will include analysis of the major functions of the physical (PHY) and medium access control (MAC) layers. Hardware experimentation will include MIMO channel measurements, including characterization of MIMO interference, and verification of the MIMO OFDM link in real-time on a software radio test-bed. Methods for performing MIMO OFDM channel estimation and synchronization jointly over the spatial channels are being investigated, taking into account noise and channel estimation and synchronization errors. An adaptive transmitter is also being studied that combines space-time processing based on singular-value decomposition (SVD) with adaptive modulation. Solutions to the major transmission impairments in OFDM systems, which include intersymbol interference (ISI) due to insufficient guard interval, interference from co-channel systems, and the effects of amplifier non-linearities on OFDM will be sought for MIMO OFDM systems. To provide data reliability, the combination of space-time coding, Turbo coding with iterative receivers, and hybrid ARQ strategies for error control across the PHY and MAC layers are being considered. Finally, media access protocols will be optimized to work with the parameters of the OFDM air interface, such as the OFDM symbol period.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121565</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121565</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1665" target="n1668">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Broadband MIMO OFDM Wireless Access</data>
      <data key="e_abstract">The convenience of wireless communications and the ever-increasing demand for higher data rates has motivated the proposed study of more efficient methods of transmission. One highly spectrally efficient link architecture is the multiple-input multiple-output (MIMO) architecture, which uses multiple transmit antennas and multiple receive antennas. Under ideal conditions, the theoretical capacity of a MIMO channel increases linearly in the number of transmit/receive antennas. In other words, the theory promises that more bits can be conveyed through the same bandwidth by simply adding more antennas at each end of the link. Orthogonal frequency division multiplexing (OFDM) is a high-data rate modulation technique that is already part of some single-transmitter communications standards and is known for its scalability and its convenient and cost-effective implementation using standard digital signal processing architectures. This study seeks to determine how to combine OFDM with MIMO architectures over real channels. A challenging goal of this research is to arrive at a flexible, scaleable, wireless modem with bandwidth efficiency on the order of 4 to 10 bits per second per Hertz.&lt;br/&gt;&lt;br/&gt;The study will include analysis of the major functions of the physical (PHY) and medium access control (MAC) layers. Hardware experimentation will include MIMO channel measurements, including characterization of MIMO interference, and verification of the MIMO OFDM link in real-time on a software radio test-bed. Methods for performing MIMO OFDM channel estimation and synchronization jointly over the spatial channels are being investigated, taking into account noise and channel estimation and synchronization errors. An adaptive transmitter is also being studied that combines space-time processing based on singular-value decomposition (SVD) with adaptive modulation. Solutions to the major transmission impairments in OFDM systems, which include intersymbol interference (ISI) due to insufficient guard interval, interference from co-channel systems, and the effects of amplifier non-linearities on OFDM will be sought for MIMO OFDM systems. To provide data reliability, the combination of space-time coding, Turbo coding with iterative receivers, and hybrid ARQ strategies for error control across the PHY and MAC layers are being considered. Finally, media access protocols will be optimized to work with the parameters of the OFDM air interface, such as the OFDM symbol period.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121565</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121565</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1666" target="n1667">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Broadband MIMO OFDM Wireless Access</data>
      <data key="e_abstract">The convenience of wireless communications and the ever-increasing demand for higher data rates has motivated the proposed study of more efficient methods of transmission. One highly spectrally efficient link architecture is the multiple-input multiple-output (MIMO) architecture, which uses multiple transmit antennas and multiple receive antennas. Under ideal conditions, the theoretical capacity of a MIMO channel increases linearly in the number of transmit/receive antennas. In other words, the theory promises that more bits can be conveyed through the same bandwidth by simply adding more antennas at each end of the link. Orthogonal frequency division multiplexing (OFDM) is a high-data rate modulation technique that is already part of some single-transmitter communications standards and is known for its scalability and its convenient and cost-effective implementation using standard digital signal processing architectures. This study seeks to determine how to combine OFDM with MIMO architectures over real channels. A challenging goal of this research is to arrive at a flexible, scaleable, wireless modem with bandwidth efficiency on the order of 4 to 10 bits per second per Hertz.&lt;br/&gt;&lt;br/&gt;The study will include analysis of the major functions of the physical (PHY) and medium access control (MAC) layers. Hardware experimentation will include MIMO channel measurements, including characterization of MIMO interference, and verification of the MIMO OFDM link in real-time on a software radio test-bed. Methods for performing MIMO OFDM channel estimation and synchronization jointly over the spatial channels are being investigated, taking into account noise and channel estimation and synchronization errors. An adaptive transmitter is also being studied that combines space-time processing based on singular-value decomposition (SVD) with adaptive modulation. Solutions to the major transmission impairments in OFDM systems, which include intersymbol interference (ISI) due to insufficient guard interval, interference from co-channel systems, and the effects of amplifier non-linearities on OFDM will be sought for MIMO OFDM systems. To provide data reliability, the combination of space-time coding, Turbo coding with iterative receivers, and hybrid ARQ strategies for error control across the PHY and MAC layers are being considered. Finally, media access protocols will be optimized to work with the parameters of the OFDM air interface, such as the OFDM symbol period.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121565</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121565</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1666" target="n1668">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Broadband MIMO OFDM Wireless Access</data>
      <data key="e_abstract">The convenience of wireless communications and the ever-increasing demand for higher data rates has motivated the proposed study of more efficient methods of transmission. One highly spectrally efficient link architecture is the multiple-input multiple-output (MIMO) architecture, which uses multiple transmit antennas and multiple receive antennas. Under ideal conditions, the theoretical capacity of a MIMO channel increases linearly in the number of transmit/receive antennas. In other words, the theory promises that more bits can be conveyed through the same bandwidth by simply adding more antennas at each end of the link. Orthogonal frequency division multiplexing (OFDM) is a high-data rate modulation technique that is already part of some single-transmitter communications standards and is known for its scalability and its convenient and cost-effective implementation using standard digital signal processing architectures. This study seeks to determine how to combine OFDM with MIMO architectures over real channels. A challenging goal of this research is to arrive at a flexible, scaleable, wireless modem with bandwidth efficiency on the order of 4 to 10 bits per second per Hertz.&lt;br/&gt;&lt;br/&gt;The study will include analysis of the major functions of the physical (PHY) and medium access control (MAC) layers. Hardware experimentation will include MIMO channel measurements, including characterization of MIMO interference, and verification of the MIMO OFDM link in real-time on a software radio test-bed. Methods for performing MIMO OFDM channel estimation and synchronization jointly over the spatial channels are being investigated, taking into account noise and channel estimation and synchronization errors. An adaptive transmitter is also being studied that combines space-time processing based on singular-value decomposition (SVD) with adaptive modulation. Solutions to the major transmission impairments in OFDM systems, which include intersymbol interference (ISI) due to insufficient guard interval, interference from co-channel systems, and the effects of amplifier non-linearities on OFDM will be sought for MIMO OFDM systems. To provide data reliability, the combination of space-time coding, Turbo coding with iterative receivers, and hybrid ARQ strategies for error control across the PHY and MAC layers are being considered. Finally, media access protocols will be optimized to work with the parameters of the OFDM air interface, such as the OFDM symbol period.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121565</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121565</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1667" target="n1668">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Broadband MIMO OFDM Wireless Access</data>
      <data key="e_abstract">The convenience of wireless communications and the ever-increasing demand for higher data rates has motivated the proposed study of more efficient methods of transmission. One highly spectrally efficient link architecture is the multiple-input multiple-output (MIMO) architecture, which uses multiple transmit antennas and multiple receive antennas. Under ideal conditions, the theoretical capacity of a MIMO channel increases linearly in the number of transmit/receive antennas. In other words, the theory promises that more bits can be conveyed through the same bandwidth by simply adding more antennas at each end of the link. Orthogonal frequency division multiplexing (OFDM) is a high-data rate modulation technique that is already part of some single-transmitter communications standards and is known for its scalability and its convenient and cost-effective implementation using standard digital signal processing architectures. This study seeks to determine how to combine OFDM with MIMO architectures over real channels. A challenging goal of this research is to arrive at a flexible, scaleable, wireless modem with bandwidth efficiency on the order of 4 to 10 bits per second per Hertz.&lt;br/&gt;&lt;br/&gt;The study will include analysis of the major functions of the physical (PHY) and medium access control (MAC) layers. Hardware experimentation will include MIMO channel measurements, including characterization of MIMO interference, and verification of the MIMO OFDM link in real-time on a software radio test-bed. Methods for performing MIMO OFDM channel estimation and synchronization jointly over the spatial channels are being investigated, taking into account noise and channel estimation and synchronization errors. An adaptive transmitter is also being studied that combines space-time processing based on singular-value decomposition (SVD) with adaptive modulation. Solutions to the major transmission impairments in OFDM systems, which include intersymbol interference (ISI) due to insufficient guard interval, interference from co-channel systems, and the effects of amplifier non-linearities on OFDM will be sought for MIMO OFDM systems. To provide data reliability, the combination of space-time coding, Turbo coding with iterative receivers, and hybrid ARQ strategies for error control across the PHY and MAC layers are being considered. Finally, media access protocols will be optimized to work with the parameters of the OFDM air interface, such as the OFDM symbol period.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121565</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121565</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1670" target="n1671">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Testing and Screening of the New Chip</data>
      <data key="e_abstract">This project is about testing digital integrated circuits during their manufacture -- production testing. Production testing attempts to identify manufactured parts that do not perform according to their &lt;br/&gt;specification (defective parts) as well as parts that will fail early in their operating life (weak parts). Sometimes parts are also sorted by their maximum operating speed (binning). For some selected parts, the &lt;br/&gt;exact causes of their deviation from expected functionality must be identified (diagnosed) in order to improve the manufacturing process. Improved IC production test techniques are necessary because current &lt;br/&gt;techniques are too expensive and are projected to become increasingly unsatisfactory for future IC designs. The manufacturing cost per transistor is decreasing for newer technologies while the cost per &lt;br/&gt;transisitor for production testing is remaining approximately constant. In other words, the fraction of the total cost of manufacturing process due to testing is increasing. There is interest in reversing this trend.&lt;br/&gt;&lt;br/&gt;Specific research goals are to develop test techniques to replace current burn-in, delay test and the single-stuck fault coverage metric. Other goals will to be compare test results for different generations of &lt;br/&gt;technology to find out to what extent results from one technology carry forward to a next generation, and to compare the effectiveness of functional tests with structured tests (for example, sequential circuit &lt;br/&gt;test patterns versus scan patterns). Experimental data obtained from ATE will be analyzed. This data will be used to suggest new techniques and to determine their effectiveness. We will use two types of experimental &lt;br/&gt;data: some is industrial data that we obtain as part of a joint study with a company; the other is data that we collect ourselves.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">98218</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">98218</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n100" target="n101">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/IM+PE+SY: Summer Workshops on Human Language Technology: Integrating Research and Education</data>
      <data key="e_abstract">We propose to organize yearly, intensive six-week research workshops&lt;br/&gt;at Johns Hopkins University, focusing on Language Engineering,&lt;br/&gt;including the mutually related areas of automatic speech recognition&lt;br/&gt;and synthesis, natural language processing, machine translation,&lt;br/&gt;information extraction and summarization. Applications of language&lt;br/&gt;engineering techniques to other domains such as language instruction&lt;br/&gt;or bioinformatics will also be appropriate workshop topics. These&lt;br/&gt;workshops will bring together teams of leading professionals and&lt;br/&gt;graduate and undergraduate students in a cooperative effort to&lt;br/&gt;advance the state of the art. The first goal of the proposed workshop&lt;br/&gt;series is to establish new research directions in Language Engineering.&lt;br/&gt;The second goal is to attract students to the field and to offer&lt;br/&gt;them an intensive hands-on mentored research education. The third&lt;br/&gt;is to engender extensive cross-fertilization between researchers&lt;br/&gt;distributed across industrial, academic and government institutions,&lt;br/&gt;forging intensive links in the 6-week summer period and offering&lt;br/&gt;a shared research environment for follow-on collaborative work.&lt;br/&gt;To ensure that the projects address current problems in the state&lt;br/&gt;of the art, each year an open call for workshop project proposals&lt;br/&gt;will be issued to researchers in the worldwide language engineering&lt;br/&gt;community. The proposals received will be evaluated competitively&lt;br/&gt;at planning meetings held each year. The meetings will draw together&lt;br/&gt;project proponents, government representatives, and experts from&lt;br/&gt;related fields to assess the viability and promise of the proposals&lt;br/&gt;and to identify three candidate projects for the summer workshop.&lt;br/&gt;Throughout this process of soliciting proposals and recruiting&lt;br/&gt;the personnel to carry them out, we will make a diligent attempt&lt;br/&gt;to include researchers from underrepresented institutions and&lt;br/&gt;communities.</data>
      <data key="e_pgm">T391</data>
      <data key="e_label">121285</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121285</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n100" target="n1674">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/IM+PE+SY: Summer Workshops on Human Language Technology: Integrating Research and Education</data>
      <data key="e_abstract">We propose to organize yearly, intensive six-week research workshops&lt;br/&gt;at Johns Hopkins University, focusing on Language Engineering,&lt;br/&gt;including the mutually related areas of automatic speech recognition&lt;br/&gt;and synthesis, natural language processing, machine translation,&lt;br/&gt;information extraction and summarization. Applications of language&lt;br/&gt;engineering techniques to other domains such as language instruction&lt;br/&gt;or bioinformatics will also be appropriate workshop topics. These&lt;br/&gt;workshops will bring together teams of leading professionals and&lt;br/&gt;graduate and undergraduate students in a cooperative effort to&lt;br/&gt;advance the state of the art. The first goal of the proposed workshop&lt;br/&gt;series is to establish new research directions in Language Engineering.&lt;br/&gt;The second goal is to attract students to the field and to offer&lt;br/&gt;them an intensive hands-on mentored research education. The third&lt;br/&gt;is to engender extensive cross-fertilization between researchers&lt;br/&gt;distributed across industrial, academic and government institutions,&lt;br/&gt;forging intensive links in the 6-week summer period and offering&lt;br/&gt;a shared research environment for follow-on collaborative work.&lt;br/&gt;To ensure that the projects address current problems in the state&lt;br/&gt;of the art, each year an open call for workshop project proposals&lt;br/&gt;will be issued to researchers in the worldwide language engineering&lt;br/&gt;community. The proposals received will be evaluated competitively&lt;br/&gt;at planning meetings held each year. The meetings will draw together&lt;br/&gt;project proponents, government representatives, and experts from&lt;br/&gt;related fields to assess the viability and promise of the proposals&lt;br/&gt;and to identify three candidate projects for the summer workshop.&lt;br/&gt;Throughout this process of soliciting proposals and recruiting&lt;br/&gt;the personnel to carry them out, we will make a diligent attempt&lt;br/&gt;to include researchers from underrepresented institutions and&lt;br/&gt;communities.</data>
      <data key="e_pgm">T391</data>
      <data key="e_label">121285</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121285</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n100" target="n102">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/IM+PE+SY: Summer Workshops on Human Language Technology: Integrating Research and Education</data>
      <data key="e_abstract">We propose to organize yearly, intensive six-week research workshops&lt;br/&gt;at Johns Hopkins University, focusing on Language Engineering,&lt;br/&gt;including the mutually related areas of automatic speech recognition&lt;br/&gt;and synthesis, natural language processing, machine translation,&lt;br/&gt;information extraction and summarization. Applications of language&lt;br/&gt;engineering techniques to other domains such as language instruction&lt;br/&gt;or bioinformatics will also be appropriate workshop topics. These&lt;br/&gt;workshops will bring together teams of leading professionals and&lt;br/&gt;graduate and undergraduate students in a cooperative effort to&lt;br/&gt;advance the state of the art. The first goal of the proposed workshop&lt;br/&gt;series is to establish new research directions in Language Engineering.&lt;br/&gt;The second goal is to attract students to the field and to offer&lt;br/&gt;them an intensive hands-on mentored research education. The third&lt;br/&gt;is to engender extensive cross-fertilization between researchers&lt;br/&gt;distributed across industrial, academic and government institutions,&lt;br/&gt;forging intensive links in the 6-week summer period and offering&lt;br/&gt;a shared research environment for follow-on collaborative work.&lt;br/&gt;To ensure that the projects address current problems in the state&lt;br/&gt;of the art, each year an open call for workshop project proposals&lt;br/&gt;will be issued to researchers in the worldwide language engineering&lt;br/&gt;community. The proposals received will be evaluated competitively&lt;br/&gt;at planning meetings held each year. The meetings will draw together&lt;br/&gt;project proponents, government representatives, and experts from&lt;br/&gt;related fields to assess the viability and promise of the proposals&lt;br/&gt;and to identify three candidate projects for the summer workshop.&lt;br/&gt;Throughout this process of soliciting proposals and recruiting&lt;br/&gt;the personnel to carry them out, we will make a diligent attempt&lt;br/&gt;to include researchers from underrepresented institutions and&lt;br/&gt;communities.</data>
      <data key="e_pgm">T391</data>
      <data key="e_label">121285</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121285</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n100" target="n1676">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/IM+PE+SY: Summer Workshops on Human Language Technology: Integrating Research and Education</data>
      <data key="e_abstract">We propose to organize yearly, intensive six-week research workshops&lt;br/&gt;at Johns Hopkins University, focusing on Language Engineering,&lt;br/&gt;including the mutually related areas of automatic speech recognition&lt;br/&gt;and synthesis, natural language processing, machine translation,&lt;br/&gt;information extraction and summarization. Applications of language&lt;br/&gt;engineering techniques to other domains such as language instruction&lt;br/&gt;or bioinformatics will also be appropriate workshop topics. These&lt;br/&gt;workshops will bring together teams of leading professionals and&lt;br/&gt;graduate and undergraduate students in a cooperative effort to&lt;br/&gt;advance the state of the art. The first goal of the proposed workshop&lt;br/&gt;series is to establish new research directions in Language Engineering.&lt;br/&gt;The second goal is to attract students to the field and to offer&lt;br/&gt;them an intensive hands-on mentored research education. The third&lt;br/&gt;is to engender extensive cross-fertilization between researchers&lt;br/&gt;distributed across industrial, academic and government institutions,&lt;br/&gt;forging intensive links in the 6-week summer period and offering&lt;br/&gt;a shared research environment for follow-on collaborative work.&lt;br/&gt;To ensure that the projects address current problems in the state&lt;br/&gt;of the art, each year an open call for workshop project proposals&lt;br/&gt;will be issued to researchers in the worldwide language engineering&lt;br/&gt;community. The proposals received will be evaluated competitively&lt;br/&gt;at planning meetings held each year. The meetings will draw together&lt;br/&gt;project proponents, government representatives, and experts from&lt;br/&gt;related fields to assess the viability and promise of the proposals&lt;br/&gt;and to identify three candidate projects for the summer workshop.&lt;br/&gt;Throughout this process of soliciting proposals and recruiting&lt;br/&gt;the personnel to carry them out, we will make a diligent attempt&lt;br/&gt;to include researchers from underrepresented institutions and&lt;br/&gt;communities.</data>
      <data key="e_pgm">T391</data>
      <data key="e_label">121285</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121285</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n101" target="n1674">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/IM+PE+SY: Summer Workshops on Human Language Technology: Integrating Research and Education</data>
      <data key="e_abstract">We propose to organize yearly, intensive six-week research workshops&lt;br/&gt;at Johns Hopkins University, focusing on Language Engineering,&lt;br/&gt;including the mutually related areas of automatic speech recognition&lt;br/&gt;and synthesis, natural language processing, machine translation,&lt;br/&gt;information extraction and summarization. Applications of language&lt;br/&gt;engineering techniques to other domains such as language instruction&lt;br/&gt;or bioinformatics will also be appropriate workshop topics. These&lt;br/&gt;workshops will bring together teams of leading professionals and&lt;br/&gt;graduate and undergraduate students in a cooperative effort to&lt;br/&gt;advance the state of the art. The first goal of the proposed workshop&lt;br/&gt;series is to establish new research directions in Language Engineering.&lt;br/&gt;The second goal is to attract students to the field and to offer&lt;br/&gt;them an intensive hands-on mentored research education. The third&lt;br/&gt;is to engender extensive cross-fertilization between researchers&lt;br/&gt;distributed across industrial, academic and government institutions,&lt;br/&gt;forging intensive links in the 6-week summer period and offering&lt;br/&gt;a shared research environment for follow-on collaborative work.&lt;br/&gt;To ensure that the projects address current problems in the state&lt;br/&gt;of the art, each year an open call for workshop project proposals&lt;br/&gt;will be issued to researchers in the worldwide language engineering&lt;br/&gt;community. The proposals received will be evaluated competitively&lt;br/&gt;at planning meetings held each year. The meetings will draw together&lt;br/&gt;project proponents, government representatives, and experts from&lt;br/&gt;related fields to assess the viability and promise of the proposals&lt;br/&gt;and to identify three candidate projects for the summer workshop.&lt;br/&gt;Throughout this process of soliciting proposals and recruiting&lt;br/&gt;the personnel to carry them out, we will make a diligent attempt&lt;br/&gt;to include researchers from underrepresented institutions and&lt;br/&gt;communities.</data>
      <data key="e_pgm">T391</data>
      <data key="e_label">121285</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121285</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n101" target="n102">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/IM+PE+SY: Summer Workshops on Human Language Technology: Integrating Research and Education</data>
      <data key="e_abstract">We propose to organize yearly, intensive six-week research workshops&lt;br/&gt;at Johns Hopkins University, focusing on Language Engineering,&lt;br/&gt;including the mutually related areas of automatic speech recognition&lt;br/&gt;and synthesis, natural language processing, machine translation,&lt;br/&gt;information extraction and summarization. Applications of language&lt;br/&gt;engineering techniques to other domains such as language instruction&lt;br/&gt;or bioinformatics will also be appropriate workshop topics. These&lt;br/&gt;workshops will bring together teams of leading professionals and&lt;br/&gt;graduate and undergraduate students in a cooperative effort to&lt;br/&gt;advance the state of the art. The first goal of the proposed workshop&lt;br/&gt;series is to establish new research directions in Language Engineering.&lt;br/&gt;The second goal is to attract students to the field and to offer&lt;br/&gt;them an intensive hands-on mentored research education. The third&lt;br/&gt;is to engender extensive cross-fertilization between researchers&lt;br/&gt;distributed across industrial, academic and government institutions,&lt;br/&gt;forging intensive links in the 6-week summer period and offering&lt;br/&gt;a shared research environment for follow-on collaborative work.&lt;br/&gt;To ensure that the projects address current problems in the state&lt;br/&gt;of the art, each year an open call for workshop project proposals&lt;br/&gt;will be issued to researchers in the worldwide language engineering&lt;br/&gt;community. The proposals received will be evaluated competitively&lt;br/&gt;at planning meetings held each year. The meetings will draw together&lt;br/&gt;project proponents, government representatives, and experts from&lt;br/&gt;related fields to assess the viability and promise of the proposals&lt;br/&gt;and to identify three candidate projects for the summer workshop.&lt;br/&gt;Throughout this process of soliciting proposals and recruiting&lt;br/&gt;the personnel to carry them out, we will make a diligent attempt&lt;br/&gt;to include researchers from underrepresented institutions and&lt;br/&gt;communities.</data>
      <data key="e_pgm">T391</data>
      <data key="e_label">121285</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121285</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n101" target="n1676">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/IM+PE+SY: Summer Workshops on Human Language Technology: Integrating Research and Education</data>
      <data key="e_abstract">We propose to organize yearly, intensive six-week research workshops&lt;br/&gt;at Johns Hopkins University, focusing on Language Engineering,&lt;br/&gt;including the mutually related areas of automatic speech recognition&lt;br/&gt;and synthesis, natural language processing, machine translation,&lt;br/&gt;information extraction and summarization. Applications of language&lt;br/&gt;engineering techniques to other domains such as language instruction&lt;br/&gt;or bioinformatics will also be appropriate workshop topics. These&lt;br/&gt;workshops will bring together teams of leading professionals and&lt;br/&gt;graduate and undergraduate students in a cooperative effort to&lt;br/&gt;advance the state of the art. The first goal of the proposed workshop&lt;br/&gt;series is to establish new research directions in Language Engineering.&lt;br/&gt;The second goal is to attract students to the field and to offer&lt;br/&gt;them an intensive hands-on mentored research education. The third&lt;br/&gt;is to engender extensive cross-fertilization between researchers&lt;br/&gt;distributed across industrial, academic and government institutions,&lt;br/&gt;forging intensive links in the 6-week summer period and offering&lt;br/&gt;a shared research environment for follow-on collaborative work.&lt;br/&gt;To ensure that the projects address current problems in the state&lt;br/&gt;of the art, each year an open call for workshop project proposals&lt;br/&gt;will be issued to researchers in the worldwide language engineering&lt;br/&gt;community. The proposals received will be evaluated competitively&lt;br/&gt;at planning meetings held each year. The meetings will draw together&lt;br/&gt;project proponents, government representatives, and experts from&lt;br/&gt;related fields to assess the viability and promise of the proposals&lt;br/&gt;and to identify three candidate projects for the summer workshop.&lt;br/&gt;Throughout this process of soliciting proposals and recruiting&lt;br/&gt;the personnel to carry them out, we will make a diligent attempt&lt;br/&gt;to include researchers from underrepresented institutions and&lt;br/&gt;communities.</data>
      <data key="e_pgm">T391</data>
      <data key="e_label">121285</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121285</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n102" target="n1674">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/IM+PE+SY: Summer Workshops on Human Language Technology: Integrating Research and Education</data>
      <data key="e_abstract">We propose to organize yearly, intensive six-week research workshops&lt;br/&gt;at Johns Hopkins University, focusing on Language Engineering,&lt;br/&gt;including the mutually related areas of automatic speech recognition&lt;br/&gt;and synthesis, natural language processing, machine translation,&lt;br/&gt;information extraction and summarization. Applications of language&lt;br/&gt;engineering techniques to other domains such as language instruction&lt;br/&gt;or bioinformatics will also be appropriate workshop topics. These&lt;br/&gt;workshops will bring together teams of leading professionals and&lt;br/&gt;graduate and undergraduate students in a cooperative effort to&lt;br/&gt;advance the state of the art. The first goal of the proposed workshop&lt;br/&gt;series is to establish new research directions in Language Engineering.&lt;br/&gt;The second goal is to attract students to the field and to offer&lt;br/&gt;them an intensive hands-on mentored research education. The third&lt;br/&gt;is to engender extensive cross-fertilization between researchers&lt;br/&gt;distributed across industrial, academic and government institutions,&lt;br/&gt;forging intensive links in the 6-week summer period and offering&lt;br/&gt;a shared research environment for follow-on collaborative work.&lt;br/&gt;To ensure that the projects address current problems in the state&lt;br/&gt;of the art, each year an open call for workshop project proposals&lt;br/&gt;will be issued to researchers in the worldwide language engineering&lt;br/&gt;community. The proposals received will be evaluated competitively&lt;br/&gt;at planning meetings held each year. The meetings will draw together&lt;br/&gt;project proponents, government representatives, and experts from&lt;br/&gt;related fields to assess the viability and promise of the proposals&lt;br/&gt;and to identify three candidate projects for the summer workshop.&lt;br/&gt;Throughout this process of soliciting proposals and recruiting&lt;br/&gt;the personnel to carry them out, we will make a diligent attempt&lt;br/&gt;to include researchers from underrepresented institutions and&lt;br/&gt;communities.</data>
      <data key="e_pgm">T391</data>
      <data key="e_label">121285</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121285</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1674" target="n1676">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/IM+PE+SY: Summer Workshops on Human Language Technology: Integrating Research and Education</data>
      <data key="e_abstract">We propose to organize yearly, intensive six-week research workshops&lt;br/&gt;at Johns Hopkins University, focusing on Language Engineering,&lt;br/&gt;including the mutually related areas of automatic speech recognition&lt;br/&gt;and synthesis, natural language processing, machine translation,&lt;br/&gt;information extraction and summarization. Applications of language&lt;br/&gt;engineering techniques to other domains such as language instruction&lt;br/&gt;or bioinformatics will also be appropriate workshop topics. These&lt;br/&gt;workshops will bring together teams of leading professionals and&lt;br/&gt;graduate and undergraduate students in a cooperative effort to&lt;br/&gt;advance the state of the art. The first goal of the proposed workshop&lt;br/&gt;series is to establish new research directions in Language Engineering.&lt;br/&gt;The second goal is to attract students to the field and to offer&lt;br/&gt;them an intensive hands-on mentored research education. The third&lt;br/&gt;is to engender extensive cross-fertilization between researchers&lt;br/&gt;distributed across industrial, academic and government institutions,&lt;br/&gt;forging intensive links in the 6-week summer period and offering&lt;br/&gt;a shared research environment for follow-on collaborative work.&lt;br/&gt;To ensure that the projects address current problems in the state&lt;br/&gt;of the art, each year an open call for workshop project proposals&lt;br/&gt;will be issued to researchers in the worldwide language engineering&lt;br/&gt;community. The proposals received will be evaluated competitively&lt;br/&gt;at planning meetings held each year. The meetings will draw together&lt;br/&gt;project proponents, government representatives, and experts from&lt;br/&gt;related fields to assess the viability and promise of the proposals&lt;br/&gt;and to identify three candidate projects for the summer workshop.&lt;br/&gt;Throughout this process of soliciting proposals and recruiting&lt;br/&gt;the personnel to carry them out, we will make a diligent attempt&lt;br/&gt;to include researchers from underrepresented institutions and&lt;br/&gt;communities.</data>
      <data key="e_pgm">T391</data>
      <data key="e_label">121285</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121285</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n102" target="n1676">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/IM+PE+SY: Summer Workshops on Human Language Technology: Integrating Research and Education</data>
      <data key="e_abstract">We propose to organize yearly, intensive six-week research workshops&lt;br/&gt;at Johns Hopkins University, focusing on Language Engineering,&lt;br/&gt;including the mutually related areas of automatic speech recognition&lt;br/&gt;and synthesis, natural language processing, machine translation,&lt;br/&gt;information extraction and summarization. Applications of language&lt;br/&gt;engineering techniques to other domains such as language instruction&lt;br/&gt;or bioinformatics will also be appropriate workshop topics. These&lt;br/&gt;workshops will bring together teams of leading professionals and&lt;br/&gt;graduate and undergraduate students in a cooperative effort to&lt;br/&gt;advance the state of the art. The first goal of the proposed workshop&lt;br/&gt;series is to establish new research directions in Language Engineering.&lt;br/&gt;The second goal is to attract students to the field and to offer&lt;br/&gt;them an intensive hands-on mentored research education. The third&lt;br/&gt;is to engender extensive cross-fertilization between researchers&lt;br/&gt;distributed across industrial, academic and government institutions,&lt;br/&gt;forging intensive links in the 6-week summer period and offering&lt;br/&gt;a shared research environment for follow-on collaborative work.&lt;br/&gt;To ensure that the projects address current problems in the state&lt;br/&gt;of the art, each year an open call for workshop project proposals&lt;br/&gt;will be issued to researchers in the worldwide language engineering&lt;br/&gt;community. The proposals received will be evaluated competitively&lt;br/&gt;at planning meetings held each year. The meetings will draw together&lt;br/&gt;project proponents, government representatives, and experts from&lt;br/&gt;related fields to assess the viability and promise of the proposals&lt;br/&gt;and to identify three candidate projects for the summer workshop.&lt;br/&gt;Throughout this process of soliciting proposals and recruiting&lt;br/&gt;the personnel to carry them out, we will make a diligent attempt&lt;br/&gt;to include researchers from underrepresented institutions and&lt;br/&gt;communities.</data>
      <data key="e_pgm">T391</data>
      <data key="e_label">121285</data>
      <data key="e_expirationDate">2008-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121285</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1677" target="n1678">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY+PE+IM+AP Collaborative Research: Computer-Linked Auto-Fabricated Models for Education in Molecular Biology</data>
      <data key="e_abstract">This is a research project in the design and implementation of novel Human Computer Interfaces for educational use in structural molecular biology. This project will explore, define, and assess the role of computer-generated physical models in teaching scientific content and concepts. The purpose is to enhance the understanding and communication of the complex world of life&apos;s molecular machinery to a broad community. Using structural data on biological molecules and their complexes, the participants will prototype novel physical models via automated design and fabrication technologies and develop replication processes for broader distribution. The models will be used directly for enhance visualization and as input/output devices that interactively integrate with commutation and computer graphics for information retrieval, manipulation, and simulation.&lt;br/&gt;&lt;br/&gt;This project will provide new, tangible modes of interacting with, and understanding of, both the fundamental concepts and the complex data that are coming from the rapid advances in genomics, proteomics, and other areas of structural molecular biology. It will enable a broad range of students to learn, query, and explore in a field of growing scientific and social importance - the molecular basis of life. Physical models will give both haptic and visual support of the importance of shape in biological function. Use of physical models as tangible computer interfaces will revolutionize how students explore and understand biomolecular structure, interaction, and function.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121282</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121282</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1677" target="n1679">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY+PE+IM+AP Collaborative Research: Computer-Linked Auto-Fabricated Models for Education in Molecular Biology</data>
      <data key="e_abstract">This is a research project in the design and implementation of novel Human Computer Interfaces for educational use in structural molecular biology. This project will explore, define, and assess the role of computer-generated physical models in teaching scientific content and concepts. The purpose is to enhance the understanding and communication of the complex world of life&apos;s molecular machinery to a broad community. Using structural data on biological molecules and their complexes, the participants will prototype novel physical models via automated design and fabrication technologies and develop replication processes for broader distribution. The models will be used directly for enhance visualization and as input/output devices that interactively integrate with commutation and computer graphics for information retrieval, manipulation, and simulation.&lt;br/&gt;&lt;br/&gt;This project will provide new, tangible modes of interacting with, and understanding of, both the fundamental concepts and the complex data that are coming from the rapid advances in genomics, proteomics, and other areas of structural molecular biology. It will enable a broad range of students to learn, query, and explore in a field of growing scientific and social importance - the molecular basis of life. Physical models will give both haptic and visual support of the importance of shape in biological function. Use of physical models as tangible computer interfaces will revolutionize how students explore and understand biomolecular structure, interaction, and function.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121282</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121282</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1677" target="n1680">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY+PE+IM+AP Collaborative Research: Computer-Linked Auto-Fabricated Models for Education in Molecular Biology</data>
      <data key="e_abstract">This is a research project in the design and implementation of novel Human Computer Interfaces for educational use in structural molecular biology. This project will explore, define, and assess the role of computer-generated physical models in teaching scientific content and concepts. The purpose is to enhance the understanding and communication of the complex world of life&apos;s molecular machinery to a broad community. Using structural data on biological molecules and their complexes, the participants will prototype novel physical models via automated design and fabrication technologies and develop replication processes for broader distribution. The models will be used directly for enhance visualization and as input/output devices that interactively integrate with commutation and computer graphics for information retrieval, manipulation, and simulation.&lt;br/&gt;&lt;br/&gt;This project will provide new, tangible modes of interacting with, and understanding of, both the fundamental concepts and the complex data that are coming from the rapid advances in genomics, proteomics, and other areas of structural molecular biology. It will enable a broad range of students to learn, query, and explore in a field of growing scientific and social importance - the molecular basis of life. Physical models will give both haptic and visual support of the importance of shape in biological function. Use of physical models as tangible computer interfaces will revolutionize how students explore and understand biomolecular structure, interaction, and function.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121282</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121282</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1678" target="n1679">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY+PE+IM+AP Collaborative Research: Computer-Linked Auto-Fabricated Models for Education in Molecular Biology</data>
      <data key="e_abstract">This is a research project in the design and implementation of novel Human Computer Interfaces for educational use in structural molecular biology. This project will explore, define, and assess the role of computer-generated physical models in teaching scientific content and concepts. The purpose is to enhance the understanding and communication of the complex world of life&apos;s molecular machinery to a broad community. Using structural data on biological molecules and their complexes, the participants will prototype novel physical models via automated design and fabrication technologies and develop replication processes for broader distribution. The models will be used directly for enhance visualization and as input/output devices that interactively integrate with commutation and computer graphics for information retrieval, manipulation, and simulation.&lt;br/&gt;&lt;br/&gt;This project will provide new, tangible modes of interacting with, and understanding of, both the fundamental concepts and the complex data that are coming from the rapid advances in genomics, proteomics, and other areas of structural molecular biology. It will enable a broad range of students to learn, query, and explore in a field of growing scientific and social importance - the molecular basis of life. Physical models will give both haptic and visual support of the importance of shape in biological function. Use of physical models as tangible computer interfaces will revolutionize how students explore and understand biomolecular structure, interaction, and function.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121282</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121282</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1678" target="n1680">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY+PE+IM+AP Collaborative Research: Computer-Linked Auto-Fabricated Models for Education in Molecular Biology</data>
      <data key="e_abstract">This is a research project in the design and implementation of novel Human Computer Interfaces for educational use in structural molecular biology. This project will explore, define, and assess the role of computer-generated physical models in teaching scientific content and concepts. The purpose is to enhance the understanding and communication of the complex world of life&apos;s molecular machinery to a broad community. Using structural data on biological molecules and their complexes, the participants will prototype novel physical models via automated design and fabrication technologies and develop replication processes for broader distribution. The models will be used directly for enhance visualization and as input/output devices that interactively integrate with commutation and computer graphics for information retrieval, manipulation, and simulation.&lt;br/&gt;&lt;br/&gt;This project will provide new, tangible modes of interacting with, and understanding of, both the fundamental concepts and the complex data that are coming from the rapid advances in genomics, proteomics, and other areas of structural molecular biology. It will enable a broad range of students to learn, query, and explore in a field of growing scientific and social importance - the molecular basis of life. Physical models will give both haptic and visual support of the importance of shape in biological function. Use of physical models as tangible computer interfaces will revolutionize how students explore and understand biomolecular structure, interaction, and function.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121282</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121282</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1679" target="n1680">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY+PE+IM+AP Collaborative Research: Computer-Linked Auto-Fabricated Models for Education in Molecular Biology</data>
      <data key="e_abstract">This is a research project in the design and implementation of novel Human Computer Interfaces for educational use in structural molecular biology. This project will explore, define, and assess the role of computer-generated physical models in teaching scientific content and concepts. The purpose is to enhance the understanding and communication of the complex world of life&apos;s molecular machinery to a broad community. Using structural data on biological molecules and their complexes, the participants will prototype novel physical models via automated design and fabrication technologies and develop replication processes for broader distribution. The models will be used directly for enhance visualization and as input/output devices that interactively integrate with commutation and computer graphics for information retrieval, manipulation, and simulation.&lt;br/&gt;&lt;br/&gt;This project will provide new, tangible modes of interacting with, and understanding of, both the fundamental concepts and the complex data that are coming from the rapid advances in genomics, proteomics, and other areas of structural molecular biology. It will enable a broad range of students to learn, query, and explore in a field of growing scientific and social importance - the molecular basis of life. Physical models will give both haptic and visual support of the importance of shape in biological function. Use of physical models as tangible computer interfaces will revolutionize how students explore and understand biomolecular structure, interaction, and function.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121282</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121282</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1681" target="n1682">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Collaborative Research: ITR/AP&amp;IM Data Intense Challenge: The Instrumented Oil Field of the Future</data>
      <data key="e_abstract">Collaborative Research: ITR/AP&amp;IM Data Intense Challenge: &lt;br/&gt;The Instrumented Oil Field of the Future&lt;br/&gt;&lt;br/&gt;Mary Wheeler - University of Texas at Austin - 0121523&lt;br/&gt;Alan Sussman - University of Maryland, College Park - 0121161&lt;br/&gt;Joel Saltz - Ohio State University Research Foundation - 0121177&lt;br/&gt;Manish Parashar - Rutgers University - 0120934&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Increasing production from existing oil and natural gas reservoirs is crucial for the US economy. In order to better monitor and optimize oil and gas production, advanced technologies from field instrumentation to information technology and computational science are essential. Field technologies include time-lapse surface and borehole seismic, permanent downhole sensors, intelligent well completions, fiber optics, and remote control operations. IT technologies include data management, data visualization, parallel computing, and decision-making tools such as new wave propagation and multiphase, multi-component flow and transport computational portals. These diverse technologies can be integrated to achieve real-time monitoring and optimization of reservoir production: The Instrumented Oilfield.&lt;br/&gt;&lt;br/&gt;A major outcome of the proposed research is a computing portal which will enable reservoir simulation and geophysical calculations to interact dynamically with the data and with each other and which will provide a variety of visual and quantitative tools. Test data will be provided by oil and service companies currently participating in UT Austin industrial affiliate programs. Since the proposed research is directed towards the general problem of modeling and characterization of the earth&apos;s subsurface, it has immediate application to other areas, including environmental remediation and storage of hazardous wastes.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121523</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121523</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1681" target="n1683">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Collaborative Research: ITR/AP&amp;IM Data Intense Challenge: The Instrumented Oil Field of the Future</data>
      <data key="e_abstract">Collaborative Research: ITR/AP&amp;IM Data Intense Challenge: &lt;br/&gt;The Instrumented Oil Field of the Future&lt;br/&gt;&lt;br/&gt;Mary Wheeler - University of Texas at Austin - 0121523&lt;br/&gt;Alan Sussman - University of Maryland, College Park - 0121161&lt;br/&gt;Joel Saltz - Ohio State University Research Foundation - 0121177&lt;br/&gt;Manish Parashar - Rutgers University - 0120934&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Increasing production from existing oil and natural gas reservoirs is crucial for the US economy. In order to better monitor and optimize oil and gas production, advanced technologies from field instrumentation to information technology and computational science are essential. Field technologies include time-lapse surface and borehole seismic, permanent downhole sensors, intelligent well completions, fiber optics, and remote control operations. IT technologies include data management, data visualization, parallel computing, and decision-making tools such as new wave propagation and multiphase, multi-component flow and transport computational portals. These diverse technologies can be integrated to achieve real-time monitoring and optimization of reservoir production: The Instrumented Oilfield.&lt;br/&gt;&lt;br/&gt;A major outcome of the proposed research is a computing portal which will enable reservoir simulation and geophysical calculations to interact dynamically with the data and with each other and which will provide a variety of visual and quantitative tools. Test data will be provided by oil and service companies currently participating in UT Austin industrial affiliate programs. Since the proposed research is directed towards the general problem of modeling and characterization of the earth&apos;s subsurface, it has immediate application to other areas, including environmental remediation and storage of hazardous wastes.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121523</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121523</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1681" target="n1684">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Collaborative Research: ITR/AP&amp;IM Data Intense Challenge: The Instrumented Oil Field of the Future</data>
      <data key="e_abstract">Collaborative Research: ITR/AP&amp;IM Data Intense Challenge: &lt;br/&gt;The Instrumented Oil Field of the Future&lt;br/&gt;&lt;br/&gt;Mary Wheeler - University of Texas at Austin - 0121523&lt;br/&gt;Alan Sussman - University of Maryland, College Park - 0121161&lt;br/&gt;Joel Saltz - Ohio State University Research Foundation - 0121177&lt;br/&gt;Manish Parashar - Rutgers University - 0120934&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Increasing production from existing oil and natural gas reservoirs is crucial for the US economy. In order to better monitor and optimize oil and gas production, advanced technologies from field instrumentation to information technology and computational science are essential. Field technologies include time-lapse surface and borehole seismic, permanent downhole sensors, intelligent well completions, fiber optics, and remote control operations. IT technologies include data management, data visualization, parallel computing, and decision-making tools such as new wave propagation and multiphase, multi-component flow and transport computational portals. These diverse technologies can be integrated to achieve real-time monitoring and optimization of reservoir production: The Instrumented Oilfield.&lt;br/&gt;&lt;br/&gt;A major outcome of the proposed research is a computing portal which will enable reservoir simulation and geophysical calculations to interact dynamically with the data and with each other and which will provide a variety of visual and quantitative tools. Test data will be provided by oil and service companies currently participating in UT Austin industrial affiliate programs. Since the proposed research is directed towards the general problem of modeling and characterization of the earth&apos;s subsurface, it has immediate application to other areas, including environmental remediation and storage of hazardous wastes.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121523</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121523</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1682" target="n1683">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Collaborative Research: ITR/AP&amp;IM Data Intense Challenge: The Instrumented Oil Field of the Future</data>
      <data key="e_abstract">Collaborative Research: ITR/AP&amp;IM Data Intense Challenge: &lt;br/&gt;The Instrumented Oil Field of the Future&lt;br/&gt;&lt;br/&gt;Mary Wheeler - University of Texas at Austin - 0121523&lt;br/&gt;Alan Sussman - University of Maryland, College Park - 0121161&lt;br/&gt;Joel Saltz - Ohio State University Research Foundation - 0121177&lt;br/&gt;Manish Parashar - Rutgers University - 0120934&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Increasing production from existing oil and natural gas reservoirs is crucial for the US economy. In order to better monitor and optimize oil and gas production, advanced technologies from field instrumentation to information technology and computational science are essential. Field technologies include time-lapse surface and borehole seismic, permanent downhole sensors, intelligent well completions, fiber optics, and remote control operations. IT technologies include data management, data visualization, parallel computing, and decision-making tools such as new wave propagation and multiphase, multi-component flow and transport computational portals. These diverse technologies can be integrated to achieve real-time monitoring and optimization of reservoir production: The Instrumented Oilfield.&lt;br/&gt;&lt;br/&gt;A major outcome of the proposed research is a computing portal which will enable reservoir simulation and geophysical calculations to interact dynamically with the data and with each other and which will provide a variety of visual and quantitative tools. Test data will be provided by oil and service companies currently participating in UT Austin industrial affiliate programs. Since the proposed research is directed towards the general problem of modeling and characterization of the earth&apos;s subsurface, it has immediate application to other areas, including environmental remediation and storage of hazardous wastes.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121523</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121523</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1682" target="n1684">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Collaborative Research: ITR/AP&amp;IM Data Intense Challenge: The Instrumented Oil Field of the Future</data>
      <data key="e_abstract">Collaborative Research: ITR/AP&amp;IM Data Intense Challenge: &lt;br/&gt;The Instrumented Oil Field of the Future&lt;br/&gt;&lt;br/&gt;Mary Wheeler - University of Texas at Austin - 0121523&lt;br/&gt;Alan Sussman - University of Maryland, College Park - 0121161&lt;br/&gt;Joel Saltz - Ohio State University Research Foundation - 0121177&lt;br/&gt;Manish Parashar - Rutgers University - 0120934&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Increasing production from existing oil and natural gas reservoirs is crucial for the US economy. In order to better monitor and optimize oil and gas production, advanced technologies from field instrumentation to information technology and computational science are essential. Field technologies include time-lapse surface and borehole seismic, permanent downhole sensors, intelligent well completions, fiber optics, and remote control operations. IT technologies include data management, data visualization, parallel computing, and decision-making tools such as new wave propagation and multiphase, multi-component flow and transport computational portals. These diverse technologies can be integrated to achieve real-time monitoring and optimization of reservoir production: The Instrumented Oilfield.&lt;br/&gt;&lt;br/&gt;A major outcome of the proposed research is a computing portal which will enable reservoir simulation and geophysical calculations to interact dynamically with the data and with each other and which will provide a variety of visual and quantitative tools. Test data will be provided by oil and service companies currently participating in UT Austin industrial affiliate programs. Since the proposed research is directed towards the general problem of modeling and characterization of the earth&apos;s subsurface, it has immediate application to other areas, including environmental remediation and storage of hazardous wastes.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121523</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121523</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1683" target="n1684">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Collaborative Research: ITR/AP&amp;IM Data Intense Challenge: The Instrumented Oil Field of the Future</data>
      <data key="e_abstract">Collaborative Research: ITR/AP&amp;IM Data Intense Challenge: &lt;br/&gt;The Instrumented Oil Field of the Future&lt;br/&gt;&lt;br/&gt;Mary Wheeler - University of Texas at Austin - 0121523&lt;br/&gt;Alan Sussman - University of Maryland, College Park - 0121161&lt;br/&gt;Joel Saltz - Ohio State University Research Foundation - 0121177&lt;br/&gt;Manish Parashar - Rutgers University - 0120934&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Increasing production from existing oil and natural gas reservoirs is crucial for the US economy. In order to better monitor and optimize oil and gas production, advanced technologies from field instrumentation to information technology and computational science are essential. Field technologies include time-lapse surface and borehole seismic, permanent downhole sensors, intelligent well completions, fiber optics, and remote control operations. IT technologies include data management, data visualization, parallel computing, and decision-making tools such as new wave propagation and multiphase, multi-component flow and transport computational portals. These diverse technologies can be integrated to achieve real-time monitoring and optimization of reservoir production: The Instrumented Oilfield.&lt;br/&gt;&lt;br/&gt;A major outcome of the proposed research is a computing portal which will enable reservoir simulation and geophysical calculations to interact dynamically with the data and with each other and which will provide a variety of visual and quantitative tools. Test data will be provided by oil and service companies currently participating in UT Austin industrial affiliate programs. Since the proposed research is directed towards the general problem of modeling and characterization of the earth&apos;s subsurface, it has immediate application to other areas, including environmental remediation and storage of hazardous wastes.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121523</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121523</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1686" target="n1687">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/AP COLLABORATIVE RESEARCH: Real Time Optimization for Data Assimilation and Control of Large Scale Dynamic Simulations</data>
      <data key="e_abstract">This project will create and apply algorithms and software tools for on-line simulations that continuously (1) assimilate sensor data from dynamic physical processes, and (2) generate optimal strategies for their control. A number of critical industrial, scientific, and societal problems stand to benefit from this research such as aerodynamics, energy, geophysics, infrastructure, manufacturing, medicine, chemical process and environmental applications; two of these will be the focus of the current research. In these and many other cases, the underlying models have become capable of sufficient fidelity to yield meaningful predictions, provided unknown parameters (typically initial/boundary conditions, material coefficients, sources, or geometry) can be estimated appropriately using observational data.&lt;br/&gt;&lt;br/&gt;The critical step is the solution of a large-scale nonlinear optimization problem that is constrained by the simulation equations, typically PDEs or their reduced order models. A data assimilation phase will seek to minimize the mismatch between sensor data and model-based predictions by adjusting unknown parameters of the PDE simulation, and the optimal control phase will find an optimal control strategy based on the updated model.&lt;br/&gt;&lt;br/&gt;Despite advances in hardware, networks, parallel PDE solvers, large-scale optimization algorithms, and real-time ODE optimization, significant algorithmic and software challenges must be overcome before the ultimate goal of real-time PDE data assimilation and optimal control can be realized. Needed are fundamentally new PDE optimization algorithms that must: (1) run sufficiently quickly to permit decision-making at time scales of interest; (2) scale to the large numbers of variables and constraints that characterize PDE optimization and processors that characterize high-end systems; (3) adjust to different solution accuracy requirements; (4) target time-dependent objectives and constraints; (5) tolerate incomplete, uncertain, or errant data; (6) be capable of bootstrapping current solutions; (7) yield meaningful results when terminated prematurely; and (8) be robust in the face of ill-posedness.&lt;br/&gt;&lt;br/&gt;To create, apply, and disseminate the enabling technologies for real-time PDE data assimilation and optimal control, the project will: (1) Develop algorithms and tools for real-time data assimilation and optimal control that meet the above specifications for a class of important applications. (2) Implement and publicly distribute these algorithms within an object-oriented framework that incorporates problem structure, interfaces easily with high performance PDE solver libraries fosters applicability of our tools to a broad range of real-time data assimilation and optimal control problems, and enables extension of the algorithms without interfering with applications. (3) Apply these algorithms and tools to two critical environmental and industrial problems: modeling and control of chemical vapor deposition (CVD) reactors and of wildland firespread. (4) Interact and work with other user communities to ensure that the algorithms and software we produce are useful across a broad range of applications.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121667</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121667</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1690" target="n1691">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">UMEB: Fostering Student Confidence and Capability in Environmental Science</data>
      <data key="e_abstract">0102495&lt;br/&gt;Robles&lt;br/&gt;This project would establish a structured undergraduate mentoring program in environmental biology at a minority serving university - California State University, Los Angeles (CSLA). It would strengthen collaboration among faculty, their supporting research institution, and a network of off-campus partners devoted to environmental science education. A unifying science theme, &quot;acquisition and use of spatial information in field studies,&quot; provides a focus for curricular and research activities. Student programs may include special courses in Geographic Information Systems, field instrumentation, computer modeling, molecular genetic population differentiation, as well as more traditional environmental science offerings. Students would gain research experience in interdisciplinary teams of mentors, graduate students, and collaborating scientists. Students will rotate and adopt one of three teams: (1) Molecular Genetics of Populations (2) Perturbations of the Terrestrial Carbon Cycle, and (3) Models of Marine Benthic Populations. Program activities are staged over two to three years in phases: (I) Orientation - introduction to the environmental science community and encouragement in basic scholarship, (II) In-Depth Training - emphasis on advanced research skills, and (III) Culmination - presentation of scientific findings and preparation for graduate school. Additional activities include internships with agency partners, intensive &quot;field immersion experiences,&quot; and mentoring by accomplished role models. An ethics component will feature seminars and discussions with local and national ethicists. &lt;br/&gt;&lt;br/&gt;This mentoring program is based on the fact that students&apos; motivation is crucial to their success; therefore, all activities seek to build student confidence and capability. Motivation will be heightened by imparting awareness of scientists&apos; social responsibilities and by fostering an appreciation for the contribution students from underrepresented groups can make. By promoting on and off campus educational partnerships, the project will positively affect the future of environmental science education at CSLA, and benefit students beyond the cohort of exemplary UMEB undergraduate fellows.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">102495</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">102495</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1694" target="n1695">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY+SI:The System Architecture of a Computing Utility</data>
      <data key="e_abstract">Computing services in the future should become just as available and easy to use as any of the modern&lt;br/&gt;utilities: power, water, or telephone service. A compute utility should work reliably and invisibly,&lt;br/&gt;providing users with simple, efficient access to their data and individualized computing environments&lt;br/&gt;anywhere in the world. Data kept in the utility may be private, publicly available, or shared between&lt;br/&gt;designated parties.&lt;br/&gt; Our proposed compute utility architecture is based on the idea of a compute capsule, which captures&lt;br/&gt;the entire logical state of an active computing environment. A capsule is portable, persistent,&lt;br/&gt;host-independent, self-contained and can be suspended on disk, arbitrarily bound to different machines,&lt;br/&gt;and transparently resumed. Compute servers in a utility run a small trusted computing base, which&lt;br/&gt;manages the compute capsules like a cache. Users&apos; capsules can be run on any of the compute servers in&lt;br/&gt;the utility and are typically run on local machines for enhanced interactive performance. Capsules provide&lt;br/&gt;users with customizabililty as well as a guarantee of isolation from other users. As persistent objects that&lt;br/&gt;can be shared, duplicated and version-controlled, capsules can be easily managed and administered.&lt;br/&gt; The compute utility model has significant advantages over our current environment. First, instead of&lt;br/&gt;requiring end users to procure, administer and upgrade their equipment individually, resources of a utility&lt;br/&gt;are shared and managed centrally by experts, thus resulting in a more efficient system. Second, a global&lt;br/&gt;utility allows users efficient access to their computing environment everywhere. They need not juggle a&lt;br/&gt;large number of different interfaces and deal with the discontinuities as they move between home and work&lt;br/&gt;every day. Third, a professionally managed compute utility designed to support global mobility and&lt;br/&gt;sharing can provide greater security than our current environment. In contrast, today novice users are&lt;br/&gt;responsible for keeping their systems secure, and the lack of adequate support for sharing and mobility&lt;br/&gt;leads to practices that jeopardize security. Fourth, the infrastructure of a utility serves as an excellent&lt;br/&gt;platform for several emerging computing trends: software hosted remotely by application service&lt;br/&gt;providers, support of ubiquitous access devices, and large-scale distributed computing.&lt;br/&gt; The goal of this research project is to investigate the viability of a compute utility and to lay the&lt;br/&gt;technical foundation for such an infrastructure. The techniques developed can also be applied to manage an&lt;br/&gt;institution&apos;s distributed resources.&lt;br/&gt; First, we will investigate the concept of capsules fully by studying their design and applications.&lt;br/&gt;Capsules can be implemented at the machine, operating system and application level; trade-offs between&lt;br/&gt;these different approaches will be studied. We will also explore novel uses of capsules: capsules can be&lt;br/&gt;used to run untrusted software; they can provide secure and shared environments for group projects;&lt;br/&gt;capsules can also serve as pre-installed software packages ready to run on any machine. We will develop&lt;br/&gt;the associated tools to make capsuleseasy to use.&lt;br/&gt; Second, we will develop the technologies to ensure security in a compute utility. Security will be&lt;br/&gt;provided at three levels: a small trusted kernel that provides isolation between capsules and monitors&lt;br/&gt;capsules for added security; the use of certified capsules to create trusted computing environments; data&lt;br/&gt;security will be provided by encryption, and sharing will be supported by a flexible key management&lt;br/&gt;scheme. We plan to conduct a careful analysis of the vulnerabilities of the system.&lt;br/&gt; Third, this research will produce the technologies useful for creating scalable, efficient, and easily&lt;br/&gt;maintainable compute utilities. They include techniques for managing machines in a globally distributed&lt;br/&gt;environment by treating them as caches of capsules, and techniques that combine capsule caching and&lt;br/&gt;remote display technology to support global mobility and sharing.&lt;br/&gt; This research will develop a prototype compute utility to validate the technology developed. To make&lt;br/&gt;extensive experimentation possible, the prototype will support legacy software. The experiments will be&lt;br/&gt;designed especially to address the information technology needs of universities.&lt;br/&gt; Computer systems have gone through two major eras, time-sharing on mainframes and personal&lt;br/&gt;computing. The success of this research may usher in a new era of compute utility and have a significant&lt;br/&gt;impact on our future computing practice.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121481</data>
      <data key="e_expirationDate">2009-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121481</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1694" target="n1696">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY+SI:The System Architecture of a Computing Utility</data>
      <data key="e_abstract">Computing services in the future should become just as available and easy to use as any of the modern&lt;br/&gt;utilities: power, water, or telephone service. A compute utility should work reliably and invisibly,&lt;br/&gt;providing users with simple, efficient access to their data and individualized computing environments&lt;br/&gt;anywhere in the world. Data kept in the utility may be private, publicly available, or shared between&lt;br/&gt;designated parties.&lt;br/&gt; Our proposed compute utility architecture is based on the idea of a compute capsule, which captures&lt;br/&gt;the entire logical state of an active computing environment. A capsule is portable, persistent,&lt;br/&gt;host-independent, self-contained and can be suspended on disk, arbitrarily bound to different machines,&lt;br/&gt;and transparently resumed. Compute servers in a utility run a small trusted computing base, which&lt;br/&gt;manages the compute capsules like a cache. Users&apos; capsules can be run on any of the compute servers in&lt;br/&gt;the utility and are typically run on local machines for enhanced interactive performance. Capsules provide&lt;br/&gt;users with customizabililty as well as a guarantee of isolation from other users. As persistent objects that&lt;br/&gt;can be shared, duplicated and version-controlled, capsules can be easily managed and administered.&lt;br/&gt; The compute utility model has significant advantages over our current environment. First, instead of&lt;br/&gt;requiring end users to procure, administer and upgrade their equipment individually, resources of a utility&lt;br/&gt;are shared and managed centrally by experts, thus resulting in a more efficient system. Second, a global&lt;br/&gt;utility allows users efficient access to their computing environment everywhere. They need not juggle a&lt;br/&gt;large number of different interfaces and deal with the discontinuities as they move between home and work&lt;br/&gt;every day. Third, a professionally managed compute utility designed to support global mobility and&lt;br/&gt;sharing can provide greater security than our current environment. In contrast, today novice users are&lt;br/&gt;responsible for keeping their systems secure, and the lack of adequate support for sharing and mobility&lt;br/&gt;leads to practices that jeopardize security. Fourth, the infrastructure of a utility serves as an excellent&lt;br/&gt;platform for several emerging computing trends: software hosted remotely by application service&lt;br/&gt;providers, support of ubiquitous access devices, and large-scale distributed computing.&lt;br/&gt; The goal of this research project is to investigate the viability of a compute utility and to lay the&lt;br/&gt;technical foundation for such an infrastructure. The techniques developed can also be applied to manage an&lt;br/&gt;institution&apos;s distributed resources.&lt;br/&gt; First, we will investigate the concept of capsules fully by studying their design and applications.&lt;br/&gt;Capsules can be implemented at the machine, operating system and application level; trade-offs between&lt;br/&gt;these different approaches will be studied. We will also explore novel uses of capsules: capsules can be&lt;br/&gt;used to run untrusted software; they can provide secure and shared environments for group projects;&lt;br/&gt;capsules can also serve as pre-installed software packages ready to run on any machine. We will develop&lt;br/&gt;the associated tools to make capsuleseasy to use.&lt;br/&gt; Second, we will develop the technologies to ensure security in a compute utility. Security will be&lt;br/&gt;provided at three levels: a small trusted kernel that provides isolation between capsules and monitors&lt;br/&gt;capsules for added security; the use of certified capsules to create trusted computing environments; data&lt;br/&gt;security will be provided by encryption, and sharing will be supported by a flexible key management&lt;br/&gt;scheme. We plan to conduct a careful analysis of the vulnerabilities of the system.&lt;br/&gt; Third, this research will produce the technologies useful for creating scalable, efficient, and easily&lt;br/&gt;maintainable compute utilities. They include techniques for managing machines in a globally distributed&lt;br/&gt;environment by treating them as caches of capsules, and techniques that combine capsule caching and&lt;br/&gt;remote display technology to support global mobility and sharing.&lt;br/&gt; This research will develop a prototype compute utility to validate the technology developed. To make&lt;br/&gt;extensive experimentation possible, the prototype will support legacy software. The experiments will be&lt;br/&gt;designed especially to address the information technology needs of universities.&lt;br/&gt; Computer systems have gone through two major eras, time-sharing on mainframes and personal&lt;br/&gt;computing. The success of this research may usher in a new era of compute utility and have a significant&lt;br/&gt;impact on our future computing practice.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121481</data>
      <data key="e_expirationDate">2009-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121481</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1695" target="n1696">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY+SI:The System Architecture of a Computing Utility</data>
      <data key="e_abstract">Computing services in the future should become just as available and easy to use as any of the modern&lt;br/&gt;utilities: power, water, or telephone service. A compute utility should work reliably and invisibly,&lt;br/&gt;providing users with simple, efficient access to their data and individualized computing environments&lt;br/&gt;anywhere in the world. Data kept in the utility may be private, publicly available, or shared between&lt;br/&gt;designated parties.&lt;br/&gt; Our proposed compute utility architecture is based on the idea of a compute capsule, which captures&lt;br/&gt;the entire logical state of an active computing environment. A capsule is portable, persistent,&lt;br/&gt;host-independent, self-contained and can be suspended on disk, arbitrarily bound to different machines,&lt;br/&gt;and transparently resumed. Compute servers in a utility run a small trusted computing base, which&lt;br/&gt;manages the compute capsules like a cache. Users&apos; capsules can be run on any of the compute servers in&lt;br/&gt;the utility and are typically run on local machines for enhanced interactive performance. Capsules provide&lt;br/&gt;users with customizabililty as well as a guarantee of isolation from other users. As persistent objects that&lt;br/&gt;can be shared, duplicated and version-controlled, capsules can be easily managed and administered.&lt;br/&gt; The compute utility model has significant advantages over our current environment. First, instead of&lt;br/&gt;requiring end users to procure, administer and upgrade their equipment individually, resources of a utility&lt;br/&gt;are shared and managed centrally by experts, thus resulting in a more efficient system. Second, a global&lt;br/&gt;utility allows users efficient access to their computing environment everywhere. They need not juggle a&lt;br/&gt;large number of different interfaces and deal with the discontinuities as they move between home and work&lt;br/&gt;every day. Third, a professionally managed compute utility designed to support global mobility and&lt;br/&gt;sharing can provide greater security than our current environment. In contrast, today novice users are&lt;br/&gt;responsible for keeping their systems secure, and the lack of adequate support for sharing and mobility&lt;br/&gt;leads to practices that jeopardize security. Fourth, the infrastructure of a utility serves as an excellent&lt;br/&gt;platform for several emerging computing trends: software hosted remotely by application service&lt;br/&gt;providers, support of ubiquitous access devices, and large-scale distributed computing.&lt;br/&gt; The goal of this research project is to investigate the viability of a compute utility and to lay the&lt;br/&gt;technical foundation for such an infrastructure. The techniques developed can also be applied to manage an&lt;br/&gt;institution&apos;s distributed resources.&lt;br/&gt; First, we will investigate the concept of capsules fully by studying their design and applications.&lt;br/&gt;Capsules can be implemented at the machine, operating system and application level; trade-offs between&lt;br/&gt;these different approaches will be studied. We will also explore novel uses of capsules: capsules can be&lt;br/&gt;used to run untrusted software; they can provide secure and shared environments for group projects;&lt;br/&gt;capsules can also serve as pre-installed software packages ready to run on any machine. We will develop&lt;br/&gt;the associated tools to make capsuleseasy to use.&lt;br/&gt; Second, we will develop the technologies to ensure security in a compute utility. Security will be&lt;br/&gt;provided at three levels: a small trusted kernel that provides isolation between capsules and monitors&lt;br/&gt;capsules for added security; the use of certified capsules to create trusted computing environments; data&lt;br/&gt;security will be provided by encryption, and sharing will be supported by a flexible key management&lt;br/&gt;scheme. We plan to conduct a careful analysis of the vulnerabilities of the system.&lt;br/&gt; Third, this research will produce the technologies useful for creating scalable, efficient, and easily&lt;br/&gt;maintainable compute utilities. They include techniques for managing machines in a globally distributed&lt;br/&gt;environment by treating them as caches of capsules, and techniques that combine capsule caching and&lt;br/&gt;remote display technology to support global mobility and sharing.&lt;br/&gt; This research will develop a prototype compute utility to validate the technology developed. To make&lt;br/&gt;extensive experimentation possible, the prototype will support legacy software. The experiments will be&lt;br/&gt;designed especially to address the information technology needs of universities.&lt;br/&gt; Computer systems have gone through two major eras, time-sharing on mainframes and personal&lt;br/&gt;computing. The success of this research may usher in a new era of compute utility and have a significant&lt;br/&gt;impact on our future computing practice.</data>
      <data key="e_pgm">1687</data>
      <data key="e_label">121481</data>
      <data key="e_expirationDate">2009-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">121481</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n850" target="n1699">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics: Event and Process Tagging for Information Integration for the International Gulf of Main Watershed</data>
      <data key="e_abstract">EIA-0131912&lt;br/&gt;Beard-Tisdale, Mary-Kate&lt;br/&gt;University of Maine&lt;br/&gt;&lt;br/&gt;BDEI: Event and process tagging for information integration for the International Gulf of Maine Watershed&lt;br/&gt;&lt;br/&gt;Project Summary&lt;br/&gt;&lt;br/&gt;This incubation proposal addresses the issue of integrating large, diverse, and autonomous&lt;br/&gt;collections of scientific data within a complex institutional setting. The goal is to convert these&lt;br/&gt;autonomous collections into a shareable repository that supports synthesis of data through new&lt;br/&gt;metadata structures based on events and processes. The institutional setting is the data and data-&lt;br/&gt;gathering activities of over 80 agencies, NGOS, and academic and research institutions operating&lt;br/&gt;within the Gulf of Maine watershed. The metadata development will be coordinated by library and&lt;br/&gt;spatial information scientists working jointly with domain scientists. An essential task of this&lt;br/&gt;incubation effort will be the development of a shared understanding of environmental processes and&lt;br/&gt;events that becomes a shareable ontology.&lt;br/&gt;&lt;br/&gt;Libraries play a vital role in organizing intellectual access to creative works. Scientific data has&lt;br/&gt;tended to be outside this traditional purview and has thus lacked the benefits of cataloging and&lt;br/&gt;indexing that promote shared access. We are proposing a new metadata structure that exploits&lt;br/&gt;common units of analysis in environmental studies: events and processes. Associating scientific data&lt;br/&gt;sets with event and process tags, in addition to other metadata elements, can substantially improve&lt;br/&gt;the ability to integrate and synthesize diverse scientific collections. The metadata initiative of this&lt;br/&gt;proposal will lay the foundation for specifying events and processes through the collaboration of&lt;br/&gt;data collections content specialists and information management specialists.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">131912</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">131912</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n850" target="n1700">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics: Event and Process Tagging for Information Integration for the International Gulf of Main Watershed</data>
      <data key="e_abstract">EIA-0131912&lt;br/&gt;Beard-Tisdale, Mary-Kate&lt;br/&gt;University of Maine&lt;br/&gt;&lt;br/&gt;BDEI: Event and process tagging for information integration for the International Gulf of Maine Watershed&lt;br/&gt;&lt;br/&gt;Project Summary&lt;br/&gt;&lt;br/&gt;This incubation proposal addresses the issue of integrating large, diverse, and autonomous&lt;br/&gt;collections of scientific data within a complex institutional setting. The goal is to convert these&lt;br/&gt;autonomous collections into a shareable repository that supports synthesis of data through new&lt;br/&gt;metadata structures based on events and processes. The institutional setting is the data and data-&lt;br/&gt;gathering activities of over 80 agencies, NGOS, and academic and research institutions operating&lt;br/&gt;within the Gulf of Maine watershed. The metadata development will be coordinated by library and&lt;br/&gt;spatial information scientists working jointly with domain scientists. An essential task of this&lt;br/&gt;incubation effort will be the development of a shared understanding of environmental processes and&lt;br/&gt;events that becomes a shareable ontology.&lt;br/&gt;&lt;br/&gt;Libraries play a vital role in organizing intellectual access to creative works. Scientific data has&lt;br/&gt;tended to be outside this traditional purview and has thus lacked the benefits of cataloging and&lt;br/&gt;indexing that promote shared access. We are proposing a new metadata structure that exploits&lt;br/&gt;common units of analysis in environmental studies: events and processes. Associating scientific data&lt;br/&gt;sets with event and process tags, in addition to other metadata elements, can substantially improve&lt;br/&gt;the ability to integrate and synthesize diverse scientific collections. The metadata initiative of this&lt;br/&gt;proposal will lay the foundation for specifying events and processes through the collaboration of&lt;br/&gt;data collections content specialists and information management specialists.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">131912</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">131912</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n850" target="n1701">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics: Event and Process Tagging for Information Integration for the International Gulf of Main Watershed</data>
      <data key="e_abstract">EIA-0131912&lt;br/&gt;Beard-Tisdale, Mary-Kate&lt;br/&gt;University of Maine&lt;br/&gt;&lt;br/&gt;BDEI: Event and process tagging for information integration for the International Gulf of Maine Watershed&lt;br/&gt;&lt;br/&gt;Project Summary&lt;br/&gt;&lt;br/&gt;This incubation proposal addresses the issue of integrating large, diverse, and autonomous&lt;br/&gt;collections of scientific data within a complex institutional setting. The goal is to convert these&lt;br/&gt;autonomous collections into a shareable repository that supports synthesis of data through new&lt;br/&gt;metadata structures based on events and processes. The institutional setting is the data and data-&lt;br/&gt;gathering activities of over 80 agencies, NGOS, and academic and research institutions operating&lt;br/&gt;within the Gulf of Maine watershed. The metadata development will be coordinated by library and&lt;br/&gt;spatial information scientists working jointly with domain scientists. An essential task of this&lt;br/&gt;incubation effort will be the development of a shared understanding of environmental processes and&lt;br/&gt;events that becomes a shareable ontology.&lt;br/&gt;&lt;br/&gt;Libraries play a vital role in organizing intellectual access to creative works. Scientific data has&lt;br/&gt;tended to be outside this traditional purview and has thus lacked the benefits of cataloging and&lt;br/&gt;indexing that promote shared access. We are proposing a new metadata structure that exploits&lt;br/&gt;common units of analysis in environmental studies: events and processes. Associating scientific data&lt;br/&gt;sets with event and process tags, in addition to other metadata elements, can substantially improve&lt;br/&gt;the ability to integrate and synthesize diverse scientific collections. The metadata initiative of this&lt;br/&gt;proposal will lay the foundation for specifying events and processes through the collaboration of&lt;br/&gt;data collections content specialists and information management specialists.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">131912</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">131912</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n850" target="n1702">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics: Event and Process Tagging for Information Integration for the International Gulf of Main Watershed</data>
      <data key="e_abstract">EIA-0131912&lt;br/&gt;Beard-Tisdale, Mary-Kate&lt;br/&gt;University of Maine&lt;br/&gt;&lt;br/&gt;BDEI: Event and process tagging for information integration for the International Gulf of Maine Watershed&lt;br/&gt;&lt;br/&gt;Project Summary&lt;br/&gt;&lt;br/&gt;This incubation proposal addresses the issue of integrating large, diverse, and autonomous&lt;br/&gt;collections of scientific data within a complex institutional setting. The goal is to convert these&lt;br/&gt;autonomous collections into a shareable repository that supports synthesis of data through new&lt;br/&gt;metadata structures based on events and processes. The institutional setting is the data and data-&lt;br/&gt;gathering activities of over 80 agencies, NGOS, and academic and research institutions operating&lt;br/&gt;within the Gulf of Maine watershed. The metadata development will be coordinated by library and&lt;br/&gt;spatial information scientists working jointly with domain scientists. An essential task of this&lt;br/&gt;incubation effort will be the development of a shared understanding of environmental processes and&lt;br/&gt;events that becomes a shareable ontology.&lt;br/&gt;&lt;br/&gt;Libraries play a vital role in organizing intellectual access to creative works. Scientific data has&lt;br/&gt;tended to be outside this traditional purview and has thus lacked the benefits of cataloging and&lt;br/&gt;indexing that promote shared access. We are proposing a new metadata structure that exploits&lt;br/&gt;common units of analysis in environmental studies: events and processes. Associating scientific data&lt;br/&gt;sets with event and process tags, in addition to other metadata elements, can substantially improve&lt;br/&gt;the ability to integrate and synthesize diverse scientific collections. The metadata initiative of this&lt;br/&gt;proposal will lay the foundation for specifying events and processes through the collaboration of&lt;br/&gt;data collections content specialists and information management specialists.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">131912</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">131912</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1699" target="n1700">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics: Event and Process Tagging for Information Integration for the International Gulf of Main Watershed</data>
      <data key="e_abstract">EIA-0131912&lt;br/&gt;Beard-Tisdale, Mary-Kate&lt;br/&gt;University of Maine&lt;br/&gt;&lt;br/&gt;BDEI: Event and process tagging for information integration for the International Gulf of Maine Watershed&lt;br/&gt;&lt;br/&gt;Project Summary&lt;br/&gt;&lt;br/&gt;This incubation proposal addresses the issue of integrating large, diverse, and autonomous&lt;br/&gt;collections of scientific data within a complex institutional setting. The goal is to convert these&lt;br/&gt;autonomous collections into a shareable repository that supports synthesis of data through new&lt;br/&gt;metadata structures based on events and processes. The institutional setting is the data and data-&lt;br/&gt;gathering activities of over 80 agencies, NGOS, and academic and research institutions operating&lt;br/&gt;within the Gulf of Maine watershed. The metadata development will be coordinated by library and&lt;br/&gt;spatial information scientists working jointly with domain scientists. An essential task of this&lt;br/&gt;incubation effort will be the development of a shared understanding of environmental processes and&lt;br/&gt;events that becomes a shareable ontology.&lt;br/&gt;&lt;br/&gt;Libraries play a vital role in organizing intellectual access to creative works. Scientific data has&lt;br/&gt;tended to be outside this traditional purview and has thus lacked the benefits of cataloging and&lt;br/&gt;indexing that promote shared access. We are proposing a new metadata structure that exploits&lt;br/&gt;common units of analysis in environmental studies: events and processes. Associating scientific data&lt;br/&gt;sets with event and process tags, in addition to other metadata elements, can substantially improve&lt;br/&gt;the ability to integrate and synthesize diverse scientific collections. The metadata initiative of this&lt;br/&gt;proposal will lay the foundation for specifying events and processes through the collaboration of&lt;br/&gt;data collections content specialists and information management specialists.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">131912</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">131912</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1699" target="n1701">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics: Event and Process Tagging for Information Integration for the International Gulf of Main Watershed</data>
      <data key="e_abstract">EIA-0131912&lt;br/&gt;Beard-Tisdale, Mary-Kate&lt;br/&gt;University of Maine&lt;br/&gt;&lt;br/&gt;BDEI: Event and process tagging for information integration for the International Gulf of Maine Watershed&lt;br/&gt;&lt;br/&gt;Project Summary&lt;br/&gt;&lt;br/&gt;This incubation proposal addresses the issue of integrating large, diverse, and autonomous&lt;br/&gt;collections of scientific data within a complex institutional setting. The goal is to convert these&lt;br/&gt;autonomous collections into a shareable repository that supports synthesis of data through new&lt;br/&gt;metadata structures based on events and processes. The institutional setting is the data and data-&lt;br/&gt;gathering activities of over 80 agencies, NGOS, and academic and research institutions operating&lt;br/&gt;within the Gulf of Maine watershed. The metadata development will be coordinated by library and&lt;br/&gt;spatial information scientists working jointly with domain scientists. An essential task of this&lt;br/&gt;incubation effort will be the development of a shared understanding of environmental processes and&lt;br/&gt;events that becomes a shareable ontology.&lt;br/&gt;&lt;br/&gt;Libraries play a vital role in organizing intellectual access to creative works. Scientific data has&lt;br/&gt;tended to be outside this traditional purview and has thus lacked the benefits of cataloging and&lt;br/&gt;indexing that promote shared access. We are proposing a new metadata structure that exploits&lt;br/&gt;common units of analysis in environmental studies: events and processes. Associating scientific data&lt;br/&gt;sets with event and process tags, in addition to other metadata elements, can substantially improve&lt;br/&gt;the ability to integrate and synthesize diverse scientific collections. The metadata initiative of this&lt;br/&gt;proposal will lay the foundation for specifying events and processes through the collaboration of&lt;br/&gt;data collections content specialists and information management specialists.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">131912</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">131912</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1699" target="n1702">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics: Event and Process Tagging for Information Integration for the International Gulf of Main Watershed</data>
      <data key="e_abstract">EIA-0131912&lt;br/&gt;Beard-Tisdale, Mary-Kate&lt;br/&gt;University of Maine&lt;br/&gt;&lt;br/&gt;BDEI: Event and process tagging for information integration for the International Gulf of Maine Watershed&lt;br/&gt;&lt;br/&gt;Project Summary&lt;br/&gt;&lt;br/&gt;This incubation proposal addresses the issue of integrating large, diverse, and autonomous&lt;br/&gt;collections of scientific data within a complex institutional setting. The goal is to convert these&lt;br/&gt;autonomous collections into a shareable repository that supports synthesis of data through new&lt;br/&gt;metadata structures based on events and processes. The institutional setting is the data and data-&lt;br/&gt;gathering activities of over 80 agencies, NGOS, and academic and research institutions operating&lt;br/&gt;within the Gulf of Maine watershed. The metadata development will be coordinated by library and&lt;br/&gt;spatial information scientists working jointly with domain scientists. An essential task of this&lt;br/&gt;incubation effort will be the development of a shared understanding of environmental processes and&lt;br/&gt;events that becomes a shareable ontology.&lt;br/&gt;&lt;br/&gt;Libraries play a vital role in organizing intellectual access to creative works. Scientific data has&lt;br/&gt;tended to be outside this traditional purview and has thus lacked the benefits of cataloging and&lt;br/&gt;indexing that promote shared access. We are proposing a new metadata structure that exploits&lt;br/&gt;common units of analysis in environmental studies: events and processes. Associating scientific data&lt;br/&gt;sets with event and process tags, in addition to other metadata elements, can substantially improve&lt;br/&gt;the ability to integrate and synthesize diverse scientific collections. The metadata initiative of this&lt;br/&gt;proposal will lay the foundation for specifying events and processes through the collaboration of&lt;br/&gt;data collections content specialists and information management specialists.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">131912</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">131912</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1700" target="n1701">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics: Event and Process Tagging for Information Integration for the International Gulf of Main Watershed</data>
      <data key="e_abstract">EIA-0131912&lt;br/&gt;Beard-Tisdale, Mary-Kate&lt;br/&gt;University of Maine&lt;br/&gt;&lt;br/&gt;BDEI: Event and process tagging for information integration for the International Gulf of Maine Watershed&lt;br/&gt;&lt;br/&gt;Project Summary&lt;br/&gt;&lt;br/&gt;This incubation proposal addresses the issue of integrating large, diverse, and autonomous&lt;br/&gt;collections of scientific data within a complex institutional setting. The goal is to convert these&lt;br/&gt;autonomous collections into a shareable repository that supports synthesis of data through new&lt;br/&gt;metadata structures based on events and processes. The institutional setting is the data and data-&lt;br/&gt;gathering activities of over 80 agencies, NGOS, and academic and research institutions operating&lt;br/&gt;within the Gulf of Maine watershed. The metadata development will be coordinated by library and&lt;br/&gt;spatial information scientists working jointly with domain scientists. An essential task of this&lt;br/&gt;incubation effort will be the development of a shared understanding of environmental processes and&lt;br/&gt;events that becomes a shareable ontology.&lt;br/&gt;&lt;br/&gt;Libraries play a vital role in organizing intellectual access to creative works. Scientific data has&lt;br/&gt;tended to be outside this traditional purview and has thus lacked the benefits of cataloging and&lt;br/&gt;indexing that promote shared access. We are proposing a new metadata structure that exploits&lt;br/&gt;common units of analysis in environmental studies: events and processes. Associating scientific data&lt;br/&gt;sets with event and process tags, in addition to other metadata elements, can substantially improve&lt;br/&gt;the ability to integrate and synthesize diverse scientific collections. The metadata initiative of this&lt;br/&gt;proposal will lay the foundation for specifying events and processes through the collaboration of&lt;br/&gt;data collections content specialists and information management specialists.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">131912</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">131912</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1700" target="n1702">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics: Event and Process Tagging for Information Integration for the International Gulf of Main Watershed</data>
      <data key="e_abstract">EIA-0131912&lt;br/&gt;Beard-Tisdale, Mary-Kate&lt;br/&gt;University of Maine&lt;br/&gt;&lt;br/&gt;BDEI: Event and process tagging for information integration for the International Gulf of Maine Watershed&lt;br/&gt;&lt;br/&gt;Project Summary&lt;br/&gt;&lt;br/&gt;This incubation proposal addresses the issue of integrating large, diverse, and autonomous&lt;br/&gt;collections of scientific data within a complex institutional setting. The goal is to convert these&lt;br/&gt;autonomous collections into a shareable repository that supports synthesis of data through new&lt;br/&gt;metadata structures based on events and processes. The institutional setting is the data and data-&lt;br/&gt;gathering activities of over 80 agencies, NGOS, and academic and research institutions operating&lt;br/&gt;within the Gulf of Maine watershed. The metadata development will be coordinated by library and&lt;br/&gt;spatial information scientists working jointly with domain scientists. An essential task of this&lt;br/&gt;incubation effort will be the development of a shared understanding of environmental processes and&lt;br/&gt;events that becomes a shareable ontology.&lt;br/&gt;&lt;br/&gt;Libraries play a vital role in organizing intellectual access to creative works. Scientific data has&lt;br/&gt;tended to be outside this traditional purview and has thus lacked the benefits of cataloging and&lt;br/&gt;indexing that promote shared access. We are proposing a new metadata structure that exploits&lt;br/&gt;common units of analysis in environmental studies: events and processes. Associating scientific data&lt;br/&gt;sets with event and process tags, in addition to other metadata elements, can substantially improve&lt;br/&gt;the ability to integrate and synthesize diverse scientific collections. The metadata initiative of this&lt;br/&gt;proposal will lay the foundation for specifying events and processes through the collaboration of&lt;br/&gt;data collections content specialists and information management specialists.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">131912</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">131912</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1701" target="n1702">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics: Event and Process Tagging for Information Integration for the International Gulf of Main Watershed</data>
      <data key="e_abstract">EIA-0131912&lt;br/&gt;Beard-Tisdale, Mary-Kate&lt;br/&gt;University of Maine&lt;br/&gt;&lt;br/&gt;BDEI: Event and process tagging for information integration for the International Gulf of Maine Watershed&lt;br/&gt;&lt;br/&gt;Project Summary&lt;br/&gt;&lt;br/&gt;This incubation proposal addresses the issue of integrating large, diverse, and autonomous&lt;br/&gt;collections of scientific data within a complex institutional setting. The goal is to convert these&lt;br/&gt;autonomous collections into a shareable repository that supports synthesis of data through new&lt;br/&gt;metadata structures based on events and processes. The institutional setting is the data and data-&lt;br/&gt;gathering activities of over 80 agencies, NGOS, and academic and research institutions operating&lt;br/&gt;within the Gulf of Maine watershed. The metadata development will be coordinated by library and&lt;br/&gt;spatial information scientists working jointly with domain scientists. An essential task of this&lt;br/&gt;incubation effort will be the development of a shared understanding of environmental processes and&lt;br/&gt;events that becomes a shareable ontology.&lt;br/&gt;&lt;br/&gt;Libraries play a vital role in organizing intellectual access to creative works. Scientific data has&lt;br/&gt;tended to be outside this traditional purview and has thus lacked the benefits of cataloging and&lt;br/&gt;indexing that promote shared access. We are proposing a new metadata structure that exploits&lt;br/&gt;common units of analysis in environmental studies: events and processes. Associating scientific data&lt;br/&gt;sets with event and process tags, in addition to other metadata elements, can substantially improve&lt;br/&gt;the ability to integrate and synthesize diverse scientific collections. The metadata initiative of this&lt;br/&gt;proposal will lay the foundation for specifying events and processes through the collaboration of&lt;br/&gt;data collections content specialists and information management specialists.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">131912</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">131912</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1703" target="n1704">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Societal Scale Information Systems: Technologies, Design and Applications</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;ITR/SI: Societal Scale Information Systems: Technologies, design and applications&lt;br/&gt;&lt;br/&gt;James Demmel, University of California, Berkeley&lt;br/&gt;0122599&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Information technology (IT) is transforming society at an accelerating pace, but critical research is needed to realize IT&apos;s potential for solving complex societal problems, including energy, disaster response and education. We are establishing the Center for Information Technology Research in the Interest of Society (CITRIS) to sponsor collaborative, IT-focused research to find solutions to grand-challenge societal problems affecting the quality of life of individuals and organizations. CITRIS is a multi-campus center, including UC Berkeley, UC Davis, UC Santa Cruz and UC Merced, with many industrial partners. CITRIS&apos;s driving applications include (1) boosting efficiency of energy production and consumption, and (2) saving lives and property and establishing emergency response IT infrastructure in the wake of disasters, among others. The solutions to these applications share a need for highly-distributed, reliable, and secure information systems that can evolve and adapt to radical changes in their environment, delivering networked information services and up-to-date sensor network data stores over ad-hoc, flexible and fault tolerant networks that adapt to the people and organizations that need them. We call such systems Societal-Scale Information Systems. Our research and outreach partner is UC Merced, a new research campus to be constructed, with a special mission to expand participation of underrepresented, first generation college-going, low income and rural students.</data>
      <data key="e_pgm">1688</data>
      <data key="e_label">122599</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">122599</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n399" target="n1703">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Societal Scale Information Systems: Technologies, Design and Applications</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;ITR/SI: Societal Scale Information Systems: Technologies, design and applications&lt;br/&gt;&lt;br/&gt;James Demmel, University of California, Berkeley&lt;br/&gt;0122599&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Information technology (IT) is transforming society at an accelerating pace, but critical research is needed to realize IT&apos;s potential for solving complex societal problems, including energy, disaster response and education. We are establishing the Center for Information Technology Research in the Interest of Society (CITRIS) to sponsor collaborative, IT-focused research to find solutions to grand-challenge societal problems affecting the quality of life of individuals and organizations. CITRIS is a multi-campus center, including UC Berkeley, UC Davis, UC Santa Cruz and UC Merced, with many industrial partners. CITRIS&apos;s driving applications include (1) boosting efficiency of energy production and consumption, and (2) saving lives and property and establishing emergency response IT infrastructure in the wake of disasters, among others. The solutions to these applications share a need for highly-distributed, reliable, and secure information systems that can evolve and adapt to radical changes in their environment, delivering networked information services and up-to-date sensor network data stores over ad-hoc, flexible and fault tolerant networks that adapt to the people and organizations that need them. We call such systems Societal-Scale Information Systems. Our research and outreach partner is UC Merced, a new research campus to be constructed, with a special mission to expand participation of underrepresented, first generation college-going, low income and rural students.</data>
      <data key="e_pgm">1688</data>
      <data key="e_label">122599</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">122599</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1703" target="n1706">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Societal Scale Information Systems: Technologies, Design and Applications</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;ITR/SI: Societal Scale Information Systems: Technologies, design and applications&lt;br/&gt;&lt;br/&gt;James Demmel, University of California, Berkeley&lt;br/&gt;0122599&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Information technology (IT) is transforming society at an accelerating pace, but critical research is needed to realize IT&apos;s potential for solving complex societal problems, including energy, disaster response and education. We are establishing the Center for Information Technology Research in the Interest of Society (CITRIS) to sponsor collaborative, IT-focused research to find solutions to grand-challenge societal problems affecting the quality of life of individuals and organizations. CITRIS is a multi-campus center, including UC Berkeley, UC Davis, UC Santa Cruz and UC Merced, with many industrial partners. CITRIS&apos;s driving applications include (1) boosting efficiency of energy production and consumption, and (2) saving lives and property and establishing emergency response IT infrastructure in the wake of disasters, among others. The solutions to these applications share a need for highly-distributed, reliable, and secure information systems that can evolve and adapt to radical changes in their environment, delivering networked information services and up-to-date sensor network data stores over ad-hoc, flexible and fault tolerant networks that adapt to the people and organizations that need them. We call such systems Societal-Scale Information Systems. Our research and outreach partner is UC Merced, a new research campus to be constructed, with a special mission to expand participation of underrepresented, first generation college-going, low income and rural students.</data>
      <data key="e_pgm">1688</data>
      <data key="e_label">122599</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">122599</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1703" target="n1707">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Societal Scale Information Systems: Technologies, Design and Applications</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;ITR/SI: Societal Scale Information Systems: Technologies, design and applications&lt;br/&gt;&lt;br/&gt;James Demmel, University of California, Berkeley&lt;br/&gt;0122599&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Information technology (IT) is transforming society at an accelerating pace, but critical research is needed to realize IT&apos;s potential for solving complex societal problems, including energy, disaster response and education. We are establishing the Center for Information Technology Research in the Interest of Society (CITRIS) to sponsor collaborative, IT-focused research to find solutions to grand-challenge societal problems affecting the quality of life of individuals and organizations. CITRIS is a multi-campus center, including UC Berkeley, UC Davis, UC Santa Cruz and UC Merced, with many industrial partners. CITRIS&apos;s driving applications include (1) boosting efficiency of energy production and consumption, and (2) saving lives and property and establishing emergency response IT infrastructure in the wake of disasters, among others. The solutions to these applications share a need for highly-distributed, reliable, and secure information systems that can evolve and adapt to radical changes in their environment, delivering networked information services and up-to-date sensor network data stores over ad-hoc, flexible and fault tolerant networks that adapt to the people and organizations that need them. We call such systems Societal-Scale Information Systems. Our research and outreach partner is UC Merced, a new research campus to be constructed, with a special mission to expand participation of underrepresented, first generation college-going, low income and rural students.</data>
      <data key="e_pgm">1688</data>
      <data key="e_label">122599</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">122599</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n399" target="n1704">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Societal Scale Information Systems: Technologies, Design and Applications</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;ITR/SI: Societal Scale Information Systems: Technologies, design and applications&lt;br/&gt;&lt;br/&gt;James Demmel, University of California, Berkeley&lt;br/&gt;0122599&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Information technology (IT) is transforming society at an accelerating pace, but critical research is needed to realize IT&apos;s potential for solving complex societal problems, including energy, disaster response and education. We are establishing the Center for Information Technology Research in the Interest of Society (CITRIS) to sponsor collaborative, IT-focused research to find solutions to grand-challenge societal problems affecting the quality of life of individuals and organizations. CITRIS is a multi-campus center, including UC Berkeley, UC Davis, UC Santa Cruz and UC Merced, with many industrial partners. CITRIS&apos;s driving applications include (1) boosting efficiency of energy production and consumption, and (2) saving lives and property and establishing emergency response IT infrastructure in the wake of disasters, among others. The solutions to these applications share a need for highly-distributed, reliable, and secure information systems that can evolve and adapt to radical changes in their environment, delivering networked information services and up-to-date sensor network data stores over ad-hoc, flexible and fault tolerant networks that adapt to the people and organizations that need them. We call such systems Societal-Scale Information Systems. Our research and outreach partner is UC Merced, a new research campus to be constructed, with a special mission to expand participation of underrepresented, first generation college-going, low income and rural students.</data>
      <data key="e_pgm">1688</data>
      <data key="e_label">122599</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">122599</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1704" target="n1706">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Societal Scale Information Systems: Technologies, Design and Applications</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;ITR/SI: Societal Scale Information Systems: Technologies, design and applications&lt;br/&gt;&lt;br/&gt;James Demmel, University of California, Berkeley&lt;br/&gt;0122599&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Information technology (IT) is transforming society at an accelerating pace, but critical research is needed to realize IT&apos;s potential for solving complex societal problems, including energy, disaster response and education. We are establishing the Center for Information Technology Research in the Interest of Society (CITRIS) to sponsor collaborative, IT-focused research to find solutions to grand-challenge societal problems affecting the quality of life of individuals and organizations. CITRIS is a multi-campus center, including UC Berkeley, UC Davis, UC Santa Cruz and UC Merced, with many industrial partners. CITRIS&apos;s driving applications include (1) boosting efficiency of energy production and consumption, and (2) saving lives and property and establishing emergency response IT infrastructure in the wake of disasters, among others. The solutions to these applications share a need for highly-distributed, reliable, and secure information systems that can evolve and adapt to radical changes in their environment, delivering networked information services and up-to-date sensor network data stores over ad-hoc, flexible and fault tolerant networks that adapt to the people and organizations that need them. We call such systems Societal-Scale Information Systems. Our research and outreach partner is UC Merced, a new research campus to be constructed, with a special mission to expand participation of underrepresented, first generation college-going, low income and rural students.</data>
      <data key="e_pgm">1688</data>
      <data key="e_label">122599</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">122599</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1704" target="n1707">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Societal Scale Information Systems: Technologies, Design and Applications</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;ITR/SI: Societal Scale Information Systems: Technologies, design and applications&lt;br/&gt;&lt;br/&gt;James Demmel, University of California, Berkeley&lt;br/&gt;0122599&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Information technology (IT) is transforming society at an accelerating pace, but critical research is needed to realize IT&apos;s potential for solving complex societal problems, including energy, disaster response and education. We are establishing the Center for Information Technology Research in the Interest of Society (CITRIS) to sponsor collaborative, IT-focused research to find solutions to grand-challenge societal problems affecting the quality of life of individuals and organizations. CITRIS is a multi-campus center, including UC Berkeley, UC Davis, UC Santa Cruz and UC Merced, with many industrial partners. CITRIS&apos;s driving applications include (1) boosting efficiency of energy production and consumption, and (2) saving lives and property and establishing emergency response IT infrastructure in the wake of disasters, among others. The solutions to these applications share a need for highly-distributed, reliable, and secure information systems that can evolve and adapt to radical changes in their environment, delivering networked information services and up-to-date sensor network data stores over ad-hoc, flexible and fault tolerant networks that adapt to the people and organizations that need them. We call such systems Societal-Scale Information Systems. Our research and outreach partner is UC Merced, a new research campus to be constructed, with a special mission to expand participation of underrepresented, first generation college-going, low income and rural students.</data>
      <data key="e_pgm">1688</data>
      <data key="e_label">122599</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">122599</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n399" target="n1706">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Societal Scale Information Systems: Technologies, Design and Applications</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;ITR/SI: Societal Scale Information Systems: Technologies, design and applications&lt;br/&gt;&lt;br/&gt;James Demmel, University of California, Berkeley&lt;br/&gt;0122599&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Information technology (IT) is transforming society at an accelerating pace, but critical research is needed to realize IT&apos;s potential for solving complex societal problems, including energy, disaster response and education. We are establishing the Center for Information Technology Research in the Interest of Society (CITRIS) to sponsor collaborative, IT-focused research to find solutions to grand-challenge societal problems affecting the quality of life of individuals and organizations. CITRIS is a multi-campus center, including UC Berkeley, UC Davis, UC Santa Cruz and UC Merced, with many industrial partners. CITRIS&apos;s driving applications include (1) boosting efficiency of energy production and consumption, and (2) saving lives and property and establishing emergency response IT infrastructure in the wake of disasters, among others. The solutions to these applications share a need for highly-distributed, reliable, and secure information systems that can evolve and adapt to radical changes in their environment, delivering networked information services and up-to-date sensor network data stores over ad-hoc, flexible and fault tolerant networks that adapt to the people and organizations that need them. We call such systems Societal-Scale Information Systems. Our research and outreach partner is UC Merced, a new research campus to be constructed, with a special mission to expand participation of underrepresented, first generation college-going, low income and rural students.</data>
      <data key="e_pgm">1688</data>
      <data key="e_label">122599</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">122599</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n399" target="n1707">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Societal Scale Information Systems: Technologies, Design and Applications</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;ITR/SI: Societal Scale Information Systems: Technologies, design and applications&lt;br/&gt;&lt;br/&gt;James Demmel, University of California, Berkeley&lt;br/&gt;0122599&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Information technology (IT) is transforming society at an accelerating pace, but critical research is needed to realize IT&apos;s potential for solving complex societal problems, including energy, disaster response and education. We are establishing the Center for Information Technology Research in the Interest of Society (CITRIS) to sponsor collaborative, IT-focused research to find solutions to grand-challenge societal problems affecting the quality of life of individuals and organizations. CITRIS is a multi-campus center, including UC Berkeley, UC Davis, UC Santa Cruz and UC Merced, with many industrial partners. CITRIS&apos;s driving applications include (1) boosting efficiency of energy production and consumption, and (2) saving lives and property and establishing emergency response IT infrastructure in the wake of disasters, among others. The solutions to these applications share a need for highly-distributed, reliable, and secure information systems that can evolve and adapt to radical changes in their environment, delivering networked information services and up-to-date sensor network data stores over ad-hoc, flexible and fault tolerant networks that adapt to the people and organizations that need them. We call such systems Societal-Scale Information Systems. Our research and outreach partner is UC Merced, a new research campus to be constructed, with a special mission to expand participation of underrepresented, first generation college-going, low income and rural students.</data>
      <data key="e_pgm">1688</data>
      <data key="e_label">122599</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">122599</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1706" target="n1707">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI: Societal Scale Information Systems: Technologies, Design and Applications</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;ITR/SI: Societal Scale Information Systems: Technologies, design and applications&lt;br/&gt;&lt;br/&gt;James Demmel, University of California, Berkeley&lt;br/&gt;0122599&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Information technology (IT) is transforming society at an accelerating pace, but critical research is needed to realize IT&apos;s potential for solving complex societal problems, including energy, disaster response and education. We are establishing the Center for Information Technology Research in the Interest of Society (CITRIS) to sponsor collaborative, IT-focused research to find solutions to grand-challenge societal problems affecting the quality of life of individuals and organizations. CITRIS is a multi-campus center, including UC Berkeley, UC Davis, UC Santa Cruz and UC Merced, with many industrial partners. CITRIS&apos;s driving applications include (1) boosting efficiency of energy production and consumption, and (2) saving lives and property and establishing emergency response IT infrastructure in the wake of disasters, among others. The solutions to these applications share a need for highly-distributed, reliable, and secure information systems that can evolve and adapt to radical changes in their environment, delivering networked information services and up-to-date sensor network data stores over ad-hoc, flexible and fault tolerant networks that adapt to the people and organizations that need them. We call such systems Societal-Scale Information Systems. Our research and outreach partner is UC Merced, a new research campus to be constructed, with a special mission to expand participation of underrepresented, first generation college-going, low income and rural students.</data>
      <data key="e_pgm">1688</data>
      <data key="e_label">122599</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">122599</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1708" target="n1709">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Ultra-High-Capacity Optical Communications and Networking: Towards 100 Tb/s Communication on a Single Optical Fiber</data>
      <data key="e_abstract">We propose to develop technologies which will enable communication on a single&lt;br/&gt;optical fiber at rates in excess of 100 Tb/s within a few years. To accomplish this in the&lt;br/&gt;low-loss window of silica fibers (1200-1700 nm), there will be a need for novel very&lt;br/&gt;wideband devices, particularly optical amplifiers. In addition, modulation techniques with&lt;br/&gt;spectral efficiencies well in excess of unity will be needed. Our research activities will be&lt;br/&gt;centered in these two areas.&lt;br/&gt; We will investigate modulation techniques, and associated detection schemes,&lt;br/&gt;suitable to obtain a spectral efficiency of 4 b/s/Hz by the end of the project. The&lt;br/&gt;techniques will be designed to be resistant to impairments due to PMD and fiber&lt;br/&gt;nonlinearities. In particular, we will investigate novel techniques using appropriate&lt;br/&gt;combinations of PSK, QAM, and polarization-based techniques. Coherent detection will&lt;br/&gt;be utilized where indicated to boost spectral efficiency.&lt;br/&gt;We will investigate fiber optical parametric amplifiers (OPAs) and discrete&lt;br/&gt;Raman amplifiers made from novel highly-nonlinear fibers. These fibers will have&lt;br/&gt;nonlinearity coefficients exceeding those of today&apos;s most nonlinear fibers by several&lt;br/&gt;orders of magnitude. These fibers will enable the development of novel nonlinear&lt;br/&gt;amplifiers , with gain bandwidth of a single device covering most of the 1200-1700 nm&lt;br/&gt;window. These devices will also use shorter fibers and lower pump powers than their&lt;br/&gt;current versions, and may thus lead to practical applications in a relatively short time.</data>
      <data key="e_pgm">4090</data>
      <data key="e_label">123441</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">123441</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1711" target="n1712">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Ultra-High-Capacity Optical Communications and Networking: Data processing modules using high-nonlinearity fiber for advanced optical networking</data>
      <data key="e_abstract">The high demand likely to be placed on the capacity of telecommunication networks in the near future urges&lt;br/&gt;conversion of hybrid electro-optical signal processing to all-optical processing, exploiting the largest bandwidth available in the optical domain. One way of catering to this demand is by multiplexing in time as&lt;br/&gt;well as wavelength domains. Using picosecond-duration optical pulses, which could be soliton-like over&lt;br/&gt;portions of the network, one can first perform multiplexing in the time domain (i.e., time-division multiplexing, or TDM) for local to metropolitan-area network applications and then in the wavelength domain (wavelength-division multiplexing, or WDM) for wide area coverage. This scenario leads one to conclude that the key issue to be addressed is how to take advantage of the powerful digital-processing techniques in the pure-optical domain, that minimize the detrimental effects of noise at a very fundamental level. The idea is that, for digitally encoded data [1&apos;s (0&apos;s) represented by the presence (absence) of pulses], instead of using linear amplifiers which act on signals in an analog fashion and inevitably introduce 3 dB of noise one can employ digital-switching amplifiers or optical regenerators. At the same time, pure-optical digital switching is potentially much more reliable and faster than electro-optical switching. Furthermore, optical switching will also be needed to implement other networking functions, such as demultiplexing to process at very high speed the header of a data packet used for addressing to different users on the network.&lt;br/&gt; Our preliminary experiments show that the parametric nonlinearity of optical fibers can be exploited&lt;br/&gt;to perform functionalities that will be needed in packet-switched all-optical networks, such as fiber-optic&lt;br/&gt;cache-memory buffers, picosecond-pulse all-optical regenerators, all-optical limiters, and tunable clock re-covery modules. These devices take advantage of the ultrafast parametric nonlinearity of glass fiber and&lt;br/&gt;hence are capable of operating at speeds in excess of 100 Gb/s. Moreover, they will be essential for deploying&lt;br/&gt;packet-switched, ultrahigh-speed time-division and wavelength-division multiplexed all-optical networks.&lt;br/&gt; In all of our experiments thus far, standard dispersion-shifted fiber (DSF) has been used. Fiber lengths&lt;br/&gt;on the order of 100&apos;s of meters are required for used with ps-duration pulses of a few watts peak power to&lt;br/&gt;achieve the data processing functions. Here we propose to explore the use of high-nonlinearity fiber, such&lt;br/&gt;as microstructure fiber (MF, which is only now becoming commercially available), to perform essential&lt;br/&gt;functions in high-speed all-optical processing. Because of their strongly guiding behavior, the MFs can&lt;br/&gt;be wound into very tight loops, suggesting that they could potentially fit into a compact modular switching&lt;br/&gt;package. Specifically, we propose to utilize the high-nonlinearity microstructure fibers to develop all-optical data processing modules. These include a cache storage buffer based upon parametric amplification that will be capable of operating in the 10&apos;s of Gb/s range. With use of the high-nonlinearity fiber, the average pump power requirement can be met with commercially-available watt-class optical amplifiers. We will carry out experiments to explore various ways of reading, writing, and erasing the stored data patterns.&lt;br/&gt;Our work has shown that the ultrafast parametric nonlinearity can be exploited either to provide broadband&lt;br/&gt;tunable gain or dynamic gain modulation for clock-recovery. We propose to combine the two to demonstrate&lt;br/&gt;optical phase-lock loops, which in principle can be extremely fast as they rely on the Kerr nonlinearity&lt;br/&gt;for envelope-phase discrimination. Simultaneous to the above experimental studies we will also develop&lt;br/&gt;numerical models of the various optical systems. This will provide a design tool to determine the parameter&lt;br/&gt;values allowing the most efficient operation of the experimental setups. We have previously demonstrated&lt;br/&gt;the possibility of stably propagating sub-picosecond pulses in fiber lines in which conjugating gain is used&lt;br/&gt;to compensate the linear loss. We propose to assemble a re-circulating loop experiment in which linear loss&lt;br/&gt;will be compensated by a pair of non-degenerate parametric (conjugating) amplifiers. The location of the&lt;br/&gt;two amplifiers will be chosen based upon further theoretical/numerical results. We will experimentally and&lt;br/&gt;theoretically study the stability properties of the sub-picosecond pulses by making various signal and noise&lt;br/&gt;measurements, and will compare the experimental results directly with numerical simulations.</data>
      <data key="e_pgm">4090</data>
      <data key="e_label">123495</data>
      <data key="e_expirationDate">2005-03-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">123495</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1714" target="n1715">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">MRI: Acquisition of a Power4-based IBM SP and PC-based Scalable Display Wall for Multidisciplinary Computational Science Research</data>
      <data key="e_abstract">EIA-0116549&lt;br/&gt;Claudio Rebbi&lt;br/&gt;Boston University &lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Power4-based IBM SP and PC-based Scalable Display Wall for Multidisciplinary Computational Science Research&lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a broad range of advanced scientific computing applications. The proposed supercomputer and visualization instrumentation would contribute to computational investigations of subnuclear particle phenomena, the dynamics of quantum systems, space weather modeling, the electrodynamics of plasma processes in the ionosphere, managing the manufacturing supply chain, subsurface sensing and imaging systems, among many other projects.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116549</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116549</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n790" target="n1714">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">MRI: Acquisition of a Power4-based IBM SP and PC-based Scalable Display Wall for Multidisciplinary Computational Science Research</data>
      <data key="e_abstract">EIA-0116549&lt;br/&gt;Claudio Rebbi&lt;br/&gt;Boston University &lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Power4-based IBM SP and PC-based Scalable Display Wall for Multidisciplinary Computational Science Research&lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a broad range of advanced scientific computing applications. The proposed supercomputer and visualization instrumentation would contribute to computational investigations of subnuclear particle phenomena, the dynamics of quantum systems, space weather modeling, the electrodynamics of plasma processes in the ionosphere, managing the manufacturing supply chain, subsurface sensing and imaging systems, among many other projects.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116549</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116549</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1714" target="n1717">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">MRI: Acquisition of a Power4-based IBM SP and PC-based Scalable Display Wall for Multidisciplinary Computational Science Research</data>
      <data key="e_abstract">EIA-0116549&lt;br/&gt;Claudio Rebbi&lt;br/&gt;Boston University &lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Power4-based IBM SP and PC-based Scalable Display Wall for Multidisciplinary Computational Science Research&lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a broad range of advanced scientific computing applications. The proposed supercomputer and visualization instrumentation would contribute to computational investigations of subnuclear particle phenomena, the dynamics of quantum systems, space weather modeling, the electrodynamics of plasma processes in the ionosphere, managing the manufacturing supply chain, subsurface sensing and imaging systems, among many other projects.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116549</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116549</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n790" target="n1715">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">MRI: Acquisition of a Power4-based IBM SP and PC-based Scalable Display Wall for Multidisciplinary Computational Science Research</data>
      <data key="e_abstract">EIA-0116549&lt;br/&gt;Claudio Rebbi&lt;br/&gt;Boston University &lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Power4-based IBM SP and PC-based Scalable Display Wall for Multidisciplinary Computational Science Research&lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a broad range of advanced scientific computing applications. The proposed supercomputer and visualization instrumentation would contribute to computational investigations of subnuclear particle phenomena, the dynamics of quantum systems, space weather modeling, the electrodynamics of plasma processes in the ionosphere, managing the manufacturing supply chain, subsurface sensing and imaging systems, among many other projects.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116549</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116549</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1715" target="n1717">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">MRI: Acquisition of a Power4-based IBM SP and PC-based Scalable Display Wall for Multidisciplinary Computational Science Research</data>
      <data key="e_abstract">EIA-0116549&lt;br/&gt;Claudio Rebbi&lt;br/&gt;Boston University &lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Power4-based IBM SP and PC-based Scalable Display Wall for Multidisciplinary Computational Science Research&lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a broad range of advanced scientific computing applications. The proposed supercomputer and visualization instrumentation would contribute to computational investigations of subnuclear particle phenomena, the dynamics of quantum systems, space weather modeling, the electrodynamics of plasma processes in the ionosphere, managing the manufacturing supply chain, subsurface sensing and imaging systems, among many other projects.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116549</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116549</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n790" target="n1717">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">MRI: Acquisition of a Power4-based IBM SP and PC-based Scalable Display Wall for Multidisciplinary Computational Science Research</data>
      <data key="e_abstract">EIA-0116549&lt;br/&gt;Claudio Rebbi&lt;br/&gt;Boston University &lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Power4-based IBM SP and PC-based Scalable Display Wall for Multidisciplinary Computational Science Research&lt;br/&gt;&lt;br/&gt;This is a proposal for equipment acquisition under the Major Research Instrumentation (MRI) program to support research and student training across a broad range of advanced scientific computing applications. The proposed supercomputer and visualization instrumentation would contribute to computational investigations of subnuclear particle phenomena, the dynamics of quantum systems, space weather modeling, the electrodynamics of plasma processes in the ionosphere, managing the manufacturing supply chain, subsurface sensing and imaging systems, among many other projects.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">116549</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">116549</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1718" target="n1719">
      <data key="e_effectiveDate">2001-10-15</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics - BDEI - Spatial Data Infrastructure for Ecological Research (Planning Grant)</data>
      <data key="e_abstract">EIA-0131952&lt;br/&gt;Cushing, Judith&lt;br/&gt;Evergreen State College&lt;br/&gt;&lt;br/&gt;BDEI: Spatial Data Infrastructure for Ecological Research (Planning Grant)&lt;br/&gt;&lt;br/&gt;Summary&lt;br/&gt;&lt;br/&gt;The Biodiversity and Ecosystem Informatics Research Agenda (http://bio.gsfc.nasa.gov) notes&lt;br/&gt;the current inability to compare data across spatial scales as a critical problem: &quot;Biological data&lt;br/&gt;from different sources are frequently collected and presented in different scales and resolutions&lt;br/&gt;resulting in a loss of detail when multiple data sets are required for data synthesis and analysis&quot;.&lt;br/&gt;&lt;br/&gt;This grant brings together a team of ecologists and computer scientists to plan a proposal for a&lt;br/&gt;&quot;Spatial Data Infrastructure for Ecological Sciences.&quot; The proposed infrastructure would allow a&lt;br/&gt;scientist to define a data set by putting together the proper &quot;spatial.&quot; building blocks -- building&lt;br/&gt;blocks represented in conceptually familiar, domain-specific terms. Data sets thus constructed&lt;br/&gt;(or recast) would be amenable to the automatic integration of spatial locations and measurements&lt;br/&gt;with automatic spatial data transformations.&lt;br/&gt;&lt;br/&gt;This would allow spatial analysis of individual field data sets and the linking together at same&lt;br/&gt;and different spatial scales data sets defined within the spatial infrastructure. We aim to include&lt;br/&gt;productivity enhancing research tools such as field data gathering devices and data validation&lt;br/&gt;and analysis, and to automatically tag field data with metadata early in the research cycle. We&lt;br/&gt;thus also contribute to the synthesis of metadata and data.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">131952</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">131952</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n76" target="n1722">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Information Technology Workforce - ITWF: Why So Few Women in Information Technology?: A Comparative Study</data>
      <data key="e_abstract">This 1-year empirical pilot study will investigate the reasons for the under-representation women majoring in Information Technology(IT) related disciplines in institutions of higher education. The study will focus on two questions: 1) Why do women who have the potential to succeed in the study of IT disciplines, take alternative educational paths? 2) What barriers and obstacles must be overcome to attract more women to IT education and careers?</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">120055</data>
      <data key="e_expirationDate">2003-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">120055</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n76" target="n1723">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Information Technology Workforce - ITWF: Why So Few Women in Information Technology?: A Comparative Study</data>
      <data key="e_abstract">This 1-year empirical pilot study will investigate the reasons for the under-representation women majoring in Information Technology(IT) related disciplines in institutions of higher education. The study will focus on two questions: 1) Why do women who have the potential to succeed in the study of IT disciplines, take alternative educational paths? 2) What barriers and obstacles must be overcome to attract more women to IT education and careers?</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">120055</data>
      <data key="e_expirationDate">2003-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">120055</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1722" target="n1723">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Information Technology Workforce - ITWF: Why So Few Women in Information Technology?: A Comparative Study</data>
      <data key="e_abstract">This 1-year empirical pilot study will investigate the reasons for the under-representation women majoring in Information Technology(IT) related disciplines in institutions of higher education. The study will focus on two questions: 1) Why do women who have the potential to succeed in the study of IT disciplines, take alternative educational paths? 2) What barriers and obstacles must be overcome to attract more women to IT education and careers?</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">120055</data>
      <data key="e_expirationDate">2003-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">120055</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1724" target="n1725">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Information Technology Workforce - ITWF: Getting the Media Message: The Portrayal of Gender, Race, and Information Technology in the Media Environment of Middle School Students</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;Information Technology Workforce (ITWF)&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Proposal Id: EIA-0120056&lt;br/&gt;Investigator: Phyllis Bernt, Joseph Bernt and Sandra Turner&lt;br/&gt;Institution: Ohio University&lt;br/&gt;Title: Getting the Media Message: The Portrayal of Gender, Race, and Information Technology in the Media Environment of Middle School Students. &lt;br/&gt;&lt;br/&gt;This award provides support for a study examining media messages about Information Technology (IT) careers that middle school students receive from television, popular magazines, videotapes, movies, and from books, brochures and Internet sites that provide career information. The analysis will concentrate on the media portrayal of IT occupations, especially in terms of gender and race, that students are likely to encounter during the middle school years. Sixth and Seventh grade students in 12 demographically diverse classrooms across the nation will participate in classroom-based action research employing mathematics, language arts, and social science skills in surveying their peers and conducting their own content analysis of media messages about IT careers.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">120056</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">120056</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1724" target="n1726">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Information Technology Workforce - ITWF: Getting the Media Message: The Portrayal of Gender, Race, and Information Technology in the Media Environment of Middle School Students</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;Information Technology Workforce (ITWF)&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Proposal Id: EIA-0120056&lt;br/&gt;Investigator: Phyllis Bernt, Joseph Bernt and Sandra Turner&lt;br/&gt;Institution: Ohio University&lt;br/&gt;Title: Getting the Media Message: The Portrayal of Gender, Race, and Information Technology in the Media Environment of Middle School Students. &lt;br/&gt;&lt;br/&gt;This award provides support for a study examining media messages about Information Technology (IT) careers that middle school students receive from television, popular magazines, videotapes, movies, and from books, brochures and Internet sites that provide career information. The analysis will concentrate on the media portrayal of IT occupations, especially in terms of gender and race, that students are likely to encounter during the middle school years. Sixth and Seventh grade students in 12 demographically diverse classrooms across the nation will participate in classroom-based action research employing mathematics, language arts, and social science skills in surveying their peers and conducting their own content analysis of media messages about IT careers.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">120056</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">120056</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1725" target="n1726">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Information Technology Workforce - ITWF: Getting the Media Message: The Portrayal of Gender, Race, and Information Technology in the Media Environment of Middle School Students</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;Information Technology Workforce (ITWF)&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Proposal Id: EIA-0120056&lt;br/&gt;Investigator: Phyllis Bernt, Joseph Bernt and Sandra Turner&lt;br/&gt;Institution: Ohio University&lt;br/&gt;Title: Getting the Media Message: The Portrayal of Gender, Race, and Information Technology in the Media Environment of Middle School Students. &lt;br/&gt;&lt;br/&gt;This award provides support for a study examining media messages about Information Technology (IT) careers that middle school students receive from television, popular magazines, videotapes, movies, and from books, brochures and Internet sites that provide career information. The analysis will concentrate on the media portrayal of IT occupations, especially in terms of gender and race, that students are likely to encounter during the middle school years. Sixth and Seventh grade students in 12 demographically diverse classrooms across the nation will participate in classroom-based action research employing mathematics, language arts, and social science skills in surveying their peers and conducting their own content analysis of media messages about IT careers.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">120056</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">120056</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1727" target="n1728">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">SGER: Algorithmic Infrastructure for Knowledge Management</data>
      <data key="e_abstract">This project explores algorithmic techniques for archival, indexing and reuse of complex knowledge. Hierarchical graphs have emerged as a powerful knowledge representation structures, however existing database and data mining techniques are not able to manage the extreme combinatorial complexities required to compare, classify, index and cluster these structures as primitive data elements. Research challenges include (1) capturing the inner structure of graph based models, and their topological sub-structures, to use for pattern matching purposes; (2) classify the stability of these approximation techniques in the presence of noise; (3) identifying how to translate these techniques into suitable database mechanisms. The basis of the approach is a mapping of the topological structure of a graph into a low-dimensional vector space through an eigenvalue characterization. This SGER studies these problems in the context of collaborative and distributed engineering design, where teams of agents (human and computational) interact over the network to realize a product (e.g., software, electro-mechanical device, building, etc.). This process is modeled as a directed acyclic graph (DAG) that encodes the design modeling operations and decisions, as well as the flow of the information along the modeling time-steps. Knowledge acquisition agents will be fielded to capture these knowledge structures and a validation of the theoretical methodology for indexing and reuse of process knowledge will performed. Some of this work will be done in collaboration with Bentley Systems. If successful, these techniques will lead to dramatic new possibilities for database and information management systems: allowing them to efficiently store complex graphs as single large objects and identify useful patterns in and across large sets of these combinatorial structures.</data>
      <data key="e_pgm">6856</data>
      <data key="e_label">136337</data>
      <data key="e_expirationDate">2002-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">136337</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1730" target="n1731">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">Information Technology Workforce - ITWF: Designing for Diversity: Investigating Electronic Games as Pathways for Girls into Information Technology Professions</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;Information Technology Workforce (ITWF)&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Proposal Id: EIA-0120093&lt;br/&gt;PI: Dorothy T. Bennett and Cornelia Brunner&lt;br/&gt;Institution: Education Development Center (EDC)&lt;br/&gt;Title: Designing for Diversity: Investigating Electronic Games as Pathways for Girls into Information Technology Professions&lt;br/&gt;&lt;br/&gt;This award provides support for a 2-year study to research design criteria of electronic games that affirm and support 8-14 year old girls&apos; positive notions of (Information Technology) IT professions. While much is known about general characteristics of games that appeal to girls and boys, there is a little information about the specific game design features, characteristics and problems that might successfully attract different children into IT. The study is expected to result in the development of specific design criteria for game designers and practical guidance for educators interested in using games to foster children&apos;s IT skills and interests.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">120093</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">120093</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1732" target="n1733">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SY+PE+IM+AP COLLABORATIVE RESEARCH: Computer-linked Auto-Fabricated Models for Education in Molecular Biology</data>
      <data key="e_abstract">This is a research project in the design and implementation of novel Human Computer Interfaces for educational use in structural molecular biology. This project will explore, define, and assess the role of computer-generated physical models in teaching scientific content and concepts. The purpose is to enhance the understanding and communication of the complex world of life&apos;s molecular machinery to a broad community. Using structural data on biological molecules and their complexes, the participants will prototype novel physical models via automated design and fabrication technologies and develop replication processes for broader distribution. The models will be used directly for enhance visualization and as input/output devices that interactively integrate with commutation and computer graphics for information retrieval, manipulation, and simulation.&lt;br/&gt;&lt;br/&gt;This project will provide new, tangible modes of interacting with, and understanding of, both the fundamental concepts and the complex data that are coming from the rapid advances in genomics, proteomics, and other areas of structural molecular biology. It will enable a broad range of students to learn, query, and explore in a field of growing scientific and social importance - the molecular basis of life. Physical models will give both haptic and visual support of the importance of shape in biological function. Use of physical models as tangible computer interfaces will revolutionize how students explore and understand biomolecular structure, interaction, and function.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">121533</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">121533</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n102" target="n673">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR Large: MALACH: Multilingual Access to Large spoken ArChives</data>
      <data key="e_abstract">The project proposes to advance the state of the art in automatic&lt;br/&gt;speech recognition by detecting emotional and highly accented speech&lt;br/&gt;and differences based on age and gender, and then optimizing the&lt;br/&gt;acoustic model for those conditions. It will apply long-term&lt;br/&gt;adaptation techniques to improve robustness, and will implement&lt;br/&gt;an innovative second-pass decoding technique to improve accuracy&lt;br/&gt;by using side information such as thesaurus terms and&lt;br/&gt;human-prepared summaries.&lt;br/&gt;&lt;br/&gt;The techniques to be developed will dramatically improve the efficiency&lt;br/&gt;of professional catalogers, leveraging automatic segmentation to&lt;br/&gt;suggest topic boundaries in interviews, using domain-tuned&lt;br/&gt;classification algorithms to recommend thesaurus terms, and&lt;br/&gt;providing automated tools to support generation of event timelines.&lt;br/&gt;Volunteers will help assign metadata and provide transcripts.&lt;br/&gt;Efforts will be made to automate transferring capabilities developed&lt;br/&gt;originally for English to other languages. Access to multilingual&lt;br/&gt;materials will be done by combining knowledge-based and corpus-based&lt;br/&gt;techniques to extend existing thesauri to new languages and by supporting&lt;br/&gt;cross-language searching of manually prepared segment-level&lt;br/&gt;summaries and automatic speech recognition transcripts.&lt;br/&gt;&lt;br/&gt;Each component will be evaluated and user studies done to measure&lt;br/&gt;the overall impact on support for cataloging, search and exploration.&lt;br/&gt;This will produce significant impact, both through improved access&lt;br/&gt;to our cultural heritage and through the application of the techniques&lt;br/&gt;to other important problems.&lt;br/&gt;&lt;br/&gt;The collection of spoken material used in this project will be the 116,000&lt;br/&gt;hours held by the Survivors of the Shoah Visual History Foundation, a&lt;br/&gt;set of already digitized video recordings of great historical importance.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">122466</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">122466</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n673" target="n1737">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR Large: MALACH: Multilingual Access to Large spoken ArChives</data>
      <data key="e_abstract">The project proposes to advance the state of the art in automatic&lt;br/&gt;speech recognition by detecting emotional and highly accented speech&lt;br/&gt;and differences based on age and gender, and then optimizing the&lt;br/&gt;acoustic model for those conditions. It will apply long-term&lt;br/&gt;adaptation techniques to improve robustness, and will implement&lt;br/&gt;an innovative second-pass decoding technique to improve accuracy&lt;br/&gt;by using side information such as thesaurus terms and&lt;br/&gt;human-prepared summaries.&lt;br/&gt;&lt;br/&gt;The techniques to be developed will dramatically improve the efficiency&lt;br/&gt;of professional catalogers, leveraging automatic segmentation to&lt;br/&gt;suggest topic boundaries in interviews, using domain-tuned&lt;br/&gt;classification algorithms to recommend thesaurus terms, and&lt;br/&gt;providing automated tools to support generation of event timelines.&lt;br/&gt;Volunteers will help assign metadata and provide transcripts.&lt;br/&gt;Efforts will be made to automate transferring capabilities developed&lt;br/&gt;originally for English to other languages. Access to multilingual&lt;br/&gt;materials will be done by combining knowledge-based and corpus-based&lt;br/&gt;techniques to extend existing thesauri to new languages and by supporting&lt;br/&gt;cross-language searching of manually prepared segment-level&lt;br/&gt;summaries and automatic speech recognition transcripts.&lt;br/&gt;&lt;br/&gt;Each component will be evaluated and user studies done to measure&lt;br/&gt;the overall impact on support for cataloging, search and exploration.&lt;br/&gt;This will produce significant impact, both through improved access&lt;br/&gt;to our cultural heritage and through the application of the techniques&lt;br/&gt;to other important problems.&lt;br/&gt;&lt;br/&gt;The collection of spoken material used in this project will be the 116,000&lt;br/&gt;hours held by the Survivors of the Shoah Visual History Foundation, a&lt;br/&gt;set of already digitized video recordings of great historical importance.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">122466</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">122466</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n673" target="n1738">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR Large: MALACH: Multilingual Access to Large spoken ArChives</data>
      <data key="e_abstract">The project proposes to advance the state of the art in automatic&lt;br/&gt;speech recognition by detecting emotional and highly accented speech&lt;br/&gt;and differences based on age and gender, and then optimizing the&lt;br/&gt;acoustic model for those conditions. It will apply long-term&lt;br/&gt;adaptation techniques to improve robustness, and will implement&lt;br/&gt;an innovative second-pass decoding technique to improve accuracy&lt;br/&gt;by using side information such as thesaurus terms and&lt;br/&gt;human-prepared summaries.&lt;br/&gt;&lt;br/&gt;The techniques to be developed will dramatically improve the efficiency&lt;br/&gt;of professional catalogers, leveraging automatic segmentation to&lt;br/&gt;suggest topic boundaries in interviews, using domain-tuned&lt;br/&gt;classification algorithms to recommend thesaurus terms, and&lt;br/&gt;providing automated tools to support generation of event timelines.&lt;br/&gt;Volunteers will help assign metadata and provide transcripts.&lt;br/&gt;Efforts will be made to automate transferring capabilities developed&lt;br/&gt;originally for English to other languages. Access to multilingual&lt;br/&gt;materials will be done by combining knowledge-based and corpus-based&lt;br/&gt;techniques to extend existing thesauri to new languages and by supporting&lt;br/&gt;cross-language searching of manually prepared segment-level&lt;br/&gt;summaries and automatic speech recognition transcripts.&lt;br/&gt;&lt;br/&gt;Each component will be evaluated and user studies done to measure&lt;br/&gt;the overall impact on support for cataloging, search and exploration.&lt;br/&gt;This will produce significant impact, both through improved access&lt;br/&gt;to our cultural heritage and through the application of the techniques&lt;br/&gt;to other important problems.&lt;br/&gt;&lt;br/&gt;The collection of spoken material used in this project will be the 116,000&lt;br/&gt;hours held by the Survivors of the Shoah Visual History Foundation, a&lt;br/&gt;set of already digitized video recordings of great historical importance.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">122466</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">122466</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n673" target="n1739">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR Large: MALACH: Multilingual Access to Large spoken ArChives</data>
      <data key="e_abstract">The project proposes to advance the state of the art in automatic&lt;br/&gt;speech recognition by detecting emotional and highly accented speech&lt;br/&gt;and differences based on age and gender, and then optimizing the&lt;br/&gt;acoustic model for those conditions. It will apply long-term&lt;br/&gt;adaptation techniques to improve robustness, and will implement&lt;br/&gt;an innovative second-pass decoding technique to improve accuracy&lt;br/&gt;by using side information such as thesaurus terms and&lt;br/&gt;human-prepared summaries.&lt;br/&gt;&lt;br/&gt;The techniques to be developed will dramatically improve the efficiency&lt;br/&gt;of professional catalogers, leveraging automatic segmentation to&lt;br/&gt;suggest topic boundaries in interviews, using domain-tuned&lt;br/&gt;classification algorithms to recommend thesaurus terms, and&lt;br/&gt;providing automated tools to support generation of event timelines.&lt;br/&gt;Volunteers will help assign metadata and provide transcripts.&lt;br/&gt;Efforts will be made to automate transferring capabilities developed&lt;br/&gt;originally for English to other languages. Access to multilingual&lt;br/&gt;materials will be done by combining knowledge-based and corpus-based&lt;br/&gt;techniques to extend existing thesauri to new languages and by supporting&lt;br/&gt;cross-language searching of manually prepared segment-level&lt;br/&gt;summaries and automatic speech recognition transcripts.&lt;br/&gt;&lt;br/&gt;Each component will be evaluated and user studies done to measure&lt;br/&gt;the overall impact on support for cataloging, search and exploration.&lt;br/&gt;This will produce significant impact, both through improved access&lt;br/&gt;to our cultural heritage and through the application of the techniques&lt;br/&gt;to other important problems.&lt;br/&gt;&lt;br/&gt;The collection of spoken material used in this project will be the 116,000&lt;br/&gt;hours held by the Survivors of the Shoah Visual History Foundation, a&lt;br/&gt;set of already digitized video recordings of great historical importance.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">122466</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">122466</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n102" target="n1737">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR Large: MALACH: Multilingual Access to Large spoken ArChives</data>
      <data key="e_abstract">The project proposes to advance the state of the art in automatic&lt;br/&gt;speech recognition by detecting emotional and highly accented speech&lt;br/&gt;and differences based on age and gender, and then optimizing the&lt;br/&gt;acoustic model for those conditions. It will apply long-term&lt;br/&gt;adaptation techniques to improve robustness, and will implement&lt;br/&gt;an innovative second-pass decoding technique to improve accuracy&lt;br/&gt;by using side information such as thesaurus terms and&lt;br/&gt;human-prepared summaries.&lt;br/&gt;&lt;br/&gt;The techniques to be developed will dramatically improve the efficiency&lt;br/&gt;of professional catalogers, leveraging automatic segmentation to&lt;br/&gt;suggest topic boundaries in interviews, using domain-tuned&lt;br/&gt;classification algorithms to recommend thesaurus terms, and&lt;br/&gt;providing automated tools to support generation of event timelines.&lt;br/&gt;Volunteers will help assign metadata and provide transcripts.&lt;br/&gt;Efforts will be made to automate transferring capabilities developed&lt;br/&gt;originally for English to other languages. Access to multilingual&lt;br/&gt;materials will be done by combining knowledge-based and corpus-based&lt;br/&gt;techniques to extend existing thesauri to new languages and by supporting&lt;br/&gt;cross-language searching of manually prepared segment-level&lt;br/&gt;summaries and automatic speech recognition transcripts.&lt;br/&gt;&lt;br/&gt;Each component will be evaluated and user studies done to measure&lt;br/&gt;the overall impact on support for cataloging, search and exploration.&lt;br/&gt;This will produce significant impact, both through improved access&lt;br/&gt;to our cultural heritage and through the application of the techniques&lt;br/&gt;to other important problems.&lt;br/&gt;&lt;br/&gt;The collection of spoken material used in this project will be the 116,000&lt;br/&gt;hours held by the Survivors of the Shoah Visual History Foundation, a&lt;br/&gt;set of already digitized video recordings of great historical importance.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">122466</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">122466</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n102" target="n1738">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR Large: MALACH: Multilingual Access to Large spoken ArChives</data>
      <data key="e_abstract">The project proposes to advance the state of the art in automatic&lt;br/&gt;speech recognition by detecting emotional and highly accented speech&lt;br/&gt;and differences based on age and gender, and then optimizing the&lt;br/&gt;acoustic model for those conditions. It will apply long-term&lt;br/&gt;adaptation techniques to improve robustness, and will implement&lt;br/&gt;an innovative second-pass decoding technique to improve accuracy&lt;br/&gt;by using side information such as thesaurus terms and&lt;br/&gt;human-prepared summaries.&lt;br/&gt;&lt;br/&gt;The techniques to be developed will dramatically improve the efficiency&lt;br/&gt;of professional catalogers, leveraging automatic segmentation to&lt;br/&gt;suggest topic boundaries in interviews, using domain-tuned&lt;br/&gt;classification algorithms to recommend thesaurus terms, and&lt;br/&gt;providing automated tools to support generation of event timelines.&lt;br/&gt;Volunteers will help assign metadata and provide transcripts.&lt;br/&gt;Efforts will be made to automate transferring capabilities developed&lt;br/&gt;originally for English to other languages. Access to multilingual&lt;br/&gt;materials will be done by combining knowledge-based and corpus-based&lt;br/&gt;techniques to extend existing thesauri to new languages and by supporting&lt;br/&gt;cross-language searching of manually prepared segment-level&lt;br/&gt;summaries and automatic speech recognition transcripts.&lt;br/&gt;&lt;br/&gt;Each component will be evaluated and user studies done to measure&lt;br/&gt;the overall impact on support for cataloging, search and exploration.&lt;br/&gt;This will produce significant impact, both through improved access&lt;br/&gt;to our cultural heritage and through the application of the techniques&lt;br/&gt;to other important problems.&lt;br/&gt;&lt;br/&gt;The collection of spoken material used in this project will be the 116,000&lt;br/&gt;hours held by the Survivors of the Shoah Visual History Foundation, a&lt;br/&gt;set of already digitized video recordings of great historical importance.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">122466</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">122466</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n102" target="n1739">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR Large: MALACH: Multilingual Access to Large spoken ArChives</data>
      <data key="e_abstract">The project proposes to advance the state of the art in automatic&lt;br/&gt;speech recognition by detecting emotional and highly accented speech&lt;br/&gt;and differences based on age and gender, and then optimizing the&lt;br/&gt;acoustic model for those conditions. It will apply long-term&lt;br/&gt;adaptation techniques to improve robustness, and will implement&lt;br/&gt;an innovative second-pass decoding technique to improve accuracy&lt;br/&gt;by using side information such as thesaurus terms and&lt;br/&gt;human-prepared summaries.&lt;br/&gt;&lt;br/&gt;The techniques to be developed will dramatically improve the efficiency&lt;br/&gt;of professional catalogers, leveraging automatic segmentation to&lt;br/&gt;suggest topic boundaries in interviews, using domain-tuned&lt;br/&gt;classification algorithms to recommend thesaurus terms, and&lt;br/&gt;providing automated tools to support generation of event timelines.&lt;br/&gt;Volunteers will help assign metadata and provide transcripts.&lt;br/&gt;Efforts will be made to automate transferring capabilities developed&lt;br/&gt;originally for English to other languages. Access to multilingual&lt;br/&gt;materials will be done by combining knowledge-based and corpus-based&lt;br/&gt;techniques to extend existing thesauri to new languages and by supporting&lt;br/&gt;cross-language searching of manually prepared segment-level&lt;br/&gt;summaries and automatic speech recognition transcripts.&lt;br/&gt;&lt;br/&gt;Each component will be evaluated and user studies done to measure&lt;br/&gt;the overall impact on support for cataloging, search and exploration.&lt;br/&gt;This will produce significant impact, both through improved access&lt;br/&gt;to our cultural heritage and through the application of the techniques&lt;br/&gt;to other important problems.&lt;br/&gt;&lt;br/&gt;The collection of spoken material used in this project will be the 116,000&lt;br/&gt;hours held by the Survivors of the Shoah Visual History Foundation, a&lt;br/&gt;set of already digitized video recordings of great historical importance.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">122466</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">122466</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1737" target="n1738">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR Large: MALACH: Multilingual Access to Large spoken ArChives</data>
      <data key="e_abstract">The project proposes to advance the state of the art in automatic&lt;br/&gt;speech recognition by detecting emotional and highly accented speech&lt;br/&gt;and differences based on age and gender, and then optimizing the&lt;br/&gt;acoustic model for those conditions. It will apply long-term&lt;br/&gt;adaptation techniques to improve robustness, and will implement&lt;br/&gt;an innovative second-pass decoding technique to improve accuracy&lt;br/&gt;by using side information such as thesaurus terms and&lt;br/&gt;human-prepared summaries.&lt;br/&gt;&lt;br/&gt;The techniques to be developed will dramatically improve the efficiency&lt;br/&gt;of professional catalogers, leveraging automatic segmentation to&lt;br/&gt;suggest topic boundaries in interviews, using domain-tuned&lt;br/&gt;classification algorithms to recommend thesaurus terms, and&lt;br/&gt;providing automated tools to support generation of event timelines.&lt;br/&gt;Volunteers will help assign metadata and provide transcripts.&lt;br/&gt;Efforts will be made to automate transferring capabilities developed&lt;br/&gt;originally for English to other languages. Access to multilingual&lt;br/&gt;materials will be done by combining knowledge-based and corpus-based&lt;br/&gt;techniques to extend existing thesauri to new languages and by supporting&lt;br/&gt;cross-language searching of manually prepared segment-level&lt;br/&gt;summaries and automatic speech recognition transcripts.&lt;br/&gt;&lt;br/&gt;Each component will be evaluated and user studies done to measure&lt;br/&gt;the overall impact on support for cataloging, search and exploration.&lt;br/&gt;This will produce significant impact, both through improved access&lt;br/&gt;to our cultural heritage and through the application of the techniques&lt;br/&gt;to other important problems.&lt;br/&gt;&lt;br/&gt;The collection of spoken material used in this project will be the 116,000&lt;br/&gt;hours held by the Survivors of the Shoah Visual History Foundation, a&lt;br/&gt;set of already digitized video recordings of great historical importance.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">122466</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">122466</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1737" target="n1739">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR Large: MALACH: Multilingual Access to Large spoken ArChives</data>
      <data key="e_abstract">The project proposes to advance the state of the art in automatic&lt;br/&gt;speech recognition by detecting emotional and highly accented speech&lt;br/&gt;and differences based on age and gender, and then optimizing the&lt;br/&gt;acoustic model for those conditions. It will apply long-term&lt;br/&gt;adaptation techniques to improve robustness, and will implement&lt;br/&gt;an innovative second-pass decoding technique to improve accuracy&lt;br/&gt;by using side information such as thesaurus terms and&lt;br/&gt;human-prepared summaries.&lt;br/&gt;&lt;br/&gt;The techniques to be developed will dramatically improve the efficiency&lt;br/&gt;of professional catalogers, leveraging automatic segmentation to&lt;br/&gt;suggest topic boundaries in interviews, using domain-tuned&lt;br/&gt;classification algorithms to recommend thesaurus terms, and&lt;br/&gt;providing automated tools to support generation of event timelines.&lt;br/&gt;Volunteers will help assign metadata and provide transcripts.&lt;br/&gt;Efforts will be made to automate transferring capabilities developed&lt;br/&gt;originally for English to other languages. Access to multilingual&lt;br/&gt;materials will be done by combining knowledge-based and corpus-based&lt;br/&gt;techniques to extend existing thesauri to new languages and by supporting&lt;br/&gt;cross-language searching of manually prepared segment-level&lt;br/&gt;summaries and automatic speech recognition transcripts.&lt;br/&gt;&lt;br/&gt;Each component will be evaluated and user studies done to measure&lt;br/&gt;the overall impact on support for cataloging, search and exploration.&lt;br/&gt;This will produce significant impact, both through improved access&lt;br/&gt;to our cultural heritage and through the application of the techniques&lt;br/&gt;to other important problems.&lt;br/&gt;&lt;br/&gt;The collection of spoken material used in this project will be the 116,000&lt;br/&gt;hours held by the Survivors of the Shoah Visual History Foundation, a&lt;br/&gt;set of already digitized video recordings of great historical importance.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">122466</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">122466</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1738" target="n1739">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR Large: MALACH: Multilingual Access to Large spoken ArChives</data>
      <data key="e_abstract">The project proposes to advance the state of the art in automatic&lt;br/&gt;speech recognition by detecting emotional and highly accented speech&lt;br/&gt;and differences based on age and gender, and then optimizing the&lt;br/&gt;acoustic model for those conditions. It will apply long-term&lt;br/&gt;adaptation techniques to improve robustness, and will implement&lt;br/&gt;an innovative second-pass decoding technique to improve accuracy&lt;br/&gt;by using side information such as thesaurus terms and&lt;br/&gt;human-prepared summaries.&lt;br/&gt;&lt;br/&gt;The techniques to be developed will dramatically improve the efficiency&lt;br/&gt;of professional catalogers, leveraging automatic segmentation to&lt;br/&gt;suggest topic boundaries in interviews, using domain-tuned&lt;br/&gt;classification algorithms to recommend thesaurus terms, and&lt;br/&gt;providing automated tools to support generation of event timelines.&lt;br/&gt;Volunteers will help assign metadata and provide transcripts.&lt;br/&gt;Efforts will be made to automate transferring capabilities developed&lt;br/&gt;originally for English to other languages. Access to multilingual&lt;br/&gt;materials will be done by combining knowledge-based and corpus-based&lt;br/&gt;techniques to extend existing thesauri to new languages and by supporting&lt;br/&gt;cross-language searching of manually prepared segment-level&lt;br/&gt;summaries and automatic speech recognition transcripts.&lt;br/&gt;&lt;br/&gt;Each component will be evaluated and user studies done to measure&lt;br/&gt;the overall impact on support for cataloging, search and exploration.&lt;br/&gt;This will produce significant impact, both through improved access&lt;br/&gt;to our cultural heritage and through the application of the techniques&lt;br/&gt;to other important problems.&lt;br/&gt;&lt;br/&gt;The collection of spoken material used in this project will be the 116,000&lt;br/&gt;hours held by the Survivors of the Shoah Visual History Foundation, a&lt;br/&gt;set of already digitized video recordings of great historical importance.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">122466</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">122466</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n163" target="n368">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/AP (BIO) Computational tools for determining the 3-D static and dynamic structure of viruses</data>
      <data key="e_abstract">ITR Proposal 0112672, Doerschuk&lt;br/&gt;&lt;br/&gt;Abstract&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Understanding the dynamical behavior of virus particles, e.g., the self assembly and subsequent maturation steps that lead to an infectious virus particle, is a key challenge in basic biology (e.g., the understanding of protein-nucleic acid interactions) and in medicine (e.g., the development of drugs that interfere with viral replication). This research contributes to that goal by developing new computational tools for structural biology, emphasizing experimental approaches appropriate for the study of dynamics and emphasizing the bi-directional interplay of new structural biology problems and new formulations of existing structural biology problems with the development of new computational tools.&lt;br/&gt;&lt;br/&gt;Models and computation for new types of experiments are under development, e.g., x-ray scattering from solutions of labeled viral particles which are oriented by immersion in a strong electric field thereby transforming solution scattering to something analogous to the more informative fiber diffraction. New computational approaches to standard problems are also under development, e.g., phase retrieval for the x-ray crystallography of particles with non-crystallographic symmetries based on iterative phase retrieval algorithms applied to lattices that are computationally oversampled by interpolators based on the symmetry. Computational areas involved include fast 3-D Radon transform algorithms, a component of computing the cryo electron micrograph predicted from a given virus structure, and global optimization, in order to improve the values of the parameters that describe the virus structure.&lt;br/&gt;&lt;br/&gt;The PIs have a long term interest in undergraduate involvement in research and financial support for undergraduate research assistants is included.</data>
      <data key="e_pgm">1686</data>
      <data key="e_label">112672</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">112672</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1744" target="n1745">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI+AP: A Mobile Sensor Web for Polar Ice Sheet Measurements</data>
      <data key="e_abstract">0122520&lt;br/&gt;Gogineni&lt;br/&gt;&lt;br/&gt;Sea level has been rising over the last century. Although the immediate impact of sea level rise may be less severe than other effects of global climate change, the long-term consequences can be much more devastating since nearly 60% of the world population lives in coastal regions. Scientists have postulated that excess water is being released from polar ice sheets due to long-term, global climate change, but there are insufficient data to confirm these theories. Understanding the interactions between the ice sheets, oceans and atmosphere is essential to quantifying the role of ice sheets in sea level rise. Toward that end, this research project involves the innovative application of information technology in the development and deployment of intelligent radar sensors for measuring key glaciological parameters. &lt;br/&gt;&lt;br/&gt;Radar instrumentation will consist of a synthetic aperture radar (SAR) that can operate in bistatic or monostatic mode. One important application of the SAR will be in the determination of basal conditions, particularly the presence and distribution of basal water. Basal water lubricates the ice/bed interface, enhancing flow, and increasing the amount of ice discharged into the ocean. Another application of the SAR will be to measure ice thickness and map internal layers in both shallow and deep ice. Information on near-surface internal layers will be used to estimate the average, recent accumulation rate, while the deeper layers provide a history of past accumulation and flow rates. A tracked vehicle and an automated snowmobile will be used to test and demonstrate the utility of an intelligent radar in glaciological investigations.&lt;br/&gt;&lt;br/&gt;The system will be developed to collect, process and analyze data in real time and in conjunction with a priori information derived from archived sources. The combined real time and archived information will be used onboard the vehicles to select and generate an optimum sensor configuration. This project thus involves innovative research in intelligent systems, sounding radars and ice sheet modeling. In addition it has a very strong public outreach and education program, which include near-real-time image broadcasts via the world wide web</data>
      <data key="e_pgm">8393</data>
      <data key="e_label">122520</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0102</data>
      <data key="e_awardID">122520</data>
      <data key="e_dir">01</data>
    </edge>
    <edge source="n1744" target="n1746">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI+AP: A Mobile Sensor Web for Polar Ice Sheet Measurements</data>
      <data key="e_abstract">0122520&lt;br/&gt;Gogineni&lt;br/&gt;&lt;br/&gt;Sea level has been rising over the last century. Although the immediate impact of sea level rise may be less severe than other effects of global climate change, the long-term consequences can be much more devastating since nearly 60% of the world population lives in coastal regions. Scientists have postulated that excess water is being released from polar ice sheets due to long-term, global climate change, but there are insufficient data to confirm these theories. Understanding the interactions between the ice sheets, oceans and atmosphere is essential to quantifying the role of ice sheets in sea level rise. Toward that end, this research project involves the innovative application of information technology in the development and deployment of intelligent radar sensors for measuring key glaciological parameters. &lt;br/&gt;&lt;br/&gt;Radar instrumentation will consist of a synthetic aperture radar (SAR) that can operate in bistatic or monostatic mode. One important application of the SAR will be in the determination of basal conditions, particularly the presence and distribution of basal water. Basal water lubricates the ice/bed interface, enhancing flow, and increasing the amount of ice discharged into the ocean. Another application of the SAR will be to measure ice thickness and map internal layers in both shallow and deep ice. Information on near-surface internal layers will be used to estimate the average, recent accumulation rate, while the deeper layers provide a history of past accumulation and flow rates. A tracked vehicle and an automated snowmobile will be used to test and demonstrate the utility of an intelligent radar in glaciological investigations.&lt;br/&gt;&lt;br/&gt;The system will be developed to collect, process and analyze data in real time and in conjunction with a priori information derived from archived sources. The combined real time and archived information will be used onboard the vehicles to select and generate an optimum sensor configuration. This project thus involves innovative research in intelligent systems, sounding radars and ice sheet modeling. In addition it has a very strong public outreach and education program, which include near-real-time image broadcasts via the world wide web</data>
      <data key="e_pgm">8393</data>
      <data key="e_label">122520</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0102</data>
      <data key="e_awardID">122520</data>
      <data key="e_dir">01</data>
    </edge>
    <edge source="n1744" target="n1747">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI+AP: A Mobile Sensor Web for Polar Ice Sheet Measurements</data>
      <data key="e_abstract">0122520&lt;br/&gt;Gogineni&lt;br/&gt;&lt;br/&gt;Sea level has been rising over the last century. Although the immediate impact of sea level rise may be less severe than other effects of global climate change, the long-term consequences can be much more devastating since nearly 60% of the world population lives in coastal regions. Scientists have postulated that excess water is being released from polar ice sheets due to long-term, global climate change, but there are insufficient data to confirm these theories. Understanding the interactions between the ice sheets, oceans and atmosphere is essential to quantifying the role of ice sheets in sea level rise. Toward that end, this research project involves the innovative application of information technology in the development and deployment of intelligent radar sensors for measuring key glaciological parameters. &lt;br/&gt;&lt;br/&gt;Radar instrumentation will consist of a synthetic aperture radar (SAR) that can operate in bistatic or monostatic mode. One important application of the SAR will be in the determination of basal conditions, particularly the presence and distribution of basal water. Basal water lubricates the ice/bed interface, enhancing flow, and increasing the amount of ice discharged into the ocean. Another application of the SAR will be to measure ice thickness and map internal layers in both shallow and deep ice. Information on near-surface internal layers will be used to estimate the average, recent accumulation rate, while the deeper layers provide a history of past accumulation and flow rates. A tracked vehicle and an automated snowmobile will be used to test and demonstrate the utility of an intelligent radar in glaciological investigations.&lt;br/&gt;&lt;br/&gt;The system will be developed to collect, process and analyze data in real time and in conjunction with a priori information derived from archived sources. The combined real time and archived information will be used onboard the vehicles to select and generate an optimum sensor configuration. This project thus involves innovative research in intelligent systems, sounding radars and ice sheet modeling. In addition it has a very strong public outreach and education program, which include near-real-time image broadcasts via the world wide web</data>
      <data key="e_pgm">8393</data>
      <data key="e_label">122520</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0102</data>
      <data key="e_awardID">122520</data>
      <data key="e_dir">01</data>
    </edge>
    <edge source="n1744" target="n1748">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI+AP: A Mobile Sensor Web for Polar Ice Sheet Measurements</data>
      <data key="e_abstract">0122520&lt;br/&gt;Gogineni&lt;br/&gt;&lt;br/&gt;Sea level has been rising over the last century. Although the immediate impact of sea level rise may be less severe than other effects of global climate change, the long-term consequences can be much more devastating since nearly 60% of the world population lives in coastal regions. Scientists have postulated that excess water is being released from polar ice sheets due to long-term, global climate change, but there are insufficient data to confirm these theories. Understanding the interactions between the ice sheets, oceans and atmosphere is essential to quantifying the role of ice sheets in sea level rise. Toward that end, this research project involves the innovative application of information technology in the development and deployment of intelligent radar sensors for measuring key glaciological parameters. &lt;br/&gt;&lt;br/&gt;Radar instrumentation will consist of a synthetic aperture radar (SAR) that can operate in bistatic or monostatic mode. One important application of the SAR will be in the determination of basal conditions, particularly the presence and distribution of basal water. Basal water lubricates the ice/bed interface, enhancing flow, and increasing the amount of ice discharged into the ocean. Another application of the SAR will be to measure ice thickness and map internal layers in both shallow and deep ice. Information on near-surface internal layers will be used to estimate the average, recent accumulation rate, while the deeper layers provide a history of past accumulation and flow rates. A tracked vehicle and an automated snowmobile will be used to test and demonstrate the utility of an intelligent radar in glaciological investigations.&lt;br/&gt;&lt;br/&gt;The system will be developed to collect, process and analyze data in real time and in conjunction with a priori information derived from archived sources. The combined real time and archived information will be used onboard the vehicles to select and generate an optimum sensor configuration. This project thus involves innovative research in intelligent systems, sounding radars and ice sheet modeling. In addition it has a very strong public outreach and education program, which include near-real-time image broadcasts via the world wide web</data>
      <data key="e_pgm">8393</data>
      <data key="e_label">122520</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0102</data>
      <data key="e_awardID">122520</data>
      <data key="e_dir">01</data>
    </edge>
    <edge source="n1745" target="n1746">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI+AP: A Mobile Sensor Web for Polar Ice Sheet Measurements</data>
      <data key="e_abstract">0122520&lt;br/&gt;Gogineni&lt;br/&gt;&lt;br/&gt;Sea level has been rising over the last century. Although the immediate impact of sea level rise may be less severe than other effects of global climate change, the long-term consequences can be much more devastating since nearly 60% of the world population lives in coastal regions. Scientists have postulated that excess water is being released from polar ice sheets due to long-term, global climate change, but there are insufficient data to confirm these theories. Understanding the interactions between the ice sheets, oceans and atmosphere is essential to quantifying the role of ice sheets in sea level rise. Toward that end, this research project involves the innovative application of information technology in the development and deployment of intelligent radar sensors for measuring key glaciological parameters. &lt;br/&gt;&lt;br/&gt;Radar instrumentation will consist of a synthetic aperture radar (SAR) that can operate in bistatic or monostatic mode. One important application of the SAR will be in the determination of basal conditions, particularly the presence and distribution of basal water. Basal water lubricates the ice/bed interface, enhancing flow, and increasing the amount of ice discharged into the ocean. Another application of the SAR will be to measure ice thickness and map internal layers in both shallow and deep ice. Information on near-surface internal layers will be used to estimate the average, recent accumulation rate, while the deeper layers provide a history of past accumulation and flow rates. A tracked vehicle and an automated snowmobile will be used to test and demonstrate the utility of an intelligent radar in glaciological investigations.&lt;br/&gt;&lt;br/&gt;The system will be developed to collect, process and analyze data in real time and in conjunction with a priori information derived from archived sources. The combined real time and archived information will be used onboard the vehicles to select and generate an optimum sensor configuration. This project thus involves innovative research in intelligent systems, sounding radars and ice sheet modeling. In addition it has a very strong public outreach and education program, which include near-real-time image broadcasts via the world wide web</data>
      <data key="e_pgm">8393</data>
      <data key="e_label">122520</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0102</data>
      <data key="e_awardID">122520</data>
      <data key="e_dir">01</data>
    </edge>
    <edge source="n1745" target="n1747">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI+AP: A Mobile Sensor Web for Polar Ice Sheet Measurements</data>
      <data key="e_abstract">0122520&lt;br/&gt;Gogineni&lt;br/&gt;&lt;br/&gt;Sea level has been rising over the last century. Although the immediate impact of sea level rise may be less severe than other effects of global climate change, the long-term consequences can be much more devastating since nearly 60% of the world population lives in coastal regions. Scientists have postulated that excess water is being released from polar ice sheets due to long-term, global climate change, but there are insufficient data to confirm these theories. Understanding the interactions between the ice sheets, oceans and atmosphere is essential to quantifying the role of ice sheets in sea level rise. Toward that end, this research project involves the innovative application of information technology in the development and deployment of intelligent radar sensors for measuring key glaciological parameters. &lt;br/&gt;&lt;br/&gt;Radar instrumentation will consist of a synthetic aperture radar (SAR) that can operate in bistatic or monostatic mode. One important application of the SAR will be in the determination of basal conditions, particularly the presence and distribution of basal water. Basal water lubricates the ice/bed interface, enhancing flow, and increasing the amount of ice discharged into the ocean. Another application of the SAR will be to measure ice thickness and map internal layers in both shallow and deep ice. Information on near-surface internal layers will be used to estimate the average, recent accumulation rate, while the deeper layers provide a history of past accumulation and flow rates. A tracked vehicle and an automated snowmobile will be used to test and demonstrate the utility of an intelligent radar in glaciological investigations.&lt;br/&gt;&lt;br/&gt;The system will be developed to collect, process and analyze data in real time and in conjunction with a priori information derived from archived sources. The combined real time and archived information will be used onboard the vehicles to select and generate an optimum sensor configuration. This project thus involves innovative research in intelligent systems, sounding radars and ice sheet modeling. In addition it has a very strong public outreach and education program, which include near-real-time image broadcasts via the world wide web</data>
      <data key="e_pgm">8393</data>
      <data key="e_label">122520</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0102</data>
      <data key="e_awardID">122520</data>
      <data key="e_dir">01</data>
    </edge>
    <edge source="n1745" target="n1748">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI+AP: A Mobile Sensor Web for Polar Ice Sheet Measurements</data>
      <data key="e_abstract">0122520&lt;br/&gt;Gogineni&lt;br/&gt;&lt;br/&gt;Sea level has been rising over the last century. Although the immediate impact of sea level rise may be less severe than other effects of global climate change, the long-term consequences can be much more devastating since nearly 60% of the world population lives in coastal regions. Scientists have postulated that excess water is being released from polar ice sheets due to long-term, global climate change, but there are insufficient data to confirm these theories. Understanding the interactions between the ice sheets, oceans and atmosphere is essential to quantifying the role of ice sheets in sea level rise. Toward that end, this research project involves the innovative application of information technology in the development and deployment of intelligent radar sensors for measuring key glaciological parameters. &lt;br/&gt;&lt;br/&gt;Radar instrumentation will consist of a synthetic aperture radar (SAR) that can operate in bistatic or monostatic mode. One important application of the SAR will be in the determination of basal conditions, particularly the presence and distribution of basal water. Basal water lubricates the ice/bed interface, enhancing flow, and increasing the amount of ice discharged into the ocean. Another application of the SAR will be to measure ice thickness and map internal layers in both shallow and deep ice. Information on near-surface internal layers will be used to estimate the average, recent accumulation rate, while the deeper layers provide a history of past accumulation and flow rates. A tracked vehicle and an automated snowmobile will be used to test and demonstrate the utility of an intelligent radar in glaciological investigations.&lt;br/&gt;&lt;br/&gt;The system will be developed to collect, process and analyze data in real time and in conjunction with a priori information derived from archived sources. The combined real time and archived information will be used onboard the vehicles to select and generate an optimum sensor configuration. This project thus involves innovative research in intelligent systems, sounding radars and ice sheet modeling. In addition it has a very strong public outreach and education program, which include near-real-time image broadcasts via the world wide web</data>
      <data key="e_pgm">8393</data>
      <data key="e_label">122520</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0102</data>
      <data key="e_awardID">122520</data>
      <data key="e_dir">01</data>
    </edge>
    <edge source="n1746" target="n1747">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI+AP: A Mobile Sensor Web for Polar Ice Sheet Measurements</data>
      <data key="e_abstract">0122520&lt;br/&gt;Gogineni&lt;br/&gt;&lt;br/&gt;Sea level has been rising over the last century. Although the immediate impact of sea level rise may be less severe than other effects of global climate change, the long-term consequences can be much more devastating since nearly 60% of the world population lives in coastal regions. Scientists have postulated that excess water is being released from polar ice sheets due to long-term, global climate change, but there are insufficient data to confirm these theories. Understanding the interactions between the ice sheets, oceans and atmosphere is essential to quantifying the role of ice sheets in sea level rise. Toward that end, this research project involves the innovative application of information technology in the development and deployment of intelligent radar sensors for measuring key glaciological parameters. &lt;br/&gt;&lt;br/&gt;Radar instrumentation will consist of a synthetic aperture radar (SAR) that can operate in bistatic or monostatic mode. One important application of the SAR will be in the determination of basal conditions, particularly the presence and distribution of basal water. Basal water lubricates the ice/bed interface, enhancing flow, and increasing the amount of ice discharged into the ocean. Another application of the SAR will be to measure ice thickness and map internal layers in both shallow and deep ice. Information on near-surface internal layers will be used to estimate the average, recent accumulation rate, while the deeper layers provide a history of past accumulation and flow rates. A tracked vehicle and an automated snowmobile will be used to test and demonstrate the utility of an intelligent radar in glaciological investigations.&lt;br/&gt;&lt;br/&gt;The system will be developed to collect, process and analyze data in real time and in conjunction with a priori information derived from archived sources. The combined real time and archived information will be used onboard the vehicles to select and generate an optimum sensor configuration. This project thus involves innovative research in intelligent systems, sounding radars and ice sheet modeling. In addition it has a very strong public outreach and education program, which include near-real-time image broadcasts via the world wide web</data>
      <data key="e_pgm">8393</data>
      <data key="e_label">122520</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0102</data>
      <data key="e_awardID">122520</data>
      <data key="e_dir">01</data>
    </edge>
    <edge source="n1746" target="n1748">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI+AP: A Mobile Sensor Web for Polar Ice Sheet Measurements</data>
      <data key="e_abstract">0122520&lt;br/&gt;Gogineni&lt;br/&gt;&lt;br/&gt;Sea level has been rising over the last century. Although the immediate impact of sea level rise may be less severe than other effects of global climate change, the long-term consequences can be much more devastating since nearly 60% of the world population lives in coastal regions. Scientists have postulated that excess water is being released from polar ice sheets due to long-term, global climate change, but there are insufficient data to confirm these theories. Understanding the interactions between the ice sheets, oceans and atmosphere is essential to quantifying the role of ice sheets in sea level rise. Toward that end, this research project involves the innovative application of information technology in the development and deployment of intelligent radar sensors for measuring key glaciological parameters. &lt;br/&gt;&lt;br/&gt;Radar instrumentation will consist of a synthetic aperture radar (SAR) that can operate in bistatic or monostatic mode. One important application of the SAR will be in the determination of basal conditions, particularly the presence and distribution of basal water. Basal water lubricates the ice/bed interface, enhancing flow, and increasing the amount of ice discharged into the ocean. Another application of the SAR will be to measure ice thickness and map internal layers in both shallow and deep ice. Information on near-surface internal layers will be used to estimate the average, recent accumulation rate, while the deeper layers provide a history of past accumulation and flow rates. A tracked vehicle and an automated snowmobile will be used to test and demonstrate the utility of an intelligent radar in glaciological investigations.&lt;br/&gt;&lt;br/&gt;The system will be developed to collect, process and analyze data in real time and in conjunction with a priori information derived from archived sources. The combined real time and archived information will be used onboard the vehicles to select and generate an optimum sensor configuration. This project thus involves innovative research in intelligent systems, sounding radars and ice sheet modeling. In addition it has a very strong public outreach and education program, which include near-real-time image broadcasts via the world wide web</data>
      <data key="e_pgm">8393</data>
      <data key="e_label">122520</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0102</data>
      <data key="e_awardID">122520</data>
      <data key="e_dir">01</data>
    </edge>
    <edge source="n1747" target="n1748">
      <data key="e_effectiveDate">2001-10-01</data>
      <data key="e_title">ITR/SI+AP: A Mobile Sensor Web for Polar Ice Sheet Measurements</data>
      <data key="e_abstract">0122520&lt;br/&gt;Gogineni&lt;br/&gt;&lt;br/&gt;Sea level has been rising over the last century. Although the immediate impact of sea level rise may be less severe than other effects of global climate change, the long-term consequences can be much more devastating since nearly 60% of the world population lives in coastal regions. Scientists have postulated that excess water is being released from polar ice sheets due to long-term, global climate change, but there are insufficient data to confirm these theories. Understanding the interactions between the ice sheets, oceans and atmosphere is essential to quantifying the role of ice sheets in sea level rise. Toward that end, this research project involves the innovative application of information technology in the development and deployment of intelligent radar sensors for measuring key glaciological parameters. &lt;br/&gt;&lt;br/&gt;Radar instrumentation will consist of a synthetic aperture radar (SAR) that can operate in bistatic or monostatic mode. One important application of the SAR will be in the determination of basal conditions, particularly the presence and distribution of basal water. Basal water lubricates the ice/bed interface, enhancing flow, and increasing the amount of ice discharged into the ocean. Another application of the SAR will be to measure ice thickness and map internal layers in both shallow and deep ice. Information on near-surface internal layers will be used to estimate the average, recent accumulation rate, while the deeper layers provide a history of past accumulation and flow rates. A tracked vehicle and an automated snowmobile will be used to test and demonstrate the utility of an intelligent radar in glaciological investigations.&lt;br/&gt;&lt;br/&gt;The system will be developed to collect, process and analyze data in real time and in conjunction with a priori information derived from archived sources. The combined real time and archived information will be used onboard the vehicles to select and generate an optimum sensor configuration. This project thus involves innovative research in intelligent systems, sounding radars and ice sheet modeling. In addition it has a very strong public outreach and education program, which include near-real-time image broadcasts via the world wide web</data>
      <data key="e_pgm">8393</data>
      <data key="e_label">122520</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0102</data>
      <data key="e_awardID">122520</data>
      <data key="e_dir">01</data>
    </edge>
    <edge source="n301" target="n1749">
      <data key="e_effectiveDate">2001-11-01</data>
      <data key="e_title">SGER: Flexible Index Structure for Relevance Feedback Content-Based Retrieval in Large Image Databases</data>
      <data key="e_abstract">The objective of this project is to design efficient indexing strategies that support flexible retrieval metric through relevance feedback learning. However, trying to satisfy both goals (efficiency and flexibility) at the same time leads to a conflict. A novel approach is explored to capture the inherent interplay between flexible metrics and indexing that has the potential to resolve the conflict. It is hypothesized that the interplay can be exploited to create effective content-based retrieval systems that meet performance and computational challenges encountered in practical image database applications. This exploratory project seeks to establish the proof of concept of an approach that trades off accuracy for efficiency, and that can avoid exhaustive search in large-scale image databases. The methods to be explored are based on bump-hunting in high-dimensional data for inducing a set of (possibly overlapping) boxes that capture the local data distributions. The induced boxes effectively cover the feature space, thereby providing an index to the image database. The flexibility and efficiency of the novel indexing technique will be tested in heterogeneous image databases that support a variety of query types, ranging from query-by-image to query-by-region. If successful, the results of this project will enable the use of flexible metric learning in large scale image databases, which will have a significant impact in content-based image retrieval in broad areas such as health-care, scientific images, education, or art.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">136348</data>
      <data key="e_expirationDate">2002-11-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">136348</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n578" target="n579">
      <data key="e_effectiveDate">2001-11-15</data>
      <data key="e_title">Million Book Project Meeting</data>
      <data key="e_abstract">This proposal will fund a workshop which brings together leading US librarians and computer specialists to address the needs for quality information in digital libraries. It will address selection of materials for scanning and content needs for large-scale digital libraries, in conjunction with the major scanning project being directed by CMU. It will survey national needs and opportunities in the digital content area.&lt;br/&gt;&lt;br/&gt;The workshop will take place in Pittsburgh, Pennsylvania on November 15 and 16, 2001.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">131312</data>
      <data key="e_expirationDate">2003-10-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">131312</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n834" target="n1755">
      <data key="e_effectiveDate">2001-11-01</data>
      <data key="e_title">Ultra-High-Capacity Optical Communications and Networking: Optical CDMA with Femtosecond Pulses for Ultra-High-Capacity Communications and Networking</data>
      <data key="e_abstract">In this proposal, three researchers from the University of California, San Diego (UCSD), specializing in the fields of optics, communications, and computer networks, are collaborating on the Ultra-High-Capacity Optical Communications and Networking initiative. It is felt by many researchers that the most efficient and economical way to utilize optical transmission technology for large scale networking is to use wavelength division multiplexing (WDM) in a circuit switched mode, overlaid with packet switching implemented with electronics. While this may indeed be the case, it is important to investigate alternative approaches that have great potential. The UCSD team has been investigating novel techniques of information transmission via optical fiber, where code division multiple access (CDMA) using ultrashort laser pulses is employed. Compact, low cost fiber-based ultrashort pulse sources are currently being developed, making the technology suitable for future practical networks. When an ultrashort pulse is encoded for CDMA, the pulse spreads out in time and resembles a noise burst that is transmitted on the optical fiber. At the receiving node, a decoder is applied to the received signals from multiple users, which matches only the encoding of the desired transmitter. The matching signal component is transformed back to an ultrashort pulse form that can be detected over the remaining interference from other users with nonlinear optical techniques. A novel high resolution pulse synthesis and detection technique for ultrashort pulses developed at UCSD enable various data transmission formats to be considered, such as ultrafast packet transmission with on/off keying, pulse position modulation, and amplitude modulation. The CDMA scheme enables large scale, asynchronous, concurrent access to the transmission resources. With a suitable architecture, this can be exploited to simplify network control, and increase reliability and flexibility.&lt;br/&gt;&lt;br/&gt; The objective of this proposal is to conduct basic research by investigating theoretically and verifying&lt;br/&gt;experimentally data modulation schemes for efficient information transmission in conjunction with CDMA encoded ultrashort pulses in an optical fiber network. Efficient modulation formats will result in aggregate transmission rates exceeding 10&apos;s of terabits/second, with individual user rates on the order of 1-10 gigabits/second. The specific objectives of this proposal include modeling of the optical CDMA for ultrashort Gaussian pulses, complete statistical analysis of the transmitted waveforms, investigation of various optical CDMA codes that support thousands of users with minimal interference, bit error rate analysis of received optical signals for various modulation schemes, modeling and characterization of the distortions induced by the fiber channel, adaptive equalization techniques for reducing dispersion and other fiber distortions, computer simulations of the modulation schemes, and experimental evaluation of the communication system: transmitter, optical channel, and receiver. The various phases of the proposed project complement each other. Combined together, they provide for in-depth knowledge of the theoretical and experimental issues of communicating with CDMA encoded ultrashort pulses. These findings will be shared with the scientific community, enhancing not only the knowledge base of other researchers in the field, but also of the students conducting the research. We shall demonstrate a prototype optical network with several users employing the modulation format that will carry over 10 terabits per second of information, when scaled up to the full number of users.&lt;br/&gt;&lt;br/&gt; The potential impact of the work will be in the proof that optical CDMA encoding of ultrashort pulses is a&lt;br/&gt;realizable and desirable alternative to WDM. Currently, WDM is the preferred multiplexing method due to its&lt;br/&gt;simplicity and low cost. While WDM does increase the transmitted bandwidth significantly, it still does not fully utilize the available optical bandwidth due to both the need for guard bands between channels and the under utilization of channels. In contrast, CDMA encoded ultrashort pulses share the entire bandwidth without the need for guard bands, leading to efficient utilization of transmission resources. Using CDMA can also provide a highly flexible and robust infrastructure, upon which packet switching can be overlaid. The CDMA format also provides a degree of security, as no data can be extracted without knowledge of the codes employed.</data>
      <data key="e_pgm">4090</data>
      <data key="e_label">123405</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">123405</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1755" target="n1757">
      <data key="e_effectiveDate">2001-11-01</data>
      <data key="e_title">Ultra-High-Capacity Optical Communications and Networking: Optical CDMA with Femtosecond Pulses for Ultra-High-Capacity Communications and Networking</data>
      <data key="e_abstract">In this proposal, three researchers from the University of California, San Diego (UCSD), specializing in the fields of optics, communications, and computer networks, are collaborating on the Ultra-High-Capacity Optical Communications and Networking initiative. It is felt by many researchers that the most efficient and economical way to utilize optical transmission technology for large scale networking is to use wavelength division multiplexing (WDM) in a circuit switched mode, overlaid with packet switching implemented with electronics. While this may indeed be the case, it is important to investigate alternative approaches that have great potential. The UCSD team has been investigating novel techniques of information transmission via optical fiber, where code division multiple access (CDMA) using ultrashort laser pulses is employed. Compact, low cost fiber-based ultrashort pulse sources are currently being developed, making the technology suitable for future practical networks. When an ultrashort pulse is encoded for CDMA, the pulse spreads out in time and resembles a noise burst that is transmitted on the optical fiber. At the receiving node, a decoder is applied to the received signals from multiple users, which matches only the encoding of the desired transmitter. The matching signal component is transformed back to an ultrashort pulse form that can be detected over the remaining interference from other users with nonlinear optical techniques. A novel high resolution pulse synthesis and detection technique for ultrashort pulses developed at UCSD enable various data transmission formats to be considered, such as ultrafast packet transmission with on/off keying, pulse position modulation, and amplitude modulation. The CDMA scheme enables large scale, asynchronous, concurrent access to the transmission resources. With a suitable architecture, this can be exploited to simplify network control, and increase reliability and flexibility.&lt;br/&gt;&lt;br/&gt; The objective of this proposal is to conduct basic research by investigating theoretically and verifying&lt;br/&gt;experimentally data modulation schemes for efficient information transmission in conjunction with CDMA encoded ultrashort pulses in an optical fiber network. Efficient modulation formats will result in aggregate transmission rates exceeding 10&apos;s of terabits/second, with individual user rates on the order of 1-10 gigabits/second. The specific objectives of this proposal include modeling of the optical CDMA for ultrashort Gaussian pulses, complete statistical analysis of the transmitted waveforms, investigation of various optical CDMA codes that support thousands of users with minimal interference, bit error rate analysis of received optical signals for various modulation schemes, modeling and characterization of the distortions induced by the fiber channel, adaptive equalization techniques for reducing dispersion and other fiber distortions, computer simulations of the modulation schemes, and experimental evaluation of the communication system: transmitter, optical channel, and receiver. The various phases of the proposed project complement each other. Combined together, they provide for in-depth knowledge of the theoretical and experimental issues of communicating with CDMA encoded ultrashort pulses. These findings will be shared with the scientific community, enhancing not only the knowledge base of other researchers in the field, but also of the students conducting the research. We shall demonstrate a prototype optical network with several users employing the modulation format that will carry over 10 terabits per second of information, when scaled up to the full number of users.&lt;br/&gt;&lt;br/&gt; The potential impact of the work will be in the proof that optical CDMA encoding of ultrashort pulses is a&lt;br/&gt;realizable and desirable alternative to WDM. Currently, WDM is the preferred multiplexing method due to its&lt;br/&gt;simplicity and low cost. While WDM does increase the transmitted bandwidth significantly, it still does not fully utilize the available optical bandwidth due to both the need for guard bands between channels and the under utilization of channels. In contrast, CDMA encoded ultrashort pulses share the entire bandwidth without the need for guard bands, leading to efficient utilization of transmission resources. Using CDMA can also provide a highly flexible and robust infrastructure, upon which packet switching can be overlaid. The CDMA format also provides a degree of security, as no data can be extracted without knowledge of the codes employed.</data>
      <data key="e_pgm">4090</data>
      <data key="e_label">123405</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">123405</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n834" target="n1757">
      <data key="e_effectiveDate">2001-11-01</data>
      <data key="e_title">Ultra-High-Capacity Optical Communications and Networking: Optical CDMA with Femtosecond Pulses for Ultra-High-Capacity Communications and Networking</data>
      <data key="e_abstract">In this proposal, three researchers from the University of California, San Diego (UCSD), specializing in the fields of optics, communications, and computer networks, are collaborating on the Ultra-High-Capacity Optical Communications and Networking initiative. It is felt by many researchers that the most efficient and economical way to utilize optical transmission technology for large scale networking is to use wavelength division multiplexing (WDM) in a circuit switched mode, overlaid with packet switching implemented with electronics. While this may indeed be the case, it is important to investigate alternative approaches that have great potential. The UCSD team has been investigating novel techniques of information transmission via optical fiber, where code division multiple access (CDMA) using ultrashort laser pulses is employed. Compact, low cost fiber-based ultrashort pulse sources are currently being developed, making the technology suitable for future practical networks. When an ultrashort pulse is encoded for CDMA, the pulse spreads out in time and resembles a noise burst that is transmitted on the optical fiber. At the receiving node, a decoder is applied to the received signals from multiple users, which matches only the encoding of the desired transmitter. The matching signal component is transformed back to an ultrashort pulse form that can be detected over the remaining interference from other users with nonlinear optical techniques. A novel high resolution pulse synthesis and detection technique for ultrashort pulses developed at UCSD enable various data transmission formats to be considered, such as ultrafast packet transmission with on/off keying, pulse position modulation, and amplitude modulation. The CDMA scheme enables large scale, asynchronous, concurrent access to the transmission resources. With a suitable architecture, this can be exploited to simplify network control, and increase reliability and flexibility.&lt;br/&gt;&lt;br/&gt; The objective of this proposal is to conduct basic research by investigating theoretically and verifying&lt;br/&gt;experimentally data modulation schemes for efficient information transmission in conjunction with CDMA encoded ultrashort pulses in an optical fiber network. Efficient modulation formats will result in aggregate transmission rates exceeding 10&apos;s of terabits/second, with individual user rates on the order of 1-10 gigabits/second. The specific objectives of this proposal include modeling of the optical CDMA for ultrashort Gaussian pulses, complete statistical analysis of the transmitted waveforms, investigation of various optical CDMA codes that support thousands of users with minimal interference, bit error rate analysis of received optical signals for various modulation schemes, modeling and characterization of the distortions induced by the fiber channel, adaptive equalization techniques for reducing dispersion and other fiber distortions, computer simulations of the modulation schemes, and experimental evaluation of the communication system: transmitter, optical channel, and receiver. The various phases of the proposed project complement each other. Combined together, they provide for in-depth knowledge of the theoretical and experimental issues of communicating with CDMA encoded ultrashort pulses. These findings will be shared with the scientific community, enhancing not only the knowledge base of other researchers in the field, but also of the students conducting the research. We shall demonstrate a prototype optical network with several users employing the modulation format that will carry over 10 terabits per second of information, when scaled up to the full number of users.&lt;br/&gt;&lt;br/&gt; The potential impact of the work will be in the proof that optical CDMA encoding of ultrashort pulses is a&lt;br/&gt;realizable and desirable alternative to WDM. Currently, WDM is the preferred multiplexing method due to its&lt;br/&gt;simplicity and low cost. While WDM does increase the transmitted bandwidth significantly, it still does not fully utilize the available optical bandwidth due to both the need for guard bands between channels and the under utilization of channels. In contrast, CDMA encoded ultrashort pulses share the entire bandwidth without the need for guard bands, leading to efficient utilization of transmission resources. Using CDMA can also provide a highly flexible and robust infrastructure, upon which packet switching can be overlaid. The CDMA format also provides a degree of security, as no data can be extracted without knowledge of the codes employed.</data>
      <data key="e_pgm">4090</data>
      <data key="e_label">123405</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">123405</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n154" target="n1759">
      <data key="e_effectiveDate">2001-11-01</data>
      <data key="e_title">Image Coding for Constrained Systems</data>
      <data key="e_abstract">TITLE: Image Coding with Constraints&lt;br/&gt;&lt;br/&gt;ABSTRACT&lt;br/&gt;&lt;br/&gt;This project studies image and video coding schemes for emerging and future&lt;br/&gt;networks. The study concentrates on certain specific types of image coding&lt;br/&gt;including facsimile, still imagery, and video. Networks of the future will&lt;br/&gt;consist of hybrid combinations of wired and wireless components. Commercial&lt;br/&gt;products such as digital cameras, cell phones, and PDAs will send and/or&lt;br/&gt;receive images of various types over a wide range of network channel&lt;br/&gt;conditions. The networks impose constraints such as: low data rate and&lt;br/&gt;high/unpredictable packet losses. The devices impose constraints including&lt;br/&gt;small batteries, small displays, low delay, limited memory, and limited&lt;br/&gt;processing power. Together, these constraints make the design of inexpensive&lt;br/&gt;and efficient image transmission devices of the future a very challenging task.&lt;br/&gt;In addition to designing such systems, a solid theoretical understanding of the&lt;br/&gt;achievable qualities and limitations is important to know.&lt;br/&gt;&lt;br/&gt;The main objectives of this research are to achieve deep theoretical&lt;br/&gt;understanding of source and channel coding for images transmitted on lossy&lt;br/&gt;networks and to develop practical algorithms that can be effectively used in&lt;br/&gt;real applications. The work exploits the diverse backgrounds of the PIs in&lt;br/&gt;image and video coding and combined source/channel coding. The investigation&lt;br/&gt;involves code design, theoretical analysis, and computer simulation.&lt;br/&gt;The main topics investigated for constrained image coding include:&lt;br/&gt;&lt;br/&gt;(1) Robust facsimile transmission,&lt;br/&gt;(2) Robust low rate video source coding,&lt;br/&gt;(3) Error correction, resilience, and concealment.</data>
      <data key="e_pgm">4720</data>
      <data key="e_label">105734</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">105734</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n263" target="n1761">
      <data key="e_effectiveDate">2001-11-01</data>
      <data key="e_title">Next Generation Software: A Framework For Developing Complex Applications On High-End Petaflop-Class Machines</data>
      <data key="e_abstract">0103723-University of Delaware-Guang R. Gao &lt;br/&gt;&lt;br/&gt;A Framework for Developing Complex Applications On High-End Petaflop-Class Machines&lt;br/&gt;&lt;br/&gt;This research proposes to develop novel software technologies for supporting the design and implementation of complex applications on next-generation high-performance computers such as the IBM Blue Gene machine, the teraflop-class systems of the ASCI program, and the architecture (s) proposed in the NSF peta-flops point-design studies.&lt;br/&gt;&lt;br/&gt;Future high-performance computers such as these will have large numbers of processors (from a few thousand to several hundred thousands, or even more), and a complex multilevel memory hierarchy with memories physically distributed across different parts of the machine. To use such machines effectively, enormous amounts of parallelism must be exposed in user programs, and careful attention must be paid to the latency and bandwidth of access to different levels of the memory hierarchy. The applications enabled by such high-end machines are also expected to be significantly more complex and dynamic than applications in the past. Therefore, supporting the development of such applications on next-generation high-end computers is a major software challenge.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103723</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103723</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n263" target="n1762">
      <data key="e_effectiveDate">2001-11-01</data>
      <data key="e_title">Next Generation Software: A Framework For Developing Complex Applications On High-End Petaflop-Class Machines</data>
      <data key="e_abstract">0103723-University of Delaware-Guang R. Gao &lt;br/&gt;&lt;br/&gt;A Framework for Developing Complex Applications On High-End Petaflop-Class Machines&lt;br/&gt;&lt;br/&gt;This research proposes to develop novel software technologies for supporting the design and implementation of complex applications on next-generation high-performance computers such as the IBM Blue Gene machine, the teraflop-class systems of the ASCI program, and the architecture (s) proposed in the NSF peta-flops point-design studies.&lt;br/&gt;&lt;br/&gt;Future high-performance computers such as these will have large numbers of processors (from a few thousand to several hundred thousands, or even more), and a complex multilevel memory hierarchy with memories physically distributed across different parts of the machine. To use such machines effectively, enormous amounts of parallelism must be exposed in user programs, and careful attention must be paid to the latency and bandwidth of access to different levels of the memory hierarchy. The applications enabled by such high-end machines are also expected to be significantly more complex and dynamic than applications in the past. Therefore, supporting the development of such applications on next-generation high-end computers is a major software challenge.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103723</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103723</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n263" target="n375">
      <data key="e_effectiveDate">2001-11-01</data>
      <data key="e_title">Next Generation Software: A Framework For Developing Complex Applications On High-End Petaflop-Class Machines</data>
      <data key="e_abstract">0103723-University of Delaware-Guang R. Gao &lt;br/&gt;&lt;br/&gt;A Framework for Developing Complex Applications On High-End Petaflop-Class Machines&lt;br/&gt;&lt;br/&gt;This research proposes to develop novel software technologies for supporting the design and implementation of complex applications on next-generation high-performance computers such as the IBM Blue Gene machine, the teraflop-class systems of the ASCI program, and the architecture (s) proposed in the NSF peta-flops point-design studies.&lt;br/&gt;&lt;br/&gt;Future high-performance computers such as these will have large numbers of processors (from a few thousand to several hundred thousands, or even more), and a complex multilevel memory hierarchy with memories physically distributed across different parts of the machine. To use such machines effectively, enormous amounts of parallelism must be exposed in user programs, and careful attention must be paid to the latency and bandwidth of access to different levels of the memory hierarchy. The applications enabled by such high-end machines are also expected to be significantly more complex and dynamic than applications in the past. Therefore, supporting the development of such applications on next-generation high-end computers is a major software challenge.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103723</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103723</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n263" target="n1764">
      <data key="e_effectiveDate">2001-11-01</data>
      <data key="e_title">Next Generation Software: A Framework For Developing Complex Applications On High-End Petaflop-Class Machines</data>
      <data key="e_abstract">0103723-University of Delaware-Guang R. Gao &lt;br/&gt;&lt;br/&gt;A Framework for Developing Complex Applications On High-End Petaflop-Class Machines&lt;br/&gt;&lt;br/&gt;This research proposes to develop novel software technologies for supporting the design and implementation of complex applications on next-generation high-performance computers such as the IBM Blue Gene machine, the teraflop-class systems of the ASCI program, and the architecture (s) proposed in the NSF peta-flops point-design studies.&lt;br/&gt;&lt;br/&gt;Future high-performance computers such as these will have large numbers of processors (from a few thousand to several hundred thousands, or even more), and a complex multilevel memory hierarchy with memories physically distributed across different parts of the machine. To use such machines effectively, enormous amounts of parallelism must be exposed in user programs, and careful attention must be paid to the latency and bandwidth of access to different levels of the memory hierarchy. The applications enabled by such high-end machines are also expected to be significantly more complex and dynamic than applications in the past. Therefore, supporting the development of such applications on next-generation high-end computers is a major software challenge.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103723</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103723</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1761" target="n1762">
      <data key="e_effectiveDate">2001-11-01</data>
      <data key="e_title">Next Generation Software: A Framework For Developing Complex Applications On High-End Petaflop-Class Machines</data>
      <data key="e_abstract">0103723-University of Delaware-Guang R. Gao &lt;br/&gt;&lt;br/&gt;A Framework for Developing Complex Applications On High-End Petaflop-Class Machines&lt;br/&gt;&lt;br/&gt;This research proposes to develop novel software technologies for supporting the design and implementation of complex applications on next-generation high-performance computers such as the IBM Blue Gene machine, the teraflop-class systems of the ASCI program, and the architecture (s) proposed in the NSF peta-flops point-design studies.&lt;br/&gt;&lt;br/&gt;Future high-performance computers such as these will have large numbers of processors (from a few thousand to several hundred thousands, or even more), and a complex multilevel memory hierarchy with memories physically distributed across different parts of the machine. To use such machines effectively, enormous amounts of parallelism must be exposed in user programs, and careful attention must be paid to the latency and bandwidth of access to different levels of the memory hierarchy. The applications enabled by such high-end machines are also expected to be significantly more complex and dynamic than applications in the past. Therefore, supporting the development of such applications on next-generation high-end computers is a major software challenge.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103723</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103723</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n375" target="n1761">
      <data key="e_effectiveDate">2001-11-01</data>
      <data key="e_title">Next Generation Software: A Framework For Developing Complex Applications On High-End Petaflop-Class Machines</data>
      <data key="e_abstract">0103723-University of Delaware-Guang R. Gao &lt;br/&gt;&lt;br/&gt;A Framework for Developing Complex Applications On High-End Petaflop-Class Machines&lt;br/&gt;&lt;br/&gt;This research proposes to develop novel software technologies for supporting the design and implementation of complex applications on next-generation high-performance computers such as the IBM Blue Gene machine, the teraflop-class systems of the ASCI program, and the architecture (s) proposed in the NSF peta-flops point-design studies.&lt;br/&gt;&lt;br/&gt;Future high-performance computers such as these will have large numbers of processors (from a few thousand to several hundred thousands, or even more), and a complex multilevel memory hierarchy with memories physically distributed across different parts of the machine. To use such machines effectively, enormous amounts of parallelism must be exposed in user programs, and careful attention must be paid to the latency and bandwidth of access to different levels of the memory hierarchy. The applications enabled by such high-end machines are also expected to be significantly more complex and dynamic than applications in the past. Therefore, supporting the development of such applications on next-generation high-end computers is a major software challenge.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103723</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103723</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1761" target="n1764">
      <data key="e_effectiveDate">2001-11-01</data>
      <data key="e_title">Next Generation Software: A Framework For Developing Complex Applications On High-End Petaflop-Class Machines</data>
      <data key="e_abstract">0103723-University of Delaware-Guang R. Gao &lt;br/&gt;&lt;br/&gt;A Framework for Developing Complex Applications On High-End Petaflop-Class Machines&lt;br/&gt;&lt;br/&gt;This research proposes to develop novel software technologies for supporting the design and implementation of complex applications on next-generation high-performance computers such as the IBM Blue Gene machine, the teraflop-class systems of the ASCI program, and the architecture (s) proposed in the NSF peta-flops point-design studies.&lt;br/&gt;&lt;br/&gt;Future high-performance computers such as these will have large numbers of processors (from a few thousand to several hundred thousands, or even more), and a complex multilevel memory hierarchy with memories physically distributed across different parts of the machine. To use such machines effectively, enormous amounts of parallelism must be exposed in user programs, and careful attention must be paid to the latency and bandwidth of access to different levels of the memory hierarchy. The applications enabled by such high-end machines are also expected to be significantly more complex and dynamic than applications in the past. Therefore, supporting the development of such applications on next-generation high-end computers is a major software challenge.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103723</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103723</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n375" target="n1762">
      <data key="e_effectiveDate">2001-11-01</data>
      <data key="e_title">Next Generation Software: A Framework For Developing Complex Applications On High-End Petaflop-Class Machines</data>
      <data key="e_abstract">0103723-University of Delaware-Guang R. Gao &lt;br/&gt;&lt;br/&gt;A Framework for Developing Complex Applications On High-End Petaflop-Class Machines&lt;br/&gt;&lt;br/&gt;This research proposes to develop novel software technologies for supporting the design and implementation of complex applications on next-generation high-performance computers such as the IBM Blue Gene machine, the teraflop-class systems of the ASCI program, and the architecture (s) proposed in the NSF peta-flops point-design studies.&lt;br/&gt;&lt;br/&gt;Future high-performance computers such as these will have large numbers of processors (from a few thousand to several hundred thousands, or even more), and a complex multilevel memory hierarchy with memories physically distributed across different parts of the machine. To use such machines effectively, enormous amounts of parallelism must be exposed in user programs, and careful attention must be paid to the latency and bandwidth of access to different levels of the memory hierarchy. The applications enabled by such high-end machines are also expected to be significantly more complex and dynamic than applications in the past. Therefore, supporting the development of such applications on next-generation high-end computers is a major software challenge.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103723</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103723</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1762" target="n1764">
      <data key="e_effectiveDate">2001-11-01</data>
      <data key="e_title">Next Generation Software: A Framework For Developing Complex Applications On High-End Petaflop-Class Machines</data>
      <data key="e_abstract">0103723-University of Delaware-Guang R. Gao &lt;br/&gt;&lt;br/&gt;A Framework for Developing Complex Applications On High-End Petaflop-Class Machines&lt;br/&gt;&lt;br/&gt;This research proposes to develop novel software technologies for supporting the design and implementation of complex applications on next-generation high-performance computers such as the IBM Blue Gene machine, the teraflop-class systems of the ASCI program, and the architecture (s) proposed in the NSF peta-flops point-design studies.&lt;br/&gt;&lt;br/&gt;Future high-performance computers such as these will have large numbers of processors (from a few thousand to several hundred thousands, or even more), and a complex multilevel memory hierarchy with memories physically distributed across different parts of the machine. To use such machines effectively, enormous amounts of parallelism must be exposed in user programs, and careful attention must be paid to the latency and bandwidth of access to different levels of the memory hierarchy. The applications enabled by such high-end machines are also expected to be significantly more complex and dynamic than applications in the past. Therefore, supporting the development of such applications on next-generation high-end computers is a major software challenge.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103723</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103723</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n375" target="n1764">
      <data key="e_effectiveDate">2001-11-01</data>
      <data key="e_title">Next Generation Software: A Framework For Developing Complex Applications On High-End Petaflop-Class Machines</data>
      <data key="e_abstract">0103723-University of Delaware-Guang R. Gao &lt;br/&gt;&lt;br/&gt;A Framework for Developing Complex Applications On High-End Petaflop-Class Machines&lt;br/&gt;&lt;br/&gt;This research proposes to develop novel software technologies for supporting the design and implementation of complex applications on next-generation high-performance computers such as the IBM Blue Gene machine, the teraflop-class systems of the ASCI program, and the architecture (s) proposed in the NSF peta-flops point-design studies.&lt;br/&gt;&lt;br/&gt;Future high-performance computers such as these will have large numbers of processors (from a few thousand to several hundred thousands, or even more), and a complex multilevel memory hierarchy with memories physically distributed across different parts of the machine. To use such machines effectively, enormous amounts of parallelism must be exposed in user programs, and careful attention must be paid to the latency and bandwidth of access to different levels of the memory hierarchy. The applications enabled by such high-end machines are also expected to be significantly more complex and dynamic than applications in the past. Therefore, supporting the development of such applications on next-generation high-end computers is a major software challenge.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103723</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103723</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1768" target="n1769">
      <data key="e_effectiveDate">2001-12-01</data>
      <data key="e_title">Identification of Threshold, Regular and Submodular Monotone Systems: Theory and Algorithms</data>
      <data key="e_abstract">The focus of this research is the development of algorithms, theory, and applications for the identification of all minimal representatives, for implicitly given monotone systems. Such systems are a frequent target of knowledge discovery as they represent important information hidden in large databases, complex networks, etc. Problems involving the determination of monotone systems arise naturally in a multitude of areas including: data mining (finding all maximal frequent, minimal infrequent sets); text mining (finding the best linear query in vector space models); machine learning (finding the best rules or patterns); hypergraph dualization (generating all minimal transversals); reliability theory (generating all minimal working and/or maximal failing states); integer programming (generating all minimal feasible solutions); stochastic programming (constructing deterministic equivalents to certain stochastic models); etc. A coherent theory of identification problems for monotone systems based on several recent mathematical results will be developed. Tractable classes will be outlined, efficient algorithms for such classes created and realized by programs. The resulting new methods will be tested on a variety of applications, such as data mining, logical data analysis, machine learning and text mining.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">118635</data>
      <data key="e_expirationDate">2005-11-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">118635</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1768" target="n1770">
      <data key="e_effectiveDate">2001-12-01</data>
      <data key="e_title">Identification of Threshold, Regular and Submodular Monotone Systems: Theory and Algorithms</data>
      <data key="e_abstract">The focus of this research is the development of algorithms, theory, and applications for the identification of all minimal representatives, for implicitly given monotone systems. Such systems are a frequent target of knowledge discovery as they represent important information hidden in large databases, complex networks, etc. Problems involving the determination of monotone systems arise naturally in a multitude of areas including: data mining (finding all maximal frequent, minimal infrequent sets); text mining (finding the best linear query in vector space models); machine learning (finding the best rules or patterns); hypergraph dualization (generating all minimal transversals); reliability theory (generating all minimal working and/or maximal failing states); integer programming (generating all minimal feasible solutions); stochastic programming (constructing deterministic equivalents to certain stochastic models); etc. A coherent theory of identification problems for monotone systems based on several recent mathematical results will be developed. Tractable classes will be outlined, efficient algorithms for such classes created and realized by programs. The resulting new methods will be tested on a variety of applications, such as data mining, logical data analysis, machine learning and text mining.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">118635</data>
      <data key="e_expirationDate">2005-11-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">118635</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1769" target="n1770">
      <data key="e_effectiveDate">2001-12-01</data>
      <data key="e_title">Identification of Threshold, Regular and Submodular Monotone Systems: Theory and Algorithms</data>
      <data key="e_abstract">The focus of this research is the development of algorithms, theory, and applications for the identification of all minimal representatives, for implicitly given monotone systems. Such systems are a frequent target of knowledge discovery as they represent important information hidden in large databases, complex networks, etc. Problems involving the determination of monotone systems arise naturally in a multitude of areas including: data mining (finding all maximal frequent, minimal infrequent sets); text mining (finding the best linear query in vector space models); machine learning (finding the best rules or patterns); hypergraph dualization (generating all minimal transversals); reliability theory (generating all minimal working and/or maximal failing states); integer programming (generating all minimal feasible solutions); stochastic programming (constructing deterministic equivalents to certain stochastic models); etc. A coherent theory of identification problems for monotone systems based on several recent mathematical results will be developed. Tractable classes will be outlined, efficient algorithms for such classes created and realized by programs. The resulting new methods will be tested on a variety of applications, such as data mining, logical data analysis, machine learning and text mining.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">118635</data>
      <data key="e_expirationDate">2005-11-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">118635</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1526" target="n1774">
      <data key="e_effectiveDate">2001-12-01</data>
      <data key="e_title">Better Freight Flow Data for Analysis and Planning</data>
      <data key="e_abstract">EIA-0138998-Genevieve Giuliano-University of Southern California-SGER: Better Freight Flow Data for Analysis and Planning&lt;br/&gt;&lt;br/&gt;This grant will support preliminary explorations in integrating 10 datasets, some digital and some paper, related to the modeling of the flow of freight carried by trucks, rail, and ship in the greater Los Angeles area. The collaboration will be between information scientists and experts in transportation planning and policy and will involve the collaboration of several government agencies.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">138998</data>
      <data key="e_expirationDate">2002-11-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">138998</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1774" target="n1776">
      <data key="e_effectiveDate">2001-12-01</data>
      <data key="e_title">Better Freight Flow Data for Analysis and Planning</data>
      <data key="e_abstract">EIA-0138998-Genevieve Giuliano-University of Southern California-SGER: Better Freight Flow Data for Analysis and Planning&lt;br/&gt;&lt;br/&gt;This grant will support preliminary explorations in integrating 10 datasets, some digital and some paper, related to the modeling of the flow of freight carried by trucks, rail, and ship in the greater Los Angeles area. The collaboration will be between information scientists and experts in transportation planning and policy and will involve the collaboration of several government agencies.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">138998</data>
      <data key="e_expirationDate">2002-11-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">138998</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1526" target="n1776">
      <data key="e_effectiveDate">2001-12-01</data>
      <data key="e_title">Better Freight Flow Data for Analysis and Planning</data>
      <data key="e_abstract">EIA-0138998-Genevieve Giuliano-University of Southern California-SGER: Better Freight Flow Data for Analysis and Planning&lt;br/&gt;&lt;br/&gt;This grant will support preliminary explorations in integrating 10 datasets, some digital and some paper, related to the modeling of the flow of freight carried by trucks, rail, and ship in the greater Los Angeles area. The collaboration will be between information scientists and experts in transportation planning and policy and will involve the collaboration of several government agencies.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">138998</data>
      <data key="e_expirationDate">2002-11-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">138998</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1779" target="n1780">
      <data key="e_effectiveDate">2001-12-15</data>
      <data key="e_title">Adaptive Neurally-Inspired Computing: Models, Algorithms, and Silicon-Based Architectures</data>
      <data key="e_abstract">EIA-0130705 -University of Washington-Guang R. Gao-Adaptive Neurally-Inspired Computing: Models, Algorithms, and Silicon-Based Architectures&lt;br/&gt;&lt;br/&gt;Rapid advances in silicon technology over the past few decades have allowed digital&lt;br/&gt;devices to achieve ultra-high speeds in numerical computation. However, a majority of&lt;br/&gt;these devices are prone to catastrophic failure when confronted with circumstances&lt;br/&gt;unforeseen at programming time. Endowing such devices with the ability to adapt and&lt;br/&gt;learn from experience is rapidly becoming a problem of fundamental importance in&lt;br/&gt;information technology. We propose a new approach to solving this problem: building&lt;br/&gt;information technology systems based on neurobiological computation and learning.&lt;br/&gt;&lt;br/&gt;We intend to achieve this goal by developing computational models of plasticity and information processing in neurons and networks of neurons in selected sensory and motor areas of the brain; testing these models and their corresponding algorithms in software-based simulations, and designing real-time implementations of these algorithms in silicon using synapse transistors and field-programmable learning arrays (FPLAs).&lt;br/&gt;&lt;br/&gt;We expect our research to provide a better understanding of computation within neuronal&lt;br/&gt;networks, and to lead to a new generation of adaptive neuromorphic devices that could be&lt;br/&gt;used for a variety of information technology applications, ranging from signal processing&lt;br/&gt;and pattern recognition to ubiquitous computing and robotic control.</data>
      <data key="e_pgm">1705</data>
      <data key="e_label">130705</data>
      <data key="e_expirationDate">2007-11-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">130705</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1781" target="n1782">
      <data key="e_effectiveDate">2001-12-01</data>
      <data key="e_title">NGS: Computational Vortals for Next-Generation Scalable Computing</data>
      <data key="e_abstract">EIA-0103594 Tomasz Haupt Mississippi State University Computational Vortals for Next Generation Scalable Computing&lt;br/&gt;&lt;br/&gt;Computational Power constantly is opening new opportunities for numerical simulations, in turn opening opportunities for new science. This computational power is expected to be a low cost alternative for design and validation, boosting efficiency of manufacturing, and be a reliable source of forecast (e.g., weather earthquakes. the stock market, to name just a few domains), as well as all the other claimed and realized advantages for academic computing. This constant demand for faster and faster compute servers drives vendors to introduce more and more sophisticated; scalable architectures including scalable interconnects.&lt;br/&gt;&lt;br/&gt;The PI&apos;s will develop high-level user directed assists to enable application programs to exploit cluster-computing resources; this will connect application level resource requirements with lower-level scheduler and middleware resources. The work will advance the usability of clusters by a significant class of applications.</data>
      <data key="e_pgm">2884</data>
      <data key="e_label">103594</data>
      <data key="e_expirationDate">2005-11-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">103594</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1783" target="n1784">
      <data key="e_effectiveDate">2001-12-01</data>
      <data key="e_title">Human Factors Research on Voting Machines and Ballot Designs: An Exploratory Assessment</data>
      <data key="e_abstract">EIA-0210634 Paul Herrnson University of Maryland SGER: Human Factors Research on Voting Machines and Ballot Design: An Exploratory Assessment&lt;br/&gt;&lt;br/&gt;This grant will support an exploratory assessment of the current state of knowledge regarding the human element in the voting process. The grantee will lay a groundwork for the systematic comparison of different methods of voting, including comparisons of the accuracy of different methods of voting, including ease of use and other factors related to voter confidence in the voting system. This work is especially relevant given the controversy surrounding the November 2000 national elections, and the movement of many political jurisdictions toward the purchase of new voting systems.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">201634</data>
      <data key="e_expirationDate">2002-11-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">201634</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n578" target="n1786">
      <data key="e_effectiveDate">2001-12-15</data>
      <data key="e_title">Applicability of Computational Language Technologies to Identify Independent Protein Folding Domains in Human Proteins</data>
      <data key="e_abstract">Several lines of evidence suggest that there exists a strong analogy between natural languages and biological sequences, i.e. there appear to be organism-specific words, phrases and paragraphs in the collective of proteins encoded by the genomes of fully sequenced organisms. It is proposed that the biological analogy of meaning in a natural text is the ability of a protein sequence to fold into its functional three-dimensional fold. In natural languages, frequent words carry little meaning, while rare words often allow identification of the topic of a particular text. The hypothesis predicts, therefore, that rare stretches of amino acids indicate the location of folding initiators. The distribution of global properties along the sequence of lysozyme, a model protein for protein folding studies, indicated that features in these properties can be recognized when inverse frequencies of amino acid n-grams were plotted, supporting the analogy to natural languages. In the next 12 months the focus will be on studying the distribution of rare n-grams in the human genome. If a correlation between folding domains and distribution of rare n-grams can be established, this would (i) provide compelling support for the hypothesis and (ii) shed light on one of the major unsolved questions in biology today, the mechanism by which functional three-dimensional structures are formed from a one-dimensional sequence of amino acids.</data>
      <data key="e_pgm">1994</data>
      <data key="e_label">204078</data>
      <data key="e_expirationDate">2002-11-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">204078</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1788" target="n1789">
      <data key="e_effectiveDate">2001-12-01</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics - BDEI -Computation and Uncertainty in Ecological Forecasting</data>
      <data key="e_abstract">EIA-0131905 -James Clark-Duke University-BDEI: Computation and Uncertainty in Ecological Forecasting.&lt;br/&gt;&lt;br/&gt;Planning for global change and decision making will be improved by access to reliable forecasts of ecosystem change. A recent initiative by the Ecological Society of America identifies three challenges that must be met for forecasts to be successful: 1) computational approaches (algorithm development and data structures) that would permit simulation of complex systems, 2) feasible methods to track statistical uncertainty, and 3) data inadequacy.&lt;br/&gt;&lt;br/&gt; The project &quot;BDEI: Computation and uncertainty in ecological forecasting&quot; is an incubation that addresses these three challenges with an integrated approach. We propose to develop new computational and statistical techniques that will provide the capacity to forecast at broader spatial and temporal extents than possible with current approaches. The success of the proposed techniques will be evaluated using the data gathered from field experiments. &lt;br/&gt;&lt;br/&gt;A team of three researchers-an ecologist (Clark), a computer scientist (Agarwal), and a statistician (Lavine)- propose a working group for Fall and Spring 2002 that will focus on forecasting forest compositional change. The working group will integrate new computational techniques into stand simulators and use models to estimate&lt;br/&gt;uncertainty. The working group will develop an agenda for a broad initiative in ecological forecasting as basis for future proposals.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">131905</data>
      <data key="e_expirationDate">2003-11-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">131905</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1788" target="n1790">
      <data key="e_effectiveDate">2001-12-01</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics - BDEI -Computation and Uncertainty in Ecological Forecasting</data>
      <data key="e_abstract">EIA-0131905 -James Clark-Duke University-BDEI: Computation and Uncertainty in Ecological Forecasting.&lt;br/&gt;&lt;br/&gt;Planning for global change and decision making will be improved by access to reliable forecasts of ecosystem change. A recent initiative by the Ecological Society of America identifies three challenges that must be met for forecasts to be successful: 1) computational approaches (algorithm development and data structures) that would permit simulation of complex systems, 2) feasible methods to track statistical uncertainty, and 3) data inadequacy.&lt;br/&gt;&lt;br/&gt; The project &quot;BDEI: Computation and uncertainty in ecological forecasting&quot; is an incubation that addresses these three challenges with an integrated approach. We propose to develop new computational and statistical techniques that will provide the capacity to forecast at broader spatial and temporal extents than possible with current approaches. The success of the proposed techniques will be evaluated using the data gathered from field experiments. &lt;br/&gt;&lt;br/&gt;A team of three researchers-an ecologist (Clark), a computer scientist (Agarwal), and a statistician (Lavine)- propose a working group for Fall and Spring 2002 that will focus on forecasting forest compositional change. The working group will integrate new computational techniques into stand simulators and use models to estimate&lt;br/&gt;uncertainty. The working group will develop an agenda for a broad initiative in ecological forecasting as basis for future proposals.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">131905</data>
      <data key="e_expirationDate">2003-11-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">131905</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1789" target="n1790">
      <data key="e_effectiveDate">2001-12-01</data>
      <data key="e_title">Biodiversity and Ecosystem Informatics - BDEI -Computation and Uncertainty in Ecological Forecasting</data>
      <data key="e_abstract">EIA-0131905 -James Clark-Duke University-BDEI: Computation and Uncertainty in Ecological Forecasting.&lt;br/&gt;&lt;br/&gt;Planning for global change and decision making will be improved by access to reliable forecasts of ecosystem change. A recent initiative by the Ecological Society of America identifies three challenges that must be met for forecasts to be successful: 1) computational approaches (algorithm development and data structures) that would permit simulation of complex systems, 2) feasible methods to track statistical uncertainty, and 3) data inadequacy.&lt;br/&gt;&lt;br/&gt; The project &quot;BDEI: Computation and uncertainty in ecological forecasting&quot; is an incubation that addresses these three challenges with an integrated approach. We propose to develop new computational and statistical techniques that will provide the capacity to forecast at broader spatial and temporal extents than possible with current approaches. The success of the proposed techniques will be evaluated using the data gathered from field experiments. &lt;br/&gt;&lt;br/&gt;A team of three researchers-an ecologist (Clark), a computer scientist (Agarwal), and a statistician (Lavine)- propose a working group for Fall and Spring 2002 that will focus on forecasting forest compositional change. The working group will integrate new computational techniques into stand simulators and use models to estimate&lt;br/&gt;uncertainty. The working group will develop an agenda for a broad initiative in ecological forecasting as basis for future proposals.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">131905</data>
      <data key="e_expirationDate">2003-11-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">131905</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1791" target="n1792">
      <data key="e_effectiveDate">2001-12-01</data>
      <data key="e_title">ITR: KDD Workshop</data>
      <data key="e_abstract">Information Technology will be playing an important role in addressing the research problems relevant to national security. The National Science Foundation will be calling upon its research communities to focus their research efforts on problems relating to national security. NSF has identified current projects that are important to this area, including but not limited to the following: Knowledge Discovery and Dissemination (datamining, knowledge representation, and knowledge sharing) and Biological Defense (bioinformatics, biosensing, pathogen normalcy baselining). This workshop is designed to bring selected university researchers together with NSF and other appropriate government staff to communicate their needs and get community input. CNRI will hold the workshop in order to assess some of the current programs and projects, gather government input, identify which projects hold the most relevance to the mission, and help identify new areas of research.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">204867</data>
      <data key="e_expirationDate">2003-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">204867</data>
      <data key="e_dir">05</data>
    </edge>
  </graph>
</graphml>
