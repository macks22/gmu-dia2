<?xml version="1.0" encoding="UTF-8"?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns
         http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
<!-- Created by igraph -->
  <key id="v_name" for="node" attr.name="name" attr.type="double"/>
  <key id="v_label" for="node" attr.name="label" attr.type="double"/>
  <key id="e_effectiveDate" for="edge" attr.name="effectiveDate" attr.type="string"/>
  <key id="e_title" for="edge" attr.name="title" attr.type="string"/>
  <key id="e_abstract" for="edge" attr.name="abstract" attr.type="string"/>
  <key id="e_pgm" for="edge" attr.name="pgm" attr.type="string"/>
  <key id="e_label" for="edge" attr.name="label" attr.type="double"/>
  <key id="e_expirationDate" for="edge" attr.name="expirationDate" attr.type="string"/>
  <key id="e_div" for="edge" attr.name="div" attr.type="string"/>
  <key id="e_awardID" for="edge" attr.name="awardID" attr.type="double"/>
  <key id="e_dir" for="edge" attr.name="dir" attr.type="string"/>
  <graph id="G" edgedefault="undirected">
    <node id="n0">
      <data key="v_name">559843</data>
      <data key="v_label">559843</data>
    </node>
    <node id="n1">
      <data key="v_name">365820</data>
      <data key="v_label">365820</data>
    </node>
    <node id="n2">
      <data key="v_name">519226</data>
      <data key="v_label">519226</data>
    </node>
    <node id="n3">
      <data key="v_name">450496</data>
      <data key="v_label">450496</data>
    </node>
    <node id="n4">
      <data key="v_name">560072</data>
      <data key="v_label">560072</data>
    </node>
    <node id="n5">
      <data key="v_name">211338</data>
      <data key="v_label">211338</data>
    </node>
    <node id="n6">
      <data key="v_name">267279</data>
      <data key="v_label">267279</data>
    </node>
    <node id="n7">
      <data key="v_name">530355</data>
      <data key="v_label">530355</data>
    </node>
    <node id="n8">
      <data key="v_name">475211</data>
      <data key="v_label">475211</data>
    </node>
    <node id="n9">
      <data key="v_name">559377</data>
      <data key="v_label">559377</data>
    </node>
    <node id="n10">
      <data key="v_name">360958</data>
      <data key="v_label">360958</data>
    </node>
    <node id="n11">
      <data key="v_name">120179</data>
      <data key="v_label">120179</data>
    </node>
    <node id="n12">
      <data key="v_name">471476</data>
      <data key="v_label">471476</data>
    </node>
    <node id="n13">
      <data key="v_name">297550</data>
      <data key="v_label">297550</data>
    </node>
    <node id="n14">
      <data key="v_name">450215</data>
      <data key="v_label">450215</data>
    </node>
    <node id="n15">
      <data key="v_name">328464</data>
      <data key="v_label">328464</data>
    </node>
    <node id="n16">
      <data key="v_name">473862</data>
      <data key="v_label">473862</data>
    </node>
    <node id="n17">
      <data key="v_name">109362</data>
      <data key="v_label">109362</data>
    </node>
    <node id="n18">
      <data key="v_name">553162</data>
      <data key="v_label">553162</data>
    </node>
    <node id="n19">
      <data key="v_name">202714</data>
      <data key="v_label">202714</data>
    </node>
    <node id="n20">
      <data key="v_name">555869</data>
      <data key="v_label">555869</data>
    </node>
    <node id="n21">
      <data key="v_name">559377</data>
    </node>
    <node id="n22">
      <data key="v_name">558959</data>
      <data key="v_label">558959</data>
    </node>
    <node id="n23">
      <data key="v_name">172773</data>
      <data key="v_label">172773</data>
    </node>
    <node id="n24">
      <data key="v_name">260968</data>
      <data key="v_label">260968</data>
    </node>
    <node id="n25">
      <data key="v_name">553162</data>
    </node>
    <node id="n26">
      <data key="v_name">410638</data>
      <data key="v_label">410638</data>
    </node>
    <node id="n27">
      <data key="v_name">383005</data>
      <data key="v_label">383005</data>
    </node>
    <node id="n28">
      <data key="v_name">426467</data>
      <data key="v_label">426467</data>
    </node>
    <node id="n29">
      <data key="v_name">518585</data>
      <data key="v_label">518585</data>
    </node>
    <node id="n30">
      <data key="v_name">553754</data>
      <data key="v_label">553754</data>
    </node>
    <node id="n31">
      <data key="v_name">553228</data>
      <data key="v_label">553228</data>
    </node>
    <node id="n32">
      <data key="v_name">485994</data>
      <data key="v_label">485994</data>
    </node>
    <node id="n33">
      <data key="v_name">298189</data>
      <data key="v_label">298189</data>
    </node>
    <node id="n34">
      <data key="v_name">234699</data>
      <data key="v_label">234699</data>
    </node>
    <node id="n35">
      <data key="v_name">235705</data>
      <data key="v_label">235705</data>
    </node>
    <node id="n36">
      <data key="v_name">246561</data>
      <data key="v_label">246561</data>
    </node>
    <node id="n37">
      <data key="v_name">117956</data>
      <data key="v_label">117956</data>
    </node>
    <node id="n38">
      <data key="v_name">117957</data>
      <data key="v_label">117957</data>
    </node>
    <node id="n39">
      <data key="v_name">269497</data>
      <data key="v_label">269497</data>
    </node>
    <node id="n40">
      <data key="v_name">426503</data>
      <data key="v_label">426503</data>
    </node>
    <node id="n41">
      <data key="v_name">316342</data>
      <data key="v_label">316342</data>
    </node>
    <node id="n42">
      <data key="v_name">332610</data>
      <data key="v_label">332610</data>
    </node>
    <node id="n43">
      <data key="v_name">549076</data>
      <data key="v_label">549076</data>
    </node>
    <node id="n44">
      <data key="v_name">460241</data>
      <data key="v_label">460241</data>
    </node>
    <node id="n45">
      <data key="v_name">252476</data>
      <data key="v_label">252476</data>
    </node>
    <node id="n46">
      <data key="v_name">523953</data>
      <data key="v_label">523953</data>
    </node>
    <node id="n47">
      <data key="v_name">477247</data>
      <data key="v_label">477247</data>
    </node>
    <node id="n48">
      <data key="v_name">421964</data>
      <data key="v_label">421964</data>
    </node>
    <node id="n49">
      <data key="v_name">120016</data>
      <data key="v_label">120016</data>
    </node>
    <node id="n50">
      <data key="v_name">421965</data>
      <data key="v_label">421965</data>
    </node>
    <node id="n51">
      <data key="v_name">531255</data>
      <data key="v_label">531255</data>
    </node>
    <node id="n52">
      <data key="v_name">410638</data>
    </node>
    <node id="n53">
      <data key="v_name">482465</data>
      <data key="v_label">482465</data>
    </node>
    <node id="n54">
      <data key="v_name">556912</data>
      <data key="v_label">556912</data>
    </node>
    <node id="n55">
      <data key="v_name">123526</data>
      <data key="v_label">123526</data>
    </node>
    <node id="n56">
      <data key="v_name">561945</data>
      <data key="v_label">561945</data>
    </node>
    <node id="n57">
      <data key="v_name">323778</data>
      <data key="v_label">323778</data>
    </node>
    <node id="n58">
      <data key="v_name">506171</data>
      <data key="v_label">506171</data>
    </node>
    <node id="n59">
      <data key="v_name">182478</data>
      <data key="v_label">182478</data>
    </node>
    <node id="n60">
      <data key="v_name">389044</data>
      <data key="v_label">389044</data>
    </node>
    <node id="n61">
      <data key="v_name">558550</data>
      <data key="v_label">558550</data>
    </node>
    <node id="n62">
      <data key="v_name">364563</data>
      <data key="v_label">364563</data>
    </node>
    <node id="n63">
      <data key="v_name">398611</data>
      <data key="v_label">398611</data>
    </node>
    <node id="n64">
      <data key="v_name">530745</data>
      <data key="v_label">530745</data>
    </node>
    <node id="n65">
      <data key="v_name">114423</data>
      <data key="v_label">114423</data>
    </node>
    <node id="n66">
      <data key="v_name">552243</data>
      <data key="v_label">552243</data>
    </node>
    <node id="n67">
      <data key="v_name">122633</data>
      <data key="v_label">122633</data>
    </node>
    <node id="n68">
      <data key="v_name">561562</data>
      <data key="v_label">561562</data>
    </node>
    <node id="n69">
      <data key="v_name">550903</data>
      <data key="v_label">550903</data>
    </node>
    <node id="n70">
      <data key="v_name">483488</data>
      <data key="v_label">483488</data>
    </node>
    <node id="n71">
      <data key="v_name">550380</data>
      <data key="v_label">550380</data>
    </node>
    <node id="n72">
      <data key="v_name">531622</data>
      <data key="v_label">531622</data>
    </node>
    <node id="n73">
      <data key="v_name">521488</data>
      <data key="v_label">521488</data>
    </node>
    <node id="n74">
      <data key="v_name">380297</data>
      <data key="v_label">380297</data>
    </node>
    <node id="n75">
      <data key="v_name">513354</data>
      <data key="v_label">513354</data>
    </node>
    <node id="n76">
      <data key="v_name">496204</data>
      <data key="v_label">496204</data>
    </node>
    <node id="n77">
      <data key="v_name">558600</data>
      <data key="v_label">558600</data>
    </node>
    <node id="n78">
      <data key="v_name">309009</data>
      <data key="v_label">309009</data>
    </node>
    <node id="n79">
      <data key="v_name">100779</data>
      <data key="v_label">100779</data>
    </node>
    <node id="n80">
      <data key="v_name">450892</data>
      <data key="v_label">450892</data>
    </node>
    <node id="n81">
      <data key="v_name">518008</data>
      <data key="v_label">518008</data>
    </node>
    <node id="n82">
      <data key="v_name">207227</data>
      <data key="v_label">207227</data>
    </node>
    <node id="n83">
      <data key="v_name">105560</data>
      <data key="v_label">105560</data>
    </node>
    <node id="n84">
      <data key="v_name">499410</data>
      <data key="v_label">499410</data>
    </node>
    <node id="n85">
      <data key="v_name">528361</data>
      <data key="v_label">528361</data>
    </node>
    <node id="n86">
      <data key="v_name">543562</data>
      <data key="v_label">543562</data>
    </node>
    <node id="n87">
      <data key="v_name">550344</data>
      <data key="v_label">550344</data>
    </node>
    <node id="n88">
      <data key="v_name">558595</data>
      <data key="v_label">558595</data>
    </node>
    <node id="n89">
      <data key="v_name">342332</data>
      <data key="v_label">342332</data>
    </node>
    <node id="n90">
      <data key="v_name">550778</data>
      <data key="v_label">550778</data>
    </node>
    <node id="n91">
      <data key="v_name">513513</data>
      <data key="v_label">513513</data>
    </node>
    <node id="n92">
      <data key="v_name">122795</data>
      <data key="v_label">122795</data>
    </node>
    <node id="n93">
      <data key="v_name">561621</data>
      <data key="v_label">561621</data>
    </node>
    <node id="n94">
      <data key="v_name">122768</data>
      <data key="v_label">122768</data>
    </node>
    <node id="n95">
      <data key="v_name">214934</data>
      <data key="v_label">214934</data>
    </node>
    <node id="n96">
      <data key="v_name">517411</data>
      <data key="v_label">517411</data>
    </node>
    <node id="n97">
      <data key="v_name">362108</data>
      <data key="v_label">362108</data>
    </node>
    <node id="n98">
      <data key="v_name">135899</data>
      <data key="v_label">135899</data>
    </node>
    <node id="n99">
      <data key="v_name">108468</data>
      <data key="v_label">108468</data>
    </node>
    <node id="n100">
      <data key="v_name">425148</data>
      <data key="v_label">425148</data>
    </node>
    <node id="n101">
      <data key="v_name">559100</data>
      <data key="v_label">559100</data>
    </node>
    <node id="n102">
      <data key="v_name">178655</data>
      <data key="v_label">178655</data>
    </node>
    <node id="n103">
      <data key="v_name">494776</data>
      <data key="v_label">494776</data>
    </node>
    <node id="n104">
      <data key="v_name">434747</data>
      <data key="v_label">434747</data>
    </node>
    <node id="n105">
      <data key="v_name">451774</data>
      <data key="v_label">451774</data>
    </node>
    <node id="n106">
      <data key="v_name">553586</data>
      <data key="v_label">553586</data>
    </node>
    <node id="n107">
      <data key="v_name">201891</data>
      <data key="v_label">201891</data>
    </node>
    <node id="n108">
      <data key="v_name">518498</data>
      <data key="v_label">518498</data>
    </node>
    <node id="n109">
      <data key="v_name">108497</data>
      <data key="v_label">108497</data>
    </node>
    <node id="n110">
      <data key="v_name">496511</data>
      <data key="v_label">496511</data>
    </node>
    <node id="n111">
      <data key="v_name">502549</data>
      <data key="v_label">502549</data>
    </node>
    <node id="n112">
      <data key="v_name">338010</data>
      <data key="v_label">338010</data>
    </node>
    <node id="n113">
      <data key="v_name">226262</data>
      <data key="v_label">226262</data>
    </node>
    <node id="n114">
      <data key="v_name">388677</data>
      <data key="v_label">388677</data>
    </node>
    <node id="n115">
      <data key="v_name">562858</data>
      <data key="v_label">562858</data>
    </node>
    <node id="n116">
      <data key="v_name">562859</data>
      <data key="v_label">562859</data>
    </node>
    <node id="n117">
      <data key="v_name">338010</data>
    </node>
    <node id="n118">
      <data key="v_name">473862</data>
    </node>
    <node id="n119">
      <data key="v_name">261975</data>
      <data key="v_label">261975</data>
    </node>
    <node id="n120">
      <data key="v_name">217452</data>
      <data key="v_label">217452</data>
    </node>
    <node id="n121">
      <data key="v_name">424003</data>
      <data key="v_label">424003</data>
    </node>
    <node id="n122">
      <data key="v_name">550986</data>
      <data key="v_label">550986</data>
    </node>
    <node id="n123">
      <data key="v_name">505744</data>
      <data key="v_label">505744</data>
    </node>
    <node id="n124">
      <data key="v_name">329570</data>
      <data key="v_label">329570</data>
    </node>
    <node id="n125">
      <data key="v_name">521239</data>
      <data key="v_label">521239</data>
    </node>
    <node id="n126">
      <data key="v_name">530256</data>
      <data key="v_label">530256</data>
    </node>
    <node id="n127">
      <data key="v_name">267292</data>
      <data key="v_label">267292</data>
    </node>
    <node id="n128">
      <data key="v_name">154611</data>
      <data key="v_label">154611</data>
    </node>
    <node id="n129">
      <data key="v_name">246649</data>
      <data key="v_label">246649</data>
    </node>
    <node id="n130">
      <data key="v_name">298277</data>
      <data key="v_label">298277</data>
    </node>
    <node id="n131">
      <data key="v_name">206846</data>
      <data key="v_label">206846</data>
    </node>
    <node id="n132">
      <data key="v_name">408476</data>
      <data key="v_label">408476</data>
    </node>
    <node id="n133">
      <data key="v_name">319517</data>
      <data key="v_label">319517</data>
    </node>
    <node id="n134">
      <data key="v_name">39192</data>
      <data key="v_label">39192</data>
    </node>
    <node id="n135">
      <data key="v_name">511542</data>
      <data key="v_label">511542</data>
    </node>
    <node id="n136">
      <data key="v_name">507484</data>
      <data key="v_label">507484</data>
    </node>
    <node id="n137">
      <data key="v_name">196076</data>
      <data key="v_label">196076</data>
    </node>
    <node id="n138">
      <data key="v_name">345982</data>
      <data key="v_label">345982</data>
    </node>
    <node id="n139">
      <data key="v_name">108530</data>
      <data key="v_label">108530</data>
    </node>
    <node id="n140">
      <data key="v_name">140260</data>
      <data key="v_label">140260</data>
    </node>
    <node id="n141">
      <data key="v_name">317120</data>
      <data key="v_label">317120</data>
    </node>
    <node id="n142">
      <data key="v_name">521625</data>
      <data key="v_label">521625</data>
    </node>
    <node id="n143">
      <data key="v_name">560676</data>
      <data key="v_label">560676</data>
    </node>
    <node id="n144">
      <data key="v_name">122742</data>
      <data key="v_label">122742</data>
    </node>
    <node id="n145">
      <data key="v_name">508604</data>
      <data key="v_label">508604</data>
    </node>
    <node id="n146">
      <data key="v_name">448964</data>
      <data key="v_label">448964</data>
    </node>
    <node id="n147">
      <data key="v_name">564069</data>
      <data key="v_label">564069</data>
    </node>
    <node id="n148">
      <data key="v_name">558205</data>
      <data key="v_label">558205</data>
    </node>
    <node id="n149">
      <data key="v_name">410444</data>
      <data key="v_label">410444</data>
    </node>
    <node id="n150">
      <data key="v_name">559445</data>
      <data key="v_label">559445</data>
    </node>
    <node id="n151">
      <data key="v_name">496821</data>
      <data key="v_label">496821</data>
    </node>
    <node id="n152">
      <data key="v_name">517335</data>
      <data key="v_label">517335</data>
    </node>
    <node id="n153">
      <data key="v_name">119210</data>
      <data key="v_label">119210</data>
    </node>
    <node id="n154">
      <data key="v_name">559496</data>
      <data key="v_label">559496</data>
    </node>
    <node id="n155">
      <data key="v_name">560510</data>
      <data key="v_label">560510</data>
    </node>
    <node id="n156">
      <data key="v_name">318715</data>
      <data key="v_label">318715</data>
    </node>
    <node id="n157">
      <data key="v_name">486425</data>
      <data key="v_label">486425</data>
    </node>
    <node id="n158">
      <data key="v_name">486839</data>
      <data key="v_label">486839</data>
    </node>
    <node id="n159">
      <data key="v_name">559843</data>
    </node>
    <node id="n160">
      <data key="v_name">556616</data>
      <data key="v_label">556616</data>
    </node>
    <node id="n161">
      <data key="v_name">475422</data>
      <data key="v_label">475422</data>
    </node>
    <node id="n162">
      <data key="v_name">156449</data>
      <data key="v_label">156449</data>
    </node>
    <node id="n163">
      <data key="v_name">191820</data>
      <data key="v_label">191820</data>
    </node>
    <node id="n164">
      <data key="v_name">551076</data>
      <data key="v_label">551076</data>
    </node>
    <node id="n165">
      <data key="v_name">433032</data>
      <data key="v_label">433032</data>
    </node>
    <node id="n166">
      <data key="v_name">365926</data>
      <data key="v_label">365926</data>
    </node>
    <node id="n167">
      <data key="v_name">549890</data>
      <data key="v_label">549890</data>
    </node>
    <node id="n168">
      <data key="v_name">202606</data>
      <data key="v_label">202606</data>
    </node>
    <node id="n169">
      <data key="v_name">508479</data>
      <data key="v_label">508479</data>
    </node>
    <node id="n170">
      <data key="v_name">223414</data>
      <data key="v_label">223414</data>
    </node>
    <node id="n171">
      <data key="v_name">292906</data>
      <data key="v_label">292906</data>
    </node>
    <node id="n172">
      <data key="v_name">228004</data>
      <data key="v_label">228004</data>
    </node>
    <node id="n173">
      <data key="v_name">519936</data>
      <data key="v_label">519936</data>
    </node>
    <node id="n174">
      <data key="v_name">483342</data>
      <data key="v_label">483342</data>
    </node>
    <node id="n175">
      <data key="v_name">408667</data>
      <data key="v_label">408667</data>
    </node>
    <node id="n176">
      <data key="v_name">557365</data>
      <data key="v_label">557365</data>
    </node>
    <node id="n177">
      <data key="v_name">554348</data>
      <data key="v_label">554348</data>
    </node>
    <node id="n178">
      <data key="v_name">485505</data>
      <data key="v_label">485505</data>
    </node>
    <node id="n179">
      <data key="v_name">258835</data>
      <data key="v_label">258835</data>
    </node>
    <node id="n180">
      <data key="v_name">517321</data>
      <data key="v_label">517321</data>
    </node>
    <node id="n181">
      <data key="v_name">558563</data>
      <data key="v_label">558563</data>
    </node>
    <node id="n182">
      <data key="v_name">172543</data>
      <data key="v_label">172543</data>
    </node>
    <node id="n183">
      <data key="v_name">564721</data>
      <data key="v_label">564721</data>
    </node>
    <node id="n184">
      <data key="v_name">532931</data>
      <data key="v_label">532931</data>
    </node>
    <node id="n185">
      <data key="v_name">124572</data>
      <data key="v_label">124572</data>
    </node>
    <node id="n186">
      <data key="v_name">560130</data>
      <data key="v_label">560130</data>
    </node>
    <node id="n187">
      <data key="v_name">182786</data>
      <data key="v_label">182786</data>
    </node>
    <node id="n188">
      <data key="v_name">548177</data>
      <data key="v_label">548177</data>
    </node>
    <node id="n189">
      <data key="v_name">393510</data>
      <data key="v_label">393510</data>
    </node>
    <node id="n190">
      <data key="v_name">291712</data>
      <data key="v_label">291712</data>
    </node>
    <node id="n191">
      <data key="v_name">109514</data>
      <data key="v_label">109514</data>
    </node>
    <node id="n192">
      <data key="v_name">475378</data>
      <data key="v_label">475378</data>
    </node>
    <node id="n193">
      <data key="v_name">549356</data>
      <data key="v_label">549356</data>
    </node>
    <node id="n194">
      <data key="v_name">225924</data>
      <data key="v_label">225924</data>
    </node>
    <node id="n195">
      <data key="v_name">539717</data>
      <data key="v_label">539717</data>
    </node>
    <node id="n196">
      <data key="v_name">239076</data>
      <data key="v_label">239076</data>
    </node>
    <node id="n197">
      <data key="v_name">486633</data>
      <data key="v_label">486633</data>
    </node>
    <node id="n198">
      <data key="v_name">497799</data>
      <data key="v_label">497799</data>
    </node>
    <node id="n199">
      <data key="v_name">419800</data>
      <data key="v_label">419800</data>
    </node>
    <node id="n200">
      <data key="v_name">338149</data>
      <data key="v_label">338149</data>
    </node>
    <node id="n201">
      <data key="v_name">402216</data>
      <data key="v_label">402216</data>
    </node>
    <node id="n202">
      <data key="v_name">283200</data>
      <data key="v_label">283200</data>
    </node>
    <node id="n203">
      <data key="v_name">550942</data>
      <data key="v_label">550942</data>
    </node>
    <node id="n204">
      <data key="v_name">475375</data>
      <data key="v_label">475375</data>
    </node>
    <node id="n205">
      <data key="v_name">497161</data>
      <data key="v_label">497161</data>
    </node>
    <node id="n206">
      <data key="v_name">550894</data>
      <data key="v_label">550894</data>
    </node>
    <node id="n207">
      <data key="v_name">387321</data>
      <data key="v_label">387321</data>
    </node>
    <node id="n208">
      <data key="v_name">428481</data>
      <data key="v_label">428481</data>
    </node>
    <node id="n209">
      <data key="v_name">254137</data>
      <data key="v_label">254137</data>
    </node>
    <node id="n210">
      <data key="v_name">121982</data>
      <data key="v_label">121982</data>
    </node>
    <node id="n211">
      <data key="v_name">464438</data>
      <data key="v_label">464438</data>
    </node>
    <node id="n212">
      <data key="v_name">194169</data>
      <data key="v_label">194169</data>
    </node>
    <node id="n213">
      <data key="v_name">431233</data>
      <data key="v_label">431233</data>
    </node>
    <node id="n214">
      <data key="v_name">486265</data>
      <data key="v_label">486265</data>
    </node>
    <node id="n215">
      <data key="v_name">440832</data>
      <data key="v_label">440832</data>
    </node>
    <node id="n216">
      <data key="v_name">166259</data>
      <data key="v_label">166259</data>
    </node>
    <node id="n217">
      <data key="v_name">121344</data>
      <data key="v_label">121344</data>
    </node>
    <node id="n218">
      <data key="v_name">550972</data>
      <data key="v_label">550972</data>
    </node>
    <node id="n219">
      <data key="v_name">546359</data>
      <data key="v_label">546359</data>
    </node>
    <node id="n220">
      <data key="v_name">542095</data>
      <data key="v_label">542095</data>
    </node>
    <node id="n221">
      <data key="v_name">440155</data>
      <data key="v_label">440155</data>
    </node>
    <node id="n222">
      <data key="v_name">504532</data>
      <data key="v_label">504532</data>
    </node>
    <node id="n223">
      <data key="v_name">486150</data>
      <data key="v_label">486150</data>
    </node>
    <node id="n224">
      <data key="v_name">320975</data>
      <data key="v_label">320975</data>
    </node>
    <node id="n225">
      <data key="v_name">218116</data>
      <data key="v_label">218116</data>
    </node>
    <node id="n226">
      <data key="v_name">518523</data>
      <data key="v_label">518523</data>
    </node>
    <node id="n227">
      <data key="v_name">134270</data>
      <data key="v_label">134270</data>
    </node>
    <node id="n228">
      <data key="v_name">258813</data>
      <data key="v_label">258813</data>
    </node>
    <node id="n229">
      <data key="v_name">475203</data>
      <data key="v_label">475203</data>
    </node>
    <node id="n230">
      <data key="v_name">109806</data>
      <data key="v_label">109806</data>
    </node>
    <node id="n231">
      <data key="v_name">472054</data>
      <data key="v_label">472054</data>
    </node>
    <node id="n232">
      <data key="v_name">528441</data>
      <data key="v_label">528441</data>
    </node>
    <node id="n233">
      <data key="v_name">550624</data>
      <data key="v_label">550624</data>
    </node>
    <node id="n234">
      <data key="v_name">518029</data>
      <data key="v_label">518029</data>
    </node>
    <node id="n235">
      <data key="v_name">124456</data>
      <data key="v_label">124456</data>
    </node>
    <node id="n236">
      <data key="v_name">441541</data>
      <data key="v_label">441541</data>
    </node>
    <node id="n237">
      <data key="v_name">215949</data>
      <data key="v_label">215949</data>
    </node>
    <node id="n238">
      <data key="v_name">120690</data>
      <data key="v_label">120690</data>
    </node>
    <node id="n239">
      <data key="v_name">439404</data>
      <data key="v_label">439404</data>
    </node>
    <node id="n240">
      <data key="v_name">120692</data>
      <data key="v_label">120692</data>
    </node>
    <node id="n241">
      <data key="v_name">559125</data>
      <data key="v_label">559125</data>
    </node>
    <node id="n242">
      <data key="v_name">120694</data>
      <data key="v_label">120694</data>
    </node>
    <node id="n243">
      <data key="v_name">550462</data>
      <data key="v_label">550462</data>
    </node>
    <node id="n244">
      <data key="v_name">178169</data>
      <data key="v_label">178169</data>
    </node>
    <node id="n245">
      <data key="v_name">124057</data>
      <data key="v_label">124057</data>
    </node>
    <node id="n246">
      <data key="v_name">109667</data>
      <data key="v_label">109667</data>
    </node>
    <node id="n247">
      <data key="v_name">530391</data>
      <data key="v_label">530391</data>
    </node>
    <node id="n248">
      <data key="v_name">230874</data>
      <data key="v_label">230874</data>
    </node>
    <node id="n249">
      <data key="v_name">468846</data>
      <data key="v_label">468846</data>
    </node>
    <node id="n250">
      <data key="v_name">332348</data>
      <data key="v_label">332348</data>
    </node>
    <node id="n251">
      <data key="v_name">483871</data>
      <data key="v_label">483871</data>
    </node>
    <node id="n252">
      <data key="v_name">226146</data>
      <data key="v_label">226146</data>
    </node>
    <node id="n253">
      <data key="v_name">122729</data>
      <data key="v_label">122729</data>
    </node>
    <node id="n254">
      <data key="v_name">513187</data>
      <data key="v_label">513187</data>
    </node>
    <node id="n255">
      <data key="v_name">355388</data>
      <data key="v_label">355388</data>
    </node>
    <node id="n256">
      <data key="v_name">563333</data>
      <data key="v_label">563333</data>
    </node>
    <node id="n257">
      <data key="v_name">277357</data>
      <data key="v_label">277357</data>
    </node>
    <node id="n258">
      <data key="v_name">532951</data>
      <data key="v_label">532951</data>
    </node>
    <node id="n259">
      <data key="v_name">561947</data>
      <data key="v_label">561947</data>
    </node>
    <node id="n260">
      <data key="v_name">518376</data>
      <data key="v_label">518376</data>
    </node>
    <node id="n261">
      <data key="v_name">196100</data>
      <data key="v_label">196100</data>
    </node>
    <node id="n262">
      <data key="v_name">122802</data>
      <data key="v_label">122802</data>
    </node>
    <node id="n263">
      <data key="v_name">279984</data>
      <data key="v_label">279984</data>
    </node>
    <node id="n264">
      <data key="v_name">564591</data>
      <data key="v_label">564591</data>
    </node>
    <node id="n265">
      <data key="v_name">490801</data>
      <data key="v_label">490801</data>
    </node>
    <node id="n266">
      <data key="v_name">472322</data>
      <data key="v_label">472322</data>
    </node>
    <node id="n267">
      <data key="v_name">562052</data>
      <data key="v_label">562052</data>
    </node>
    <node id="n268">
      <data key="v_name">555866</data>
      <data key="v_label">555866</data>
    </node>
    <node id="n269">
      <data key="v_name">551029</data>
      <data key="v_label">551029</data>
    </node>
    <node id="n270">
      <data key="v_name">551030</data>
      <data key="v_label">551030</data>
    </node>
    <node id="n271">
      <data key="v_name">490760</data>
      <data key="v_label">490760</data>
    </node>
    <node id="n272">
      <data key="v_name">482838</data>
      <data key="v_label">482838</data>
    </node>
    <node id="n273">
      <data key="v_name">561141</data>
      <data key="v_label">561141</data>
    </node>
    <node id="n274">
      <data key="v_name">527733</data>
      <data key="v_label">527733</data>
    </node>
    <node id="n275">
      <data key="v_name">501442</data>
      <data key="v_label">501442</data>
    </node>
    <node id="n276">
      <data key="v_name">417615</data>
      <data key="v_label">417615</data>
    </node>
    <node id="n277">
      <data key="v_name">224406</data>
      <data key="v_label">224406</data>
    </node>
    <node id="n278">
      <data key="v_name">451177</data>
      <data key="v_label">451177</data>
    </node>
    <node id="n279">
      <data key="v_name">353750</data>
      <data key="v_label">353750</data>
    </node>
    <node id="n280">
      <data key="v_name">555929</data>
      <data key="v_label">555929</data>
    </node>
    <node id="n281">
      <data key="v_name">485593</data>
      <data key="v_label">485593</data>
    </node>
    <node id="n282">
      <data key="v_name">308683</data>
      <data key="v_label">308683</data>
    </node>
    <node id="n283">
      <data key="v_name">121542</data>
      <data key="v_label">121542</data>
    </node>
    <node id="n284">
      <data key="v_name">306467</data>
      <data key="v_label">306467</data>
    </node>
    <node id="n285">
      <data key="v_name">135484</data>
      <data key="v_label">135484</data>
    </node>
    <node id="n286">
      <data key="v_name">135485</data>
      <data key="v_label">135485</data>
    </node>
    <node id="n287">
      <data key="v_name">518070</data>
      <data key="v_label">518070</data>
    </node>
    <node id="n288">
      <data key="v_name">339843</data>
      <data key="v_label">339843</data>
    </node>
    <node id="n289">
      <data key="v_name">197478</data>
      <data key="v_label">197478</data>
    </node>
    <node id="n290">
      <data key="v_name">226162</data>
      <data key="v_label">226162</data>
    </node>
    <node id="n291">
      <data key="v_name">501477</data>
      <data key="v_label">501477</data>
    </node>
    <node id="n292">
      <data key="v_name">533851</data>
      <data key="v_label">533851</data>
    </node>
    <node id="n293">
      <data key="v_name">194325</data>
      <data key="v_label">194325</data>
    </node>
    <node id="n294">
      <data key="v_name">102770</data>
      <data key="v_label">102770</data>
    </node>
    <node id="n295">
      <data key="v_name">550549</data>
      <data key="v_label">550549</data>
    </node>
    <node id="n296">
      <data key="v_name">545655</data>
      <data key="v_label">545655</data>
    </node>
    <node id="n297">
      <data key="v_name">526867</data>
      <data key="v_label">526867</data>
    </node>
    <node id="n298">
      <data key="v_name">502875</data>
      <data key="v_label">502875</data>
    </node>
    <node id="n299">
      <data key="v_name">531411</data>
      <data key="v_label">531411</data>
    </node>
    <node id="n300">
      <data key="v_name">466669</data>
      <data key="v_label">466669</data>
    </node>
    <node id="n301">
      <data key="v_name">550000</data>
      <data key="v_label">550000</data>
    </node>
    <node id="n302">
      <data key="v_name">161696</data>
      <data key="v_label">161696</data>
    </node>
    <node id="n303">
      <data key="v_name">554462</data>
      <data key="v_label">554462</data>
    </node>
    <node id="n304">
      <data key="v_name">562984</data>
      <data key="v_label">562984</data>
    </node>
    <node id="n305">
      <data key="v_name">488681</data>
      <data key="v_label">488681</data>
    </node>
    <node id="n306">
      <data key="v_name">506686</data>
      <data key="v_label">506686</data>
    </node>
    <node id="n307">
      <data key="v_name">353970</data>
      <data key="v_label">353970</data>
    </node>
    <node id="n308">
      <data key="v_name">541975</data>
      <data key="v_label">541975</data>
    </node>
    <node id="n309">
      <data key="v_name">541976</data>
      <data key="v_label">541976</data>
    </node>
    <node id="n310">
      <data key="v_name">125061</data>
      <data key="v_label">125061</data>
    </node>
    <node id="n311">
      <data key="v_name">125062</data>
      <data key="v_label">125062</data>
    </node>
    <node id="n312">
      <data key="v_name">415854</data>
      <data key="v_label">415854</data>
    </node>
    <node id="n313">
      <data key="v_name">124371</data>
      <data key="v_label">124371</data>
    </node>
    <node id="n314">
      <data key="v_name">527853</data>
      <data key="v_label">527853</data>
    </node>
    <node id="n315">
      <data key="v_name">332331</data>
      <data key="v_label">332331</data>
    </node>
    <node id="n316">
      <data key="v_name">550484</data>
      <data key="v_label">550484</data>
    </node>
    <node id="n317">
      <data key="v_name">518600</data>
      <data key="v_label">518600</data>
    </node>
    <node id="n318">
      <data key="v_name">416490</data>
      <data key="v_label">416490</data>
    </node>
    <node id="n319">
      <data key="v_name">451934</data>
      <data key="v_label">451934</data>
    </node>
    <node id="n320">
      <data key="v_name">558131</data>
      <data key="v_label">558131</data>
    </node>
    <node id="n321">
      <data key="v_name">125664</data>
      <data key="v_label">125664</data>
    </node>
    <node id="n322">
      <data key="v_name">217779</data>
      <data key="v_label">217779</data>
    </node>
    <node id="n323">
      <data key="v_name">140002</data>
      <data key="v_label">140002</data>
    </node>
    <node id="n324">
      <data key="v_name">530277</data>
      <data key="v_label">530277</data>
    </node>
    <node id="n325">
      <data key="v_name">184201</data>
      <data key="v_label">184201</data>
    </node>
    <node id="n326">
      <data key="v_name">7594</data>
      <data key="v_label">7594</data>
    </node>
    <node id="n327">
      <data key="v_name">315298</data>
      <data key="v_label">315298</data>
    </node>
    <node id="n328">
      <data key="v_name">517936</data>
      <data key="v_label">517936</data>
    </node>
    <node id="n329">
      <data key="v_name">561562</data>
    </node>
    <node id="n330">
      <data key="v_name">121430</data>
      <data key="v_label">121430</data>
    </node>
    <node id="n331">
      <data key="v_name">485433</data>
      <data key="v_label">485433</data>
    </node>
    <node id="n332">
      <data key="v_name">211323</data>
      <data key="v_label">211323</data>
    </node>
    <node id="n333">
      <data key="v_name">555004</data>
      <data key="v_label">555004</data>
    </node>
    <node id="n334">
      <data key="v_name">362927</data>
      <data key="v_label">362927</data>
    </node>
    <node id="n335">
      <data key="v_name">124871</data>
      <data key="v_label">124871</data>
    </node>
    <node id="n336">
      <data key="v_name">450636</data>
      <data key="v_label">450636</data>
    </node>
    <node id="n337">
      <data key="v_name">516928</data>
      <data key="v_label">516928</data>
    </node>
    <node id="n338">
      <data key="v_name">550549</data>
    </node>
    <node id="n339">
      <data key="v_name">518569</data>
      <data key="v_label">518569</data>
    </node>
    <node id="n340">
      <data key="v_name">293096</data>
      <data key="v_label">293096</data>
    </node>
    <node id="n341">
      <data key="v_name">120659</data>
      <data key="v_label">120659</data>
    </node>
    <node id="n342">
      <data key="v_name">151489</data>
      <data key="v_label">151489</data>
    </node>
    <node id="n343">
      <data key="v_name">451576</data>
      <data key="v_label">451576</data>
    </node>
    <node id="n344">
      <data key="v_name">495182</data>
      <data key="v_label">495182</data>
    </node>
    <node id="n345">
      <data key="v_name">552243</data>
    </node>
    <node id="n346">
      <data key="v_name">438745</data>
      <data key="v_label">438745</data>
    </node>
    <node id="n347">
      <data key="v_name">316718</data>
      <data key="v_label">316718</data>
    </node>
    <node id="n348">
      <data key="v_name">541869</data>
      <data key="v_label">541869</data>
    </node>
    <node id="n349">
      <data key="v_name">408793</data>
      <data key="v_label">408793</data>
    </node>
    <node id="n350">
      <data key="v_name">117711</data>
      <data key="v_label">117711</data>
    </node>
    <node id="n351">
      <data key="v_name">563358</data>
      <data key="v_label">563358</data>
    </node>
    <node id="n352">
      <data key="v_name">125021</data>
      <data key="v_label">125021</data>
    </node>
    <node id="n353">
      <data key="v_name">173374</data>
      <data key="v_label">173374</data>
    </node>
    <node id="n354">
      <data key="v_name">211384</data>
      <data key="v_label">211384</data>
    </node>
    <node id="n355">
      <data key="v_name">197952</data>
      <data key="v_label">197952</data>
    </node>
    <node id="n356">
      <data key="v_name">489739</data>
      <data key="v_label">489739</data>
    </node>
    <node id="n357">
      <data key="v_name">264401</data>
      <data key="v_label">264401</data>
    </node>
    <node id="n358">
      <data key="v_name">128206</data>
      <data key="v_label">128206</data>
    </node>
    <node id="n359">
      <data key="v_name">128207</data>
      <data key="v_label">128207</data>
    </node>
    <node id="n360">
      <data key="v_name">438934</data>
      <data key="v_label">438934</data>
    </node>
    <node id="n361">
      <data key="v_name">511583</data>
      <data key="v_label">511583</data>
    </node>
    <node id="n362">
      <data key="v_name">548532</data>
      <data key="v_label">548532</data>
    </node>
    <node id="n363">
      <data key="v_name">549989</data>
      <data key="v_label">549989</data>
    </node>
    <node id="n364">
      <data key="v_name">547027</data>
      <data key="v_label">547027</data>
    </node>
    <node id="n365">
      <data key="v_name">354943</data>
      <data key="v_label">354943</data>
    </node>
    <node id="n366">
      <data key="v_name">499298</data>
      <data key="v_label">499298</data>
    </node>
    <node id="n367">
      <data key="v_name">362926</data>
      <data key="v_label">362926</data>
    </node>
    <node id="n368">
      <data key="v_name">550960</data>
      <data key="v_label">550960</data>
    </node>
    <node id="n369">
      <data key="v_name">124941</data>
      <data key="v_label">124941</data>
    </node>
    <node id="n370">
      <data key="v_name">531670</data>
      <data key="v_label">531670</data>
    </node>
    <node id="n371">
      <data key="v_name">122839</data>
      <data key="v_label">122839</data>
    </node>
    <node id="n372">
      <data key="v_name">259414</data>
      <data key="v_label">259414</data>
    </node>
    <node id="n373">
      <data key="v_name">264120</data>
      <data key="v_label">264120</data>
    </node>
    <node id="n374">
      <data key="v_name">550688</data>
      <data key="v_label">550688</data>
    </node>
    <node id="n375">
      <data key="v_name">438618</data>
      <data key="v_label">438618</data>
    </node>
    <node id="n376">
      <data key="v_name">187175</data>
      <data key="v_label">187175</data>
    </node>
    <node id="n377">
      <data key="v_name">555363</data>
      <data key="v_label">555363</data>
    </node>
    <node id="n378">
      <data key="v_name">560901</data>
      <data key="v_label">560901</data>
    </node>
    <node id="n379">
      <data key="v_name">211900</data>
      <data key="v_label">211900</data>
    </node>
    <node id="n380">
      <data key="v_name">445172</data>
      <data key="v_label">445172</data>
    </node>
    <node id="n381">
      <data key="v_name">445173</data>
      <data key="v_label">445173</data>
    </node>
    <node id="n382">
      <data key="v_name">168075</data>
      <data key="v_label">168075</data>
    </node>
    <node id="n383">
      <data key="v_name">495935</data>
      <data key="v_label">495935</data>
    </node>
    <node id="n384">
      <data key="v_name">562752</data>
      <data key="v_label">562752</data>
    </node>
    <node id="n385">
      <data key="v_name">543509</data>
      <data key="v_label">543509</data>
    </node>
    <node id="n386">
      <data key="v_name">483279</data>
      <data key="v_label">483279</data>
    </node>
    <node id="n387">
      <data key="v_name">565342</data>
      <data key="v_label">565342</data>
    </node>
    <node id="n388">
      <data key="v_name">528387</data>
      <data key="v_label">528387</data>
    </node>
    <node id="n389">
      <data key="v_name">451154</data>
      <data key="v_label">451154</data>
    </node>
    <node id="n390">
      <data key="v_name">450928</data>
      <data key="v_label">450928</data>
    </node>
    <node id="n391">
      <data key="v_name">420994</data>
      <data key="v_label">420994</data>
    </node>
    <node id="n392">
      <data key="v_name">562577</data>
      <data key="v_label">562577</data>
    </node>
    <node id="n393">
      <data key="v_name">499399</data>
      <data key="v_label">499399</data>
    </node>
    <node id="n394">
      <data key="v_name">559287</data>
      <data key="v_label">559287</data>
    </node>
    <node id="n395">
      <data key="v_name">123234</data>
      <data key="v_label">123234</data>
    </node>
    <node id="n396">
      <data key="v_name">537808</data>
      <data key="v_label">537808</data>
    </node>
    <node id="n397">
      <data key="v_name">560092</data>
      <data key="v_label">560092</data>
    </node>
    <node id="n398">
      <data key="v_name">451830</data>
      <data key="v_label">451830</data>
    </node>
    <node id="n399">
      <data key="v_name">549003</data>
      <data key="v_label">549003</data>
    </node>
    <node id="n400">
      <data key="v_name">549757</data>
      <data key="v_label">549757</data>
    </node>
    <node id="n401">
      <data key="v_name">319445</data>
      <data key="v_label">319445</data>
    </node>
    <node id="n402">
      <data key="v_name">520508</data>
      <data key="v_label">520508</data>
    </node>
    <node id="n403">
      <data key="v_name">448647</data>
      <data key="v_label">448647</data>
    </node>
    <node id="n404">
      <data key="v_name">561784</data>
      <data key="v_label">561784</data>
    </node>
    <node id="n405">
      <data key="v_name">561775</data>
      <data key="v_label">561775</data>
    </node>
    <node id="n406">
      <data key="v_name">121084</data>
      <data key="v_label">121084</data>
    </node>
    <node id="n407">
      <data key="v_name">121085</data>
      <data key="v_label">121085</data>
    </node>
    <node id="n408">
      <data key="v_name">121086</data>
      <data key="v_label">121086</data>
    </node>
    <node id="n409">
      <data key="v_name">531608</data>
      <data key="v_label">531608</data>
    </node>
    <node id="n410">
      <data key="v_name">339525</data>
      <data key="v_label">339525</data>
    </node>
    <node id="n411">
      <data key="v_name">564815</data>
      <data key="v_label">564815</data>
    </node>
    <node id="n412">
      <data key="v_name">556692</data>
      <data key="v_label">556692</data>
    </node>
    <node id="n413">
      <data key="v_name">492224</data>
      <data key="v_label">492224</data>
    </node>
    <node id="n414">
      <data key="v_name">563803</data>
      <data key="v_label">563803</data>
    </node>
    <node id="n415">
      <data key="v_name">495104</data>
      <data key="v_label">495104</data>
    </node>
    <node id="n416">
      <data key="v_name">94260</data>
      <data key="v_label">94260</data>
    </node>
    <node id="n417">
      <data key="v_name">121884</data>
      <data key="v_label">121884</data>
    </node>
    <node id="n418">
      <data key="v_name">428845</data>
      <data key="v_label">428845</data>
    </node>
    <node id="n419">
      <data key="v_name">530335</data>
      <data key="v_label">530335</data>
    </node>
    <node id="n420">
      <data key="v_name">353685</data>
      <data key="v_label">353685</data>
    </node>
    <node id="n421">
      <data key="v_name">467354</data>
      <data key="v_label">467354</data>
    </node>
    <node id="n422">
      <data key="v_name">537066</data>
      <data key="v_label">537066</data>
    </node>
    <node id="n423">
      <data key="v_name">551145</data>
      <data key="v_label">551145</data>
    </node>
    <node id="n424">
      <data key="v_name">348451</data>
      <data key="v_label">348451</data>
    </node>
    <node id="n425">
      <data key="v_name">477225</data>
      <data key="v_label">477225</data>
    </node>
    <node id="n426">
      <data key="v_name">124605</data>
      <data key="v_label">124605</data>
    </node>
    <node id="n427">
      <data key="v_name">553216</data>
      <data key="v_label">553216</data>
    </node>
    <node id="n428">
      <data key="v_name">249126</data>
      <data key="v_label">249126</data>
    </node>
    <node id="n429">
      <data key="v_name">351740</data>
      <data key="v_label">351740</data>
    </node>
    <node id="n430">
      <data key="v_name">424191</data>
      <data key="v_label">424191</data>
    </node>
    <node id="n431">
      <data key="v_name">562312</data>
      <data key="v_label">562312</data>
    </node>
    <node id="n432">
      <data key="v_name">125027</data>
      <data key="v_label">125027</data>
    </node>
    <node id="n433">
      <data key="v_name">553656</data>
      <data key="v_label">553656</data>
    </node>
    <node id="n434">
      <data key="v_name">553657</data>
      <data key="v_label">553657</data>
    </node>
    <node id="n435">
      <data key="v_name">431683</data>
      <data key="v_label">431683</data>
    </node>
    <node id="n436">
      <data key="v_name">427181</data>
      <data key="v_label">427181</data>
    </node>
    <node id="n437">
      <data key="v_name">478524</data>
      <data key="v_label">478524</data>
    </node>
    <node id="n438">
      <data key="v_name">306142</data>
      <data key="v_label">306142</data>
    </node>
    <node id="n439">
      <data key="v_name">508237</data>
      <data key="v_label">508237</data>
    </node>
    <node id="n440">
      <data key="v_name">225924</data>
    </node>
    <node id="n441">
      <data key="v_name">508283</data>
      <data key="v_label">508283</data>
    </node>
    <node id="n442">
      <data key="v_name">475663</data>
      <data key="v_label">475663</data>
    </node>
    <node id="n443">
      <data key="v_name">487820</data>
      <data key="v_label">487820</data>
    </node>
    <node id="n444">
      <data key="v_name">159012</data>
      <data key="v_label">159012</data>
    </node>
    <node id="n445">
      <data key="v_name">390128</data>
      <data key="v_label">390128</data>
    </node>
    <node id="n446">
      <data key="v_name">120034</data>
      <data key="v_label">120034</data>
    </node>
    <node id="n447">
      <data key="v_name">402282</data>
      <data key="v_label">402282</data>
    </node>
    <node id="n448">
      <data key="v_name">513297</data>
      <data key="v_label">513297</data>
    </node>
    <node id="n449">
      <data key="v_name">421129</data>
      <data key="v_label">421129</data>
    </node>
    <node id="n450">
      <data key="v_name">445169</data>
      <data key="v_label">445169</data>
    </node>
    <node id="n451">
      <data key="v_name">456909</data>
      <data key="v_label">456909</data>
    </node>
    <node id="n452">
      <data key="v_name">223414</data>
    </node>
    <node id="n453">
      <data key="v_name">527564</data>
      <data key="v_label">527564</data>
    </node>
    <node id="n454">
      <data key="v_name">120052</data>
      <data key="v_label">120052</data>
    </node>
    <node id="n455">
      <data key="v_name">543520</data>
      <data key="v_label">543520</data>
    </node>
    <node id="n456">
      <data key="v_name">120041</data>
      <data key="v_label">120041</data>
    </node>
    <node id="n457">
      <data key="v_name">540935</data>
      <data key="v_label">540935</data>
    </node>
    <node id="n458">
      <data key="v_name">370768</data>
      <data key="v_label">370768</data>
    </node>
    <node id="n459">
      <data key="v_name">540494</data>
      <data key="v_label">540494</data>
    </node>
    <node id="n460">
      <data key="v_name">549952</data>
      <data key="v_label">549952</data>
    </node>
    <node id="n461">
      <data key="v_name">438750</data>
      <data key="v_label">438750</data>
    </node>
    <node id="n462">
      <data key="v_name">405531</data>
      <data key="v_label">405531</data>
    </node>
    <node id="n463">
      <data key="v_name">548216</data>
      <data key="v_label">548216</data>
    </node>
    <node id="n464">
      <data key="v_name">387279</data>
      <data key="v_label">387279</data>
    </node>
    <node id="n465">
      <data key="v_name">258181</data>
      <data key="v_label">258181</data>
    </node>
    <node id="n466">
      <data key="v_name">356244</data>
      <data key="v_label">356244</data>
    </node>
    <node id="n467">
      <data key="v_name">157565</data>
      <data key="v_label">157565</data>
    </node>
    <node id="n468">
      <data key="v_name">486066</data>
      <data key="v_label">486066</data>
    </node>
    <node id="n469">
      <data key="v_name">43608</data>
      <data key="v_label">43608</data>
    </node>
    <node id="n470">
      <data key="v_name">547713</data>
      <data key="v_label">547713</data>
    </node>
    <node id="n471">
      <data key="v_name">532971</data>
      <data key="v_label">532971</data>
    </node>
    <node id="n472">
      <data key="v_name">521101</data>
      <data key="v_label">521101</data>
    </node>
    <node id="n473">
      <data key="v_name">134347</data>
      <data key="v_label">134347</data>
    </node>
    <node id="n474">
      <data key="v_name">517075</data>
      <data key="v_label">517075</data>
    </node>
    <node id="n475">
      <data key="v_name">554177</data>
      <data key="v_label">554177</data>
    </node>
    <node id="n476">
      <data key="v_name">552796</data>
      <data key="v_label">552796</data>
    </node>
    <node id="n477">
      <data key="v_name">131015</data>
      <data key="v_label">131015</data>
    </node>
    <node id="n478">
      <data key="v_name">223414</data>
    </node>
    <node id="n479">
      <data key="v_name">306467</data>
    </node>
    <node id="n480">
      <data key="v_name">517640</data>
      <data key="v_label">517640</data>
    </node>
    <node id="n481">
      <data key="v_name">135485</data>
    </node>
    <node id="n482">
      <data key="v_name">133275</data>
      <data key="v_label">133275</data>
    </node>
    <node id="n483">
      <data key="v_name">550779</data>
      <data key="v_label">550779</data>
    </node>
    <node id="n484">
      <data key="v_name">152126</data>
      <data key="v_label">152126</data>
    </node>
    <node id="n485">
      <data key="v_name">450928</data>
    </node>
    <node id="n486">
      <data key="v_name">197870</data>
      <data key="v_label">197870</data>
    </node>
    <node id="n487">
      <data key="v_name">133935</data>
      <data key="v_label">133935</data>
    </node>
    <node id="n488">
      <data key="v_name">485309</data>
      <data key="v_label">485309</data>
    </node>
    <node id="n489">
      <data key="v_name">554181</data>
      <data key="v_label">554181</data>
    </node>
    <node id="n490">
      <data key="v_name">524130</data>
      <data key="v_label">524130</data>
    </node>
    <node id="n491">
      <data key="v_name">562750</data>
      <data key="v_label">562750</data>
    </node>
    <node id="n492">
      <data key="v_name">507668</data>
      <data key="v_label">507668</data>
    </node>
    <node id="n493">
      <data key="v_name">526820</data>
      <data key="v_label">526820</data>
    </node>
    <node id="n494">
      <data key="v_name">472159</data>
      <data key="v_label">472159</data>
    </node>
    <node id="n495">
      <data key="v_name">392036</data>
      <data key="v_label">392036</data>
    </node>
    <node id="n496">
      <data key="v_name">546219</data>
      <data key="v_label">546219</data>
    </node>
    <node id="n497">
      <data key="v_name">329896</data>
      <data key="v_label">329896</data>
    </node>
    <node id="n498">
      <data key="v_name">554471</data>
      <data key="v_label">554471</data>
    </node>
    <node id="n499">
      <data key="v_name">130583</data>
      <data key="v_label">130583</data>
    </node>
    <node id="n500">
      <data key="v_name">486421</data>
      <data key="v_label">486421</data>
    </node>
    <node id="n501">
      <data key="v_name">403485</data>
      <data key="v_label">403485</data>
    </node>
    <node id="n502">
      <data key="v_name">403486</data>
      <data key="v_label">403486</data>
    </node>
    <node id="n503">
      <data key="v_name">560175</data>
      <data key="v_label">560175</data>
    </node>
    <node id="n504">
      <data key="v_name">135321</data>
      <data key="v_label">135321</data>
    </node>
    <node id="n505">
      <data key="v_name">548297</data>
      <data key="v_label">548297</data>
    </node>
    <node id="n506">
      <data key="v_name">402665</data>
      <data key="v_label">402665</data>
    </node>
    <node id="n507">
      <data key="v_name">134510</data>
      <data key="v_label">134510</data>
    </node>
    <node id="n508">
      <data key="v_name">342042</data>
      <data key="v_label">342042</data>
    </node>
    <node id="n509">
      <data key="v_name">152485</data>
      <data key="v_label">152485</data>
    </node>
    <node id="n510">
      <data key="v_name">152486</data>
      <data key="v_label">152486</data>
    </node>
    <node id="n511">
      <data key="v_name">533237</data>
      <data key="v_label">533237</data>
    </node>
    <node id="n512">
      <data key="v_name">508256</data>
      <data key="v_label">508256</data>
    </node>
    <node id="n513">
      <data key="v_name">40586</data>
      <data key="v_label">40586</data>
    </node>
    <node id="n514">
      <data key="v_name">518420</data>
      <data key="v_label">518420</data>
    </node>
    <node id="n515">
      <data key="v_name">474015</data>
      <data key="v_label">474015</data>
    </node>
    <node id="n516">
      <data key="v_name">135266</data>
      <data key="v_label">135266</data>
    </node>
    <node id="n517">
      <data key="v_name">561775</data>
    </node>
    <node id="n518">
      <data key="v_name">451272</data>
      <data key="v_label">451272</data>
    </node>
    <node id="n519">
      <data key="v_name">486396</data>
      <data key="v_label">486396</data>
    </node>
    <node id="n520">
      <data key="v_name">506031</data>
      <data key="v_label">506031</data>
    </node>
    <node id="n521">
      <data key="v_name">451745</data>
      <data key="v_label">451745</data>
    </node>
    <node id="n522">
      <data key="v_name">124818</data>
      <data key="v_label">124818</data>
    </node>
    <node id="n523">
      <data key="v_name">564706</data>
      <data key="v_label">564706</data>
    </node>
    <node id="n524">
      <data key="v_name">362803</data>
      <data key="v_label">362803</data>
    </node>
    <node id="n525">
      <data key="v_name">548326</data>
      <data key="v_label">548326</data>
    </node>
    <node id="n526">
      <data key="v_name">281301</data>
      <data key="v_label">281301</data>
    </node>
    <node id="n527">
      <data key="v_name">318996</data>
      <data key="v_label">318996</data>
    </node>
    <node id="n528">
      <data key="v_name">103660</data>
      <data key="v_label">103660</data>
    </node>
    <node id="n529">
      <data key="v_name">140618</data>
      <data key="v_label">140618</data>
    </node>
    <node id="n530">
      <data key="v_name">130583</data>
    </node>
    <node id="n531">
      <data key="v_name">216913</data>
      <data key="v_label">216913</data>
    </node>
    <node id="n532">
      <data key="v_name">216914</data>
      <data key="v_label">216914</data>
    </node>
    <node id="n533">
      <data key="v_name">216915</data>
      <data key="v_label">216915</data>
    </node>
    <node id="n534">
      <data key="v_name">455742</data>
      <data key="v_label">455742</data>
    </node>
    <node id="n535">
      <data key="v_name">138507</data>
      <data key="v_label">138507</data>
    </node>
    <node id="n536">
      <data key="v_name">521239</data>
    </node>
    <node id="n537">
      <data key="v_name">138509</data>
      <data key="v_label">138509</data>
    </node>
    <node id="n538">
      <data key="v_name">255102</data>
      <data key="v_label">255102</data>
    </node>
    <node id="n539">
      <data key="v_name">354802</data>
      <data key="v_label">354802</data>
    </node>
    <node id="n540">
      <data key="v_name">550936</data>
      <data key="v_label">550936</data>
    </node>
    <node id="n541">
      <data key="v_name">560926</data>
      <data key="v_label">560926</data>
    </node>
    <node id="n542">
      <data key="v_name">377594</data>
      <data key="v_label">377594</data>
    </node>
    <node id="n543">
      <data key="v_name">559198</data>
      <data key="v_label">559198</data>
    </node>
    <node id="n544">
      <data key="v_name">484576</data>
      <data key="v_label">484576</data>
    </node>
    <node id="n545">
      <data key="v_name">257988</data>
      <data key="v_label">257988</data>
    </node>
    <node id="n546">
      <data key="v_name">130137</data>
      <data key="v_label">130137</data>
    </node>
    <node id="n547">
      <data key="v_name">486561</data>
      <data key="v_label">486561</data>
    </node>
    <node id="n548">
      <data key="v_name">545777</data>
      <data key="v_label">545777</data>
    </node>
    <node id="n549">
      <data key="v_name">364780</data>
      <data key="v_label">364780</data>
    </node>
    <node id="n550">
      <data key="v_name">557396</data>
      <data key="v_label">557396</data>
    </node>
    <node id="n551">
      <data key="v_name">550184</data>
      <data key="v_label">550184</data>
    </node>
    <node id="n552">
      <data key="v_name">138623</data>
      <data key="v_label">138623</data>
    </node>
    <node id="n553">
      <data key="v_name">464050</data>
      <data key="v_label">464050</data>
    </node>
    <node id="n554">
      <data key="v_name">550597</data>
      <data key="v_label">550597</data>
    </node>
    <node id="n555">
      <data key="v_name">538402</data>
      <data key="v_label">538402</data>
    </node>
    <node id="n556">
      <data key="v_name">284993</data>
      <data key="v_label">284993</data>
    </node>
    <node id="n557">
      <data key="v_name">414327</data>
      <data key="v_label">414327</data>
    </node>
    <node id="n558">
      <data key="v_name">199938</data>
      <data key="v_label">199938</data>
    </node>
    <node id="n559">
      <data key="v_name">515512</data>
      <data key="v_label">515512</data>
    </node>
    <node id="n560">
      <data key="v_name">558488</data>
      <data key="v_label">558488</data>
    </node>
    <node id="n561">
      <data key="v_name">485665</data>
      <data key="v_label">485665</data>
    </node>
    <node id="n562">
      <data key="v_name">541367</data>
      <data key="v_label">541367</data>
    </node>
    <node id="n563">
      <data key="v_name">476888</data>
      <data key="v_label">476888</data>
    </node>
    <node id="n564">
      <data key="v_name">565263</data>
      <data key="v_label">565263</data>
    </node>
    <node id="n565">
      <data key="v_name">124398</data>
      <data key="v_label">124398</data>
    </node>
    <node id="n566">
      <data key="v_name">498314</data>
      <data key="v_label">498314</data>
    </node>
    <node id="n567">
      <data key="v_name">513223</data>
      <data key="v_label">513223</data>
    </node>
    <node id="n568">
      <data key="v_name">513225</data>
      <data key="v_label">513225</data>
    </node>
    <node id="n569">
      <data key="v_name">477856</data>
      <data key="v_label">477856</data>
    </node>
    <node id="n570">
      <data key="v_name">528109</data>
      <data key="v_label">528109</data>
    </node>
    <node id="n571">
      <data key="v_name">542015</data>
      <data key="v_label">542015</data>
    </node>
    <node id="n572">
      <data key="v_name">532660</data>
      <data key="v_label">532660</data>
    </node>
    <node id="n573">
      <data key="v_name">547864</data>
      <data key="v_label">547864</data>
    </node>
    <node id="n574">
      <data key="v_name">495569</data>
      <data key="v_label">495569</data>
    </node>
    <node id="n575">
      <data key="v_name">120129</data>
      <data key="v_label">120129</data>
    </node>
    <node id="n576">
      <data key="v_name">124112</data>
      <data key="v_label">124112</data>
    </node>
    <node id="n577">
      <data key="v_name">125010</data>
      <data key="v_label">125010</data>
    </node>
    <node id="n578">
      <data key="v_name">549174</data>
      <data key="v_label">549174</data>
    </node>
    <node id="n579">
      <data key="v_name">540846</data>
      <data key="v_label">540846</data>
    </node>
    <node id="n580">
      <data key="v_name">243970</data>
      <data key="v_label">243970</data>
    </node>
    <node id="n581">
      <data key="v_name">475429</data>
      <data key="v_label">475429</data>
    </node>
    <node id="n582">
      <data key="v_name">134414</data>
      <data key="v_label">134414</data>
    </node>
    <node id="n583">
      <data key="v_name">279768</data>
      <data key="v_label">279768</data>
    </node>
    <node id="n584">
      <data key="v_name">517796</data>
      <data key="v_label">517796</data>
    </node>
    <node id="n585">
      <data key="v_name">124355</data>
      <data key="v_label">124355</data>
    </node>
    <node id="n586">
      <data key="v_name">451456</data>
      <data key="v_label">451456</data>
    </node>
    <node id="n587">
      <data key="v_name">121416</data>
      <data key="v_label">121416</data>
    </node>
    <node id="n588">
      <data key="v_name">524344</data>
      <data key="v_label">524344</data>
    </node>
    <node id="n589">
      <data key="v_name">412000</data>
      <data key="v_label">412000</data>
    </node>
    <node id="n590">
      <data key="v_name">352325</data>
      <data key="v_label">352325</data>
    </node>
    <node id="n591">
      <data key="v_name">561580</data>
      <data key="v_label">561580</data>
    </node>
    <node id="n592">
      <data key="v_name">300351</data>
      <data key="v_label">300351</data>
    </node>
    <node id="n593">
      <data key="v_name">465262</data>
      <data key="v_label">465262</data>
    </node>
    <node id="n594">
      <data key="v_name">300352</data>
      <data key="v_label">300352</data>
    </node>
    <node id="n595">
      <data key="v_name">490257</data>
      <data key="v_label">490257</data>
    </node>
    <node id="n596">
      <data key="v_name">351123</data>
      <data key="v_label">351123</data>
    </node>
    <node id="n597">
      <data key="v_name">218060</data>
      <data key="v_label">218060</data>
    </node>
    <node id="n598">
      <data key="v_name">309341</data>
      <data key="v_label">309341</data>
    </node>
    <node id="n599">
      <data key="v_name">159102</data>
      <data key="v_label">159102</data>
    </node>
    <node id="n600">
      <data key="v_name">350925</data>
      <data key="v_label">350925</data>
    </node>
    <node id="n601">
      <data key="v_name">334005</data>
      <data key="v_label">334005</data>
    </node>
    <node id="n602">
      <data key="v_name">550462</data>
    </node>
    <node id="n603">
      <data key="v_name">542246</data>
      <data key="v_label">542246</data>
    </node>
    <node id="n604">
      <data key="v_name">518277</data>
      <data key="v_label">518277</data>
    </node>
    <node id="n605">
      <data key="v_name">550108</data>
      <data key="v_label">550108</data>
    </node>
    <node id="n606">
      <data key="v_name">558209</data>
      <data key="v_label">558209</data>
    </node>
    <node id="n607">
      <data key="v_name">248451</data>
      <data key="v_label">248451</data>
    </node>
    <node id="n608">
      <data key="v_name">457136</data>
      <data key="v_label">457136</data>
    </node>
    <node id="n609">
      <data key="v_name">391834</data>
      <data key="v_label">391834</data>
    </node>
    <node id="n610">
      <data key="v_name">258283</data>
      <data key="v_label">258283</data>
    </node>
    <node id="n611">
      <data key="v_name">278933</data>
      <data key="v_label">278933</data>
    </node>
    <node id="n612">
      <data key="v_name">480627</data>
      <data key="v_label">480627</data>
    </node>
    <node id="n613">
      <data key="v_name">217240</data>
      <data key="v_label">217240</data>
    </node>
    <node id="n614">
      <data key="v_name">364563</data>
    </node>
    <node id="n615">
      <data key="v_name">135973</data>
      <data key="v_label">135973</data>
    </node>
    <node id="n616">
      <data key="v_name">550407</data>
      <data key="v_label">550407</data>
    </node>
    <node id="n617">
      <data key="v_name">559172</data>
      <data key="v_label">559172</data>
    </node>
    <node id="n618">
      <data key="v_name">557162</data>
      <data key="v_label">557162</data>
    </node>
    <node id="n619">
      <data key="v_name">124900</data>
      <data key="v_label">124900</data>
    </node>
    <node id="n620">
      <data key="v_name">537808</data>
    </node>
    <node id="n621">
      <data key="v_name">121233</data>
      <data key="v_label">121233</data>
    </node>
    <node id="n622">
      <data key="v_name">121234</data>
      <data key="v_label">121234</data>
    </node>
    <node id="n623">
      <data key="v_name">535948</data>
      <data key="v_label">535948</data>
    </node>
    <node id="n624">
      <data key="v_name">533242</data>
      <data key="v_label">533242</data>
    </node>
    <node id="n625">
      <data key="v_name">517181</data>
      <data key="v_label">517181</data>
    </node>
    <node id="n626">
      <data key="v_name">550344</data>
    </node>
    <node id="n627">
      <data key="v_name">485659</data>
      <data key="v_label">485659</data>
    </node>
    <node id="n628">
      <data key="v_name">260890</data>
      <data key="v_label">260890</data>
    </node>
    <node id="n629">
      <data key="v_name">551905</data>
      <data key="v_label">551905</data>
    </node>
    <node id="n630">
      <data key="v_name">491915</data>
      <data key="v_label">491915</data>
    </node>
    <node id="n631">
      <data key="v_name">561475</data>
      <data key="v_label">561475</data>
    </node>
    <node id="n632">
      <data key="v_name">518427</data>
      <data key="v_label">518427</data>
    </node>
    <node id="n633">
      <data key="v_name">140772</data>
      <data key="v_label">140772</data>
    </node>
    <node id="n634">
      <data key="v_name">512491</data>
      <data key="v_label">512491</data>
    </node>
    <node id="n635">
      <data key="v_name">555560</data>
      <data key="v_label">555560</data>
    </node>
    <node id="n636">
      <data key="v_name">518944</data>
      <data key="v_label">518944</data>
    </node>
    <node id="n637">
      <data key="v_name">213464</data>
      <data key="v_label">213464</data>
    </node>
    <node id="n638">
      <data key="v_name">483518</data>
      <data key="v_label">483518</data>
    </node>
    <node id="n639">
      <data key="v_name">564706</data>
    </node>
    <node id="n640">
      <data key="v_name">518427</data>
    </node>
    <node id="n641">
      <data key="v_name">528208</data>
      <data key="v_label">528208</data>
    </node>
    <node id="n642">
      <data key="v_name">542246</data>
    </node>
    <node id="n643">
      <data key="v_name">518277</data>
    </node>
    <node id="n644">
      <data key="v_name">402586</data>
      <data key="v_label">402586</data>
    </node>
    <node id="n645">
      <data key="v_name">535404</data>
      <data key="v_label">535404</data>
    </node>
    <node id="n646">
      <data key="v_name">487144</data>
      <data key="v_label">487144</data>
    </node>
    <node id="n647">
      <data key="v_name">179503</data>
      <data key="v_label">179503</data>
    </node>
    <node id="n648">
      <data key="v_name">381437</data>
      <data key="v_label">381437</data>
    </node>
    <node id="n649">
      <data key="v_name">348133</data>
      <data key="v_label">348133</data>
    </node>
    <node id="n650">
      <data key="v_name">320975</data>
    </node>
    <node id="n651">
      <data key="v_name">364876</data>
      <data key="v_label">364876</data>
    </node>
    <node id="n652">
      <data key="v_name">450482</data>
      <data key="v_label">450482</data>
    </node>
    <node id="n653">
      <data key="v_name">347050</data>
      <data key="v_label">347050</data>
    </node>
    <node id="n654">
      <data key="v_name">134296</data>
      <data key="v_label">134296</data>
    </node>
    <node id="n655">
      <data key="v_name">544512</data>
      <data key="v_label">544512</data>
    </node>
    <node id="n656">
      <data key="v_name">362485</data>
      <data key="v_label">362485</data>
    </node>
    <node id="n657">
      <data key="v_name">380971</data>
      <data key="v_label">380971</data>
    </node>
    <node id="n658">
      <data key="v_name">179869</data>
      <data key="v_label">179869</data>
    </node>
    <node id="n659">
      <data key="v_name">537412</data>
      <data key="v_label">537412</data>
    </node>
    <node id="n660">
      <data key="v_name">558578</data>
      <data key="v_label">558578</data>
    </node>
    <node id="n661">
      <data key="v_name">553768</data>
      <data key="v_label">553768</data>
    </node>
    <node id="n662">
      <data key="v_name">543581</data>
      <data key="v_label">543581</data>
    </node>
    <node id="n663">
      <data key="v_name">519685</data>
      <data key="v_label">519685</data>
    </node>
    <node id="n664">
      <data key="v_name">531150</data>
      <data key="v_label">531150</data>
    </node>
    <node id="n665">
      <data key="v_name">496031</data>
      <data key="v_label">496031</data>
    </node>
    <node id="n666">
      <data key="v_name">550555</data>
      <data key="v_label">550555</data>
    </node>
    <node id="n667">
      <data key="v_name">456909</data>
    </node>
    <node id="n668">
      <data key="v_name">483300</data>
      <data key="v_label">483300</data>
    </node>
    <node id="n669">
      <data key="v_name">533018</data>
      <data key="v_label">533018</data>
    </node>
    <node id="n670">
      <data key="v_name">468145</data>
      <data key="v_label">468145</data>
    </node>
    <node id="n671">
      <data key="v_name">121472</data>
      <data key="v_label">121472</data>
    </node>
    <node id="n672">
      <data key="v_name">121460</data>
      <data key="v_label">121460</data>
    </node>
    <node id="n673">
      <data key="v_name">418686</data>
      <data key="v_label">418686</data>
    </node>
    <node id="n674">
      <data key="v_name">536696</data>
      <data key="v_label">536696</data>
    </node>
    <node id="n675">
      <data key="v_name">536697</data>
      <data key="v_label">536697</data>
    </node>
    <node id="n676">
      <data key="v_name">550914</data>
      <data key="v_label">550914</data>
    </node>
    <node id="n677">
      <data key="v_name">426604</data>
      <data key="v_label">426604</data>
    </node>
    <node id="n678">
      <data key="v_name">381444</data>
      <data key="v_label">381444</data>
    </node>
    <node id="n679">
      <data key="v_name">451177</data>
    </node>
    <node id="n680">
      <data key="v_name">523297</data>
      <data key="v_label">523297</data>
    </node>
    <node id="n681">
      <data key="v_name">362937</data>
      <data key="v_label">362937</data>
    </node>
    <node id="n682">
      <data key="v_name">125303</data>
      <data key="v_label">125303</data>
    </node>
    <node id="n683">
      <data key="v_name">506686</data>
    </node>
    <node id="n684">
      <data key="v_name">538206</data>
      <data key="v_label">538206</data>
    </node>
    <node id="n685">
      <data key="v_name">135301</data>
      <data key="v_label">135301</data>
    </node>
    <node id="n686">
      <data key="v_name">556513</data>
      <data key="v_label">556513</data>
    </node>
    <node id="n687">
      <data key="v_name">356253</data>
      <data key="v_label">356253</data>
    </node>
    <node id="n688">
      <data key="v_name">326169</data>
      <data key="v_label">326169</data>
    </node>
    <node id="n689">
      <data key="v_name">128803</data>
      <data key="v_label">128803</data>
    </node>
    <node id="n690">
      <data key="v_name">483545</data>
      <data key="v_label">483545</data>
    </node>
    <node id="n691">
      <data key="v_name">128805</data>
      <data key="v_label">128805</data>
    </node>
    <node id="n692">
      <data key="v_name">142856</data>
      <data key="v_label">142856</data>
    </node>
    <node id="n693">
      <data key="v_name">384752</data>
      <data key="v_label">384752</data>
    </node>
    <node id="n694">
      <data key="v_name">558131</data>
    </node>
    <node id="n695">
      <data key="v_name">472184</data>
      <data key="v_label">472184</data>
    </node>
    <node id="n696">
      <data key="v_name">515835</data>
      <data key="v_label">515835</data>
    </node>
    <node id="n697">
      <data key="v_name">528453</data>
      <data key="v_label">528453</data>
    </node>
    <node id="n698">
      <data key="v_name">248906</data>
      <data key="v_label">248906</data>
    </node>
    <node id="n699">
      <data key="v_name">507484</data>
    </node>
    <node id="n700">
      <data key="v_name">501444</data>
      <data key="v_label">501444</data>
    </node>
    <node id="n701">
      <data key="v_name">508293</data>
      <data key="v_label">508293</data>
    </node>
    <node id="n702">
      <data key="v_name">560221</data>
      <data key="v_label">560221</data>
    </node>
    <node id="n703">
      <data key="v_name">316328</data>
      <data key="v_label">316328</data>
    </node>
    <node id="n704">
      <data key="v_name">525973</data>
      <data key="v_label">525973</data>
    </node>
    <node id="n705">
      <data key="v_name">491898</data>
      <data key="v_label">491898</data>
    </node>
    <node id="n706">
      <data key="v_name">438934</data>
    </node>
    <node id="n707">
      <data key="v_name">555524</data>
      <data key="v_label">555524</data>
    </node>
    <node id="n708">
      <data key="v_name">475390</data>
      <data key="v_label">475390</data>
    </node>
    <node id="n709">
      <data key="v_name">229244</data>
      <data key="v_label">229244</data>
    </node>
    <node id="n710">
      <data key="v_name">558497</data>
      <data key="v_label">558497</data>
    </node>
    <node id="n711">
      <data key="v_name">161876</data>
      <data key="v_label">161876</data>
    </node>
    <node id="n712">
      <data key="v_name">561775</data>
    </node>
    <node id="n713">
      <data key="v_name">549952</data>
    </node>
    <node id="n714">
      <data key="v_name">309212</data>
      <data key="v_label">309212</data>
    </node>
    <node id="n715">
      <data key="v_name">142994</data>
      <data key="v_label">142994</data>
    </node>
    <node id="n716">
      <data key="v_name">518005</data>
      <data key="v_label">518005</data>
    </node>
    <node id="n717">
      <data key="v_name">466946</data>
      <data key="v_label">466946</data>
    </node>
    <node id="n718">
      <data key="v_name">456858</data>
      <data key="v_label">456858</data>
    </node>
    <node id="n719">
      <data key="v_name">554765</data>
      <data key="v_label">554765</data>
    </node>
    <node id="n720">
      <data key="v_name">491562</data>
      <data key="v_label">491562</data>
    </node>
    <node id="n721">
      <data key="v_name">550681</data>
      <data key="v_label">550681</data>
    </node>
    <node id="n722">
      <data key="v_name">527384</data>
      <data key="v_label">527384</data>
    </node>
    <node id="n723">
      <data key="v_name">528453</data>
    </node>
    <node id="n724">
      <data key="v_name">554765</data>
    </node>
    <node id="n725">
      <data key="v_name">31436</data>
      <data key="v_label">31436</data>
    </node>
    <node id="n726">
      <data key="v_name">518412</data>
      <data key="v_label">518412</data>
    </node>
    <node id="n727">
      <data key="v_name">553223</data>
      <data key="v_label">553223</data>
    </node>
    <node id="n728">
      <data key="v_name">550044</data>
      <data key="v_label">550044</data>
    </node>
    <node id="n729">
      <data key="v_name">526817</data>
      <data key="v_label">526817</data>
    </node>
    <node id="n730">
      <data key="v_name">161738</data>
      <data key="v_label">161738</data>
    </node>
    <node id="n731">
      <data key="v_name">453727</data>
      <data key="v_label">453727</data>
    </node>
    <node id="n732">
      <data key="v_name">530863</data>
      <data key="v_label">530863</data>
    </node>
    <node id="n733">
      <data key="v_name">368857</data>
      <data key="v_label">368857</data>
    </node>
    <node id="n734">
      <data key="v_name">368857</data>
    </node>
    <node id="n735">
      <data key="v_name">550643</data>
      <data key="v_label">550643</data>
    </node>
    <node id="n736">
      <data key="v_name">565158</data>
      <data key="v_label">565158</data>
    </node>
    <node id="n737">
      <data key="v_name">409678</data>
      <data key="v_label">409678</data>
    </node>
    <node id="n738">
      <data key="v_name">438709</data>
      <data key="v_label">438709</data>
    </node>
    <node id="n739">
      <data key="v_name">557651</data>
      <data key="v_label">557651</data>
    </node>
    <node id="n740">
      <data key="v_name">450764</data>
      <data key="v_label">450764</data>
    </node>
    <node id="n741">
      <data key="v_name">450774</data>
      <data key="v_label">450774</data>
    </node>
    <node id="n742">
      <data key="v_name">541775</data>
      <data key="v_label">541775</data>
    </node>
    <node id="n743">
      <data key="v_name">381508</data>
      <data key="v_label">381508</data>
    </node>
    <node id="n744">
      <data key="v_name">128418</data>
      <data key="v_label">128418</data>
    </node>
    <node id="n745">
      <data key="v_name">536010</data>
      <data key="v_label">536010</data>
    </node>
    <node id="n746">
      <data key="v_name">554881</data>
      <data key="v_label">554881</data>
    </node>
    <node id="n747">
      <data key="v_name">526637</data>
      <data key="v_label">526637</data>
    </node>
    <node id="n748">
      <data key="v_name">269338</data>
      <data key="v_label">269338</data>
    </node>
    <node id="n749">
      <data key="v_name">282036</data>
      <data key="v_label">282036</data>
    </node>
    <node id="n750">
      <data key="v_name">296385</data>
      <data key="v_label">296385</data>
    </node>
    <node id="n751">
      <data key="v_name">448703</data>
      <data key="v_label">448703</data>
    </node>
    <node id="n752">
      <data key="v_name">217949</data>
      <data key="v_label">217949</data>
    </node>
    <node id="n753">
      <data key="v_name">296371</data>
      <data key="v_label">296371</data>
    </node>
    <node id="n754">
      <data key="v_name">534923</data>
      <data key="v_label">534923</data>
    </node>
    <node id="n755">
      <data key="v_name">142891</data>
      <data key="v_label">142891</data>
    </node>
    <node id="n756">
      <data key="v_name">543625</data>
      <data key="v_label">543625</data>
    </node>
    <node id="n757">
      <data key="v_name">516488</data>
      <data key="v_label">516488</data>
    </node>
    <node id="n758">
      <data key="v_name">142610</data>
      <data key="v_label">142610</data>
    </node>
    <node id="n759">
      <data key="v_name">447448</data>
      <data key="v_label">447448</data>
    </node>
    <node id="n760">
      <data key="v_name">401830</data>
      <data key="v_label">401830</data>
    </node>
    <node id="n761">
      <data key="v_name">384241</data>
      <data key="v_label">384241</data>
    </node>
    <node id="n762">
      <data key="v_name">459571</data>
      <data key="v_label">459571</data>
    </node>
    <node id="n763">
      <data key="v_name">562760</data>
      <data key="v_label">562760</data>
    </node>
    <node id="n764">
      <data key="v_name">196076</data>
    </node>
    <node id="n765">
      <data key="v_name">140260</data>
    </node>
    <node id="n766">
      <data key="v_name">345982</data>
    </node>
    <node id="n767">
      <data key="v_name">142849</data>
      <data key="v_label">142849</data>
    </node>
    <node id="n768">
      <data key="v_name">551122</data>
      <data key="v_label">551122</data>
    </node>
    <node id="n769">
      <data key="v_name">463668</data>
      <data key="v_label">463668</data>
    </node>
    <node id="n770">
      <data key="v_name">483791</data>
      <data key="v_label">483791</data>
    </node>
    <node id="n771">
      <data key="v_name">140250</data>
      <data key="v_label">140250</data>
    </node>
    <node id="n772">
      <data key="v_name">483791</data>
    </node>
    <node id="n773">
      <data key="v_name">542482</data>
      <data key="v_label">542482</data>
    </node>
    <node id="n774">
      <data key="v_name">557160</data>
      <data key="v_label">557160</data>
    </node>
    <node id="n775">
      <data key="v_name">345579</data>
      <data key="v_label">345579</data>
    </node>
    <node id="n776">
      <data key="v_name">557162</data>
    </node>
    <node id="n777">
      <data key="v_name">353897</data>
      <data key="v_label">353897</data>
    </node>
    <node id="n778">
      <data key="v_name">456792</data>
      <data key="v_label">456792</data>
    </node>
    <node id="n779">
      <data key="v_name">517444</data>
      <data key="v_label">517444</data>
    </node>
    <node id="n780">
      <data key="v_name">142866</data>
      <data key="v_label">142866</data>
    </node>
    <node id="n781">
      <data key="v_name">220803</data>
      <data key="v_label">220803</data>
    </node>
    <node id="n782">
      <data key="v_name">515756</data>
      <data key="v_label">515756</data>
    </node>
    <node id="n783">
      <data key="v_name">142856</data>
    </node>
    <node id="n784">
      <data key="v_name">296399</data>
      <data key="v_label">296399</data>
    </node>
    <node id="n785">
      <data key="v_name">549664</data>
      <data key="v_label">549664</data>
    </node>
    <node id="n786">
      <data key="v_name">193664</data>
      <data key="v_label">193664</data>
    </node>
    <node id="n787">
      <data key="v_name">341681</data>
      <data key="v_label">341681</data>
    </node>
    <node id="n788">
      <data key="v_name">246370</data>
      <data key="v_label">246370</data>
    </node>
    <node id="n789">
      <data key="v_name">125068</data>
      <data key="v_label">125068</data>
    </node>
    <node id="n790">
      <data key="v_name">129466</data>
      <data key="v_label">129466</data>
    </node>
    <node id="n791">
      <data key="v_name">536698</data>
      <data key="v_label">536698</data>
    </node>
    <node id="n792">
      <data key="v_name">483390</data>
      <data key="v_label">483390</data>
    </node>
    <node id="n793">
      <data key="v_name">551079</data>
      <data key="v_label">551079</data>
    </node>
    <node id="n794">
      <data key="v_name">201028</data>
      <data key="v_label">201028</data>
    </node>
    <node id="n795">
      <data key="v_name">140046</data>
      <data key="v_label">140046</data>
    </node>
    <node id="n796">
      <data key="v_name">548135</data>
      <data key="v_label">548135</data>
    </node>
    <node id="n797">
      <data key="v_name">548132</data>
      <data key="v_label">548132</data>
    </node>
    <node id="n798">
      <data key="v_name">524966</data>
      <data key="v_label">524966</data>
    </node>
    <node id="n799">
      <data key="v_name">384098</data>
      <data key="v_label">384098</data>
    </node>
    <node id="n800">
      <data key="v_name">261975</data>
    </node>
    <node id="n801">
      <data key="v_name">518625</data>
      <data key="v_label">518625</data>
    </node>
    <node id="n802">
      <data key="v_name">320975</data>
    </node>
    <node id="n803">
      <data key="v_name">533380</data>
      <data key="v_label">533380</data>
    </node>
    <node id="n804">
      <data key="v_name">550995</data>
      <data key="v_label">550995</data>
    </node>
    <node id="n805">
      <data key="v_name">515465</data>
      <data key="v_label">515465</data>
    </node>
    <node id="n806">
      <data key="v_name">507752</data>
      <data key="v_label">507752</data>
    </node>
    <node id="n807">
      <data key="v_name">549542</data>
      <data key="v_label">549542</data>
    </node>
    <node id="n808">
      <data key="v_name">490628</data>
      <data key="v_label">490628</data>
    </node>
    <node id="n809">
      <data key="v_name">450848</data>
      <data key="v_label">450848</data>
    </node>
    <node id="n810">
      <data key="v_name">139678</data>
      <data key="v_label">139678</data>
    </node>
    <node id="n811">
      <data key="v_name">257164</data>
      <data key="v_label">257164</data>
    </node>
    <node id="n812">
      <data key="v_name">498256</data>
      <data key="v_label">498256</data>
    </node>
    <node id="n813">
      <data key="v_name">564156</data>
      <data key="v_label">564156</data>
    </node>
    <node id="n814">
      <data key="v_name">549757</data>
    </node>
    <node id="n815">
      <data key="v_name">186547</data>
      <data key="v_label">186547</data>
    </node>
    <node id="n816">
      <data key="v_name">508256</data>
    </node>
    <node id="n817">
      <data key="v_name">447061</data>
      <data key="v_label">447061</data>
    </node>
    <node id="n818">
      <data key="v_name">139665</data>
      <data key="v_label">139665</data>
    </node>
    <node id="n819">
      <data key="v_name">278132</data>
      <data key="v_label">278132</data>
    </node>
    <node id="n820">
      <data key="v_name">139667</data>
      <data key="v_label">139667</data>
    </node>
    <node id="n821">
      <data key="v_name">561621</data>
    </node>
    <node id="n822">
      <data key="v_name">521278</data>
      <data key="v_label">521278</data>
    </node>
    <node id="n823">
      <data key="v_name">397849</data>
      <data key="v_label">397849</data>
    </node>
    <node id="n824">
      <data key="v_name">495182</data>
    </node>
    <node id="n825">
      <data key="v_name">279394</data>
      <data key="v_label">279394</data>
    </node>
    <node id="n826">
      <data key="v_name">416071</data>
      <data key="v_label">416071</data>
    </node>
    <node id="n827">
      <data key="v_name">428845</data>
    </node>
    <node id="n828">
      <data key="v_name">530335</data>
    </node>
    <node id="n829">
      <data key="v_name">309350</data>
      <data key="v_label">309350</data>
    </node>
    <node id="n830">
      <data key="v_name">342042</data>
    </node>
    <node id="n831">
      <data key="v_name">409715</data>
      <data key="v_label">409715</data>
    </node>
    <node id="n832">
      <data key="v_name">434524</data>
      <data key="v_label">434524</data>
    </node>
    <node id="n833">
      <data key="v_name">563479</data>
      <data key="v_label">563479</data>
    </node>
    <node id="n834">
      <data key="v_name">542046</data>
      <data key="v_label">542046</data>
    </node>
    <node id="n835">
      <data key="v_name">159328</data>
      <data key="v_label">159328</data>
    </node>
    <node id="n836">
      <data key="v_name">138468</data>
      <data key="v_label">138468</data>
    </node>
    <node id="n837">
      <data key="v_name">156364</data>
      <data key="v_label">156364</data>
    </node>
    <node id="n838">
      <data key="v_name">228890</data>
      <data key="v_label">228890</data>
    </node>
    <node id="n839">
      <data key="v_name">515703</data>
      <data key="v_label">515703</data>
    </node>
    <node id="n840">
      <data key="v_name">438618</data>
    </node>
    <node id="n841">
      <data key="v_name">483871</data>
    </node>
    <node id="n842">
      <data key="v_name">519250</data>
      <data key="v_label">519250</data>
    </node>
    <node id="n843">
      <data key="v_name">174830</data>
      <data key="v_label">174830</data>
    </node>
    <node id="n844">
      <data key="v_name">229260</data>
      <data key="v_label">229260</data>
    </node>
    <node id="n845">
      <data key="v_name">560601</data>
      <data key="v_label">560601</data>
    </node>
    <node id="n846">
      <data key="v_name">241298</data>
      <data key="v_label">241298</data>
    </node>
    <node id="n847">
      <data key="v_name">550173</data>
      <data key="v_label">550173</data>
    </node>
    <node id="n848">
      <data key="v_name">217789</data>
      <data key="v_label">217789</data>
    </node>
    <node id="n849">
      <data key="v_name">549718</data>
      <data key="v_label">549718</data>
    </node>
    <node id="n850">
      <data key="v_name">549718</data>
    </node>
    <node id="n851">
      <data key="v_name">540805</data>
      <data key="v_label">540805</data>
    </node>
    <node id="n852">
      <data key="v_name">564815</data>
    </node>
    <node id="n853">
      <data key="v_name">214928</data>
      <data key="v_label">214928</data>
    </node>
    <node id="n854">
      <data key="v_name">138901</data>
      <data key="v_label">138901</data>
    </node>
    <node id="n855">
      <data key="v_name">450767</data>
      <data key="v_label">450767</data>
    </node>
    <node id="n856">
      <data key="v_name">450799</data>
      <data key="v_label">450799</data>
    </node>
    <node id="n857">
      <data key="v_name">140323</data>
      <data key="v_label">140323</data>
    </node>
    <node id="n858">
      <data key="v_name">536697</data>
    </node>
    <node id="n859">
      <data key="v_name">500153</data>
      <data key="v_label">500153</data>
    </node>
    <node id="n860">
      <data key="v_name">426604</data>
    </node>
    <node id="n861">
      <data key="v_name">486626</data>
      <data key="v_label">486626</data>
    </node>
    <node id="n862">
      <data key="v_name">549356</data>
    </node>
    <node id="n863">
      <data key="v_name">143021</data>
      <data key="v_label">143021</data>
    </node>
    <node id="n864">
      <data key="v_name">236644</data>
      <data key="v_label">236644</data>
    </node>
    <node id="n865">
      <data key="v_name">263691</data>
      <data key="v_label">263691</data>
    </node>
    <node id="n866">
      <data key="v_name">494943</data>
      <data key="v_label">494943</data>
    </node>
    <node id="n867">
      <data key="v_name">543625</data>
    </node>
    <node id="n868">
      <data key="v_name">472850</data>
      <data key="v_label">472850</data>
    </node>
    <node id="n869">
      <data key="v_name">475511</data>
      <data key="v_label">475511</data>
    </node>
    <node id="n870">
      <data key="v_name">518545</data>
      <data key="v_label">518545</data>
    </node>
    <node id="n871">
      <data key="v_name">410068</data>
      <data key="v_label">410068</data>
    </node>
    <node id="n872">
      <data key="v_name">173279</data>
      <data key="v_label">173279</data>
    </node>
    <node id="n873">
      <data key="v_name">517415</data>
      <data key="v_label">517415</data>
    </node>
    <node id="n874">
      <data key="v_name">475390</data>
    </node>
    <node id="n875">
      <data key="v_name">550740</data>
      <data key="v_label">550740</data>
    </node>
    <node id="n876">
      <data key="v_name">489873</data>
      <data key="v_label">489873</data>
    </node>
    <node id="n877">
      <data key="v_name">142436</data>
      <data key="v_label">142436</data>
    </node>
    <node id="n878">
      <data key="v_name">128390</data>
      <data key="v_label">128390</data>
    </node>
    <node id="n879">
      <data key="v_name">128391</data>
      <data key="v_label">128391</data>
    </node>
    <node id="n880">
      <data key="v_name">168543</data>
      <data key="v_label">168543</data>
    </node>
    <node id="n881">
      <data key="v_name">408878</data>
      <data key="v_label">408878</data>
    </node>
    <node id="n882">
      <data key="v_name">540776</data>
      <data key="v_label">540776</data>
    </node>
    <node id="n883">
      <data key="v_name">557461</data>
      <data key="v_label">557461</data>
    </node>
    <node id="n884">
      <data key="v_name">561784</data>
    </node>
    <node id="n885">
      <data key="v_name">503214</data>
      <data key="v_label">503214</data>
    </node>
    <node id="n886">
      <data key="v_name">560649</data>
      <data key="v_label">560649</data>
    </node>
    <node id="n887">
      <data key="v_name">549458</data>
      <data key="v_label">549458</data>
    </node>
    <node id="n888">
      <data key="v_name">31436</data>
    </node>
    <node id="n889">
      <data key="v_name">518412</data>
    </node>
    <node id="n890">
      <data key="v_name">550173</data>
    </node>
    <node id="n891">
      <data key="v_name">540163</data>
      <data key="v_label">540163</data>
    </node>
    <node id="n892">
      <data key="v_name">518069</data>
      <data key="v_label">518069</data>
    </node>
    <node id="n893">
      <data key="v_name">531512</data>
      <data key="v_label">531512</data>
    </node>
    <node id="n894">
      <data key="v_name">472159</data>
    </node>
    <node id="n895">
      <data key="v_name">543568</data>
      <data key="v_label">543568</data>
    </node>
    <node id="n896">
      <data key="v_name">332348</data>
    </node>
    <node id="n897">
      <data key="v_name">346688</data>
      <data key="v_label">346688</data>
    </node>
    <node id="n898">
      <data key="v_name">140158</data>
      <data key="v_label">140158</data>
    </node>
    <node id="n899">
      <data key="v_name">507608</data>
      <data key="v_label">507608</data>
    </node>
    <node id="n900">
      <data key="v_name">380326</data>
      <data key="v_label">380326</data>
    </node>
    <node id="n901">
      <data key="v_name">539762</data>
      <data key="v_label">539762</data>
    </node>
    <node id="n902">
      <data key="v_name">315673</data>
      <data key="v_label">315673</data>
    </node>
    <node id="n903">
      <data key="v_name">308372</data>
      <data key="v_label">308372</data>
    </node>
    <node id="n904">
      <data key="v_name">199459</data>
      <data key="v_label">199459</data>
    </node>
    <node id="n905">
      <data key="v_name">549841</data>
      <data key="v_label">549841</data>
    </node>
    <node id="n906">
      <data key="v_name">549542</data>
    </node>
    <node id="n907">
      <data key="v_name">517887</data>
      <data key="v_label">517887</data>
    </node>
    <node id="n908">
      <data key="v_name">543509</data>
    </node>
    <node id="n909">
      <data key="v_name">550864</data>
      <data key="v_label">550864</data>
    </node>
    <node id="n910">
      <data key="v_name">517880</data>
      <data key="v_label">517880</data>
    </node>
    <node id="n911">
      <data key="v_name">519591</data>
      <data key="v_label">519591</data>
    </node>
    <node id="n912">
      <data key="v_name">356244</data>
    </node>
    <node id="n913">
      <data key="v_name">196924</data>
      <data key="v_label">196924</data>
    </node>
    <node id="n914">
      <data key="v_name">562016</data>
      <data key="v_label">562016</data>
    </node>
    <node id="n915">
      <data key="v_name">367467</data>
      <data key="v_label">367467</data>
    </node>
    <node id="n916">
      <data key="v_name">497068</data>
      <data key="v_label">497068</data>
    </node>
    <node id="n917">
      <data key="v_name">540846</data>
    </node>
    <node id="n918">
      <data key="v_name">555419</data>
      <data key="v_label">555419</data>
    </node>
    <node id="n919">
      <data key="v_name">557751</data>
      <data key="v_label">557751</data>
    </node>
    <node id="n920">
      <data key="v_name">551055</data>
      <data key="v_label">551055</data>
    </node>
    <node id="n921">
      <data key="v_name">382922</data>
      <data key="v_label">382922</data>
    </node>
    <node id="n922">
      <data key="v_name">561785</data>
      <data key="v_label">561785</data>
    </node>
    <node id="n923">
      <data key="v_name">554891</data>
      <data key="v_label">554891</data>
    </node>
    <node id="n924">
      <data key="v_name">388441</data>
      <data key="v_label">388441</data>
    </node>
    <node id="n925">
      <data key="v_name">491915</data>
    </node>
    <node id="n926">
      <data key="v_name">531589</data>
      <data key="v_label">531589</data>
    </node>
    <node id="n927">
      <data key="v_name">466424</data>
      <data key="v_label">466424</data>
    </node>
    <node id="n928">
      <data key="v_name">139706</data>
      <data key="v_label">139706</data>
    </node>
    <node id="n929">
      <data key="v_name">375530</data>
      <data key="v_label">375530</data>
    </node>
    <node id="n930">
      <data key="v_name">187937</data>
      <data key="v_label">187937</data>
    </node>
    <node id="n931">
      <data key="v_name">375531</data>
      <data key="v_label">375531</data>
    </node>
    <node id="n932">
      <data key="v_name">384275</data>
      <data key="v_label">384275</data>
    </node>
    <node id="n933">
      <data key="v_name">548297</data>
    </node>
    <node id="n934">
      <data key="v_name">450550</data>
      <data key="v_label">450550</data>
    </node>
    <node id="n935">
      <data key="v_name">560669</data>
      <data key="v_label">560669</data>
    </node>
    <node id="n936">
      <data key="v_name">406425</data>
      <data key="v_label">406425</data>
    </node>
    <node id="n937">
      <data key="v_name">251935</data>
      <data key="v_label">251935</data>
    </node>
    <node id="n938">
      <data key="v_name">426282</data>
      <data key="v_label">426282</data>
    </node>
    <node id="n939">
      <data key="v_name">235605</data>
      <data key="v_label">235605</data>
    </node>
    <node id="n940">
      <data key="v_name">547942</data>
      <data key="v_label">547942</data>
    </node>
    <node id="n941">
      <data key="v_name">184104</data>
      <data key="v_label">184104</data>
    </node>
    <node id="n942">
      <data key="v_name">396781</data>
      <data key="v_label">396781</data>
    </node>
    <node id="n943">
      <data key="v_name">227957</data>
      <data key="v_label">227957</data>
    </node>
    <node id="n944">
      <data key="v_name">409565</data>
      <data key="v_label">409565</data>
    </node>
    <node id="n945">
      <data key="v_name">450916</data>
      <data key="v_label">450916</data>
    </node>
    <node id="n946">
      <data key="v_name">553906</data>
      <data key="v_label">553906</data>
    </node>
    <node id="n947">
      <data key="v_name">485593</data>
    </node>
    <node id="n948">
      <data key="v_name">494668</data>
      <data key="v_label">494668</data>
    </node>
    <node id="n949">
      <data key="v_name">140350</data>
      <data key="v_label">140350</data>
    </node>
    <node id="n950">
      <data key="v_name">510422</data>
      <data key="v_label">510422</data>
    </node>
    <node id="n951">
      <data key="v_name">384253</data>
      <data key="v_label">384253</data>
    </node>
    <node id="n952">
      <data key="v_name">511691</data>
      <data key="v_label">511691</data>
    </node>
    <node id="n953">
      <data key="v_name">193642</data>
      <data key="v_label">193642</data>
    </node>
    <node id="n954">
      <data key="v_name">123164</data>
      <data key="v_label">123164</data>
    </node>
    <node id="n955">
      <data key="v_name">461864</data>
      <data key="v_label">461864</data>
    </node>
    <node id="n956">
      <data key="v_name">553286</data>
      <data key="v_label">553286</data>
    </node>
    <node id="n957">
      <data key="v_name">554983</data>
      <data key="v_label">554983</data>
    </node>
    <node id="n958">
      <data key="v_name">456808</data>
      <data key="v_label">456808</data>
    </node>
    <node id="n959">
      <data key="v_name">550631</data>
      <data key="v_label">550631</data>
    </node>
    <node id="n960">
      <data key="v_name">549998</data>
      <data key="v_label">549998</data>
    </node>
    <node id="n961">
      <data key="v_name">283133</data>
      <data key="v_label">283133</data>
    </node>
    <node id="n962">
      <data key="v_name">521101</data>
    </node>
    <node id="n963">
      <data key="v_name">542485</data>
      <data key="v_label">542485</data>
    </node>
    <node id="n964">
      <data key="v_name">465209</data>
      <data key="v_label">465209</data>
    </node>
    <node id="n965">
      <data key="v_name">139747</data>
      <data key="v_label">139747</data>
    </node>
    <node id="n966">
      <data key="v_name">321051</data>
      <data key="v_label">321051</data>
    </node>
    <node id="n967">
      <data key="v_name">561855</data>
      <data key="v_label">561855</data>
    </node>
    <node id="n968">
      <data key="v_name">555188</data>
      <data key="v_label">555188</data>
    </node>
    <node id="n969">
      <data key="v_name">474015</data>
    </node>
    <node id="n970">
      <data key="v_name">413474</data>
      <data key="v_label">413474</data>
    </node>
    <node id="n971">
      <data key="v_name">140136</data>
      <data key="v_label">140136</data>
    </node>
    <node id="n972">
      <data key="v_name">224018</data>
      <data key="v_label">224018</data>
    </node>
    <node id="n973">
      <data key="v_name">224019</data>
      <data key="v_label">224019</data>
    </node>
    <node id="n974">
      <data key="v_name">140139</data>
      <data key="v_label">140139</data>
    </node>
    <node id="n975">
      <data key="v_name">224072</data>
      <data key="v_label">224072</data>
    </node>
    <node id="n976">
      <data key="v_name">428505</data>
      <data key="v_label">428505</data>
    </node>
    <node id="n977">
      <data key="v_name">529626</data>
      <data key="v_label">529626</data>
    </node>
    <node id="n978">
      <data key="v_name">138749</data>
      <data key="v_label">138749</data>
    </node>
    <node id="n979">
      <data key="v_name">109619</data>
      <data key="v_label">109619</data>
    </node>
    <node id="n980">
      <data key="v_name">531979</data>
      <data key="v_label">531979</data>
    </node>
    <node id="n981">
      <data key="v_name">560997</data>
      <data key="v_label">560997</data>
    </node>
    <node id="n982">
      <data key="v_name">139560</data>
      <data key="v_label">139560</data>
    </node>
    <node id="n983">
      <data key="v_name">291712</data>
    </node>
    <node id="n984">
      <data key="v_name">517727</data>
      <data key="v_label">517727</data>
    </node>
    <node id="n985">
      <data key="v_name">179869</data>
    </node>
    <node id="n986">
      <data key="v_name">425345</data>
      <data key="v_label">425345</data>
    </node>
    <node id="n987">
      <data key="v_name">548359</data>
      <data key="v_label">548359</data>
    </node>
    <node id="n988">
      <data key="v_name">394020</data>
      <data key="v_label">394020</data>
    </node>
    <node id="n989">
      <data key="v_name">217452</data>
    </node>
    <node id="n990">
      <data key="v_name">499628</data>
      <data key="v_label">499628</data>
    </node>
    <node id="n991">
      <data key="v_name">485281</data>
      <data key="v_label">485281</data>
    </node>
    <node id="n992">
      <data key="v_name">464121</data>
      <data key="v_label">464121</data>
    </node>
    <node id="n993">
      <data key="v_name">316235</data>
      <data key="v_label">316235</data>
    </node>
    <node id="n994">
      <data key="v_name">316236</data>
      <data key="v_label">316236</data>
    </node>
    <node id="n995">
      <data key="v_name">279647</data>
      <data key="v_label">279647</data>
    </node>
    <node id="n996">
      <data key="v_name">231158</data>
      <data key="v_label">231158</data>
    </node>
    <node id="n997">
      <data key="v_name">558959</data>
    </node>
    <node id="n998">
      <data key="v_name">123263</data>
      <data key="v_label">123263</data>
    </node>
    <node id="n999">
      <data key="v_name">543560</data>
      <data key="v_label">543560</data>
    </node>
    <node id="n1000">
      <data key="v_name">451118</data>
      <data key="v_label">451118</data>
    </node>
    <node id="n1001">
      <data key="v_name">530418</data>
      <data key="v_label">530418</data>
    </node>
    <node id="n1002">
      <data key="v_name">337768</data>
      <data key="v_label">337768</data>
    </node>
    <node id="n1003">
      <data key="v_name">418721</data>
      <data key="v_label">418721</data>
    </node>
    <node id="n1004">
      <data key="v_name">361065</data>
      <data key="v_label">361065</data>
    </node>
    <node id="n1005">
      <data key="v_name">135703</data>
      <data key="v_label">135703</data>
    </node>
    <node id="n1006">
      <data key="v_name">518063</data>
      <data key="v_label">518063</data>
    </node>
    <node id="n1007">
      <data key="v_name">542106</data>
      <data key="v_label">542106</data>
    </node>
    <node id="n1008">
      <data key="v_name">542107</data>
      <data key="v_label">542107</data>
    </node>
    <node id="n1009">
      <data key="v_name">127055</data>
      <data key="v_label">127055</data>
    </node>
    <node id="n1010">
      <data key="v_name">485591</data>
      <data key="v_label">485591</data>
    </node>
    <node id="n1011">
      <data key="v_name">435447</data>
      <data key="v_label">435447</data>
    </node>
    <node id="n1012">
      <data key="v_name">518640</data>
      <data key="v_label">518640</data>
    </node>
    <node id="n1013">
      <data key="v_name">444397</data>
      <data key="v_label">444397</data>
    </node>
    <node id="n1014">
      <data key="v_name">472357</data>
      <data key="v_label">472357</data>
    </node>
    <node id="n1015">
      <data key="v_name">535599</data>
      <data key="v_label">535599</data>
    </node>
    <node id="n1016">
      <data key="v_name">430298</data>
      <data key="v_label">430298</data>
    </node>
    <node id="n1017">
      <data key="v_name">409909</data>
      <data key="v_label">409909</data>
    </node>
    <node id="n1018">
      <data key="v_name">480998</data>
      <data key="v_label">480998</data>
    </node>
    <node id="n1019">
      <data key="v_name">550080</data>
      <data key="v_label">550080</data>
    </node>
    <node id="n1020">
      <data key="v_name">525682</data>
      <data key="v_label">525682</data>
    </node>
    <node id="n1021">
      <data key="v_name">495891</data>
      <data key="v_label">495891</data>
    </node>
    <node id="n1022">
      <data key="v_name">511555</data>
      <data key="v_label">511555</data>
    </node>
    <node id="n1023">
      <data key="v_name">525751</data>
      <data key="v_label">525751</data>
    </node>
    <node id="n1024">
      <data key="v_name">548125</data>
      <data key="v_label">548125</data>
    </node>
    <node id="n1025">
      <data key="v_name">485106</data>
      <data key="v_label">485106</data>
    </node>
    <node id="n1026">
      <data key="v_name">559638</data>
      <data key="v_label">559638</data>
    </node>
    <node id="n1027">
      <data key="v_name">501195</data>
      <data key="v_label">501195</data>
    </node>
    <node id="n1028">
      <data key="v_name">335880</data>
      <data key="v_label">335880</data>
    </node>
    <node id="n1029">
      <data key="v_name">550443</data>
      <data key="v_label">550443</data>
    </node>
    <node id="n1030">
      <data key="v_name">433915</data>
      <data key="v_label">433915</data>
    </node>
    <node id="n1031">
      <data key="v_name">309332</data>
      <data key="v_label">309332</data>
    </node>
    <node id="n1032">
      <data key="v_name">517321</data>
    </node>
    <node id="n1033">
      <data key="v_name">125136</data>
      <data key="v_label">125136</data>
    </node>
    <node id="n1034">
      <data key="v_name">450550</data>
    </node>
    <node id="n1035">
      <data key="v_name">262266</data>
      <data key="v_label">262266</data>
    </node>
    <node id="n1036">
      <data key="v_name">142819</data>
      <data key="v_label">142819</data>
    </node>
    <node id="n1037">
      <data key="v_name">562621</data>
      <data key="v_label">562621</data>
    </node>
    <node id="n1038">
      <data key="v_name">532981</data>
      <data key="v_label">532981</data>
    </node>
    <node id="n1039">
      <data key="v_name">397101</data>
      <data key="v_label">397101</data>
    </node>
    <node id="n1040">
      <data key="v_name">448702</data>
      <data key="v_label">448702</data>
    </node>
    <node id="n1041">
      <data key="v_name">483625</data>
      <data key="v_label">483625</data>
    </node>
    <node id="n1042">
      <data key="v_name">497162</data>
      <data key="v_label">497162</data>
    </node>
    <node id="n1043">
      <data key="v_name">518052</data>
      <data key="v_label">518052</data>
    </node>
    <node id="n1044">
      <data key="v_name">449907</data>
      <data key="v_label">449907</data>
    </node>
    <node id="n1045">
      <data key="v_name">244163</data>
      <data key="v_label">244163</data>
    </node>
    <node id="n1046">
      <data key="v_name">524314</data>
      <data key="v_label">524314</data>
    </node>
    <node id="n1047">
      <data key="v_name">486626</data>
    </node>
    <node id="n1048">
      <data key="v_name">275409</data>
      <data key="v_label">275409</data>
    </node>
    <node id="n1049">
      <data key="v_name">517149</data>
      <data key="v_label">517149</data>
    </node>
    <node id="n1050">
      <data key="v_name">517413</data>
      <data key="v_label">517413</data>
    </node>
    <node id="n1051">
      <data key="v_name">550484</data>
    </node>
    <node id="n1052">
      <data key="v_name">564485</data>
      <data key="v_label">564485</data>
    </node>
    <node id="n1053">
      <data key="v_name">456909</data>
    </node>
    <node id="n1054">
      <data key="v_name">470450</data>
      <data key="v_label">470450</data>
    </node>
    <node id="n1055">
      <data key="v_name">381437</data>
    </node>
    <node id="n1056">
      <data key="v_name">509442</data>
      <data key="v_label">509442</data>
    </node>
    <node id="n1057">
      <data key="v_name">348133</data>
    </node>
    <node id="n1058">
      <data key="v_name">303296</data>
      <data key="v_label">303296</data>
    </node>
    <node id="n1059">
      <data key="v_name">142639</data>
      <data key="v_label">142639</data>
    </node>
    <node id="n1060">
      <data key="v_name">398856</data>
      <data key="v_label">398856</data>
    </node>
    <node id="n1061">
      <data key="v_name">142632</data>
      <data key="v_label">142632</data>
    </node>
    <node id="n1062">
      <data key="v_name">490497</data>
      <data key="v_label">490497</data>
    </node>
    <node id="n1063">
      <data key="v_name">340620</data>
      <data key="v_label">340620</data>
    </node>
    <node id="n1064">
      <data key="v_name">142635</data>
      <data key="v_label">142635</data>
    </node>
    <node id="n1065">
      <data key="v_name">473916</data>
      <data key="v_label">473916</data>
    </node>
    <node id="n1066">
      <data key="v_name">435983</data>
      <data key="v_label">435983</data>
    </node>
    <node id="n1067">
      <data key="v_name">483572</data>
      <data key="v_label">483572</data>
    </node>
    <node id="n1068">
      <data key="v_name">421048</data>
      <data key="v_label">421048</data>
    </node>
    <node id="n1069">
      <data key="v_name">456451</data>
      <data key="v_label">456451</data>
    </node>
    <node id="n1070">
      <data key="v_name">553663</data>
      <data key="v_label">553663</data>
    </node>
    <node id="n1071">
      <data key="v_name">550244</data>
      <data key="v_label">550244</data>
    </node>
    <node id="n1072">
      <data key="v_name">271545</data>
      <data key="v_label">271545</data>
    </node>
    <node id="n1073">
      <data key="v_name">538402</data>
    </node>
    <node id="n1074">
      <data key="v_name">559805</data>
      <data key="v_label">559805</data>
    </node>
    <node id="n1075">
      <data key="v_name">436421</data>
      <data key="v_label">436421</data>
    </node>
    <node id="n1076">
      <data key="v_name">211900</data>
    </node>
    <node id="n1077">
      <data key="v_name">333038</data>
      <data key="v_label">333038</data>
    </node>
    <node id="n1078">
      <data key="v_name">381437</data>
    </node>
    <node id="n1079">
      <data key="v_name">483541</data>
      <data key="v_label">483541</data>
    </node>
    <node id="n1080">
      <data key="v_name">286162</data>
      <data key="v_label">286162</data>
    </node>
    <node id="n1081">
      <data key="v_name">286163</data>
      <data key="v_label">286163</data>
    </node>
    <node id="n1082">
      <data key="v_name">286164</data>
      <data key="v_label">286164</data>
    </node>
    <node id="n1083">
      <data key="v_name">345982</data>
    </node>
    <node id="n1084">
      <data key="v_name">142146</data>
      <data key="v_label">142146</data>
    </node>
    <node id="n1085">
      <data key="v_name">303970</data>
      <data key="v_label">303970</data>
    </node>
    <node id="n1086">
      <data key="v_name">537184</data>
      <data key="v_label">537184</data>
    </node>
    <node id="n1087">
      <data key="v_name">507869</data>
      <data key="v_label">507869</data>
    </node>
    <node id="n1088">
      <data key="v_name">146322</data>
      <data key="v_label">146322</data>
    </node>
    <node id="n1089">
      <data key="v_name">529904</data>
      <data key="v_label">529904</data>
    </node>
    <node id="n1090">
      <data key="v_name">431529</data>
      <data key="v_label">431529</data>
    </node>
    <node id="n1091">
      <data key="v_name">182786</data>
    </node>
    <node id="n1092">
      <data key="v_name">547185</data>
      <data key="v_label">547185</data>
    </node>
    <node id="n1093">
      <data key="v_name">425934</data>
      <data key="v_label">425934</data>
    </node>
    <node id="n1094">
      <data key="v_name">494943</data>
    </node>
    <node id="n1095">
      <data key="v_name">519850</data>
      <data key="v_label">519850</data>
    </node>
    <node id="n1096">
      <data key="v_name">146337</data>
      <data key="v_label">146337</data>
    </node>
    <node id="n1097">
      <data key="v_name">510094</data>
      <data key="v_label">510094</data>
    </node>
    <node id="n1098">
      <data key="v_name">142729</data>
      <data key="v_label">142729</data>
    </node>
    <node id="n1099">
      <data key="v_name">370768</data>
    </node>
    <node id="n1100">
      <data key="v_name">540494</data>
    </node>
    <node id="n1101">
      <data key="v_name">142732</data>
      <data key="v_label">142732</data>
    </node>
    <node id="n1102">
      <data key="v_name">463100</data>
      <data key="v_label">463100</data>
    </node>
    <node id="n1103">
      <data key="v_name">475434</data>
      <data key="v_label">475434</data>
    </node>
    <node id="n1104">
      <data key="v_name">466323</data>
      <data key="v_label">466323</data>
    </node>
    <node id="n1105">
      <data key="v_name">262262</data>
      <data key="v_label">262262</data>
    </node>
    <node id="n1106">
      <data key="v_name">504468</data>
      <data key="v_label">504468</data>
    </node>
    <node id="n1107">
      <data key="v_name">450771</data>
      <data key="v_label">450771</data>
    </node>
    <node id="n1108">
      <data key="v_name">469318</data>
      <data key="v_label">469318</data>
    </node>
    <node id="n1109">
      <data key="v_name">335067</data>
      <data key="v_label">335067</data>
    </node>
    <node id="n1110">
      <data key="v_name">451905</data>
      <data key="v_label">451905</data>
    </node>
    <node id="n1111">
      <data key="v_name">142702</data>
      <data key="v_label">142702</data>
    </node>
    <node id="n1112">
      <data key="v_name">550184</data>
    </node>
    <node id="n1113">
      <data key="v_name">513826</data>
      <data key="v_label">513826</data>
    </node>
    <node id="n1114">
      <data key="v_name">426630</data>
      <data key="v_label">426630</data>
    </node>
    <node id="n1115">
      <data key="v_name">145824</data>
      <data key="v_label">145824</data>
    </node>
    <node id="n1116">
      <data key="v_name">555866</data>
    </node>
    <node id="n1117">
      <data key="v_name">403485</data>
    </node>
    <node id="n1118">
      <data key="v_name">551079</data>
    </node>
    <node id="n1119">
      <data key="v_name">550948</data>
      <data key="v_label">550948</data>
    </node>
    <node id="n1120">
      <data key="v_name">142722</data>
      <data key="v_label">142722</data>
    </node>
    <node id="n1121">
      <data key="v_name">408589</data>
      <data key="v_label">408589</data>
    </node>
    <node id="n1122">
      <data key="v_name">485891</data>
      <data key="v_label">485891</data>
    </node>
    <node id="n1123">
      <data key="v_name">168133</data>
      <data key="v_label">168133</data>
    </node>
    <node id="n1124">
      <data key="v_name">142715</data>
      <data key="v_label">142715</data>
    </node>
    <node id="n1125">
      <data key="v_name">554195</data>
      <data key="v_label">554195</data>
    </node>
    <node id="n1126">
      <data key="v_name">548200</data>
      <data key="v_label">548200</data>
    </node>
    <node id="n1127">
      <data key="v_name">555556</data>
      <data key="v_label">555556</data>
    </node>
    <node id="n1128">
      <data key="v_name">518562</data>
      <data key="v_label">518562</data>
    </node>
    <node id="n1129">
      <data key="v_name">529662</data>
      <data key="v_label">529662</data>
    </node>
    <node id="n1130">
      <data key="v_name">138562</data>
      <data key="v_label">138562</data>
    </node>
    <node id="n1131">
      <data key="v_name">471780</data>
      <data key="v_label">471780</data>
    </node>
    <node id="n1132">
      <data key="v_name">83531</data>
      <data key="v_label">83531</data>
    </node>
    <node id="n1133">
      <data key="v_name">138565</data>
      <data key="v_label">138565</data>
    </node>
    <node id="n1134">
      <data key="v_name">557468</data>
      <data key="v_label">557468</data>
    </node>
    <node id="n1135">
      <data key="v_name">139872</data>
      <data key="v_label">139872</data>
    </node>
    <node id="n1136">
      <data key="v_name">293070</data>
      <data key="v_label">293070</data>
    </node>
    <node id="n1137">
      <data key="v_name">564736</data>
      <data key="v_label">564736</data>
    </node>
    <node id="n1138">
      <data key="v_name">375746</data>
      <data key="v_label">375746</data>
    </node>
    <node id="n1139">
      <data key="v_name">517411</data>
    </node>
    <node id="n1140">
      <data key="v_name">139870</data>
      <data key="v_label">139870</data>
    </node>
    <node id="n1141">
      <data key="v_name">291821</data>
      <data key="v_label">291821</data>
    </node>
    <node id="n1142">
      <data key="v_name">289699</data>
      <data key="v_label">289699</data>
    </node>
    <node id="n1143">
      <data key="v_name">550629</data>
      <data key="v_label">550629</data>
    </node>
    <node id="n1144">
      <data key="v_name">530056</data>
      <data key="v_label">530056</data>
    </node>
    <node id="n1145">
      <data key="v_name">139885</data>
      <data key="v_label">139885</data>
    </node>
    <node id="n1146">
      <data key="v_name">381930</data>
      <data key="v_label">381930</data>
    </node>
    <node id="n1147">
      <data key="v_name">409364</data>
      <data key="v_label">409364</data>
    </node>
    <node id="n1148">
      <data key="v_name">340351</data>
      <data key="v_label">340351</data>
    </node>
    <node id="n1149">
      <data key="v_name">491832</data>
      <data key="v_label">491832</data>
    </node>
    <node id="n1150">
      <data key="v_name">540775</data>
      <data key="v_label">540775</data>
    </node>
    <node id="n1151">
      <data key="v_name">517236</data>
      <data key="v_label">517236</data>
    </node>
    <node id="n1152">
      <data key="v_name">550988</data>
      <data key="v_label">550988</data>
    </node>
    <node id="n1153">
      <data key="v_name">557516</data>
      <data key="v_label">557516</data>
    </node>
    <node id="n1154">
      <data key="v_name">557263</data>
      <data key="v_label">557263</data>
    </node>
    <node id="n1155">
      <data key="v_name">517319</data>
      <data key="v_label">517319</data>
    </node>
    <node id="n1156">
      <data key="v_name">428590</data>
      <data key="v_label">428590</data>
    </node>
    <node id="n1157">
      <data key="v_name">565135</data>
      <data key="v_label">565135</data>
    </node>
    <node id="n1158">
      <data key="v_name">532976</data>
      <data key="v_label">532976</data>
    </node>
    <node id="n1159">
      <data key="v_name">563679</data>
      <data key="v_label">563679</data>
    </node>
    <node id="n1160">
      <data key="v_name">332082</data>
      <data key="v_label">332082</data>
    </node>
    <node id="n1161">
      <data key="v_name">425109</data>
      <data key="v_label">425109</data>
    </node>
    <node id="n1162">
      <data key="v_name">518562</data>
    </node>
    <node id="n1163">
      <data key="v_name">485748</data>
      <data key="v_label">485748</data>
    </node>
    <node id="n1164">
      <data key="v_name">490112</data>
      <data key="v_label">490112</data>
    </node>
    <node id="n1165">
      <data key="v_name">511760</data>
      <data key="v_label">511760</data>
    </node>
    <node id="n1166">
      <data key="v_name">490256</data>
      <data key="v_label">490256</data>
    </node>
    <node id="n1167">
      <data key="v_name">550549</data>
    </node>
    <node id="n1168">
      <data key="v_name">438201</data>
      <data key="v_label">438201</data>
    </node>
    <node id="n1169">
      <data key="v_name">365875</data>
      <data key="v_label">365875</data>
    </node>
    <node id="n1170">
      <data key="v_name">351449</data>
      <data key="v_label">351449</data>
    </node>
    <node id="n1171">
      <data key="v_name">451671</data>
      <data key="v_label">451671</data>
    </node>
    <node id="n1172">
      <data key="v_name">424003</data>
    </node>
    <node id="n1173">
      <data key="v_name">478348</data>
      <data key="v_label">478348</data>
    </node>
    <node id="n1174">
      <data key="v_name">554177</data>
    </node>
    <node id="n1175">
      <data key="v_name">521232</data>
      <data key="v_label">521232</data>
    </node>
    <node id="n1176">
      <data key="v_name">142752</data>
      <data key="v_label">142752</data>
    </node>
    <node id="n1177">
      <data key="v_name">554471</data>
    </node>
    <node id="n1178">
      <data key="v_name">138376</data>
      <data key="v_label">138376</data>
    </node>
    <node id="n1179">
      <data key="v_name">328588</data>
      <data key="v_label">328588</data>
    </node>
    <node id="n1180">
      <data key="v_name">138378</data>
      <data key="v_label">138378</data>
    </node>
    <node id="n1181">
      <data key="v_name">448745</data>
      <data key="v_label">448745</data>
    </node>
    <node id="n1182">
      <data key="v_name">382726</data>
      <data key="v_label">382726</data>
    </node>
    <node id="n1183">
      <data key="v_name">551003</data>
      <data key="v_label">551003</data>
    </node>
    <node id="n1184">
      <data key="v_name">560228</data>
      <data key="v_label">560228</data>
    </node>
    <node id="n1185">
      <data key="v_name">562301</data>
      <data key="v_label">562301</data>
    </node>
    <node id="n1186">
      <data key="v_name">481145</data>
      <data key="v_label">481145</data>
    </node>
    <node id="n1187">
      <data key="v_name">293079</data>
      <data key="v_label">293079</data>
    </node>
    <node id="n1188">
      <data key="v_name">451874</data>
      <data key="v_label">451874</data>
    </node>
    <node id="n1189">
      <data key="v_name">550517</data>
      <data key="v_label">550517</data>
    </node>
    <node id="n1190">
      <data key="v_name">522422</data>
      <data key="v_label">522422</data>
    </node>
    <node id="n1191">
      <data key="v_name">140387</data>
      <data key="v_label">140387</data>
    </node>
    <node id="n1192">
      <data key="v_name">541975</data>
    </node>
    <node id="n1193">
      <data key="v_name">140389</data>
      <data key="v_label">140389</data>
    </node>
    <node id="n1194">
      <data key="v_name">530199</data>
      <data key="v_label">530199</data>
    </node>
    <node id="n1195">
      <data key="v_name">140393</data>
      <data key="v_label">140393</data>
    </node>
    <node id="n1196">
      <data key="v_name">335186</data>
      <data key="v_label">335186</data>
    </node>
    <node id="n1197">
      <data key="v_name">438709</data>
    </node>
    <node id="n1198">
      <data key="v_name">289699</data>
    </node>
    <node id="n1199">
      <data key="v_name">421048</data>
    </node>
    <node id="n1200">
      <data key="v_name">527253</data>
      <data key="v_label">527253</data>
    </node>
    <node id="n1201">
      <data key="v_name">276437</data>
      <data key="v_label">276437</data>
    </node>
    <node id="n1202">
      <data key="v_name">148166</data>
      <data key="v_label">148166</data>
    </node>
    <node id="n1203">
      <data key="v_name">140100</data>
      <data key="v_label">140100</data>
    </node>
    <node id="n1204">
      <data key="v_name">140101</data>
      <data key="v_label">140101</data>
    </node>
    <node id="n1205">
      <data key="v_name">226709</data>
      <data key="v_label">226709</data>
    </node>
    <node id="n1206">
      <data key="v_name">450650</data>
      <data key="v_label">450650</data>
    </node>
    <node id="n1207">
      <data key="v_name">213809</data>
      <data key="v_label">213809</data>
    </node>
    <node id="n1208">
      <data key="v_name">138675</data>
      <data key="v_label">138675</data>
    </node>
    <node id="n1209">
      <data key="v_name">561154</data>
      <data key="v_label">561154</data>
    </node>
    <node id="n1210">
      <data key="v_name">496841</data>
      <data key="v_label">496841</data>
    </node>
    <node id="n1211">
      <data key="v_name">455453</data>
      <data key="v_label">455453</data>
    </node>
    <node id="n1212">
      <data key="v_name">552787</data>
      <data key="v_label">552787</data>
    </node>
    <node id="n1213">
      <data key="v_name">550876</data>
      <data key="v_label">550876</data>
    </node>
    <node id="n1214">
      <data key="v_name">550429</data>
      <data key="v_label">550429</data>
    </node>
    <node id="n1215">
      <data key="v_name">499748</data>
      <data key="v_label">499748</data>
    </node>
    <node id="n1216">
      <data key="v_name">507476</data>
      <data key="v_label">507476</data>
    </node>
    <node id="n1217">
      <data key="v_name">561154</data>
    </node>
    <node id="n1218">
      <data key="v_name">139779</data>
      <data key="v_label">139779</data>
    </node>
    <node id="n1219">
      <data key="v_name">483342</data>
    </node>
    <node id="n1220">
      <data key="v_name">525973</data>
    </node>
    <node id="n1221">
      <data key="v_name">522718</data>
      <data key="v_label">522718</data>
    </node>
    <node id="n1222">
      <data key="v_name">550545</data>
      <data key="v_label">550545</data>
    </node>
    <node id="n1223">
      <data key="v_name">518438</data>
      <data key="v_label">518438</data>
    </node>
    <node id="n1224">
      <data key="v_name">538628</data>
      <data key="v_label">538628</data>
    </node>
    <node id="n1225">
      <data key="v_name">225569</data>
      <data key="v_label">225569</data>
    </node>
    <node id="n1226">
      <data key="v_name">438709</data>
    </node>
    <node id="n1227">
      <data key="v_name">142535</data>
      <data key="v_label">142535</data>
    </node>
    <node id="n1228">
      <data key="v_name">523294</data>
      <data key="v_label">523294</data>
    </node>
    <node id="n1229">
      <data key="v_name">139990</data>
      <data key="v_label">139990</data>
    </node>
    <node id="n1230">
      <data key="v_name">296879</data>
      <data key="v_label">296879</data>
    </node>
    <node id="n1231">
      <data key="v_name">532978</data>
      <data key="v_label">532978</data>
    </node>
    <node id="n1232">
      <data key="v_name">109588</data>
      <data key="v_label">109588</data>
    </node>
    <node id="n1233">
      <data key="v_name">410651</data>
      <data key="v_label">410651</data>
    </node>
    <node id="n1234">
      <data key="v_name">550000</data>
    </node>
    <node id="n1235">
      <data key="v_name">466762</data>
      <data key="v_label">466762</data>
    </node>
    <node id="n1236">
      <data key="v_name">518256</data>
      <data key="v_label">518256</data>
    </node>
    <node id="n1237">
      <data key="v_name">438734</data>
      <data key="v_label">438734</data>
    </node>
    <node id="n1238">
      <data key="v_name">290833</data>
      <data key="v_label">290833</data>
    </node>
    <node id="n1239">
      <data key="v_name">183507</data>
      <data key="v_label">183507</data>
    </node>
    <node id="n1240">
      <data key="v_name">530243</data>
      <data key="v_label">530243</data>
    </node>
    <node id="n1241">
      <data key="v_name">179404</data>
      <data key="v_label">179404</data>
    </node>
    <node id="n1242">
      <data key="v_name">179405</data>
      <data key="v_label">179405</data>
    </node>
    <node id="n1243">
      <data key="v_name">436633</data>
      <data key="v_label">436633</data>
    </node>
    <node id="n1244">
      <data key="v_name">146310</data>
      <data key="v_label">146310</data>
    </node>
    <node id="n1245">
      <data key="v_name">548698</data>
      <data key="v_label">548698</data>
    </node>
    <node id="n1246">
      <data key="v_name">492139</data>
      <data key="v_label">492139</data>
    </node>
    <node id="n1247">
      <data key="v_name">549541</data>
      <data key="v_label">549541</data>
    </node>
    <node id="n1248">
      <data key="v_name">549540</data>
      <data key="v_label">549540</data>
    </node>
    <node id="n1249">
      <data key="v_name">385408</data>
      <data key="v_label">385408</data>
    </node>
    <node id="n1250">
      <data key="v_name">182731</data>
      <data key="v_label">182731</data>
    </node>
    <node id="n1251">
      <data key="v_name">385409</data>
      <data key="v_label">385409</data>
    </node>
    <node id="n1252">
      <data key="v_name">134351</data>
      <data key="v_label">134351</data>
    </node>
    <node id="n1253">
      <data key="v_name">406694</data>
      <data key="v_label">406694</data>
    </node>
    <node id="n1254">
      <data key="v_name">456329</data>
      <data key="v_label">456329</data>
    </node>
    <node id="n1255">
      <data key="v_name">133899</data>
      <data key="v_label">133899</data>
    </node>
    <node id="n1256">
      <data key="v_name">517778</data>
      <data key="v_label">517778</data>
    </node>
    <node id="n1257">
      <data key="v_name">548698</data>
    </node>
    <node id="n1258">
      <data key="v_name">543625</data>
    </node>
    <node id="n1259">
      <data key="v_name">552216</data>
      <data key="v_label">552216</data>
    </node>
    <node id="n1260">
      <data key="v_name">438221</data>
      <data key="v_label">438221</data>
    </node>
    <node id="n1261">
      <data key="v_name">523508</data>
      <data key="v_label">523508</data>
    </node>
    <node id="n1262">
      <data key="v_name">491949</data>
      <data key="v_label">491949</data>
    </node>
    <node id="n1263">
      <data key="v_name">119323</data>
      <data key="v_label">119323</data>
    </node>
    <node id="n1264">
      <data key="v_name">553458</data>
      <data key="v_label">553458</data>
    </node>
    <node id="n1265">
      <data key="v_name">384098</data>
    </node>
    <node id="n1266">
      <data key="v_name">508186</data>
      <data key="v_label">508186</data>
    </node>
    <node id="n1267">
      <data key="v_name">466234</data>
      <data key="v_label">466234</data>
    </node>
    <node id="n1268">
      <data key="v_name">184035</data>
      <data key="v_label">184035</data>
    </node>
    <node id="n1269">
      <data key="v_name">562965</data>
      <data key="v_label">562965</data>
    </node>
    <node id="n1270">
      <data key="v_name">527631</data>
      <data key="v_label">527631</data>
    </node>
    <node id="n1271">
      <data key="v_name">543480</data>
      <data key="v_label">543480</data>
    </node>
    <node id="n1272">
      <data key="v_name">445172</data>
    </node>
    <node id="n1273">
      <data key="v_name">516082</data>
      <data key="v_label">516082</data>
    </node>
    <node id="n1274">
      <data key="v_name">488662</data>
      <data key="v_label">488662</data>
    </node>
    <node id="n1275">
      <data key="v_name">401760</data>
      <data key="v_label">401760</data>
    </node>
    <node id="n1276">
      <data key="v_name">355335</data>
      <data key="v_label">355335</data>
    </node>
    <node id="n1277">
      <data key="v_name">531355</data>
      <data key="v_label">531355</data>
    </node>
    <node id="n1278">
      <data key="v_name">231158</data>
    </node>
    <node id="n1279">
      <data key="v_name">563576</data>
      <data key="v_label">563576</data>
    </node>
    <node id="n1280">
      <data key="v_name">558176</data>
      <data key="v_label">558176</data>
    </node>
    <node id="n1281">
      <data key="v_name">533183</data>
      <data key="v_label">533183</data>
    </node>
    <node id="n1282">
      <data key="v_name">547185</data>
    </node>
    <node id="n1283">
      <data key="v_name">560216</data>
      <data key="v_label">560216</data>
    </node>
    <node id="n1284">
      <data key="v_name">183349</data>
      <data key="v_label">183349</data>
    </node>
    <node id="n1285">
      <data key="v_name">524497</data>
      <data key="v_label">524497</data>
    </node>
    <node id="n1286">
      <data key="v_name">140208</data>
      <data key="v_label">140208</data>
    </node>
    <node id="n1287">
      <data key="v_name">558205</data>
    </node>
    <node id="n1288">
      <data key="v_name">511660</data>
      <data key="v_label">511660</data>
    </node>
    <node id="n1289">
      <data key="v_name">489472</data>
      <data key="v_label">489472</data>
    </node>
    <node id="n1290">
      <data key="v_name">551029</data>
    </node>
    <node id="n1291">
      <data key="v_name">426166</data>
      <data key="v_label">426166</data>
    </node>
    <node id="n1292">
      <data key="v_name">229722</data>
      <data key="v_label">229722</data>
    </node>
    <node id="n1293">
      <data key="v_name">385689</data>
      <data key="v_label">385689</data>
    </node>
    <node id="n1294">
      <data key="v_name">486626</data>
    </node>
    <node id="n1295">
      <data key="v_name">530056</data>
    </node>
    <node id="n1296">
      <data key="v_name">140221</data>
      <data key="v_label">140221</data>
    </node>
    <node id="n1297">
      <data key="v_name">528451</data>
      <data key="v_label">528451</data>
    </node>
    <node id="n1298">
      <data key="v_name">495163</data>
      <data key="v_label">495163</data>
    </node>
    <node id="n1299">
      <data key="v_name">140224</data>
      <data key="v_label">140224</data>
    </node>
    <node id="n1300">
      <data key="v_name">559268</data>
      <data key="v_label">559268</data>
    </node>
    <node id="n1301">
      <data key="v_name">284993</data>
    </node>
    <node id="n1302">
      <data key="v_name">483634</data>
      <data key="v_label">483634</data>
    </node>
    <node id="n1303">
      <data key="v_name">272419</data>
      <data key="v_label">272419</data>
    </node>
    <node id="n1304">
      <data key="v_name">138955</data>
      <data key="v_label">138955</data>
    </node>
    <node id="n1305">
      <data key="v_name">152351</data>
      <data key="v_label">152351</data>
    </node>
    <node id="n1306">
      <data key="v_name">410444</data>
    </node>
    <node id="n1307">
      <data key="v_name">245234</data>
      <data key="v_label">245234</data>
    </node>
    <node id="n1308">
      <data key="v_name">409678</data>
    </node>
    <node id="n1309">
      <data key="v_name">139538</data>
      <data key="v_label">139538</data>
    </node>
    <node id="n1310">
      <data key="v_name">408502</data>
      <data key="v_label">408502</data>
    </node>
    <node id="n1311">
      <data key="v_name">318770</data>
      <data key="v_label">318770</data>
    </node>
    <node id="n1312">
      <data key="v_name">559621</data>
      <data key="v_label">559621</data>
    </node>
    <node id="n1313">
      <data key="v_name">438709</data>
    </node>
    <node id="n1314">
      <data key="v_name">530768</data>
      <data key="v_label">530768</data>
    </node>
    <node id="n1315">
      <data key="v_name">250152</data>
      <data key="v_label">250152</data>
    </node>
    <node id="n1316">
      <data key="v_name">140597</data>
      <data key="v_label">140597</data>
    </node>
    <node id="n1317">
      <data key="v_name">142450</data>
      <data key="v_label">142450</data>
    </node>
    <node id="n1318">
      <data key="v_name">551789</data>
      <data key="v_label">551789</data>
    </node>
    <node id="n1319">
      <data key="v_name">439054</data>
      <data key="v_label">439054</data>
    </node>
    <node id="n1320">
      <data key="v_name">142453</data>
      <data key="v_label">142453</data>
    </node>
    <node id="n1321">
      <data key="v_name">142454</data>
      <data key="v_label">142454</data>
    </node>
    <node id="n1322">
      <data key="v_name">552216</data>
    </node>
    <node id="n1323">
      <data key="v_name">549841</data>
    </node>
    <node id="n1324">
      <data key="v_name">548132</data>
    </node>
    <node id="n1325">
      <data key="v_name">518212</data>
      <data key="v_label">518212</data>
    </node>
    <node id="n1326">
      <data key="v_name">139519</data>
      <data key="v_label">139519</data>
    </node>
    <node id="n1327">
      <data key="v_name">255932</data>
      <data key="v_label">255932</data>
    </node>
    <node id="n1328">
      <data key="v_name">139521</data>
      <data key="v_label">139521</data>
    </node>
    <node id="n1329">
      <data key="v_name">431683</data>
    </node>
    <node id="n1330">
      <data key="v_name">550002</data>
      <data key="v_label">550002</data>
    </node>
    <node id="n1331">
      <data key="v_name">213414</data>
      <data key="v_label">213414</data>
    </node>
    <node id="n1332">
      <data key="v_name">495323</data>
      <data key="v_label">495323</data>
    </node>
    <node id="n1333">
      <data key="v_name">549458</data>
    </node>
    <node id="n1334">
      <data key="v_name">142503</data>
      <data key="v_label">142503</data>
    </node>
    <node id="n1335">
      <data key="v_name">142504</data>
      <data key="v_label">142504</data>
    </node>
    <node id="n1336">
      <data key="v_name">549932</data>
      <data key="v_label">549932</data>
    </node>
    <node id="n1337">
      <data key="v_name">518062</data>
      <data key="v_label">518062</data>
    </node>
    <node id="n1338">
      <data key="v_name">548342</data>
      <data key="v_label">548342</data>
    </node>
    <node id="n1339">
      <data key="v_name">562714</data>
      <data key="v_label">562714</data>
    </node>
    <node id="n1340">
      <data key="v_name">388358</data>
      <data key="v_label">388358</data>
    </node>
    <node id="n1341">
      <data key="v_name">439586</data>
      <data key="v_label">439586</data>
    </node>
    <node id="n1342">
      <data key="v_name">556730</data>
      <data key="v_label">556730</data>
    </node>
    <node id="n1343">
      <data key="v_name">260339</data>
      <data key="v_label">260339</data>
    </node>
    <node id="n1344">
      <data key="v_name">211338</data>
    </node>
    <node id="n1345">
      <data key="v_name">362246</data>
      <data key="v_label">362246</data>
    </node>
    <node id="n1346">
      <data key="v_name">143021</data>
    </node>
    <node id="n1347">
      <data key="v_name">143022</data>
      <data key="v_label">143022</data>
    </node>
    <node id="n1348">
      <data key="v_name">461768</data>
      <data key="v_label">461768</data>
    </node>
    <node id="n1349">
      <data key="v_name">475166</data>
      <data key="v_label">475166</data>
    </node>
    <node id="n1350">
      <data key="v_name">565263</data>
    </node>
    <node id="n1351">
      <data key="v_name">393034</data>
      <data key="v_label">393034</data>
    </node>
    <node id="n1352">
      <data key="v_name">143031</data>
      <data key="v_label">143031</data>
    </node>
    <node id="n1353">
      <data key="v_name">562016</data>
    </node>
    <node id="n1354">
      <data key="v_name">142659</data>
      <data key="v_label">142659</data>
    </node>
    <node id="n1355">
      <data key="v_name">142660</data>
      <data key="v_label">142660</data>
    </node>
    <node id="n1356">
      <data key="v_name">142661</data>
      <data key="v_label">142661</data>
    </node>
    <node id="n1357">
      <data key="v_name">533266</data>
      <data key="v_label">533266</data>
    </node>
    <node id="n1358">
      <data key="v_name">561136</data>
      <data key="v_label">561136</data>
    </node>
    <node id="n1359">
      <data key="v_name">549235</data>
      <data key="v_label">549235</data>
    </node>
    <node id="n1360">
      <data key="v_name">467756</data>
      <data key="v_label">467756</data>
    </node>
    <node id="n1361">
      <data key="v_name">521278</data>
    </node>
    <node id="n1362">
      <data key="v_name">495168</data>
      <data key="v_label">495168</data>
    </node>
    <node id="n1363">
      <data key="v_name">507674</data>
      <data key="v_label">507674</data>
    </node>
    <node id="n1364">
      <data key="v_name">226709</data>
    </node>
    <node id="n1365">
      <data key="v_name">451874</data>
    </node>
    <node id="n1366">
      <data key="v_name">551830</data>
      <data key="v_label">551830</data>
    </node>
    <node id="n1367">
      <data key="v_name">551829</data>
      <data key="v_label">551829</data>
    </node>
    <node id="n1368">
      <data key="v_name">209323</data>
      <data key="v_label">209323</data>
    </node>
    <node id="n1369">
      <data key="v_name">498457</data>
      <data key="v_label">498457</data>
    </node>
    <node id="n1370">
      <data key="v_name">397790</data>
      <data key="v_label">397790</data>
    </node>
    <node id="n1371">
      <data key="v_name">263351</data>
      <data key="v_label">263351</data>
    </node>
    <node id="n1372">
      <data key="v_name">540768</data>
      <data key="v_label">540768</data>
    </node>
    <node id="n1373">
      <data key="v_name">142689</data>
      <data key="v_label">142689</data>
    </node>
    <node id="n1374">
      <data key="v_name">402054</data>
      <data key="v_label">402054</data>
    </node>
    <node id="n1375">
      <data key="v_name">558152</data>
      <data key="v_label">558152</data>
    </node>
    <node id="n1376">
      <data key="v_name">139823</data>
      <data key="v_label">139823</data>
    </node>
    <node id="n1377">
      <data key="v_name">561682</data>
      <data key="v_label">561682</data>
    </node>
    <node id="n1378">
      <data key="v_name">558880</data>
      <data key="v_label">558880</data>
    </node>
    <node id="n1379">
      <data key="v_name">560562</data>
      <data key="v_label">560562</data>
    </node>
    <node id="n1380">
      <data key="v_name">508443</data>
      <data key="v_label">508443</data>
    </node>
    <node id="n1381">
      <data key="v_name">502515</data>
      <data key="v_label">502515</data>
    </node>
    <node id="n1382">
      <data key="v_name">418679</data>
      <data key="v_label">418679</data>
    </node>
    <node id="n1383">
      <data key="v_name">139808</data>
      <data key="v_label">139808</data>
    </node>
    <node id="n1384">
      <data key="v_name">420959</data>
      <data key="v_label">420959</data>
    </node>
    <node id="n1385">
      <data key="v_name">139814</data>
      <data key="v_label">139814</data>
    </node>
    <node id="n1386">
      <data key="v_name">560179</data>
      <data key="v_label">560179</data>
    </node>
    <node id="n1387">
      <data key="v_name">144663</data>
      <data key="v_label">144663</data>
    </node>
    <node id="n1388">
      <data key="v_name">421302</data>
      <data key="v_label">421302</data>
    </node>
    <node id="n1389">
      <data key="v_name">144665</data>
      <data key="v_label">144665</data>
    </node>
    <node id="n1390">
      <data key="v_name">144666</data>
      <data key="v_label">144666</data>
    </node>
    <node id="n1391">
      <data key="v_name">545047</data>
      <data key="v_label">545047</data>
    </node>
    <node id="n1392">
      <data key="v_name">425389</data>
      <data key="v_label">425389</data>
    </node>
    <node id="n1393">
      <data key="v_name">455118</data>
      <data key="v_label">455118</data>
    </node>
    <node id="n1394">
      <data key="v_name">366560</data>
      <data key="v_label">366560</data>
    </node>
    <node id="n1395">
      <data key="v_name">122839</data>
    </node>
    <node id="n1396">
      <data key="v_name">213414</data>
    </node>
    <node id="n1397">
      <data key="v_name">489472</data>
    </node>
    <node id="n1398">
      <data key="v_name">495323</data>
    </node>
    <node id="n1399">
      <data key="v_name">508528</data>
      <data key="v_label">508528</data>
    </node>
    <node id="n1400">
      <data key="v_name">52879</data>
      <data key="v_label">52879</data>
    </node>
    <node id="n1401">
      <data key="v_name">485670</data>
      <data key="v_label">485670</data>
    </node>
    <node id="n1402">
      <data key="v_name">431529</data>
    </node>
    <node id="n1403">
      <data key="v_name">485671</data>
      <data key="v_label">485671</data>
    </node>
    <node id="n1404">
      <data key="v_name">560130</data>
    </node>
    <node id="n1405">
      <data key="v_name">424601</data>
      <data key="v_label">424601</data>
    </node>
    <node id="n1406">
      <data key="v_name">151982</data>
      <data key="v_label">151982</data>
    </node>
    <node id="n1407">
      <data key="v_name">311939</data>
      <data key="v_label">311939</data>
    </node>
    <node id="n1408">
      <data key="v_name">501549</data>
      <data key="v_label">501549</data>
    </node>
    <node id="n1409">
      <data key="v_name">535238</data>
      <data key="v_label">535238</data>
    </node>
    <node id="n1410">
      <data key="v_name">142767</data>
      <data key="v_label">142767</data>
    </node>
    <node id="n1411">
      <data key="v_name">241709</data>
      <data key="v_label">241709</data>
    </node>
    <node id="n1412">
      <data key="v_name">138647</data>
      <data key="v_label">138647</data>
    </node>
    <node id="n1413">
      <data key="v_name">554760</data>
      <data key="v_label">554760</data>
    </node>
    <node id="n1414">
      <data key="v_name">379044</data>
      <data key="v_label">379044</data>
    </node>
    <node id="n1415">
      <data key="v_name">366025</data>
      <data key="v_label">366025</data>
    </node>
    <node id="n1416">
      <data key="v_name">550822</data>
      <data key="v_label">550822</data>
    </node>
    <node id="n1417">
      <data key="v_name">554823</data>
      <data key="v_label">554823</data>
    </node>
    <node id="n1418">
      <data key="v_name">293343</data>
      <data key="v_label">293343</data>
    </node>
    <node id="n1419">
      <data key="v_name">506031</data>
    </node>
    <node id="n1420">
      <data key="v_name">138666</data>
      <data key="v_label">138666</data>
    </node>
    <node id="n1421">
      <data key="v_name">138667</data>
      <data key="v_label">138667</data>
    </node>
    <node id="n1422">
      <data key="v_name">140656</data>
      <data key="v_label">140656</data>
    </node>
    <node id="n1423">
      <data key="v_name">285304</data>
      <data key="v_label">285304</data>
    </node>
    <node id="n1424">
      <data key="v_name">453840</data>
      <data key="v_label">453840</data>
    </node>
    <node id="n1425">
      <data key="v_name">518079</data>
      <data key="v_label">518079</data>
    </node>
    <node id="n1426">
      <data key="v_name">518079</data>
    </node>
    <node id="n1427">
      <data key="v_name">179250</data>
      <data key="v_label">179250</data>
    </node>
    <node id="n1428">
      <data key="v_name">283483</data>
      <data key="v_label">283483</data>
    </node>
    <node id="n1429">
      <data key="v_name">438709</data>
    </node>
    <node id="n1430">
      <data key="v_name">179263</data>
      <data key="v_label">179263</data>
    </node>
    <node id="n1431">
      <data key="v_name">543581</data>
    </node>
    <node id="n1432">
      <data key="v_name">456688</data>
      <data key="v_label">456688</data>
    </node>
    <node id="n1433">
      <data key="v_name">472085</data>
      <data key="v_label">472085</data>
    </node>
    <node id="n1434">
      <data key="v_name">225702</data>
      <data key="v_label">225702</data>
    </node>
    <node id="n1435">
      <data key="v_name">317910</data>
      <data key="v_label">317910</data>
    </node>
    <node id="n1436">
      <data key="v_name">139905</data>
      <data key="v_label">139905</data>
    </node>
    <node id="n1437">
      <data key="v_name">139906</data>
      <data key="v_label">139906</data>
    </node>
    <node id="n1438">
      <data key="v_name">527253</data>
    </node>
    <node id="n1439">
      <data key="v_name">485986</data>
      <data key="v_label">485986</data>
    </node>
    <node id="n1440">
      <data key="v_name">465808</data>
      <data key="v_label">465808</data>
    </node>
    <node id="n1441">
      <data key="v_name">564721</data>
    </node>
    <node id="n1442">
      <data key="v_name">151489</data>
    </node>
    <node id="n1443">
      <data key="v_name">151490</data>
      <data key="v_label">151490</data>
    </node>
    <node id="n1444">
      <data key="v_name">532931</data>
    </node>
    <node id="n1445">
      <data key="v_name">478385</data>
      <data key="v_label">478385</data>
    </node>
    <node id="n1446">
      <data key="v_name">146163</data>
      <data key="v_label">146163</data>
    </node>
    <node id="n1447">
      <data key="v_name">441174</data>
      <data key="v_label">441174</data>
    </node>
    <node id="n1448">
      <data key="v_name">475261</data>
      <data key="v_label">475261</data>
    </node>
    <node id="n1449">
      <data key="v_name">485154</data>
      <data key="v_label">485154</data>
    </node>
    <node id="n1450">
      <data key="v_name">548200</data>
    </node>
    <node id="n1451">
      <data key="v_name">527004</data>
      <data key="v_label">527004</data>
    </node>
    <node id="n1452">
      <data key="v_name">563534</data>
      <data key="v_label">563534</data>
    </node>
    <node id="n1453">
      <data key="v_name">426630</data>
    </node>
    <node id="n1454">
      <data key="v_name">561784</data>
    </node>
    <node id="n1455">
      <data key="v_name">561785</data>
    </node>
    <node id="n1456">
      <data key="v_name">289744</data>
      <data key="v_label">289744</data>
    </node>
    <node id="n1457">
      <data key="v_name">249524</data>
      <data key="v_label">249524</data>
    </node>
    <node id="n1458">
      <data key="v_name">438166</data>
      <data key="v_label">438166</data>
    </node>
    <node id="n1459">
      <data key="v_name">486283</data>
      <data key="v_label">486283</data>
    </node>
    <node id="n1460">
      <data key="v_name">371069</data>
      <data key="v_label">371069</data>
    </node>
    <node id="n1461">
      <data key="v_name">231101</data>
      <data key="v_label">231101</data>
    </node>
    <node id="n1462">
      <data key="v_name">153088</data>
      <data key="v_label">153088</data>
    </node>
    <node id="n1463">
      <data key="v_name">562118</data>
      <data key="v_label">562118</data>
    </node>
    <node id="n1464">
      <data key="v_name">335170</data>
      <data key="v_label">335170</data>
    </node>
    <node id="n1465">
      <data key="v_name">517719</data>
      <data key="v_label">517719</data>
    </node>
    <node id="n1466">
      <data key="v_name">298189</data>
    </node>
    <node id="n1467">
      <data key="v_name">234699</data>
    </node>
    <node id="n1468">
      <data key="v_name">408627</data>
      <data key="v_label">408627</data>
    </node>
    <node id="n1469">
      <data key="v_name">559496</data>
    </node>
    <node id="n1470">
      <data key="v_name">525612</data>
      <data key="v_label">525612</data>
    </node>
    <node id="n1471">
      <data key="v_name">560510</data>
    </node>
    <node id="n1472">
      <data key="v_name">140271</data>
      <data key="v_label">140271</data>
    </node>
    <node id="n1473">
      <data key="v_name">496273</data>
      <data key="v_label">496273</data>
    </node>
    <node id="n1474">
      <data key="v_name">140273</data>
      <data key="v_label">140273</data>
    </node>
    <node id="n1475">
      <data key="v_name">441455</data>
      <data key="v_label">441455</data>
    </node>
    <node id="n1476">
      <data key="v_name">289727</data>
      <data key="v_label">289727</data>
    </node>
    <node id="n1477">
      <data key="v_name">526786</data>
      <data key="v_label">526786</data>
    </node>
    <node id="n1478">
      <data key="v_name">497008</data>
      <data key="v_label">497008</data>
    </node>
    <node id="n1479">
      <data key="v_name">377641</data>
      <data key="v_label">377641</data>
    </node>
    <node id="n1480">
      <data key="v_name">499269</data>
      <data key="v_label">499269</data>
    </node>
    <node id="n1481">
      <data key="v_name">140288</data>
      <data key="v_label">140288</data>
    </node>
    <node id="n1482">
      <data key="v_name">140289</data>
      <data key="v_label">140289</data>
    </node>
    <node id="n1483">
      <data key="v_name">326919</data>
      <data key="v_label">326919</data>
    </node>
    <node id="n1484">
      <data key="v_name">335202</data>
      <data key="v_label">335202</data>
    </node>
    <node id="n1485">
      <data key="v_name">497068</data>
    </node>
    <node id="n1486">
      <data key="v_name">228577</data>
      <data key="v_label">228577</data>
    </node>
    <node id="n1487">
      <data key="v_name">557651</data>
    </node>
    <node id="n1488">
      <data key="v_name">523591</data>
      <data key="v_label">523591</data>
    </node>
    <node id="n1489">
      <data key="v_name">411407</data>
      <data key="v_label">411407</data>
    </node>
    <node id="n1490">
      <data key="v_name">537855</data>
      <data key="v_label">537855</data>
    </node>
    <node id="n1491">
      <data key="v_name">411408</data>
      <data key="v_label">411408</data>
    </node>
    <node id="n1492">
      <data key="v_name">411409</data>
      <data key="v_label">411409</data>
    </node>
    <node id="n1493">
      <data key="v_name">437989</data>
      <data key="v_label">437989</data>
    </node>
    <node id="n1494">
      <data key="v_name">355581</data>
      <data key="v_label">355581</data>
    </node>
    <node id="n1495">
      <data key="v_name">249657</data>
      <data key="v_label">249657</data>
    </node>
    <node id="n1496">
      <data key="v_name">379633</data>
      <data key="v_label">379633</data>
    </node>
    <node id="n1497">
      <data key="v_name">353894</data>
      <data key="v_label">353894</data>
    </node>
    <node id="n1498">
      <data key="v_name">565263</data>
    </node>
    <node id="n1499">
      <data key="v_name">117785</data>
      <data key="v_label">117785</data>
    </node>
    <node id="n1500">
      <data key="v_name">225924</data>
    </node>
    <node id="n1501">
      <data key="v_name">562301</data>
    </node>
    <node id="n1502">
      <data key="v_name">257944</data>
      <data key="v_label">257944</data>
    </node>
    <node id="n1503">
      <data key="v_name">313394</data>
      <data key="v_label">313394</data>
    </node>
    <node id="n1504">
      <data key="v_name">553906</data>
    </node>
    <node id="n1505">
      <data key="v_name">543625</data>
    </node>
    <node id="n1506">
      <data key="v_name">529582</data>
      <data key="v_label">529582</data>
    </node>
    <node id="n1507">
      <data key="v_name">492859</data>
      <data key="v_label">492859</data>
    </node>
    <node id="n1508">
      <data key="v_name">338154</data>
      <data key="v_label">338154</data>
    </node>
    <node id="n1509">
      <data key="v_name">531245</data>
      <data key="v_label">531245</data>
    </node>
    <node id="n1510">
      <data key="v_name">517075</data>
    </node>
    <node id="n1511">
      <data key="v_name">530767</data>
      <data key="v_label">530767</data>
    </node>
    <node id="n1512">
      <data key="v_name">299749</data>
      <data key="v_label">299749</data>
    </node>
    <node id="n1513">
      <data key="v_name">415200</data>
      <data key="v_label">415200</data>
    </node>
    <node id="n1514">
      <data key="v_name">425934</data>
    </node>
    <node id="n1515">
      <data key="v_name">353750</data>
    </node>
    <node id="n1516">
      <data key="v_name">555929</data>
    </node>
    <node id="n1517">
      <data key="v_name">485593</data>
    </node>
    <node id="n1518">
      <data key="v_name">267325</data>
      <data key="v_label">267325</data>
    </node>
    <node id="n1519">
      <data key="v_name">518190</data>
      <data key="v_label">518190</data>
    </node>
    <node id="n1520">
      <data key="v_name">140071</data>
      <data key="v_label">140071</data>
    </node>
    <node id="n1521">
      <data key="v_name">507792</data>
      <data key="v_label">507792</data>
    </node>
    <node id="n1522">
      <data key="v_name">103939</data>
      <data key="v_label">103939</data>
    </node>
    <node id="n1523">
      <data key="v_name">518464</data>
      <data key="v_label">518464</data>
    </node>
    <node id="n1524">
      <data key="v_name">292992</data>
      <data key="v_label">292992</data>
    </node>
    <node id="n1525">
      <data key="v_name">280545</data>
      <data key="v_label">280545</data>
    </node>
    <node id="n1526">
      <data key="v_name">339844</data>
      <data key="v_label">339844</data>
    </node>
    <node id="n1527">
      <data key="v_name">290043</data>
      <data key="v_label">290043</data>
    </node>
    <node id="n1528">
      <data key="v_name">513561</data>
      <data key="v_label">513561</data>
    </node>
    <node id="n1529">
      <data key="v_name">451038</data>
      <data key="v_label">451038</data>
    </node>
    <node id="n1530">
      <data key="v_name">254458</data>
      <data key="v_label">254458</data>
    </node>
    <node id="n1531">
      <data key="v_name">139931</data>
      <data key="v_label">139931</data>
    </node>
    <node id="n1532">
      <data key="v_name">496785</data>
      <data key="v_label">496785</data>
    </node>
    <node id="n1533">
      <data key="v_name">449790</data>
      <data key="v_label">449790</data>
    </node>
    <node id="n1534">
      <data key="v_name">355556</data>
      <data key="v_label">355556</data>
    </node>
    <node id="n1535">
      <data key="v_name">486226</data>
      <data key="v_label">486226</data>
    </node>
    <node id="n1536">
      <data key="v_name">139929</data>
      <data key="v_label">139929</data>
    </node>
    <node id="n1537">
      <data key="v_name">226113</data>
      <data key="v_label">226113</data>
    </node>
    <node id="n1538">
      <data key="v_name">226114</data>
      <data key="v_label">226114</data>
    </node>
    <node id="n1539">
      <data key="v_name">139604</data>
      <data key="v_label">139604</data>
    </node>
    <node id="n1540">
      <data key="v_name">447954</data>
      <data key="v_label">447954</data>
    </node>
    <node id="n1541">
      <data key="v_name">562061</data>
      <data key="v_label">562061</data>
    </node>
    <node id="n1542">
      <data key="v_name">486062</data>
      <data key="v_label">486062</data>
    </node>
    <node id="n1543">
      <data key="v_name">313342</data>
      <data key="v_label">313342</data>
    </node>
    <node id="n1544">
      <data key="v_name">556038</data>
      <data key="v_label">556038</data>
    </node>
    <node id="n1545">
      <data key="v_name">560705</data>
      <data key="v_label">560705</data>
    </node>
    <node id="n1546">
      <data key="v_name">490060</data>
      <data key="v_label">490060</data>
    </node>
    <node id="n1547">
      <data key="v_name">486561</data>
    </node>
    <node id="n1548">
      <data key="v_name">545777</data>
    </node>
    <node id="n1549">
      <data key="v_name">364780</data>
    </node>
    <node id="n1550">
      <data key="v_name">557396</data>
    </node>
    <node id="n1551">
      <data key="v_name">562577</data>
    </node>
    <node id="n1552">
      <data key="v_name">140002</data>
    </node>
    <node id="n1553">
      <data key="v_name">558467</data>
      <data key="v_label">558467</data>
    </node>
    <node id="n1554">
      <data key="v_name">74434</data>
      <data key="v_label">74434</data>
    </node>
    <node id="n1555">
      <data key="v_name">548310</data>
      <data key="v_label">548310</data>
    </node>
    <node id="n1556">
      <data key="v_name">499259</data>
      <data key="v_label">499259</data>
    </node>
    <node id="n1557">
      <data key="v_name">564706</data>
    </node>
    <node id="n1558">
      <data key="v_name">545655</data>
    </node>
    <node id="n1559">
      <data key="v_name">483300</data>
    </node>
    <node id="n1560">
      <data key="v_name">527619</data>
      <data key="v_label">527619</data>
    </node>
    <node id="n1561">
      <data key="v_name">426259</data>
      <data key="v_label">426259</data>
    </node>
    <node id="n1562">
      <data key="v_name">226157</data>
      <data key="v_label">226157</data>
    </node>
    <node id="n1563">
      <data key="v_name">558563</data>
    </node>
    <node id="n1564">
      <data key="v_name">545655</data>
    </node>
    <node id="n1565">
      <data key="v_name">443993</data>
      <data key="v_label">443993</data>
    </node>
    <node id="n1566">
      <data key="v_name">545656</data>
      <data key="v_label">545656</data>
    </node>
    <node id="n1567">
      <data key="v_name">557272</data>
      <data key="v_label">557272</data>
    </node>
    <node id="n1568">
      <data key="v_name">138361</data>
      <data key="v_label">138361</data>
    </node>
    <node id="n1569">
      <data key="v_name">138362</data>
      <data key="v_label">138362</data>
    </node>
    <node id="n1570">
      <data key="v_name">175521</data>
      <data key="v_label">175521</data>
    </node>
    <node id="n1571">
      <data key="v_name">187305</data>
      <data key="v_label">187305</data>
    </node>
    <node id="n1572">
      <data key="v_name">139944</data>
      <data key="v_label">139944</data>
    </node>
    <node id="n1573">
      <data key="v_name">552012</data>
      <data key="v_label">552012</data>
    </node>
    <node id="n1574">
      <data key="v_name">531979</data>
    </node>
    <node id="n1575">
      <data key="v_name">547494</data>
      <data key="v_label">547494</data>
    </node>
    <node id="n1576">
      <data key="v_name">351137</data>
      <data key="v_label">351137</data>
    </node>
    <node id="n1577">
      <data key="v_name">411375</data>
      <data key="v_label">411375</data>
    </node>
    <node id="n1578">
      <data key="v_name">439630</data>
      <data key="v_label">439630</data>
    </node>
    <node id="n1579">
      <data key="v_name">146271</data>
      <data key="v_label">146271</data>
    </node>
    <node id="n1580">
      <data key="v_name">250084</data>
      <data key="v_label">250084</data>
    </node>
    <node id="n1581">
      <data key="v_name">464116</data>
      <data key="v_label">464116</data>
    </node>
    <node id="n1582">
      <data key="v_name">424949</data>
      <data key="v_label">424949</data>
    </node>
    <node id="n1583">
      <data key="v_name">513746</data>
      <data key="v_label">513746</data>
    </node>
    <node id="n1584">
      <data key="v_name">410030</data>
      <data key="v_label">410030</data>
    </node>
    <node id="n1585">
      <data key="v_name">331999</data>
      <data key="v_label">331999</data>
    </node>
    <node id="n1586">
      <data key="v_name">562707</data>
      <data key="v_label">562707</data>
    </node>
    <node id="n1587">
      <data key="v_name">533380</data>
    </node>
    <node id="n1588">
      <data key="v_name">485665</data>
    </node>
    <node id="n1589">
      <data key="v_name">333704</data>
      <data key="v_label">333704</data>
    </node>
    <node id="n1590">
      <data key="v_name">13545</data>
      <data key="v_label">13545</data>
    </node>
    <node id="n1591">
      <data key="v_name">550175</data>
      <data key="v_label">550175</data>
    </node>
    <node id="n1592">
      <data key="v_name">554071</data>
      <data key="v_label">554071</data>
    </node>
    <node id="n1593">
      <data key="v_name">467768</data>
      <data key="v_label">467768</data>
    </node>
    <node id="n1594">
      <data key="v_name">382922</data>
    </node>
    <node id="n1595">
      <data key="v_name">497068</data>
    </node>
    <node id="n1596">
      <data key="v_name">430172</data>
      <data key="v_label">430172</data>
    </node>
    <node id="n1597">
      <data key="v_name">400310</data>
      <data key="v_label">400310</data>
    </node>
    <node id="n1598">
      <data key="v_name">337904</data>
      <data key="v_label">337904</data>
    </node>
    <node id="n1599">
      <data key="v_name">541922</data>
      <data key="v_label">541922</data>
    </node>
    <node id="n1600">
      <data key="v_name">508477</data>
      <data key="v_label">508477</data>
    </node>
    <node id="n1601">
      <data key="v_name">513294</data>
      <data key="v_label">513294</data>
    </node>
    <node id="n1602">
      <data key="v_name">132553</data>
      <data key="v_label">132553</data>
    </node>
    <node id="n1603">
      <data key="v_name">548220</data>
      <data key="v_label">548220</data>
    </node>
    <node id="n1604">
      <data key="v_name">561562</data>
    </node>
    <node id="n1605">
      <data key="v_name">409933</data>
      <data key="v_label">409933</data>
    </node>
    <node id="n1606">
      <data key="v_name">139962</data>
      <data key="v_label">139962</data>
    </node>
    <node id="n1607">
      <data key="v_name">134059</data>
      <data key="v_label">134059</data>
    </node>
    <node id="n1608">
      <data key="v_name">451785</data>
      <data key="v_label">451785</data>
    </node>
    <node id="n1609">
      <data key="v_name">332185</data>
      <data key="v_label">332185</data>
    </node>
    <node id="n1610">
      <data key="v_name">530729</data>
      <data key="v_label">530729</data>
    </node>
    <node id="n1611">
      <data key="v_name">449081</data>
      <data key="v_label">449081</data>
    </node>
    <node id="n1612">
      <data key="v_name">451105</data>
      <data key="v_label">451105</data>
    </node>
    <node id="n1613">
      <data key="v_name">228082</data>
      <data key="v_label">228082</data>
    </node>
    <node id="n1614">
      <data key="v_name">555363</data>
    </node>
    <node id="n1615">
      <data key="v_name">226190</data>
      <data key="v_label">226190</data>
    </node>
    <node id="n1616">
      <data key="v_name">398834</data>
      <data key="v_label">398834</data>
    </node>
    <node id="n1617">
      <data key="v_name">483394</data>
      <data key="v_label">483394</data>
    </node>
    <node id="n1618">
      <data key="v_name">362801</data>
      <data key="v_label">362801</data>
    </node>
    <node id="n1619">
      <data key="v_name">508252</data>
      <data key="v_label">508252</data>
    </node>
    <node id="n1620">
      <data key="v_name">358445</data>
      <data key="v_label">358445</data>
    </node>
    <node id="n1621">
      <data key="v_name">140191</data>
      <data key="v_label">140191</data>
    </node>
    <node id="n1622">
      <data key="v_name">563567</data>
      <data key="v_label">563567</data>
    </node>
    <node id="n1623">
      <data key="v_name">554550</data>
      <data key="v_label">554550</data>
    </node>
    <node id="n1624">
      <data key="v_name">361939</data>
      <data key="v_label">361939</data>
    </node>
    <node id="n1625">
      <data key="v_name">563733</data>
      <data key="v_label">563733</data>
    </node>
    <node id="n1626">
      <data key="v_name">559883</data>
      <data key="v_label">559883</data>
    </node>
    <node id="n1627">
      <data key="v_name">306467</data>
    </node>
    <node id="n1628">
      <data key="v_name">517847</data>
      <data key="v_label">517847</data>
    </node>
    <node id="n1629">
      <data key="v_name">146286</data>
      <data key="v_label">146286</data>
    </node>
    <node id="n1630">
      <data key="v_name">146287</data>
      <data key="v_label">146287</data>
    </node>
    <node id="n1631">
      <data key="v_name">146288</data>
      <data key="v_label">146288</data>
    </node>
    <node id="n1632">
      <data key="v_name">133791</data>
      <data key="v_label">133791</data>
    </node>
    <node id="n1633">
      <data key="v_name">420868</data>
      <data key="v_label">420868</data>
    </node>
    <node id="n1634">
      <data key="v_name">515717</data>
      <data key="v_label">515717</data>
    </node>
    <node id="n1635">
      <data key="v_name">368044</data>
      <data key="v_label">368044</data>
    </node>
    <node id="n1636">
      <data key="v_name">424054</data>
      <data key="v_label">424054</data>
    </node>
    <node id="n1637">
      <data key="v_name">146307</data>
      <data key="v_label">146307</data>
    </node>
    <node id="n1638">
      <data key="v_name">518691</data>
      <data key="v_label">518691</data>
    </node>
    <node id="n1639">
      <data key="v_name">455743</data>
      <data key="v_label">455743</data>
    </node>
    <node id="n1640">
      <data key="v_name">451155</data>
      <data key="v_label">451155</data>
    </node>
    <node id="n1641">
      <data key="v_name">557651</data>
    </node>
    <node id="n1642">
      <data key="v_name">553228</data>
    </node>
    <node id="n1643">
      <data key="v_name">507668</data>
    </node>
    <node id="n1644">
      <data key="v_name">553855</data>
      <data key="v_label">553855</data>
    </node>
    <node id="n1645">
      <data key="v_name">144588</data>
      <data key="v_label">144588</data>
    </node>
    <node id="n1646">
      <data key="v_name">517378</data>
      <data key="v_label">517378</data>
    </node>
    <node id="n1647">
      <data key="v_name">467324</data>
      <data key="v_label">467324</data>
    </node>
    <node id="n1648">
      <data key="v_name">499259</data>
    </node>
    <node id="n1649">
      <data key="v_name">152242</data>
      <data key="v_label">152242</data>
    </node>
    <node id="n1650">
      <data key="v_name">564706</data>
    </node>
    <node id="n1651">
      <data key="v_name">443993</data>
    </node>
    <node id="n1652">
      <data key="v_name">553615</data>
      <data key="v_label">553615</data>
    </node>
    <node id="n1653">
      <data key="v_name">234699</data>
    </node>
    <node id="n1654">
      <data key="v_name">533266</data>
    </node>
    <node id="n1655">
      <data key="v_name">205770</data>
      <data key="v_label">205770</data>
    </node>
    <node id="n1656">
      <data key="v_name">555532</data>
      <data key="v_label">555532</data>
    </node>
    <node id="n1657">
      <data key="v_name">409909</data>
    </node>
    <node id="n1658">
      <data key="v_name">516930</data>
      <data key="v_label">516930</data>
    </node>
    <node id="n1659">
      <data key="v_name">472358</data>
      <data key="v_label">472358</data>
    </node>
    <node id="n1660">
      <data key="v_name">515757</data>
      <data key="v_label">515757</data>
    </node>
    <node id="n1661">
      <data key="v_name">499298</data>
    </node>
    <node id="n1662">
      <data key="v_name">515717</data>
    </node>
    <node id="n1663">
      <data key="v_name">550822</data>
    </node>
    <node id="n1664">
      <data key="v_name">376529</data>
      <data key="v_label">376529</data>
    </node>
    <node id="n1665">
      <data key="v_name">121252</data>
      <data key="v_label">121252</data>
    </node>
    <node id="n1666">
      <data key="v_name">376530</data>
      <data key="v_label">376530</data>
    </node>
    <node id="n1667">
      <data key="v_name">448702</data>
    </node>
    <node id="n1668">
      <data key="v_name">322848</data>
      <data key="v_label">322848</data>
    </node>
    <node id="n1669">
      <data key="v_name">144524</data>
      <data key="v_label">144524</data>
    </node>
    <node id="n1670">
      <data key="v_name">451739</data>
      <data key="v_label">451739</data>
    </node>
    <node id="n1671">
      <data key="v_name">447448</data>
    </node>
    <node id="n1672">
      <data key="v_name">438512</data>
      <data key="v_label">438512</data>
    </node>
    <node id="n1673">
      <data key="v_name">353596</data>
      <data key="v_label">353596</data>
    </node>
    <node id="n1674">
      <data key="v_name">460460</data>
      <data key="v_label">460460</data>
    </node>
    <node id="n1675">
      <data key="v_name">109205</data>
      <data key="v_label">109205</data>
    </node>
    <node id="n1676">
      <data key="v_name">267052</data>
      <data key="v_label">267052</data>
    </node>
    <node id="n1677">
      <data key="v_name">542353</data>
      <data key="v_label">542353</data>
    </node>
    <node id="n1678">
      <data key="v_name">460103</data>
      <data key="v_label">460103</data>
    </node>
    <node id="n1679">
      <data key="v_name">456660</data>
      <data key="v_label">456660</data>
    </node>
    <node id="n1680">
      <data key="v_name">142856</data>
    </node>
    <node id="n1681">
      <data key="v_name">565000</data>
      <data key="v_label">565000</data>
    </node>
    <node id="n1682">
      <data key="v_name">458851</data>
      <data key="v_label">458851</data>
    </node>
    <node id="n1683">
      <data key="v_name">560444</data>
      <data key="v_label">560444</data>
    </node>
    <node id="n1684">
      <data key="v_name">480998</data>
    </node>
    <node id="n1685">
      <data key="v_name">550080</data>
    </node>
    <node id="n1686">
      <data key="v_name">560926</data>
    </node>
    <node id="n1687">
      <data key="v_name">558935</data>
      <data key="v_label">558935</data>
    </node>
    <node id="n1688">
      <data key="v_name">553707</data>
      <data key="v_label">553707</data>
    </node>
    <node id="n1689">
      <data key="v_name">560335</data>
      <data key="v_label">560335</data>
    </node>
    <node id="n1690">
      <data key="v_name">377594</data>
    </node>
    <node id="n1691">
      <data key="v_name">344532</data>
      <data key="v_label">344532</data>
    </node>
    <node id="n1692">
      <data key="v_name">527291</data>
      <data key="v_label">527291</data>
    </node>
    <node id="n1693">
      <data key="v_name">106595</data>
      <data key="v_label">106595</data>
    </node>
    <node id="n1694">
      <data key="v_name">106596</data>
      <data key="v_label">106596</data>
    </node>
    <node id="n1695">
      <data key="v_name">106597</data>
      <data key="v_label">106597</data>
    </node>
    <node id="n1696">
      <data key="v_name">106598</data>
      <data key="v_label">106598</data>
    </node>
    <node id="n1697">
      <data key="v_name">167616</data>
      <data key="v_label">167616</data>
    </node>
    <node id="n1698">
      <data key="v_name">314705</data>
      <data key="v_label">314705</data>
    </node>
    <node id="n1699">
      <data key="v_name">106601</data>
      <data key="v_label">106601</data>
    </node>
    <node id="n1700">
      <data key="v_name">285595</data>
      <data key="v_label">285595</data>
    </node>
    <node id="n1701">
      <data key="v_name">106603</data>
      <data key="v_label">106603</data>
    </node>
    <node id="n1702">
      <data key="v_name">106604</data>
      <data key="v_label">106604</data>
    </node>
    <node id="n1703">
      <data key="v_name">106605</data>
      <data key="v_label">106605</data>
    </node>
    <node id="n1704">
      <data key="v_name">406626</data>
      <data key="v_label">406626</data>
    </node>
    <node id="n1705">
      <data key="v_name">542091</data>
      <data key="v_label">542091</data>
    </node>
    <node id="n1706">
      <data key="v_name">385658</data>
      <data key="v_label">385658</data>
    </node>
    <node id="n1707">
      <data key="v_name">343510</data>
      <data key="v_label">343510</data>
    </node>
    <node id="n1708">
      <data key="v_name">470450</data>
    </node>
    <node id="n1709">
      <data key="v_name">348133</data>
    </node>
    <node id="n1710">
      <data key="v_name">468846</data>
    </node>
    <node id="n1711">
      <data key="v_name">518052</data>
    </node>
    <node id="n1712">
      <data key="v_name">154576</data>
      <data key="v_label">154576</data>
    </node>
    <node id="n1713">
      <data key="v_name">179576</data>
      <data key="v_label">179576</data>
    </node>
    <node id="n1714">
      <data key="v_name">179577</data>
      <data key="v_label">179577</data>
    </node>
    <node id="n1715">
      <data key="v_name">179578</data>
      <data key="v_label">179578</data>
    </node>
    <node id="n1716">
      <data key="v_name">503787</data>
      <data key="v_label">503787</data>
    </node>
    <node id="n1717">
      <data key="v_name">429092</data>
      <data key="v_label">429092</data>
    </node>
    <node id="n1718">
      <data key="v_name">541895</data>
      <data key="v_label">541895</data>
    </node>
    <node id="n1719">
      <data key="v_name">513341</data>
      <data key="v_label">513341</data>
    </node>
    <node id="n1720">
      <data key="v_name">513340</data>
      <data key="v_label">513340</data>
    </node>
    <node id="n1721">
      <data key="v_name">499259</data>
    </node>
    <node id="n1722">
      <data key="v_name">545655</data>
    </node>
    <node id="n1723">
      <data key="v_name">214815</data>
      <data key="v_label">214815</data>
    </node>
    <node id="n1724">
      <data key="v_name">562151</data>
      <data key="v_label">562151</data>
    </node>
    <node id="n1725">
      <data key="v_name">562152</data>
      <data key="v_label">562152</data>
    </node>
    <node id="n1726">
      <data key="v_name">543444</data>
      <data key="v_label">543444</data>
    </node>
    <node id="n1727">
      <data key="v_name">543446</data>
      <data key="v_label">543446</data>
    </node>
    <node id="n1728">
      <data key="v_name">543447</data>
      <data key="v_label">543447</data>
    </node>
    <node id="n1729">
      <data key="v_name">448584</data>
      <data key="v_label">448584</data>
    </node>
    <node id="n1730">
      <data key="v_name">560897</data>
      <data key="v_label">560897</data>
    </node>
    <node id="n1731">
      <data key="v_name">518366</data>
      <data key="v_label">518366</data>
    </node>
    <node id="n1732">
      <data key="v_name">477226</data>
      <data key="v_label">477226</data>
    </node>
    <node id="n1733">
      <data key="v_name">518005</data>
    </node>
    <node id="n1734">
      <data key="v_name">169508</data>
      <data key="v_label">169508</data>
    </node>
    <node id="n1735">
      <data key="v_name">120188</data>
      <data key="v_label">120188</data>
    </node>
    <node id="n1736">
      <data key="v_name">438592</data>
      <data key="v_label">438592</data>
    </node>
    <node id="n1737">
      <data key="v_name">335395</data>
      <data key="v_label">335395</data>
    </node>
    <node id="n1738">
      <data key="v_name">430635</data>
      <data key="v_label">430635</data>
    </node>
    <node id="n1739">
      <data key="v_name">127057</data>
      <data key="v_label">127057</data>
    </node>
    <node id="n1740">
      <data key="v_name">543277</data>
      <data key="v_label">543277</data>
    </node>
    <node id="n1741">
      <data key="v_name">470080</data>
      <data key="v_label">470080</data>
    </node>
    <node id="n1742">
      <data key="v_name">353750</data>
    </node>
    <node id="n1743">
      <data key="v_name">531300</data>
      <data key="v_label">531300</data>
    </node>
    <node id="n1744">
      <data key="v_name">485593</data>
    </node>
    <node id="n1745">
      <data key="v_name">543520</data>
    </node>
    <node id="n1746">
      <data key="v_name">485591</data>
    </node>
    <node id="n1747">
      <data key="v_name">532762</data>
      <data key="v_label">532762</data>
    </node>
    <node id="n1748">
      <data key="v_name">532760</data>
      <data key="v_label">532760</data>
    </node>
    <node id="n1749">
      <data key="v_name">105956</data>
      <data key="v_label">105956</data>
    </node>
    <node id="n1750">
      <data key="v_name">565263</data>
    </node>
    <node id="n1751">
      <data key="v_name">260669</data>
      <data key="v_label">260669</data>
    </node>
    <node id="n1752">
      <data key="v_name">141023</data>
      <data key="v_label">141023</data>
    </node>
    <node id="n1753">
      <data key="v_name">440622</data>
      <data key="v_label">440622</data>
    </node>
    <node id="n1754">
      <data key="v_name">141025</data>
      <data key="v_label">141025</data>
    </node>
    <node id="n1755">
      <data key="v_name">108227</data>
      <data key="v_label">108227</data>
    </node>
    <node id="n1756">
      <data key="v_name">108228</data>
      <data key="v_label">108228</data>
    </node>
    <node id="n1757">
      <data key="v_name">147305</data>
      <data key="v_label">147305</data>
    </node>
    <node id="n1758">
      <data key="v_name">147306</data>
      <data key="v_label">147306</data>
    </node>
    <node id="n1759">
      <data key="v_name">147307</data>
      <data key="v_label">147307</data>
    </node>
    <node id="n1760">
      <data key="v_name">390118</data>
      <data key="v_label">390118</data>
    </node>
    <node id="n1761">
      <data key="v_name">467010</data>
      <data key="v_label">467010</data>
    </node>
    <node id="n1762">
      <data key="v_name">451067</data>
      <data key="v_label">451067</data>
    </node>
    <node id="n1763">
      <data key="v_name">456688</data>
    </node>
    <node id="n1764">
      <data key="v_name">529350</data>
      <data key="v_label">529350</data>
    </node>
    <node id="n1765">
      <data key="v_name">484094</data>
      <data key="v_label">484094</data>
    </node>
    <node id="n1766">
      <data key="v_name">214771</data>
      <data key="v_label">214771</data>
    </node>
    <node id="n1767">
      <data key="v_name">464128</data>
      <data key="v_label">464128</data>
    </node>
    <node id="n1768">
      <data key="v_name">178018</data>
      <data key="v_label">178018</data>
    </node>
    <node id="n1769">
      <data key="v_name">261954</data>
      <data key="v_label">261954</data>
    </node>
    <node id="n1770">
      <data key="v_name">402028</data>
      <data key="v_label">402028</data>
    </node>
    <node id="n1771">
      <data key="v_name">560864</data>
      <data key="v_label">560864</data>
    </node>
    <node id="n1772">
      <data key="v_name">426630</data>
    </node>
    <node id="n1773">
      <data key="v_name">531589</data>
    </node>
    <node id="n1774">
      <data key="v_name">119553</data>
      <data key="v_label">119553</data>
    </node>
    <node id="n1775">
      <data key="v_name">518663</data>
      <data key="v_label">518663</data>
    </node>
    <node id="n1776">
      <data key="v_name">493252</data>
      <data key="v_label">493252</data>
    </node>
    <node id="n1777">
      <data key="v_name">490009</data>
      <data key="v_label">490009</data>
    </node>
    <node id="n1778">
      <data key="v_name">490010</data>
      <data key="v_label">490010</data>
    </node>
    <node id="n1779">
      <data key="v_name">490011</data>
      <data key="v_label">490011</data>
    </node>
    <node id="n1780">
      <data key="v_name">518562</data>
    </node>
    <node id="n1781">
      <data key="v_name">365889</data>
      <data key="v_label">365889</data>
    </node>
    <node id="n1782">
      <data key="v_name">145482</data>
      <data key="v_label">145482</data>
    </node>
    <node id="n1783">
      <data key="v_name">251935</data>
    </node>
    <node id="n1784">
      <data key="v_name">550262</data>
      <data key="v_label">550262</data>
    </node>
    <node id="n1785">
      <data key="v_name">522496</data>
      <data key="v_label">522496</data>
    </node>
    <node id="n1786">
      <data key="v_name">334101</data>
      <data key="v_label">334101</data>
    </node>
    <node id="n1787">
      <data key="v_name">364273</data>
      <data key="v_label">364273</data>
    </node>
    <node id="n1788">
      <data key="v_name">290193</data>
      <data key="v_label">290193</data>
    </node>
    <node id="n1789">
      <data key="v_name">514912</data>
      <data key="v_label">514912</data>
    </node>
    <node id="n1790">
      <data key="v_name">549712</data>
      <data key="v_label">549712</data>
    </node>
    <node id="n1791">
      <data key="v_name">523378</data>
      <data key="v_label">523378</data>
    </node>
    <node id="n1792">
      <data key="v_name">562540</data>
      <data key="v_label">562540</data>
    </node>
    <node id="n1793">
      <data key="v_name">179914</data>
      <data key="v_label">179914</data>
    </node>
    <node id="n1794">
      <data key="v_name">417590</data>
      <data key="v_label">417590</data>
    </node>
    <node id="n1795">
      <data key="v_name">549541</data>
    </node>
    <node id="n1796">
      <data key="v_name">207128</data>
      <data key="v_label">207128</data>
    </node>
    <node id="n1797">
      <data key="v_name">478308</data>
      <data key="v_label">478308</data>
    </node>
    <node id="n1798">
      <data key="v_name">318568</data>
      <data key="v_label">318568</data>
    </node>
    <node id="n1799">
      <data key="v_name">551159</data>
      <data key="v_label">551159</data>
    </node>
    <node id="n1800">
      <data key="v_name">515757</data>
    </node>
    <node id="n1801">
      <data key="v_name">125002</data>
      <data key="v_label">125002</data>
    </node>
    <node id="n1802">
      <data key="v_name">542982</data>
      <data key="v_label">542982</data>
    </node>
    <edge source="n0" target="n1">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">FRG: Materials Computation Center</data>
      <data key="e_abstract">9976550&lt;br/&gt;Martin&lt;br/&gt;This Focused Research Group award supports research and education in computational materials theory and associated algorithm development. It assists in establishing a Materials Computation Center at the University of Illinois at Urbana-Champaign for interdisciplinary materials research. The award is jointly funded by the Divisions of Materials Research, Chemistry, Physics, Mathematical Sciences and the Office of Multidisciplinary Activities in the Directorate for Mathematical and Physical Sciences, and by the Division of Advanced Computational Infrastructure and Research in the Directorate for Computer and Information Science and Engineering. The Center will provide a stimulating environment for state-of-the-art computational materials research and for development of new algorithms and computational approaches. The intellectual thrust of the research will be organized around broad research themes that will be coupled to experiment. The PIs will develop and maintain well-structured modular codes, educate students in the most modern computational materials science techniques, and apply computational methods to important materials problems. Algorithmic developments will span the range from programs for teaching to forefront programs for research. Included are programs for molecular dynamics and Monte Carlo simulations, density functional theory calculations, and linear-scaling algorithms for large complex systems. The PIs aim to work toward defining common analysis tools, input/output data structures, and well-defined interfaces between programs written by different people. This helps enable compatibility and coupling in multi-scale applications, as well as heterogeneous computing via portable browser-type interfaces. The PIs intend to benchmark results on test problems and compare results with experiments. The Center will have a core group of faculty from different disciplines committed to a program of collaborative research with shared postdoctoral associates and students. The Center will provide educational training and thesis research themes that address national needs for computational scientists with experience in applications to real problems and materials research. In conjunction with the Computational Science and Engineering program at UIUC, students will acquire comprehensive computer science experience as well as basic science and engineering training, and they will gain valuable experience working in teams across disciplines. &lt;br/&gt;&lt;br/&gt;This award supports research and education in computational materials theory and associated algorithm development. It assists in establishing a Materials Computation Center at the University of Illinois at Urbana-Champaign for interdisciplinary materials research. The award is jointly funded by the Divisions of Materials Research, Chemistry, Physics, Mathematical Sciences and the Office of Multidisciplinary Activities in the Directorate for Mathematical and Physical Sciences, and by the Division of Advanced Computational Infrastructure and Research in the Directorate for Computer and Information Science and Engineering. The Center will provide a stimulating environment for state-of-the-art computational materials research and for development of new algorithms and computational approaches. The intellectual thrust of the research will be organized around broad research themes that will be coupled to experiment. The PIs will develop and maintain well-structured modular codes, educate students in the most modern computational materials science techniques, and apply computational methods to important materials problems. Algorithmic developments will span the range from programs for teaching to forefront programs for research. Included are programs for molecular dynamics and Monte Carlo simulations, density functional theory calculations, and linear-scaling algorithms for large complex systems. The PIs aim to work toward defining common analysis tools, input/output data structures, and well-defined interfaces between programs written by different people. This helps enable compatibility and coupling in multi-scale applications, as well as heterogeneous computing via portable browser-type interfaces. The PIs intend to benchmark results on test problems and compare results with experiments. The Center will have a core group of faculty from different disciplines committed to a program of collaborative research with shared postdoctoral associates and students. The Center will provide educational training and thesis research themes that address national needs for computational scientists with experience in applications to real problems and materials research. In conjunction with the Computational Science and Engineering program at UIUC, students will acquire comprehensive computer science experience as well as basic science and engineering training, and they will gain valuable experience working in teams across disciplines.</data>
      <data key="e_pgm">1284</data>
      <data key="e_label">9.97655e+06</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">9.97655e+06</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n4" target="n5">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">EVITA - A Prototype System for Efficient Visualization and Interrogation of Terascale Datasets</data>
      <data key="e_abstract">Large-scale computational simulations of physical phenomena produce data of unprecedented size (terabytes and petabytes). Unfortunately, development of appropriate data management and visualization techniques has not kept pace with the size and complexity of such datasets. For many simulations, the storage of the data itself, not to mention seeing and understanding it, is a serious problem. This project responds to this by developing a prototype, integrated system (EVITA) to facilitate exploration of terascale datasets. This system will be concerned with the storage and representation of very large datasets, their analysis and manipulation and rendering and presentation of the resulting images. This will allow unprecedented access to our CFD data sets, which we expect to lead to new fundamental understanding.&lt;br/&gt;&lt;br/&gt;The project focuses on time-varying datasets from computational fluid dynamic/hydrodynamic and oceanographic applications on structured, rectilinear or curvilinear grids; however, the EVITA system is applicable as a general visualization environment for other terascale datasets with similar underlying structure. Additionally, the system is amenable to distributed- or even remote-visualization uses. The cornerstone of the EVITA system is a representational scheme that allows ranked access to macroscopic features in the dataset. The data and grid are transformed using wavelet techniques while a feature-detection algorithm is used to identify and rank contextually significant features directly. This ranking is used to generate a four-dimensional significance map that incorporates application-specific knowledge, which in turn allows an encoding with efficient compression and a progressive representation of significant features in the data. From this, the EVITA system creates as a preview an efficient, &quot;lossy&quot; image of the desired data and lets the user select regions of interest (ROls) for further enrichment. These &apos; Rols are given higher priority in the encoded bitstream, and subsequently displayed faster. Preliminary results demonstrate the efficacy of this approach. The project takes a novel approach to terascale visualization by capitalizing on the expertise of a multidisciplinary team to integrate research from several disciplines into the EVITA system. This both brings many perspectives to bear on the problem, and ensures that our results are practically useful.</data>
      <data key="e_pgm">1597</data>
      <data key="e_label">9.98234e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">9.98234e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n4" target="n6">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">EVITA - A Prototype System for Efficient Visualization and Interrogation of Terascale Datasets</data>
      <data key="e_abstract">Large-scale computational simulations of physical phenomena produce data of unprecedented size (terabytes and petabytes). Unfortunately, development of appropriate data management and visualization techniques has not kept pace with the size and complexity of such datasets. For many simulations, the storage of the data itself, not to mention seeing and understanding it, is a serious problem. This project responds to this by developing a prototype, integrated system (EVITA) to facilitate exploration of terascale datasets. This system will be concerned with the storage and representation of very large datasets, their analysis and manipulation and rendering and presentation of the resulting images. This will allow unprecedented access to our CFD data sets, which we expect to lead to new fundamental understanding.&lt;br/&gt;&lt;br/&gt;The project focuses on time-varying datasets from computational fluid dynamic/hydrodynamic and oceanographic applications on structured, rectilinear or curvilinear grids; however, the EVITA system is applicable as a general visualization environment for other terascale datasets with similar underlying structure. Additionally, the system is amenable to distributed- or even remote-visualization uses. The cornerstone of the EVITA system is a representational scheme that allows ranked access to macroscopic features in the dataset. The data and grid are transformed using wavelet techniques while a feature-detection algorithm is used to identify and rank contextually significant features directly. This ranking is used to generate a four-dimensional significance map that incorporates application-specific knowledge, which in turn allows an encoding with efficient compression and a progressive representation of significant features in the data. From this, the EVITA system creates as a preview an efficient, &quot;lossy&quot; image of the desired data and lets the user select regions of interest (ROls) for further enrichment. These &apos; Rols are given higher priority in the encoded bitstream, and subsequently displayed faster. Preliminary results demonstrate the efficacy of this approach. The project takes a novel approach to terascale visualization by capitalizing on the expertise of a multidisciplinary team to integrate research from several disciplines into the EVITA system. This both brings many perspectives to bear on the problem, and ensures that our results are practically useful.</data>
      <data key="e_pgm">1597</data>
      <data key="e_label">9.98234e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">9.98234e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n4" target="n7">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">EVITA - A Prototype System for Efficient Visualization and Interrogation of Terascale Datasets</data>
      <data key="e_abstract">Large-scale computational simulations of physical phenomena produce data of unprecedented size (terabytes and petabytes). Unfortunately, development of appropriate data management and visualization techniques has not kept pace with the size and complexity of such datasets. For many simulations, the storage of the data itself, not to mention seeing and understanding it, is a serious problem. This project responds to this by developing a prototype, integrated system (EVITA) to facilitate exploration of terascale datasets. This system will be concerned with the storage and representation of very large datasets, their analysis and manipulation and rendering and presentation of the resulting images. This will allow unprecedented access to our CFD data sets, which we expect to lead to new fundamental understanding.&lt;br/&gt;&lt;br/&gt;The project focuses on time-varying datasets from computational fluid dynamic/hydrodynamic and oceanographic applications on structured, rectilinear or curvilinear grids; however, the EVITA system is applicable as a general visualization environment for other terascale datasets with similar underlying structure. Additionally, the system is amenable to distributed- or even remote-visualization uses. The cornerstone of the EVITA system is a representational scheme that allows ranked access to macroscopic features in the dataset. The data and grid are transformed using wavelet techniques while a feature-detection algorithm is used to identify and rank contextually significant features directly. This ranking is used to generate a four-dimensional significance map that incorporates application-specific knowledge, which in turn allows an encoding with efficient compression and a progressive representation of significant features in the data. From this, the EVITA system creates as a preview an efficient, &quot;lossy&quot; image of the desired data and lets the user select regions of interest (ROls) for further enrichment. These &apos; Rols are given higher priority in the encoded bitstream, and subsequently displayed faster. Preliminary results demonstrate the efficacy of this approach. The project takes a novel approach to terascale visualization by capitalizing on the expertise of a multidisciplinary team to integrate research from several disciplines into the EVITA system. This both brings many perspectives to bear on the problem, and ensures that our results are practically useful.</data>
      <data key="e_pgm">1597</data>
      <data key="e_label">9.98234e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">9.98234e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n4" target="n8">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">EVITA - A Prototype System for Efficient Visualization and Interrogation of Terascale Datasets</data>
      <data key="e_abstract">Large-scale computational simulations of physical phenomena produce data of unprecedented size (terabytes and petabytes). Unfortunately, development of appropriate data management and visualization techniques has not kept pace with the size and complexity of such datasets. For many simulations, the storage of the data itself, not to mention seeing and understanding it, is a serious problem. This project responds to this by developing a prototype, integrated system (EVITA) to facilitate exploration of terascale datasets. This system will be concerned with the storage and representation of very large datasets, their analysis and manipulation and rendering and presentation of the resulting images. This will allow unprecedented access to our CFD data sets, which we expect to lead to new fundamental understanding.&lt;br/&gt;&lt;br/&gt;The project focuses on time-varying datasets from computational fluid dynamic/hydrodynamic and oceanographic applications on structured, rectilinear or curvilinear grids; however, the EVITA system is applicable as a general visualization environment for other terascale datasets with similar underlying structure. Additionally, the system is amenable to distributed- or even remote-visualization uses. The cornerstone of the EVITA system is a representational scheme that allows ranked access to macroscopic features in the dataset. The data and grid are transformed using wavelet techniques while a feature-detection algorithm is used to identify and rank contextually significant features directly. This ranking is used to generate a four-dimensional significance map that incorporates application-specific knowledge, which in turn allows an encoding with efficient compression and a progressive representation of significant features in the data. From this, the EVITA system creates as a preview an efficient, &quot;lossy&quot; image of the desired data and lets the user select regions of interest (ROls) for further enrichment. These &apos; Rols are given higher priority in the encoded bitstream, and subsequently displayed faster. Preliminary results demonstrate the efficacy of this approach. The project takes a novel approach to terascale visualization by capitalizing on the expertise of a multidisciplinary team to integrate research from several disciplines into the EVITA system. This both brings many perspectives to bear on the problem, and ensures that our results are practically useful.</data>
      <data key="e_pgm">1597</data>
      <data key="e_label">9.98234e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">9.98234e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n5" target="n6">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">EVITA - A Prototype System for Efficient Visualization and Interrogation of Terascale Datasets</data>
      <data key="e_abstract">Large-scale computational simulations of physical phenomena produce data of unprecedented size (terabytes and petabytes). Unfortunately, development of appropriate data management and visualization techniques has not kept pace with the size and complexity of such datasets. For many simulations, the storage of the data itself, not to mention seeing and understanding it, is a serious problem. This project responds to this by developing a prototype, integrated system (EVITA) to facilitate exploration of terascale datasets. This system will be concerned with the storage and representation of very large datasets, their analysis and manipulation and rendering and presentation of the resulting images. This will allow unprecedented access to our CFD data sets, which we expect to lead to new fundamental understanding.&lt;br/&gt;&lt;br/&gt;The project focuses on time-varying datasets from computational fluid dynamic/hydrodynamic and oceanographic applications on structured, rectilinear or curvilinear grids; however, the EVITA system is applicable as a general visualization environment for other terascale datasets with similar underlying structure. Additionally, the system is amenable to distributed- or even remote-visualization uses. The cornerstone of the EVITA system is a representational scheme that allows ranked access to macroscopic features in the dataset. The data and grid are transformed using wavelet techniques while a feature-detection algorithm is used to identify and rank contextually significant features directly. This ranking is used to generate a four-dimensional significance map that incorporates application-specific knowledge, which in turn allows an encoding with efficient compression and a progressive representation of significant features in the data. From this, the EVITA system creates as a preview an efficient, &quot;lossy&quot; image of the desired data and lets the user select regions of interest (ROls) for further enrichment. These &apos; Rols are given higher priority in the encoded bitstream, and subsequently displayed faster. Preliminary results demonstrate the efficacy of this approach. The project takes a novel approach to terascale visualization by capitalizing on the expertise of a multidisciplinary team to integrate research from several disciplines into the EVITA system. This both brings many perspectives to bear on the problem, and ensures that our results are practically useful.</data>
      <data key="e_pgm">1597</data>
      <data key="e_label">9.98234e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">9.98234e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n5" target="n7">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">EVITA - A Prototype System for Efficient Visualization and Interrogation of Terascale Datasets</data>
      <data key="e_abstract">Large-scale computational simulations of physical phenomena produce data of unprecedented size (terabytes and petabytes). Unfortunately, development of appropriate data management and visualization techniques has not kept pace with the size and complexity of such datasets. For many simulations, the storage of the data itself, not to mention seeing and understanding it, is a serious problem. This project responds to this by developing a prototype, integrated system (EVITA) to facilitate exploration of terascale datasets. This system will be concerned with the storage and representation of very large datasets, their analysis and manipulation and rendering and presentation of the resulting images. This will allow unprecedented access to our CFD data sets, which we expect to lead to new fundamental understanding.&lt;br/&gt;&lt;br/&gt;The project focuses on time-varying datasets from computational fluid dynamic/hydrodynamic and oceanographic applications on structured, rectilinear or curvilinear grids; however, the EVITA system is applicable as a general visualization environment for other terascale datasets with similar underlying structure. Additionally, the system is amenable to distributed- or even remote-visualization uses. The cornerstone of the EVITA system is a representational scheme that allows ranked access to macroscopic features in the dataset. The data and grid are transformed using wavelet techniques while a feature-detection algorithm is used to identify and rank contextually significant features directly. This ranking is used to generate a four-dimensional significance map that incorporates application-specific knowledge, which in turn allows an encoding with efficient compression and a progressive representation of significant features in the data. From this, the EVITA system creates as a preview an efficient, &quot;lossy&quot; image of the desired data and lets the user select regions of interest (ROls) for further enrichment. These &apos; Rols are given higher priority in the encoded bitstream, and subsequently displayed faster. Preliminary results demonstrate the efficacy of this approach. The project takes a novel approach to terascale visualization by capitalizing on the expertise of a multidisciplinary team to integrate research from several disciplines into the EVITA system. This both brings many perspectives to bear on the problem, and ensures that our results are practically useful.</data>
      <data key="e_pgm">1597</data>
      <data key="e_label">9.98234e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">9.98234e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n5" target="n8">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">EVITA - A Prototype System for Efficient Visualization and Interrogation of Terascale Datasets</data>
      <data key="e_abstract">Large-scale computational simulations of physical phenomena produce data of unprecedented size (terabytes and petabytes). Unfortunately, development of appropriate data management and visualization techniques has not kept pace with the size and complexity of such datasets. For many simulations, the storage of the data itself, not to mention seeing and understanding it, is a serious problem. This project responds to this by developing a prototype, integrated system (EVITA) to facilitate exploration of terascale datasets. This system will be concerned with the storage and representation of very large datasets, their analysis and manipulation and rendering and presentation of the resulting images. This will allow unprecedented access to our CFD data sets, which we expect to lead to new fundamental understanding.&lt;br/&gt;&lt;br/&gt;The project focuses on time-varying datasets from computational fluid dynamic/hydrodynamic and oceanographic applications on structured, rectilinear or curvilinear grids; however, the EVITA system is applicable as a general visualization environment for other terascale datasets with similar underlying structure. Additionally, the system is amenable to distributed- or even remote-visualization uses. The cornerstone of the EVITA system is a representational scheme that allows ranked access to macroscopic features in the dataset. The data and grid are transformed using wavelet techniques while a feature-detection algorithm is used to identify and rank contextually significant features directly. This ranking is used to generate a four-dimensional significance map that incorporates application-specific knowledge, which in turn allows an encoding with efficient compression and a progressive representation of significant features in the data. From this, the EVITA system creates as a preview an efficient, &quot;lossy&quot; image of the desired data and lets the user select regions of interest (ROls) for further enrichment. These &apos; Rols are given higher priority in the encoded bitstream, and subsequently displayed faster. Preliminary results demonstrate the efficacy of this approach. The project takes a novel approach to terascale visualization by capitalizing on the expertise of a multidisciplinary team to integrate research from several disciplines into the EVITA system. This both brings many perspectives to bear on the problem, and ensures that our results are practically useful.</data>
      <data key="e_pgm">1597</data>
      <data key="e_label">9.98234e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">9.98234e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n6" target="n7">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">EVITA - A Prototype System for Efficient Visualization and Interrogation of Terascale Datasets</data>
      <data key="e_abstract">Large-scale computational simulations of physical phenomena produce data of unprecedented size (terabytes and petabytes). Unfortunately, development of appropriate data management and visualization techniques has not kept pace with the size and complexity of such datasets. For many simulations, the storage of the data itself, not to mention seeing and understanding it, is a serious problem. This project responds to this by developing a prototype, integrated system (EVITA) to facilitate exploration of terascale datasets. This system will be concerned with the storage and representation of very large datasets, their analysis and manipulation and rendering and presentation of the resulting images. This will allow unprecedented access to our CFD data sets, which we expect to lead to new fundamental understanding.&lt;br/&gt;&lt;br/&gt;The project focuses on time-varying datasets from computational fluid dynamic/hydrodynamic and oceanographic applications on structured, rectilinear or curvilinear grids; however, the EVITA system is applicable as a general visualization environment for other terascale datasets with similar underlying structure. Additionally, the system is amenable to distributed- or even remote-visualization uses. The cornerstone of the EVITA system is a representational scheme that allows ranked access to macroscopic features in the dataset. The data and grid are transformed using wavelet techniques while a feature-detection algorithm is used to identify and rank contextually significant features directly. This ranking is used to generate a four-dimensional significance map that incorporates application-specific knowledge, which in turn allows an encoding with efficient compression and a progressive representation of significant features in the data. From this, the EVITA system creates as a preview an efficient, &quot;lossy&quot; image of the desired data and lets the user select regions of interest (ROls) for further enrichment. These &apos; Rols are given higher priority in the encoded bitstream, and subsequently displayed faster. Preliminary results demonstrate the efficacy of this approach. The project takes a novel approach to terascale visualization by capitalizing on the expertise of a multidisciplinary team to integrate research from several disciplines into the EVITA system. This both brings many perspectives to bear on the problem, and ensures that our results are practically useful.</data>
      <data key="e_pgm">1597</data>
      <data key="e_label">9.98234e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">9.98234e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n6" target="n8">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">EVITA - A Prototype System for Efficient Visualization and Interrogation of Terascale Datasets</data>
      <data key="e_abstract">Large-scale computational simulations of physical phenomena produce data of unprecedented size (terabytes and petabytes). Unfortunately, development of appropriate data management and visualization techniques has not kept pace with the size and complexity of such datasets. For many simulations, the storage of the data itself, not to mention seeing and understanding it, is a serious problem. This project responds to this by developing a prototype, integrated system (EVITA) to facilitate exploration of terascale datasets. This system will be concerned with the storage and representation of very large datasets, their analysis and manipulation and rendering and presentation of the resulting images. This will allow unprecedented access to our CFD data sets, which we expect to lead to new fundamental understanding.&lt;br/&gt;&lt;br/&gt;The project focuses on time-varying datasets from computational fluid dynamic/hydrodynamic and oceanographic applications on structured, rectilinear or curvilinear grids; however, the EVITA system is applicable as a general visualization environment for other terascale datasets with similar underlying structure. Additionally, the system is amenable to distributed- or even remote-visualization uses. The cornerstone of the EVITA system is a representational scheme that allows ranked access to macroscopic features in the dataset. The data and grid are transformed using wavelet techniques while a feature-detection algorithm is used to identify and rank contextually significant features directly. This ranking is used to generate a four-dimensional significance map that incorporates application-specific knowledge, which in turn allows an encoding with efficient compression and a progressive representation of significant features in the data. From this, the EVITA system creates as a preview an efficient, &quot;lossy&quot; image of the desired data and lets the user select regions of interest (ROls) for further enrichment. These &apos; Rols are given higher priority in the encoded bitstream, and subsequently displayed faster. Preliminary results demonstrate the efficacy of this approach. The project takes a novel approach to terascale visualization by capitalizing on the expertise of a multidisciplinary team to integrate research from several disciplines into the EVITA system. This both brings many perspectives to bear on the problem, and ensures that our results are practically useful.</data>
      <data key="e_pgm">1597</data>
      <data key="e_label">9.98234e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">9.98234e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n7" target="n8">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">EVITA - A Prototype System for Efficient Visualization and Interrogation of Terascale Datasets</data>
      <data key="e_abstract">Large-scale computational simulations of physical phenomena produce data of unprecedented size (terabytes and petabytes). Unfortunately, development of appropriate data management and visualization techniques has not kept pace with the size and complexity of such datasets. For many simulations, the storage of the data itself, not to mention seeing and understanding it, is a serious problem. This project responds to this by developing a prototype, integrated system (EVITA) to facilitate exploration of terascale datasets. This system will be concerned with the storage and representation of very large datasets, their analysis and manipulation and rendering and presentation of the resulting images. This will allow unprecedented access to our CFD data sets, which we expect to lead to new fundamental understanding.&lt;br/&gt;&lt;br/&gt;The project focuses on time-varying datasets from computational fluid dynamic/hydrodynamic and oceanographic applications on structured, rectilinear or curvilinear grids; however, the EVITA system is applicable as a general visualization environment for other terascale datasets with similar underlying structure. Additionally, the system is amenable to distributed- or even remote-visualization uses. The cornerstone of the EVITA system is a representational scheme that allows ranked access to macroscopic features in the dataset. The data and grid are transformed using wavelet techniques while a feature-detection algorithm is used to identify and rank contextually significant features directly. This ranking is used to generate a four-dimensional significance map that incorporates application-specific knowledge, which in turn allows an encoding with efficient compression and a progressive representation of significant features in the data. From this, the EVITA system creates as a preview an efficient, &quot;lossy&quot; image of the desired data and lets the user select regions of interest (ROls) for further enrichment. These &apos; Rols are given higher priority in the encoded bitstream, and subsequently displayed faster. Preliminary results demonstrate the efficacy of this approach. The project takes a novel approach to terascale visualization by capitalizing on the expertise of a multidisciplinary team to integrate research from several disciplines into the EVITA system. This both brings many perspectives to bear on the problem, and ensures that our results are practically useful.</data>
      <data key="e_pgm">1597</data>
      <data key="e_label">9.98234e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">9.98234e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n9" target="n10">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">Biocomplexity: Collaborative Research: Oceanic N2 Fixation and Global Climate</data>
      <data key="e_abstract">Oceanic nitrogen (N2) fixation has recently been identified as a significant part of the&lt;br/&gt;oceanic nitrogen (N) cycle and may directly influence the sequestration of atmospheric CO2 in&lt;br/&gt;the oceans by providing a new source of N to the upper water column. The prokaryotic&lt;br/&gt;microorganisms that convert N2 gas to reactive N are an unique subcomponent of planktonic&lt;br/&gt;ecosystems and exhibit a variety of complex dynamics including the formation of microbial&lt;br/&gt;consortia and symbioses and, at times, massive blooms. Accumulating evidence indicates that&lt;br/&gt;iron (Fe) availability may be a key controlling factor for these planktonic marine diazotrophs.&lt;br/&gt;The primary pathway of Fe delivery to the upper oceans is through dust deposition.&lt;br/&gt;N2 fixers may therefore be directly involved in global feedbacks with the climate system&lt;br/&gt;and these feedbacks may also exhibit complex dynamics on many different time-scales. &lt;br/&gt;&lt;br/&gt;The hypothesized feedback mechanisms will have the following component parts: The rate of N2&lt;br/&gt;fixation in the world&apos;s oceans can have an impact on the concentration of the greenhouse gas,&lt;br/&gt;carbon dioxide (CO2), in the atmosphere on time-scales of decades (variability in surface&lt;br/&gt;biogeochemistry) to millennia (changes in the total NO3 - stock from the balance of N2 fixation&lt;br/&gt;and denitrification). CO2 concentrations in the atmosphere influence the climate. The climate&lt;br/&gt;system, in turn, can influence the rate of N2 fixation in the oceans by controlling the supply of Fe&lt;br/&gt;on dust and by influencing the stratification of the upper ocean. Humans also have a direct role&lt;br/&gt;in the current manifestation of this feedback cycle by their influence on dust production, through&lt;br/&gt;agriculture at the margins of deserts, and by our own production of CO2 into the atmosphere.&lt;br/&gt;The circular nature of these influences can lead to a feedback system, particularly on longer time-scales.&lt;br/&gt;&lt;br/&gt;This collaborative and interdisciplinary group of investigators, led by Dr. Anthony Michaels, will study each of the components of this system and then to model the hypothesized feedback processes. Because of the interaction of the various parts of this system, keyed around the unique behavior and biogeochemistry of the prokaryotic microorganisms that can fix N2, this feedback loop should exhibit complex behaviors on a variety of time-scales. In this research, we will conduct a targeted series of experiments and field observations to understand and parameterize each of the pieces of this global process including the direct control of marine N2 fixation by dust deposition. This understanding will then feed a modeling process that examines the complex dynamics of this system on time-scales of years to millennia. The modeling process will be evaluated by comparison with data on the time-dependent behavior of ocean biogeochemistry</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">9.98154e+06</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">9.98154e+06</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n9" target="n11">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">Biocomplexity: Collaborative Research: Oceanic N2 Fixation and Global Climate</data>
      <data key="e_abstract">Oceanic nitrogen (N2) fixation has recently been identified as a significant part of the&lt;br/&gt;oceanic nitrogen (N) cycle and may directly influence the sequestration of atmospheric CO2 in&lt;br/&gt;the oceans by providing a new source of N to the upper water column. The prokaryotic&lt;br/&gt;microorganisms that convert N2 gas to reactive N are an unique subcomponent of planktonic&lt;br/&gt;ecosystems and exhibit a variety of complex dynamics including the formation of microbial&lt;br/&gt;consortia and symbioses and, at times, massive blooms. Accumulating evidence indicates that&lt;br/&gt;iron (Fe) availability may be a key controlling factor for these planktonic marine diazotrophs.&lt;br/&gt;The primary pathway of Fe delivery to the upper oceans is through dust deposition.&lt;br/&gt;N2 fixers may therefore be directly involved in global feedbacks with the climate system&lt;br/&gt;and these feedbacks may also exhibit complex dynamics on many different time-scales. &lt;br/&gt;&lt;br/&gt;The hypothesized feedback mechanisms will have the following component parts: The rate of N2&lt;br/&gt;fixation in the world&apos;s oceans can have an impact on the concentration of the greenhouse gas,&lt;br/&gt;carbon dioxide (CO2), in the atmosphere on time-scales of decades (variability in surface&lt;br/&gt;biogeochemistry) to millennia (changes in the total NO3 - stock from the balance of N2 fixation&lt;br/&gt;and denitrification). CO2 concentrations in the atmosphere influence the climate. The climate&lt;br/&gt;system, in turn, can influence the rate of N2 fixation in the oceans by controlling the supply of Fe&lt;br/&gt;on dust and by influencing the stratification of the upper ocean. Humans also have a direct role&lt;br/&gt;in the current manifestation of this feedback cycle by their influence on dust production, through&lt;br/&gt;agriculture at the margins of deserts, and by our own production of CO2 into the atmosphere.&lt;br/&gt;The circular nature of these influences can lead to a feedback system, particularly on longer time-scales.&lt;br/&gt;&lt;br/&gt;This collaborative and interdisciplinary group of investigators, led by Dr. Anthony Michaels, will study each of the components of this system and then to model the hypothesized feedback processes. Because of the interaction of the various parts of this system, keyed around the unique behavior and biogeochemistry of the prokaryotic microorganisms that can fix N2, this feedback loop should exhibit complex behaviors on a variety of time-scales. In this research, we will conduct a targeted series of experiments and field observations to understand and parameterize each of the pieces of this global process including the direct control of marine N2 fixation by dust deposition. This understanding will then feed a modeling process that examines the complex dynamics of this system on time-scales of years to millennia. The modeling process will be evaluated by comparison with data on the time-dependent behavior of ocean biogeochemistry</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">9.98154e+06</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">9.98154e+06</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n9" target="n12">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">Biocomplexity: Collaborative Research: Oceanic N2 Fixation and Global Climate</data>
      <data key="e_abstract">Oceanic nitrogen (N2) fixation has recently been identified as a significant part of the&lt;br/&gt;oceanic nitrogen (N) cycle and may directly influence the sequestration of atmospheric CO2 in&lt;br/&gt;the oceans by providing a new source of N to the upper water column. The prokaryotic&lt;br/&gt;microorganisms that convert N2 gas to reactive N are an unique subcomponent of planktonic&lt;br/&gt;ecosystems and exhibit a variety of complex dynamics including the formation of microbial&lt;br/&gt;consortia and symbioses and, at times, massive blooms. Accumulating evidence indicates that&lt;br/&gt;iron (Fe) availability may be a key controlling factor for these planktonic marine diazotrophs.&lt;br/&gt;The primary pathway of Fe delivery to the upper oceans is through dust deposition.&lt;br/&gt;N2 fixers may therefore be directly involved in global feedbacks with the climate system&lt;br/&gt;and these feedbacks may also exhibit complex dynamics on many different time-scales. &lt;br/&gt;&lt;br/&gt;The hypothesized feedback mechanisms will have the following component parts: The rate of N2&lt;br/&gt;fixation in the world&apos;s oceans can have an impact on the concentration of the greenhouse gas,&lt;br/&gt;carbon dioxide (CO2), in the atmosphere on time-scales of decades (variability in surface&lt;br/&gt;biogeochemistry) to millennia (changes in the total NO3 - stock from the balance of N2 fixation&lt;br/&gt;and denitrification). CO2 concentrations in the atmosphere influence the climate. The climate&lt;br/&gt;system, in turn, can influence the rate of N2 fixation in the oceans by controlling the supply of Fe&lt;br/&gt;on dust and by influencing the stratification of the upper ocean. Humans also have a direct role&lt;br/&gt;in the current manifestation of this feedback cycle by their influence on dust production, through&lt;br/&gt;agriculture at the margins of deserts, and by our own production of CO2 into the atmosphere.&lt;br/&gt;The circular nature of these influences can lead to a feedback system, particularly on longer time-scales.&lt;br/&gt;&lt;br/&gt;This collaborative and interdisciplinary group of investigators, led by Dr. Anthony Michaels, will study each of the components of this system and then to model the hypothesized feedback processes. Because of the interaction of the various parts of this system, keyed around the unique behavior and biogeochemistry of the prokaryotic microorganisms that can fix N2, this feedback loop should exhibit complex behaviors on a variety of time-scales. In this research, we will conduct a targeted series of experiments and field observations to understand and parameterize each of the pieces of this global process including the direct control of marine N2 fixation by dust deposition. This understanding will then feed a modeling process that examines the complex dynamics of this system on time-scales of years to millennia. The modeling process will be evaluated by comparison with data on the time-dependent behavior of ocean biogeochemistry</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">9.98154e+06</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">9.98154e+06</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n10" target="n11">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">Biocomplexity: Collaborative Research: Oceanic N2 Fixation and Global Climate</data>
      <data key="e_abstract">Oceanic nitrogen (N2) fixation has recently been identified as a significant part of the&lt;br/&gt;oceanic nitrogen (N) cycle and may directly influence the sequestration of atmospheric CO2 in&lt;br/&gt;the oceans by providing a new source of N to the upper water column. The prokaryotic&lt;br/&gt;microorganisms that convert N2 gas to reactive N are an unique subcomponent of planktonic&lt;br/&gt;ecosystems and exhibit a variety of complex dynamics including the formation of microbial&lt;br/&gt;consortia and symbioses and, at times, massive blooms. Accumulating evidence indicates that&lt;br/&gt;iron (Fe) availability may be a key controlling factor for these planktonic marine diazotrophs.&lt;br/&gt;The primary pathway of Fe delivery to the upper oceans is through dust deposition.&lt;br/&gt;N2 fixers may therefore be directly involved in global feedbacks with the climate system&lt;br/&gt;and these feedbacks may also exhibit complex dynamics on many different time-scales. &lt;br/&gt;&lt;br/&gt;The hypothesized feedback mechanisms will have the following component parts: The rate of N2&lt;br/&gt;fixation in the world&apos;s oceans can have an impact on the concentration of the greenhouse gas,&lt;br/&gt;carbon dioxide (CO2), in the atmosphere on time-scales of decades (variability in surface&lt;br/&gt;biogeochemistry) to millennia (changes in the total NO3 - stock from the balance of N2 fixation&lt;br/&gt;and denitrification). CO2 concentrations in the atmosphere influence the climate. The climate&lt;br/&gt;system, in turn, can influence the rate of N2 fixation in the oceans by controlling the supply of Fe&lt;br/&gt;on dust and by influencing the stratification of the upper ocean. Humans also have a direct role&lt;br/&gt;in the current manifestation of this feedback cycle by their influence on dust production, through&lt;br/&gt;agriculture at the margins of deserts, and by our own production of CO2 into the atmosphere.&lt;br/&gt;The circular nature of these influences can lead to a feedback system, particularly on longer time-scales.&lt;br/&gt;&lt;br/&gt;This collaborative and interdisciplinary group of investigators, led by Dr. Anthony Michaels, will study each of the components of this system and then to model the hypothesized feedback processes. Because of the interaction of the various parts of this system, keyed around the unique behavior and biogeochemistry of the prokaryotic microorganisms that can fix N2, this feedback loop should exhibit complex behaviors on a variety of time-scales. In this research, we will conduct a targeted series of experiments and field observations to understand and parameterize each of the pieces of this global process including the direct control of marine N2 fixation by dust deposition. This understanding will then feed a modeling process that examines the complex dynamics of this system on time-scales of years to millennia. The modeling process will be evaluated by comparison with data on the time-dependent behavior of ocean biogeochemistry</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">9.98154e+06</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">9.98154e+06</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n10" target="n12">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">Biocomplexity: Collaborative Research: Oceanic N2 Fixation and Global Climate</data>
      <data key="e_abstract">Oceanic nitrogen (N2) fixation has recently been identified as a significant part of the&lt;br/&gt;oceanic nitrogen (N) cycle and may directly influence the sequestration of atmospheric CO2 in&lt;br/&gt;the oceans by providing a new source of N to the upper water column. The prokaryotic&lt;br/&gt;microorganisms that convert N2 gas to reactive N are an unique subcomponent of planktonic&lt;br/&gt;ecosystems and exhibit a variety of complex dynamics including the formation of microbial&lt;br/&gt;consortia and symbioses and, at times, massive blooms. Accumulating evidence indicates that&lt;br/&gt;iron (Fe) availability may be a key controlling factor for these planktonic marine diazotrophs.&lt;br/&gt;The primary pathway of Fe delivery to the upper oceans is through dust deposition.&lt;br/&gt;N2 fixers may therefore be directly involved in global feedbacks with the climate system&lt;br/&gt;and these feedbacks may also exhibit complex dynamics on many different time-scales. &lt;br/&gt;&lt;br/&gt;The hypothesized feedback mechanisms will have the following component parts: The rate of N2&lt;br/&gt;fixation in the world&apos;s oceans can have an impact on the concentration of the greenhouse gas,&lt;br/&gt;carbon dioxide (CO2), in the atmosphere on time-scales of decades (variability in surface&lt;br/&gt;biogeochemistry) to millennia (changes in the total NO3 - stock from the balance of N2 fixation&lt;br/&gt;and denitrification). CO2 concentrations in the atmosphere influence the climate. The climate&lt;br/&gt;system, in turn, can influence the rate of N2 fixation in the oceans by controlling the supply of Fe&lt;br/&gt;on dust and by influencing the stratification of the upper ocean. Humans also have a direct role&lt;br/&gt;in the current manifestation of this feedback cycle by their influence on dust production, through&lt;br/&gt;agriculture at the margins of deserts, and by our own production of CO2 into the atmosphere.&lt;br/&gt;The circular nature of these influences can lead to a feedback system, particularly on longer time-scales.&lt;br/&gt;&lt;br/&gt;This collaborative and interdisciplinary group of investigators, led by Dr. Anthony Michaels, will study each of the components of this system and then to model the hypothesized feedback processes. Because of the interaction of the various parts of this system, keyed around the unique behavior and biogeochemistry of the prokaryotic microorganisms that can fix N2, this feedback loop should exhibit complex behaviors on a variety of time-scales. In this research, we will conduct a targeted series of experiments and field observations to understand and parameterize each of the pieces of this global process including the direct control of marine N2 fixation by dust deposition. This understanding will then feed a modeling process that examines the complex dynamics of this system on time-scales of years to millennia. The modeling process will be evaluated by comparison with data on the time-dependent behavior of ocean biogeochemistry</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">9.98154e+06</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">9.98154e+06</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n11" target="n12">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">Biocomplexity: Collaborative Research: Oceanic N2 Fixation and Global Climate</data>
      <data key="e_abstract">Oceanic nitrogen (N2) fixation has recently been identified as a significant part of the&lt;br/&gt;oceanic nitrogen (N) cycle and may directly influence the sequestration of atmospheric CO2 in&lt;br/&gt;the oceans by providing a new source of N to the upper water column. The prokaryotic&lt;br/&gt;microorganisms that convert N2 gas to reactive N are an unique subcomponent of planktonic&lt;br/&gt;ecosystems and exhibit a variety of complex dynamics including the formation of microbial&lt;br/&gt;consortia and symbioses and, at times, massive blooms. Accumulating evidence indicates that&lt;br/&gt;iron (Fe) availability may be a key controlling factor for these planktonic marine diazotrophs.&lt;br/&gt;The primary pathway of Fe delivery to the upper oceans is through dust deposition.&lt;br/&gt;N2 fixers may therefore be directly involved in global feedbacks with the climate system&lt;br/&gt;and these feedbacks may also exhibit complex dynamics on many different time-scales. &lt;br/&gt;&lt;br/&gt;The hypothesized feedback mechanisms will have the following component parts: The rate of N2&lt;br/&gt;fixation in the world&apos;s oceans can have an impact on the concentration of the greenhouse gas,&lt;br/&gt;carbon dioxide (CO2), in the atmosphere on time-scales of decades (variability in surface&lt;br/&gt;biogeochemistry) to millennia (changes in the total NO3 - stock from the balance of N2 fixation&lt;br/&gt;and denitrification). CO2 concentrations in the atmosphere influence the climate. The climate&lt;br/&gt;system, in turn, can influence the rate of N2 fixation in the oceans by controlling the supply of Fe&lt;br/&gt;on dust and by influencing the stratification of the upper ocean. Humans also have a direct role&lt;br/&gt;in the current manifestation of this feedback cycle by their influence on dust production, through&lt;br/&gt;agriculture at the margins of deserts, and by our own production of CO2 into the atmosphere.&lt;br/&gt;The circular nature of these influences can lead to a feedback system, particularly on longer time-scales.&lt;br/&gt;&lt;br/&gt;This collaborative and interdisciplinary group of investigators, led by Dr. Anthony Michaels, will study each of the components of this system and then to model the hypothesized feedback processes. Because of the interaction of the various parts of this system, keyed around the unique behavior and biogeochemistry of the prokaryotic microorganisms that can fix N2, this feedback loop should exhibit complex behaviors on a variety of time-scales. In this research, we will conduct a targeted series of experiments and field observations to understand and parameterize each of the pieces of this global process including the direct control of marine N2 fixation by dust deposition. This understanding will then feed a modeling process that examines the complex dynamics of this system on time-scales of years to millennia. The modeling process will be evaluated by comparison with data on the time-dependent behavior of ocean biogeochemistry</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">9.98154e+06</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">9.98154e+06</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n14" target="n15">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">Soft-Decision Decoding of Codes</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">4096</data>
      <data key="e_label">96191</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">96191</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n9" target="n20">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">BIOCOMPLEXITY: Collaborative Research: Factors Affecting, and Impact of, Diazotrophic Microorganisms in the Western Equatorial Atlantic Ocean</data>
      <data key="e_abstract">BIOCOMPLEXITY: Collaborative Research: Factors affecting, and impact of,&lt;br/&gt;diazotrophic microorganisms in the western Equatorial Atlantic Ocean&lt;br/&gt;&lt;br/&gt; This biocomplexity research focuses on plankton dynamics in the western Equatorial Atlantic Ocean (WEQAT). This is a complex and understudied ecosystem that has significant impacts on marine resources&lt;br/&gt;in the region as well as in downstream areas such as the Caribbean Sea. The study centers on&lt;br/&gt;diazotrophic (nitrogen fixing) microorganisms as keystone species. Geological, physical, biological,&lt;br/&gt;chemical and even social factors all have a major influence on population biology and activity of&lt;br/&gt;diazotrophs in the WEQAT. Diazotrophs in turn have a major impact on other phytoplankton and&lt;br/&gt;trophic levels through input of fixed nitrogen (N). The Amazon River affects the region&lt;br/&gt;physically by changing salinity and thereby water column stratification, and geochemically by&lt;br/&gt;introducing iron and silicate which can then biologically stimulate the growth of diatoms that&lt;br/&gt;contain the N2 fixing endosymbiont Richelia intracellularis. Furthermore, the area receives&lt;br/&gt;significant seasonal atmospheric inputs of iron in dust from the Sahel region of Africa, which can&lt;br/&gt;promote the growth of the important N2 fixing cyanobacterium Trichodesmium. This&lt;br/&gt;atmospheric iron source is directly deposited on the surface waters where biological activity is&lt;br/&gt;greatest. For Trichodesmium, the physical environment (e.g. high wind speed) can also inhibit&lt;br/&gt;activity and the formation of blooms. Diazotrophs may be affected by land use practices in the&lt;br/&gt;Amazon Basin and the African Sahel, and N2 fixed by marine plankton can affect humans by&lt;br/&gt;stimulating primary productivity and fishery yields.&lt;br/&gt; Using both remote sensing and shipboard measurements, scientists will examine the complex processes which structure these planktonic diazotroph populations, influence their importance in CO2 and&lt;br/&gt;N2 fixation, which, in turn, affect other planktonic processes. The seasonal and spatial&lt;br/&gt;relationships of Trichodesmium and Hemiaulus / Richelia associations will be examined with&lt;br/&gt;direct reference to the major routes of inputs of Fe and Si, and with regard to the physical&lt;br/&gt;environment. The group of collaborating scientists will examine the trophic structures associated with each diazotrophic community, including the vertical distribution of processes and associated autotrophic and&lt;br/&gt;heterotrophic plankton populations. These data will be used to develop and verify&lt;br/&gt;biogeochemical and trophodynamic models that incorporate the complex physical, chemical and&lt;br/&gt;biological interactions that characterize the WEQAT region. The models will, in turn, be used to&lt;br/&gt;examine the hypothesis that physical forcing, through its effect on the diazotrophic populations&lt;br/&gt;and the structure of the food web, influences N2 fixation and, in part, determines the high&lt;br/&gt;productivity of the WEQAT.&lt;br/&gt; The work uses a combination of both observations and models to address three&lt;br/&gt;fundamental issues in biocomplexity: 1) the relationship between ecosystem structure and&lt;br/&gt;function in a system that is both nonlinear and high-dimensional; 2) the response of a nonlinear&lt;br/&gt;ecosystem to environmental forcing; and 3) the relevant level of detail, including the resolution&lt;br/&gt;of physical space, that must be incorporated in nonlinear systems to capture the dynamics of a&lt;br/&gt;global ecosystem property (here, high productivity). The research will significantly advance our understanding of the interaction between physical and biogeochemical processes in an important area the world&apos;s oceans, and identify how these interactions regulate variability in marine ecosystem productivity.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">9.98137e+06</data>
      <data key="e_expirationDate">2005-12-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">9.98137e+06</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n22" target="n23">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">High Speed Network Measurements: Techniques and Tools</data>
      <data key="e_abstract">This award provides funding for two years of work focused on the measurement and analysis of network performance from the perspective of endpoint applications. To date, much of the high performance network measurement activity has focused on the core network and its potential. This work will help to develop understanding about the additional problems faced in marrying campus infrastructures and LAN infrastructures to a high speed backbone. Building on prior work with tools such as OCXmon, Coral, SNMP, and others, the research will utilize experiments customized to high speed applications and will incorporate the data collection, postprocessing, and eventual opportunity for network feedback to applications for purposes of adaptivity or monitoring. One additional emphasis will be to work with those engaged in developing new generations of network-based applications in order to factor in their requirements and to verify the measurement results.</data>
      <data key="e_pgm">4090</data>
      <data key="e_label">9.98314e+06</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98314e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n22" target="n24">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">High Speed Network Measurements: Techniques and Tools</data>
      <data key="e_abstract">This award provides funding for two years of work focused on the measurement and analysis of network performance from the perspective of endpoint applications. To date, much of the high performance network measurement activity has focused on the core network and its potential. This work will help to develop understanding about the additional problems faced in marrying campus infrastructures and LAN infrastructures to a high speed backbone. Building on prior work with tools such as OCXmon, Coral, SNMP, and others, the research will utilize experiments customized to high speed applications and will incorporate the data collection, postprocessing, and eventual opportunity for network feedback to applications for purposes of adaptivity or monitoring. One additional emphasis will be to work with those engaged in developing new generations of network-based applications in order to factor in their requirements and to verify the measurement results.</data>
      <data key="e_pgm">4090</data>
      <data key="e_label">9.98314e+06</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98314e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n23" target="n24">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">High Speed Network Measurements: Techniques and Tools</data>
      <data key="e_abstract">This award provides funding for two years of work focused on the measurement and analysis of network performance from the perspective of endpoint applications. To date, much of the high performance network measurement activity has focused on the core network and its potential. This work will help to develop understanding about the additional problems faced in marrying campus infrastructures and LAN infrastructures to a high speed backbone. Building on prior work with tools such as OCXmon, Coral, SNMP, and others, the research will utilize experiments customized to high speed applications and will incorporate the data collection, postprocessing, and eventual opportunity for network feedback to applications for purposes of adaptivity or monitoring. One additional emphasis will be to work with those engaged in developing new generations of network-based applications in order to factor in their requirements and to verify the measurement results.</data>
      <data key="e_pgm">4090</data>
      <data key="e_label">9.98314e+06</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98314e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n31" target="n32">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">Dynamic Composition of Information Retrieval Techniques</data>
      <data key="e_abstract">This is a collaborative project between the Information Retrieval Center and the Resource-Bounded Reasoning Lab at UMass. The project is aimed at developing a new approach to meta-level control of search and applying it to improve the flexibility, adaptability, quality of service, and robustness of information retrieval search engines. Currently, such systems are built by integrating a fixed set of modules and techniques that perform such tasks as query formation, query optimization, query evaluation, precision improvement, and recall improvement. The new approach consists of context-dependent mechanisms for optimal selection of information retrieval techniques based on a probabilistic description of their performance. The approach addresses effectively the high level of uncertainty regarding the duration of complex retrieval techniques and the quality of the result they produce. The current static approach to integration of information retrieval modules continues to produce performance gains of about 10% each year, but the systems are extremely specialized for each task, and it is not clear how well results will generalize to new types of retrieval. This project provides significant advantages because it allows a system to configure itself dynamically to the specific task at hand, to the person using the system, and to limited computational resources. This study will result in systems that are far more flexible in handling a large set of retrieval tasks with possible applications to a range of other problems such as dynamic selection of tasks for autonomous robots to optimize the quality of service. &lt;br/&gt;http://anytime.cs.umass.edu/shlomo/research/DCIR.html</data>
      <data key="e_pgm">5980</data>
      <data key="e_label">9.90733e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">9.90733e+06</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n33" target="n34">
      <data key="e_effectiveDate">2000-01-15</data>
      <data key="e_title">Internet Addressing and the Domain Name Systems: Technical Alternatives and Policy Implications</data>
      <data key="e_abstract">This award provides partial support, over an eighteen-month period, for a study project to examine ways in which new technological approaches may help reduce policy-related conflicts associated with the Internet Domain Name System. Focus areas will include identifying different technical options for providing services associated with the domain name management system, and identifying policies and procedures to complement each of the technical alternatives. A report will be generated summarizing the findings and recommendations and will be made broadly available to the community.</data>
      <data key="e_pgm">4090</data>
      <data key="e_label">9.90985e+06</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.90985e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n33" target="n35">
      <data key="e_effectiveDate">2000-01-15</data>
      <data key="e_title">Internet Addressing and the Domain Name Systems: Technical Alternatives and Policy Implications</data>
      <data key="e_abstract">This award provides partial support, over an eighteen-month period, for a study project to examine ways in which new technological approaches may help reduce policy-related conflicts associated with the Internet Domain Name System. Focus areas will include identifying different technical options for providing services associated with the domain name management system, and identifying policies and procedures to complement each of the technical alternatives. A report will be generated summarizing the findings and recommendations and will be made broadly available to the community.</data>
      <data key="e_pgm">4090</data>
      <data key="e_label">9.90985e+06</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.90985e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n34" target="n35">
      <data key="e_effectiveDate">2000-01-15</data>
      <data key="e_title">Internet Addressing and the Domain Name Systems: Technical Alternatives and Policy Implications</data>
      <data key="e_abstract">This award provides partial support, over an eighteen-month period, for a study project to examine ways in which new technological approaches may help reduce policy-related conflicts associated with the Internet Domain Name System. Focus areas will include identifying different technical options for providing services associated with the domain name management system, and identifying policies and procedures to complement each of the technical alternatives. A report will be generated summarizing the findings and recommendations and will be made broadly available to the community.</data>
      <data key="e_pgm">4090</data>
      <data key="e_label">9.90985e+06</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.90985e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n36" target="n37">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">U.S.-Finland Cooperative Research: Fermi Surface Determination of Quasicrystal via Compton Scattering</data>
      <data key="e_abstract">9978382&lt;br/&gt;Moss&lt;br/&gt;This award supports Simon Moss, Paul Chow, Joseph Kulik, and a graduate student from the University of Houston in a collaboration with Hannu Pekka Suortti at the X-Ray Laboratory at the University of Helsinki, Finland. The project will focus on establishing the existence of a Fermi surface and measuring its dimensions using Compton scattering from selected quasicrystals and related intermetallic structures. A long-range goal is to understand the transport and electronic properties of complex conductors which can eventually include metallic glasses, conducting oxides, and magnetic materials. The collaboration draws on the individual expertise of researchers in the fields of quasicrystals (U. of Houston, Ames Laboratory, and City College of New York) and Compton scattering (U. of Helsinki). The overall scientific benefit of the collaboration to the research community will be to provide important information on alloy physics which would be difficult for any individual group to collect and analyze due to the complicated nature of quasicrystals and the sophisticated techniques required for high-energy Compton scattering.</data>
      <data key="e_pgm">5980</data>
      <data key="e_label">9.97838e+06</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0109</data>
      <data key="e_awardID">9.97838e+06</data>
      <data key="e_dir">01</data>
    </edge>
    <edge source="n36" target="n38">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">U.S.-Finland Cooperative Research: Fermi Surface Determination of Quasicrystal via Compton Scattering</data>
      <data key="e_abstract">9978382&lt;br/&gt;Moss&lt;br/&gt;This award supports Simon Moss, Paul Chow, Joseph Kulik, and a graduate student from the University of Houston in a collaboration with Hannu Pekka Suortti at the X-Ray Laboratory at the University of Helsinki, Finland. The project will focus on establishing the existence of a Fermi surface and measuring its dimensions using Compton scattering from selected quasicrystals and related intermetallic structures. A long-range goal is to understand the transport and electronic properties of complex conductors which can eventually include metallic glasses, conducting oxides, and magnetic materials. The collaboration draws on the individual expertise of researchers in the fields of quasicrystals (U. of Houston, Ames Laboratory, and City College of New York) and Compton scattering (U. of Helsinki). The overall scientific benefit of the collaboration to the research community will be to provide important information on alloy physics which would be difficult for any individual group to collect and analyze due to the complicated nature of quasicrystals and the sophisticated techniques required for high-energy Compton scattering.</data>
      <data key="e_pgm">5980</data>
      <data key="e_label">9.97838e+06</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0109</data>
      <data key="e_awardID">9.97838e+06</data>
      <data key="e_dir">01</data>
    </edge>
    <edge source="n37" target="n38">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">U.S.-Finland Cooperative Research: Fermi Surface Determination of Quasicrystal via Compton Scattering</data>
      <data key="e_abstract">9978382&lt;br/&gt;Moss&lt;br/&gt;This award supports Simon Moss, Paul Chow, Joseph Kulik, and a graduate student from the University of Houston in a collaboration with Hannu Pekka Suortti at the X-Ray Laboratory at the University of Helsinki, Finland. The project will focus on establishing the existence of a Fermi surface and measuring its dimensions using Compton scattering from selected quasicrystals and related intermetallic structures. A long-range goal is to understand the transport and electronic properties of complex conductors which can eventually include metallic glasses, conducting oxides, and magnetic materials. The collaboration draws on the individual expertise of researchers in the fields of quasicrystals (U. of Houston, Ames Laboratory, and City College of New York) and Compton scattering (U. of Helsinki). The overall scientific benefit of the collaboration to the research community will be to provide important information on alloy physics which would be difficult for any individual group to collect and analyze due to the complicated nature of quasicrystals and the sophisticated techniques required for high-energy Compton scattering.</data>
      <data key="e_pgm">5980</data>
      <data key="e_label">9.97838e+06</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0109</data>
      <data key="e_awardID">9.97838e+06</data>
      <data key="e_dir">01</data>
    </edge>
    <edge source="n39" target="n40">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">GK-12 Engineering Fellows: A K-12 Resource for Integrating Engineering, Math and Science</data>
      <data key="e_abstract">This program, a project of the College of Engineering with collaborative support from the Department of Education at Tufts University and the Nashoba Regional School District, is infusing the K-10 curriculum with engineering approaches and activities that are aligned with the new Massachusetts Science and Technology/Engineering Standards by placing engineering and computer science students (fellows) in the classrooms. Using their backgrounds, and equipped with a set of hands-on engineering-based project ideas and pedagogy know-how, the fellows are working with teacher liaisons to introduce engineering principles into the K-10 grades. The material being introduced conforms with the recently developed Massachusetts Standards for Science and Technology/Engineering and National Standards of Technology and Science. This material is being collected and created by an experienced team of Tufts faculty, K-12 professionals and graduate students. This is the first systemic and formal attempt to introduce engineering throughout the K-10 student experiences, with special emphasis on middle school. As a by-product, it is expected that this program will lead to new and productive collaborations between engineers and mathematical scientists, educators, and education researchers, and create a model program for engineering education in K-12 environments.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97959e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.97959e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n39" target="n41">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">GK-12 Engineering Fellows: A K-12 Resource for Integrating Engineering, Math and Science</data>
      <data key="e_abstract">This program, a project of the College of Engineering with collaborative support from the Department of Education at Tufts University and the Nashoba Regional School District, is infusing the K-10 curriculum with engineering approaches and activities that are aligned with the new Massachusetts Science and Technology/Engineering Standards by placing engineering and computer science students (fellows) in the classrooms. Using their backgrounds, and equipped with a set of hands-on engineering-based project ideas and pedagogy know-how, the fellows are working with teacher liaisons to introduce engineering principles into the K-10 grades. The material being introduced conforms with the recently developed Massachusetts Standards for Science and Technology/Engineering and National Standards of Technology and Science. This material is being collected and created by an experienced team of Tufts faculty, K-12 professionals and graduate students. This is the first systemic and formal attempt to introduce engineering throughout the K-10 student experiences, with special emphasis on middle school. As a by-product, it is expected that this program will lead to new and productive collaborations between engineers and mathematical scientists, educators, and education researchers, and create a model program for engineering education in K-12 environments.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97959e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.97959e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n39" target="n42">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">GK-12 Engineering Fellows: A K-12 Resource for Integrating Engineering, Math and Science</data>
      <data key="e_abstract">This program, a project of the College of Engineering with collaborative support from the Department of Education at Tufts University and the Nashoba Regional School District, is infusing the K-10 curriculum with engineering approaches and activities that are aligned with the new Massachusetts Science and Technology/Engineering Standards by placing engineering and computer science students (fellows) in the classrooms. Using their backgrounds, and equipped with a set of hands-on engineering-based project ideas and pedagogy know-how, the fellows are working with teacher liaisons to introduce engineering principles into the K-10 grades. The material being introduced conforms with the recently developed Massachusetts Standards for Science and Technology/Engineering and National Standards of Technology and Science. This material is being collected and created by an experienced team of Tufts faculty, K-12 professionals and graduate students. This is the first systemic and formal attempt to introduce engineering throughout the K-10 student experiences, with special emphasis on middle school. As a by-product, it is expected that this program will lead to new and productive collaborations between engineers and mathematical scientists, educators, and education researchers, and create a model program for engineering education in K-12 environments.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97959e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.97959e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n39" target="n43">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">GK-12 Engineering Fellows: A K-12 Resource for Integrating Engineering, Math and Science</data>
      <data key="e_abstract">This program, a project of the College of Engineering with collaborative support from the Department of Education at Tufts University and the Nashoba Regional School District, is infusing the K-10 curriculum with engineering approaches and activities that are aligned with the new Massachusetts Science and Technology/Engineering Standards by placing engineering and computer science students (fellows) in the classrooms. Using their backgrounds, and equipped with a set of hands-on engineering-based project ideas and pedagogy know-how, the fellows are working with teacher liaisons to introduce engineering principles into the K-10 grades. The material being introduced conforms with the recently developed Massachusetts Standards for Science and Technology/Engineering and National Standards of Technology and Science. This material is being collected and created by an experienced team of Tufts faculty, K-12 professionals and graduate students. This is the first systemic and formal attempt to introduce engineering throughout the K-10 student experiences, with special emphasis on middle school. As a by-product, it is expected that this program will lead to new and productive collaborations between engineers and mathematical scientists, educators, and education researchers, and create a model program for engineering education in K-12 environments.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97959e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.97959e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n39" target="n44">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">GK-12 Engineering Fellows: A K-12 Resource for Integrating Engineering, Math and Science</data>
      <data key="e_abstract">This program, a project of the College of Engineering with collaborative support from the Department of Education at Tufts University and the Nashoba Regional School District, is infusing the K-10 curriculum with engineering approaches and activities that are aligned with the new Massachusetts Science and Technology/Engineering Standards by placing engineering and computer science students (fellows) in the classrooms. Using their backgrounds, and equipped with a set of hands-on engineering-based project ideas and pedagogy know-how, the fellows are working with teacher liaisons to introduce engineering principles into the K-10 grades. The material being introduced conforms with the recently developed Massachusetts Standards for Science and Technology/Engineering and National Standards of Technology and Science. This material is being collected and created by an experienced team of Tufts faculty, K-12 professionals and graduate students. This is the first systemic and formal attempt to introduce engineering throughout the K-10 student experiences, with special emphasis on middle school. As a by-product, it is expected that this program will lead to new and productive collaborations between engineers and mathematical scientists, educators, and education researchers, and create a model program for engineering education in K-12 environments.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97959e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.97959e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n40" target="n41">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">GK-12 Engineering Fellows: A K-12 Resource for Integrating Engineering, Math and Science</data>
      <data key="e_abstract">This program, a project of the College of Engineering with collaborative support from the Department of Education at Tufts University and the Nashoba Regional School District, is infusing the K-10 curriculum with engineering approaches and activities that are aligned with the new Massachusetts Science and Technology/Engineering Standards by placing engineering and computer science students (fellows) in the classrooms. Using their backgrounds, and equipped with a set of hands-on engineering-based project ideas and pedagogy know-how, the fellows are working with teacher liaisons to introduce engineering principles into the K-10 grades. The material being introduced conforms with the recently developed Massachusetts Standards for Science and Technology/Engineering and National Standards of Technology and Science. This material is being collected and created by an experienced team of Tufts faculty, K-12 professionals and graduate students. This is the first systemic and formal attempt to introduce engineering throughout the K-10 student experiences, with special emphasis on middle school. As a by-product, it is expected that this program will lead to new and productive collaborations between engineers and mathematical scientists, educators, and education researchers, and create a model program for engineering education in K-12 environments.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97959e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.97959e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n40" target="n42">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">GK-12 Engineering Fellows: A K-12 Resource for Integrating Engineering, Math and Science</data>
      <data key="e_abstract">This program, a project of the College of Engineering with collaborative support from the Department of Education at Tufts University and the Nashoba Regional School District, is infusing the K-10 curriculum with engineering approaches and activities that are aligned with the new Massachusetts Science and Technology/Engineering Standards by placing engineering and computer science students (fellows) in the classrooms. Using their backgrounds, and equipped with a set of hands-on engineering-based project ideas and pedagogy know-how, the fellows are working with teacher liaisons to introduce engineering principles into the K-10 grades. The material being introduced conforms with the recently developed Massachusetts Standards for Science and Technology/Engineering and National Standards of Technology and Science. This material is being collected and created by an experienced team of Tufts faculty, K-12 professionals and graduate students. This is the first systemic and formal attempt to introduce engineering throughout the K-10 student experiences, with special emphasis on middle school. As a by-product, it is expected that this program will lead to new and productive collaborations between engineers and mathematical scientists, educators, and education researchers, and create a model program for engineering education in K-12 environments.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97959e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.97959e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n40" target="n43">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">GK-12 Engineering Fellows: A K-12 Resource for Integrating Engineering, Math and Science</data>
      <data key="e_abstract">This program, a project of the College of Engineering with collaborative support from the Department of Education at Tufts University and the Nashoba Regional School District, is infusing the K-10 curriculum with engineering approaches and activities that are aligned with the new Massachusetts Science and Technology/Engineering Standards by placing engineering and computer science students (fellows) in the classrooms. Using their backgrounds, and equipped with a set of hands-on engineering-based project ideas and pedagogy know-how, the fellows are working with teacher liaisons to introduce engineering principles into the K-10 grades. The material being introduced conforms with the recently developed Massachusetts Standards for Science and Technology/Engineering and National Standards of Technology and Science. This material is being collected and created by an experienced team of Tufts faculty, K-12 professionals and graduate students. This is the first systemic and formal attempt to introduce engineering throughout the K-10 student experiences, with special emphasis on middle school. As a by-product, it is expected that this program will lead to new and productive collaborations between engineers and mathematical scientists, educators, and education researchers, and create a model program for engineering education in K-12 environments.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97959e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.97959e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n40" target="n44">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">GK-12 Engineering Fellows: A K-12 Resource for Integrating Engineering, Math and Science</data>
      <data key="e_abstract">This program, a project of the College of Engineering with collaborative support from the Department of Education at Tufts University and the Nashoba Regional School District, is infusing the K-10 curriculum with engineering approaches and activities that are aligned with the new Massachusetts Science and Technology/Engineering Standards by placing engineering and computer science students (fellows) in the classrooms. Using their backgrounds, and equipped with a set of hands-on engineering-based project ideas and pedagogy know-how, the fellows are working with teacher liaisons to introduce engineering principles into the K-10 grades. The material being introduced conforms with the recently developed Massachusetts Standards for Science and Technology/Engineering and National Standards of Technology and Science. This material is being collected and created by an experienced team of Tufts faculty, K-12 professionals and graduate students. This is the first systemic and formal attempt to introduce engineering throughout the K-10 student experiences, with special emphasis on middle school. As a by-product, it is expected that this program will lead to new and productive collaborations between engineers and mathematical scientists, educators, and education researchers, and create a model program for engineering education in K-12 environments.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97959e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.97959e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n41" target="n42">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">GK-12 Engineering Fellows: A K-12 Resource for Integrating Engineering, Math and Science</data>
      <data key="e_abstract">This program, a project of the College of Engineering with collaborative support from the Department of Education at Tufts University and the Nashoba Regional School District, is infusing the K-10 curriculum with engineering approaches and activities that are aligned with the new Massachusetts Science and Technology/Engineering Standards by placing engineering and computer science students (fellows) in the classrooms. Using their backgrounds, and equipped with a set of hands-on engineering-based project ideas and pedagogy know-how, the fellows are working with teacher liaisons to introduce engineering principles into the K-10 grades. The material being introduced conforms with the recently developed Massachusetts Standards for Science and Technology/Engineering and National Standards of Technology and Science. This material is being collected and created by an experienced team of Tufts faculty, K-12 professionals and graduate students. This is the first systemic and formal attempt to introduce engineering throughout the K-10 student experiences, with special emphasis on middle school. As a by-product, it is expected that this program will lead to new and productive collaborations between engineers and mathematical scientists, educators, and education researchers, and create a model program for engineering education in K-12 environments.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97959e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.97959e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n41" target="n43">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">GK-12 Engineering Fellows: A K-12 Resource for Integrating Engineering, Math and Science</data>
      <data key="e_abstract">This program, a project of the College of Engineering with collaborative support from the Department of Education at Tufts University and the Nashoba Regional School District, is infusing the K-10 curriculum with engineering approaches and activities that are aligned with the new Massachusetts Science and Technology/Engineering Standards by placing engineering and computer science students (fellows) in the classrooms. Using their backgrounds, and equipped with a set of hands-on engineering-based project ideas and pedagogy know-how, the fellows are working with teacher liaisons to introduce engineering principles into the K-10 grades. The material being introduced conforms with the recently developed Massachusetts Standards for Science and Technology/Engineering and National Standards of Technology and Science. This material is being collected and created by an experienced team of Tufts faculty, K-12 professionals and graduate students. This is the first systemic and formal attempt to introduce engineering throughout the K-10 student experiences, with special emphasis on middle school. As a by-product, it is expected that this program will lead to new and productive collaborations between engineers and mathematical scientists, educators, and education researchers, and create a model program for engineering education in K-12 environments.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97959e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.97959e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n41" target="n44">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">GK-12 Engineering Fellows: A K-12 Resource for Integrating Engineering, Math and Science</data>
      <data key="e_abstract">This program, a project of the College of Engineering with collaborative support from the Department of Education at Tufts University and the Nashoba Regional School District, is infusing the K-10 curriculum with engineering approaches and activities that are aligned with the new Massachusetts Science and Technology/Engineering Standards by placing engineering and computer science students (fellows) in the classrooms. Using their backgrounds, and equipped with a set of hands-on engineering-based project ideas and pedagogy know-how, the fellows are working with teacher liaisons to introduce engineering principles into the K-10 grades. The material being introduced conforms with the recently developed Massachusetts Standards for Science and Technology/Engineering and National Standards of Technology and Science. This material is being collected and created by an experienced team of Tufts faculty, K-12 professionals and graduate students. This is the first systemic and formal attempt to introduce engineering throughout the K-10 student experiences, with special emphasis on middle school. As a by-product, it is expected that this program will lead to new and productive collaborations between engineers and mathematical scientists, educators, and education researchers, and create a model program for engineering education in K-12 environments.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97959e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.97959e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n42" target="n43">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">GK-12 Engineering Fellows: A K-12 Resource for Integrating Engineering, Math and Science</data>
      <data key="e_abstract">This program, a project of the College of Engineering with collaborative support from the Department of Education at Tufts University and the Nashoba Regional School District, is infusing the K-10 curriculum with engineering approaches and activities that are aligned with the new Massachusetts Science and Technology/Engineering Standards by placing engineering and computer science students (fellows) in the classrooms. Using their backgrounds, and equipped with a set of hands-on engineering-based project ideas and pedagogy know-how, the fellows are working with teacher liaisons to introduce engineering principles into the K-10 grades. The material being introduced conforms with the recently developed Massachusetts Standards for Science and Technology/Engineering and National Standards of Technology and Science. This material is being collected and created by an experienced team of Tufts faculty, K-12 professionals and graduate students. This is the first systemic and formal attempt to introduce engineering throughout the K-10 student experiences, with special emphasis on middle school. As a by-product, it is expected that this program will lead to new and productive collaborations between engineers and mathematical scientists, educators, and education researchers, and create a model program for engineering education in K-12 environments.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97959e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.97959e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n42" target="n44">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">GK-12 Engineering Fellows: A K-12 Resource for Integrating Engineering, Math and Science</data>
      <data key="e_abstract">This program, a project of the College of Engineering with collaborative support from the Department of Education at Tufts University and the Nashoba Regional School District, is infusing the K-10 curriculum with engineering approaches and activities that are aligned with the new Massachusetts Science and Technology/Engineering Standards by placing engineering and computer science students (fellows) in the classrooms. Using their backgrounds, and equipped with a set of hands-on engineering-based project ideas and pedagogy know-how, the fellows are working with teacher liaisons to introduce engineering principles into the K-10 grades. The material being introduced conforms with the recently developed Massachusetts Standards for Science and Technology/Engineering and National Standards of Technology and Science. This material is being collected and created by an experienced team of Tufts faculty, K-12 professionals and graduate students. This is the first systemic and formal attempt to introduce engineering throughout the K-10 student experiences, with special emphasis on middle school. As a by-product, it is expected that this program will lead to new and productive collaborations between engineers and mathematical scientists, educators, and education researchers, and create a model program for engineering education in K-12 environments.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97959e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.97959e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n43" target="n44">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">GK-12 Engineering Fellows: A K-12 Resource for Integrating Engineering, Math and Science</data>
      <data key="e_abstract">This program, a project of the College of Engineering with collaborative support from the Department of Education at Tufts University and the Nashoba Regional School District, is infusing the K-10 curriculum with engineering approaches and activities that are aligned with the new Massachusetts Science and Technology/Engineering Standards by placing engineering and computer science students (fellows) in the classrooms. Using their backgrounds, and equipped with a set of hands-on engineering-based project ideas and pedagogy know-how, the fellows are working with teacher liaisons to introduce engineering principles into the K-10 grades. The material being introduced conforms with the recently developed Massachusetts Standards for Science and Technology/Engineering and National Standards of Technology and Science. This material is being collected and created by an experienced team of Tufts faculty, K-12 professionals and graduate students. This is the first systemic and formal attempt to introduce engineering throughout the K-10 student experiences, with special emphasis on middle school. As a by-product, it is expected that this program will lead to new and productive collaborations between engineers and mathematical scientists, educators, and education researchers, and create a model program for engineering education in K-12 environments.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97959e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.97959e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n46" target="n47">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">Collaborative Research: Oceanic N2 Fixation and Global Climate</data>
      <data key="e_abstract">BIOCOMPLEXITY: OCEANIC N2 FIXATION AND GLOBAL CLIMATE&lt;br/&gt;&lt;br/&gt; Oceanic nitrogen (N2) fixation has recently been identified as a significant part of the&lt;br/&gt;oceanic nitrogen (N) cycle and may directly influence the sequestration of atmospheric CO2 in&lt;br/&gt;the oceans by providing a new source of N to the upper water column. The prokaryotic&lt;br/&gt;microorganisms that convert N2 gas to reactive N are an unique subcomponent of planktonic&lt;br/&gt;ecosystems and exhibit a variety of complex dynamics including the formation of microbial&lt;br/&gt;consortia and symbioses and, at times, massive blooms. Accumulating evidence indicates that&lt;br/&gt;iron (Fe) availability may be a key controlling factor for these planktonic marine diazotrophs.&lt;br/&gt;The primary pathway of Fe delivery to the upper oceans is through dust deposition.&lt;br/&gt;N2 fixers may therefore be directly involved in global feedbacks with the climate system&lt;br/&gt;and these feedbacks may also exhibit complex dynamics on many different time-scales. &lt;br/&gt; The hypothesized feedback mechanisms will have the following component parts: The rate of N2&lt;br/&gt;fixation in the world&apos;s oceans can have an impact on the concentration of the greenhouse gas,&lt;br/&gt;carbon dioxide (CO2), in the atmosphere on time-scales of decades (variability in surface&lt;br/&gt;biogeochemistry) to millennia (changes in the total NO3 - stock from the balance of N2 fixation&lt;br/&gt;and denitrification). CO2 concentrations in the atmosphere influence the climate. The climate&lt;br/&gt;system, in turn, can influence the rate of N2 fixation in the oceans by controlling the supply of Fe&lt;br/&gt;on dust and by influencing the stratification of the upper ocean. Humans also have a direct role&lt;br/&gt;in the current manifestation of this feedback cycle by their influence on dust production, through&lt;br/&gt;agriculture at the margins of deserts, and by our own production of CO2 into the atmosphere.&lt;br/&gt;The circular nature of these influences can lead to a feedback system, particularly on longer time-scales.&lt;br/&gt; This collaborative and interdisciplinary group of investigators, led by Dr. Anthony Michaels, will study each of the components of this system and then to model the hypothesized feedback processes. Because of the interaction of the various parts of this system, keyed around the unique behavior and biogeochemistry of the prokaryotic microorganisms that can fix N2, this feedback loop should exhibit complex behaviors on a variety of time-scales. In this research, we will conduct a targeted series of experiments and field observations to understand and parameterize each of the pieces of this global process including the direct control of marine N2 fixation by dust deposition. This understanding will then feed a modeling process that examines the complex dynamics of this system on time-scales of years to millennia. The modeling process will be evaluated by comparison with data on the time-dependent behavior of&lt;br/&gt;ocean biogeochemistry</data>
      <data key="e_pgm">1705</data>
      <data key="e_label">9.9814e+06</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.9814e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n48" target="n49">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">BIOCOMPLEXITY: Collaborative Research: Factors Affecting, and Impact of, Diazotrophic Microorganisms in the Western Equatorial Atlantic Ocean</data>
      <data key="e_abstract">BIOCOMPLEXITY: Collaborative Research: Factors affecting, and impact of,&lt;br/&gt;diazotrophic microorganisms in the western Equatorial Atlantic Ocean&lt;br/&gt;&lt;br/&gt; This biocomplexity research focuses on plankton dynamics in the western Equatorial Atlantic Ocean (WEQAT). This is a complex and understudied ecosystem that has significant impacts on marine resources&lt;br/&gt;in the region as well as in downstream areas such as the Caribbean Sea. The study centers on&lt;br/&gt;diazotrophic (nitrogen fixing) microorganisms as keystone species. Geological, physical, biological,&lt;br/&gt;chemical and even social factors all have a major influence on population biology and activity of&lt;br/&gt;diazotrophs in the WEQAT. Diazotrophs in turn have a major impact on other phytoplankton and&lt;br/&gt;trophic levels through input of fixed nitrogen (N). The Amazon River affects the region&lt;br/&gt;physically by changing salinity and thereby water column stratification, and geochemically by&lt;br/&gt;introducing iron and silicate which can then biologically stimulate the growth of diatoms that&lt;br/&gt;contain the N2 fixing endosymbiont Richelia intracellularis. Furthermore, the area receives&lt;br/&gt;significant seasonal atmospheric inputs of iron in dust from the Sahel region of Africa, which can&lt;br/&gt;promote the growth of the important N2 fixing cyanobacterium Trichodesmium. This&lt;br/&gt;atmospheric iron source is directly deposited on the surface waters where biological activity is&lt;br/&gt;greatest. For Trichodesmium, the physical environment (e.g. high wind speed) can also inhibit&lt;br/&gt;activity and the formation of blooms. Diazotrophs may be affected by land use practices in the&lt;br/&gt;Amazon Basin and the African Sahel, and N2 fixed by marine plankton can affect humans by&lt;br/&gt;stimulating primary productivity and fishery yields.&lt;br/&gt; Using both remote sensing and shipboard measurements, scientists will examine the complex processes which structure these planktonic diazotroph populations, influence their importance in CO2 and&lt;br/&gt;N2 fixation, which, in turn, affect other planktonic processes. The seasonal and spatial&lt;br/&gt;relationships of Trichodesmium and Hemiaulus / Richelia associations will be examined with&lt;br/&gt;direct reference to the major routes of inputs of Fe and Si, and with regard to the physical&lt;br/&gt;environment. The group of collaborating scientists will examine the trophic structures associated with each diazotrophic community, including the vertical distribution of processes and associated autotrophic and&lt;br/&gt;heterotrophic plankton populations. These data will be used to develop and verify&lt;br/&gt;biogeochemical and trophodynamic models that incorporate the complex physical, chemical and&lt;br/&gt;biological interactions that characterize the WEQAT region. The models will, in turn, be used to&lt;br/&gt;examine the hypothesis that physical forcing, through its effect on the diazotrophic populations&lt;br/&gt;and the structure of the food web, influences N2 fixation and, in part, determines the high&lt;br/&gt;productivity of the WEQAT.&lt;br/&gt; The work uses a combination of both observations and models to address three&lt;br/&gt;fundamental issues in biocomplexity: 1) the relationship between ecosystem structure and&lt;br/&gt;function in a system that is both nonlinear and high-dimensional; 2) the response of a nonlinear&lt;br/&gt;ecosystem to environmental forcing; and 3) the relevant level of detail, including the resolution&lt;br/&gt;of physical space, that must be incorporated in nonlinear systems to capture the dynamics of a&lt;br/&gt;global ecosystem property (here, high productivity). The research will significantly advance our understanding of the interaction between physical and biogeochemical processes in an important area the world&apos;s oceans, and identify how these interactions regulate variability in marine ecosystem productivity.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">9.98122e+06</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">9.98122e+06</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n48" target="n50">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">BIOCOMPLEXITY: Collaborative Research: Factors Affecting, and Impact of, Diazotrophic Microorganisms in the Western Equatorial Atlantic Ocean</data>
      <data key="e_abstract">BIOCOMPLEXITY: Collaborative Research: Factors affecting, and impact of,&lt;br/&gt;diazotrophic microorganisms in the western Equatorial Atlantic Ocean&lt;br/&gt;&lt;br/&gt; This biocomplexity research focuses on plankton dynamics in the western Equatorial Atlantic Ocean (WEQAT). This is a complex and understudied ecosystem that has significant impacts on marine resources&lt;br/&gt;in the region as well as in downstream areas such as the Caribbean Sea. The study centers on&lt;br/&gt;diazotrophic (nitrogen fixing) microorganisms as keystone species. Geological, physical, biological,&lt;br/&gt;chemical and even social factors all have a major influence on population biology and activity of&lt;br/&gt;diazotrophs in the WEQAT. Diazotrophs in turn have a major impact on other phytoplankton and&lt;br/&gt;trophic levels through input of fixed nitrogen (N). The Amazon River affects the region&lt;br/&gt;physically by changing salinity and thereby water column stratification, and geochemically by&lt;br/&gt;introducing iron and silicate which can then biologically stimulate the growth of diatoms that&lt;br/&gt;contain the N2 fixing endosymbiont Richelia intracellularis. Furthermore, the area receives&lt;br/&gt;significant seasonal atmospheric inputs of iron in dust from the Sahel region of Africa, which can&lt;br/&gt;promote the growth of the important N2 fixing cyanobacterium Trichodesmium. This&lt;br/&gt;atmospheric iron source is directly deposited on the surface waters where biological activity is&lt;br/&gt;greatest. For Trichodesmium, the physical environment (e.g. high wind speed) can also inhibit&lt;br/&gt;activity and the formation of blooms. Diazotrophs may be affected by land use practices in the&lt;br/&gt;Amazon Basin and the African Sahel, and N2 fixed by marine plankton can affect humans by&lt;br/&gt;stimulating primary productivity and fishery yields.&lt;br/&gt; Using both remote sensing and shipboard measurements, scientists will examine the complex processes which structure these planktonic diazotroph populations, influence their importance in CO2 and&lt;br/&gt;N2 fixation, which, in turn, affect other planktonic processes. The seasonal and spatial&lt;br/&gt;relationships of Trichodesmium and Hemiaulus / Richelia associations will be examined with&lt;br/&gt;direct reference to the major routes of inputs of Fe and Si, and with regard to the physical&lt;br/&gt;environment. The group of collaborating scientists will examine the trophic structures associated with each diazotrophic community, including the vertical distribution of processes and associated autotrophic and&lt;br/&gt;heterotrophic plankton populations. These data will be used to develop and verify&lt;br/&gt;biogeochemical and trophodynamic models that incorporate the complex physical, chemical and&lt;br/&gt;biological interactions that characterize the WEQAT region. The models will, in turn, be used to&lt;br/&gt;examine the hypothesis that physical forcing, through its effect on the diazotrophic populations&lt;br/&gt;and the structure of the food web, influences N2 fixation and, in part, determines the high&lt;br/&gt;productivity of the WEQAT.&lt;br/&gt; The work uses a combination of both observations and models to address three&lt;br/&gt;fundamental issues in biocomplexity: 1) the relationship between ecosystem structure and&lt;br/&gt;function in a system that is both nonlinear and high-dimensional; 2) the response of a nonlinear&lt;br/&gt;ecosystem to environmental forcing; and 3) the relevant level of detail, including the resolution&lt;br/&gt;of physical space, that must be incorporated in nonlinear systems to capture the dynamics of a&lt;br/&gt;global ecosystem property (here, high productivity). The research will significantly advance our understanding of the interaction between physical and biogeochemical processes in an important area the world&apos;s oceans, and identify how these interactions regulate variability in marine ecosystem productivity.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">9.98122e+06</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">9.98122e+06</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n49" target="n50">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">BIOCOMPLEXITY: Collaborative Research: Factors Affecting, and Impact of, Diazotrophic Microorganisms in the Western Equatorial Atlantic Ocean</data>
      <data key="e_abstract">BIOCOMPLEXITY: Collaborative Research: Factors affecting, and impact of,&lt;br/&gt;diazotrophic microorganisms in the western Equatorial Atlantic Ocean&lt;br/&gt;&lt;br/&gt; This biocomplexity research focuses on plankton dynamics in the western Equatorial Atlantic Ocean (WEQAT). This is a complex and understudied ecosystem that has significant impacts on marine resources&lt;br/&gt;in the region as well as in downstream areas such as the Caribbean Sea. The study centers on&lt;br/&gt;diazotrophic (nitrogen fixing) microorganisms as keystone species. Geological, physical, biological,&lt;br/&gt;chemical and even social factors all have a major influence on population biology and activity of&lt;br/&gt;diazotrophs in the WEQAT. Diazotrophs in turn have a major impact on other phytoplankton and&lt;br/&gt;trophic levels through input of fixed nitrogen (N). The Amazon River affects the region&lt;br/&gt;physically by changing salinity and thereby water column stratification, and geochemically by&lt;br/&gt;introducing iron and silicate which can then biologically stimulate the growth of diatoms that&lt;br/&gt;contain the N2 fixing endosymbiont Richelia intracellularis. Furthermore, the area receives&lt;br/&gt;significant seasonal atmospheric inputs of iron in dust from the Sahel region of Africa, which can&lt;br/&gt;promote the growth of the important N2 fixing cyanobacterium Trichodesmium. This&lt;br/&gt;atmospheric iron source is directly deposited on the surface waters where biological activity is&lt;br/&gt;greatest. For Trichodesmium, the physical environment (e.g. high wind speed) can also inhibit&lt;br/&gt;activity and the formation of blooms. Diazotrophs may be affected by land use practices in the&lt;br/&gt;Amazon Basin and the African Sahel, and N2 fixed by marine plankton can affect humans by&lt;br/&gt;stimulating primary productivity and fishery yields.&lt;br/&gt; Using both remote sensing and shipboard measurements, scientists will examine the complex processes which structure these planktonic diazotroph populations, influence their importance in CO2 and&lt;br/&gt;N2 fixation, which, in turn, affect other planktonic processes. The seasonal and spatial&lt;br/&gt;relationships of Trichodesmium and Hemiaulus / Richelia associations will be examined with&lt;br/&gt;direct reference to the major routes of inputs of Fe and Si, and with regard to the physical&lt;br/&gt;environment. The group of collaborating scientists will examine the trophic structures associated with each diazotrophic community, including the vertical distribution of processes and associated autotrophic and&lt;br/&gt;heterotrophic plankton populations. These data will be used to develop and verify&lt;br/&gt;biogeochemical and trophodynamic models that incorporate the complex physical, chemical and&lt;br/&gt;biological interactions that characterize the WEQAT region. The models will, in turn, be used to&lt;br/&gt;examine the hypothesis that physical forcing, through its effect on the diazotrophic populations&lt;br/&gt;and the structure of the food web, influences N2 fixation and, in part, determines the high&lt;br/&gt;productivity of the WEQAT.&lt;br/&gt; The work uses a combination of both observations and models to address three&lt;br/&gt;fundamental issues in biocomplexity: 1) the relationship between ecosystem structure and&lt;br/&gt;function in a system that is both nonlinear and high-dimensional; 2) the response of a nonlinear&lt;br/&gt;ecosystem to environmental forcing; and 3) the relevant level of detail, including the resolution&lt;br/&gt;of physical space, that must be incorporated in nonlinear systems to capture the dynamics of a&lt;br/&gt;global ecosystem property (here, high productivity). The research will significantly advance our understanding of the interaction between physical and biogeochemical processes in an important area the world&apos;s oceans, and identify how these interactions regulate variability in marine ecosystem productivity.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">9.98122e+06</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">9.98122e+06</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n26" target="n51">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">Noise Management for Systems-on-a-Chip</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">4710</data>
      <data key="e_label">96176</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">96176</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n55" target="n56">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">SGER: Investigations into Micro-Scale Heat Transfer and CAD Tools for Thermal Management in Deep-Submicron Integrated Circuits</data>
      <data key="e_abstract">Power management as one of the major challenges in integrated circuit (IC) design that will be faced beyond 2006. This project is aimed at power management using micro-scale heat-removal devices that can be integrated with functional logic in dense, high-performance ICs. An evaluation of heat-removal microstructures such as an array of micro heat pipes is being carried out. An on-chip heat-removal architecture using micro heat pipes is being investigated, and computer-aided design tools for automatic insertion of heat spreaders are being developed. It is expected that this research will lead to a synthesis environment in which design primitives for heat removal will be integrated with functional primitives such as gates, latches, and standard cells.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">9.98695e+06</data>
      <data key="e_expirationDate">2000-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98695e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n55" target="n57">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">SGER: Investigations into Micro-Scale Heat Transfer and CAD Tools for Thermal Management in Deep-Submicron Integrated Circuits</data>
      <data key="e_abstract">Power management as one of the major challenges in integrated circuit (IC) design that will be faced beyond 2006. This project is aimed at power management using micro-scale heat-removal devices that can be integrated with functional logic in dense, high-performance ICs. An evaluation of heat-removal microstructures such as an array of micro heat pipes is being carried out. An on-chip heat-removal architecture using micro heat pipes is being investigated, and computer-aided design tools for automatic insertion of heat spreaders are being developed. It is expected that this research will lead to a synthesis environment in which design primitives for heat removal will be integrated with functional primitives such as gates, latches, and standard cells.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">9.98695e+06</data>
      <data key="e_expirationDate">2000-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98695e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n55" target="n58">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">SGER: Investigations into Micro-Scale Heat Transfer and CAD Tools for Thermal Management in Deep-Submicron Integrated Circuits</data>
      <data key="e_abstract">Power management as one of the major challenges in integrated circuit (IC) design that will be faced beyond 2006. This project is aimed at power management using micro-scale heat-removal devices that can be integrated with functional logic in dense, high-performance ICs. An evaluation of heat-removal microstructures such as an array of micro heat pipes is being carried out. An on-chip heat-removal architecture using micro heat pipes is being investigated, and computer-aided design tools for automatic insertion of heat spreaders are being developed. It is expected that this research will lead to a synthesis environment in which design primitives for heat removal will be integrated with functional primitives such as gates, latches, and standard cells.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">9.98695e+06</data>
      <data key="e_expirationDate">2000-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98695e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n56" target="n57">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">SGER: Investigations into Micro-Scale Heat Transfer and CAD Tools for Thermal Management in Deep-Submicron Integrated Circuits</data>
      <data key="e_abstract">Power management as one of the major challenges in integrated circuit (IC) design that will be faced beyond 2006. This project is aimed at power management using micro-scale heat-removal devices that can be integrated with functional logic in dense, high-performance ICs. An evaluation of heat-removal microstructures such as an array of micro heat pipes is being carried out. An on-chip heat-removal architecture using micro heat pipes is being investigated, and computer-aided design tools for automatic insertion of heat spreaders are being developed. It is expected that this research will lead to a synthesis environment in which design primitives for heat removal will be integrated with functional primitives such as gates, latches, and standard cells.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">9.98695e+06</data>
      <data key="e_expirationDate">2000-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98695e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n56" target="n58">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">SGER: Investigations into Micro-Scale Heat Transfer and CAD Tools for Thermal Management in Deep-Submicron Integrated Circuits</data>
      <data key="e_abstract">Power management as one of the major challenges in integrated circuit (IC) design that will be faced beyond 2006. This project is aimed at power management using micro-scale heat-removal devices that can be integrated with functional logic in dense, high-performance ICs. An evaluation of heat-removal microstructures such as an array of micro heat pipes is being carried out. An on-chip heat-removal architecture using micro heat pipes is being investigated, and computer-aided design tools for automatic insertion of heat spreaders are being developed. It is expected that this research will lead to a synthesis environment in which design primitives for heat removal will be integrated with functional primitives such as gates, latches, and standard cells.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">9.98695e+06</data>
      <data key="e_expirationDate">2000-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98695e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n57" target="n58">
      <data key="e_effectiveDate">2000-01-01</data>
      <data key="e_title">SGER: Investigations into Micro-Scale Heat Transfer and CAD Tools for Thermal Management in Deep-Submicron Integrated Circuits</data>
      <data key="e_abstract">Power management as one of the major challenges in integrated circuit (IC) design that will be faced beyond 2006. This project is aimed at power management using micro-scale heat-removal devices that can be integrated with functional logic in dense, high-performance ICs. An evaluation of heat-removal microstructures such as an array of micro heat pipes is being carried out. An on-chip heat-removal architecture using micro heat pipes is being investigated, and computer-aided design tools for automatic insertion of heat spreaders are being developed. It is expected that this research will lead to a synthesis environment in which design primitives for heat removal will be integrated with functional primitives such as gates, latches, and standard cells.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">9.98695e+06</data>
      <data key="e_expirationDate">2000-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98695e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n61" target="n62">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">Research Infrastructure: The Scalable Intracampus Research Grid for Computer Science Research</data>
      <data key="e_abstract">EIA-9972889&lt;br/&gt;Jack J. Dongarra&lt;br/&gt;Micah D. Beck&lt;br/&gt;Michael W. Berry&lt;br/&gt;Jens Gregor&lt;br/&gt;Michael A. Langston&lt;br/&gt;University of Tennessee&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: The Scalable Intracampus Research Grid for Computer Science Research&lt;br/&gt;&lt;br/&gt;The University of Tennessee (Knoxville) will build a computational grid at the University that will support their research in grid computing systems as well as advance applications that the grid will eventually support. The campus grid will consist of several grid service clusters - cycle servers linked in an advanced network. Grid research will address middleware to support performance based management that can seek out most appropriate software and other services, support distributed data management, process migration, load balancing, scheduling and fault-tolerance. Middleware will be based on their existing NetSolve project. Other systems supporting the grid include the Internet Backplane Protocol for fine-grained naming and distributed storage, checkpointing research, and the Network Weather Service for performance monitoring. Applications to be supported with the developing grid include medical imaging, materials design (such as lubricants), computational ecology, and machine design. Applications will benefit from the full capabilities of several clusters as well as provide challenging applications to push the grid programming interface and technology.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97289e+06</data>
      <data key="e_expirationDate">2006-01-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.97289e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n61" target="n63">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">Research Infrastructure: The Scalable Intracampus Research Grid for Computer Science Research</data>
      <data key="e_abstract">EIA-9972889&lt;br/&gt;Jack J. Dongarra&lt;br/&gt;Micah D. Beck&lt;br/&gt;Michael W. Berry&lt;br/&gt;Jens Gregor&lt;br/&gt;Michael A. Langston&lt;br/&gt;University of Tennessee&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: The Scalable Intracampus Research Grid for Computer Science Research&lt;br/&gt;&lt;br/&gt;The University of Tennessee (Knoxville) will build a computational grid at the University that will support their research in grid computing systems as well as advance applications that the grid will eventually support. The campus grid will consist of several grid service clusters - cycle servers linked in an advanced network. Grid research will address middleware to support performance based management that can seek out most appropriate software and other services, support distributed data management, process migration, load balancing, scheduling and fault-tolerance. Middleware will be based on their existing NetSolve project. Other systems supporting the grid include the Internet Backplane Protocol for fine-grained naming and distributed storage, checkpointing research, and the Network Weather Service for performance monitoring. Applications to be supported with the developing grid include medical imaging, materials design (such as lubricants), computational ecology, and machine design. Applications will benefit from the full capabilities of several clusters as well as provide challenging applications to push the grid programming interface and technology.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97289e+06</data>
      <data key="e_expirationDate">2006-01-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.97289e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n61" target="n64">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">Research Infrastructure: The Scalable Intracampus Research Grid for Computer Science Research</data>
      <data key="e_abstract">EIA-9972889&lt;br/&gt;Jack J. Dongarra&lt;br/&gt;Micah D. Beck&lt;br/&gt;Michael W. Berry&lt;br/&gt;Jens Gregor&lt;br/&gt;Michael A. Langston&lt;br/&gt;University of Tennessee&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: The Scalable Intracampus Research Grid for Computer Science Research&lt;br/&gt;&lt;br/&gt;The University of Tennessee (Knoxville) will build a computational grid at the University that will support their research in grid computing systems as well as advance applications that the grid will eventually support. The campus grid will consist of several grid service clusters - cycle servers linked in an advanced network. Grid research will address middleware to support performance based management that can seek out most appropriate software and other services, support distributed data management, process migration, load balancing, scheduling and fault-tolerance. Middleware will be based on their existing NetSolve project. Other systems supporting the grid include the Internet Backplane Protocol for fine-grained naming and distributed storage, checkpointing research, and the Network Weather Service for performance monitoring. Applications to be supported with the developing grid include medical imaging, materials design (such as lubricants), computational ecology, and machine design. Applications will benefit from the full capabilities of several clusters as well as provide challenging applications to push the grid programming interface and technology.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97289e+06</data>
      <data key="e_expirationDate">2006-01-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.97289e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n61" target="n65">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">Research Infrastructure: The Scalable Intracampus Research Grid for Computer Science Research</data>
      <data key="e_abstract">EIA-9972889&lt;br/&gt;Jack J. Dongarra&lt;br/&gt;Micah D. Beck&lt;br/&gt;Michael W. Berry&lt;br/&gt;Jens Gregor&lt;br/&gt;Michael A. Langston&lt;br/&gt;University of Tennessee&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: The Scalable Intracampus Research Grid for Computer Science Research&lt;br/&gt;&lt;br/&gt;The University of Tennessee (Knoxville) will build a computational grid at the University that will support their research in grid computing systems as well as advance applications that the grid will eventually support. The campus grid will consist of several grid service clusters - cycle servers linked in an advanced network. Grid research will address middleware to support performance based management that can seek out most appropriate software and other services, support distributed data management, process migration, load balancing, scheduling and fault-tolerance. Middleware will be based on their existing NetSolve project. Other systems supporting the grid include the Internet Backplane Protocol for fine-grained naming and distributed storage, checkpointing research, and the Network Weather Service for performance monitoring. Applications to be supported with the developing grid include medical imaging, materials design (such as lubricants), computational ecology, and machine design. Applications will benefit from the full capabilities of several clusters as well as provide challenging applications to push the grid programming interface and technology.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97289e+06</data>
      <data key="e_expirationDate">2006-01-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.97289e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n62" target="n63">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">Research Infrastructure: The Scalable Intracampus Research Grid for Computer Science Research</data>
      <data key="e_abstract">EIA-9972889&lt;br/&gt;Jack J. Dongarra&lt;br/&gt;Micah D. Beck&lt;br/&gt;Michael W. Berry&lt;br/&gt;Jens Gregor&lt;br/&gt;Michael A. Langston&lt;br/&gt;University of Tennessee&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: The Scalable Intracampus Research Grid for Computer Science Research&lt;br/&gt;&lt;br/&gt;The University of Tennessee (Knoxville) will build a computational grid at the University that will support their research in grid computing systems as well as advance applications that the grid will eventually support. The campus grid will consist of several grid service clusters - cycle servers linked in an advanced network. Grid research will address middleware to support performance based management that can seek out most appropriate software and other services, support distributed data management, process migration, load balancing, scheduling and fault-tolerance. Middleware will be based on their existing NetSolve project. Other systems supporting the grid include the Internet Backplane Protocol for fine-grained naming and distributed storage, checkpointing research, and the Network Weather Service for performance monitoring. Applications to be supported with the developing grid include medical imaging, materials design (such as lubricants), computational ecology, and machine design. Applications will benefit from the full capabilities of several clusters as well as provide challenging applications to push the grid programming interface and technology.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97289e+06</data>
      <data key="e_expirationDate">2006-01-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.97289e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n62" target="n64">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">Research Infrastructure: The Scalable Intracampus Research Grid for Computer Science Research</data>
      <data key="e_abstract">EIA-9972889&lt;br/&gt;Jack J. Dongarra&lt;br/&gt;Micah D. Beck&lt;br/&gt;Michael W. Berry&lt;br/&gt;Jens Gregor&lt;br/&gt;Michael A. Langston&lt;br/&gt;University of Tennessee&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: The Scalable Intracampus Research Grid for Computer Science Research&lt;br/&gt;&lt;br/&gt;The University of Tennessee (Knoxville) will build a computational grid at the University that will support their research in grid computing systems as well as advance applications that the grid will eventually support. The campus grid will consist of several grid service clusters - cycle servers linked in an advanced network. Grid research will address middleware to support performance based management that can seek out most appropriate software and other services, support distributed data management, process migration, load balancing, scheduling and fault-tolerance. Middleware will be based on their existing NetSolve project. Other systems supporting the grid include the Internet Backplane Protocol for fine-grained naming and distributed storage, checkpointing research, and the Network Weather Service for performance monitoring. Applications to be supported with the developing grid include medical imaging, materials design (such as lubricants), computational ecology, and machine design. Applications will benefit from the full capabilities of several clusters as well as provide challenging applications to push the grid programming interface and technology.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97289e+06</data>
      <data key="e_expirationDate">2006-01-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.97289e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n62" target="n65">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">Research Infrastructure: The Scalable Intracampus Research Grid for Computer Science Research</data>
      <data key="e_abstract">EIA-9972889&lt;br/&gt;Jack J. Dongarra&lt;br/&gt;Micah D. Beck&lt;br/&gt;Michael W. Berry&lt;br/&gt;Jens Gregor&lt;br/&gt;Michael A. Langston&lt;br/&gt;University of Tennessee&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: The Scalable Intracampus Research Grid for Computer Science Research&lt;br/&gt;&lt;br/&gt;The University of Tennessee (Knoxville) will build a computational grid at the University that will support their research in grid computing systems as well as advance applications that the grid will eventually support. The campus grid will consist of several grid service clusters - cycle servers linked in an advanced network. Grid research will address middleware to support performance based management that can seek out most appropriate software and other services, support distributed data management, process migration, load balancing, scheduling and fault-tolerance. Middleware will be based on their existing NetSolve project. Other systems supporting the grid include the Internet Backplane Protocol for fine-grained naming and distributed storage, checkpointing research, and the Network Weather Service for performance monitoring. Applications to be supported with the developing grid include medical imaging, materials design (such as lubricants), computational ecology, and machine design. Applications will benefit from the full capabilities of several clusters as well as provide challenging applications to push the grid programming interface and technology.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97289e+06</data>
      <data key="e_expirationDate">2006-01-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.97289e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n63" target="n64">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">Research Infrastructure: The Scalable Intracampus Research Grid for Computer Science Research</data>
      <data key="e_abstract">EIA-9972889&lt;br/&gt;Jack J. Dongarra&lt;br/&gt;Micah D. Beck&lt;br/&gt;Michael W. Berry&lt;br/&gt;Jens Gregor&lt;br/&gt;Michael A. Langston&lt;br/&gt;University of Tennessee&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: The Scalable Intracampus Research Grid for Computer Science Research&lt;br/&gt;&lt;br/&gt;The University of Tennessee (Knoxville) will build a computational grid at the University that will support their research in grid computing systems as well as advance applications that the grid will eventually support. The campus grid will consist of several grid service clusters - cycle servers linked in an advanced network. Grid research will address middleware to support performance based management that can seek out most appropriate software and other services, support distributed data management, process migration, load balancing, scheduling and fault-tolerance. Middleware will be based on their existing NetSolve project. Other systems supporting the grid include the Internet Backplane Protocol for fine-grained naming and distributed storage, checkpointing research, and the Network Weather Service for performance monitoring. Applications to be supported with the developing grid include medical imaging, materials design (such as lubricants), computational ecology, and machine design. Applications will benefit from the full capabilities of several clusters as well as provide challenging applications to push the grid programming interface and technology.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97289e+06</data>
      <data key="e_expirationDate">2006-01-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.97289e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n63" target="n65">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">Research Infrastructure: The Scalable Intracampus Research Grid for Computer Science Research</data>
      <data key="e_abstract">EIA-9972889&lt;br/&gt;Jack J. Dongarra&lt;br/&gt;Micah D. Beck&lt;br/&gt;Michael W. Berry&lt;br/&gt;Jens Gregor&lt;br/&gt;Michael A. Langston&lt;br/&gt;University of Tennessee&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: The Scalable Intracampus Research Grid for Computer Science Research&lt;br/&gt;&lt;br/&gt;The University of Tennessee (Knoxville) will build a computational grid at the University that will support their research in grid computing systems as well as advance applications that the grid will eventually support. The campus grid will consist of several grid service clusters - cycle servers linked in an advanced network. Grid research will address middleware to support performance based management that can seek out most appropriate software and other services, support distributed data management, process migration, load balancing, scheduling and fault-tolerance. Middleware will be based on their existing NetSolve project. Other systems supporting the grid include the Internet Backplane Protocol for fine-grained naming and distributed storage, checkpointing research, and the Network Weather Service for performance monitoring. Applications to be supported with the developing grid include medical imaging, materials design (such as lubricants), computational ecology, and machine design. Applications will benefit from the full capabilities of several clusters as well as provide challenging applications to push the grid programming interface and technology.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97289e+06</data>
      <data key="e_expirationDate">2006-01-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.97289e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n64" target="n65">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">Research Infrastructure: The Scalable Intracampus Research Grid for Computer Science Research</data>
      <data key="e_abstract">EIA-9972889&lt;br/&gt;Jack J. Dongarra&lt;br/&gt;Micah D. Beck&lt;br/&gt;Michael W. Berry&lt;br/&gt;Jens Gregor&lt;br/&gt;Michael A. Langston&lt;br/&gt;University of Tennessee&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: The Scalable Intracampus Research Grid for Computer Science Research&lt;br/&gt;&lt;br/&gt;The University of Tennessee (Knoxville) will build a computational grid at the University that will support their research in grid computing systems as well as advance applications that the grid will eventually support. The campus grid will consist of several grid service clusters - cycle servers linked in an advanced network. Grid research will address middleware to support performance based management that can seek out most appropriate software and other services, support distributed data management, process migration, load balancing, scheduling and fault-tolerance. Middleware will be based on their existing NetSolve project. Other systems supporting the grid include the Internet Backplane Protocol for fine-grained naming and distributed storage, checkpointing research, and the Network Weather Service for performance monitoring. Applications to be supported with the developing grid include medical imaging, materials design (such as lubricants), computational ecology, and machine design. Applications will benefit from the full capabilities of several clusters as well as provide challenging applications to push the grid programming interface and technology.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97289e+06</data>
      <data key="e_expirationDate">2006-01-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.97289e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n68" target="n69">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">CISE Research Instrumentation: Cluster Computing for Knowledge Discovery in Diverse Data Sets</data>
      <data key="e_abstract">EIA-9986042&lt;br/&gt;George Karypis&lt;br/&gt;University of Minnesota&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Cluster Computing for Knowledge Discovery in Diverse Data Sets&lt;br/&gt;&lt;br/&gt;The Department of Computer Science &amp; Engineering at the University of Minnesota will purchase the following equipment which will be dedicated to support research in computer and information science and engineering. The requested instrumentation consists of 10 dual processor Dell 410 workstations with a total of 10GB of main memory, a CISCO Catalyst 4000 Gigabit switch, and a Hitachi 5750 E RAID system with a total of 360GB storage. This equipment will be used to form a computer cluster suited for data-and compute-intensive applications.&lt;br/&gt;&lt;br/&gt;The equipment will be primarily used by four research projects which address research problems in the area of knowledge discovery, including in particular: data mining of genomic and scientific data sets, spatial data mining, next-generation recommender systems, and bid evaluation in multi-agent contracting. For several of these projects, the currently available equipment are either inadequate or obsolete for supporting their core needs. For other projects, the availability of this equipment will make a significant increment to their capabilities. The collective needs of these projects are for a tightly-coupled, high-performance, large-memory cluster of computers with a high-performance high-capacity storage server attached to it.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98604e+06</data>
      <data key="e_expirationDate">2003-01-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98604e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n68" target="n70">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">CISE Research Instrumentation: Cluster Computing for Knowledge Discovery in Diverse Data Sets</data>
      <data key="e_abstract">EIA-9986042&lt;br/&gt;George Karypis&lt;br/&gt;University of Minnesota&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Cluster Computing for Knowledge Discovery in Diverse Data Sets&lt;br/&gt;&lt;br/&gt;The Department of Computer Science &amp; Engineering at the University of Minnesota will purchase the following equipment which will be dedicated to support research in computer and information science and engineering. The requested instrumentation consists of 10 dual processor Dell 410 workstations with a total of 10GB of main memory, a CISCO Catalyst 4000 Gigabit switch, and a Hitachi 5750 E RAID system with a total of 360GB storage. This equipment will be used to form a computer cluster suited for data-and compute-intensive applications.&lt;br/&gt;&lt;br/&gt;The equipment will be primarily used by four research projects which address research problems in the area of knowledge discovery, including in particular: data mining of genomic and scientific data sets, spatial data mining, next-generation recommender systems, and bid evaluation in multi-agent contracting. For several of these projects, the currently available equipment are either inadequate or obsolete for supporting their core needs. For other projects, the availability of this equipment will make a significant increment to their capabilities. The collective needs of these projects are for a tightly-coupled, high-performance, large-memory cluster of computers with a high-performance high-capacity storage server attached to it.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98604e+06</data>
      <data key="e_expirationDate">2003-01-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98604e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n68" target="n71">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">CISE Research Instrumentation: Cluster Computing for Knowledge Discovery in Diverse Data Sets</data>
      <data key="e_abstract">EIA-9986042&lt;br/&gt;George Karypis&lt;br/&gt;University of Minnesota&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Cluster Computing for Knowledge Discovery in Diverse Data Sets&lt;br/&gt;&lt;br/&gt;The Department of Computer Science &amp; Engineering at the University of Minnesota will purchase the following equipment which will be dedicated to support research in computer and information science and engineering. The requested instrumentation consists of 10 dual processor Dell 410 workstations with a total of 10GB of main memory, a CISCO Catalyst 4000 Gigabit switch, and a Hitachi 5750 E RAID system with a total of 360GB storage. This equipment will be used to form a computer cluster suited for data-and compute-intensive applications.&lt;br/&gt;&lt;br/&gt;The equipment will be primarily used by four research projects which address research problems in the area of knowledge discovery, including in particular: data mining of genomic and scientific data sets, spatial data mining, next-generation recommender systems, and bid evaluation in multi-agent contracting. For several of these projects, the currently available equipment are either inadequate or obsolete for supporting their core needs. For other projects, the availability of this equipment will make a significant increment to their capabilities. The collective needs of these projects are for a tightly-coupled, high-performance, large-memory cluster of computers with a high-performance high-capacity storage server attached to it.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98604e+06</data>
      <data key="e_expirationDate">2003-01-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98604e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n68" target="n72">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">CISE Research Instrumentation: Cluster Computing for Knowledge Discovery in Diverse Data Sets</data>
      <data key="e_abstract">EIA-9986042&lt;br/&gt;George Karypis&lt;br/&gt;University of Minnesota&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Cluster Computing for Knowledge Discovery in Diverse Data Sets&lt;br/&gt;&lt;br/&gt;The Department of Computer Science &amp; Engineering at the University of Minnesota will purchase the following equipment which will be dedicated to support research in computer and information science and engineering. The requested instrumentation consists of 10 dual processor Dell 410 workstations with a total of 10GB of main memory, a CISCO Catalyst 4000 Gigabit switch, and a Hitachi 5750 E RAID system with a total of 360GB storage. This equipment will be used to form a computer cluster suited for data-and compute-intensive applications.&lt;br/&gt;&lt;br/&gt;The equipment will be primarily used by four research projects which address research problems in the area of knowledge discovery, including in particular: data mining of genomic and scientific data sets, spatial data mining, next-generation recommender systems, and bid evaluation in multi-agent contracting. For several of these projects, the currently available equipment are either inadequate or obsolete for supporting their core needs. For other projects, the availability of this equipment will make a significant increment to their capabilities. The collective needs of these projects are for a tightly-coupled, high-performance, large-memory cluster of computers with a high-performance high-capacity storage server attached to it.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98604e+06</data>
      <data key="e_expirationDate">2003-01-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98604e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n69" target="n70">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">CISE Research Instrumentation: Cluster Computing for Knowledge Discovery in Diverse Data Sets</data>
      <data key="e_abstract">EIA-9986042&lt;br/&gt;George Karypis&lt;br/&gt;University of Minnesota&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Cluster Computing for Knowledge Discovery in Diverse Data Sets&lt;br/&gt;&lt;br/&gt;The Department of Computer Science &amp; Engineering at the University of Minnesota will purchase the following equipment which will be dedicated to support research in computer and information science and engineering. The requested instrumentation consists of 10 dual processor Dell 410 workstations with a total of 10GB of main memory, a CISCO Catalyst 4000 Gigabit switch, and a Hitachi 5750 E RAID system with a total of 360GB storage. This equipment will be used to form a computer cluster suited for data-and compute-intensive applications.&lt;br/&gt;&lt;br/&gt;The equipment will be primarily used by four research projects which address research problems in the area of knowledge discovery, including in particular: data mining of genomic and scientific data sets, spatial data mining, next-generation recommender systems, and bid evaluation in multi-agent contracting. For several of these projects, the currently available equipment are either inadequate or obsolete for supporting their core needs. For other projects, the availability of this equipment will make a significant increment to their capabilities. The collective needs of these projects are for a tightly-coupled, high-performance, large-memory cluster of computers with a high-performance high-capacity storage server attached to it.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98604e+06</data>
      <data key="e_expirationDate">2003-01-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98604e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n69" target="n71">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">CISE Research Instrumentation: Cluster Computing for Knowledge Discovery in Diverse Data Sets</data>
      <data key="e_abstract">EIA-9986042&lt;br/&gt;George Karypis&lt;br/&gt;University of Minnesota&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Cluster Computing for Knowledge Discovery in Diverse Data Sets&lt;br/&gt;&lt;br/&gt;The Department of Computer Science &amp; Engineering at the University of Minnesota will purchase the following equipment which will be dedicated to support research in computer and information science and engineering. The requested instrumentation consists of 10 dual processor Dell 410 workstations with a total of 10GB of main memory, a CISCO Catalyst 4000 Gigabit switch, and a Hitachi 5750 E RAID system with a total of 360GB storage. This equipment will be used to form a computer cluster suited for data-and compute-intensive applications.&lt;br/&gt;&lt;br/&gt;The equipment will be primarily used by four research projects which address research problems in the area of knowledge discovery, including in particular: data mining of genomic and scientific data sets, spatial data mining, next-generation recommender systems, and bid evaluation in multi-agent contracting. For several of these projects, the currently available equipment are either inadequate or obsolete for supporting their core needs. For other projects, the availability of this equipment will make a significant increment to their capabilities. The collective needs of these projects are for a tightly-coupled, high-performance, large-memory cluster of computers with a high-performance high-capacity storage server attached to it.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98604e+06</data>
      <data key="e_expirationDate">2003-01-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98604e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n69" target="n72">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">CISE Research Instrumentation: Cluster Computing for Knowledge Discovery in Diverse Data Sets</data>
      <data key="e_abstract">EIA-9986042&lt;br/&gt;George Karypis&lt;br/&gt;University of Minnesota&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Cluster Computing for Knowledge Discovery in Diverse Data Sets&lt;br/&gt;&lt;br/&gt;The Department of Computer Science &amp; Engineering at the University of Minnesota will purchase the following equipment which will be dedicated to support research in computer and information science and engineering. The requested instrumentation consists of 10 dual processor Dell 410 workstations with a total of 10GB of main memory, a CISCO Catalyst 4000 Gigabit switch, and a Hitachi 5750 E RAID system with a total of 360GB storage. This equipment will be used to form a computer cluster suited for data-and compute-intensive applications.&lt;br/&gt;&lt;br/&gt;The equipment will be primarily used by four research projects which address research problems in the area of knowledge discovery, including in particular: data mining of genomic and scientific data sets, spatial data mining, next-generation recommender systems, and bid evaluation in multi-agent contracting. For several of these projects, the currently available equipment are either inadequate or obsolete for supporting their core needs. For other projects, the availability of this equipment will make a significant increment to their capabilities. The collective needs of these projects are for a tightly-coupled, high-performance, large-memory cluster of computers with a high-performance high-capacity storage server attached to it.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98604e+06</data>
      <data key="e_expirationDate">2003-01-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98604e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n70" target="n71">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">CISE Research Instrumentation: Cluster Computing for Knowledge Discovery in Diverse Data Sets</data>
      <data key="e_abstract">EIA-9986042&lt;br/&gt;George Karypis&lt;br/&gt;University of Minnesota&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Cluster Computing for Knowledge Discovery in Diverse Data Sets&lt;br/&gt;&lt;br/&gt;The Department of Computer Science &amp; Engineering at the University of Minnesota will purchase the following equipment which will be dedicated to support research in computer and information science and engineering. The requested instrumentation consists of 10 dual processor Dell 410 workstations with a total of 10GB of main memory, a CISCO Catalyst 4000 Gigabit switch, and a Hitachi 5750 E RAID system with a total of 360GB storage. This equipment will be used to form a computer cluster suited for data-and compute-intensive applications.&lt;br/&gt;&lt;br/&gt;The equipment will be primarily used by four research projects which address research problems in the area of knowledge discovery, including in particular: data mining of genomic and scientific data sets, spatial data mining, next-generation recommender systems, and bid evaluation in multi-agent contracting. For several of these projects, the currently available equipment are either inadequate or obsolete for supporting their core needs. For other projects, the availability of this equipment will make a significant increment to their capabilities. The collective needs of these projects are for a tightly-coupled, high-performance, large-memory cluster of computers with a high-performance high-capacity storage server attached to it.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98604e+06</data>
      <data key="e_expirationDate">2003-01-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98604e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n70" target="n72">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">CISE Research Instrumentation: Cluster Computing for Knowledge Discovery in Diverse Data Sets</data>
      <data key="e_abstract">EIA-9986042&lt;br/&gt;George Karypis&lt;br/&gt;University of Minnesota&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Cluster Computing for Knowledge Discovery in Diverse Data Sets&lt;br/&gt;&lt;br/&gt;The Department of Computer Science &amp; Engineering at the University of Minnesota will purchase the following equipment which will be dedicated to support research in computer and information science and engineering. The requested instrumentation consists of 10 dual processor Dell 410 workstations with a total of 10GB of main memory, a CISCO Catalyst 4000 Gigabit switch, and a Hitachi 5750 E RAID system with a total of 360GB storage. This equipment will be used to form a computer cluster suited for data-and compute-intensive applications.&lt;br/&gt;&lt;br/&gt;The equipment will be primarily used by four research projects which address research problems in the area of knowledge discovery, including in particular: data mining of genomic and scientific data sets, spatial data mining, next-generation recommender systems, and bid evaluation in multi-agent contracting. For several of these projects, the currently available equipment are either inadequate or obsolete for supporting their core needs. For other projects, the availability of this equipment will make a significant increment to their capabilities. The collective needs of these projects are for a tightly-coupled, high-performance, large-memory cluster of computers with a high-performance high-capacity storage server attached to it.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98604e+06</data>
      <data key="e_expirationDate">2003-01-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98604e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n71" target="n72">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">CISE Research Instrumentation: Cluster Computing for Knowledge Discovery in Diverse Data Sets</data>
      <data key="e_abstract">EIA-9986042&lt;br/&gt;George Karypis&lt;br/&gt;University of Minnesota&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Cluster Computing for Knowledge Discovery in Diverse Data Sets&lt;br/&gt;&lt;br/&gt;The Department of Computer Science &amp; Engineering at the University of Minnesota will purchase the following equipment which will be dedicated to support research in computer and information science and engineering. The requested instrumentation consists of 10 dual processor Dell 410 workstations with a total of 10GB of main memory, a CISCO Catalyst 4000 Gigabit switch, and a Hitachi 5750 E RAID system with a total of 360GB storage. This equipment will be used to form a computer cluster suited for data-and compute-intensive applications.&lt;br/&gt;&lt;br/&gt;The equipment will be primarily used by four research projects which address research problems in the area of knowledge discovery, including in particular: data mining of genomic and scientific data sets, spatial data mining, next-generation recommender systems, and bid evaluation in multi-agent contracting. For several of these projects, the currently available equipment are either inadequate or obsolete for supporting their core needs. For other projects, the availability of this equipment will make a significant increment to their capabilities. The collective needs of these projects are for a tightly-coupled, high-performance, large-memory cluster of computers with a high-performance high-capacity storage server attached to it.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98604e+06</data>
      <data key="e_expirationDate">2003-01-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98604e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n75" target="n76">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">Interactive Flow Visualization Using Haptics</data>
      <data key="e_abstract">This project will extend global visual display techniques with local visualization&lt;br/&gt;methods, which combine graphics with haptic (force feedback) displays in three areas:&lt;br/&gt;&lt;br/&gt;1. Allow users to apply well-known local techniques (e.g. streamlines or&lt;br/&gt; releasing colored dyes) within a globally visualized display (created by line integral&lt;br/&gt; convolution (LIC), volume rendering, or feature extraction).&lt;br/&gt;2. Develop haptic interfaces to complement the local visual methods for&lt;br/&gt; representing vector fields. These interfaces will be tested and compared with each other and with visual methods.&lt;br/&gt;3. Integrate haptic and visual algorithms into a computational steering environment. This will allow for fast feedback upon changing visualization parameters, such as the location of a 3D, stream line advection widget or the method of haptic interaction.&lt;br/&gt;&lt;br/&gt;The project will use a co-registered, semi-immersive environment consisting of a Fakespace Immersive Workbench and a large SenseAble Phantom 3.0 haptic device; other haptic devices may be added later. By co-registering haptics in the semi-immersive workspace, the user will have the ability to both see and feel the field under study.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">9.97806e+06</data>
      <data key="e_expirationDate">2004-01-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.97806e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n75" target="n77">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">Interactive Flow Visualization Using Haptics</data>
      <data key="e_abstract">This project will extend global visual display techniques with local visualization&lt;br/&gt;methods, which combine graphics with haptic (force feedback) displays in three areas:&lt;br/&gt;&lt;br/&gt;1. Allow users to apply well-known local techniques (e.g. streamlines or&lt;br/&gt; releasing colored dyes) within a globally visualized display (created by line integral&lt;br/&gt; convolution (LIC), volume rendering, or feature extraction).&lt;br/&gt;2. Develop haptic interfaces to complement the local visual methods for&lt;br/&gt; representing vector fields. These interfaces will be tested and compared with each other and with visual methods.&lt;br/&gt;3. Integrate haptic and visual algorithms into a computational steering environment. This will allow for fast feedback upon changing visualization parameters, such as the location of a 3D, stream line advection widget or the method of haptic interaction.&lt;br/&gt;&lt;br/&gt;The project will use a co-registered, semi-immersive environment consisting of a Fakespace Immersive Workbench and a large SenseAble Phantom 3.0 haptic device; other haptic devices may be added later. By co-registering haptics in the semi-immersive workspace, the user will have the ability to both see and feel the field under study.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">9.97806e+06</data>
      <data key="e_expirationDate">2004-01-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.97806e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n76" target="n77">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">Interactive Flow Visualization Using Haptics</data>
      <data key="e_abstract">This project will extend global visual display techniques with local visualization&lt;br/&gt;methods, which combine graphics with haptic (force feedback) displays in three areas:&lt;br/&gt;&lt;br/&gt;1. Allow users to apply well-known local techniques (e.g. streamlines or&lt;br/&gt; releasing colored dyes) within a globally visualized display (created by line integral&lt;br/&gt; convolution (LIC), volume rendering, or feature extraction).&lt;br/&gt;2. Develop haptic interfaces to complement the local visual methods for&lt;br/&gt; representing vector fields. These interfaces will be tested and compared with each other and with visual methods.&lt;br/&gt;3. Integrate haptic and visual algorithms into a computational steering environment. This will allow for fast feedback upon changing visualization parameters, such as the location of a 3D, stream line advection widget or the method of haptic interaction.&lt;br/&gt;&lt;br/&gt;The project will use a co-registered, semi-immersive environment consisting of a Fakespace Immersive Workbench and a large SenseAble Phantom 3.0 haptic device; other haptic devices may be added later. By co-registering haptics in the semi-immersive workspace, the user will have the ability to both see and feel the field under study.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">9.97806e+06</data>
      <data key="e_expirationDate">2004-01-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.97806e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n79" target="n80">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">High Performance Analog-to-Digital Interfaces in CMOS Technologies</data>
      <data key="e_abstract">Many signal-processing systems operate at least partly in the digital domain on signals that originate in analog form. In implementing the required digital-signal-processing (DSP) circuits for such systems, CMOS technologies have become dominant because of their high densities and low power dissipations. To reduce system cost and increases portability, both increased levels of integration and reduced power dissipations are required, forcing the associated analog interface circuits to use CMOS compatible technologies. When implemented in CMOS technologies, however, such interface circuitry often limits the performance of the overall signal-processing systems.&lt;br/&gt;&lt;br/&gt;Therefore, the theme of this research is to study techniques that can overcome the accuracy and speed limitations in CMOS analog interface circuits. In particular, this research will focus on techniques to overcome the mismatches in high-speed, parallel, time-interleaved analog-to-digital converters and to improve linearity of analog-to-digital converters. Specific objectives of the proposed research are:&lt;br/&gt;1. to develop background-calibration techniques that can overcome the accuracy, linearity and/or conversion-rate limitations of analog interface circuits and&lt;br/&gt;2. to demonstrate these techniques by fabricating prototypes.</data>
      <data key="e_pgm">4720</data>
      <data key="e_label">9.90192e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.90192e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n85" target="n86">
      <data key="e_effectiveDate">2000-02-01</data>
      <data key="e_title">CISE Research Instrumentation: Prototype development of the Simultaneous Optical Multiprocessor Exchange Bus computer</data>
      <data key="e_abstract">EIA-9985971&lt;br/&gt;Constantine Katsinis&lt;br/&gt;Drexel University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Prototype Development of the Simultaneous Optical Multiprocessor Exchange Bus Computer&lt;br/&gt;&lt;br/&gt;The Department of Electrical and Computer Engineering at Drexel University will purchase digital design and test equipment, optical design and test equipment processor boards, programmable logic design equipment, software, and personal computers to support programming, designing, testing and debugging activities. The focus of this research is on the Simultaneous Optical Multiprocessor Exchange Bus (SOME-Bus) computer network architecture. Work on the SOME-Bus involves multidisciplinary research in computer architecture and networks, optoelectronic devices, fault tolerance, performance analysis (mathematical modeling and simulation), and VLSI design. Its purpose is to create an optoelectronic prototype of the SOME-Bus, a research platform that will demonstrate the enhanced performance necessary to support current and future data processing and scientific processing requirements. The equipment will be used for several research projects, in particular:&lt;br/&gt;&lt;br/&gt;1. Prototype development of the SOME-Bus computer network architecture.&lt;br/&gt;2. High-sensitivity high-bandwidth detectors based on heterodimensional contacts.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98597e+06</data>
      <data key="e_expirationDate">2004-01-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98597e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n88" target="n89">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Infrastructure for Cluster and Parallel Systems Research</data>
      <data key="e_abstract">EIA-9986020&lt;br/&gt;Rudolf Eigenmann&lt;br/&gt;Purdue University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Infrastructure for Cluster and Parallel Systems Research&lt;br/&gt;&lt;br/&gt;The School of Electrical and Computer Engineering at Purdue University will acquire four Sun Ultra Enterprise 450 quad-processor servers plus Myrinet interconnections for research in computer and information science and engineering. The equipment will be used for several research projects, including in particular:&lt;br/&gt; &lt;br/&gt; Speculative Distributed Shared Memory&lt;br/&gt;&lt;br/&gt; Latency-Reducing Compiler Techniques for Distributed Shared Memory,&lt;br/&gt;&lt;br/&gt; Studies of large-scale Applications for Symmetric Multiprocessor Clusters&lt;br/&gt;&lt;br/&gt; Multiplex: Future Billion-Transistor Microarchitectures&lt;br/&gt;&lt;br/&gt;The new equipment will provide a testbed platform for fine-grain software distributed shared memory (DSM) to implement and evaluate speculative coherent DSM architectures and serve as a target for a compiler. In addition, the cluster and individual servers will also be used in single-user mode to perform unperturbed measurements of our large-scale applications and will allow single-user access to individual SMPs without inhibiting long-running multi-user applications.&lt;br/&gt;&lt;br/&gt;The servers will provide the required memory and processing power as well as memory bandwidth to perform the memory-and computer-intensive simulations of the Multiplex architecture. The new equipment will deliver high simulation throughput rates by running many simulations concurrently. &lt;br/&gt;&lt;br/&gt;The cluster and individual servers will provide the high memory bandwidth, large aggregate memory and processing power required to run detailed, large-scale parallel simulations for the speculative DSM research .</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98602e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98602e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n88" target="n90">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Infrastructure for Cluster and Parallel Systems Research</data>
      <data key="e_abstract">EIA-9986020&lt;br/&gt;Rudolf Eigenmann&lt;br/&gt;Purdue University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Infrastructure for Cluster and Parallel Systems Research&lt;br/&gt;&lt;br/&gt;The School of Electrical and Computer Engineering at Purdue University will acquire four Sun Ultra Enterprise 450 quad-processor servers plus Myrinet interconnections for research in computer and information science and engineering. The equipment will be used for several research projects, including in particular:&lt;br/&gt; &lt;br/&gt; Speculative Distributed Shared Memory&lt;br/&gt;&lt;br/&gt; Latency-Reducing Compiler Techniques for Distributed Shared Memory,&lt;br/&gt;&lt;br/&gt; Studies of large-scale Applications for Symmetric Multiprocessor Clusters&lt;br/&gt;&lt;br/&gt; Multiplex: Future Billion-Transistor Microarchitectures&lt;br/&gt;&lt;br/&gt;The new equipment will provide a testbed platform for fine-grain software distributed shared memory (DSM) to implement and evaluate speculative coherent DSM architectures and serve as a target for a compiler. In addition, the cluster and individual servers will also be used in single-user mode to perform unperturbed measurements of our large-scale applications and will allow single-user access to individual SMPs without inhibiting long-running multi-user applications.&lt;br/&gt;&lt;br/&gt;The servers will provide the required memory and processing power as well as memory bandwidth to perform the memory-and computer-intensive simulations of the Multiplex architecture. The new equipment will deliver high simulation throughput rates by running many simulations concurrently. &lt;br/&gt;&lt;br/&gt;The cluster and individual servers will provide the high memory bandwidth, large aggregate memory and processing power required to run detailed, large-scale parallel simulations for the speculative DSM research .</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98602e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98602e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n89" target="n90">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Infrastructure for Cluster and Parallel Systems Research</data>
      <data key="e_abstract">EIA-9986020&lt;br/&gt;Rudolf Eigenmann&lt;br/&gt;Purdue University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Infrastructure for Cluster and Parallel Systems Research&lt;br/&gt;&lt;br/&gt;The School of Electrical and Computer Engineering at Purdue University will acquire four Sun Ultra Enterprise 450 quad-processor servers plus Myrinet interconnections for research in computer and information science and engineering. The equipment will be used for several research projects, including in particular:&lt;br/&gt; &lt;br/&gt; Speculative Distributed Shared Memory&lt;br/&gt;&lt;br/&gt; Latency-Reducing Compiler Techniques for Distributed Shared Memory,&lt;br/&gt;&lt;br/&gt; Studies of large-scale Applications for Symmetric Multiprocessor Clusters&lt;br/&gt;&lt;br/&gt; Multiplex: Future Billion-Transistor Microarchitectures&lt;br/&gt;&lt;br/&gt;The new equipment will provide a testbed platform for fine-grain software distributed shared memory (DSM) to implement and evaluate speculative coherent DSM architectures and serve as a target for a compiler. In addition, the cluster and individual servers will also be used in single-user mode to perform unperturbed measurements of our large-scale applications and will allow single-user access to individual SMPs without inhibiting long-running multi-user applications.&lt;br/&gt;&lt;br/&gt;The servers will provide the required memory and processing power as well as memory bandwidth to perform the memory-and computer-intensive simulations of the Multiplex architecture. The new equipment will deliver high simulation throughput rates by running many simulations concurrently. &lt;br/&gt;&lt;br/&gt;The cluster and individual servers will provide the high memory bandwidth, large aggregate memory and processing power required to run detailed, large-scale parallel simulations for the speculative DSM research .</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98602e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98602e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n91" target="n92">
      <data key="e_effectiveDate">2000-03-01</data>
      <data key="e_title">CISE Research Instrumentation: Augmented Reality and Interactive Distributed Physical Modeling</data>
      <data key="e_abstract">EIA-9986051&lt;br/&gt;Jannick P. Rolland&lt;br/&gt;University of Central Florida&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Augmented Reality and Interactive Distributed Physical Modeling&lt;br/&gt;&lt;br/&gt;The schools of Optics and Computer Science at the University of Central Florida will purchase Silicon Graphics computers that will be dedicated to support research in computer and information science and engineering. The equipment will be used for several projects, including in particular:&lt;br/&gt;&lt;br/&gt;1. Augmented Reality&lt;br/&gt;Build a framework for augmented reality visualization. The framework includes the development of 3D displays, including head-mounted devices; quantitative assessment methods for accuracy and precision of 3D rendered depth; physically based models of motion of virtual objects; registration methods for real and virtual objects; and statistical texture synthesis.&lt;br/&gt;&lt;br/&gt;2. A Framework for Distributed Physical Modeling at Interactive Speed&lt;br/&gt;Extend the framework developed previously for distributed physical modeling to study the effective utilization of non-homogeneous machine clusters and to provide data at interactive-speed to augmented and virtual reality systems.&lt;br/&gt;&lt;br/&gt;3. Dynamic Environmental Features in Virtual Worlds &lt;br/&gt;Develop algorithms for the dynamic generation of vegetation in virtual worlds. This will require the design of new level of detail management algorithms and lightweight communication protocols to maintain functionally.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98605e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98605e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n91" target="n93">
      <data key="e_effectiveDate">2000-03-01</data>
      <data key="e_title">CISE Research Instrumentation: Augmented Reality and Interactive Distributed Physical Modeling</data>
      <data key="e_abstract">EIA-9986051&lt;br/&gt;Jannick P. Rolland&lt;br/&gt;University of Central Florida&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Augmented Reality and Interactive Distributed Physical Modeling&lt;br/&gt;&lt;br/&gt;The schools of Optics and Computer Science at the University of Central Florida will purchase Silicon Graphics computers that will be dedicated to support research in computer and information science and engineering. The equipment will be used for several projects, including in particular:&lt;br/&gt;&lt;br/&gt;1. Augmented Reality&lt;br/&gt;Build a framework for augmented reality visualization. The framework includes the development of 3D displays, including head-mounted devices; quantitative assessment methods for accuracy and precision of 3D rendered depth; physically based models of motion of virtual objects; registration methods for real and virtual objects; and statistical texture synthesis.&lt;br/&gt;&lt;br/&gt;2. A Framework for Distributed Physical Modeling at Interactive Speed&lt;br/&gt;Extend the framework developed previously for distributed physical modeling to study the effective utilization of non-homogeneous machine clusters and to provide data at interactive-speed to augmented and virtual reality systems.&lt;br/&gt;&lt;br/&gt;3. Dynamic Environmental Features in Virtual Worlds &lt;br/&gt;Develop algorithms for the dynamic generation of vegetation in virtual worlds. This will require the design of new level of detail management algorithms and lightweight communication protocols to maintain functionally.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98605e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98605e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n92" target="n93">
      <data key="e_effectiveDate">2000-03-01</data>
      <data key="e_title">CISE Research Instrumentation: Augmented Reality and Interactive Distributed Physical Modeling</data>
      <data key="e_abstract">EIA-9986051&lt;br/&gt;Jannick P. Rolland&lt;br/&gt;University of Central Florida&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Augmented Reality and Interactive Distributed Physical Modeling&lt;br/&gt;&lt;br/&gt;The schools of Optics and Computer Science at the University of Central Florida will purchase Silicon Graphics computers that will be dedicated to support research in computer and information science and engineering. The equipment will be used for several projects, including in particular:&lt;br/&gt;&lt;br/&gt;1. Augmented Reality&lt;br/&gt;Build a framework for augmented reality visualization. The framework includes the development of 3D displays, including head-mounted devices; quantitative assessment methods for accuracy and precision of 3D rendered depth; physically based models of motion of virtual objects; registration methods for real and virtual objects; and statistical texture synthesis.&lt;br/&gt;&lt;br/&gt;2. A Framework for Distributed Physical Modeling at Interactive Speed&lt;br/&gt;Extend the framework developed previously for distributed physical modeling to study the effective utilization of non-homogeneous machine clusters and to provide data at interactive-speed to augmented and virtual reality systems.&lt;br/&gt;&lt;br/&gt;3. Dynamic Environmental Features in Virtual Worlds &lt;br/&gt;Develop algorithms for the dynamic generation of vegetation in virtual worlds. This will require the design of new level of detail management algorithms and lightweight communication protocols to maintain functionally.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98605e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98605e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n94" target="n95">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">Instrumentation Grant for Research in Parallel and Distributed Computing</data>
      <data key="e_abstract">EIA-9986032&lt;br/&gt;David K. Lowenthal&lt;br/&gt;University of Georgia&lt;br/&gt;&lt;br/&gt;Instrumentation Grant for Research in Parallel and Distributed Computing&lt;br/&gt;&lt;br/&gt;The Parallel and Distributed Computing Laboratory at the University of Georgia, established in 1998 is dedicated to making parallel computing simple, portable, and efficient. The activities of the laboratory include designing and implementing efficient parallel algorithms, developing highly configurable environments for the monitoring, visualization, and interactive steering of distributed programs, and developing lower-level systems that transparently and dynamically determine how to distribute data across processors. Currently research in the laboratory is primarily done on networks of commodity uniprocessor workstations. The attractive price/performance ration of small, bus-based multiprocessors relative to uniprocessors makes it likely that two-and four-way multiprocessors will be commonplace within a few years. Still, to create large parallel machines, these multiprocessors will be networked together to create a hybrid (both shared-and distributed-memory) machine. This poses interesting problems, including (1) should the machine be treated as distributed or both shared and distributed. (2) How do we make full use of such machines and (3) how can a user monitor and control program on such machines? The Department of computer Science at the University of Georgia seeks funds to purchase eight Pentium-based multiprocessors with four processors each. A 100Mb/s Ethernet and an SGI fronted to serve as a console for visualization and interactive control. This equipment will be used by three projects.&lt;br/&gt;&lt;br/&gt;Parallel and Distributed algorithms for computational biology&lt;br/&gt;&lt;br/&gt;Monitoring, visualization, and interactive steering, and&lt;br/&gt;&lt;br/&gt;An integrated compiler/run-time system for global data distribution</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98603e+06</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98603e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n94" target="n96">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">Instrumentation Grant for Research in Parallel and Distributed Computing</data>
      <data key="e_abstract">EIA-9986032&lt;br/&gt;David K. Lowenthal&lt;br/&gt;University of Georgia&lt;br/&gt;&lt;br/&gt;Instrumentation Grant for Research in Parallel and Distributed Computing&lt;br/&gt;&lt;br/&gt;The Parallel and Distributed Computing Laboratory at the University of Georgia, established in 1998 is dedicated to making parallel computing simple, portable, and efficient. The activities of the laboratory include designing and implementing efficient parallel algorithms, developing highly configurable environments for the monitoring, visualization, and interactive steering of distributed programs, and developing lower-level systems that transparently and dynamically determine how to distribute data across processors. Currently research in the laboratory is primarily done on networks of commodity uniprocessor workstations. The attractive price/performance ration of small, bus-based multiprocessors relative to uniprocessors makes it likely that two-and four-way multiprocessors will be commonplace within a few years. Still, to create large parallel machines, these multiprocessors will be networked together to create a hybrid (both shared-and distributed-memory) machine. This poses interesting problems, including (1) should the machine be treated as distributed or both shared and distributed. (2) How do we make full use of such machines and (3) how can a user monitor and control program on such machines? The Department of computer Science at the University of Georgia seeks funds to purchase eight Pentium-based multiprocessors with four processors each. A 100Mb/s Ethernet and an SGI fronted to serve as a console for visualization and interactive control. This equipment will be used by three projects.&lt;br/&gt;&lt;br/&gt;Parallel and Distributed algorithms for computational biology&lt;br/&gt;&lt;br/&gt;Monitoring, visualization, and interactive steering, and&lt;br/&gt;&lt;br/&gt;An integrated compiler/run-time system for global data distribution</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98603e+06</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98603e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n95" target="n96">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">Instrumentation Grant for Research in Parallel and Distributed Computing</data>
      <data key="e_abstract">EIA-9986032&lt;br/&gt;David K. Lowenthal&lt;br/&gt;University of Georgia&lt;br/&gt;&lt;br/&gt;Instrumentation Grant for Research in Parallel and Distributed Computing&lt;br/&gt;&lt;br/&gt;The Parallel and Distributed Computing Laboratory at the University of Georgia, established in 1998 is dedicated to making parallel computing simple, portable, and efficient. The activities of the laboratory include designing and implementing efficient parallel algorithms, developing highly configurable environments for the monitoring, visualization, and interactive steering of distributed programs, and developing lower-level systems that transparently and dynamically determine how to distribute data across processors. Currently research in the laboratory is primarily done on networks of commodity uniprocessor workstations. The attractive price/performance ration of small, bus-based multiprocessors relative to uniprocessors makes it likely that two-and four-way multiprocessors will be commonplace within a few years. Still, to create large parallel machines, these multiprocessors will be networked together to create a hybrid (both shared-and distributed-memory) machine. This poses interesting problems, including (1) should the machine be treated as distributed or both shared and distributed. (2) How do we make full use of such machines and (3) how can a user monitor and control program on such machines? The Department of computer Science at the University of Georgia seeks funds to purchase eight Pentium-based multiprocessors with four processors each. A 100Mb/s Ethernet and an SGI fronted to serve as a console for visualization and interactive control. This equipment will be used by three projects.&lt;br/&gt;&lt;br/&gt;Parallel and Distributed algorithms for computational biology&lt;br/&gt;&lt;br/&gt;Monitoring, visualization, and interactive steering, and&lt;br/&gt;&lt;br/&gt;An integrated compiler/run-time system for global data distribution</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98603e+06</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98603e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n97" target="n98">
      <data key="e_effectiveDate">2000-03-01</data>
      <data key="e_title">Workshop on Research Topics in Space Macrosystems: Autonomous Construction and Manufacturing for Space Electrical Power Systems to be held on April 5-7, 2000 in Washington, DC</data>
      <data key="e_abstract">0075691&lt;br/&gt;Bekey&lt;br/&gt;&lt;br/&gt;This workshop will attempt to identify fundamental and applied research issues associated with very large construction and manufacturing systems in space and/or on the moon. Such issues will arise in connection with the assembly of solar power satellite systems (which may require the use of hundreds of autonomous, free-flying robots) and with the manufacture of photo-voltaic systems on the moon. The proposed workshop will concentrate on the system issues, including control, intelligence, cooperation, and self-replication. Partial support for the workshop has been obtained from NASA Headquarters.&lt;br/&gt;***</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">75691</data>
      <data key="e_expirationDate">2000-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">75691</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n99" target="n100">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Emulate the Performance of Wide Area Networks</data>
      <data key="e_abstract">EIA-9911074&lt;br/&gt;Matt W. Mutka&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Emulate the Performance of Wide Area Networks&lt;br/&gt;&lt;br/&gt;The Department of Computer Science and Engineering at Michigan State University will purchase computing and network equipment. This equipment includes two 24-port rackmount switches, an 8-port router, PCs for network emulation, four web servers, one RAID server, six mobile computing devices with wireless networking components, a wireless access point, and racks, monitors, and other devices to support the rackmounted system. This equipment will be used to implement the EMPOWER (EMulate the PerfOmance of WidE aRea networks) environment that will be dedicated to support research in computer science and engineering. EMPOWER enables the emulation of arbitrary network topologies, including interconnected host machines and routers contains both WAN ports and LAN ports. Beyond emulation of network bandwidth, drop rates and patterns, delay distributions, and background traffic, EMPOWER provides an excellent environment to implement and evaluate different QoS schemes (such as differentiated services) and label swapping schemes (such as MPLS). Current research includes emulation kernel design, routing table configuration, network statistics visualization, and GUI to configure the network easily.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.91107e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.91107e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n103" target="n104">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Research Infrastructure for Distributed and Data-Intensive Computing</data>
      <data key="e_abstract">EIA-9911099&lt;br/&gt;Kanad Ghose&lt;br/&gt;SUNY @ Binghamton&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Research Infrastructure for Distributed and Data-Intensive Computing&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at Binghamton University will purchase components for a 32-node-dual-Pentium cluster and a closely coupled set of high-end workstations and servers, which will be dedicated to support research in computer and information science and engineering. The equipment will be used for several research projects, including in particular: (i) large-scale data visualization; (ii) distributed digital library; (iii) parallel discrete event simulation and (iv) object-based distributed computing infrastructure. All four projects require a distributed high-performance platform (as provided by the cluster). Moreover, the visualization and metasearch engine projects require large disk space and memory sizes to handle the immense data sets required by these applications (as provided by the high-end workstations and cluster node4s). The simulation and visualization projects need the high-bandwidth low-latency communication provided by the Myrinet SAN network. The choice of the cluster for the distributed platform is clear; the cost-effectiveness provided by tapping into the commodity component market makes them a superior value to custom parallel machines (shared Memory, NUMAs or MPPs). There is no equivalent (or comparable) infrastructure available in the department; this equipment will be critical to the success of all four projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.9111e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.9111e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n103" target="n105">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Research Infrastructure for Distributed and Data-Intensive Computing</data>
      <data key="e_abstract">EIA-9911099&lt;br/&gt;Kanad Ghose&lt;br/&gt;SUNY @ Binghamton&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Research Infrastructure for Distributed and Data-Intensive Computing&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at Binghamton University will purchase components for a 32-node-dual-Pentium cluster and a closely coupled set of high-end workstations and servers, which will be dedicated to support research in computer and information science and engineering. The equipment will be used for several research projects, including in particular: (i) large-scale data visualization; (ii) distributed digital library; (iii) parallel discrete event simulation and (iv) object-based distributed computing infrastructure. All four projects require a distributed high-performance platform (as provided by the cluster). Moreover, the visualization and metasearch engine projects require large disk space and memory sizes to handle the immense data sets required by these applications (as provided by the high-end workstations and cluster node4s). The simulation and visualization projects need the high-bandwidth low-latency communication provided by the Myrinet SAN network. The choice of the cluster for the distributed platform is clear; the cost-effectiveness provided by tapping into the commodity component market makes them a superior value to custom parallel machines (shared Memory, NUMAs or MPPs). There is no equivalent (or comparable) infrastructure available in the department; this equipment will be critical to the success of all four projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.9111e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.9111e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n103" target="n106">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Research Infrastructure for Distributed and Data-Intensive Computing</data>
      <data key="e_abstract">EIA-9911099&lt;br/&gt;Kanad Ghose&lt;br/&gt;SUNY @ Binghamton&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Research Infrastructure for Distributed and Data-Intensive Computing&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at Binghamton University will purchase components for a 32-node-dual-Pentium cluster and a closely coupled set of high-end workstations and servers, which will be dedicated to support research in computer and information science and engineering. The equipment will be used for several research projects, including in particular: (i) large-scale data visualization; (ii) distributed digital library; (iii) parallel discrete event simulation and (iv) object-based distributed computing infrastructure. All four projects require a distributed high-performance platform (as provided by the cluster). Moreover, the visualization and metasearch engine projects require large disk space and memory sizes to handle the immense data sets required by these applications (as provided by the high-end workstations and cluster node4s). The simulation and visualization projects need the high-bandwidth low-latency communication provided by the Myrinet SAN network. The choice of the cluster for the distributed platform is clear; the cost-effectiveness provided by tapping into the commodity component market makes them a superior value to custom parallel machines (shared Memory, NUMAs or MPPs). There is no equivalent (or comparable) infrastructure available in the department; this equipment will be critical to the success of all four projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.9111e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.9111e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n104" target="n105">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Research Infrastructure for Distributed and Data-Intensive Computing</data>
      <data key="e_abstract">EIA-9911099&lt;br/&gt;Kanad Ghose&lt;br/&gt;SUNY @ Binghamton&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Research Infrastructure for Distributed and Data-Intensive Computing&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at Binghamton University will purchase components for a 32-node-dual-Pentium cluster and a closely coupled set of high-end workstations and servers, which will be dedicated to support research in computer and information science and engineering. The equipment will be used for several research projects, including in particular: (i) large-scale data visualization; (ii) distributed digital library; (iii) parallel discrete event simulation and (iv) object-based distributed computing infrastructure. All four projects require a distributed high-performance platform (as provided by the cluster). Moreover, the visualization and metasearch engine projects require large disk space and memory sizes to handle the immense data sets required by these applications (as provided by the high-end workstations and cluster node4s). The simulation and visualization projects need the high-bandwidth low-latency communication provided by the Myrinet SAN network. The choice of the cluster for the distributed platform is clear; the cost-effectiveness provided by tapping into the commodity component market makes them a superior value to custom parallel machines (shared Memory, NUMAs or MPPs). There is no equivalent (or comparable) infrastructure available in the department; this equipment will be critical to the success of all four projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.9111e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.9111e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n104" target="n106">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Research Infrastructure for Distributed and Data-Intensive Computing</data>
      <data key="e_abstract">EIA-9911099&lt;br/&gt;Kanad Ghose&lt;br/&gt;SUNY @ Binghamton&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Research Infrastructure for Distributed and Data-Intensive Computing&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at Binghamton University will purchase components for a 32-node-dual-Pentium cluster and a closely coupled set of high-end workstations and servers, which will be dedicated to support research in computer and information science and engineering. The equipment will be used for several research projects, including in particular: (i) large-scale data visualization; (ii) distributed digital library; (iii) parallel discrete event simulation and (iv) object-based distributed computing infrastructure. All four projects require a distributed high-performance platform (as provided by the cluster). Moreover, the visualization and metasearch engine projects require large disk space and memory sizes to handle the immense data sets required by these applications (as provided by the high-end workstations and cluster node4s). The simulation and visualization projects need the high-bandwidth low-latency communication provided by the Myrinet SAN network. The choice of the cluster for the distributed platform is clear; the cost-effectiveness provided by tapping into the commodity component market makes them a superior value to custom parallel machines (shared Memory, NUMAs or MPPs). There is no equivalent (or comparable) infrastructure available in the department; this equipment will be critical to the success of all four projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.9111e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.9111e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n105" target="n106">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Research Infrastructure for Distributed and Data-Intensive Computing</data>
      <data key="e_abstract">EIA-9911099&lt;br/&gt;Kanad Ghose&lt;br/&gt;SUNY @ Binghamton&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Research Infrastructure for Distributed and Data-Intensive Computing&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at Binghamton University will purchase components for a 32-node-dual-Pentium cluster and a closely coupled set of high-end workstations and servers, which will be dedicated to support research in computer and information science and engineering. The equipment will be used for several research projects, including in particular: (i) large-scale data visualization; (ii) distributed digital library; (iii) parallel discrete event simulation and (iv) object-based distributed computing infrastructure. All four projects require a distributed high-performance platform (as provided by the cluster). Moreover, the visualization and metasearch engine projects require large disk space and memory sizes to handle the immense data sets required by these applications (as provided by the high-end workstations and cluster node4s). The simulation and visualization projects need the high-bandwidth low-latency communication provided by the Myrinet SAN network. The choice of the cluster for the distributed platform is clear; the cost-effectiveness provided by tapping into the commodity component market makes them a superior value to custom parallel machines (shared Memory, NUMAs or MPPs). There is no equivalent (or comparable) infrastructure available in the department; this equipment will be critical to the success of all four projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.9111e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.9111e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n107" target="n108">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Advanced Human Computer Interfaces for Biomedicine</data>
      <data key="e_abstract">EIA-9911095&lt;br/&gt;Yunxin Zhao&lt;br/&gt;University of Missouri&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Advanced Human Computer Interfaces for Biomedicine&lt;br/&gt;&lt;br/&gt;The Department of Computer Engineering and Computer Science at the University of Missouri-Columbia will purchase audio, video, and virtual reality (VR) equipment which will be dedicated to support research in computer and information science and engineering. The equipment will be used for several research projects.&lt;br/&gt;&lt;br/&gt;1. Spoken Language Processing for Telemedicine and Collaborative VR Environments&lt;br/&gt; Voice-driven automatic captioning in telemedicine for people with hearing disabilities&lt;br/&gt; Speech separation and recognition for collaborative VR environments&lt;br/&gt;&lt;br/&gt;2. Video Communications over Telemedicine Networks and Geometric Coding of Anatomical Data&lt;br/&gt; Lips and hands focused real-time video coding for lipreading and hand-signing in telemedicine&lt;br/&gt; Three-dimensional geometric coding for collaborative VR-based endoscopic simulator&lt;br/&gt;&lt;br/&gt;3. Collaborative Virtual Endoscopy and Digital Human Atlas with Speech Interface&lt;br/&gt; Prototype Intelligent Endoscopic Simulator (IES) for examining internal tissues and organs &lt;br/&gt; Virtual operating table interface for teaching and examining complex multisource anatomical data</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.9111e+06</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.9111e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n107" target="n109">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Advanced Human Computer Interfaces for Biomedicine</data>
      <data key="e_abstract">EIA-9911095&lt;br/&gt;Yunxin Zhao&lt;br/&gt;University of Missouri&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Advanced Human Computer Interfaces for Biomedicine&lt;br/&gt;&lt;br/&gt;The Department of Computer Engineering and Computer Science at the University of Missouri-Columbia will purchase audio, video, and virtual reality (VR) equipment which will be dedicated to support research in computer and information science and engineering. The equipment will be used for several research projects.&lt;br/&gt;&lt;br/&gt;1. Spoken Language Processing for Telemedicine and Collaborative VR Environments&lt;br/&gt; Voice-driven automatic captioning in telemedicine for people with hearing disabilities&lt;br/&gt; Speech separation and recognition for collaborative VR environments&lt;br/&gt;&lt;br/&gt;2. Video Communications over Telemedicine Networks and Geometric Coding of Anatomical Data&lt;br/&gt; Lips and hands focused real-time video coding for lipreading and hand-signing in telemedicine&lt;br/&gt; Three-dimensional geometric coding for collaborative VR-based endoscopic simulator&lt;br/&gt;&lt;br/&gt;3. Collaborative Virtual Endoscopy and Digital Human Atlas with Speech Interface&lt;br/&gt; Prototype Intelligent Endoscopic Simulator (IES) for examining internal tissues and organs &lt;br/&gt; Virtual operating table interface for teaching and examining complex multisource anatomical data</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.9111e+06</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.9111e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n108" target="n109">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Advanced Human Computer Interfaces for Biomedicine</data>
      <data key="e_abstract">EIA-9911095&lt;br/&gt;Yunxin Zhao&lt;br/&gt;University of Missouri&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Advanced Human Computer Interfaces for Biomedicine&lt;br/&gt;&lt;br/&gt;The Department of Computer Engineering and Computer Science at the University of Missouri-Columbia will purchase audio, video, and virtual reality (VR) equipment which will be dedicated to support research in computer and information science and engineering. The equipment will be used for several research projects.&lt;br/&gt;&lt;br/&gt;1. Spoken Language Processing for Telemedicine and Collaborative VR Environments&lt;br/&gt; Voice-driven automatic captioning in telemedicine for people with hearing disabilities&lt;br/&gt; Speech separation and recognition for collaborative VR environments&lt;br/&gt;&lt;br/&gt;2. Video Communications over Telemedicine Networks and Geometric Coding of Anatomical Data&lt;br/&gt; Lips and hands focused real-time video coding for lipreading and hand-signing in telemedicine&lt;br/&gt; Three-dimensional geometric coding for collaborative VR-based endoscopic simulator&lt;br/&gt;&lt;br/&gt;3. Collaborative Virtual Endoscopy and Digital Human Atlas with Speech Interface&lt;br/&gt; Prototype Intelligent Endoscopic Simulator (IES) for examining internal tissues and organs &lt;br/&gt; Virtual operating table interface for teaching and examining complex multisource anatomical data</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.9111e+06</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.9111e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n110" target="n111">
      <data key="e_effectiveDate">2000-03-01</data>
      <data key="e_title">MLIAM: NESPOLE! - Negotiating through Spoken Language in E-commerce</data>
      <data key="e_abstract">This project, which will be carried out by a consortium of six partners in the US and Europe, aims to remove language barriers and enable the broad general public to do business on the internet via an effective cross lingual human to human communication facility. Two major obstacles toward this goal will be attacked: the domain limitation of speech translation systems, and the use of advanced multi-modal interfaces and their integration with language assistance functions. The PI&apos;s team will exploit a mixture of strengths in a multi engine approach, while also expanding the basic capabilities of the engines to enhance their robustness and portability. Prototype systems involving actual service providers will be configured and studied in the context of extensive e-commerce use. Four of the partners (CMU, University of Karlsruhe, IRST, CLIPS) are scientific institutions who have worked together for many years within the Consortium for Speech Translation Advanced Research (C-STAR) and have an elaborate and solid base of state-of-the art speech translation technology at their disposal. The two commercial partners (APT and AETHRA) will provide Web-based and video-conferencing infrastructure along with a user/customer business community for extensive and realistic user studies. The prototype system (or set of systems) constructed in the course of the project will be tested and demonstrated via two showcase applications- one in the more limited travel domain, the other in a very broad help-desk scenario in which real users will conduct multi-modal and translingual e-commerce transactions. The technology resulting from this project will have immediate relevancy to numerous and diverse other applications that require natural language understanding in multi-modal settings.&lt;br/&gt;&lt;br/&gt;I</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.98223e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98223e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n110" target="n112">
      <data key="e_effectiveDate">2000-03-01</data>
      <data key="e_title">MLIAM: NESPOLE! - Negotiating through Spoken Language in E-commerce</data>
      <data key="e_abstract">This project, which will be carried out by a consortium of six partners in the US and Europe, aims to remove language barriers and enable the broad general public to do business on the internet via an effective cross lingual human to human communication facility. Two major obstacles toward this goal will be attacked: the domain limitation of speech translation systems, and the use of advanced multi-modal interfaces and their integration with language assistance functions. The PI&apos;s team will exploit a mixture of strengths in a multi engine approach, while also expanding the basic capabilities of the engines to enhance their robustness and portability. Prototype systems involving actual service providers will be configured and studied in the context of extensive e-commerce use. Four of the partners (CMU, University of Karlsruhe, IRST, CLIPS) are scientific institutions who have worked together for many years within the Consortium for Speech Translation Advanced Research (C-STAR) and have an elaborate and solid base of state-of-the art speech translation technology at their disposal. The two commercial partners (APT and AETHRA) will provide Web-based and video-conferencing infrastructure along with a user/customer business community for extensive and realistic user studies. The prototype system (or set of systems) constructed in the course of the project will be tested and demonstrated via two showcase applications- one in the more limited travel domain, the other in a very broad help-desk scenario in which real users will conduct multi-modal and translingual e-commerce transactions. The technology resulting from this project will have immediate relevancy to numerous and diverse other applications that require natural language understanding in multi-modal settings.&lt;br/&gt;&lt;br/&gt;I</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.98223e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98223e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n110" target="n113">
      <data key="e_effectiveDate">2000-03-01</data>
      <data key="e_title">MLIAM: NESPOLE! - Negotiating through Spoken Language in E-commerce</data>
      <data key="e_abstract">This project, which will be carried out by a consortium of six partners in the US and Europe, aims to remove language barriers and enable the broad general public to do business on the internet via an effective cross lingual human to human communication facility. Two major obstacles toward this goal will be attacked: the domain limitation of speech translation systems, and the use of advanced multi-modal interfaces and their integration with language assistance functions. The PI&apos;s team will exploit a mixture of strengths in a multi engine approach, while also expanding the basic capabilities of the engines to enhance their robustness and portability. Prototype systems involving actual service providers will be configured and studied in the context of extensive e-commerce use. Four of the partners (CMU, University of Karlsruhe, IRST, CLIPS) are scientific institutions who have worked together for many years within the Consortium for Speech Translation Advanced Research (C-STAR) and have an elaborate and solid base of state-of-the art speech translation technology at their disposal. The two commercial partners (APT and AETHRA) will provide Web-based and video-conferencing infrastructure along with a user/customer business community for extensive and realistic user studies. The prototype system (or set of systems) constructed in the course of the project will be tested and demonstrated via two showcase applications- one in the more limited travel domain, the other in a very broad help-desk scenario in which real users will conduct multi-modal and translingual e-commerce transactions. The technology resulting from this project will have immediate relevancy to numerous and diverse other applications that require natural language understanding in multi-modal settings.&lt;br/&gt;&lt;br/&gt;I</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.98223e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98223e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n111" target="n112">
      <data key="e_effectiveDate">2000-03-01</data>
      <data key="e_title">MLIAM: NESPOLE! - Negotiating through Spoken Language in E-commerce</data>
      <data key="e_abstract">This project, which will be carried out by a consortium of six partners in the US and Europe, aims to remove language barriers and enable the broad general public to do business on the internet via an effective cross lingual human to human communication facility. Two major obstacles toward this goal will be attacked: the domain limitation of speech translation systems, and the use of advanced multi-modal interfaces and their integration with language assistance functions. The PI&apos;s team will exploit a mixture of strengths in a multi engine approach, while also expanding the basic capabilities of the engines to enhance their robustness and portability. Prototype systems involving actual service providers will be configured and studied in the context of extensive e-commerce use. Four of the partners (CMU, University of Karlsruhe, IRST, CLIPS) are scientific institutions who have worked together for many years within the Consortium for Speech Translation Advanced Research (C-STAR) and have an elaborate and solid base of state-of-the art speech translation technology at their disposal. The two commercial partners (APT and AETHRA) will provide Web-based and video-conferencing infrastructure along with a user/customer business community for extensive and realistic user studies. The prototype system (or set of systems) constructed in the course of the project will be tested and demonstrated via two showcase applications- one in the more limited travel domain, the other in a very broad help-desk scenario in which real users will conduct multi-modal and translingual e-commerce transactions. The technology resulting from this project will have immediate relevancy to numerous and diverse other applications that require natural language understanding in multi-modal settings.&lt;br/&gt;&lt;br/&gt;I</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.98223e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98223e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n111" target="n113">
      <data key="e_effectiveDate">2000-03-01</data>
      <data key="e_title">MLIAM: NESPOLE! - Negotiating through Spoken Language in E-commerce</data>
      <data key="e_abstract">This project, which will be carried out by a consortium of six partners in the US and Europe, aims to remove language barriers and enable the broad general public to do business on the internet via an effective cross lingual human to human communication facility. Two major obstacles toward this goal will be attacked: the domain limitation of speech translation systems, and the use of advanced multi-modal interfaces and their integration with language assistance functions. The PI&apos;s team will exploit a mixture of strengths in a multi engine approach, while also expanding the basic capabilities of the engines to enhance their robustness and portability. Prototype systems involving actual service providers will be configured and studied in the context of extensive e-commerce use. Four of the partners (CMU, University of Karlsruhe, IRST, CLIPS) are scientific institutions who have worked together for many years within the Consortium for Speech Translation Advanced Research (C-STAR) and have an elaborate and solid base of state-of-the art speech translation technology at their disposal. The two commercial partners (APT and AETHRA) will provide Web-based and video-conferencing infrastructure along with a user/customer business community for extensive and realistic user studies. The prototype system (or set of systems) constructed in the course of the project will be tested and demonstrated via two showcase applications- one in the more limited travel domain, the other in a very broad help-desk scenario in which real users will conduct multi-modal and translingual e-commerce transactions. The technology resulting from this project will have immediate relevancy to numerous and diverse other applications that require natural language understanding in multi-modal settings.&lt;br/&gt;&lt;br/&gt;I</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.98223e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98223e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n112" target="n113">
      <data key="e_effectiveDate">2000-03-01</data>
      <data key="e_title">MLIAM: NESPOLE! - Negotiating through Spoken Language in E-commerce</data>
      <data key="e_abstract">This project, which will be carried out by a consortium of six partners in the US and Europe, aims to remove language barriers and enable the broad general public to do business on the internet via an effective cross lingual human to human communication facility. Two major obstacles toward this goal will be attacked: the domain limitation of speech translation systems, and the use of advanced multi-modal interfaces and their integration with language assistance functions. The PI&apos;s team will exploit a mixture of strengths in a multi engine approach, while also expanding the basic capabilities of the engines to enhance their robustness and portability. Prototype systems involving actual service providers will be configured and studied in the context of extensive e-commerce use. Four of the partners (CMU, University of Karlsruhe, IRST, CLIPS) are scientific institutions who have worked together for many years within the Consortium for Speech Translation Advanced Research (C-STAR) and have an elaborate and solid base of state-of-the art speech translation technology at their disposal. The two commercial partners (APT and AETHRA) will provide Web-based and video-conferencing infrastructure along with a user/customer business community for extensive and realistic user studies. The prototype system (or set of systems) constructed in the course of the project will be tested and demonstrated via two showcase applications- one in the more limited travel domain, the other in a very broad help-desk scenario in which real users will conduct multi-modal and translingual e-commerce transactions. The technology resulting from this project will have immediate relevancy to numerous and diverse other applications that require natural language understanding in multi-modal settings.&lt;br/&gt;&lt;br/&gt;I</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.98223e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98223e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n114" target="n115">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">MLIAM: MUCHMORE: Multilingual Concept Hierarchies for Medical Information Organization and Retrieval</data>
      <data key="e_abstract">This project will extend the state of the art in high performance multilingual information access, both in terms of underlying science and its technological realization via a functional prototype for English and German in the biomedical domain. Heretofore, Cross-Lingual Information Retrieval (CLIR) was founded upon dictionary-based query translation methods or corpus-based statistical learning of vocabulary mappings, combined with various IR methods. The existence of large, well accepted ontological resources in biomedicine (e.g., MeSH and UMLS) enables a new interlingual approach wherein both queries and documents are mapped into multiple taxonomic categories automatically, permitting direct conceptual matching. This research will compare existing techniques (dictionarybased and corpus-based) with the new interlingual methods on various evaluative dimensions, such as I I-point average precision, computational tractability, and end-to-end user acceptability. To judge the latter, a full prototype system will be developed; that is the main focus of the European side of the project. In addition to developing and evaluating these new CLIR methods, and producing and evaluating a usable prototype application, this project will provide other benefits beyond CLIR proper improving IR precision via automated corpus-based word sense disambiguation; developing statistical methods for the creation of multilingual lexical and phrasal resources; providing automated on-demand summarization of retrieved documents using the Maximal Marginal Relevance method; and improving multilingual information access and management systems forthe biomedical domain.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.98223e+06</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98223e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n114" target="n116">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">MLIAM: MUCHMORE: Multilingual Concept Hierarchies for Medical Information Organization and Retrieval</data>
      <data key="e_abstract">This project will extend the state of the art in high performance multilingual information access, both in terms of underlying science and its technological realization via a functional prototype for English and German in the biomedical domain. Heretofore, Cross-Lingual Information Retrieval (CLIR) was founded upon dictionary-based query translation methods or corpus-based statistical learning of vocabulary mappings, combined with various IR methods. The existence of large, well accepted ontological resources in biomedicine (e.g., MeSH and UMLS) enables a new interlingual approach wherein both queries and documents are mapped into multiple taxonomic categories automatically, permitting direct conceptual matching. This research will compare existing techniques (dictionarybased and corpus-based) with the new interlingual methods on various evaluative dimensions, such as I I-point average precision, computational tractability, and end-to-end user acceptability. To judge the latter, a full prototype system will be developed; that is the main focus of the European side of the project. In addition to developing and evaluating these new CLIR methods, and producing and evaluating a usable prototype application, this project will provide other benefits beyond CLIR proper improving IR precision via automated corpus-based word sense disambiguation; developing statistical methods for the creation of multilingual lexical and phrasal resources; providing automated on-demand summarization of retrieved documents using the Maximal Marginal Relevance method; and improving multilingual information access and management systems forthe biomedical domain.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.98223e+06</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98223e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n112" target="n114">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">MLIAM: MUCHMORE: Multilingual Concept Hierarchies for Medical Information Organization and Retrieval</data>
      <data key="e_abstract">This project will extend the state of the art in high performance multilingual information access, both in terms of underlying science and its technological realization via a functional prototype for English and German in the biomedical domain. Heretofore, Cross-Lingual Information Retrieval (CLIR) was founded upon dictionary-based query translation methods or corpus-based statistical learning of vocabulary mappings, combined with various IR methods. The existence of large, well accepted ontological resources in biomedicine (e.g., MeSH and UMLS) enables a new interlingual approach wherein both queries and documents are mapped into multiple taxonomic categories automatically, permitting direct conceptual matching. This research will compare existing techniques (dictionarybased and corpus-based) with the new interlingual methods on various evaluative dimensions, such as I I-point average precision, computational tractability, and end-to-end user acceptability. To judge the latter, a full prototype system will be developed; that is the main focus of the European side of the project. In addition to developing and evaluating these new CLIR methods, and producing and evaluating a usable prototype application, this project will provide other benefits beyond CLIR proper improving IR precision via automated corpus-based word sense disambiguation; developing statistical methods for the creation of multilingual lexical and phrasal resources; providing automated on-demand summarization of retrieved documents using the Maximal Marginal Relevance method; and improving multilingual information access and management systems forthe biomedical domain.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.98223e+06</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98223e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n115" target="n116">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">MLIAM: MUCHMORE: Multilingual Concept Hierarchies for Medical Information Organization and Retrieval</data>
      <data key="e_abstract">This project will extend the state of the art in high performance multilingual information access, both in terms of underlying science and its technological realization via a functional prototype for English and German in the biomedical domain. Heretofore, Cross-Lingual Information Retrieval (CLIR) was founded upon dictionary-based query translation methods or corpus-based statistical learning of vocabulary mappings, combined with various IR methods. The existence of large, well accepted ontological resources in biomedicine (e.g., MeSH and UMLS) enables a new interlingual approach wherein both queries and documents are mapped into multiple taxonomic categories automatically, permitting direct conceptual matching. This research will compare existing techniques (dictionarybased and corpus-based) with the new interlingual methods on various evaluative dimensions, such as I I-point average precision, computational tractability, and end-to-end user acceptability. To judge the latter, a full prototype system will be developed; that is the main focus of the European side of the project. In addition to developing and evaluating these new CLIR methods, and producing and evaluating a usable prototype application, this project will provide other benefits beyond CLIR proper improving IR precision via automated corpus-based word sense disambiguation; developing statistical methods for the creation of multilingual lexical and phrasal resources; providing automated on-demand summarization of retrieved documents using the Maximal Marginal Relevance method; and improving multilingual information access and management systems forthe biomedical domain.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.98223e+06</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98223e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n112" target="n115">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">MLIAM: MUCHMORE: Multilingual Concept Hierarchies for Medical Information Organization and Retrieval</data>
      <data key="e_abstract">This project will extend the state of the art in high performance multilingual information access, both in terms of underlying science and its technological realization via a functional prototype for English and German in the biomedical domain. Heretofore, Cross-Lingual Information Retrieval (CLIR) was founded upon dictionary-based query translation methods or corpus-based statistical learning of vocabulary mappings, combined with various IR methods. The existence of large, well accepted ontological resources in biomedicine (e.g., MeSH and UMLS) enables a new interlingual approach wherein both queries and documents are mapped into multiple taxonomic categories automatically, permitting direct conceptual matching. This research will compare existing techniques (dictionarybased and corpus-based) with the new interlingual methods on various evaluative dimensions, such as I I-point average precision, computational tractability, and end-to-end user acceptability. To judge the latter, a full prototype system will be developed; that is the main focus of the European side of the project. In addition to developing and evaluating these new CLIR methods, and producing and evaluating a usable prototype application, this project will provide other benefits beyond CLIR proper improving IR precision via automated corpus-based word sense disambiguation; developing statistical methods for the creation of multilingual lexical and phrasal resources; providing automated on-demand summarization of retrieved documents using the Maximal Marginal Relevance method; and improving multilingual information access and management systems forthe biomedical domain.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.98223e+06</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98223e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n112" target="n116">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">MLIAM: MUCHMORE: Multilingual Concept Hierarchies for Medical Information Organization and Retrieval</data>
      <data key="e_abstract">This project will extend the state of the art in high performance multilingual information access, both in terms of underlying science and its technological realization via a functional prototype for English and German in the biomedical domain. Heretofore, Cross-Lingual Information Retrieval (CLIR) was founded upon dictionary-based query translation methods or corpus-based statistical learning of vocabulary mappings, combined with various IR methods. The existence of large, well accepted ontological resources in biomedicine (e.g., MeSH and UMLS) enables a new interlingual approach wherein both queries and documents are mapped into multiple taxonomic categories automatically, permitting direct conceptual matching. This research will compare existing techniques (dictionarybased and corpus-based) with the new interlingual methods on various evaluative dimensions, such as I I-point average precision, computational tractability, and end-to-end user acceptability. To judge the latter, a full prototype system will be developed; that is the main focus of the European side of the project. In addition to developing and evaluating these new CLIR methods, and producing and evaluating a usable prototype application, this project will provide other benefits beyond CLIR proper improving IR precision via automated corpus-based word sense disambiguation; developing statistical methods for the creation of multilingual lexical and phrasal resources; providing automated on-demand summarization of retrieved documents using the Maximal Marginal Relevance method; and improving multilingual information access and management systems forthe biomedical domain.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.98223e+06</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98223e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n16" target="n119">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation</data>
      <data key="e_abstract">EIA-9985991&lt;br/&gt;Doug Burger&lt;br/&gt;University of Texas-Austin&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at the University of Texas will purchase a cluster of high performance, multigranular workstations, dedicated to support research in computer and information science and engineering. The equipment will initially be used to support four research projects in the department. &lt;br/&gt;&lt;br/&gt;The first project will use the cluster for detailed evaluations of future high-performance microprocessor designs. The second project will use the cluster to improve algorithms for reconstructing large evolutionary trees, with applications in evolutionary biology, pharmaceutical design, and linguistics. The third project will use the cluster to improve I/O -intensive data mining simulations using numerical techniques. The fourth project will develop software support for scientific cluster-based computing, including support for effective heterogeneous job scheduling, low-overhead application fault-tolerance, and run-time dynamic load balancing. &lt;br/&gt;&lt;br/&gt;The cluster will consist of heterogeneous machines, with varied quantities of memory, disk, and processors per machine. We will support several types of jobs submitted through a common interface: uniprocessor jobs, shared-memory parallel jobs, PVM message-passing jobs, and distributed-shared memory jobs. Our long-term goal is to build a centralized but scalable resource that can meet the needs of numerous scientific workloads concurrently.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98599e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98599e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n16" target="n120">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation</data>
      <data key="e_abstract">EIA-9985991&lt;br/&gt;Doug Burger&lt;br/&gt;University of Texas-Austin&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at the University of Texas will purchase a cluster of high performance, multigranular workstations, dedicated to support research in computer and information science and engineering. The equipment will initially be used to support four research projects in the department. &lt;br/&gt;&lt;br/&gt;The first project will use the cluster for detailed evaluations of future high-performance microprocessor designs. The second project will use the cluster to improve algorithms for reconstructing large evolutionary trees, with applications in evolutionary biology, pharmaceutical design, and linguistics. The third project will use the cluster to improve I/O -intensive data mining simulations using numerical techniques. The fourth project will develop software support for scientific cluster-based computing, including support for effective heterogeneous job scheduling, low-overhead application fault-tolerance, and run-time dynamic load balancing. &lt;br/&gt;&lt;br/&gt;The cluster will consist of heterogeneous machines, with varied quantities of memory, disk, and processors per machine. We will support several types of jobs submitted through a common interface: uniprocessor jobs, shared-memory parallel jobs, PVM message-passing jobs, and distributed-shared memory jobs. Our long-term goal is to build a centralized but scalable resource that can meet the needs of numerous scientific workloads concurrently.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98599e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98599e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n16" target="n121">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation</data>
      <data key="e_abstract">EIA-9985991&lt;br/&gt;Doug Burger&lt;br/&gt;University of Texas-Austin&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at the University of Texas will purchase a cluster of high performance, multigranular workstations, dedicated to support research in computer and information science and engineering. The equipment will initially be used to support four research projects in the department. &lt;br/&gt;&lt;br/&gt;The first project will use the cluster for detailed evaluations of future high-performance microprocessor designs. The second project will use the cluster to improve algorithms for reconstructing large evolutionary trees, with applications in evolutionary biology, pharmaceutical design, and linguistics. The third project will use the cluster to improve I/O -intensive data mining simulations using numerical techniques. The fourth project will develop software support for scientific cluster-based computing, including support for effective heterogeneous job scheduling, low-overhead application fault-tolerance, and run-time dynamic load balancing. &lt;br/&gt;&lt;br/&gt;The cluster will consist of heterogeneous machines, with varied quantities of memory, disk, and processors per machine. We will support several types of jobs submitted through a common interface: uniprocessor jobs, shared-memory parallel jobs, PVM message-passing jobs, and distributed-shared memory jobs. Our long-term goal is to build a centralized but scalable resource that can meet the needs of numerous scientific workloads concurrently.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98599e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98599e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n16" target="n122">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation</data>
      <data key="e_abstract">EIA-9985991&lt;br/&gt;Doug Burger&lt;br/&gt;University of Texas-Austin&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at the University of Texas will purchase a cluster of high performance, multigranular workstations, dedicated to support research in computer and information science and engineering. The equipment will initially be used to support four research projects in the department. &lt;br/&gt;&lt;br/&gt;The first project will use the cluster for detailed evaluations of future high-performance microprocessor designs. The second project will use the cluster to improve algorithms for reconstructing large evolutionary trees, with applications in evolutionary biology, pharmaceutical design, and linguistics. The third project will use the cluster to improve I/O -intensive data mining simulations using numerical techniques. The fourth project will develop software support for scientific cluster-based computing, including support for effective heterogeneous job scheduling, low-overhead application fault-tolerance, and run-time dynamic load balancing. &lt;br/&gt;&lt;br/&gt;The cluster will consist of heterogeneous machines, with varied quantities of memory, disk, and processors per machine. We will support several types of jobs submitted through a common interface: uniprocessor jobs, shared-memory parallel jobs, PVM message-passing jobs, and distributed-shared memory jobs. Our long-term goal is to build a centralized but scalable resource that can meet the needs of numerous scientific workloads concurrently.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98599e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98599e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n119" target="n120">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation</data>
      <data key="e_abstract">EIA-9985991&lt;br/&gt;Doug Burger&lt;br/&gt;University of Texas-Austin&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at the University of Texas will purchase a cluster of high performance, multigranular workstations, dedicated to support research in computer and information science and engineering. The equipment will initially be used to support four research projects in the department. &lt;br/&gt;&lt;br/&gt;The first project will use the cluster for detailed evaluations of future high-performance microprocessor designs. The second project will use the cluster to improve algorithms for reconstructing large evolutionary trees, with applications in evolutionary biology, pharmaceutical design, and linguistics. The third project will use the cluster to improve I/O -intensive data mining simulations using numerical techniques. The fourth project will develop software support for scientific cluster-based computing, including support for effective heterogeneous job scheduling, low-overhead application fault-tolerance, and run-time dynamic load balancing. &lt;br/&gt;&lt;br/&gt;The cluster will consist of heterogeneous machines, with varied quantities of memory, disk, and processors per machine. We will support several types of jobs submitted through a common interface: uniprocessor jobs, shared-memory parallel jobs, PVM message-passing jobs, and distributed-shared memory jobs. Our long-term goal is to build a centralized but scalable resource that can meet the needs of numerous scientific workloads concurrently.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98599e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98599e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n119" target="n121">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation</data>
      <data key="e_abstract">EIA-9985991&lt;br/&gt;Doug Burger&lt;br/&gt;University of Texas-Austin&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at the University of Texas will purchase a cluster of high performance, multigranular workstations, dedicated to support research in computer and information science and engineering. The equipment will initially be used to support four research projects in the department. &lt;br/&gt;&lt;br/&gt;The first project will use the cluster for detailed evaluations of future high-performance microprocessor designs. The second project will use the cluster to improve algorithms for reconstructing large evolutionary trees, with applications in evolutionary biology, pharmaceutical design, and linguistics. The third project will use the cluster to improve I/O -intensive data mining simulations using numerical techniques. The fourth project will develop software support for scientific cluster-based computing, including support for effective heterogeneous job scheduling, low-overhead application fault-tolerance, and run-time dynamic load balancing. &lt;br/&gt;&lt;br/&gt;The cluster will consist of heterogeneous machines, with varied quantities of memory, disk, and processors per machine. We will support several types of jobs submitted through a common interface: uniprocessor jobs, shared-memory parallel jobs, PVM message-passing jobs, and distributed-shared memory jobs. Our long-term goal is to build a centralized but scalable resource that can meet the needs of numerous scientific workloads concurrently.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98599e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98599e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n119" target="n122">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation</data>
      <data key="e_abstract">EIA-9985991&lt;br/&gt;Doug Burger&lt;br/&gt;University of Texas-Austin&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at the University of Texas will purchase a cluster of high performance, multigranular workstations, dedicated to support research in computer and information science and engineering. The equipment will initially be used to support four research projects in the department. &lt;br/&gt;&lt;br/&gt;The first project will use the cluster for detailed evaluations of future high-performance microprocessor designs. The second project will use the cluster to improve algorithms for reconstructing large evolutionary trees, with applications in evolutionary biology, pharmaceutical design, and linguistics. The third project will use the cluster to improve I/O -intensive data mining simulations using numerical techniques. The fourth project will develop software support for scientific cluster-based computing, including support for effective heterogeneous job scheduling, low-overhead application fault-tolerance, and run-time dynamic load balancing. &lt;br/&gt;&lt;br/&gt;The cluster will consist of heterogeneous machines, with varied quantities of memory, disk, and processors per machine. We will support several types of jobs submitted through a common interface: uniprocessor jobs, shared-memory parallel jobs, PVM message-passing jobs, and distributed-shared memory jobs. Our long-term goal is to build a centralized but scalable resource that can meet the needs of numerous scientific workloads concurrently.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98599e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98599e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n120" target="n121">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation</data>
      <data key="e_abstract">EIA-9985991&lt;br/&gt;Doug Burger&lt;br/&gt;University of Texas-Austin&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at the University of Texas will purchase a cluster of high performance, multigranular workstations, dedicated to support research in computer and information science and engineering. The equipment will initially be used to support four research projects in the department. &lt;br/&gt;&lt;br/&gt;The first project will use the cluster for detailed evaluations of future high-performance microprocessor designs. The second project will use the cluster to improve algorithms for reconstructing large evolutionary trees, with applications in evolutionary biology, pharmaceutical design, and linguistics. The third project will use the cluster to improve I/O -intensive data mining simulations using numerical techniques. The fourth project will develop software support for scientific cluster-based computing, including support for effective heterogeneous job scheduling, low-overhead application fault-tolerance, and run-time dynamic load balancing. &lt;br/&gt;&lt;br/&gt;The cluster will consist of heterogeneous machines, with varied quantities of memory, disk, and processors per machine. We will support several types of jobs submitted through a common interface: uniprocessor jobs, shared-memory parallel jobs, PVM message-passing jobs, and distributed-shared memory jobs. Our long-term goal is to build a centralized but scalable resource that can meet the needs of numerous scientific workloads concurrently.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98599e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98599e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n120" target="n122">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation</data>
      <data key="e_abstract">EIA-9985991&lt;br/&gt;Doug Burger&lt;br/&gt;University of Texas-Austin&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at the University of Texas will purchase a cluster of high performance, multigranular workstations, dedicated to support research in computer and information science and engineering. The equipment will initially be used to support four research projects in the department. &lt;br/&gt;&lt;br/&gt;The first project will use the cluster for detailed evaluations of future high-performance microprocessor designs. The second project will use the cluster to improve algorithms for reconstructing large evolutionary trees, with applications in evolutionary biology, pharmaceutical design, and linguistics. The third project will use the cluster to improve I/O -intensive data mining simulations using numerical techniques. The fourth project will develop software support for scientific cluster-based computing, including support for effective heterogeneous job scheduling, low-overhead application fault-tolerance, and run-time dynamic load balancing. &lt;br/&gt;&lt;br/&gt;The cluster will consist of heterogeneous machines, with varied quantities of memory, disk, and processors per machine. We will support several types of jobs submitted through a common interface: uniprocessor jobs, shared-memory parallel jobs, PVM message-passing jobs, and distributed-shared memory jobs. Our long-term goal is to build a centralized but scalable resource that can meet the needs of numerous scientific workloads concurrently.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98599e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98599e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n121" target="n122">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation</data>
      <data key="e_abstract">EIA-9985991&lt;br/&gt;Doug Burger&lt;br/&gt;University of Texas-Austin&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation&lt;br/&gt;&lt;br/&gt;The Department of Computer Science at the University of Texas will purchase a cluster of high performance, multigranular workstations, dedicated to support research in computer and information science and engineering. The equipment will initially be used to support four research projects in the department. &lt;br/&gt;&lt;br/&gt;The first project will use the cluster for detailed evaluations of future high-performance microprocessor designs. The second project will use the cluster to improve algorithms for reconstructing large evolutionary trees, with applications in evolutionary biology, pharmaceutical design, and linguistics. The third project will use the cluster to improve I/O -intensive data mining simulations using numerical techniques. The fourth project will develop software support for scientific cluster-based computing, including support for effective heterogeneous job scheduling, low-overhead application fault-tolerance, and run-time dynamic load balancing. &lt;br/&gt;&lt;br/&gt;The cluster will consist of heterogeneous machines, with varied quantities of memory, disk, and processors per machine. We will support several types of jobs submitted through a common interface: uniprocessor jobs, shared-memory parallel jobs, PVM message-passing jobs, and distributed-shared memory jobs. Our long-term goal is to build a centralized but scalable resource that can meet the needs of numerous scientific workloads concurrently.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98599e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98599e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n124" target="n125">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Adaptive Computing Cluster</data>
      <data key="e_abstract">EIA-9985986&lt;br/&gt;Ronald R. Sass&lt;br/&gt;Clemson University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Adaptive Computing Cluster&lt;br/&gt;&lt;br/&gt;The Department of Electrical and Computer Engineering at Clemson University will purchase a cluster of workstations with reconfigurable computing to support research in cluster computing, reconfigurable computing, bioinformatics, and robotics, science and engineering. The equipment will be used for several research projects including in particular &quot;Adaptive Computing Cluster&quot;, Fast Sequence Searching on an Adaptable Computing Cluster&apos;, &quot;Reconfigurable Hardware Support for a Parallel File System, and &quot;Performance Validation of Nonlinear Controllers for Magnetic Bearings via Dynamic Simulation&apos;. Each of these projects require the unique combination of configurable hardware and cluster computing resources requested to obtain experimental results. The use of the equipment will be restricted to use by the faculty and students involved in these projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98599e+06</data>
      <data key="e_expirationDate">2002-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98599e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n124" target="n126">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Adaptive Computing Cluster</data>
      <data key="e_abstract">EIA-9985986&lt;br/&gt;Ronald R. Sass&lt;br/&gt;Clemson University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Adaptive Computing Cluster&lt;br/&gt;&lt;br/&gt;The Department of Electrical and Computer Engineering at Clemson University will purchase a cluster of workstations with reconfigurable computing to support research in cluster computing, reconfigurable computing, bioinformatics, and robotics, science and engineering. The equipment will be used for several research projects including in particular &quot;Adaptive Computing Cluster&quot;, Fast Sequence Searching on an Adaptable Computing Cluster&apos;, &quot;Reconfigurable Hardware Support for a Parallel File System, and &quot;Performance Validation of Nonlinear Controllers for Magnetic Bearings via Dynamic Simulation&apos;. Each of these projects require the unique combination of configurable hardware and cluster computing resources requested to obtain experimental results. The use of the equipment will be restricted to use by the faculty and students involved in these projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98599e+06</data>
      <data key="e_expirationDate">2002-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98599e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n124" target="n127">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Adaptive Computing Cluster</data>
      <data key="e_abstract">EIA-9985986&lt;br/&gt;Ronald R. Sass&lt;br/&gt;Clemson University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Adaptive Computing Cluster&lt;br/&gt;&lt;br/&gt;The Department of Electrical and Computer Engineering at Clemson University will purchase a cluster of workstations with reconfigurable computing to support research in cluster computing, reconfigurable computing, bioinformatics, and robotics, science and engineering. The equipment will be used for several research projects including in particular &quot;Adaptive Computing Cluster&quot;, Fast Sequence Searching on an Adaptable Computing Cluster&apos;, &quot;Reconfigurable Hardware Support for a Parallel File System, and &quot;Performance Validation of Nonlinear Controllers for Magnetic Bearings via Dynamic Simulation&apos;. Each of these projects require the unique combination of configurable hardware and cluster computing resources requested to obtain experimental results. The use of the equipment will be restricted to use by the faculty and students involved in these projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98599e+06</data>
      <data key="e_expirationDate">2002-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98599e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n125" target="n126">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Adaptive Computing Cluster</data>
      <data key="e_abstract">EIA-9985986&lt;br/&gt;Ronald R. Sass&lt;br/&gt;Clemson University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Adaptive Computing Cluster&lt;br/&gt;&lt;br/&gt;The Department of Electrical and Computer Engineering at Clemson University will purchase a cluster of workstations with reconfigurable computing to support research in cluster computing, reconfigurable computing, bioinformatics, and robotics, science and engineering. The equipment will be used for several research projects including in particular &quot;Adaptive Computing Cluster&quot;, Fast Sequence Searching on an Adaptable Computing Cluster&apos;, &quot;Reconfigurable Hardware Support for a Parallel File System, and &quot;Performance Validation of Nonlinear Controllers for Magnetic Bearings via Dynamic Simulation&apos;. Each of these projects require the unique combination of configurable hardware and cluster computing resources requested to obtain experimental results. The use of the equipment will be restricted to use by the faculty and students involved in these projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98599e+06</data>
      <data key="e_expirationDate">2002-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98599e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n125" target="n127">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Adaptive Computing Cluster</data>
      <data key="e_abstract">EIA-9985986&lt;br/&gt;Ronald R. Sass&lt;br/&gt;Clemson University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Adaptive Computing Cluster&lt;br/&gt;&lt;br/&gt;The Department of Electrical and Computer Engineering at Clemson University will purchase a cluster of workstations with reconfigurable computing to support research in cluster computing, reconfigurable computing, bioinformatics, and robotics, science and engineering. The equipment will be used for several research projects including in particular &quot;Adaptive Computing Cluster&quot;, Fast Sequence Searching on an Adaptable Computing Cluster&apos;, &quot;Reconfigurable Hardware Support for a Parallel File System, and &quot;Performance Validation of Nonlinear Controllers for Magnetic Bearings via Dynamic Simulation&apos;. Each of these projects require the unique combination of configurable hardware and cluster computing resources requested to obtain experimental results. The use of the equipment will be restricted to use by the faculty and students involved in these projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98599e+06</data>
      <data key="e_expirationDate">2002-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98599e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n126" target="n127">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Adaptive Computing Cluster</data>
      <data key="e_abstract">EIA-9985986&lt;br/&gt;Ronald R. Sass&lt;br/&gt;Clemson University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Adaptive Computing Cluster&lt;br/&gt;&lt;br/&gt;The Department of Electrical and Computer Engineering at Clemson University will purchase a cluster of workstations with reconfigurable computing to support research in cluster computing, reconfigurable computing, bioinformatics, and robotics, science and engineering. The equipment will be used for several research projects including in particular &quot;Adaptive Computing Cluster&quot;, Fast Sequence Searching on an Adaptable Computing Cluster&apos;, &quot;Reconfigurable Hardware Support for a Parallel File System, and &quot;Performance Validation of Nonlinear Controllers for Magnetic Bearings via Dynamic Simulation&apos;. Each of these projects require the unique combination of configurable hardware and cluster computing resources requested to obtain experimental results. The use of the equipment will be restricted to use by the faculty and students involved in these projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98599e+06</data>
      <data key="e_expirationDate">2002-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98599e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n130" target="n131">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">REU: Research Experiences In Computer Science for Students at Undergraduate Institutions</data>
      <data key="e_abstract">EIA-9911626&lt;br/&gt;Berque, David&lt;br/&gt;DePauw University&lt;br/&gt;&lt;br/&gt;Research Experiences for Undergraduates: Research Experiences In Computer Science for Students at Undergraduate Institutions&lt;br/&gt;&lt;br/&gt;Berque, DePauw University, CISE/REU sites The primary objective of this project is to encourage talented students enrolled at undergraduate institutions to pursue graduate study and research careers in Computer Science. The program directly impacts twenty-four students (eight per summer) by giving them first-hand experience with several of the most rewarding activities that characterize graduate study and research careers. research project topics are selected from the faculty mentors&apos; current areas of interest. Students have full access to DePauw University&apos;s library and computing facilities including specialized equipment such as a 72&quot; diagonal touchsensitive interactive whiteboard which supports some of the projects. Each student has several opportunities to present his/hers work more formally to other participants in the REU program, as well as to external audiences. Finally, the students have the chance to attend talks given by the faculty mentors and visitors, and to learn about the graduate school application process in part by visiting a major research university.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">9.91163e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.91163e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n135" target="n136">
      <data key="e_effectiveDate">2000-03-01</data>
      <data key="e_title">MLIAM: ISLE-International Standards for Language Engineering</data>
      <data key="e_abstract">This is the first year of funding of a 2 year continuing award. The increased interest in multilingual information processing, which requires detailed mappings between languages, has highlighted the need for international standards, and agreed upon evaluation/validation procedures. We can no longer afford to (re)develop language resources for each new application; a shared broad scale platform of basic components is an absolute necessity as a common infrastructure to ensure the interoperability of systems through compatible interfaces and components that can be readily integrated and reused (plug and play). This project provides a framework for achieving international consensus on essential standards that would enable the sharing of resources and components on a global scale. The PI&apos;s approach builds on an already existing methodology for achieving consensus that has been developedvithin the EAGLES standardization initiative in Europe (http://www.ilc.pi.cnr.it/EAGLES96/rep2.html). The PI will spearhead the formation of an equivalent American group, AIGLES, the American Interest Group on Language Engineering Standards (also French for eagles), that will join forces with the Europeans in the development of International Standardization for Language Engineering (ISLE). The underlying philosophy of the effort will be not to prove the truth or correctness of a particular theoretical approach, but rather to agree on a common format that can allow the merger of multiple sources of information. In order to move forward rapidly, coarse distinctions will be made initially and the results laterrefiner. (an approach that was anathema some years ago). The work will focus on three distinct areas the P1 considers the most critical for continued progress in multilingual information processing: standardization of multilingual lexicons, with lexical semantics; standardization of paradigms fornatural interaction inmultimodal systems; evaluation of machine translation systems and spoken language systems.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.9106e+06</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.9106e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n137" target="n138">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Multimedia Data Over Wireless Networks with Virtual Reality Applications</data>
      <data key="e_abstract">EIA-9911123&lt;br/&gt;Herman Hughes&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Multimedia Data over Wireless Networks with Virtual Reality Applications&lt;br/&gt;&lt;br/&gt;The Computer Science and Engineering and Telecommunication Department at Michigan State University (MSU) will purchase equipment for upgrading three existing research laboratories: High Speed Networking &amp; Performance Lab (HSNP), Media and Entertainment Technologies Lab (MET), and Media Interface and Network Design Lab (MIND). HSPN supports a testbed that consists of an assortment of ATM equipment and the research focus has been on ATM technology issues, including wireless ATM networks. MET has workstations equipped with video capture and presentation facilities and the research emphasis has been on ubiquitous access to multimedia databases. The MIND Lab has an array of virtual reality facilities and the current research examines communication and human-computer interaction in virtual environments. These three labs will conduct collaborative interdisciplinary research, with the primary emphasis on the support of multimedia data streams over both wireless and hybrid networks, with the multimedia data being generated/displayed by the research activities to take place in the MET and MIND Labs. &lt;br/&gt;&lt;br/&gt;These three labs will conduct collaborative interdisciplinary research, with the primary emphasis on the support of multimedia data streams over both wireless and hybrid networks, with the multimedia data being generated/displayed by the research activities to take place in the MET and MIND Labs. To carry out the proposed research MSU will purchase the following equipment: (1) t multimedia lap-tops, (2) 10Mbps WaveLan products, 2 video-on-demand servers, 2 head centered: 2 head&apos;s up status displays: info hemisphere, body centered data sphere, virtual tool belt, sensor array, lipstick camera, and microphones.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.91112e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.91112e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n137" target="n139">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Multimedia Data Over Wireless Networks with Virtual Reality Applications</data>
      <data key="e_abstract">EIA-9911123&lt;br/&gt;Herman Hughes&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Multimedia Data over Wireless Networks with Virtual Reality Applications&lt;br/&gt;&lt;br/&gt;The Computer Science and Engineering and Telecommunication Department at Michigan State University (MSU) will purchase equipment for upgrading three existing research laboratories: High Speed Networking &amp; Performance Lab (HSNP), Media and Entertainment Technologies Lab (MET), and Media Interface and Network Design Lab (MIND). HSPN supports a testbed that consists of an assortment of ATM equipment and the research focus has been on ATM technology issues, including wireless ATM networks. MET has workstations equipped with video capture and presentation facilities and the research emphasis has been on ubiquitous access to multimedia databases. The MIND Lab has an array of virtual reality facilities and the current research examines communication and human-computer interaction in virtual environments. These three labs will conduct collaborative interdisciplinary research, with the primary emphasis on the support of multimedia data streams over both wireless and hybrid networks, with the multimedia data being generated/displayed by the research activities to take place in the MET and MIND Labs. &lt;br/&gt;&lt;br/&gt;These three labs will conduct collaborative interdisciplinary research, with the primary emphasis on the support of multimedia data streams over both wireless and hybrid networks, with the multimedia data being generated/displayed by the research activities to take place in the MET and MIND Labs. To carry out the proposed research MSU will purchase the following equipment: (1) t multimedia lap-tops, (2) 10Mbps WaveLan products, 2 video-on-demand servers, 2 head centered: 2 head&apos;s up status displays: info hemisphere, body centered data sphere, virtual tool belt, sensor array, lipstick camera, and microphones.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.91112e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.91112e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n137" target="n140">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Multimedia Data Over Wireless Networks with Virtual Reality Applications</data>
      <data key="e_abstract">EIA-9911123&lt;br/&gt;Herman Hughes&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Multimedia Data over Wireless Networks with Virtual Reality Applications&lt;br/&gt;&lt;br/&gt;The Computer Science and Engineering and Telecommunication Department at Michigan State University (MSU) will purchase equipment for upgrading three existing research laboratories: High Speed Networking &amp; Performance Lab (HSNP), Media and Entertainment Technologies Lab (MET), and Media Interface and Network Design Lab (MIND). HSPN supports a testbed that consists of an assortment of ATM equipment and the research focus has been on ATM technology issues, including wireless ATM networks. MET has workstations equipped with video capture and presentation facilities and the research emphasis has been on ubiquitous access to multimedia databases. The MIND Lab has an array of virtual reality facilities and the current research examines communication and human-computer interaction in virtual environments. These three labs will conduct collaborative interdisciplinary research, with the primary emphasis on the support of multimedia data streams over both wireless and hybrid networks, with the multimedia data being generated/displayed by the research activities to take place in the MET and MIND Labs. &lt;br/&gt;&lt;br/&gt;These three labs will conduct collaborative interdisciplinary research, with the primary emphasis on the support of multimedia data streams over both wireless and hybrid networks, with the multimedia data being generated/displayed by the research activities to take place in the MET and MIND Labs. To carry out the proposed research MSU will purchase the following equipment: (1) t multimedia lap-tops, (2) 10Mbps WaveLan products, 2 video-on-demand servers, 2 head centered: 2 head&apos;s up status displays: info hemisphere, body centered data sphere, virtual tool belt, sensor array, lipstick camera, and microphones.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.91112e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.91112e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n138" target="n139">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Multimedia Data Over Wireless Networks with Virtual Reality Applications</data>
      <data key="e_abstract">EIA-9911123&lt;br/&gt;Herman Hughes&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Multimedia Data over Wireless Networks with Virtual Reality Applications&lt;br/&gt;&lt;br/&gt;The Computer Science and Engineering and Telecommunication Department at Michigan State University (MSU) will purchase equipment for upgrading three existing research laboratories: High Speed Networking &amp; Performance Lab (HSNP), Media and Entertainment Technologies Lab (MET), and Media Interface and Network Design Lab (MIND). HSPN supports a testbed that consists of an assortment of ATM equipment and the research focus has been on ATM technology issues, including wireless ATM networks. MET has workstations equipped with video capture and presentation facilities and the research emphasis has been on ubiquitous access to multimedia databases. The MIND Lab has an array of virtual reality facilities and the current research examines communication and human-computer interaction in virtual environments. These three labs will conduct collaborative interdisciplinary research, with the primary emphasis on the support of multimedia data streams over both wireless and hybrid networks, with the multimedia data being generated/displayed by the research activities to take place in the MET and MIND Labs. &lt;br/&gt;&lt;br/&gt;These three labs will conduct collaborative interdisciplinary research, with the primary emphasis on the support of multimedia data streams over both wireless and hybrid networks, with the multimedia data being generated/displayed by the research activities to take place in the MET and MIND Labs. To carry out the proposed research MSU will purchase the following equipment: (1) t multimedia lap-tops, (2) 10Mbps WaveLan products, 2 video-on-demand servers, 2 head centered: 2 head&apos;s up status displays: info hemisphere, body centered data sphere, virtual tool belt, sensor array, lipstick camera, and microphones.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.91112e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.91112e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n138" target="n140">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Multimedia Data Over Wireless Networks with Virtual Reality Applications</data>
      <data key="e_abstract">EIA-9911123&lt;br/&gt;Herman Hughes&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Multimedia Data over Wireless Networks with Virtual Reality Applications&lt;br/&gt;&lt;br/&gt;The Computer Science and Engineering and Telecommunication Department at Michigan State University (MSU) will purchase equipment for upgrading three existing research laboratories: High Speed Networking &amp; Performance Lab (HSNP), Media and Entertainment Technologies Lab (MET), and Media Interface and Network Design Lab (MIND). HSPN supports a testbed that consists of an assortment of ATM equipment and the research focus has been on ATM technology issues, including wireless ATM networks. MET has workstations equipped with video capture and presentation facilities and the research emphasis has been on ubiquitous access to multimedia databases. The MIND Lab has an array of virtual reality facilities and the current research examines communication and human-computer interaction in virtual environments. These three labs will conduct collaborative interdisciplinary research, with the primary emphasis on the support of multimedia data streams over both wireless and hybrid networks, with the multimedia data being generated/displayed by the research activities to take place in the MET and MIND Labs. &lt;br/&gt;&lt;br/&gt;These three labs will conduct collaborative interdisciplinary research, with the primary emphasis on the support of multimedia data streams over both wireless and hybrid networks, with the multimedia data being generated/displayed by the research activities to take place in the MET and MIND Labs. To carry out the proposed research MSU will purchase the following equipment: (1) t multimedia lap-tops, (2) 10Mbps WaveLan products, 2 video-on-demand servers, 2 head centered: 2 head&apos;s up status displays: info hemisphere, body centered data sphere, virtual tool belt, sensor array, lipstick camera, and microphones.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.91112e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.91112e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n139" target="n140">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Multimedia Data Over Wireless Networks with Virtual Reality Applications</data>
      <data key="e_abstract">EIA-9911123&lt;br/&gt;Herman Hughes&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Multimedia Data over Wireless Networks with Virtual Reality Applications&lt;br/&gt;&lt;br/&gt;The Computer Science and Engineering and Telecommunication Department at Michigan State University (MSU) will purchase equipment for upgrading three existing research laboratories: High Speed Networking &amp; Performance Lab (HSNP), Media and Entertainment Technologies Lab (MET), and Media Interface and Network Design Lab (MIND). HSPN supports a testbed that consists of an assortment of ATM equipment and the research focus has been on ATM technology issues, including wireless ATM networks. MET has workstations equipped with video capture and presentation facilities and the research emphasis has been on ubiquitous access to multimedia databases. The MIND Lab has an array of virtual reality facilities and the current research examines communication and human-computer interaction in virtual environments. These three labs will conduct collaborative interdisciplinary research, with the primary emphasis on the support of multimedia data streams over both wireless and hybrid networks, with the multimedia data being generated/displayed by the research activities to take place in the MET and MIND Labs. &lt;br/&gt;&lt;br/&gt;These three labs will conduct collaborative interdisciplinary research, with the primary emphasis on the support of multimedia data streams over both wireless and hybrid networks, with the multimedia data being generated/displayed by the research activities to take place in the MET and MIND Labs. To carry out the proposed research MSU will purchase the following equipment: (1) t multimedia lap-tops, (2) 10Mbps WaveLan products, 2 video-on-demand servers, 2 head centered: 2 head&apos;s up status displays: info hemisphere, body centered data sphere, virtual tool belt, sensor array, lipstick camera, and microphones.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.91112e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.91112e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n141" target="n142">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: A Multidisciplinary Design Testbed for Research and Education</data>
      <data key="e_abstract">EIA-9986015&lt;br/&gt;Spiros Mancoridis&lt;br/&gt;Drexel University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: A Multidisciplinary Design Testbed of Research and Education&lt;br/&gt;&lt;br/&gt;The design problems of the next century will require mixing diverse disciplines and computational tools in order to deliver new information services and physical products. The Department of Mathematics and Computer Science at Drexel University will address these challenges by building a design testbed that connects facilities and researchers engaged in diverse but complementary research projects; most importantly: (1) collaborative engineering design (2) intelligent mobile robot design (3) software design recovery (4) design of symbolic and scientific Problem Solving Environments. This testbed for education and research on multi-disciplinary design problems will consist of a gigabit network backbone (1 gigabit switch, gigabit Ethernet cards), shared computing resources (1 Sun compute server, 1 Sun disk server, 1 Sag Electronics RAID, 5 SGI workstations, 2 Dell workstations) and mobile robot experimentation materials (8 Lego kits, 1 Nomad Super Scout II).</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98602e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98602e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n141" target="n143">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: A Multidisciplinary Design Testbed for Research and Education</data>
      <data key="e_abstract">EIA-9986015&lt;br/&gt;Spiros Mancoridis&lt;br/&gt;Drexel University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: A Multidisciplinary Design Testbed of Research and Education&lt;br/&gt;&lt;br/&gt;The design problems of the next century will require mixing diverse disciplines and computational tools in order to deliver new information services and physical products. The Department of Mathematics and Computer Science at Drexel University will address these challenges by building a design testbed that connects facilities and researchers engaged in diverse but complementary research projects; most importantly: (1) collaborative engineering design (2) intelligent mobile robot design (3) software design recovery (4) design of symbolic and scientific Problem Solving Environments. This testbed for education and research on multi-disciplinary design problems will consist of a gigabit network backbone (1 gigabit switch, gigabit Ethernet cards), shared computing resources (1 Sun compute server, 1 Sun disk server, 1 Sag Electronics RAID, 5 SGI workstations, 2 Dell workstations) and mobile robot experimentation materials (8 Lego kits, 1 Nomad Super Scout II).</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98602e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98602e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n141" target="n144">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: A Multidisciplinary Design Testbed for Research and Education</data>
      <data key="e_abstract">EIA-9986015&lt;br/&gt;Spiros Mancoridis&lt;br/&gt;Drexel University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: A Multidisciplinary Design Testbed of Research and Education&lt;br/&gt;&lt;br/&gt;The design problems of the next century will require mixing diverse disciplines and computational tools in order to deliver new information services and physical products. The Department of Mathematics and Computer Science at Drexel University will address these challenges by building a design testbed that connects facilities and researchers engaged in diverse but complementary research projects; most importantly: (1) collaborative engineering design (2) intelligent mobile robot design (3) software design recovery (4) design of symbolic and scientific Problem Solving Environments. This testbed for education and research on multi-disciplinary design problems will consist of a gigabit network backbone (1 gigabit switch, gigabit Ethernet cards), shared computing resources (1 Sun compute server, 1 Sun disk server, 1 Sag Electronics RAID, 5 SGI workstations, 2 Dell workstations) and mobile robot experimentation materials (8 Lego kits, 1 Nomad Super Scout II).</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98602e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98602e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n142" target="n143">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: A Multidisciplinary Design Testbed for Research and Education</data>
      <data key="e_abstract">EIA-9986015&lt;br/&gt;Spiros Mancoridis&lt;br/&gt;Drexel University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: A Multidisciplinary Design Testbed of Research and Education&lt;br/&gt;&lt;br/&gt;The design problems of the next century will require mixing diverse disciplines and computational tools in order to deliver new information services and physical products. The Department of Mathematics and Computer Science at Drexel University will address these challenges by building a design testbed that connects facilities and researchers engaged in diverse but complementary research projects; most importantly: (1) collaborative engineering design (2) intelligent mobile robot design (3) software design recovery (4) design of symbolic and scientific Problem Solving Environments. This testbed for education and research on multi-disciplinary design problems will consist of a gigabit network backbone (1 gigabit switch, gigabit Ethernet cards), shared computing resources (1 Sun compute server, 1 Sun disk server, 1 Sag Electronics RAID, 5 SGI workstations, 2 Dell workstations) and mobile robot experimentation materials (8 Lego kits, 1 Nomad Super Scout II).</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98602e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98602e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n142" target="n144">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: A Multidisciplinary Design Testbed for Research and Education</data>
      <data key="e_abstract">EIA-9986015&lt;br/&gt;Spiros Mancoridis&lt;br/&gt;Drexel University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: A Multidisciplinary Design Testbed of Research and Education&lt;br/&gt;&lt;br/&gt;The design problems of the next century will require mixing diverse disciplines and computational tools in order to deliver new information services and physical products. The Department of Mathematics and Computer Science at Drexel University will address these challenges by building a design testbed that connects facilities and researchers engaged in diverse but complementary research projects; most importantly: (1) collaborative engineering design (2) intelligent mobile robot design (3) software design recovery (4) design of symbolic and scientific Problem Solving Environments. This testbed for education and research on multi-disciplinary design problems will consist of a gigabit network backbone (1 gigabit switch, gigabit Ethernet cards), shared computing resources (1 Sun compute server, 1 Sun disk server, 1 Sag Electronics RAID, 5 SGI workstations, 2 Dell workstations) and mobile robot experimentation materials (8 Lego kits, 1 Nomad Super Scout II).</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98602e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98602e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n143" target="n144">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: A Multidisciplinary Design Testbed for Research and Education</data>
      <data key="e_abstract">EIA-9986015&lt;br/&gt;Spiros Mancoridis&lt;br/&gt;Drexel University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: A Multidisciplinary Design Testbed of Research and Education&lt;br/&gt;&lt;br/&gt;The design problems of the next century will require mixing diverse disciplines and computational tools in order to deliver new information services and physical products. The Department of Mathematics and Computer Science at Drexel University will address these challenges by building a design testbed that connects facilities and researchers engaged in diverse but complementary research projects; most importantly: (1) collaborative engineering design (2) intelligent mobile robot design (3) software design recovery (4) design of symbolic and scientific Problem Solving Environments. This testbed for education and research on multi-disciplinary design problems will consist of a gigabit network backbone (1 gigabit switch, gigabit Ethernet cards), shared computing resources (1 Sun compute server, 1 Sun disk server, 1 Sag Electronics RAID, 5 SGI workstations, 2 Dell workstations) and mobile robot experimentation materials (8 Lego kits, 1 Nomad Super Scout II).</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98602e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98602e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n145" target="n146">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: A Specialized Computing Environment for Distributed and Virtual Design and Manufacturing</data>
      <data key="e_abstract">EIA-9986012&lt;br/&gt;Dana S. Nau&lt;br/&gt;University of Maryland-College Park&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: A Specialized Computing Environment for Distributed and Virtual Design and Manufacturing.&lt;br/&gt;&lt;br/&gt;The University of Maryland will create a specialized computing environment for distributed and virtual design and manufacturing. This facility will support research in computer and information science sand engineering. The University of Maryland will purchase two graphics workstations, two NT-based servers, and the necessary software for developing the facility. The computing environment will be dedicated to several on-going projects on distributed and virtual design and manufacturing. These projects will yield specific contributions in areas related to computing and information systems. With this facility, the investigators can install and integrate our approaches, share data across various projects effectively, and test our results more thoroughly. Thus we can study and solve the computational challenges that will occur as manufacturers develop advanced, information-based product realization processes</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98601e+06</data>
      <data key="e_expirationDate">2002-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98601e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n145" target="n147">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: A Specialized Computing Environment for Distributed and Virtual Design and Manufacturing</data>
      <data key="e_abstract">EIA-9986012&lt;br/&gt;Dana S. Nau&lt;br/&gt;University of Maryland-College Park&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: A Specialized Computing Environment for Distributed and Virtual Design and Manufacturing.&lt;br/&gt;&lt;br/&gt;The University of Maryland will create a specialized computing environment for distributed and virtual design and manufacturing. This facility will support research in computer and information science sand engineering. The University of Maryland will purchase two graphics workstations, two NT-based servers, and the necessary software for developing the facility. The computing environment will be dedicated to several on-going projects on distributed and virtual design and manufacturing. These projects will yield specific contributions in areas related to computing and information systems. With this facility, the investigators can install and integrate our approaches, share data across various projects effectively, and test our results more thoroughly. Thus we can study and solve the computational challenges that will occur as manufacturers develop advanced, information-based product realization processes</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98601e+06</data>
      <data key="e_expirationDate">2002-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98601e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n146" target="n147">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: A Specialized Computing Environment for Distributed and Virtual Design and Manufacturing</data>
      <data key="e_abstract">EIA-9986012&lt;br/&gt;Dana S. Nau&lt;br/&gt;University of Maryland-College Park&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: A Specialized Computing Environment for Distributed and Virtual Design and Manufacturing.&lt;br/&gt;&lt;br/&gt;The University of Maryland will create a specialized computing environment for distributed and virtual design and manufacturing. This facility will support research in computer and information science sand engineering. The University of Maryland will purchase two graphics workstations, two NT-based servers, and the necessary software for developing the facility. The computing environment will be dedicated to several on-going projects on distributed and virtual design and manufacturing. These projects will yield specific contributions in areas related to computing and information systems. With this facility, the investigators can install and integrate our approaches, share data across various projects effectively, and test our results more thoroughly. Thus we can study and solve the computational challenges that will occur as manufacturers develop advanced, information-based product realization processes</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98601e+06</data>
      <data key="e_expirationDate">2002-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98601e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n149" target="n150">
      <data key="e_effectiveDate">2000-03-01</data>
      <data key="e_title">Compiler and Runtime Support for Data Intensive Computing on Multi-dimensional Data</data>
      <data key="e_abstract">One of the largest and fastest-growing problems in scientific computing is the analysis and processing of very large data sets. These scientific data sets can come from long-running simulations (e.g. simulations of water pollution that create &quot;snapshots&quot; of the expected water conditions at later times), archives of remote sensing data (e.g. high-resolution satellite imagery), and archives of medical images (e.g. MRI scans for a patient or group of patients). These data sets are usually multi-dimensional, including spatial coordinates, time stamps, and several physical properties at each point. Several systems now support storage, retrieval, and visualization of such data sets, but few can efficiently process the data. This project will develop methods to produce efficient programs to carry out multi-dimensional data processing and analysis using a high-level parallel language.&lt;br/&gt;&lt;br/&gt;The project will attack this problem by developing runtime routines for optimizing resource usage, appropriate language extensions, and aggressive compiler optimizations for large data processing. The runtime methods will implement policies that optimize computational efficiency on a broad range of large data set analyses, taking into account the spatial structure and partitioning of the data and the computation to be performed. Incorporating these routines into the investigator&apos;s Active Data Repository will substantially generalize and improve that system. The language extensions and compiler optimizations will then make use of the runtime system to enable applications that analyze multi-dimensional data sets to be expressed at an abstract level, yet achieve high utilization of computational, storage, and communication resources.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">9.98209e+06</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98209e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n152" target="n153">
      <data key="e_effectiveDate">2000-03-01</data>
      <data key="e_title">Fundamental Limits for Information Retrieval</data>
      <data key="e_abstract">The fundamental limits of performance for a general model of information retrieval from databases are studied. In the scenarios to be considered a large quantity of information is to be stored on some physical storage device. Requests for information are modeled as randomly generated sequences with a known distributions. The requests are assumed to be &quot;context-dependent&quot;, i.e., to vary according to the sequence of previous request. The state of the physical storage device is also assumed to depend on the history of previous requests.&lt;br/&gt;&lt;br/&gt;In general the logical structure of the information to be stored does not match the physical structure of the storage device, and consequently there are nontrivial limits on the minimum achievable average access times, where the average is over the possible sequences of user request. The propose research will apply information-theoretic methods to establish these limits. Preliminary results demonstrate the potential of such methods for this problem, giving very accurate results for some (relatively general) first cases.&lt;br/&gt;&lt;br/&gt;Allowing redundancy in general can greatly lower the achievable access times, even when the amount added is an arbitrarily small fraction of the total amount of information in the dababase. Preliminary results establish the achievable limits both with and without redundancy for some simplified cases; very many interesting problems arise in general.</data>
      <data key="e_pgm">4096</data>
      <data key="e_label">9.98042e+06</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98042e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n154" target="n155">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">Digital Government: SGER: Evaluating Information Integration Architectures for a National Statistical Data Infrastructure</data>
      <data key="e_abstract">EIA-0073578&lt;br/&gt;Baru, Chaitanya&lt;br/&gt;University of California, San Diego&lt;br/&gt;&lt;br/&gt;Digital Government: SGER: Evaluating Information Integration Architectures for a National Statistical Data Infrastructure&lt;br/&gt;&lt;br/&gt;This grant will evaluate alternative information integration architectures, using as a testbed the Federal Electronic Research and Review Tool (FERRET). FERRET has been developed by the US Bureau of the Census, Bureau of Labor Statistics, and Centers for Disease Control to support dataset extraction and cross-tabulation of data collected by the Census&apos; Current Population Survey. An additional partner will be the San Diego Association of Governments. The goals are to evaluate the scalability of the FERRET architecture and consider how emerging technologies such XML may be applied within that architecture.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">73578</data>
      <data key="e_expirationDate">2000-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">73578</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n157" target="n158">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: System Support for Scalable, Fault-Tolerant Computing and Services on PC Clusters</data>
      <data key="e_abstract">EIA-9986046&lt;br/&gt;Thu D. Nguyen&lt;br/&gt;Rutgers University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: System Support for Scalable, Fault-Tolerant Computing and Service on PC Clusters&lt;br/&gt;&lt;br/&gt;A high-end PC cluster is the crucial computing infrastructure for research in the recently established Distributed Computing Laboratory (DISCO Lab) in the Department of Computer Science at Rutgers University. The requested cluster consists of 8 quad-processor PCs, a 16-port Alteon Gb/s Ethernet LAN with layer 4 switching capabilities and a 16 NICs, and a 16-port Myrinet Gb/s LAN with 8 NICs. This cluster will support research in several projects that jointly, are intended to improve the state-of-the-art in cluster computing, moving towards the realization of a robust and efficient distributed computing environment for clusters of PCs. In particular, this proposal describes four projects: (i) developing a robust software distributed shared-memory environment to support emerging cluster applications, (ii) building system support for efficient global utilization of cluster resources (iii) exploring the construction of highly available operating systems, and (iv) implementing distributed applications such as data mining, scalable servers, and interactive continuous media applications and system mechanisms and policies to support them efficiently on clusters.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98605e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98605e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n0" target="n157">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: System Support for Scalable, Fault-Tolerant Computing and Services on PC Clusters</data>
      <data key="e_abstract">EIA-9986046&lt;br/&gt;Thu D. Nguyen&lt;br/&gt;Rutgers University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: System Support for Scalable, Fault-Tolerant Computing and Service on PC Clusters&lt;br/&gt;&lt;br/&gt;A high-end PC cluster is the crucial computing infrastructure for research in the recently established Distributed Computing Laboratory (DISCO Lab) in the Department of Computer Science at Rutgers University. The requested cluster consists of 8 quad-processor PCs, a 16-port Alteon Gb/s Ethernet LAN with layer 4 switching capabilities and a 16 NICs, and a 16-port Myrinet Gb/s LAN with 8 NICs. This cluster will support research in several projects that jointly, are intended to improve the state-of-the-art in cluster computing, moving towards the realization of a robust and efficient distributed computing environment for clusters of PCs. In particular, this proposal describes four projects: (i) developing a robust software distributed shared-memory environment to support emerging cluster applications, (ii) building system support for efficient global utilization of cluster resources (iii) exploring the construction of highly available operating systems, and (iv) implementing distributed applications such as data mining, scalable servers, and interactive continuous media applications and system mechanisms and policies to support them efficiently on clusters.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98605e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98605e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n157" target="n160">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: System Support for Scalable, Fault-Tolerant Computing and Services on PC Clusters</data>
      <data key="e_abstract">EIA-9986046&lt;br/&gt;Thu D. Nguyen&lt;br/&gt;Rutgers University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: System Support for Scalable, Fault-Tolerant Computing and Service on PC Clusters&lt;br/&gt;&lt;br/&gt;A high-end PC cluster is the crucial computing infrastructure for research in the recently established Distributed Computing Laboratory (DISCO Lab) in the Department of Computer Science at Rutgers University. The requested cluster consists of 8 quad-processor PCs, a 16-port Alteon Gb/s Ethernet LAN with layer 4 switching capabilities and a 16 NICs, and a 16-port Myrinet Gb/s LAN with 8 NICs. This cluster will support research in several projects that jointly, are intended to improve the state-of-the-art in cluster computing, moving towards the realization of a robust and efficient distributed computing environment for clusters of PCs. In particular, this proposal describes four projects: (i) developing a robust software distributed shared-memory environment to support emerging cluster applications, (ii) building system support for efficient global utilization of cluster resources (iii) exploring the construction of highly available operating systems, and (iv) implementing distributed applications such as data mining, scalable servers, and interactive continuous media applications and system mechanisms and policies to support them efficiently on clusters.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98605e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98605e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n0" target="n158">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: System Support for Scalable, Fault-Tolerant Computing and Services on PC Clusters</data>
      <data key="e_abstract">EIA-9986046&lt;br/&gt;Thu D. Nguyen&lt;br/&gt;Rutgers University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: System Support for Scalable, Fault-Tolerant Computing and Service on PC Clusters&lt;br/&gt;&lt;br/&gt;A high-end PC cluster is the crucial computing infrastructure for research in the recently established Distributed Computing Laboratory (DISCO Lab) in the Department of Computer Science at Rutgers University. The requested cluster consists of 8 quad-processor PCs, a 16-port Alteon Gb/s Ethernet LAN with layer 4 switching capabilities and a 16 NICs, and a 16-port Myrinet Gb/s LAN with 8 NICs. This cluster will support research in several projects that jointly, are intended to improve the state-of-the-art in cluster computing, moving towards the realization of a robust and efficient distributed computing environment for clusters of PCs. In particular, this proposal describes four projects: (i) developing a robust software distributed shared-memory environment to support emerging cluster applications, (ii) building system support for efficient global utilization of cluster resources (iii) exploring the construction of highly available operating systems, and (iv) implementing distributed applications such as data mining, scalable servers, and interactive continuous media applications and system mechanisms and policies to support them efficiently on clusters.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98605e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98605e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n158" target="n160">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: System Support for Scalable, Fault-Tolerant Computing and Services on PC Clusters</data>
      <data key="e_abstract">EIA-9986046&lt;br/&gt;Thu D. Nguyen&lt;br/&gt;Rutgers University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: System Support for Scalable, Fault-Tolerant Computing and Service on PC Clusters&lt;br/&gt;&lt;br/&gt;A high-end PC cluster is the crucial computing infrastructure for research in the recently established Distributed Computing Laboratory (DISCO Lab) in the Department of Computer Science at Rutgers University. The requested cluster consists of 8 quad-processor PCs, a 16-port Alteon Gb/s Ethernet LAN with layer 4 switching capabilities and a 16 NICs, and a 16-port Myrinet Gb/s LAN with 8 NICs. This cluster will support research in several projects that jointly, are intended to improve the state-of-the-art in cluster computing, moving towards the realization of a robust and efficient distributed computing environment for clusters of PCs. In particular, this proposal describes four projects: (i) developing a robust software distributed shared-memory environment to support emerging cluster applications, (ii) building system support for efficient global utilization of cluster resources (iii) exploring the construction of highly available operating systems, and (iv) implementing distributed applications such as data mining, scalable servers, and interactive continuous media applications and system mechanisms and policies to support them efficiently on clusters.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98605e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98605e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n0" target="n160">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: System Support for Scalable, Fault-Tolerant Computing and Services on PC Clusters</data>
      <data key="e_abstract">EIA-9986046&lt;br/&gt;Thu D. Nguyen&lt;br/&gt;Rutgers University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: System Support for Scalable, Fault-Tolerant Computing and Service on PC Clusters&lt;br/&gt;&lt;br/&gt;A high-end PC cluster is the crucial computing infrastructure for research in the recently established Distributed Computing Laboratory (DISCO Lab) in the Department of Computer Science at Rutgers University. The requested cluster consists of 8 quad-processor PCs, a 16-port Alteon Gb/s Ethernet LAN with layer 4 switching capabilities and a 16 NICs, and a 16-port Myrinet Gb/s LAN with 8 NICs. This cluster will support research in several projects that jointly, are intended to improve the state-of-the-art in cluster computing, moving towards the realization of a robust and efficient distributed computing environment for clusters of PCs. In particular, this proposal describes four projects: (i) developing a robust software distributed shared-memory environment to support emerging cluster applications, (ii) building system support for efficient global utilization of cluster resources (iii) exploring the construction of highly available operating systems, and (iv) implementing distributed applications such as data mining, scalable servers, and interactive continuous media applications and system mechanisms and policies to support them efficiently on clusters.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98605e+06</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98605e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n161" target="n162">
      <data key="e_effectiveDate">2000-03-15</data>
      <data key="e_title">CISE Research Instrumentation: Configurable off-the-shelf Multithreaded Experimental Testbed (COMET)</data>
      <data key="e_abstract">EIA-9986043&lt;br/&gt;Guang R. Gao&lt;br/&gt;University of Delaware&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: CONFIGURABLE OFF-THE SHELF MULTITHREADED EXPERIMENTAL TESTBED (COMET)&lt;br/&gt;&lt;br/&gt;The Department of Electrical and Computer Engineering at the University of Delaware will purchase FPGA (Field-Programmable Gate Array) design boards, a Beowulf cluster of commercial off-the-shelf (COTS) PC&apos;s and necessary accessories such as an oscilloscope. The FPGA units will form the Multithreaded Operator for Of-the shelf Nodes (MOON), which will be used for prototyping multithreading enhancements to COTS-based machines. The PC cluster will provide a baseline for comparing the performance of enhanced and non-enhanced clusters. The combination of the MOON cards and the cluster will form COMET. The test equipment will permit the development of a custom interconnect between the MOON board, and will be used for observing and debugging high-speed signals.&lt;br/&gt;&lt;br/&gt;This equipment will be used to test the thesis, supported by previous simulations, that a fine-grain multithreading model supporting both regular and irregular applications can be implemented on COTS systems if a small amount of custom hardware is used to support the operations specific to the multithreading model. The Moon would provide a flexible and multi-faceted prototyping system for verifying these results in real system, and for supporting other, more advanced multithreading models, which are specific to ongoing research projects.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98604e+06</data>
      <data key="e_expirationDate">2002-02-28</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98604e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n163" target="n164">
      <data key="e_effectiveDate">2000-03-01</data>
      <data key="e_title">Exploring the Role of Grid-Enabled OpenMP in Adaptive Mesh Calculations</data>
      <data key="e_abstract">Advances in computer and network technologies will soon make it possible to harness the collective computing power of distributed supercomputer systems as a Computational Power Grid (or Grid for short). Grid computing will entail coping with unusually high latencies and low bandwidths. To use such a Grid effectively as a platform for scientific computing will require new algorithms and programming methodologies along with compiler and run-time support to simplify the program development process.&lt;br/&gt;&lt;br/&gt;The scope of this problem is so broad that we believe that effective technologies can best be developed through focusing on concrete application domains. This proposal will perform an exploratory study to explore the development of compiler and runtime technology as well as algorithmic techniques for supporting largescale, parallel, structured adaptive mesh refinement (AMR) calculations in Computational Fluid Dynamics (CID) on the Grid. The project will be an interdisciplinary study consisting of applications scientists from NCSA and computer scientists from Rice and University of Houston.&lt;br/&gt;&lt;br/&gt;This proposal, therefore, will carry out a study to see whether such highly complex AMR applications can be efficiently run on the Grid. The course of this work will be exploring several different algorithmic strategies for supporting AMR on Grids. This will also entail developing innovative compiler technology and language extensions to the OpenMP standard that allow it to provide Grid-based support for such AMR applications. Lastly, resources (nodes, network connections) in the Grid computational environment are often contended. Therefore a Grid-enabled application has to be able to speculatively adapt to changing Grid resources. The proposal will build up tools that monitor the application&apos;s performance on the Grid as well as the available Grid resources. Information from these tools will be made available to the AMR application through standard interfaces in our OpenMP API so that the AMR application can speculatively adapt to changing Grid resources.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">9.98216e+06</data>
      <data key="e_expirationDate">2003-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98216e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n169" target="n170">
      <data key="e_effectiveDate">2000-03-01</data>
      <data key="e_title">Design Principles for Wideband Wireless Communications</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;Broadband wireless communications has been identified as a national&lt;br/&gt;imperative; it is deemed essential for maintaining the country&apos;s&lt;br/&gt;premier position in information technology and accelerating the&lt;br/&gt;advance of the information age. This research is aimed at enhancing&lt;br/&gt;the performance of wideband wireless multiaccess systems by optimizing&lt;br/&gt;tradeoffs between coding and spreading, capitalizing on advantages&lt;br/&gt;afforded by spatial diversity, and developing techniques for&lt;br/&gt;accommodating multirate users.&lt;br/&gt;&lt;br/&gt;A goal of this research is to establish, through a careful modeling of&lt;br/&gt;time dispersion and angular dispersion effects in wideband channels,&lt;br/&gt;that signals spread continuously over the available time interval and&lt;br/&gt;frequency band can send data with high reliability. When spatial&lt;br/&gt;macrodiversity also is available, the investigators contend that&lt;br/&gt;signals should be coded over spatial and spectral dimensions and their&lt;br/&gt;energy spread &quot;continuously&quot; over time, frequency and space.&lt;br/&gt;&lt;br/&gt;Linear signal space separation schemes have received considerable&lt;br/&gt;attention as approaches for providing good tradeoffs&lt;br/&gt;between throughput and system complexity in single-cell multiuser&lt;br/&gt;environments. The efficacy of linear signal space separation schemes&lt;br/&gt;diminishes substantially, however, when one generalizes to&lt;br/&gt;multiple-cell environments. The goal in such realistic multicell&lt;br/&gt;interference channels is to maximize a certain extension of classical&lt;br/&gt;spectral efficiency herein called area-spectral efficiency. This research&lt;br/&gt;investigates the validity of the contention that signals widely&lt;br/&gt;dispersed in time, frequency and space (TFS-wide signals) are nearly&lt;br/&gt;optimum in such environments and do not require the use of linear&lt;br/&gt;signal separation at the receiver.&lt;br/&gt;&lt;br/&gt;The investigators study various techniques for creating &lt;br/&gt;TFS-wide signals in the wideband wireless environment, with an&lt;br/&gt;emphasis on multicarrier signaling approaches. Provided the&lt;br/&gt;forward channel does not fade quickly relative to the capacity of the&lt;br/&gt;channel available for feeding back information from the receiver(s) to&lt;br/&gt;the transmitter(s), DMT (discrete multitone) with dynamic bit loading&lt;br/&gt;should prove to be a viable data transmission technique. Combined&lt;br/&gt;with spreading that enables multiple access, DMT can be made to&lt;br/&gt;provide both user separation and Shannon-optimal power distribution&lt;br/&gt;relative to the short-term spectrum of the wideband channel with&lt;br/&gt;fading and interference seen by each user. For cases where dynamic bit&lt;br/&gt;loading proves infeasible, OFDM (i.e., DMT with an equal number of&lt;br/&gt;bits in each subchannel) is being explored as a candidate modulation&lt;br/&gt;scheme for broadband wireless signaling.</data>
      <data key="e_pgm">4096</data>
      <data key="e_label">9.98062e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98062e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n183" target="n184">
      <data key="e_effectiveDate">2000-04-01</data>
      <data key="e_title">Textual Information Access for the Visually Impaired</data>
      <data key="e_abstract">An ever-increasing segment of the population suffers from low vision resulting from complications of disease and old age. Surveys conducted by one of the Co-PIs as part of a previous project have determined, that the key information which is not available to people with low vision is textual information, usually of a directive or warning nature. For example, shopping in a large department store in a mall might involve looking for signs indicating where the store is, reading aisle signs in the store, and looking at product names, at labels and prices. This research will develop a &quot;seeing-eye&quot; computer to help people with low vision to observe and receive such information, so that they can participate more efficiently and comfortably in every day activities, and thereby lead more fulfilling and productive lives. The system will be composed of a digital video camera, computer, user interface, and speech or magnified visual output that can detect textual information in the environment, understand it using OCR, and provide it to the user who either has low vision or is blind. To achieve these goals, the PIs will in collaboration with colleagues at Johns Hopkins University build, over the first six months and then over the first two years, prototype systems using mostly existing technology and extensions to vision algorithms we have developed for identification of text regions in images and OCR, which can be evaluated on volunteer patients at the Wilmer Ophthalmological Institute and the National Federation for the Blind.. The functionality and range of applicability of our prototypes will necessarily be limited. Simultaneously, the PIs will work on long-term research problems that must be addressed to develop next generation seeing-eye computers with greater scalability and capability. In year three patient-volunteers at Wilmer and at the NFB will perform evaluations of the developed prototypes Subsequently, successful results will be commercialized and brought to the larger patient body (as have previous developments at Wilmer). Fundamental research problems to be addressed include: real-time algorithms for detection and rectification of text on planes and cylinders subject to perspective distortions; OCR from digital video, and OCR for text on textured backgrounds; and more robust and efficient algorithms and systems for stabilization and super-resolution of text blocks from video streams.</data>
      <data key="e_pgm">6846</data>
      <data key="e_label">9.98794e+06</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98794e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n183" target="n185">
      <data key="e_effectiveDate">2000-04-01</data>
      <data key="e_title">Textual Information Access for the Visually Impaired</data>
      <data key="e_abstract">An ever-increasing segment of the population suffers from low vision resulting from complications of disease and old age. Surveys conducted by one of the Co-PIs as part of a previous project have determined, that the key information which is not available to people with low vision is textual information, usually of a directive or warning nature. For example, shopping in a large department store in a mall might involve looking for signs indicating where the store is, reading aisle signs in the store, and looking at product names, at labels and prices. This research will develop a &quot;seeing-eye&quot; computer to help people with low vision to observe and receive such information, so that they can participate more efficiently and comfortably in every day activities, and thereby lead more fulfilling and productive lives. The system will be composed of a digital video camera, computer, user interface, and speech or magnified visual output that can detect textual information in the environment, understand it using OCR, and provide it to the user who either has low vision or is blind. To achieve these goals, the PIs will in collaboration with colleagues at Johns Hopkins University build, over the first six months and then over the first two years, prototype systems using mostly existing technology and extensions to vision algorithms we have developed for identification of text regions in images and OCR, which can be evaluated on volunteer patients at the Wilmer Ophthalmological Institute and the National Federation for the Blind.. The functionality and range of applicability of our prototypes will necessarily be limited. Simultaneously, the PIs will work on long-term research problems that must be addressed to develop next generation seeing-eye computers with greater scalability and capability. In year three patient-volunteers at Wilmer and at the NFB will perform evaluations of the developed prototypes Subsequently, successful results will be commercialized and brought to the larger patient body (as have previous developments at Wilmer). Fundamental research problems to be addressed include: real-time algorithms for detection and rectification of text on planes and cylinders subject to perspective distortions; OCR from digital video, and OCR for text on textured backgrounds; and more robust and efficient algorithms and systems for stabilization and super-resolution of text blocks from video streams.</data>
      <data key="e_pgm">6846</data>
      <data key="e_label">9.98794e+06</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98794e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n184" target="n185">
      <data key="e_effectiveDate">2000-04-01</data>
      <data key="e_title">Textual Information Access for the Visually Impaired</data>
      <data key="e_abstract">An ever-increasing segment of the population suffers from low vision resulting from complications of disease and old age. Surveys conducted by one of the Co-PIs as part of a previous project have determined, that the key information which is not available to people with low vision is textual information, usually of a directive or warning nature. For example, shopping in a large department store in a mall might involve looking for signs indicating where the store is, reading aisle signs in the store, and looking at product names, at labels and prices. This research will develop a &quot;seeing-eye&quot; computer to help people with low vision to observe and receive such information, so that they can participate more efficiently and comfortably in every day activities, and thereby lead more fulfilling and productive lives. The system will be composed of a digital video camera, computer, user interface, and speech or magnified visual output that can detect textual information in the environment, understand it using OCR, and provide it to the user who either has low vision or is blind. To achieve these goals, the PIs will in collaboration with colleagues at Johns Hopkins University build, over the first six months and then over the first two years, prototype systems using mostly existing technology and extensions to vision algorithms we have developed for identification of text regions in images and OCR, which can be evaluated on volunteer patients at the Wilmer Ophthalmological Institute and the National Federation for the Blind.. The functionality and range of applicability of our prototypes will necessarily be limited. Simultaneously, the PIs will work on long-term research problems that must be addressed to develop next generation seeing-eye computers with greater scalability and capability. In year three patient-volunteers at Wilmer and at the NFB will perform evaluations of the developed prototypes Subsequently, successful results will be commercialized and brought to the larger patient body (as have previous developments at Wilmer). Fundamental research problems to be addressed include: real-time algorithms for detection and rectification of text on planes and cylinders subject to perspective distortions; OCR from digital video, and OCR for text on textured backgrounds; and more robust and efficient algorithms and systems for stabilization and super-resolution of text blocks from video streams.</data>
      <data key="e_pgm">6846</data>
      <data key="e_label">9.98794e+06</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98794e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n190" target="n191">
      <data key="e_effectiveDate">2000-04-01</data>
      <data key="e_title">REU: Robot Navigation using Composite Landmarks</data>
      <data key="e_abstract">EIA-9912247&lt;br/&gt;Sutherland, Karen&lt;br/&gt;Augsburg College&lt;br/&gt;&lt;br/&gt;Research Experiences for Undergraduates: Robot Navigation using Composite Landmarks&lt;br/&gt; &lt;br/&gt; These research projects draw from the areas of robotics, behavior learning and cognitive science within an environment of robot navigation. The PIs use small robotic vehicles to test their algorithms and model the behavior of the desert ant, Cataglyphis Bicolor. This ant uses landmarks consisting of more than one point (composite) as opposed to simple point landmarks. Examples of composite landmarks in everyday environment are ridgelines, saddle points and peak formations. Projects focus on development and testing of algorithms for recognizing composite landmarks, development of learning algorithms to map sensory input to motor output, and comparing the use of composite versus simple landmarks. The projects include building Lego test vehicles, writing simulations and implementation of image processing techniques. The program has an emphasis on technical writing and speaking skills.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">9.91225e+06</data>
      <data key="e_expirationDate">2003-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.91225e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n194" target="n195">
      <data key="e_effectiveDate">2000-04-01</data>
      <data key="e_title">REU: REU CRISP (Computer Research Internship Scholastic Program)</data>
      <data key="e_abstract">EIA -9912218&lt;br/&gt; Manoj, Franklin&lt;br/&gt; Shuvra Shikhar, Bhattacharyya&lt;br/&gt;University of Maryland, College Park&lt;br/&gt;&lt;br/&gt;Research Experiences for Undergraduates: REU CRISP (Computer Research Internship Scholastic Program)&lt;br/&gt;&lt;br/&gt; The Computer Research Intership Scholarstic Program (CRISP) incorporates research projects in the following areas of research: Computer Architecture, Embedded Systesm, Software Engineering, Real-time Systems, VLSI Design, CAD for VLSI, LowPower Design, and Optimizing Compilers. In addition to work in the lab, students participate in weekly meetings that include technical presentations from leaders in computers and visits to local companies. These activities are done jointly with two other undergraduate research programs currently undertaken by the department under the MERIT (Maryland Engineering Research Intership Teams) pogram. The professional d development activities consist of seminars/workshops on technical writing, oral communcations, graduate educations, academic careers, industrial careers, and professional ethics. At the end of the summer, representatives from indsutry serve as judges for the MERIT fair where students from the CRISP program and other similar research experience programs present the results of their research and compete against each other for the best project award.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">9.91222e+06</data>
      <data key="e_expirationDate">2004-03-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.91222e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n200" target="n201">
      <data key="e_effectiveDate">2000-04-15</data>
      <data key="e_title">Pattern Classification in Context</data>
      <data key="e_abstract">In the real world, the identity of an object is often ambiguous due to noise or lack of information. This ambiguity can sometimes be reduced by utilizing extra information, referred to as context, provided by surrounding objects. Unfortunately, existing approaches that utilize context collapse when the number of context-bearing objects is large. Furthermore, there has been no attempt to separate relevant context from irrelevant context. In this project PI will develop algorithms that can reliably identify relevant context and then exploit it in a decision making process for pattern classification that is computationally tractable. The approach employs a Bayesian framework that incorporates &quot;partial&quot; context represented as the &quot;derivative&quot; of the identities of surrounding objects. The criteria for evaluation of context relevancy include information-theoretic measures such as mutual information and &quot;expected relative entropy&quot;, as well as complete factorization of Bayesian networks. The development of the algorithms was initially motivated by and will be ultimately tested on two related yet different medical diagnostic applications: white blood cell differentiation and microscopic urinalysis image classification. The resulting classification will provide diagnostic information about real patients. Although the testbed applications relate to medical diagnosis, the results of this research will be general and can find an abundance of uses in other areas, so this work will impact the broader scope of knowledge discovery, data mining, machine learning, and pattern recognition where context plays a role.</data>
      <data key="e_pgm">6856</data>
      <data key="e_label">70842</data>
      <data key="e_expirationDate">2001-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">70842</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n206" target="n207">
      <data key="e_effectiveDate">2000-04-01</data>
      <data key="e_title">CISE Research Instrumentation: Dual-Manipulator Mobile Robot</data>
      <data key="e_abstract">EIA-9911077&lt;br/&gt;Ning XI&lt;br/&gt;Michigan State&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Dual Manipulator Mobile Robot&lt;br/&gt;&lt;br/&gt;The Department of Electrical and Computer Engineering at Michigan State University will purchase a dual-arm mobile robot system consisting of a mobile platform equipped with a dual-arm manipulator and various sensors. This equipment will be dedicated to support research in the area of computer and information science and engineering. The equipment will be used for several research projects including in particular: Planning and Control for Cooperative Operation of Multiple Mobile Manipulator in a Perceptive Frame; Task Scheduling and Control of a Reconfigurable Manufacturing Workcell; Heterogeneous Function-Based Human/Robot Cooperation; Multi-Sensor Fusion in Robot Action planning and Control. The objectives of these projects aim at developing theoretical foundations for intelligent planning and control of robotic and manufacturing systems to improve the reliability and efficiency of operations. The requested equipment will provide unique capability to experimentally implement and test new theoretical findings.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.91108e+06</data>
      <data key="e_expirationDate">2005-03-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.91108e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n220" target="n221">
      <data key="e_effectiveDate">2000-05-15</data>
      <data key="e_title">Workshop on Value-Sensitive Design: Cultivating Research and Community</data>
      <data key="e_abstract">In recent years, there has been a growing national awareness that the fast-paced development and deployment of information technology can transform society in paradoxical and/or unintended ways. There is general agreement that more research is needed to identify, understand, anticipate, and address the scope and trajectory of these changes. One new and promising approach is to better understand how the design of information technologies can be linked to social outcomes. Instead of assuming that the design of information technologies is a simple engineering feat whereby value-neutral systems specifications are produced and then - given sufficient time, money and skilled labor - achieved in a particular artifact, the design process is better understood as one where various stakeholders can have conflicting goals and priorities and where trade-offs are constantly balanced. In addition, designers often have implicit (and sometimes incorrect) assumptions about users and how they will use a particular artifact. As a consequence, a variety of social values can come to be embodied in artifacts, resulting in both intended and unintended consequences. To begin to understand how this works in practice and how designers of IT can become more sensitive to their role in fostering certain outcomes, more interest in doing this kind of research needs to be generated. This proposed workshop on Value-Sensitive Design intends to: 1) further develop and refine a research agenda; and 2) stimulate research by cultivating a broad community of researchers. To insure that multiple perspectives and communities are represented, multi-disciplinary research &quot;teams&quot; (often faculty and graduate students) will be identified by the organizers and its steering committee and invited. Each of four broad knowledge areas will be included - designers, humanists, social scientists and technologists.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">567</data>
      <data key="e_expirationDate">2001-04-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">567</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n226" target="n227">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">Multigrid Optical Diffusion Tomography</data>
      <data key="e_abstract">Recently, there has been growing interest in the development of medical imaging modalities based on light, as opposed to more conventional modalities based on ultrasound, X-ray computed tomography (CT), or magnetic resonance (MRI). Imaging based on light has the advantages of being non-invasive, safe, and requiring only inexpensive instrumentation. Conventional wisdom holds that light can not pass through tissue, but in fact, near infra-red light does pass through tissue and can therefore, in principle, be used to form images of the interior of the tissue. However, light passing through tissue is highly scattered, so that an image can not be calculated using conventional methods. This research is concerned with the development of new computational algorithms that can be used to form three-dimensional images from measurements of scattered light. These methods, known collectively as optical diffusion tomography, have the potential to provide safe, inexpensive, and portable imaging instruments.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Practical realization of optical diffusion imaging requires that a difficult non-linear inverse problem be solved in a computationally tractable manner to yield accurate images. In this research, new multigrid optimization methods are developed which have the potential to dramatically speed reconstruction. These new methods are applied in a Bayesian framework and use novel techniques to model the regions containing voids, opaque tissue, or fluorescence imaging agents. Moreover, optical diffusion tomography is representative of a broader class of important inverse problems; so that the methods developed in this research have application in problems ranging from image registration and motion estimation to environmental sensing and non-destructive evaluation.</data>
      <data key="e_pgm">4720</data>
      <data key="e_label">73357</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">73357</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n226" target="n228">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">Multigrid Optical Diffusion Tomography</data>
      <data key="e_abstract">Recently, there has been growing interest in the development of medical imaging modalities based on light, as opposed to more conventional modalities based on ultrasound, X-ray computed tomography (CT), or magnetic resonance (MRI). Imaging based on light has the advantages of being non-invasive, safe, and requiring only inexpensive instrumentation. Conventional wisdom holds that light can not pass through tissue, but in fact, near infra-red light does pass through tissue and can therefore, in principle, be used to form images of the interior of the tissue. However, light passing through tissue is highly scattered, so that an image can not be calculated using conventional methods. This research is concerned with the development of new computational algorithms that can be used to form three-dimensional images from measurements of scattered light. These methods, known collectively as optical diffusion tomography, have the potential to provide safe, inexpensive, and portable imaging instruments.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Practical realization of optical diffusion imaging requires that a difficult non-linear inverse problem be solved in a computationally tractable manner to yield accurate images. In this research, new multigrid optimization methods are developed which have the potential to dramatically speed reconstruction. These new methods are applied in a Bayesian framework and use novel techniques to model the regions containing voids, opaque tissue, or fluorescence imaging agents. Moreover, optical diffusion tomography is representative of a broader class of important inverse problems; so that the methods developed in this research have application in problems ranging from image registration and motion estimation to environmental sensing and non-destructive evaluation.</data>
      <data key="e_pgm">4720</data>
      <data key="e_label">73357</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">73357</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n227" target="n228">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">Multigrid Optical Diffusion Tomography</data>
      <data key="e_abstract">Recently, there has been growing interest in the development of medical imaging modalities based on light, as opposed to more conventional modalities based on ultrasound, X-ray computed tomography (CT), or magnetic resonance (MRI). Imaging based on light has the advantages of being non-invasive, safe, and requiring only inexpensive instrumentation. Conventional wisdom holds that light can not pass through tissue, but in fact, near infra-red light does pass through tissue and can therefore, in principle, be used to form images of the interior of the tissue. However, light passing through tissue is highly scattered, so that an image can not be calculated using conventional methods. This research is concerned with the development of new computational algorithms that can be used to form three-dimensional images from measurements of scattered light. These methods, known collectively as optical diffusion tomography, have the potential to provide safe, inexpensive, and portable imaging instruments.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Practical realization of optical diffusion imaging requires that a difficult non-linear inverse problem be solved in a computationally tractable manner to yield accurate images. In this research, new multigrid optimization methods are developed which have the potential to dramatically speed reconstruction. These new methods are applied in a Bayesian framework and use novel techniques to model the regions containing voids, opaque tissue, or fluorescence imaging agents. Moreover, optical diffusion tomography is representative of a broader class of important inverse problems; so that the methods developed in this research have application in problems ranging from image registration and motion estimation to environmental sensing and non-destructive evaluation.</data>
      <data key="e_pgm">4720</data>
      <data key="e_label">73357</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">73357</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n229" target="n230">
      <data key="e_effectiveDate">2000-05-15</data>
      <data key="e_title">Essential Tools for Computational Research on Visual-Gestural Language</data>
      <data key="e_abstract">This is the first-year funding of a three year continuing award. Research on recognition and generation of signed languages and the gestural component of spoken languages has been held back by the unavailability of large-scale linguistically annotated corpora of the kind that led to significant advances in the area of spoken language. A major obstacle to the production of such corpora has been the lack of computational tools to assist in efficient analysis and transcription of visual language data. In this project the PI and her team will develop the tools and techniques necessary to support efficient transcription of finely detailed phonological information and its integration with information provided by computational algorithms, so as to enable the creation of large-scale corpora annotated at the level of granularity essential for computer science research. Machine vision-based algorithms for semi-automation of several aspects of the transcription process will also be developed.&lt;br/&gt;&lt;br/&gt;This research will result in the production and public dissemination of corpora that will include fine-grained linguistic annotations of ASL video data. The availability of these corpora and the refinement of computational tools for analyzing visual data will be invaluable for the linguistic study of signed languages and the gestural component of spoken languages. The corpora will be used in training computer models, thereby leading to advances in both recognition and generation of signed languages and gesture. The tools will have educational applications for the teaching of ASL and other signed languages. Ultimately, this research will have implications for human-computer interfaces, enabling users to interact with computers via sign language or via a combination of speech and gesture, and may also lead to alternate input mechanisms for disabled users as well as techniques for automatic recognition and classification of human actions.</data>
      <data key="e_pgm">6846</data>
      <data key="e_label">9.91257e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.91257e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n229" target="n231">
      <data key="e_effectiveDate">2000-05-15</data>
      <data key="e_title">Essential Tools for Computational Research on Visual-Gestural Language</data>
      <data key="e_abstract">This is the first-year funding of a three year continuing award. Research on recognition and generation of signed languages and the gestural component of spoken languages has been held back by the unavailability of large-scale linguistically annotated corpora of the kind that led to significant advances in the area of spoken language. A major obstacle to the production of such corpora has been the lack of computational tools to assist in efficient analysis and transcription of visual language data. In this project the PI and her team will develop the tools and techniques necessary to support efficient transcription of finely detailed phonological information and its integration with information provided by computational algorithms, so as to enable the creation of large-scale corpora annotated at the level of granularity essential for computer science research. Machine vision-based algorithms for semi-automation of several aspects of the transcription process will also be developed.&lt;br/&gt;&lt;br/&gt;This research will result in the production and public dissemination of corpora that will include fine-grained linguistic annotations of ASL video data. The availability of these corpora and the refinement of computational tools for analyzing visual data will be invaluable for the linguistic study of signed languages and the gestural component of spoken languages. The corpora will be used in training computer models, thereby leading to advances in both recognition and generation of signed languages and gesture. The tools will have educational applications for the teaching of ASL and other signed languages. Ultimately, this research will have implications for human-computer interfaces, enabling users to interact with computers via sign language or via a combination of speech and gesture, and may also lead to alternate input mechanisms for disabled users as well as techniques for automatic recognition and classification of human actions.</data>
      <data key="e_pgm">6846</data>
      <data key="e_label">9.91257e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.91257e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n230" target="n231">
      <data key="e_effectiveDate">2000-05-15</data>
      <data key="e_title">Essential Tools for Computational Research on Visual-Gestural Language</data>
      <data key="e_abstract">This is the first-year funding of a three year continuing award. Research on recognition and generation of signed languages and the gestural component of spoken languages has been held back by the unavailability of large-scale linguistically annotated corpora of the kind that led to significant advances in the area of spoken language. A major obstacle to the production of such corpora has been the lack of computational tools to assist in efficient analysis and transcription of visual language data. In this project the PI and her team will develop the tools and techniques necessary to support efficient transcription of finely detailed phonological information and its integration with information provided by computational algorithms, so as to enable the creation of large-scale corpora annotated at the level of granularity essential for computer science research. Machine vision-based algorithms for semi-automation of several aspects of the transcription process will also be developed.&lt;br/&gt;&lt;br/&gt;This research will result in the production and public dissemination of corpora that will include fine-grained linguistic annotations of ASL video data. The availability of these corpora and the refinement of computational tools for analyzing visual data will be invaluable for the linguistic study of signed languages and the gestural component of spoken languages. The corpora will be used in training computer models, thereby leading to advances in both recognition and generation of signed languages and gesture. The tools will have educational applications for the teaching of ASL and other signed languages. Ultimately, this research will have implications for human-computer interfaces, enabling users to interact with computers via sign language or via a combination of speech and gesture, and may also lead to alternate input mechanisms for disabled users as well as techniques for automatic recognition and classification of human actions.</data>
      <data key="e_pgm">6846</data>
      <data key="e_label">9.91257e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.91257e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n233" target="n234">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">Geometric Problems in Radiosurgery, Radiation Therapy, and Other Medical Applications</data>
      <data key="e_abstract">This project is on the design, analysis, and implementation of &lt;br/&gt;algorithmic techniques for solving geometric problems that arise in &lt;br/&gt;radiosurgery, radiation therapy, and other medical applications. &lt;br/&gt;Radiosurgery is a minimally invasive surgical procedure that uses &lt;br/&gt;a set of focused beams of radiation to destroy tumors. A key step &lt;br/&gt;in radiotherapy and radiosurgery is to develop a treatment plan that &lt;br/&gt;defines the best radiation beam arrangements and time settings to &lt;br/&gt;destroy the target tumor without harming the surrounding healthy &lt;br/&gt;tissues. At the core of radiation treatment planning is a set &lt;br/&gt;of substantially non-trivial geometric optimization problems.&lt;br/&gt;We seek to investigate a number of geometric optimization &lt;br/&gt;problems arising in radiation treatment planning, such as beam &lt;br/&gt;selection (including beam probing and its many variations), surgical &lt;br/&gt;navigation and routing, sphere packing, beam shaping, image &lt;br/&gt;segmentation, shape approximation, and beam source path planning.&lt;br/&gt;This interdisciplinary research will draw diverse techniques from &lt;br/&gt;computational geometry and other theoretical areas, such as operations &lt;br/&gt;research and combinatorial optimization. Furthermore, this research &lt;br/&gt;will provide a rich source of problems and new challenges that prod &lt;br/&gt;further development of algorithmic techniques in these theoretical &lt;br/&gt;areas. The planned research includes an important experimental &lt;br/&gt;component. Educational activities on course development and student &lt;br/&gt;training in the interdisciplinary area of computational medicine are &lt;br/&gt;also part of the project.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">9.98847e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98847e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n236" target="n237">
      <data key="e_effectiveDate">2000-05-15</data>
      <data key="e_title">Gauss Mixture Quantization for Image Compression and Segmentation</data>
      <data key="e_abstract">The research is concerned with techniques from statistical signal processing and information theory as they apply to communication systems with multiple goals. Such systems arise in multimedia communications networks like the Internet. The decomposition of data streams into different types is critical to finding information desired by a user among vast available sources, and it can also provide methods for displaying, rendering, printing, or playing the received signal that take advantage of its particular structure. Signal processing and coding theory have provided powerful mathematical models of information sources and algorithms by which these sources can be communicated and processed. Typically systems are designed as a collection of separate, unrelated, components. This can result in much less than optimal overall performance. Furthermore, it can hamper theoretical understanding of the fundamental limits on achievable performance.We treat the simultaneous design of mathematical models that account at once for information sources, data compression, and signal processing and apply to extracting information from the received data. Our emphasis is on image communication and processing. Because, the techniques draw heavily from demonstrably successful methods in speech coding and recognition they are natural for both signal types, individually or together.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;The research involves a unified approach to data compression, statistical classification and regression, and density estimation. It is based on a novel combination of vector quantization, Gauss mixture models, measures of minimum discrimination information (relative entropy), and universal coding. Vector quantization provides both a theoretical framework and a method for implementation. Gauss mixture models are a flexible class by which to describe information sources. They can be fit to real data by clustering with respect to a minimum discrimination information measure of distortion. A primary objective is the development and application of conditional versions of rate-distortion extremal properties of Gaussian models in order to design robust algorithms for compression, classification, modeling, and combinations thereof. There are many open questions about relations among modeling, compression, and classification/regression. Our goal is to provide answers to as many of them as possible and in so doing to contribute to understanding the interplay of modeling, signal processing, and coding. We describe optimized and implementable robust codes for compression and classification for a variety of information sources, especially for multimodal imagery. Part of our efforts are devoted to purely mathematical aspects of tree-structured regression, which is related to martingale theory and to the differentiation of integrals.</data>
      <data key="e_pgm">4720</data>
      <data key="e_label">73050</data>
      <data key="e_expirationDate">2004-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">73050</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n238" target="n239">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">Integrating 3D Dynamic Meteorological Data and Algorithms into a Scalable Geospatial Framework</data>
      <data key="e_abstract">People, governments, manufacturers, airlines, and others rely more and more on accurate and timely weather forecasting. An increased population, especially in areas prone to flooding or severe weather, requires pinpoint weather warnings and longer range predictions. This project will help operational weather forecasters in making decisions about severe weather situations by developing tools to visualize and analyze large data sets, including real-time weather observations. Staff of the National Severe Storm Lab also will participate, especially in the evaluation and testing of visual analysis and decision-support tools and in the insertion of those tools in operational software for forecasters and weather researchers. These tools will provide a framework for the future that can be used not only by weather forecasters but also by researchers. The weather prediction and warning capabilities built on the methods proposed here may ultimately result in many billions of dollars saved in lost products, equipment, and time. Lives, too, can be saved and injuries reduced.&lt;br/&gt;&lt;br/&gt;Technically, the project will bring together, for the purposes of analysis and forecast decision-making, 3D time-dependent volumes from multiple overlapping Doppler radars, and simultaneous satellite information for the same and wider coverage areas. These will be combined with accurate terrain elevations, multiple image layers, maps, and other geospatial thematic data. This universal data collection will be organized for integrated visual analysis and made available for interactive navigation, exploration, and discovery by weather forecasters, researchers, and other users. These observational data will be displayed for the first time on accurate 3D terrain so that the correlation of landscape and weather can be revealed in detail, thus enabling new predictions of flood extents, inclusion of the effects of mountains on weather phenomena, and other new capabilities. To support fast, scalable visualization, a &quot;geo-layered volume&quot; will be introduced that will take advantage of the underlying terrain global quadtree organization and out-of-core paging structure. Hierarchical visual models will be developed within the geo-layered volume structure to produce several visual representation including volume rendering, isosurfaces, simple 3D time-dependent feature representations, and iconic annotations. A fast and scalable cluster-based feature analysis method will form a dynamic feature hierarchy also. This hierarchy will be used to produce levels of detail for the volume and isosurface visualization of the 3D time-dependent observational data. The visual analyses will be put in a decision-support framework. Since the analyses attach meaning and importance to the various observations, the observational data can be displayed in the right form for easy use. Important phenomena can be given visual forms that catch the eye, and more detail can be given to objects that the analysis says are important or to objects that the user looks at more closely.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">9.9823e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.9823e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n238" target="n240">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">Integrating 3D Dynamic Meteorological Data and Algorithms into a Scalable Geospatial Framework</data>
      <data key="e_abstract">People, governments, manufacturers, airlines, and others rely more and more on accurate and timely weather forecasting. An increased population, especially in areas prone to flooding or severe weather, requires pinpoint weather warnings and longer range predictions. This project will help operational weather forecasters in making decisions about severe weather situations by developing tools to visualize and analyze large data sets, including real-time weather observations. Staff of the National Severe Storm Lab also will participate, especially in the evaluation and testing of visual analysis and decision-support tools and in the insertion of those tools in operational software for forecasters and weather researchers. These tools will provide a framework for the future that can be used not only by weather forecasters but also by researchers. The weather prediction and warning capabilities built on the methods proposed here may ultimately result in many billions of dollars saved in lost products, equipment, and time. Lives, too, can be saved and injuries reduced.&lt;br/&gt;&lt;br/&gt;Technically, the project will bring together, for the purposes of analysis and forecast decision-making, 3D time-dependent volumes from multiple overlapping Doppler radars, and simultaneous satellite information for the same and wider coverage areas. These will be combined with accurate terrain elevations, multiple image layers, maps, and other geospatial thematic data. This universal data collection will be organized for integrated visual analysis and made available for interactive navigation, exploration, and discovery by weather forecasters, researchers, and other users. These observational data will be displayed for the first time on accurate 3D terrain so that the correlation of landscape and weather can be revealed in detail, thus enabling new predictions of flood extents, inclusion of the effects of mountains on weather phenomena, and other new capabilities. To support fast, scalable visualization, a &quot;geo-layered volume&quot; will be introduced that will take advantage of the underlying terrain global quadtree organization and out-of-core paging structure. Hierarchical visual models will be developed within the geo-layered volume structure to produce several visual representation including volume rendering, isosurfaces, simple 3D time-dependent feature representations, and iconic annotations. A fast and scalable cluster-based feature analysis method will form a dynamic feature hierarchy also. This hierarchy will be used to produce levels of detail for the volume and isosurface visualization of the 3D time-dependent observational data. The visual analyses will be put in a decision-support framework. Since the analyses attach meaning and importance to the various observations, the observational data can be displayed in the right form for easy use. Important phenomena can be given visual forms that catch the eye, and more detail can be given to objects that the analysis says are important or to objects that the user looks at more closely.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">9.9823e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.9823e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n238" target="n241">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">Integrating 3D Dynamic Meteorological Data and Algorithms into a Scalable Geospatial Framework</data>
      <data key="e_abstract">People, governments, manufacturers, airlines, and others rely more and more on accurate and timely weather forecasting. An increased population, especially in areas prone to flooding or severe weather, requires pinpoint weather warnings and longer range predictions. This project will help operational weather forecasters in making decisions about severe weather situations by developing tools to visualize and analyze large data sets, including real-time weather observations. Staff of the National Severe Storm Lab also will participate, especially in the evaluation and testing of visual analysis and decision-support tools and in the insertion of those tools in operational software for forecasters and weather researchers. These tools will provide a framework for the future that can be used not only by weather forecasters but also by researchers. The weather prediction and warning capabilities built on the methods proposed here may ultimately result in many billions of dollars saved in lost products, equipment, and time. Lives, too, can be saved and injuries reduced.&lt;br/&gt;&lt;br/&gt;Technically, the project will bring together, for the purposes of analysis and forecast decision-making, 3D time-dependent volumes from multiple overlapping Doppler radars, and simultaneous satellite information for the same and wider coverage areas. These will be combined with accurate terrain elevations, multiple image layers, maps, and other geospatial thematic data. This universal data collection will be organized for integrated visual analysis and made available for interactive navigation, exploration, and discovery by weather forecasters, researchers, and other users. These observational data will be displayed for the first time on accurate 3D terrain so that the correlation of landscape and weather can be revealed in detail, thus enabling new predictions of flood extents, inclusion of the effects of mountains on weather phenomena, and other new capabilities. To support fast, scalable visualization, a &quot;geo-layered volume&quot; will be introduced that will take advantage of the underlying terrain global quadtree organization and out-of-core paging structure. Hierarchical visual models will be developed within the geo-layered volume structure to produce several visual representation including volume rendering, isosurfaces, simple 3D time-dependent feature representations, and iconic annotations. A fast and scalable cluster-based feature analysis method will form a dynamic feature hierarchy also. This hierarchy will be used to produce levels of detail for the volume and isosurface visualization of the 3D time-dependent observational data. The visual analyses will be put in a decision-support framework. Since the analyses attach meaning and importance to the various observations, the observational data can be displayed in the right form for easy use. Important phenomena can be given visual forms that catch the eye, and more detail can be given to objects that the analysis says are important or to objects that the user looks at more closely.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">9.9823e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.9823e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n238" target="n242">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">Integrating 3D Dynamic Meteorological Data and Algorithms into a Scalable Geospatial Framework</data>
      <data key="e_abstract">People, governments, manufacturers, airlines, and others rely more and more on accurate and timely weather forecasting. An increased population, especially in areas prone to flooding or severe weather, requires pinpoint weather warnings and longer range predictions. This project will help operational weather forecasters in making decisions about severe weather situations by developing tools to visualize and analyze large data sets, including real-time weather observations. Staff of the National Severe Storm Lab also will participate, especially in the evaluation and testing of visual analysis and decision-support tools and in the insertion of those tools in operational software for forecasters and weather researchers. These tools will provide a framework for the future that can be used not only by weather forecasters but also by researchers. The weather prediction and warning capabilities built on the methods proposed here may ultimately result in many billions of dollars saved in lost products, equipment, and time. Lives, too, can be saved and injuries reduced.&lt;br/&gt;&lt;br/&gt;Technically, the project will bring together, for the purposes of analysis and forecast decision-making, 3D time-dependent volumes from multiple overlapping Doppler radars, and simultaneous satellite information for the same and wider coverage areas. These will be combined with accurate terrain elevations, multiple image layers, maps, and other geospatial thematic data. This universal data collection will be organized for integrated visual analysis and made available for interactive navigation, exploration, and discovery by weather forecasters, researchers, and other users. These observational data will be displayed for the first time on accurate 3D terrain so that the correlation of landscape and weather can be revealed in detail, thus enabling new predictions of flood extents, inclusion of the effects of mountains on weather phenomena, and other new capabilities. To support fast, scalable visualization, a &quot;geo-layered volume&quot; will be introduced that will take advantage of the underlying terrain global quadtree organization and out-of-core paging structure. Hierarchical visual models will be developed within the geo-layered volume structure to produce several visual representation including volume rendering, isosurfaces, simple 3D time-dependent feature representations, and iconic annotations. A fast and scalable cluster-based feature analysis method will form a dynamic feature hierarchy also. This hierarchy will be used to produce levels of detail for the volume and isosurface visualization of the 3D time-dependent observational data. The visual analyses will be put in a decision-support framework. Since the analyses attach meaning and importance to the various observations, the observational data can be displayed in the right form for easy use. Important phenomena can be given visual forms that catch the eye, and more detail can be given to objects that the analysis says are important or to objects that the user looks at more closely.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">9.9823e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.9823e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n239" target="n240">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">Integrating 3D Dynamic Meteorological Data and Algorithms into a Scalable Geospatial Framework</data>
      <data key="e_abstract">People, governments, manufacturers, airlines, and others rely more and more on accurate and timely weather forecasting. An increased population, especially in areas prone to flooding or severe weather, requires pinpoint weather warnings and longer range predictions. This project will help operational weather forecasters in making decisions about severe weather situations by developing tools to visualize and analyze large data sets, including real-time weather observations. Staff of the National Severe Storm Lab also will participate, especially in the evaluation and testing of visual analysis and decision-support tools and in the insertion of those tools in operational software for forecasters and weather researchers. These tools will provide a framework for the future that can be used not only by weather forecasters but also by researchers. The weather prediction and warning capabilities built on the methods proposed here may ultimately result in many billions of dollars saved in lost products, equipment, and time. Lives, too, can be saved and injuries reduced.&lt;br/&gt;&lt;br/&gt;Technically, the project will bring together, for the purposes of analysis and forecast decision-making, 3D time-dependent volumes from multiple overlapping Doppler radars, and simultaneous satellite information for the same and wider coverage areas. These will be combined with accurate terrain elevations, multiple image layers, maps, and other geospatial thematic data. This universal data collection will be organized for integrated visual analysis and made available for interactive navigation, exploration, and discovery by weather forecasters, researchers, and other users. These observational data will be displayed for the first time on accurate 3D terrain so that the correlation of landscape and weather can be revealed in detail, thus enabling new predictions of flood extents, inclusion of the effects of mountains on weather phenomena, and other new capabilities. To support fast, scalable visualization, a &quot;geo-layered volume&quot; will be introduced that will take advantage of the underlying terrain global quadtree organization and out-of-core paging structure. Hierarchical visual models will be developed within the geo-layered volume structure to produce several visual representation including volume rendering, isosurfaces, simple 3D time-dependent feature representations, and iconic annotations. A fast and scalable cluster-based feature analysis method will form a dynamic feature hierarchy also. This hierarchy will be used to produce levels of detail for the volume and isosurface visualization of the 3D time-dependent observational data. The visual analyses will be put in a decision-support framework. Since the analyses attach meaning and importance to the various observations, the observational data can be displayed in the right form for easy use. Important phenomena can be given visual forms that catch the eye, and more detail can be given to objects that the analysis says are important or to objects that the user looks at more closely.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">9.9823e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.9823e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n239" target="n241">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">Integrating 3D Dynamic Meteorological Data and Algorithms into a Scalable Geospatial Framework</data>
      <data key="e_abstract">People, governments, manufacturers, airlines, and others rely more and more on accurate and timely weather forecasting. An increased population, especially in areas prone to flooding or severe weather, requires pinpoint weather warnings and longer range predictions. This project will help operational weather forecasters in making decisions about severe weather situations by developing tools to visualize and analyze large data sets, including real-time weather observations. Staff of the National Severe Storm Lab also will participate, especially in the evaluation and testing of visual analysis and decision-support tools and in the insertion of those tools in operational software for forecasters and weather researchers. These tools will provide a framework for the future that can be used not only by weather forecasters but also by researchers. The weather prediction and warning capabilities built on the methods proposed here may ultimately result in many billions of dollars saved in lost products, equipment, and time. Lives, too, can be saved and injuries reduced.&lt;br/&gt;&lt;br/&gt;Technically, the project will bring together, for the purposes of analysis and forecast decision-making, 3D time-dependent volumes from multiple overlapping Doppler radars, and simultaneous satellite information for the same and wider coverage areas. These will be combined with accurate terrain elevations, multiple image layers, maps, and other geospatial thematic data. This universal data collection will be organized for integrated visual analysis and made available for interactive navigation, exploration, and discovery by weather forecasters, researchers, and other users. These observational data will be displayed for the first time on accurate 3D terrain so that the correlation of landscape and weather can be revealed in detail, thus enabling new predictions of flood extents, inclusion of the effects of mountains on weather phenomena, and other new capabilities. To support fast, scalable visualization, a &quot;geo-layered volume&quot; will be introduced that will take advantage of the underlying terrain global quadtree organization and out-of-core paging structure. Hierarchical visual models will be developed within the geo-layered volume structure to produce several visual representation including volume rendering, isosurfaces, simple 3D time-dependent feature representations, and iconic annotations. A fast and scalable cluster-based feature analysis method will form a dynamic feature hierarchy also. This hierarchy will be used to produce levels of detail for the volume and isosurface visualization of the 3D time-dependent observational data. The visual analyses will be put in a decision-support framework. Since the analyses attach meaning and importance to the various observations, the observational data can be displayed in the right form for easy use. Important phenomena can be given visual forms that catch the eye, and more detail can be given to objects that the analysis says are important or to objects that the user looks at more closely.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">9.9823e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.9823e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n239" target="n242">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">Integrating 3D Dynamic Meteorological Data and Algorithms into a Scalable Geospatial Framework</data>
      <data key="e_abstract">People, governments, manufacturers, airlines, and others rely more and more on accurate and timely weather forecasting. An increased population, especially in areas prone to flooding or severe weather, requires pinpoint weather warnings and longer range predictions. This project will help operational weather forecasters in making decisions about severe weather situations by developing tools to visualize and analyze large data sets, including real-time weather observations. Staff of the National Severe Storm Lab also will participate, especially in the evaluation and testing of visual analysis and decision-support tools and in the insertion of those tools in operational software for forecasters and weather researchers. These tools will provide a framework for the future that can be used not only by weather forecasters but also by researchers. The weather prediction and warning capabilities built on the methods proposed here may ultimately result in many billions of dollars saved in lost products, equipment, and time. Lives, too, can be saved and injuries reduced.&lt;br/&gt;&lt;br/&gt;Technically, the project will bring together, for the purposes of analysis and forecast decision-making, 3D time-dependent volumes from multiple overlapping Doppler radars, and simultaneous satellite information for the same and wider coverage areas. These will be combined with accurate terrain elevations, multiple image layers, maps, and other geospatial thematic data. This universal data collection will be organized for integrated visual analysis and made available for interactive navigation, exploration, and discovery by weather forecasters, researchers, and other users. These observational data will be displayed for the first time on accurate 3D terrain so that the correlation of landscape and weather can be revealed in detail, thus enabling new predictions of flood extents, inclusion of the effects of mountains on weather phenomena, and other new capabilities. To support fast, scalable visualization, a &quot;geo-layered volume&quot; will be introduced that will take advantage of the underlying terrain global quadtree organization and out-of-core paging structure. Hierarchical visual models will be developed within the geo-layered volume structure to produce several visual representation including volume rendering, isosurfaces, simple 3D time-dependent feature representations, and iconic annotations. A fast and scalable cluster-based feature analysis method will form a dynamic feature hierarchy also. This hierarchy will be used to produce levels of detail for the volume and isosurface visualization of the 3D time-dependent observational data. The visual analyses will be put in a decision-support framework. Since the analyses attach meaning and importance to the various observations, the observational data can be displayed in the right form for easy use. Important phenomena can be given visual forms that catch the eye, and more detail can be given to objects that the analysis says are important or to objects that the user looks at more closely.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">9.9823e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.9823e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n240" target="n241">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">Integrating 3D Dynamic Meteorological Data and Algorithms into a Scalable Geospatial Framework</data>
      <data key="e_abstract">People, governments, manufacturers, airlines, and others rely more and more on accurate and timely weather forecasting. An increased population, especially in areas prone to flooding or severe weather, requires pinpoint weather warnings and longer range predictions. This project will help operational weather forecasters in making decisions about severe weather situations by developing tools to visualize and analyze large data sets, including real-time weather observations. Staff of the National Severe Storm Lab also will participate, especially in the evaluation and testing of visual analysis and decision-support tools and in the insertion of those tools in operational software for forecasters and weather researchers. These tools will provide a framework for the future that can be used not only by weather forecasters but also by researchers. The weather prediction and warning capabilities built on the methods proposed here may ultimately result in many billions of dollars saved in lost products, equipment, and time. Lives, too, can be saved and injuries reduced.&lt;br/&gt;&lt;br/&gt;Technically, the project will bring together, for the purposes of analysis and forecast decision-making, 3D time-dependent volumes from multiple overlapping Doppler radars, and simultaneous satellite information for the same and wider coverage areas. These will be combined with accurate terrain elevations, multiple image layers, maps, and other geospatial thematic data. This universal data collection will be organized for integrated visual analysis and made available for interactive navigation, exploration, and discovery by weather forecasters, researchers, and other users. These observational data will be displayed for the first time on accurate 3D terrain so that the correlation of landscape and weather can be revealed in detail, thus enabling new predictions of flood extents, inclusion of the effects of mountains on weather phenomena, and other new capabilities. To support fast, scalable visualization, a &quot;geo-layered volume&quot; will be introduced that will take advantage of the underlying terrain global quadtree organization and out-of-core paging structure. Hierarchical visual models will be developed within the geo-layered volume structure to produce several visual representation including volume rendering, isosurfaces, simple 3D time-dependent feature representations, and iconic annotations. A fast and scalable cluster-based feature analysis method will form a dynamic feature hierarchy also. This hierarchy will be used to produce levels of detail for the volume and isosurface visualization of the 3D time-dependent observational data. The visual analyses will be put in a decision-support framework. Since the analyses attach meaning and importance to the various observations, the observational data can be displayed in the right form for easy use. Important phenomena can be given visual forms that catch the eye, and more detail can be given to objects that the analysis says are important or to objects that the user looks at more closely.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">9.9823e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.9823e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n240" target="n242">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">Integrating 3D Dynamic Meteorological Data and Algorithms into a Scalable Geospatial Framework</data>
      <data key="e_abstract">People, governments, manufacturers, airlines, and others rely more and more on accurate and timely weather forecasting. An increased population, especially in areas prone to flooding or severe weather, requires pinpoint weather warnings and longer range predictions. This project will help operational weather forecasters in making decisions about severe weather situations by developing tools to visualize and analyze large data sets, including real-time weather observations. Staff of the National Severe Storm Lab also will participate, especially in the evaluation and testing of visual analysis and decision-support tools and in the insertion of those tools in operational software for forecasters and weather researchers. These tools will provide a framework for the future that can be used not only by weather forecasters but also by researchers. The weather prediction and warning capabilities built on the methods proposed here may ultimately result in many billions of dollars saved in lost products, equipment, and time. Lives, too, can be saved and injuries reduced.&lt;br/&gt;&lt;br/&gt;Technically, the project will bring together, for the purposes of analysis and forecast decision-making, 3D time-dependent volumes from multiple overlapping Doppler radars, and simultaneous satellite information for the same and wider coverage areas. These will be combined with accurate terrain elevations, multiple image layers, maps, and other geospatial thematic data. This universal data collection will be organized for integrated visual analysis and made available for interactive navigation, exploration, and discovery by weather forecasters, researchers, and other users. These observational data will be displayed for the first time on accurate 3D terrain so that the correlation of landscape and weather can be revealed in detail, thus enabling new predictions of flood extents, inclusion of the effects of mountains on weather phenomena, and other new capabilities. To support fast, scalable visualization, a &quot;geo-layered volume&quot; will be introduced that will take advantage of the underlying terrain global quadtree organization and out-of-core paging structure. Hierarchical visual models will be developed within the geo-layered volume structure to produce several visual representation including volume rendering, isosurfaces, simple 3D time-dependent feature representations, and iconic annotations. A fast and scalable cluster-based feature analysis method will form a dynamic feature hierarchy also. This hierarchy will be used to produce levels of detail for the volume and isosurface visualization of the 3D time-dependent observational data. The visual analyses will be put in a decision-support framework. Since the analyses attach meaning and importance to the various observations, the observational data can be displayed in the right form for easy use. Important phenomena can be given visual forms that catch the eye, and more detail can be given to objects that the analysis says are important or to objects that the user looks at more closely.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">9.9823e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.9823e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n241" target="n242">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">Integrating 3D Dynamic Meteorological Data and Algorithms into a Scalable Geospatial Framework</data>
      <data key="e_abstract">People, governments, manufacturers, airlines, and others rely more and more on accurate and timely weather forecasting. An increased population, especially in areas prone to flooding or severe weather, requires pinpoint weather warnings and longer range predictions. This project will help operational weather forecasters in making decisions about severe weather situations by developing tools to visualize and analyze large data sets, including real-time weather observations. Staff of the National Severe Storm Lab also will participate, especially in the evaluation and testing of visual analysis and decision-support tools and in the insertion of those tools in operational software for forecasters and weather researchers. These tools will provide a framework for the future that can be used not only by weather forecasters but also by researchers. The weather prediction and warning capabilities built on the methods proposed here may ultimately result in many billions of dollars saved in lost products, equipment, and time. Lives, too, can be saved and injuries reduced.&lt;br/&gt;&lt;br/&gt;Technically, the project will bring together, for the purposes of analysis and forecast decision-making, 3D time-dependent volumes from multiple overlapping Doppler radars, and simultaneous satellite information for the same and wider coverage areas. These will be combined with accurate terrain elevations, multiple image layers, maps, and other geospatial thematic data. This universal data collection will be organized for integrated visual analysis and made available for interactive navigation, exploration, and discovery by weather forecasters, researchers, and other users. These observational data will be displayed for the first time on accurate 3D terrain so that the correlation of landscape and weather can be revealed in detail, thus enabling new predictions of flood extents, inclusion of the effects of mountains on weather phenomena, and other new capabilities. To support fast, scalable visualization, a &quot;geo-layered volume&quot; will be introduced that will take advantage of the underlying terrain global quadtree organization and out-of-core paging structure. Hierarchical visual models will be developed within the geo-layered volume structure to produce several visual representation including volume rendering, isosurfaces, simple 3D time-dependent feature representations, and iconic annotations. A fast and scalable cluster-based feature analysis method will form a dynamic feature hierarchy also. This hierarchy will be used to produce levels of detail for the volume and isosurface visualization of the 3D time-dependent observational data. The visual analyses will be put in a decision-support framework. Since the analyses attach meaning and importance to the various observations, the observational data can be displayed in the right form for easy use. Important phenomena can be given visual forms that catch the eye, and more detail can be given to objects that the analysis says are important or to objects that the user looks at more closely.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">9.9823e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.9823e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n244" target="n245">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">SGER: Hocoka: From the Heart of the Nation to the Rim of the World</data>
      <data key="e_abstract">Based on the concept of Hocoka-the center of camp life, this project will develop a pilot-scale multimedia information database designed for distribution over Internet 2 formats. Illuminating a wide array of cultural convergence on the Northern Plains, the pilot project will develop a database featuring five demonstration entries: 1) American Indian Art of Oscar Howe; 2) The Wounded Knee Occupation (1973); 3) Pioneer Missionaries; 4) The Impact of Mining in Dakota land, and 5) Sacred Practices. Each of these entries is based upon visual images of museum artifacts, audio recordings from an extensive oral history collection, and textual material from special library collections. Participating in this project are several unique cultural resources that reside at the University of South Dakota: the Institute of American Indian Studies which houses the Oral History Center containing over 1,900 interviews of American Indians; the Oscar Howe Art Collection and Archives; the W. H. Over Museum; and the Herman P. Chilson Collection of Western Americana. Technical resources brought to bear on the project include the University of South Dakota Internet 2 project and the Center for Instructional Design and Delivery.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.98741e+06</data>
      <data key="e_expirationDate">2002-04-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98741e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n251" target="n252">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">CISE Research Instrumentation: High-Speed Motion Acquisition</data>
      <data key="e_abstract">EIA-9986010&lt;br/&gt;Christoph Bregler&lt;br/&gt;Stanford University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: High-Speed Motion Acquisition&lt;br/&gt;&lt;br/&gt;The Departments of Computer Science and Mechanical Engineering will purchase 2 high-speed multi-camera setups, a RAID videodisk, and 2 Pentium PC&apos;s and an animation software package. This equipment will be dedicated to the support of research in computer and information sciences and engineering. The equipment will be used for several research areas including, in particular, projects addressing:&lt;br/&gt;&lt;br/&gt; Anterior Cruciate Ligament-Functional Biocmechanics&lt;br/&gt; Human Motion Models for Tracking and Activity Interpretation&lt;br/&gt; 3D Motion Acquisition for the Stanford Immersive Television Project&lt;br/&gt; Animation of realistic and believable characters&lt;br/&gt;&lt;br/&gt;The general thrust of these efforts is to develop high-quality acquisition and analysis techniques of human movements.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98601e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98601e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n251" target="n253">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">CISE Research Instrumentation: High-Speed Motion Acquisition</data>
      <data key="e_abstract">EIA-9986010&lt;br/&gt;Christoph Bregler&lt;br/&gt;Stanford University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: High-Speed Motion Acquisition&lt;br/&gt;&lt;br/&gt;The Departments of Computer Science and Mechanical Engineering will purchase 2 high-speed multi-camera setups, a RAID videodisk, and 2 Pentium PC&apos;s and an animation software package. This equipment will be dedicated to the support of research in computer and information sciences and engineering. The equipment will be used for several research areas including, in particular, projects addressing:&lt;br/&gt;&lt;br/&gt; Anterior Cruciate Ligament-Functional Biocmechanics&lt;br/&gt; Human Motion Models for Tracking and Activity Interpretation&lt;br/&gt; 3D Motion Acquisition for the Stanford Immersive Television Project&lt;br/&gt; Animation of realistic and believable characters&lt;br/&gt;&lt;br/&gt;The general thrust of these efforts is to develop high-quality acquisition and analysis techniques of human movements.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98601e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98601e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n251" target="n254">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">CISE Research Instrumentation: High-Speed Motion Acquisition</data>
      <data key="e_abstract">EIA-9986010&lt;br/&gt;Christoph Bregler&lt;br/&gt;Stanford University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: High-Speed Motion Acquisition&lt;br/&gt;&lt;br/&gt;The Departments of Computer Science and Mechanical Engineering will purchase 2 high-speed multi-camera setups, a RAID videodisk, and 2 Pentium PC&apos;s and an animation software package. This equipment will be dedicated to the support of research in computer and information sciences and engineering. The equipment will be used for several research areas including, in particular, projects addressing:&lt;br/&gt;&lt;br/&gt; Anterior Cruciate Ligament-Functional Biocmechanics&lt;br/&gt; Human Motion Models for Tracking and Activity Interpretation&lt;br/&gt; 3D Motion Acquisition for the Stanford Immersive Television Project&lt;br/&gt; Animation of realistic and believable characters&lt;br/&gt;&lt;br/&gt;The general thrust of these efforts is to develop high-quality acquisition and analysis techniques of human movements.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98601e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98601e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n251" target="n255">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">CISE Research Instrumentation: High-Speed Motion Acquisition</data>
      <data key="e_abstract">EIA-9986010&lt;br/&gt;Christoph Bregler&lt;br/&gt;Stanford University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: High-Speed Motion Acquisition&lt;br/&gt;&lt;br/&gt;The Departments of Computer Science and Mechanical Engineering will purchase 2 high-speed multi-camera setups, a RAID videodisk, and 2 Pentium PC&apos;s and an animation software package. This equipment will be dedicated to the support of research in computer and information sciences and engineering. The equipment will be used for several research areas including, in particular, projects addressing:&lt;br/&gt;&lt;br/&gt; Anterior Cruciate Ligament-Functional Biocmechanics&lt;br/&gt; Human Motion Models for Tracking and Activity Interpretation&lt;br/&gt; 3D Motion Acquisition for the Stanford Immersive Television Project&lt;br/&gt; Animation of realistic and believable characters&lt;br/&gt;&lt;br/&gt;The general thrust of these efforts is to develop high-quality acquisition and analysis techniques of human movements.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98601e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98601e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n252" target="n253">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">CISE Research Instrumentation: High-Speed Motion Acquisition</data>
      <data key="e_abstract">EIA-9986010&lt;br/&gt;Christoph Bregler&lt;br/&gt;Stanford University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: High-Speed Motion Acquisition&lt;br/&gt;&lt;br/&gt;The Departments of Computer Science and Mechanical Engineering will purchase 2 high-speed multi-camera setups, a RAID videodisk, and 2 Pentium PC&apos;s and an animation software package. This equipment will be dedicated to the support of research in computer and information sciences and engineering. The equipment will be used for several research areas including, in particular, projects addressing:&lt;br/&gt;&lt;br/&gt; Anterior Cruciate Ligament-Functional Biocmechanics&lt;br/&gt; Human Motion Models for Tracking and Activity Interpretation&lt;br/&gt; 3D Motion Acquisition for the Stanford Immersive Television Project&lt;br/&gt; Animation of realistic and believable characters&lt;br/&gt;&lt;br/&gt;The general thrust of these efforts is to develop high-quality acquisition and analysis techniques of human movements.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98601e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98601e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n252" target="n254">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">CISE Research Instrumentation: High-Speed Motion Acquisition</data>
      <data key="e_abstract">EIA-9986010&lt;br/&gt;Christoph Bregler&lt;br/&gt;Stanford University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: High-Speed Motion Acquisition&lt;br/&gt;&lt;br/&gt;The Departments of Computer Science and Mechanical Engineering will purchase 2 high-speed multi-camera setups, a RAID videodisk, and 2 Pentium PC&apos;s and an animation software package. This equipment will be dedicated to the support of research in computer and information sciences and engineering. The equipment will be used for several research areas including, in particular, projects addressing:&lt;br/&gt;&lt;br/&gt; Anterior Cruciate Ligament-Functional Biocmechanics&lt;br/&gt; Human Motion Models for Tracking and Activity Interpretation&lt;br/&gt; 3D Motion Acquisition for the Stanford Immersive Television Project&lt;br/&gt; Animation of realistic and believable characters&lt;br/&gt;&lt;br/&gt;The general thrust of these efforts is to develop high-quality acquisition and analysis techniques of human movements.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98601e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98601e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n252" target="n255">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">CISE Research Instrumentation: High-Speed Motion Acquisition</data>
      <data key="e_abstract">EIA-9986010&lt;br/&gt;Christoph Bregler&lt;br/&gt;Stanford University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: High-Speed Motion Acquisition&lt;br/&gt;&lt;br/&gt;The Departments of Computer Science and Mechanical Engineering will purchase 2 high-speed multi-camera setups, a RAID videodisk, and 2 Pentium PC&apos;s and an animation software package. This equipment will be dedicated to the support of research in computer and information sciences and engineering. The equipment will be used for several research areas including, in particular, projects addressing:&lt;br/&gt;&lt;br/&gt; Anterior Cruciate Ligament-Functional Biocmechanics&lt;br/&gt; Human Motion Models for Tracking and Activity Interpretation&lt;br/&gt; 3D Motion Acquisition for the Stanford Immersive Television Project&lt;br/&gt; Animation of realistic and believable characters&lt;br/&gt;&lt;br/&gt;The general thrust of these efforts is to develop high-quality acquisition and analysis techniques of human movements.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98601e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98601e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n253" target="n254">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">CISE Research Instrumentation: High-Speed Motion Acquisition</data>
      <data key="e_abstract">EIA-9986010&lt;br/&gt;Christoph Bregler&lt;br/&gt;Stanford University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: High-Speed Motion Acquisition&lt;br/&gt;&lt;br/&gt;The Departments of Computer Science and Mechanical Engineering will purchase 2 high-speed multi-camera setups, a RAID videodisk, and 2 Pentium PC&apos;s and an animation software package. This equipment will be dedicated to the support of research in computer and information sciences and engineering. The equipment will be used for several research areas including, in particular, projects addressing:&lt;br/&gt;&lt;br/&gt; Anterior Cruciate Ligament-Functional Biocmechanics&lt;br/&gt; Human Motion Models for Tracking and Activity Interpretation&lt;br/&gt; 3D Motion Acquisition for the Stanford Immersive Television Project&lt;br/&gt; Animation of realistic and believable characters&lt;br/&gt;&lt;br/&gt;The general thrust of these efforts is to develop high-quality acquisition and analysis techniques of human movements.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98601e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98601e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n253" target="n255">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">CISE Research Instrumentation: High-Speed Motion Acquisition</data>
      <data key="e_abstract">EIA-9986010&lt;br/&gt;Christoph Bregler&lt;br/&gt;Stanford University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: High-Speed Motion Acquisition&lt;br/&gt;&lt;br/&gt;The Departments of Computer Science and Mechanical Engineering will purchase 2 high-speed multi-camera setups, a RAID videodisk, and 2 Pentium PC&apos;s and an animation software package. This equipment will be dedicated to the support of research in computer and information sciences and engineering. The equipment will be used for several research areas including, in particular, projects addressing:&lt;br/&gt;&lt;br/&gt; Anterior Cruciate Ligament-Functional Biocmechanics&lt;br/&gt; Human Motion Models for Tracking and Activity Interpretation&lt;br/&gt; 3D Motion Acquisition for the Stanford Immersive Television Project&lt;br/&gt; Animation of realistic and believable characters&lt;br/&gt;&lt;br/&gt;The general thrust of these efforts is to develop high-quality acquisition and analysis techniques of human movements.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98601e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98601e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n254" target="n255">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">CISE Research Instrumentation: High-Speed Motion Acquisition</data>
      <data key="e_abstract">EIA-9986010&lt;br/&gt;Christoph Bregler&lt;br/&gt;Stanford University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: High-Speed Motion Acquisition&lt;br/&gt;&lt;br/&gt;The Departments of Computer Science and Mechanical Engineering will purchase 2 high-speed multi-camera setups, a RAID videodisk, and 2 Pentium PC&apos;s and an animation software package. This equipment will be dedicated to the support of research in computer and information sciences and engineering. The equipment will be used for several research areas including, in particular, projects addressing:&lt;br/&gt;&lt;br/&gt; Anterior Cruciate Ligament-Functional Biocmechanics&lt;br/&gt; Human Motion Models for Tracking and Activity Interpretation&lt;br/&gt; 3D Motion Acquisition for the Stanford Immersive Television Project&lt;br/&gt; Animation of realistic and believable characters&lt;br/&gt;&lt;br/&gt;The general thrust of these efforts is to develop high-quality acquisition and analysis techniques of human movements.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98601e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98601e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n258" target="n259">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">CISE Research Instrumentation: Network Computing Testbed for Interactive Visualization, Multimedia, and Metacomputing</data>
      <data key="e_abstract">EIA-9986052&lt;br/&gt;Panda K. Dhabaleswar&lt;br/&gt;Ohio State University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Network Computing Testbed for Interactive Visualization, Multimedia, and Metacomputing &lt;br/&gt;&lt;br/&gt;The Department of Computer and Information Science at the Ohio State University will purchase a network computing testbed (consisting of a set of Quad SMP Pentium systems, Gigabit Ethernet and Myrinet interconnects for intra-cluster communication, ATM interconnect for intra-cluster and WAN connectivity, a RAID File system, a set of 3D accelerator boards, and a video wall), which will be dedicated to support research in computer and information science and engineering. The equipment will be used for several research projects, including in particular:&lt;br/&gt;&lt;br/&gt;Scalable Communication and Synchronization Support on SMP Clusters for Metacomputing.&lt;br/&gt;&lt;br/&gt;A High Performance Image-Based Warehouse Across Heterogeneous Environment&lt;br/&gt;&lt;br/&gt;Parallel Visualization and Display of Tera-scale Data &lt;br/&gt;&lt;br/&gt;Multimedia Processing and Networking on SMP Clusters</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98605e+06</data>
      <data key="e_expirationDate">2003-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98605e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n258" target="n260">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">CISE Research Instrumentation: Network Computing Testbed for Interactive Visualization, Multimedia, and Metacomputing</data>
      <data key="e_abstract">EIA-9986052&lt;br/&gt;Panda K. Dhabaleswar&lt;br/&gt;Ohio State University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Network Computing Testbed for Interactive Visualization, Multimedia, and Metacomputing &lt;br/&gt;&lt;br/&gt;The Department of Computer and Information Science at the Ohio State University will purchase a network computing testbed (consisting of a set of Quad SMP Pentium systems, Gigabit Ethernet and Myrinet interconnects for intra-cluster communication, ATM interconnect for intra-cluster and WAN connectivity, a RAID File system, a set of 3D accelerator boards, and a video wall), which will be dedicated to support research in computer and information science and engineering. The equipment will be used for several research projects, including in particular:&lt;br/&gt;&lt;br/&gt;Scalable Communication and Synchronization Support on SMP Clusters for Metacomputing.&lt;br/&gt;&lt;br/&gt;A High Performance Image-Based Warehouse Across Heterogeneous Environment&lt;br/&gt;&lt;br/&gt;Parallel Visualization and Display of Tera-scale Data &lt;br/&gt;&lt;br/&gt;Multimedia Processing and Networking on SMP Clusters</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98605e+06</data>
      <data key="e_expirationDate">2003-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98605e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n258" target="n261">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">CISE Research Instrumentation: Network Computing Testbed for Interactive Visualization, Multimedia, and Metacomputing</data>
      <data key="e_abstract">EIA-9986052&lt;br/&gt;Panda K. Dhabaleswar&lt;br/&gt;Ohio State University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Network Computing Testbed for Interactive Visualization, Multimedia, and Metacomputing &lt;br/&gt;&lt;br/&gt;The Department of Computer and Information Science at the Ohio State University will purchase a network computing testbed (consisting of a set of Quad SMP Pentium systems, Gigabit Ethernet and Myrinet interconnects for intra-cluster communication, ATM interconnect for intra-cluster and WAN connectivity, a RAID File system, a set of 3D accelerator boards, and a video wall), which will be dedicated to support research in computer and information science and engineering. The equipment will be used for several research projects, including in particular:&lt;br/&gt;&lt;br/&gt;Scalable Communication and Synchronization Support on SMP Clusters for Metacomputing.&lt;br/&gt;&lt;br/&gt;A High Performance Image-Based Warehouse Across Heterogeneous Environment&lt;br/&gt;&lt;br/&gt;Parallel Visualization and Display of Tera-scale Data &lt;br/&gt;&lt;br/&gt;Multimedia Processing and Networking on SMP Clusters</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98605e+06</data>
      <data key="e_expirationDate">2003-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98605e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n258" target="n262">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">CISE Research Instrumentation: Network Computing Testbed for Interactive Visualization, Multimedia, and Metacomputing</data>
      <data key="e_abstract">EIA-9986052&lt;br/&gt;Panda K. Dhabaleswar&lt;br/&gt;Ohio State University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Network Computing Testbed for Interactive Visualization, Multimedia, and Metacomputing &lt;br/&gt;&lt;br/&gt;The Department of Computer and Information Science at the Ohio State University will purchase a network computing testbed (consisting of a set of Quad SMP Pentium systems, Gigabit Ethernet and Myrinet interconnects for intra-cluster communication, ATM interconnect for intra-cluster and WAN connectivity, a RAID File system, a set of 3D accelerator boards, and a video wall), which will be dedicated to support research in computer and information science and engineering. The equipment will be used for several research projects, including in particular:&lt;br/&gt;&lt;br/&gt;Scalable Communication and Synchronization Support on SMP Clusters for Metacomputing.&lt;br/&gt;&lt;br/&gt;A High Performance Image-Based Warehouse Across Heterogeneous Environment&lt;br/&gt;&lt;br/&gt;Parallel Visualization and Display of Tera-scale Data &lt;br/&gt;&lt;br/&gt;Multimedia Processing and Networking on SMP Clusters</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98605e+06</data>
      <data key="e_expirationDate">2003-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98605e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n259" target="n260">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">CISE Research Instrumentation: Network Computing Testbed for Interactive Visualization, Multimedia, and Metacomputing</data>
      <data key="e_abstract">EIA-9986052&lt;br/&gt;Panda K. Dhabaleswar&lt;br/&gt;Ohio State University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Network Computing Testbed for Interactive Visualization, Multimedia, and Metacomputing &lt;br/&gt;&lt;br/&gt;The Department of Computer and Information Science at the Ohio State University will purchase a network computing testbed (consisting of a set of Quad SMP Pentium systems, Gigabit Ethernet and Myrinet interconnects for intra-cluster communication, ATM interconnect for intra-cluster and WAN connectivity, a RAID File system, a set of 3D accelerator boards, and a video wall), which will be dedicated to support research in computer and information science and engineering. The equipment will be used for several research projects, including in particular:&lt;br/&gt;&lt;br/&gt;Scalable Communication and Synchronization Support on SMP Clusters for Metacomputing.&lt;br/&gt;&lt;br/&gt;A High Performance Image-Based Warehouse Across Heterogeneous Environment&lt;br/&gt;&lt;br/&gt;Parallel Visualization and Display of Tera-scale Data &lt;br/&gt;&lt;br/&gt;Multimedia Processing and Networking on SMP Clusters</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98605e+06</data>
      <data key="e_expirationDate">2003-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98605e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n259" target="n261">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">CISE Research Instrumentation: Network Computing Testbed for Interactive Visualization, Multimedia, and Metacomputing</data>
      <data key="e_abstract">EIA-9986052&lt;br/&gt;Panda K. Dhabaleswar&lt;br/&gt;Ohio State University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Network Computing Testbed for Interactive Visualization, Multimedia, and Metacomputing &lt;br/&gt;&lt;br/&gt;The Department of Computer and Information Science at the Ohio State University will purchase a network computing testbed (consisting of a set of Quad SMP Pentium systems, Gigabit Ethernet and Myrinet interconnects for intra-cluster communication, ATM interconnect for intra-cluster and WAN connectivity, a RAID File system, a set of 3D accelerator boards, and a video wall), which will be dedicated to support research in computer and information science and engineering. The equipment will be used for several research projects, including in particular:&lt;br/&gt;&lt;br/&gt;Scalable Communication and Synchronization Support on SMP Clusters for Metacomputing.&lt;br/&gt;&lt;br/&gt;A High Performance Image-Based Warehouse Across Heterogeneous Environment&lt;br/&gt;&lt;br/&gt;Parallel Visualization and Display of Tera-scale Data &lt;br/&gt;&lt;br/&gt;Multimedia Processing and Networking on SMP Clusters</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98605e+06</data>
      <data key="e_expirationDate">2003-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98605e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n259" target="n262">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">CISE Research Instrumentation: Network Computing Testbed for Interactive Visualization, Multimedia, and Metacomputing</data>
      <data key="e_abstract">EIA-9986052&lt;br/&gt;Panda K. Dhabaleswar&lt;br/&gt;Ohio State University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Network Computing Testbed for Interactive Visualization, Multimedia, and Metacomputing &lt;br/&gt;&lt;br/&gt;The Department of Computer and Information Science at the Ohio State University will purchase a network computing testbed (consisting of a set of Quad SMP Pentium systems, Gigabit Ethernet and Myrinet interconnects for intra-cluster communication, ATM interconnect for intra-cluster and WAN connectivity, a RAID File system, a set of 3D accelerator boards, and a video wall), which will be dedicated to support research in computer and information science and engineering. The equipment will be used for several research projects, including in particular:&lt;br/&gt;&lt;br/&gt;Scalable Communication and Synchronization Support on SMP Clusters for Metacomputing.&lt;br/&gt;&lt;br/&gt;A High Performance Image-Based Warehouse Across Heterogeneous Environment&lt;br/&gt;&lt;br/&gt;Parallel Visualization and Display of Tera-scale Data &lt;br/&gt;&lt;br/&gt;Multimedia Processing and Networking on SMP Clusters</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98605e+06</data>
      <data key="e_expirationDate">2003-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98605e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n260" target="n261">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">CISE Research Instrumentation: Network Computing Testbed for Interactive Visualization, Multimedia, and Metacomputing</data>
      <data key="e_abstract">EIA-9986052&lt;br/&gt;Panda K. Dhabaleswar&lt;br/&gt;Ohio State University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Network Computing Testbed for Interactive Visualization, Multimedia, and Metacomputing &lt;br/&gt;&lt;br/&gt;The Department of Computer and Information Science at the Ohio State University will purchase a network computing testbed (consisting of a set of Quad SMP Pentium systems, Gigabit Ethernet and Myrinet interconnects for intra-cluster communication, ATM interconnect for intra-cluster and WAN connectivity, a RAID File system, a set of 3D accelerator boards, and a video wall), which will be dedicated to support research in computer and information science and engineering. The equipment will be used for several research projects, including in particular:&lt;br/&gt;&lt;br/&gt;Scalable Communication and Synchronization Support on SMP Clusters for Metacomputing.&lt;br/&gt;&lt;br/&gt;A High Performance Image-Based Warehouse Across Heterogeneous Environment&lt;br/&gt;&lt;br/&gt;Parallel Visualization and Display of Tera-scale Data &lt;br/&gt;&lt;br/&gt;Multimedia Processing and Networking on SMP Clusters</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98605e+06</data>
      <data key="e_expirationDate">2003-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98605e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n260" target="n262">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">CISE Research Instrumentation: Network Computing Testbed for Interactive Visualization, Multimedia, and Metacomputing</data>
      <data key="e_abstract">EIA-9986052&lt;br/&gt;Panda K. Dhabaleswar&lt;br/&gt;Ohio State University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Network Computing Testbed for Interactive Visualization, Multimedia, and Metacomputing &lt;br/&gt;&lt;br/&gt;The Department of Computer and Information Science at the Ohio State University will purchase a network computing testbed (consisting of a set of Quad SMP Pentium systems, Gigabit Ethernet and Myrinet interconnects for intra-cluster communication, ATM interconnect for intra-cluster and WAN connectivity, a RAID File system, a set of 3D accelerator boards, and a video wall), which will be dedicated to support research in computer and information science and engineering. The equipment will be used for several research projects, including in particular:&lt;br/&gt;&lt;br/&gt;Scalable Communication and Synchronization Support on SMP Clusters for Metacomputing.&lt;br/&gt;&lt;br/&gt;A High Performance Image-Based Warehouse Across Heterogeneous Environment&lt;br/&gt;&lt;br/&gt;Parallel Visualization and Display of Tera-scale Data &lt;br/&gt;&lt;br/&gt;Multimedia Processing and Networking on SMP Clusters</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98605e+06</data>
      <data key="e_expirationDate">2003-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98605e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n261" target="n262">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">CISE Research Instrumentation: Network Computing Testbed for Interactive Visualization, Multimedia, and Metacomputing</data>
      <data key="e_abstract">EIA-9986052&lt;br/&gt;Panda K. Dhabaleswar&lt;br/&gt;Ohio State University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Network Computing Testbed for Interactive Visualization, Multimedia, and Metacomputing &lt;br/&gt;&lt;br/&gt;The Department of Computer and Information Science at the Ohio State University will purchase a network computing testbed (consisting of a set of Quad SMP Pentium systems, Gigabit Ethernet and Myrinet interconnects for intra-cluster communication, ATM interconnect for intra-cluster and WAN connectivity, a RAID File system, a set of 3D accelerator boards, and a video wall), which will be dedicated to support research in computer and information science and engineering. The equipment will be used for several research projects, including in particular:&lt;br/&gt;&lt;br/&gt;Scalable Communication and Synchronization Support on SMP Clusters for Metacomputing.&lt;br/&gt;&lt;br/&gt;A High Performance Image-Based Warehouse Across Heterogeneous Environment&lt;br/&gt;&lt;br/&gt;Parallel Visualization and Display of Tera-scale Data &lt;br/&gt;&lt;br/&gt;Multimedia Processing and Networking on SMP Clusters</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98605e+06</data>
      <data key="e_expirationDate">2003-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98605e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n263" target="n264">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">Distributed Numerical Integration Algorithms and Application</data>
      <data key="e_abstract">Computing the values of integrals is one of the fundamental problems of calculus and its applications; numerical integration solves this problem for complex functions that cannot be handled analytically. This project will significantly extend the ParInt 1.0 system for performing numerical integration developed under previous NSF support. It will add techniques that enable the system to handle integration problems with a variety of difficult characteristics (e.g. singularities, high dimensions, etc.). This includes the development of a hierarchical process structure for the computation of large collections of integrals (e.g. finite element problems), extrapolation techniques for singular problems, and Quasi-Monte Carlo techniques for solving problems of high dimensions (e.g. computational finance). Corresponding additions to the package&apos;s graphical interface will allow for easy use across research disciplines. In particular, visualization tools to help the user see why a problem is difficult (and suggest alternative formulations) and a server allowing users to submit integration problems remotely will be incorporated.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">442</data>
      <data key="e_expirationDate">2003-04-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">442</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n263" target="n265">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">Distributed Numerical Integration Algorithms and Application</data>
      <data key="e_abstract">Computing the values of integrals is one of the fundamental problems of calculus and its applications; numerical integration solves this problem for complex functions that cannot be handled analytically. This project will significantly extend the ParInt 1.0 system for performing numerical integration developed under previous NSF support. It will add techniques that enable the system to handle integration problems with a variety of difficult characteristics (e.g. singularities, high dimensions, etc.). This includes the development of a hierarchical process structure for the computation of large collections of integrals (e.g. finite element problems), extrapolation techniques for singular problems, and Quasi-Monte Carlo techniques for solving problems of high dimensions (e.g. computational finance). Corresponding additions to the package&apos;s graphical interface will allow for easy use across research disciplines. In particular, visualization tools to help the user see why a problem is difficult (and suggest alternative formulations) and a server allowing users to submit integration problems remotely will be incorporated.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">442</data>
      <data key="e_expirationDate">2003-04-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">442</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n264" target="n265">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">Distributed Numerical Integration Algorithms and Application</data>
      <data key="e_abstract">Computing the values of integrals is one of the fundamental problems of calculus and its applications; numerical integration solves this problem for complex functions that cannot be handled analytically. This project will significantly extend the ParInt 1.0 system for performing numerical integration developed under previous NSF support. It will add techniques that enable the system to handle integration problems with a variety of difficult characteristics (e.g. singularities, high dimensions, etc.). This includes the development of a hierarchical process structure for the computation of large collections of integrals (e.g. finite element problems), extrapolation techniques for singular problems, and Quasi-Monte Carlo techniques for solving problems of high dimensions (e.g. computational finance). Corresponding additions to the package&apos;s graphical interface will allow for easy use across research disciplines. In particular, visualization tools to help the user see why a problem is difficult (and suggest alternative formulations) and a server allowing users to submit integration problems remotely will be incorporated.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">442</data>
      <data key="e_expirationDate">2003-04-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">442</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n269" target="n270">
      <data key="e_effectiveDate">2000-05-15</data>
      <data key="e_title">Projection Methods for Balanced Model Reduction</data>
      <data key="e_abstract">This project proposes the development of reduced models for large-scale&lt;br/&gt;linear time invariant (LTI) systems of the form&lt;br/&gt;&lt;br/&gt; dx/dt = Ax + Bu, y = Cx&lt;br/&gt;&lt;br/&gt;through low rank approximation of certain system Grammians. Complex systems &lt;br/&gt;of this form arise in circuit simulation; they also arise through spatial &lt;br/&gt;discretization of certain time dependent PDE control systems.&lt;br/&gt;&lt;br/&gt;A new approach to model reduction will be studied, that will address some &lt;br/&gt;fundamental difficulties with existing dimension reduction techniques. &lt;br/&gt;These issues are central to the potential development of robust and widely &lt;br/&gt;applicable software. We intend to develop a computational methodology that &lt;br/&gt;will:&lt;br/&gt;&lt;br/&gt;(1) Provide rigorous bounds on the response error of the reduced system; &lt;br/&gt;(2) Naturally preserve fundamental system properties such as stability;&lt;br/&gt;(3) Be fully automatic once a desired error tolerance is specified. &lt;br/&gt;&lt;br/&gt;We propose to investigate subspace projection methods of Krylov and &lt;br/&gt;non-Krylov type. Our primary goal will be to develop implicit restarting&lt;br/&gt;methods that will iteratively produce low rank approximations to &lt;br/&gt;controllability-, observability-, and cross-Grammians which are near best &lt;br/&gt;approximations of reduced rank to the full Grammians.&lt;br/&gt;&lt;br/&gt;The methods proposed will provide balanced partial realizations of large &lt;br/&gt;state space systems. Our computational experience with model problems &lt;br/&gt;indicates that the Hankel singular values of many systems decay extremely &lt;br/&gt;rapidly. Hence very low rank approximations to the system Grammians are &lt;br/&gt;possible and accurate low order reduced models will result. &lt;br/&gt;&lt;br/&gt;We expect to find important applications in circuit simulation and in &lt;br/&gt;the control of systems governed by partial differential equations,&lt;br/&gt;particularly parabolic equations subject to boundary control. There is &lt;br/&gt;great potential in such problems for extensive dimension reduction. This &lt;br/&gt;will enable the design of real-time controllers for complex systems.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">9.98839e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98839e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n271" target="n272">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">Dynamic Data Structures in Advanced Programming Languages Implementation: Theory and Practice</data>
      <data key="e_abstract">Over the past several years there has been an advent of a host of new programming methodologies and paradigms based on sophisticated features like concurrency and parallelism. These programming features and paradigms provide users with high levels of abstraction, enabling them to stay closer to the problem domains. Supporting these features causes a considerable increase in the interpretation/compilation overhead, leading to complex and relatively slow implementations. Experience in the development and implementation of programming systems based on these new paradigms shows that one of the main sources of inefficiency is the repeated specialized searches that must be performed to realize the correct semantics at run-time. To limit inefficiency, existing languages and implementations restrict the expressiveness of the constructs.&lt;br/&gt;The aim of this project is to study the search-related problems that arise in sequential and parallel implementation of advanced programming paradigms, with specific attention to logic programming and non-deterministic languages. The specific goals of this study are: (i) to abstract the key implementation aspects of these programming frameworks as problems of design and implementation of dynamic data structures;&lt;br/&gt;(ii) to design efficient data structures for the resulting problems and analyze their complexity; and, (iii) to use the results from the complexity analysis to quantitatively evaluate the various implementation schemes described in the literature as well as to guide the design of efficient dynamic data structures and algorithms that will lead to optimal implementations.&lt;br/&gt;Precise complexity measures of the solutions to these problems would allow system implementors to quantitatively compare different execution models, propose and develop optimal implementations, and talk about performance guarantees and system efficiency in a formal and mathematically precise fashion. These results would also be of importance to the users of these implementations, since they will allow them to understand the implementation-wise troublesome aspects of the language and to make better use of the capabilities that the language possesses. The results obtained in this research are expected to be directly applicable to the specific programming paradigms studied here. Furthermore, these results are expected to be transferable to other domains like constraint management and semantic networks, where efficient specialized tree and graph searches are key components of the execution models.</data>
      <data key="e_pgm">2860</data>
      <data key="e_label">9.82085e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.82085e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n271" target="n273">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">Dynamic Data Structures in Advanced Programming Languages Implementation: Theory and Practice</data>
      <data key="e_abstract">Over the past several years there has been an advent of a host of new programming methodologies and paradigms based on sophisticated features like concurrency and parallelism. These programming features and paradigms provide users with high levels of abstraction, enabling them to stay closer to the problem domains. Supporting these features causes a considerable increase in the interpretation/compilation overhead, leading to complex and relatively slow implementations. Experience in the development and implementation of programming systems based on these new paradigms shows that one of the main sources of inefficiency is the repeated specialized searches that must be performed to realize the correct semantics at run-time. To limit inefficiency, existing languages and implementations restrict the expressiveness of the constructs.&lt;br/&gt;The aim of this project is to study the search-related problems that arise in sequential and parallel implementation of advanced programming paradigms, with specific attention to logic programming and non-deterministic languages. The specific goals of this study are: (i) to abstract the key implementation aspects of these programming frameworks as problems of design and implementation of dynamic data structures;&lt;br/&gt;(ii) to design efficient data structures for the resulting problems and analyze their complexity; and, (iii) to use the results from the complexity analysis to quantitatively evaluate the various implementation schemes described in the literature as well as to guide the design of efficient dynamic data structures and algorithms that will lead to optimal implementations.&lt;br/&gt;Precise complexity measures of the solutions to these problems would allow system implementors to quantitatively compare different execution models, propose and develop optimal implementations, and talk about performance guarantees and system efficiency in a formal and mathematically precise fashion. These results would also be of importance to the users of these implementations, since they will allow them to understand the implementation-wise troublesome aspects of the language and to make better use of the capabilities that the language possesses. The results obtained in this research are expected to be directly applicable to the specific programming paradigms studied here. Furthermore, these results are expected to be transferable to other domains like constraint management and semantic networks, where efficient specialized tree and graph searches are key components of the execution models.</data>
      <data key="e_pgm">2860</data>
      <data key="e_label">9.82085e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.82085e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n272" target="n273">
      <data key="e_effectiveDate">2000-05-01</data>
      <data key="e_title">Dynamic Data Structures in Advanced Programming Languages Implementation: Theory and Practice</data>
      <data key="e_abstract">Over the past several years there has been an advent of a host of new programming methodologies and paradigms based on sophisticated features like concurrency and parallelism. These programming features and paradigms provide users with high levels of abstraction, enabling them to stay closer to the problem domains. Supporting these features causes a considerable increase in the interpretation/compilation overhead, leading to complex and relatively slow implementations. Experience in the development and implementation of programming systems based on these new paradigms shows that one of the main sources of inefficiency is the repeated specialized searches that must be performed to realize the correct semantics at run-time. To limit inefficiency, existing languages and implementations restrict the expressiveness of the constructs.&lt;br/&gt;The aim of this project is to study the search-related problems that arise in sequential and parallel implementation of advanced programming paradigms, with specific attention to logic programming and non-deterministic languages. The specific goals of this study are: (i) to abstract the key implementation aspects of these programming frameworks as problems of design and implementation of dynamic data structures;&lt;br/&gt;(ii) to design efficient data structures for the resulting problems and analyze their complexity; and, (iii) to use the results from the complexity analysis to quantitatively evaluate the various implementation schemes described in the literature as well as to guide the design of efficient dynamic data structures and algorithms that will lead to optimal implementations.&lt;br/&gt;Precise complexity measures of the solutions to these problems would allow system implementors to quantitatively compare different execution models, propose and develop optimal implementations, and talk about performance guarantees and system efficiency in a formal and mathematically precise fashion. These results would also be of importance to the users of these implementations, since they will allow them to understand the implementation-wise troublesome aspects of the language and to make better use of the capabilities that the language possesses. The results obtained in this research are expected to be directly applicable to the specific programming paradigms studied here. Furthermore, these results are expected to be transferable to other domains like constraint management and semantic networks, where efficient specialized tree and graph searches are key components of the execution models.</data>
      <data key="e_pgm">2860</data>
      <data key="e_label">9.82085e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.82085e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n279" target="n280">
      <data key="e_effectiveDate">2000-06-15</data>
      <data key="e_title">CISE Research Instrumentation: System Support for Mobile and Embedded Workloads</data>
      <data key="e_abstract">EIA-9986024&lt;br/&gt;Carla Ellis&lt;br/&gt;Duke University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: System Support for Mobile and Embedded Workloads&lt;br/&gt;&lt;br/&gt;This proposal seeks funding to deploy a wireless infrastructure in the Computer Science Department at Duke University. &lt;br/&gt;&lt;br/&gt;The requested infrastructure will be used as a testbed for our research. The individual research results will be combined to provide a coherent system for the deployment of mobile and embedded applications. A goal of this research is to evaluate the success of the individual system components by demonstrating the viability of the disaster recovery application, in effect approximating next generation environments using currently available technology. In addition to these research results, the requested infrastructure will aid the department&apos;s continuing efforts into education and outreach. The equipment will serve as the basis of a project-oriented course to develop applications and system support for the infrastructure and will also serve as the basis for summer internship projects in Duke&apos;s continuing outreach efforts to underrepresented groups.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98602e+06</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98602e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n279" target="n281">
      <data key="e_effectiveDate">2000-06-15</data>
      <data key="e_title">CISE Research Instrumentation: System Support for Mobile and Embedded Workloads</data>
      <data key="e_abstract">EIA-9986024&lt;br/&gt;Carla Ellis&lt;br/&gt;Duke University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: System Support for Mobile and Embedded Workloads&lt;br/&gt;&lt;br/&gt;This proposal seeks funding to deploy a wireless infrastructure in the Computer Science Department at Duke University. &lt;br/&gt;&lt;br/&gt;The requested infrastructure will be used as a testbed for our research. The individual research results will be combined to provide a coherent system for the deployment of mobile and embedded applications. A goal of this research is to evaluate the success of the individual system components by demonstrating the viability of the disaster recovery application, in effect approximating next generation environments using currently available technology. In addition to these research results, the requested infrastructure will aid the department&apos;s continuing efforts into education and outreach. The equipment will serve as the basis of a project-oriented course to develop applications and system support for the infrastructure and will also serve as the basis for summer internship projects in Duke&apos;s continuing outreach efforts to underrepresented groups.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98602e+06</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98602e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n280" target="n281">
      <data key="e_effectiveDate">2000-06-15</data>
      <data key="e_title">CISE Research Instrumentation: System Support for Mobile and Embedded Workloads</data>
      <data key="e_abstract">EIA-9986024&lt;br/&gt;Carla Ellis&lt;br/&gt;Duke University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: System Support for Mobile and Embedded Workloads&lt;br/&gt;&lt;br/&gt;This proposal seeks funding to deploy a wireless infrastructure in the Computer Science Department at Duke University. &lt;br/&gt;&lt;br/&gt;The requested infrastructure will be used as a testbed for our research. The individual research results will be combined to provide a coherent system for the deployment of mobile and embedded applications. A goal of this research is to evaluate the success of the individual system components by demonstrating the viability of the disaster recovery application, in effect approximating next generation environments using currently available technology. In addition to these research results, the requested infrastructure will aid the department&apos;s continuing efforts into education and outreach. The equipment will serve as the basis of a project-oriented course to develop applications and system support for the infrastructure and will also serve as the basis for summer internship projects in Duke&apos;s continuing outreach efforts to underrepresented groups.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98602e+06</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98602e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n284" target="n285">
      <data key="e_effectiveDate">2000-06-15</data>
      <data key="e_title">WORKSHOP: Yosemite Workshop on Multiscale and Multiresolution Methods 10/29-11/01/2000, Yosemite Lodge, Yosemite National Park</data>
      <data key="e_abstract">This award supports a three day symposium devoted to the topic of multiscale and multiresolution methods in computational science. Arguably, multiscale and multiresolution methods represent the most important tool in obtaining asymptotic scalability of large scale computations and data manipulation. Although a well-known paradigm of computation, recent developments in the general area of multiscale computation such as, multiscale finite element methods as well as new developments in the area of specialized multiresolution representations such as curvelets and ridgelets have suggested important new application areas for these techniques which will be highlighted by speakers at the symposium. Since multiscale and multiresolution techniques are just beginning to penetrate diverse areas of scientific computing such as molecular dynamics and scientific visualization, one goal of the symposium is to bring together as many differing perspectives on the subject as possible with the goal and expectation of inspiring new avenues of research and application.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">75140</data>
      <data key="e_expirationDate">2001-05-31</data>
      <data key="e_div">0111</data>
      <data key="e_awardID">75140</data>
      <data key="e_dir">01</data>
    </edge>
    <edge source="n284" target="n286">
      <data key="e_effectiveDate">2000-06-15</data>
      <data key="e_title">WORKSHOP: Yosemite Workshop on Multiscale and Multiresolution Methods 10/29-11/01/2000, Yosemite Lodge, Yosemite National Park</data>
      <data key="e_abstract">This award supports a three day symposium devoted to the topic of multiscale and multiresolution methods in computational science. Arguably, multiscale and multiresolution methods represent the most important tool in obtaining asymptotic scalability of large scale computations and data manipulation. Although a well-known paradigm of computation, recent developments in the general area of multiscale computation such as, multiscale finite element methods as well as new developments in the area of specialized multiresolution representations such as curvelets and ridgelets have suggested important new application areas for these techniques which will be highlighted by speakers at the symposium. Since multiscale and multiresolution techniques are just beginning to penetrate diverse areas of scientific computing such as molecular dynamics and scientific visualization, one goal of the symposium is to bring together as many differing perspectives on the subject as possible with the goal and expectation of inspiring new avenues of research and application.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">75140</data>
      <data key="e_expirationDate">2001-05-31</data>
      <data key="e_div">0111</data>
      <data key="e_awardID">75140</data>
      <data key="e_dir">01</data>
    </edge>
    <edge source="n285" target="n286">
      <data key="e_effectiveDate">2000-06-15</data>
      <data key="e_title">WORKSHOP: Yosemite Workshop on Multiscale and Multiresolution Methods 10/29-11/01/2000, Yosemite Lodge, Yosemite National Park</data>
      <data key="e_abstract">This award supports a three day symposium devoted to the topic of multiscale and multiresolution methods in computational science. Arguably, multiscale and multiresolution methods represent the most important tool in obtaining asymptotic scalability of large scale computations and data manipulation. Although a well-known paradigm of computation, recent developments in the general area of multiscale computation such as, multiscale finite element methods as well as new developments in the area of specialized multiresolution representations such as curvelets and ridgelets have suggested important new application areas for these techniques which will be highlighted by speakers at the symposium. Since multiscale and multiresolution techniques are just beginning to penetrate diverse areas of scientific computing such as molecular dynamics and scientific visualization, one goal of the symposium is to bring together as many differing perspectives on the subject as possible with the goal and expectation of inspiring new avenues of research and application.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">75140</data>
      <data key="e_expirationDate">2001-05-31</data>
      <data key="e_div">0111</data>
      <data key="e_awardID">75140</data>
      <data key="e_dir">01</data>
    </edge>
    <edge source="n289" target="n290">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">Complexity and Design of Transparent Packet Switching Networks</data>
      <data key="e_abstract">Packet switching has been extensively studied for traffic problems but there has been very little work,&lt;br/&gt;if any, on the complexity and methodical design of packet switching networks. Indeed, in a recent&lt;br/&gt;NSF workshop report, it was stressed that a wide variety of switching system architectures have been&lt;br/&gt;proposed, but little definitive evaluation or comparisons of these architectures have been made under criteria that factor in both performance and cost measures, where technology-dependent and technology-independent factors are clearly separated.&quot;&lt;br/&gt; A buffered network model was recently introduced to examine the design and complexity problems in&lt;br/&gt;packet switching networks. Building on this model, the notion of transparency is introduced in the&lt;br/&gt;proposed research as a measure of a buffered network&apos;s ability to deliver its packets at a specified set&lt;br/&gt;of traffic rates between its inputs and outputs. The notion of transparency quantifies the ability of&lt;br/&gt;a buffered network to deliver its packets much like the nonblocking property of a circuit switching&lt;br/&gt;network quantifies its ability to provide paths between its idle inputs and idle outputs. The main goal&lt;br/&gt;of the proposed research is to use the notion of transparency to investigate the basic bounds of packet&lt;br/&gt;transmission in packet switching networks, and construct packet switching networks which come close&lt;br/&gt;to meeting these bounds. Such bounds are often imposed on a packet switching network by its topology,&lt;br/&gt;buffer space, and crosspoint and routing complexity. In constrast to circuit switching networks, the&lt;br/&gt;complexity of packet switching networks has escaped the scrutiny of packet switching research so far,&lt;br/&gt;but without any information on their complexity, the performance of packet switching networks cannot&lt;br/&gt;be quantified.&lt;br/&gt; Along with the investigation of the complexity problems, the proposed research will attempt to identify&lt;br/&gt;the types of traffic and assignments for which efficient buffered networks can potentially be obtained,&lt;br/&gt;and actually construct such networks. Informally speaking an &quot;efficient buffered network&quot; is one which&lt;br/&gt;utilizes its crosspoints and buffer space as fully as possible subject to meeting a set of traffic constraints. Both unicast and multicast buffered networks will be investigated. The main tools of the proposed research will be Hall-type theorems, Pinsker&apos;s inexplicit combinatorial constructions, and explicit recursive network construction techniques. Inexplicit constructions will be used to establish the optimality and quantify the performance of explicit recursive buffered networks. The researchers anticipate that&lt;br/&gt;the complexity bounds on packet switching and recursive buffered network construction techniques to&lt;br/&gt;be obtained in the proposed research will have a strong impact on the way that packet switches are&lt;br/&gt;designed, and can pave the way for a methodical design of efficient and practical packet switching&lt;br/&gt;networks.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">9.98119e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98119e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n292" target="n293">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">Mobility-Based Clustering for Scalable and Responsive Routing in Ad-Hoc Networks</data>
      <data key="e_abstract">In this study, the researchers propose to develop a mobility-based framework for adaptive clustering and routing in wireless ad-hoc networks-an emerging class of network architecture that is characterized by its highly dynamic topology and its limited resources. Within this framework, the researchers will address several key, as yet unanswered questions, which have been raised with respect to the routing problem in ad-hoc networks. Specifically, it has been argued that to achieve acceptable routing performance, multiple routing strategies may need to act cooperatively in the same network. However, this raises the question as to what those strategies should be, and how to effectively toggle between them. Furthermore, it has been proposed that mobility information can be used to select longer-lived routes, and to improve the efficiency of route creation and maintenance. However, no well-defined mobility based metric has been proposed that reflects a quantitative measure of path stability. In this study, the researchers explore novel and significant ideas which address each of the shortcomings described above, and builds them into a unified&lt;br/&gt;routing framework. Specifically, the researchers propose to develop a well-defined mobility based routing metric, referred to as path availability. This metric is used for adaptive cluster characterization and routing in ad-hoc networks.&lt;br/&gt; The mobility-based cluster characterization provides the basis for an efficient distributed clustering algorithm which adaptively maintains a cluster organization-the size and membership in each cluster is determined dynamically by the mobility characteristics of the local nodes. Using this metric, the researchers propose to investigate pro-active intra-cluster and inter-cluster routing strategies: intra-cluster routing will be based on table-driven routing protocols that maintain up-to-date routing information regarding all cluster destinations, whereas inter-cluster routing is managed on a demand-basis by constructing routes in a dynamic hierarchical fashion as a sequence of relatively stable clusters between the source and the destination. More efficient route search is enabled by the proactive maintenance of routes within each cluster, and reactive route repair is only needed if the source or destination depart their original clusters, or if the next cluster along a route becomes unreachable. Most of the reaction to node mobility is handled locally within the clusters, hence, the far-reaching effects of topological changes are minimized. By adapting the characteristics of the cluster organization to localized node mobility patterns, the strategy is expected to perform well over a wide range of conditions. Furthermore, this framework can be extended&lt;br/&gt;to address the need to support QoS requirements for mutlimedia communications. Very few clustering strategies have defined either an adaptive clustering technique, or have factored mobility information into the clustering decision process. Hence, this work represents a significant, and sustainable contribution to the field.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">73972</data>
      <data key="e_expirationDate">2006-05-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">73972</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n296" target="n297">
      <data key="e_effectiveDate">2000-06-15</data>
      <data key="e_title">Internet Congestion Control: Design, Modeling, and Analysis</data>
      <data key="e_abstract">Future high-speed networks will be significantly larger and more complex than any existing network. The variety and interaction of the applications, the middleware and transport protocols, and the router/switch resource management algorithms will make the design, development, control, management and evaluation of these networks an exceptionally difficult task. One of the most critical components in the network control architecture is that concerned with congestion control. The Internet has relied till now on the additive increase, multiplicative decrease mechanism, found in most variations of TCP, for congestion control. There is increasing recognition, however, that new congestion control mechanisms, which will be able to coexist with TCP, are needed in future high speed networks. This is due to the increasing need to provide for heterogeneous application requirements and because of many future applications will rely on multicast. As is, TCP congestion control is inadequate for handling either heterogeneity or multicast.&lt;br/&gt; There is also a need for a better methodology for modeling and analyzing the TCP performance and the performance of coming congestion control algorithms. The development of such a methodology is especially needed to evaluate the behavior of a mix of TCP sessions and future non-TCP sessions so as to avoid any unpleasant surprises as they are deployed. This proposal describes new and fundamental research in two broad areas:&lt;br/&gt; (1)Formula-based congestion control. The researcher proposes research on a new approach to rate-based congestion control. The project introduces a formula-based approach in which a session monitors its end-end loss rate and adapts its transmission rate accordingly. The research shows how this can be used to construct a TCP-friendly rate control algorithm and then outline a research agenda for exploring and exploiting the potential of this approach to support multicast applications, and to support heterogeneous applications.&lt;br/&gt; (2)Performance evaluation of congestion control algorithms. The researcher proposes research on the performance evaluation of different congestion control algorithms based on the use of &lt;br/&gt;stochastic differential equations.&lt;br/&gt; This proposed research represents a fundamentally important step in the understanding, design, analysis of congestion control algorithms needed by the next generation transport protocols.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">9.98055e+06</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98055e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n300" target="n301">
      <data key="e_effectiveDate">2000-06-15</data>
      <data key="e_title">Parallel Algebraic Recursive Multilevel Solvers: Advances in Scalable and Robust High Performance Linear System Solution Methods</data>
      <data key="e_abstract">Parallel iterative methods are leading candidates for solving large-scale engineering and scientific problems, which usually appear as sparse linear systems. However, their robustness and overall efficiency remain mixed and problem-specific. These characteristics are closely tied to the preconditioners used as inputs to these methods. Preconditioners for general sparse linear systems remain by far the biggest stumbling block to obtaining good performance for iterative solution methods on high-performance computers in engineering and scientific applications. Accordingly, the main thrust of this project is to develop a class of preconditioning techniques based on the researchers&apos; Algebraic Recursive Multilevel Solver (ARMS) framework. The researchers will develop the new methods and test them on realistic problems arising from the researchers&apos; collaborations.&lt;br/&gt;&lt;br/&gt;The project will develop a class of parallel multi-level ILU-type preconditioning techniques using ARMS methods. Recursive multi-level ILU methods allow the unification of many standard iterative solvers into a single generic code. Their multi-level nature allows them to bridge the gap between the excellent problem-specifc performance of multigrid methods and the general-purpose nature of preconditioned Krylov solvers. The project will examine performance and scalability for classes of existing and new procedures thus obtained, including Schwartz procedures, Schur complement methods, direct solvers, and multilevel techniques. It will also conduct extensive realistic tests. In summary, this work promises advances in three important components of developing parallel solution methods: effective and scalable algorithms, use of effective computer science tools and data structures, and testing and validation.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">443</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">443</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n306" target="n307">
      <data key="e_effectiveDate">2000-06-15</data>
      <data key="e_title">Direct Brain-Computer Interfaces</data>
      <data key="e_abstract">Previous work has hinted that human brain signals may be able to control a computer. However, traditional mouse-oriented interaction techniques are tedious and error-prone for users controlling a computer with neural signals. In order to increase the efficacy of direct brain-computer interfaces, new paradigms of human-computer interaction must be devised in order to facilitate faster and more accurate control with minimal effort on the part of the user, through use of auditory, visual, haptic feedback. The PIs will explore hysteretic (&quot;nudge and shove&quot;) user interfaces as a possible means to this end, combining the new techniques with ideas adapted from Georgia Tech&apos;s &quot;Aware Home&quot; research in an effort to find ways to improve the quality of life for locked-in patients by giving them the ability through their brain signals to control features of their personal environment such as lights, television, radio, and temperature. The PIs plan to implant six new patients and to continue working with the two patients who are already implanted. This project will lay the groundwork fir innovative methods for communication (including phonemes, codes, and iconic languages that could be mapped directly to neural signals), for Internet access (including web browsing and electronic communication); and eventually for effective ways of giving implant patients control of prosthetics which could restore movement to paralyzed limbs.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.98875e+06</data>
      <data key="e_expirationDate">2001-11-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98875e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n308" target="n309">
      <data key="e_effectiveDate">2000-06-15</data>
      <data key="e_title">GOALI: Haptic Cobots</data>
      <data key="e_abstract">We propose research underlying the use of cobots for haptic display of solid models. The project brings together CAD graphics/haptics researchers at Ford Motor Company with haptics/cobot researchers at Northwestern University. A recently completed 3R haptic cobot will be the experimental testbed for the project.&lt;br/&gt;&lt;br/&gt;Large-scale haptic display opens up new opportunities, because people interact with large objects (arm size or larger) in a very different way from that in which they interact with small objects (hand size or smaller). An example is the current use of full-size &quot;clay bucks&quot; in automobile design. A sense of the feel and sweep of an automobile body panels cannot be obtained by touching a scale model of it with a finger, as current haptic displays permit. However a full sized virtual model experienced through the proprioception of whole arm motion, in conjunction with the excellent CAD graphics now available, could bring virtual prototyping and surface editing to a new level of utility.&lt;br/&gt;&lt;br/&gt;Cobot control for haptics differs markedly from robot control for haptics, because cobots use servo-steered rolling mechanisms, rather than servomotor actuators, to create virtual surfaces. The project addresses (1) development of a control methodology for the new power-injection architecture of the 3R cobot. (2) finding cobot-appropriate algorithms for haptic surface rendering directly from NURBS descriptions of surfaces. (3) deriving control laws for dynamic behaviors beyond hard surfaces, including compliant and viscous effects, inertia masking, and artificial potentials (4) finding algorithms for solid-model collision detection which allow the collision to be predicted and rendered by the cobot without exceeding its dynamic limits.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">9.98844e+06</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98844e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n308" target="n310">
      <data key="e_effectiveDate">2000-06-15</data>
      <data key="e_title">GOALI: Haptic Cobots</data>
      <data key="e_abstract">We propose research underlying the use of cobots for haptic display of solid models. The project brings together CAD graphics/haptics researchers at Ford Motor Company with haptics/cobot researchers at Northwestern University. A recently completed 3R haptic cobot will be the experimental testbed for the project.&lt;br/&gt;&lt;br/&gt;Large-scale haptic display opens up new opportunities, because people interact with large objects (arm size or larger) in a very different way from that in which they interact with small objects (hand size or smaller). An example is the current use of full-size &quot;clay bucks&quot; in automobile design. A sense of the feel and sweep of an automobile body panels cannot be obtained by touching a scale model of it with a finger, as current haptic displays permit. However a full sized virtual model experienced through the proprioception of whole arm motion, in conjunction with the excellent CAD graphics now available, could bring virtual prototyping and surface editing to a new level of utility.&lt;br/&gt;&lt;br/&gt;Cobot control for haptics differs markedly from robot control for haptics, because cobots use servo-steered rolling mechanisms, rather than servomotor actuators, to create virtual surfaces. The project addresses (1) development of a control methodology for the new power-injection architecture of the 3R cobot. (2) finding cobot-appropriate algorithms for haptic surface rendering directly from NURBS descriptions of surfaces. (3) deriving control laws for dynamic behaviors beyond hard surfaces, including compliant and viscous effects, inertia masking, and artificial potentials (4) finding algorithms for solid-model collision detection which allow the collision to be predicted and rendered by the cobot without exceeding its dynamic limits.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">9.98844e+06</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98844e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n308" target="n311">
      <data key="e_effectiveDate">2000-06-15</data>
      <data key="e_title">GOALI: Haptic Cobots</data>
      <data key="e_abstract">We propose research underlying the use of cobots for haptic display of solid models. The project brings together CAD graphics/haptics researchers at Ford Motor Company with haptics/cobot researchers at Northwestern University. A recently completed 3R haptic cobot will be the experimental testbed for the project.&lt;br/&gt;&lt;br/&gt;Large-scale haptic display opens up new opportunities, because people interact with large objects (arm size or larger) in a very different way from that in which they interact with small objects (hand size or smaller). An example is the current use of full-size &quot;clay bucks&quot; in automobile design. A sense of the feel and sweep of an automobile body panels cannot be obtained by touching a scale model of it with a finger, as current haptic displays permit. However a full sized virtual model experienced through the proprioception of whole arm motion, in conjunction with the excellent CAD graphics now available, could bring virtual prototyping and surface editing to a new level of utility.&lt;br/&gt;&lt;br/&gt;Cobot control for haptics differs markedly from robot control for haptics, because cobots use servo-steered rolling mechanisms, rather than servomotor actuators, to create virtual surfaces. The project addresses (1) development of a control methodology for the new power-injection architecture of the 3R cobot. (2) finding cobot-appropriate algorithms for haptic surface rendering directly from NURBS descriptions of surfaces. (3) deriving control laws for dynamic behaviors beyond hard surfaces, including compliant and viscous effects, inertia masking, and artificial potentials (4) finding algorithms for solid-model collision detection which allow the collision to be predicted and rendered by the cobot without exceeding its dynamic limits.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">9.98844e+06</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98844e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n309" target="n310">
      <data key="e_effectiveDate">2000-06-15</data>
      <data key="e_title">GOALI: Haptic Cobots</data>
      <data key="e_abstract">We propose research underlying the use of cobots for haptic display of solid models. The project brings together CAD graphics/haptics researchers at Ford Motor Company with haptics/cobot researchers at Northwestern University. A recently completed 3R haptic cobot will be the experimental testbed for the project.&lt;br/&gt;&lt;br/&gt;Large-scale haptic display opens up new opportunities, because people interact with large objects (arm size or larger) in a very different way from that in which they interact with small objects (hand size or smaller). An example is the current use of full-size &quot;clay bucks&quot; in automobile design. A sense of the feel and sweep of an automobile body panels cannot be obtained by touching a scale model of it with a finger, as current haptic displays permit. However a full sized virtual model experienced through the proprioception of whole arm motion, in conjunction with the excellent CAD graphics now available, could bring virtual prototyping and surface editing to a new level of utility.&lt;br/&gt;&lt;br/&gt;Cobot control for haptics differs markedly from robot control for haptics, because cobots use servo-steered rolling mechanisms, rather than servomotor actuators, to create virtual surfaces. The project addresses (1) development of a control methodology for the new power-injection architecture of the 3R cobot. (2) finding cobot-appropriate algorithms for haptic surface rendering directly from NURBS descriptions of surfaces. (3) deriving control laws for dynamic behaviors beyond hard surfaces, including compliant and viscous effects, inertia masking, and artificial potentials (4) finding algorithms for solid-model collision detection which allow the collision to be predicted and rendered by the cobot without exceeding its dynamic limits.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">9.98844e+06</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98844e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n309" target="n311">
      <data key="e_effectiveDate">2000-06-15</data>
      <data key="e_title">GOALI: Haptic Cobots</data>
      <data key="e_abstract">We propose research underlying the use of cobots for haptic display of solid models. The project brings together CAD graphics/haptics researchers at Ford Motor Company with haptics/cobot researchers at Northwestern University. A recently completed 3R haptic cobot will be the experimental testbed for the project.&lt;br/&gt;&lt;br/&gt;Large-scale haptic display opens up new opportunities, because people interact with large objects (arm size or larger) in a very different way from that in which they interact with small objects (hand size or smaller). An example is the current use of full-size &quot;clay bucks&quot; in automobile design. A sense of the feel and sweep of an automobile body panels cannot be obtained by touching a scale model of it with a finger, as current haptic displays permit. However a full sized virtual model experienced through the proprioception of whole arm motion, in conjunction with the excellent CAD graphics now available, could bring virtual prototyping and surface editing to a new level of utility.&lt;br/&gt;&lt;br/&gt;Cobot control for haptics differs markedly from robot control for haptics, because cobots use servo-steered rolling mechanisms, rather than servomotor actuators, to create virtual surfaces. The project addresses (1) development of a control methodology for the new power-injection architecture of the 3R cobot. (2) finding cobot-appropriate algorithms for haptic surface rendering directly from NURBS descriptions of surfaces. (3) deriving control laws for dynamic behaviors beyond hard surfaces, including compliant and viscous effects, inertia masking, and artificial potentials (4) finding algorithms for solid-model collision detection which allow the collision to be predicted and rendered by the cobot without exceeding its dynamic limits.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">9.98844e+06</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98844e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n310" target="n311">
      <data key="e_effectiveDate">2000-06-15</data>
      <data key="e_title">GOALI: Haptic Cobots</data>
      <data key="e_abstract">We propose research underlying the use of cobots for haptic display of solid models. The project brings together CAD graphics/haptics researchers at Ford Motor Company with haptics/cobot researchers at Northwestern University. A recently completed 3R haptic cobot will be the experimental testbed for the project.&lt;br/&gt;&lt;br/&gt;Large-scale haptic display opens up new opportunities, because people interact with large objects (arm size or larger) in a very different way from that in which they interact with small objects (hand size or smaller). An example is the current use of full-size &quot;clay bucks&quot; in automobile design. A sense of the feel and sweep of an automobile body panels cannot be obtained by touching a scale model of it with a finger, as current haptic displays permit. However a full sized virtual model experienced through the proprioception of whole arm motion, in conjunction with the excellent CAD graphics now available, could bring virtual prototyping and surface editing to a new level of utility.&lt;br/&gt;&lt;br/&gt;Cobot control for haptics differs markedly from robot control for haptics, because cobots use servo-steered rolling mechanisms, rather than servomotor actuators, to create virtual surfaces. The project addresses (1) development of a control methodology for the new power-injection architecture of the 3R cobot. (2) finding cobot-appropriate algorithms for haptic surface rendering directly from NURBS descriptions of surfaces. (3) deriving control laws for dynamic behaviors beyond hard surfaces, including compliant and viscous effects, inertia masking, and artificial potentials (4) finding algorithms for solid-model collision detection which allow the collision to be predicted and rendered by the cobot without exceeding its dynamic limits.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">9.98844e+06</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98844e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n312" target="n313">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">Modern Statistical Techniques For Computer Vision</data>
      <data key="e_abstract">The power of modern statistical techniques is exploited to develop novel approaches to fundamental problems in image understanding. The common statistical foundation for a broad class of vision tasks is identified first, and then the underlying key problems are solved in a rigorous framework. For example, since heteroscedasticity inherently appears in most 3D vision tasks the development of a complete estimation procedure (which includes imposing further geometric constraints on the parameter estimates) is of great importance for computer vision. Such a procedure can provide a faster and more reliable alternative to the widely used (and too general) nonlinear Levenberg-Marquardt method. The task of deriving the 3D description of a scene (static or dynamic) from an image sequence captured with an uncalibrated camera was chosen as a testbed. Estimation problems related to self-calibration, object recognition supported by uncertainty information, will not only allow us to enhance our existing toolbox but also to gain expertise in integrating these tools in a closed-loop autonomous vision system.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">9.9877e+06</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.9877e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n318" target="n319">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">KDI: Segmental and Prosodic Optical Phonetics for Human and Machine Speech Processing</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.99609e+06</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.99609e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n318" target="n320">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">KDI: Segmental and Prosodic Optical Phonetics for Human and Machine Speech Processing</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.99609e+06</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.99609e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n318" target="n321">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">KDI: Segmental and Prosodic Optical Phonetics for Human and Machine Speech Processing</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.99609e+06</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.99609e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n318" target="n322">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">KDI: Segmental and Prosodic Optical Phonetics for Human and Machine Speech Processing</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.99609e+06</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.99609e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n319" target="n320">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">KDI: Segmental and Prosodic Optical Phonetics for Human and Machine Speech Processing</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.99609e+06</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.99609e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n319" target="n321">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">KDI: Segmental and Prosodic Optical Phonetics for Human and Machine Speech Processing</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.99609e+06</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.99609e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n319" target="n322">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">KDI: Segmental and Prosodic Optical Phonetics for Human and Machine Speech Processing</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.99609e+06</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.99609e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n320" target="n321">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">KDI: Segmental and Prosodic Optical Phonetics for Human and Machine Speech Processing</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.99609e+06</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.99609e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n320" target="n322">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">KDI: Segmental and Prosodic Optical Phonetics for Human and Machine Speech Processing</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.99609e+06</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.99609e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n321" target="n322">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">KDI: Segmental and Prosodic Optical Phonetics for Human and Machine Speech Processing</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.99609e+06</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.99609e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n295" target="n337">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">U.S.-France Cooperative Research: Randomness, Approximation and New Models of Computation</data>
      <data key="e_abstract">9981755&lt;br/&gt;Randall&lt;br/&gt;&lt;br/&gt;This three-year award for U.S.-France cooperative research in computation and computer research involves Dana Randall, Vijay Vazirani, Leonard Schulman and other researchers from the College of Computing at Georgia Institute of Technology, and Claire Kenyon and the research group at LRI (Laboratoire de Recherche en Informatique), University of Paris, Sud. The project is supported under a joint program of the National Science Foundation and the French National Center for Scientific Research. The project focuses on four areas of research in theoretical computer science, emphasizing applications for approximation algorithms and new models of computation. The areas are: (1) Markov chain Monte Carlo methods for analysis of physical systems; (2) approximation algorithms; (3) codes and loss-resilience in the context of constructing codes for loss information retrieval; and (3) quantum computation, in particular, quantum cryptography.&lt;br/&gt;&lt;br/&gt;This award will support the incremental costs of the international collaboration; that is, the travel to Paris, France and subsistence expenses of the U.S. investigators. The project takes advantage of complementary expertise of the U.S. and French groups and their work on similar problems. The project will advance fundamental understanding of randomness and approximation problems in computing that are connected to other fields of science, namely, statistical mechanics, data mining, coding theory, and quantum mechanics.</data>
      <data key="e_pgm">5980</data>
      <data key="e_label">9.98176e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0109</data>
      <data key="e_awardID">9.98176e+06</data>
      <data key="e_dir">01</data>
    </edge>
    <edge source="n337" target="n339">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">U.S.-France Cooperative Research: Randomness, Approximation and New Models of Computation</data>
      <data key="e_abstract">9981755&lt;br/&gt;Randall&lt;br/&gt;&lt;br/&gt;This three-year award for U.S.-France cooperative research in computation and computer research involves Dana Randall, Vijay Vazirani, Leonard Schulman and other researchers from the College of Computing at Georgia Institute of Technology, and Claire Kenyon and the research group at LRI (Laboratoire de Recherche en Informatique), University of Paris, Sud. The project is supported under a joint program of the National Science Foundation and the French National Center for Scientific Research. The project focuses on four areas of research in theoretical computer science, emphasizing applications for approximation algorithms and new models of computation. The areas are: (1) Markov chain Monte Carlo methods for analysis of physical systems; (2) approximation algorithms; (3) codes and loss-resilience in the context of constructing codes for loss information retrieval; and (3) quantum computation, in particular, quantum cryptography.&lt;br/&gt;&lt;br/&gt;This award will support the incremental costs of the international collaboration; that is, the travel to Paris, France and subsistence expenses of the U.S. investigators. The project takes advantage of complementary expertise of the U.S. and French groups and their work on similar problems. The project will advance fundamental understanding of randomness and approximation problems in computing that are connected to other fields of science, namely, statistical mechanics, data mining, coding theory, and quantum mechanics.</data>
      <data key="e_pgm">5980</data>
      <data key="e_label">9.98176e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0109</data>
      <data key="e_awardID">9.98176e+06</data>
      <data key="e_dir">01</data>
    </edge>
    <edge source="n295" target="n339">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">U.S.-France Cooperative Research: Randomness, Approximation and New Models of Computation</data>
      <data key="e_abstract">9981755&lt;br/&gt;Randall&lt;br/&gt;&lt;br/&gt;This three-year award for U.S.-France cooperative research in computation and computer research involves Dana Randall, Vijay Vazirani, Leonard Schulman and other researchers from the College of Computing at Georgia Institute of Technology, and Claire Kenyon and the research group at LRI (Laboratoire de Recherche en Informatique), University of Paris, Sud. The project is supported under a joint program of the National Science Foundation and the French National Center for Scientific Research. The project focuses on four areas of research in theoretical computer science, emphasizing applications for approximation algorithms and new models of computation. The areas are: (1) Markov chain Monte Carlo methods for analysis of physical systems; (2) approximation algorithms; (3) codes and loss-resilience in the context of constructing codes for loss information retrieval; and (3) quantum computation, in particular, quantum cryptography.&lt;br/&gt;&lt;br/&gt;This award will support the incremental costs of the international collaboration; that is, the travel to Paris, France and subsistence expenses of the U.S. investigators. The project takes advantage of complementary expertise of the U.S. and French groups and their work on similar problems. The project will advance fundamental understanding of randomness and approximation problems in computing that are connected to other fields of science, namely, statistical mechanics, data mining, coding theory, and quantum mechanics.</data>
      <data key="e_pgm">5980</data>
      <data key="e_label">9.98176e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0109</data>
      <data key="e_awardID">9.98176e+06</data>
      <data key="e_dir">01</data>
    </edge>
    <edge source="n341" target="n342">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">Multiresolution- and Topology-Based Visualization of Large Scientific Data Sets in Parallel and Distributed Computing Environments</data>
      <data key="e_abstract">Our ability to generate scientific data is growing much faster than our ability to understand it. The rapid advances in computing and imaging technology allow scientists to generate very large data sets for which appropriate means for in-depth analysis and understanding do not yet exist. Computational techniques allow the simulation of extremely complicated physical phenomena at ever increasing spatial and temporal resolutions, but data analysis technology for this type of data is still in its infancy. Today, one must choose one of these alternatives: One can either ignore a substantial fraction of a massive scientific data set and only analyze portions of it, or one can invest a significant amount of person-time to analyze and visualize a massive data set in great detail. Neither alternative is desirable. This project will develop the technology needed to address this issue, and will test the new techniques on data from Lawrence Livermore National Laboratory and NASA Ames Research Center.&lt;br/&gt;&lt;br/&gt;Technically, the project will take a 5-prong approach to the large data visualization problem. First, it will extend existing hierarchical schemes - i.e., schemes approximating a data set at multiple resolution levels - to time-varying multi-valued/multi-dimensional data. Second, it will improve topology-based approaches - i.e., approaches that extract qualitatively interesting characteristics (such as zeros, extreme, and discontinuities in scalar and vector fields) from large data sets. Third, it will develop parallel and distributed algorithms for efficient computation of hierarchical data representations, fast extraction of topology, and optimizing compute-intensive visualization processes. Fourth, it will devise interactive visualization techniques for (immersive) visualization environments that support view-dependent and user-specified, adaptive level-of-detail rendering. Fifth, it will create a simple metadata database system allow sharing of a user&apos;s experience, i.e., previously chosen rendering parameters leading to &quot;good imagery&quot; or entire rendering processes.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">9.98225e+06</data>
      <data key="e_expirationDate">2005-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98225e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n341" target="n343">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">Multiresolution- and Topology-Based Visualization of Large Scientific Data Sets in Parallel and Distributed Computing Environments</data>
      <data key="e_abstract">Our ability to generate scientific data is growing much faster than our ability to understand it. The rapid advances in computing and imaging technology allow scientists to generate very large data sets for which appropriate means for in-depth analysis and understanding do not yet exist. Computational techniques allow the simulation of extremely complicated physical phenomena at ever increasing spatial and temporal resolutions, but data analysis technology for this type of data is still in its infancy. Today, one must choose one of these alternatives: One can either ignore a substantial fraction of a massive scientific data set and only analyze portions of it, or one can invest a significant amount of person-time to analyze and visualize a massive data set in great detail. Neither alternative is desirable. This project will develop the technology needed to address this issue, and will test the new techniques on data from Lawrence Livermore National Laboratory and NASA Ames Research Center.&lt;br/&gt;&lt;br/&gt;Technically, the project will take a 5-prong approach to the large data visualization problem. First, it will extend existing hierarchical schemes - i.e., schemes approximating a data set at multiple resolution levels - to time-varying multi-valued/multi-dimensional data. Second, it will improve topology-based approaches - i.e., approaches that extract qualitatively interesting characteristics (such as zeros, extreme, and discontinuities in scalar and vector fields) from large data sets. Third, it will develop parallel and distributed algorithms for efficient computation of hierarchical data representations, fast extraction of topology, and optimizing compute-intensive visualization processes. Fourth, it will devise interactive visualization techniques for (immersive) visualization environments that support view-dependent and user-specified, adaptive level-of-detail rendering. Fifth, it will create a simple metadata database system allow sharing of a user&apos;s experience, i.e., previously chosen rendering parameters leading to &quot;good imagery&quot; or entire rendering processes.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">9.98225e+06</data>
      <data key="e_expirationDate">2005-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98225e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n341" target="n344">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">Multiresolution- and Topology-Based Visualization of Large Scientific Data Sets in Parallel and Distributed Computing Environments</data>
      <data key="e_abstract">Our ability to generate scientific data is growing much faster than our ability to understand it. The rapid advances in computing and imaging technology allow scientists to generate very large data sets for which appropriate means for in-depth analysis and understanding do not yet exist. Computational techniques allow the simulation of extremely complicated physical phenomena at ever increasing spatial and temporal resolutions, but data analysis technology for this type of data is still in its infancy. Today, one must choose one of these alternatives: One can either ignore a substantial fraction of a massive scientific data set and only analyze portions of it, or one can invest a significant amount of person-time to analyze and visualize a massive data set in great detail. Neither alternative is desirable. This project will develop the technology needed to address this issue, and will test the new techniques on data from Lawrence Livermore National Laboratory and NASA Ames Research Center.&lt;br/&gt;&lt;br/&gt;Technically, the project will take a 5-prong approach to the large data visualization problem. First, it will extend existing hierarchical schemes - i.e., schemes approximating a data set at multiple resolution levels - to time-varying multi-valued/multi-dimensional data. Second, it will improve topology-based approaches - i.e., approaches that extract qualitatively interesting characteristics (such as zeros, extreme, and discontinuities in scalar and vector fields) from large data sets. Third, it will develop parallel and distributed algorithms for efficient computation of hierarchical data representations, fast extraction of topology, and optimizing compute-intensive visualization processes. Fourth, it will devise interactive visualization techniques for (immersive) visualization environments that support view-dependent and user-specified, adaptive level-of-detail rendering. Fifth, it will create a simple metadata database system allow sharing of a user&apos;s experience, i.e., previously chosen rendering parameters leading to &quot;good imagery&quot; or entire rendering processes.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">9.98225e+06</data>
      <data key="e_expirationDate">2005-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98225e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n66" target="n341">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">Multiresolution- and Topology-Based Visualization of Large Scientific Data Sets in Parallel and Distributed Computing Environments</data>
      <data key="e_abstract">Our ability to generate scientific data is growing much faster than our ability to understand it. The rapid advances in computing and imaging technology allow scientists to generate very large data sets for which appropriate means for in-depth analysis and understanding do not yet exist. Computational techniques allow the simulation of extremely complicated physical phenomena at ever increasing spatial and temporal resolutions, but data analysis technology for this type of data is still in its infancy. Today, one must choose one of these alternatives: One can either ignore a substantial fraction of a massive scientific data set and only analyze portions of it, or one can invest a significant amount of person-time to analyze and visualize a massive data set in great detail. Neither alternative is desirable. This project will develop the technology needed to address this issue, and will test the new techniques on data from Lawrence Livermore National Laboratory and NASA Ames Research Center.&lt;br/&gt;&lt;br/&gt;Technically, the project will take a 5-prong approach to the large data visualization problem. First, it will extend existing hierarchical schemes - i.e., schemes approximating a data set at multiple resolution levels - to time-varying multi-valued/multi-dimensional data. Second, it will improve topology-based approaches - i.e., approaches that extract qualitatively interesting characteristics (such as zeros, extreme, and discontinuities in scalar and vector fields) from large data sets. Third, it will develop parallel and distributed algorithms for efficient computation of hierarchical data representations, fast extraction of topology, and optimizing compute-intensive visualization processes. Fourth, it will devise interactive visualization techniques for (immersive) visualization environments that support view-dependent and user-specified, adaptive level-of-detail rendering. Fifth, it will create a simple metadata database system allow sharing of a user&apos;s experience, i.e., previously chosen rendering parameters leading to &quot;good imagery&quot; or entire rendering processes.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">9.98225e+06</data>
      <data key="e_expirationDate">2005-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98225e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n342" target="n343">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">Multiresolution- and Topology-Based Visualization of Large Scientific Data Sets in Parallel and Distributed Computing Environments</data>
      <data key="e_abstract">Our ability to generate scientific data is growing much faster than our ability to understand it. The rapid advances in computing and imaging technology allow scientists to generate very large data sets for which appropriate means for in-depth analysis and understanding do not yet exist. Computational techniques allow the simulation of extremely complicated physical phenomena at ever increasing spatial and temporal resolutions, but data analysis technology for this type of data is still in its infancy. Today, one must choose one of these alternatives: One can either ignore a substantial fraction of a massive scientific data set and only analyze portions of it, or one can invest a significant amount of person-time to analyze and visualize a massive data set in great detail. Neither alternative is desirable. This project will develop the technology needed to address this issue, and will test the new techniques on data from Lawrence Livermore National Laboratory and NASA Ames Research Center.&lt;br/&gt;&lt;br/&gt;Technically, the project will take a 5-prong approach to the large data visualization problem. First, it will extend existing hierarchical schemes - i.e., schemes approximating a data set at multiple resolution levels - to time-varying multi-valued/multi-dimensional data. Second, it will improve topology-based approaches - i.e., approaches that extract qualitatively interesting characteristics (such as zeros, extreme, and discontinuities in scalar and vector fields) from large data sets. Third, it will develop parallel and distributed algorithms for efficient computation of hierarchical data representations, fast extraction of topology, and optimizing compute-intensive visualization processes. Fourth, it will devise interactive visualization techniques for (immersive) visualization environments that support view-dependent and user-specified, adaptive level-of-detail rendering. Fifth, it will create a simple metadata database system allow sharing of a user&apos;s experience, i.e., previously chosen rendering parameters leading to &quot;good imagery&quot; or entire rendering processes.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">9.98225e+06</data>
      <data key="e_expirationDate">2005-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98225e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n342" target="n344">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">Multiresolution- and Topology-Based Visualization of Large Scientific Data Sets in Parallel and Distributed Computing Environments</data>
      <data key="e_abstract">Our ability to generate scientific data is growing much faster than our ability to understand it. The rapid advances in computing and imaging technology allow scientists to generate very large data sets for which appropriate means for in-depth analysis and understanding do not yet exist. Computational techniques allow the simulation of extremely complicated physical phenomena at ever increasing spatial and temporal resolutions, but data analysis technology for this type of data is still in its infancy. Today, one must choose one of these alternatives: One can either ignore a substantial fraction of a massive scientific data set and only analyze portions of it, or one can invest a significant amount of person-time to analyze and visualize a massive data set in great detail. Neither alternative is desirable. This project will develop the technology needed to address this issue, and will test the new techniques on data from Lawrence Livermore National Laboratory and NASA Ames Research Center.&lt;br/&gt;&lt;br/&gt;Technically, the project will take a 5-prong approach to the large data visualization problem. First, it will extend existing hierarchical schemes - i.e., schemes approximating a data set at multiple resolution levels - to time-varying multi-valued/multi-dimensional data. Second, it will improve topology-based approaches - i.e., approaches that extract qualitatively interesting characteristics (such as zeros, extreme, and discontinuities in scalar and vector fields) from large data sets. Third, it will develop parallel and distributed algorithms for efficient computation of hierarchical data representations, fast extraction of topology, and optimizing compute-intensive visualization processes. Fourth, it will devise interactive visualization techniques for (immersive) visualization environments that support view-dependent and user-specified, adaptive level-of-detail rendering. Fifth, it will create a simple metadata database system allow sharing of a user&apos;s experience, i.e., previously chosen rendering parameters leading to &quot;good imagery&quot; or entire rendering processes.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">9.98225e+06</data>
      <data key="e_expirationDate">2005-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98225e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n66" target="n342">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">Multiresolution- and Topology-Based Visualization of Large Scientific Data Sets in Parallel and Distributed Computing Environments</data>
      <data key="e_abstract">Our ability to generate scientific data is growing much faster than our ability to understand it. The rapid advances in computing and imaging technology allow scientists to generate very large data sets for which appropriate means for in-depth analysis and understanding do not yet exist. Computational techniques allow the simulation of extremely complicated physical phenomena at ever increasing spatial and temporal resolutions, but data analysis technology for this type of data is still in its infancy. Today, one must choose one of these alternatives: One can either ignore a substantial fraction of a massive scientific data set and only analyze portions of it, or one can invest a significant amount of person-time to analyze and visualize a massive data set in great detail. Neither alternative is desirable. This project will develop the technology needed to address this issue, and will test the new techniques on data from Lawrence Livermore National Laboratory and NASA Ames Research Center.&lt;br/&gt;&lt;br/&gt;Technically, the project will take a 5-prong approach to the large data visualization problem. First, it will extend existing hierarchical schemes - i.e., schemes approximating a data set at multiple resolution levels - to time-varying multi-valued/multi-dimensional data. Second, it will improve topology-based approaches - i.e., approaches that extract qualitatively interesting characteristics (such as zeros, extreme, and discontinuities in scalar and vector fields) from large data sets. Third, it will develop parallel and distributed algorithms for efficient computation of hierarchical data representations, fast extraction of topology, and optimizing compute-intensive visualization processes. Fourth, it will devise interactive visualization techniques for (immersive) visualization environments that support view-dependent and user-specified, adaptive level-of-detail rendering. Fifth, it will create a simple metadata database system allow sharing of a user&apos;s experience, i.e., previously chosen rendering parameters leading to &quot;good imagery&quot; or entire rendering processes.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">9.98225e+06</data>
      <data key="e_expirationDate">2005-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98225e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n343" target="n344">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">Multiresolution- and Topology-Based Visualization of Large Scientific Data Sets in Parallel and Distributed Computing Environments</data>
      <data key="e_abstract">Our ability to generate scientific data is growing much faster than our ability to understand it. The rapid advances in computing and imaging technology allow scientists to generate very large data sets for which appropriate means for in-depth analysis and understanding do not yet exist. Computational techniques allow the simulation of extremely complicated physical phenomena at ever increasing spatial and temporal resolutions, but data analysis technology for this type of data is still in its infancy. Today, one must choose one of these alternatives: One can either ignore a substantial fraction of a massive scientific data set and only analyze portions of it, or one can invest a significant amount of person-time to analyze and visualize a massive data set in great detail. Neither alternative is desirable. This project will develop the technology needed to address this issue, and will test the new techniques on data from Lawrence Livermore National Laboratory and NASA Ames Research Center.&lt;br/&gt;&lt;br/&gt;Technically, the project will take a 5-prong approach to the large data visualization problem. First, it will extend existing hierarchical schemes - i.e., schemes approximating a data set at multiple resolution levels - to time-varying multi-valued/multi-dimensional data. Second, it will improve topology-based approaches - i.e., approaches that extract qualitatively interesting characteristics (such as zeros, extreme, and discontinuities in scalar and vector fields) from large data sets. Third, it will develop parallel and distributed algorithms for efficient computation of hierarchical data representations, fast extraction of topology, and optimizing compute-intensive visualization processes. Fourth, it will devise interactive visualization techniques for (immersive) visualization environments that support view-dependent and user-specified, adaptive level-of-detail rendering. Fifth, it will create a simple metadata database system allow sharing of a user&apos;s experience, i.e., previously chosen rendering parameters leading to &quot;good imagery&quot; or entire rendering processes.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">9.98225e+06</data>
      <data key="e_expirationDate">2005-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98225e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n66" target="n343">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">Multiresolution- and Topology-Based Visualization of Large Scientific Data Sets in Parallel and Distributed Computing Environments</data>
      <data key="e_abstract">Our ability to generate scientific data is growing much faster than our ability to understand it. The rapid advances in computing and imaging technology allow scientists to generate very large data sets for which appropriate means for in-depth analysis and understanding do not yet exist. Computational techniques allow the simulation of extremely complicated physical phenomena at ever increasing spatial and temporal resolutions, but data analysis technology for this type of data is still in its infancy. Today, one must choose one of these alternatives: One can either ignore a substantial fraction of a massive scientific data set and only analyze portions of it, or one can invest a significant amount of person-time to analyze and visualize a massive data set in great detail. Neither alternative is desirable. This project will develop the technology needed to address this issue, and will test the new techniques on data from Lawrence Livermore National Laboratory and NASA Ames Research Center.&lt;br/&gt;&lt;br/&gt;Technically, the project will take a 5-prong approach to the large data visualization problem. First, it will extend existing hierarchical schemes - i.e., schemes approximating a data set at multiple resolution levels - to time-varying multi-valued/multi-dimensional data. Second, it will improve topology-based approaches - i.e., approaches that extract qualitatively interesting characteristics (such as zeros, extreme, and discontinuities in scalar and vector fields) from large data sets. Third, it will develop parallel and distributed algorithms for efficient computation of hierarchical data representations, fast extraction of topology, and optimizing compute-intensive visualization processes. Fourth, it will devise interactive visualization techniques for (immersive) visualization environments that support view-dependent and user-specified, adaptive level-of-detail rendering. Fifth, it will create a simple metadata database system allow sharing of a user&apos;s experience, i.e., previously chosen rendering parameters leading to &quot;good imagery&quot; or entire rendering processes.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">9.98225e+06</data>
      <data key="e_expirationDate">2005-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98225e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n66" target="n344">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">Multiresolution- and Topology-Based Visualization of Large Scientific Data Sets in Parallel and Distributed Computing Environments</data>
      <data key="e_abstract">Our ability to generate scientific data is growing much faster than our ability to understand it. The rapid advances in computing and imaging technology allow scientists to generate very large data sets for which appropriate means for in-depth analysis and understanding do not yet exist. Computational techniques allow the simulation of extremely complicated physical phenomena at ever increasing spatial and temporal resolutions, but data analysis technology for this type of data is still in its infancy. Today, one must choose one of these alternatives: One can either ignore a substantial fraction of a massive scientific data set and only analyze portions of it, or one can invest a significant amount of person-time to analyze and visualize a massive data set in great detail. Neither alternative is desirable. This project will develop the technology needed to address this issue, and will test the new techniques on data from Lawrence Livermore National Laboratory and NASA Ames Research Center.&lt;br/&gt;&lt;br/&gt;Technically, the project will take a 5-prong approach to the large data visualization problem. First, it will extend existing hierarchical schemes - i.e., schemes approximating a data set at multiple resolution levels - to time-varying multi-valued/multi-dimensional data. Second, it will improve topology-based approaches - i.e., approaches that extract qualitatively interesting characteristics (such as zeros, extreme, and discontinuities in scalar and vector fields) from large data sets. Third, it will develop parallel and distributed algorithms for efficient computation of hierarchical data representations, fast extraction of topology, and optimizing compute-intensive visualization processes. Fourth, it will devise interactive visualization techniques for (immersive) visualization environments that support view-dependent and user-specified, adaptive level-of-detail rendering. Fifth, it will create a simple metadata database system allow sharing of a user&apos;s experience, i.e., previously chosen rendering parameters leading to &quot;good imagery&quot; or entire rendering processes.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">9.98225e+06</data>
      <data key="e_expirationDate">2005-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98225e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n346" target="n347">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">Digital Government: Database Middleware for Distributed Ontologies in State and Federal Family &amp; Social Services</data>
      <data key="e_abstract">EIA-9983249&lt;br/&gt;Elamagarmid, Ahmed K &lt;br/&gt;Purdue University&lt;br/&gt;&lt;br/&gt;Digital Government: Database Middleware for Distributed Ontologies in State and Federal Family and Social Services&lt;br/&gt;&lt;br/&gt;One of the key problems in government at all levels is data interoperability. The problem arises from the distributed nature of government agencies, the lack of top down leadership, and the remnants of &quot;silo&quot; architectures seen in legacy mainframe agency information systems. The explosive growth of the web has exposed this inability to integrate data and the consequent inability to ask sensible questions which cross agency lines; e.g., &quot;what benefits can I expect from various government agencies when I retire.&quot; Moving this distributed data to one place is not an option. Rather, technology must be developed to access the data where it is held, using common data definitions, or ontologies and an interface.&lt;br/&gt;&lt;br/&gt;This grant will support work on this problem in the areas of query infrastructure, middleware for the web, and dynamic inter-ontology support. The government collaborators are the Indiana Family and Social Services Administration, the Indiana Dept. of Workforce Development, and the US Dept. of Health and Human Services.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98325e+06</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98325e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n352" target="n353">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">Special Project: Workshop on Undergraduate Education &amp; Computer Vision</data>
      <data key="e_abstract">9988400&lt;br/&gt;Stark, Louise&lt;br/&gt;University of the Pacific&lt;br/&gt;&lt;br/&gt;Special Project: Workshop on Undergraduate Education &amp; Computer Vision&lt;br/&gt;&lt;br/&gt;This project provides support for up to 25 faculty from primarily undergraduate teaching institutions, particularly those that serve traditionally under-represented groups, to attend the IEEE sponsored Workshop on Undergraduate Education and Computer Vision, held in conjunction with the Computer Vision and Pattern Recognition (CVPR) 2000 Conference. Growth of the field of image-related computation, including especially computer vision and pattern recognition, has created a significant need for additional educational opportunities in these areas and this project provides a highly leveraged approach toward meeting a part of that need. The workshop provides well-documented examples of courses, course modules, case studies, laboratory resources and other teaching materials to help faculty design high-quality image-related undergraduate courses. This project is a follow-on to the successful similar workshop held in 1997.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.9884e+06</data>
      <data key="e_expirationDate">2002-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.9884e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n358" target="n359">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">Special Project: U.S. Attendance at the 16th World Computer Congress (IFIP Congress 2000)</data>
      <data key="e_abstract">0001995&lt;br/&gt;Aiken, Robert M&lt;br/&gt;Shriver, Bruce D&lt;br/&gt;Association for Computing Machinery&lt;br/&gt;&lt;br/&gt;Special Project: U.S. Attendance at the 16th IFIP World Computer Congress (IFIP Congress 2000)&lt;br/&gt;&lt;br/&gt;This award provides partial support for 20 representatives from the U.S. to attend the 16th IFIP World Computer Congress, in Beijing, China, August 21 - 25, 2000. The International Federation for Information Processing (IFIP) is a multinational federation of technical organizations concerned with information processing. The ACM and the IEEE Computer Society are full members of IFIP and the ACM is administering this project. The participants in the project are selected on a competitive basis by a committee of reviewers, with emphasis given to young researchers doing outstanding work and to applicants who have been accepted to present papers or are otherwise significantly involved in the conference.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">1995</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">1995</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n362" target="n363">
      <data key="e_effectiveDate">2000-06-01</data>
      <data key="e_title">Centroidal Voronoi Tessellations: Algorithms, Applications, and Theory</data>
      <data key="e_abstract">A centroidal Voronoi tessellation (CVT) is a Voronoi tessellation of a&lt;br/&gt;given set such that the associated generating points are centers of &lt;br/&gt;mass of the corresponding Voronoi regions. Applications of CVT&apos;s &lt;br/&gt;range from problems in image compression, vector quantization, &lt;br/&gt;quadrature rules, grid generation and optimization, finite difference &lt;br/&gt;schemes, distribution of resources, cellular biology, cluster analysis, &lt;br/&gt;and the territorial behavior of animals. The goals are to develop highly&lt;br/&gt;efficient parallel algorithms for the computation of CVT&apos;s, to gain &lt;br/&gt;further understanding of interesting features related to the these &lt;br/&gt;tessellations, to implement and test these algorithms, and to produce &lt;br/&gt;a useful software suite that can serve as a design tool for many&lt;br/&gt;problems in applications.&lt;br/&gt;&lt;br/&gt;Among the theoretical questions to be studied are the complexity of &lt;br/&gt;algorithms for the construction of CVT&apos;s, the effects of nonuniform &lt;br/&gt;densities, CVT&apos;s in general metrics, constrained CVT&apos;s, and generalized&lt;br/&gt;CVT&apos;s based on lines, curves and surfaces.&lt;br/&gt;&lt;br/&gt;Parallel deterministic and probabilistic algorithms will be developed &lt;br/&gt;and tested. In the former case, different averaging and communication&lt;br/&gt;strategies will be examined; for the latter,domain decomposition &lt;br/&gt;ideas will be exploited. This effort can lead to parallel grid generation&lt;br/&gt; algorithms and other software useful in applications such as&lt;br/&gt;clustering analysis.&lt;br/&gt;&lt;br/&gt;Applications of CVT&apos;s will also be studied, with particular emphasis &lt;br/&gt;on grid generation and optimization. The questions addressed in this&lt;br/&gt;connection include the role of the density functions used to generate &lt;br/&gt;the CVT&apos;s on the optimal placement of grid points, the construction of &lt;br/&gt;grids with special properties and the the use of various generalized &lt;br/&gt;CVT&apos;s. Another application that will be examined are the use of CVT&apos;s &lt;br/&gt;in the clustering analysis of large data sets.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">9.9883e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.9883e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n370" target="n371">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">DIMACS Special Year on Computational Molecular Biology</data>
      <data key="e_abstract">DIMACS, the Center for Discrete Mathematics and Theoretical Computer Science, will run a ``Special Year on Computational Molecular Biology&apos;&apos; to follow up on its highly successful ``Special Year on Mathematical Support for Molecular Biology.&apos;&apos; The special year is motivated by major developments at the interface between biology and information science: The availability of massive amounts of data in novel and diverse forms, requiring the expertise of information scientists; the availability of new technologies, powerful experimental tools that need to be integrated with fundamental algorithmic research; and the development of new mathematical and computational tools that make it possible to understand biological processes at a much more complex level than before. The special year will include workshops, visitors, and the involvement of postdoctoral fellows and graduate students from around the country. Activities will be organized around a series of workshops: Integration of Diverse Biological Data, Sequence Motif Recognition, Whole Genome Comparison, Learning Stochastic Processes, Protein Structure (Recent Computational Advances and Future Directions), System-based Modeling in Bioinformatics, Geometric Searching, Analysis of Gene Expression Data, Computational Issues in Signal Transduction, Protein Structure Comparison and Prediction Assessment, and DNA Topology. This award is co-funded with the Sloan Foundation. NSF funds will specifically be used to support the attendance of young scientists from underrepresented minorities.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">9.98298e+06</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">9.98298e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n370" target="n372">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">DIMACS Special Year on Computational Molecular Biology</data>
      <data key="e_abstract">DIMACS, the Center for Discrete Mathematics and Theoretical Computer Science, will run a ``Special Year on Computational Molecular Biology&apos;&apos; to follow up on its highly successful ``Special Year on Mathematical Support for Molecular Biology.&apos;&apos; The special year is motivated by major developments at the interface between biology and information science: The availability of massive amounts of data in novel and diverse forms, requiring the expertise of information scientists; the availability of new technologies, powerful experimental tools that need to be integrated with fundamental algorithmic research; and the development of new mathematical and computational tools that make it possible to understand biological processes at a much more complex level than before. The special year will include workshops, visitors, and the involvement of postdoctoral fellows and graduate students from around the country. Activities will be organized around a series of workshops: Integration of Diverse Biological Data, Sequence Motif Recognition, Whole Genome Comparison, Learning Stochastic Processes, Protein Structure (Recent Computational Advances and Future Directions), System-based Modeling in Bioinformatics, Geometric Searching, Analysis of Gene Expression Data, Computational Issues in Signal Transduction, Protein Structure Comparison and Prediction Assessment, and DNA Topology. This award is co-funded with the Sloan Foundation. NSF funds will specifically be used to support the attendance of young scientists from underrepresented minorities.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">9.98298e+06</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">9.98298e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n370" target="n373">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">DIMACS Special Year on Computational Molecular Biology</data>
      <data key="e_abstract">DIMACS, the Center for Discrete Mathematics and Theoretical Computer Science, will run a ``Special Year on Computational Molecular Biology&apos;&apos; to follow up on its highly successful ``Special Year on Mathematical Support for Molecular Biology.&apos;&apos; The special year is motivated by major developments at the interface between biology and information science: The availability of massive amounts of data in novel and diverse forms, requiring the expertise of information scientists; the availability of new technologies, powerful experimental tools that need to be integrated with fundamental algorithmic research; and the development of new mathematical and computational tools that make it possible to understand biological processes at a much more complex level than before. The special year will include workshops, visitors, and the involvement of postdoctoral fellows and graduate students from around the country. Activities will be organized around a series of workshops: Integration of Diverse Biological Data, Sequence Motif Recognition, Whole Genome Comparison, Learning Stochastic Processes, Protein Structure (Recent Computational Advances and Future Directions), System-based Modeling in Bioinformatics, Geometric Searching, Analysis of Gene Expression Data, Computational Issues in Signal Transduction, Protein Structure Comparison and Prediction Assessment, and DNA Topology. This award is co-funded with the Sloan Foundation. NSF funds will specifically be used to support the attendance of young scientists from underrepresented minorities.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">9.98298e+06</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">9.98298e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n371" target="n372">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">DIMACS Special Year on Computational Molecular Biology</data>
      <data key="e_abstract">DIMACS, the Center for Discrete Mathematics and Theoretical Computer Science, will run a ``Special Year on Computational Molecular Biology&apos;&apos; to follow up on its highly successful ``Special Year on Mathematical Support for Molecular Biology.&apos;&apos; The special year is motivated by major developments at the interface between biology and information science: The availability of massive amounts of data in novel and diverse forms, requiring the expertise of information scientists; the availability of new technologies, powerful experimental tools that need to be integrated with fundamental algorithmic research; and the development of new mathematical and computational tools that make it possible to understand biological processes at a much more complex level than before. The special year will include workshops, visitors, and the involvement of postdoctoral fellows and graduate students from around the country. Activities will be organized around a series of workshops: Integration of Diverse Biological Data, Sequence Motif Recognition, Whole Genome Comparison, Learning Stochastic Processes, Protein Structure (Recent Computational Advances and Future Directions), System-based Modeling in Bioinformatics, Geometric Searching, Analysis of Gene Expression Data, Computational Issues in Signal Transduction, Protein Structure Comparison and Prediction Assessment, and DNA Topology. This award is co-funded with the Sloan Foundation. NSF funds will specifically be used to support the attendance of young scientists from underrepresented minorities.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">9.98298e+06</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">9.98298e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n371" target="n373">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">DIMACS Special Year on Computational Molecular Biology</data>
      <data key="e_abstract">DIMACS, the Center for Discrete Mathematics and Theoretical Computer Science, will run a ``Special Year on Computational Molecular Biology&apos;&apos; to follow up on its highly successful ``Special Year on Mathematical Support for Molecular Biology.&apos;&apos; The special year is motivated by major developments at the interface between biology and information science: The availability of massive amounts of data in novel and diverse forms, requiring the expertise of information scientists; the availability of new technologies, powerful experimental tools that need to be integrated with fundamental algorithmic research; and the development of new mathematical and computational tools that make it possible to understand biological processes at a much more complex level than before. The special year will include workshops, visitors, and the involvement of postdoctoral fellows and graduate students from around the country. Activities will be organized around a series of workshops: Integration of Diverse Biological Data, Sequence Motif Recognition, Whole Genome Comparison, Learning Stochastic Processes, Protein Structure (Recent Computational Advances and Future Directions), System-based Modeling in Bioinformatics, Geometric Searching, Analysis of Gene Expression Data, Computational Issues in Signal Transduction, Protein Structure Comparison and Prediction Assessment, and DNA Topology. This award is co-funded with the Sloan Foundation. NSF funds will specifically be used to support the attendance of young scientists from underrepresented minorities.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">9.98298e+06</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">9.98298e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n372" target="n373">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">DIMACS Special Year on Computational Molecular Biology</data>
      <data key="e_abstract">DIMACS, the Center for Discrete Mathematics and Theoretical Computer Science, will run a ``Special Year on Computational Molecular Biology&apos;&apos; to follow up on its highly successful ``Special Year on Mathematical Support for Molecular Biology.&apos;&apos; The special year is motivated by major developments at the interface between biology and information science: The availability of massive amounts of data in novel and diverse forms, requiring the expertise of information scientists; the availability of new technologies, powerful experimental tools that need to be integrated with fundamental algorithmic research; and the development of new mathematical and computational tools that make it possible to understand biological processes at a much more complex level than before. The special year will include workshops, visitors, and the involvement of postdoctoral fellows and graduate students from around the country. Activities will be organized around a series of workshops: Integration of Diverse Biological Data, Sequence Motif Recognition, Whole Genome Comparison, Learning Stochastic Processes, Protein Structure (Recent Computational Advances and Future Directions), System-based Modeling in Bioinformatics, Geometric Searching, Analysis of Gene Expression Data, Computational Issues in Signal Transduction, Protein Structure Comparison and Prediction Assessment, and DNA Topology. This award is co-funded with the Sloan Foundation. NSF funds will specifically be used to support the attendance of young scientists from underrepresented minorities.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">9.98298e+06</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">9.98298e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n380" target="n381">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Workshop: 2000 Language Engineering Workshop for Students and Professionals: Integrating Research and Education (WS00)</data>
      <data key="e_abstract">This is a standard award. Automated systems that interact with human users in spoken and written communication will greatly enhance productivity and program usability. These systems will allow friendly access to information services and are essential for people with disabilities, or for accessing databases while performing intricate tasks. Unfortunately, current technology is inadequate for the tasks at hand, and there is a need to make progress. The number of available personnel trained in the field must be vastly increased and solutions to long standing problems must be found. At this time, relatively few universities educate students capable of performing the required tasks. Leading professionals are scattered in various industrial, academic, and governmental institutions, often duplicating each other&apos;s work. This award will allow a summer workshop on language engineering to be conducted where mixed teams of leading professionals and students can cooperate to advance the state of the art. The professionals will normally be university professors and industrial and government researchers working in widely dispersed locations. Graduate and undergraduate students will join these teams. The participation of undergraduate students is intended not only as an educational opportunity, but also to broaden the appeal of the language engineering field amongst students considering graduate studies.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">71215</data>
      <data key="e_expirationDate">2002-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">71215</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n380" target="n382">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Workshop: 2000 Language Engineering Workshop for Students and Professionals: Integrating Research and Education (WS00)</data>
      <data key="e_abstract">This is a standard award. Automated systems that interact with human users in spoken and written communication will greatly enhance productivity and program usability. These systems will allow friendly access to information services and are essential for people with disabilities, or for accessing databases while performing intricate tasks. Unfortunately, current technology is inadequate for the tasks at hand, and there is a need to make progress. The number of available personnel trained in the field must be vastly increased and solutions to long standing problems must be found. At this time, relatively few universities educate students capable of performing the required tasks. Leading professionals are scattered in various industrial, academic, and governmental institutions, often duplicating each other&apos;s work. This award will allow a summer workshop on language engineering to be conducted where mixed teams of leading professionals and students can cooperate to advance the state of the art. The professionals will normally be university professors and industrial and government researchers working in widely dispersed locations. Graduate and undergraduate students will join these teams. The participation of undergraduate students is intended not only as an educational opportunity, but also to broaden the appeal of the language engineering field amongst students considering graduate studies.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">71215</data>
      <data key="e_expirationDate">2002-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">71215</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n381" target="n382">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Workshop: 2000 Language Engineering Workshop for Students and Professionals: Integrating Research and Education (WS00)</data>
      <data key="e_abstract">This is a standard award. Automated systems that interact with human users in spoken and written communication will greatly enhance productivity and program usability. These systems will allow friendly access to information services and are essential for people with disabilities, or for accessing databases while performing intricate tasks. Unfortunately, current technology is inadequate for the tasks at hand, and there is a need to make progress. The number of available personnel trained in the field must be vastly increased and solutions to long standing problems must be found. At this time, relatively few universities educate students capable of performing the required tasks. Leading professionals are scattered in various industrial, academic, and governmental institutions, often duplicating each other&apos;s work. This award will allow a summer workshop on language engineering to be conducted where mixed teams of leading professionals and students can cooperate to advance the state of the art. The professionals will normally be university professors and industrial and government researchers working in widely dispersed locations. Graduate and undergraduate students will join these teams. The participation of undergraduate students is intended not only as an educational opportunity, but also to broaden the appeal of the language engineering field amongst students considering graduate studies.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">71215</data>
      <data key="e_expirationDate">2002-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">71215</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n386" target="n387">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Towards Friction-Free Work: A Multi-Method Study of the Use of Information Technology in the Real Estate Industry</data>
      <data key="e_abstract">Information and communication technologies (ICT) are used pervasively in the real estate industry. This study examines how people and organizations in this industry work and how this work adapts and is affected by the use of ICT. Since real estate agents act as transactional intermediaries, this industry is affected by a shift to electronic transactions and the potential disintermediation this implies. Since many industries are shifting towards more information/knowledge-based structures, understanding how ICT use is changing this industry will provide empirical evidence about potential changes that might be expected in other industries. The study has three objectives: 1. Describe how the use of ICT changes the ways individual knowledge workers conduct their work; 2. Describe organizational and industrial changes related to the use of ICT; 3. Describe how changes in individual work relate to changes in organizational and industrial structures and processes. At the individual level, the researchers focus on changes in work design and social capital. At the organization and industry levels, ideas from transaction cost and coordination theory are applied.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">178</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">178</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n395" target="n396">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Market Dynamics in Cyberspace</data>
      <data key="e_abstract">This research project is undertaken to guide the construction of new markets on the Internet. It will help answer the following questions: Should the order flow be accessible only to specialists or to everyone? Should last-second orders be penalized? Should only one side of the market (say sellers) post price, or both sides? Should automated buyers and sellers be encouraged or discouraged? In general, how effective is a new market in quickly leading intelligent (but self-interested) traders to efficient transactions and encouraging their participation? Three projects will be carried out to answer these questions. The first project examines how automated access strategies affect Internet congestion. The second project compares the effectiveness of alternative market institutions in Internet-like environments. The third project compares alternative pricing schemes for Internet access. Four complementary methodologies will be employed: 1) theoretical models of market equilibrium and transient performance; 2) computer simulations of traders and their market interactions; 3) laboratory markets with human traders interacting with automated traders; and 4) statistical analysis of recent Internet activity.</data>
      <data key="e_pgm">1320</data>
      <data key="e_label">9.98665e+06</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98665e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n406" target="n407">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Applying Universal Bases to Achieving the Full Potential of Sequencing-By-Hybridization (SBH)</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;(revised 5/30/00 -pgilna)&lt;br/&gt;&lt;br/&gt;DBI 9983081 &lt;br/&gt;&lt;br/&gt;Preparata Brown University &lt;br/&gt;&lt;br/&gt;Applying Universal Bases to Achieving Full Potential of SBH&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Summary&lt;br/&gt;&lt;br/&gt;The goal of this interdisciplinary project is to establish the feasability of a more powerful and robust experimental system for the sequencing of DNA by hybridization. A conceptually novel approach is based on a new probe design and a new algorithm for sequence reconstruction which have been demonstrated by computational modeling to provide optimum information extraction in sequencing by hybridization. The probe pattern is based on a recent combinatorial result showing that the use of probes composed of natural bases and universal bases in a well defined periodic pattern is crucial for utilizing the full power of DNA hybridization chips. &lt;br/&gt;&lt;br/&gt;The successful implementation of this new method depends on the suitable performance of universal bases (wild cards), i.e., of natural or artificial bases with the capability of &quot;hybridizing&quot; with any of the standard nucleic acid bases, on chips. This method will be tested in chemical systems in order to discover optimum patterns in the context of non-ideal behavior of oligonucleotide probes. Specific activities for the proposed grant period are:&lt;br/&gt;&lt;br/&gt; 1. A program of experiments with oligomers incorporating universal bases.&lt;br/&gt; 2. Research on the synthesis of a new best suited universal base.&lt;br/&gt; 3. Matching of sequence reconstruction procedures to developed models.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.98308e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98308e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n406" target="n408">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Applying Universal Bases to Achieving the Full Potential of Sequencing-By-Hybridization (SBH)</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;(revised 5/30/00 -pgilna)&lt;br/&gt;&lt;br/&gt;DBI 9983081 &lt;br/&gt;&lt;br/&gt;Preparata Brown University &lt;br/&gt;&lt;br/&gt;Applying Universal Bases to Achieving Full Potential of SBH&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Summary&lt;br/&gt;&lt;br/&gt;The goal of this interdisciplinary project is to establish the feasability of a more powerful and robust experimental system for the sequencing of DNA by hybridization. A conceptually novel approach is based on a new probe design and a new algorithm for sequence reconstruction which have been demonstrated by computational modeling to provide optimum information extraction in sequencing by hybridization. The probe pattern is based on a recent combinatorial result showing that the use of probes composed of natural bases and universal bases in a well defined periodic pattern is crucial for utilizing the full power of DNA hybridization chips. &lt;br/&gt;&lt;br/&gt;The successful implementation of this new method depends on the suitable performance of universal bases (wild cards), i.e., of natural or artificial bases with the capability of &quot;hybridizing&quot; with any of the standard nucleic acid bases, on chips. This method will be tested in chemical systems in order to discover optimum patterns in the context of non-ideal behavior of oligonucleotide probes. Specific activities for the proposed grant period are:&lt;br/&gt;&lt;br/&gt; 1. A program of experiments with oligomers incorporating universal bases.&lt;br/&gt; 2. Research on the synthesis of a new best suited universal base.&lt;br/&gt; 3. Matching of sequence reconstruction procedures to developed models.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.98308e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98308e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n406" target="n409">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Applying Universal Bases to Achieving the Full Potential of Sequencing-By-Hybridization (SBH)</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;(revised 5/30/00 -pgilna)&lt;br/&gt;&lt;br/&gt;DBI 9983081 &lt;br/&gt;&lt;br/&gt;Preparata Brown University &lt;br/&gt;&lt;br/&gt;Applying Universal Bases to Achieving Full Potential of SBH&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Summary&lt;br/&gt;&lt;br/&gt;The goal of this interdisciplinary project is to establish the feasability of a more powerful and robust experimental system for the sequencing of DNA by hybridization. A conceptually novel approach is based on a new probe design and a new algorithm for sequence reconstruction which have been demonstrated by computational modeling to provide optimum information extraction in sequencing by hybridization. The probe pattern is based on a recent combinatorial result showing that the use of probes composed of natural bases and universal bases in a well defined periodic pattern is crucial for utilizing the full power of DNA hybridization chips. &lt;br/&gt;&lt;br/&gt;The successful implementation of this new method depends on the suitable performance of universal bases (wild cards), i.e., of natural or artificial bases with the capability of &quot;hybridizing&quot; with any of the standard nucleic acid bases, on chips. This method will be tested in chemical systems in order to discover optimum patterns in the context of non-ideal behavior of oligonucleotide probes. Specific activities for the proposed grant period are:&lt;br/&gt;&lt;br/&gt; 1. A program of experiments with oligomers incorporating universal bases.&lt;br/&gt; 2. Research on the synthesis of a new best suited universal base.&lt;br/&gt; 3. Matching of sequence reconstruction procedures to developed models.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.98308e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98308e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n407" target="n408">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Applying Universal Bases to Achieving the Full Potential of Sequencing-By-Hybridization (SBH)</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;(revised 5/30/00 -pgilna)&lt;br/&gt;&lt;br/&gt;DBI 9983081 &lt;br/&gt;&lt;br/&gt;Preparata Brown University &lt;br/&gt;&lt;br/&gt;Applying Universal Bases to Achieving Full Potential of SBH&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Summary&lt;br/&gt;&lt;br/&gt;The goal of this interdisciplinary project is to establish the feasability of a more powerful and robust experimental system for the sequencing of DNA by hybridization. A conceptually novel approach is based on a new probe design and a new algorithm for sequence reconstruction which have been demonstrated by computational modeling to provide optimum information extraction in sequencing by hybridization. The probe pattern is based on a recent combinatorial result showing that the use of probes composed of natural bases and universal bases in a well defined periodic pattern is crucial for utilizing the full power of DNA hybridization chips. &lt;br/&gt;&lt;br/&gt;The successful implementation of this new method depends on the suitable performance of universal bases (wild cards), i.e., of natural or artificial bases with the capability of &quot;hybridizing&quot; with any of the standard nucleic acid bases, on chips. This method will be tested in chemical systems in order to discover optimum patterns in the context of non-ideal behavior of oligonucleotide probes. Specific activities for the proposed grant period are:&lt;br/&gt;&lt;br/&gt; 1. A program of experiments with oligomers incorporating universal bases.&lt;br/&gt; 2. Research on the synthesis of a new best suited universal base.&lt;br/&gt; 3. Matching of sequence reconstruction procedures to developed models.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.98308e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98308e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n407" target="n409">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Applying Universal Bases to Achieving the Full Potential of Sequencing-By-Hybridization (SBH)</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;(revised 5/30/00 -pgilna)&lt;br/&gt;&lt;br/&gt;DBI 9983081 &lt;br/&gt;&lt;br/&gt;Preparata Brown University &lt;br/&gt;&lt;br/&gt;Applying Universal Bases to Achieving Full Potential of SBH&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Summary&lt;br/&gt;&lt;br/&gt;The goal of this interdisciplinary project is to establish the feasability of a more powerful and robust experimental system for the sequencing of DNA by hybridization. A conceptually novel approach is based on a new probe design and a new algorithm for sequence reconstruction which have been demonstrated by computational modeling to provide optimum information extraction in sequencing by hybridization. The probe pattern is based on a recent combinatorial result showing that the use of probes composed of natural bases and universal bases in a well defined periodic pattern is crucial for utilizing the full power of DNA hybridization chips. &lt;br/&gt;&lt;br/&gt;The successful implementation of this new method depends on the suitable performance of universal bases (wild cards), i.e., of natural or artificial bases with the capability of &quot;hybridizing&quot; with any of the standard nucleic acid bases, on chips. This method will be tested in chemical systems in order to discover optimum patterns in the context of non-ideal behavior of oligonucleotide probes. Specific activities for the proposed grant period are:&lt;br/&gt;&lt;br/&gt; 1. A program of experiments with oligomers incorporating universal bases.&lt;br/&gt; 2. Research on the synthesis of a new best suited universal base.&lt;br/&gt; 3. Matching of sequence reconstruction procedures to developed models.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.98308e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98308e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n408" target="n409">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Applying Universal Bases to Achieving the Full Potential of Sequencing-By-Hybridization (SBH)</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;(revised 5/30/00 -pgilna)&lt;br/&gt;&lt;br/&gt;DBI 9983081 &lt;br/&gt;&lt;br/&gt;Preparata Brown University &lt;br/&gt;&lt;br/&gt;Applying Universal Bases to Achieving Full Potential of SBH&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Summary&lt;br/&gt;&lt;br/&gt;The goal of this interdisciplinary project is to establish the feasability of a more powerful and robust experimental system for the sequencing of DNA by hybridization. A conceptually novel approach is based on a new probe design and a new algorithm for sequence reconstruction which have been demonstrated by computational modeling to provide optimum information extraction in sequencing by hybridization. The probe pattern is based on a recent combinatorial result showing that the use of probes composed of natural bases and universal bases in a well defined periodic pattern is crucial for utilizing the full power of DNA hybridization chips. &lt;br/&gt;&lt;br/&gt;The successful implementation of this new method depends on the suitable performance of universal bases (wild cards), i.e., of natural or artificial bases with the capability of &quot;hybridizing&quot; with any of the standard nucleic acid bases, on chips. This method will be tested in chemical systems in order to discover optimum patterns in the context of non-ideal behavior of oligonucleotide probes. Specific activities for the proposed grant period are:&lt;br/&gt;&lt;br/&gt; 1. A program of experiments with oligomers incorporating universal bases.&lt;br/&gt; 2. Research on the synthesis of a new best suited universal base.&lt;br/&gt; 3. Matching of sequence reconstruction procedures to developed models.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.98308e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">9.98308e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n411" target="n412">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Transparent Middleware Support for Shared State on Heterogeneous Clusters</data>
      <data key="e_abstract">The explosive growth of both large and small scale distributed systems is leading to a host of new applications, most of which employ some notion of distributed shared state: information required&lt;br/&gt;at more than one location. Building on the experience of the PIs, this research will develop convenient, efficient middleware for applications running on heterogeneous platforms. The goal is to dramatically reduce the programmer effort required to develop distributed applications, and in particular to make it as easy to run applications on a local-area cluster as it is to to run them on&lt;br/&gt;a shared-memory multiprocessor. The proposed research will therefore employ a shared-memory programming model. The PIs will extend their previous work in software distributed shared memory (S-DSM) to accommodate heterogeneous languages and machine types,&lt;br/&gt;and to transparently provide shared state for independently-deployed processes. They will introduce application-specific notions of when a cached copy of data is ``recent enough&apos;&apos; to use, thereby facilitating optimized communication in more globally distributed environments.&lt;br/&gt;&lt;br/&gt;The applications driving the research include both scientific simulations and codes developed by local colleagues in the areas of datamining and intelligent multi-sense environments.&lt;br/&gt;The research will facilitate the effective distribution of these applications across the resources they require.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">9.98836e+06</data>
      <data key="e_expirationDate">2003-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98836e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n415" target="n416">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Nonstandard High Order Multigrid Techniques with Applications to Laminar Diffusion Flame Simulations</data>
      <data key="e_abstract">This project will design efficient geometric multigrid methods using &lt;br/&gt;intermediate grids to solve convection diffusion problems discretized &lt;br/&gt;by high order compact schemes. The use of intermediate grids aims at &lt;br/&gt;reducing discrepancy between the solutions obtained on different&lt;br/&gt;grids. Symbolic computation packages will be used to derive a fourth &lt;br/&gt;order finite difference scheme on the intermediate grids. Special &lt;br/&gt;relaxation schemes and intergrid transfer operators will be developed.&lt;br/&gt;The nonstandard multigrid method with high order discretization &lt;br/&gt;schemes will be used in the numerical simulation of laminar diffusion &lt;br/&gt;flames. The use of intermediate grids is expected to alleviate the &lt;br/&gt;problem haunting existing multigrid methods in the flame simulation, &lt;br/&gt;in which the converged coarse grid correction is not in the &lt;br/&gt;convergence domain of the fine grid Newton iteration.&lt;br/&gt;&lt;br/&gt;This project requires both symbolic and numerical computation &lt;br/&gt;techniques. The successful development of a useful symbolic &lt;br/&gt;computation procedure will greatly promote the awareness and use of &lt;br/&gt;symbolic computation packages in numerical computation community.&lt;br/&gt;The results of this research project will make important contribution &lt;br/&gt;to the understanding of geometric multigrid solution of convection&lt;br/&gt;diffusion problems, and of the applications of high order compact&lt;br/&gt;schemes to realistic flow simulations. Efficient solution of such &lt;br/&gt;problems is central to many numerical simulations in computational &lt;br/&gt;fluid dynamics. The fast laminar diffusion flame code is useful in &lt;br/&gt;combustion and environment protection. Certain U.S. industries related &lt;br/&gt;to commercial burners, pollutant tracking, car and airplane &lt;br/&gt;manufacturing, combustion, can benefit from this research.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">9.98816e+06</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98816e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n418" target="n419">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">CADRE: A Robotic Control Network for Distributed Experimentation</data>
      <data key="e_abstract">EIA-0079875&lt;br/&gt;Kitts, Christopher A.&lt;br/&gt;Santa Clara University&lt;br/&gt;&lt;br/&gt;CADRE: A Robotic Control Network for Distributed Experimentation&lt;br/&gt;&lt;br/&gt;Santa Clara University is developing a Distributed Robotic Control Network for use as an experimental control system for a variety of remote, unmanned robotic devices. This system supports the operation of remote robots with scientific and technology demonstration missions. In addition, the system permits experimental verification and validation of new operational control techniques. The control network will consist of a centralized mission control center, several low-cost communications stations, a Web-based user interface for distributed users, interfacility communications links, and a few low-cost test robots that can be used to demonstrate system functionality. The system will be designed and implemented to specifically support the work of geographically distributed researchers.</data>
      <data key="e_pgm">4725</data>
      <data key="e_label">79875</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79875</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n420" target="n421">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Long Range Dependence, Heavy Tails and Communication Networks</data>
      <data key="e_abstract">This project is centered on connections between long range dependence and heavy tails. The primary application areas are data networks, finance, actuarial research and industrial engineering. The project includes a consideration of statistical issues related to detection of long range dependence, self-similarity and multifractality as well as fundamental issues of basic concepts being inadequately defined. For instance, long range dependence is often defined in terms of correlations but these may be either undefined due to the presence of heavy tails or for non-Gaussian processes rather uninformative. &lt;br/&gt; Our society is a fast moving and changing one, with ever increasing amounts of information being moved from one end of the country to the other. Dramatic technological and scientific advances are, often, day-to-day phenomena. The stock market is very volatile. The Internet and other communication networks are susceptible to overloads and delays and because of their crucial role in our economic and public life, one needs to be concerned with their performance. Appropriate models are required to make sense of these wildly oscillating processes. This project is centered on models appropriate to understanding and controlling these phenomena-- models with heavy tails and/or long range dependence.</data>
      <data key="e_pgm">1263</data>
      <data key="e_label">71073</data>
      <data key="e_expirationDate">2003-06-30</data>
      <data key="e_div">0304</data>
      <data key="e_awardID">71073</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n426" target="n427">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">GOALI: Controlled Coverage With Applications to Automated Robot Painting</data>
      <data key="e_abstract">This work considers a hybrid motion planning/control problem termed controlled and constrained coverage -- i.e. the precise regulation of a mechanism&apos;s work space effects through planning, sensor observations, and long-term process monitoring to achieve the controlled execution of area-covering tasks. Typical work space effects include the sensitivity pattern of a detector used for search tasks, the reach of a lawn mower&apos;s cutting tool, but paint application in the automotive industry motivates this effort. The two basic intellectual foci of this work are understanding how coverage tasks can be accomplished while ensuring the precise control of process variables and the development of coverage path planning tools which will make it possible to adjust process variables necessitating complete replanning. These results will enable paint application&lt;br/&gt;specialists (or anyone involved in surface coating/inspection tasks) to efficiently program trajectories that satisfy performance constraints while reducing cycle time, and improving efficiency. The Principle Investigators will be working closely with researchers within Ford, including members of Ford&apos;s Scientific Research Labs, their Advanced Manufacturing Technology division, and Global Paint Engineering, to ensure practical utility and applicability of the research.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">9.98797e+06</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98797e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n426" target="n428">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">GOALI: Controlled Coverage With Applications to Automated Robot Painting</data>
      <data key="e_abstract">This work considers a hybrid motion planning/control problem termed controlled and constrained coverage -- i.e. the precise regulation of a mechanism&apos;s work space effects through planning, sensor observations, and long-term process monitoring to achieve the controlled execution of area-covering tasks. Typical work space effects include the sensitivity pattern of a detector used for search tasks, the reach of a lawn mower&apos;s cutting tool, but paint application in the automotive industry motivates this effort. The two basic intellectual foci of this work are understanding how coverage tasks can be accomplished while ensuring the precise control of process variables and the development of coverage path planning tools which will make it possible to adjust process variables necessitating complete replanning. These results will enable paint application&lt;br/&gt;specialists (or anyone involved in surface coating/inspection tasks) to efficiently program trajectories that satisfy performance constraints while reducing cycle time, and improving efficiency. The Principle Investigators will be working closely with researchers within Ford, including members of Ford&apos;s Scientific Research Labs, their Advanced Manufacturing Technology division, and Global Paint Engineering, to ensure practical utility and applicability of the research.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">9.98797e+06</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98797e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n427" target="n428">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">GOALI: Controlled Coverage With Applications to Automated Robot Painting</data>
      <data key="e_abstract">This work considers a hybrid motion planning/control problem termed controlled and constrained coverage -- i.e. the precise regulation of a mechanism&apos;s work space effects through planning, sensor observations, and long-term process monitoring to achieve the controlled execution of area-covering tasks. Typical work space effects include the sensitivity pattern of a detector used for search tasks, the reach of a lawn mower&apos;s cutting tool, but paint application in the automotive industry motivates this effort. The two basic intellectual foci of this work are understanding how coverage tasks can be accomplished while ensuring the precise control of process variables and the development of coverage path planning tools which will make it possible to adjust process variables necessitating complete replanning. These results will enable paint application&lt;br/&gt;specialists (or anyone involved in surface coating/inspection tasks) to efficiently program trajectories that satisfy performance constraints while reducing cycle time, and improving efficiency. The Principle Investigators will be working closely with researchers within Ford, including members of Ford&apos;s Scientific Research Labs, their Advanced Manufacturing Technology division, and Global Paint Engineering, to ensure practical utility and applicability of the research.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">9.98797e+06</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98797e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n433" target="n434">
      <data key="e_effectiveDate">2000-07-15</data>
      <data key="e_title">An Integrated Approach to Improving Design-Time and Run-Time Confidence</data>
      <data key="e_abstract">Proposal Number: CCR-9988409&lt;br/&gt;PI: Lee, Insup&lt;br/&gt;Co-PI: Sokolsky, Oleg&lt;br/&gt;University of Pennsylvania&lt;br/&gt;An Integrated Approach to Improving Design-Time and Run-Time Confidence&lt;br/&gt;&lt;br/&gt;The proposed research addresses two ways to improve confidence in&lt;br/&gt;software systems: design-time analysis and run-time monitoring. The&lt;br/&gt;design-time analysis is to verify design specifications with respect&lt;br/&gt;to requirements, whereas the run-time monitoring is to assure that an&lt;br/&gt;implementation is behaving as required. The proposed research will&lt;br/&gt;improve design-time analysis by means of domain-specific extensions to&lt;br/&gt;analysis techniques and tools. An extension for a chosen domain will&lt;br/&gt;be based on specification patterns typical of the domain, and on&lt;br/&gt;domain-specific abstraction mechanisms. Furthermore, the design-time&lt;br/&gt;analysis approach will be integrated with the run-time monitoring and&lt;br/&gt;checking approach. Since the success of this integration work will&lt;br/&gt;depend on the characteristics of specific domains, the identification&lt;br/&gt;of such characteristics will be part of the proposed work. The&lt;br/&gt;proposed work will be performed in the context of framework for&lt;br/&gt;end-to-end system development of high confidence software, based on a&lt;br/&gt;suite of methods and tools for the specification, analysis,&lt;br/&gt;development, testing, prototyping, simulation and monitoring. The&lt;br/&gt;integration of design-time analysis and run-time monitoring is the key&lt;br/&gt;novel aspect of the proposed work. The proposed work will be&lt;br/&gt;evaluated on a collection of applications chosen to represent domains&lt;br/&gt;requiring high confidence and real-time requirements, such as embedded&lt;br/&gt;systems and electronic commerce systems.</data>
      <data key="e_pgm">2880</data>
      <data key="e_label">9.98841e+06</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98841e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n194" target="n439">
      <data key="e_effectiveDate">2000-07-15</data>
      <data key="e_title">How Good Can Parallel Algorithms Really Be?</data>
      <data key="e_abstract">Institution: U of MD College Park&lt;br/&gt;Proposal Number: CCR-9988256&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;How Good Can Parallel Algorithms Really Be?&lt;br/&gt;Uzi Vishkin, PI and Manoj Franklin, co-PI&lt;br/&gt;Abstract&lt;br/&gt;&lt;br/&gt;Experimental study of parallel algorithms and applications is proposed. The investigators plan to conduct this study with respect to the Explicit Multi-Threaded (XMT) platform for instruction-level parallelism (ILP). A general goal of this work is to promote \thinking and programming in parallel&quot;. For this the investigators will try to better understand the performance potential of a platform such as XMT relative to existing parallel and serial platforms/models. Work which the investigators have already done provides interesting preliminary examples for mixing experimental research and theoretical analysis. This preliminary work appears to update previous knowledge by suggesting that: (i) Good speedups for much smaller inputs are possible. (ii) Incorporating analytic (non-asymptotic) performance evaluation into experimental performance analysis is possible; this includes applicability to relatively small inputs. Explicit Multi-Threading (XMT) is a fine-grained computation framework introduced in a SPAA&apos;98 paper by Vicki et al. XMT aims at faster single-task completion time by way of ILP. Building on some key ideas of parallel computing, XMT covers the spectrum from algorithms through architecture to implementation; the main implementation related innovation in XMT is through the incorporation of low-overhead hardware and software mechanisms (for more effective fine-grained parallelism).</data>
      <data key="e_pgm">2860</data>
      <data key="e_label">9.98826e+06</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98826e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n442" target="n443">
      <data key="e_effectiveDate">2000-07-15</data>
      <data key="e_title">BIOCOMPLEXITY: Collaborative Research: Microbial and Nutrient Controls in Mangrove Ecosystems</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project, which is being supported by the Directorate for Biological Sciences, the Directorate for Geosciences, the Directorate for Computer and Information Science and Engineering, and the Office of Multidisciplinary Activities in the Directorate for Mathematical and Physical Sciences, will fund research on mangrove forests that dominate the world&apos;s tropical and subtropical coasts, paralleling the geographical distribution of coral reefs. Ecological processes here are influenced by inputs from the land, sea, and sky, which result in extreme fluctuations of flooding, salinity, temperature, light, and nutrient availability. Mangrove-associated organisms have specialized physiological and structural adaptations that sustain them in this variable environment. Human-caused enrichment is one of the major global threats to these coastal ecosystems. Experiments show that nutrients are not uniformly distributed among or even within mangrove forests. Soil fertility can switch from nitrogen to phosphorus limitation across narrow gradients. This study will explore the relationships among physical and chemical factors, nutrients, microbes, trees, and elemental cycling on off-shore mangrove islands in Belize.&lt;br/&gt;&lt;br/&gt;We will examine the interactions between the environment and organisms to determine how changes in nutrient inputs from natural, agricultural, or urban sources might alter the delicate balance among these ecosystem components. Mathematical models will help us predict the contribution complexity makes to the ability of mangrove ecosystems to survive both natural and anthropogenic disturbances. The results of the study will help scientists understand how to better manage and conserve mangrove ecosystems and will contribute to our understanding of biocomplexity in other ecosystems.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">9.98154e+06</data>
      <data key="e_expirationDate">2006-06-30</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">9.98154e+06</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n442" target="n444">
      <data key="e_effectiveDate">2000-07-15</data>
      <data key="e_title">BIOCOMPLEXITY: Collaborative Research: Microbial and Nutrient Controls in Mangrove Ecosystems</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project, which is being supported by the Directorate for Biological Sciences, the Directorate for Geosciences, the Directorate for Computer and Information Science and Engineering, and the Office of Multidisciplinary Activities in the Directorate for Mathematical and Physical Sciences, will fund research on mangrove forests that dominate the world&apos;s tropical and subtropical coasts, paralleling the geographical distribution of coral reefs. Ecological processes here are influenced by inputs from the land, sea, and sky, which result in extreme fluctuations of flooding, salinity, temperature, light, and nutrient availability. Mangrove-associated organisms have specialized physiological and structural adaptations that sustain them in this variable environment. Human-caused enrichment is one of the major global threats to these coastal ecosystems. Experiments show that nutrients are not uniformly distributed among or even within mangrove forests. Soil fertility can switch from nitrogen to phosphorus limitation across narrow gradients. This study will explore the relationships among physical and chemical factors, nutrients, microbes, trees, and elemental cycling on off-shore mangrove islands in Belize.&lt;br/&gt;&lt;br/&gt;We will examine the interactions between the environment and organisms to determine how changes in nutrient inputs from natural, agricultural, or urban sources might alter the delicate balance among these ecosystem components. Mathematical models will help us predict the contribution complexity makes to the ability of mangrove ecosystems to survive both natural and anthropogenic disturbances. The results of the study will help scientists understand how to better manage and conserve mangrove ecosystems and will contribute to our understanding of biocomplexity in other ecosystems.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">9.98154e+06</data>
      <data key="e_expirationDate">2006-06-30</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">9.98154e+06</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n443" target="n444">
      <data key="e_effectiveDate">2000-07-15</data>
      <data key="e_title">BIOCOMPLEXITY: Collaborative Research: Microbial and Nutrient Controls in Mangrove Ecosystems</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project, which is being supported by the Directorate for Biological Sciences, the Directorate for Geosciences, the Directorate for Computer and Information Science and Engineering, and the Office of Multidisciplinary Activities in the Directorate for Mathematical and Physical Sciences, will fund research on mangrove forests that dominate the world&apos;s tropical and subtropical coasts, paralleling the geographical distribution of coral reefs. Ecological processes here are influenced by inputs from the land, sea, and sky, which result in extreme fluctuations of flooding, salinity, temperature, light, and nutrient availability. Mangrove-associated organisms have specialized physiological and structural adaptations that sustain them in this variable environment. Human-caused enrichment is one of the major global threats to these coastal ecosystems. Experiments show that nutrients are not uniformly distributed among or even within mangrove forests. Soil fertility can switch from nitrogen to phosphorus limitation across narrow gradients. This study will explore the relationships among physical and chemical factors, nutrients, microbes, trees, and elemental cycling on off-shore mangrove islands in Belize.&lt;br/&gt;&lt;br/&gt;We will examine the interactions between the environment and organisms to determine how changes in nutrient inputs from natural, agricultural, or urban sources might alter the delicate balance among these ecosystem components. Mathematical models will help us predict the contribution complexity makes to the ability of mangrove ecosystems to survive both natural and anthropogenic disturbances. The results of the study will help scientists understand how to better manage and conserve mangrove ecosystems and will contribute to our understanding of biocomplexity in other ecosystems.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">9.98154e+06</data>
      <data key="e_expirationDate">2006-06-30</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">9.98154e+06</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n445" target="n446">
      <data key="e_effectiveDate">2000-07-15</data>
      <data key="e_title">BIOCOMPLEXITY: Collaborative Research: Microbial and Nutrient Controls in Mangrove Ecosystems</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This project, which is being supported by the Directorate for Biological Sciences, the Directorate for Computer and Information Science and Engineering and by the Office of Multidisciplinary Activities in the Directorate for Mathematical and Physical Sciences, will fund research on mangrove forests that dominate the world&apos;s tropical and subtropical coasts, paralleling the geographical distribution of coral reefs. Ecological processes here are influenced by inputs from the land, sea, and sky, which result in extreme fluctuations of flooding, salinity, temperature, light, and nutrient availability. Mangrove-associated organisms have specialized physiological and structural adaptations that sustain them in this variable environment. Human-caused enrichment is one of the major global threats to these coastal ecosystems. Experiments show that nutrients are not uniformly distributed among or even within mangrove forests. Soil fertility can switch from nitrogen to phosphorus limitation across narrow gradients. This study will explore the relationships among physical and chemical factors, nutrients, microbes, trees, and elemental cycling on off-shore mangrove islands in Belize.&lt;br/&gt;&lt;br/&gt;We will examine the interactions between the environment and organisms to determine how changes in nutrient inputs from natural, agricultural, or urban sources might alter the delicate balance among these ecosystem components. Mathematical models will help us predict the contribution complexity makes to the ability of mangrove ecosystems to survive both natural and anthropogenic disturbances. The results of the study will help scientists understand how to better manage and conserve mangrove ecosystems and will contribute to our understanding of biocomplexity in other ecosystems.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">9.98131e+06</data>
      <data key="e_expirationDate">2006-06-30</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">9.98131e+06</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n447" target="n448">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Modeling by Manipulation</data>
      <data key="e_abstract">In this project, perceptual systems will be developed for modeling objects as they are manipulated during teleoperation. These systems will estimate those object properties that mediate the interaction between the robot and its environment, including object geometry, mass, moment of inertia, and friction. The inputs to the system will be sensor data streams from the remote environment, including force, position, and video signals, and the outputs will be models of the manipulated objects and their properties. This approach, modeling by manipulation, will create new robot perceptual capabilities in unstructured environments. In doing so, it will address a fundamental challenge in the development of autonomous robots: the ability to sense and model the environment. The proposed system will be built in the context of teleoperation to take advantage of the human operator&apos;s motor skills to execute the manipulation tasks. The sensory information returned from the remote robot will provide a comprehensive description of the task. By focusing the project&apos;s theoretical developments on specific test beds, tasks and properties, the essential perceptual capabilities will be developed to enable new progress on the other components of autonomy, including planning and control.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">9.98858e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98858e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n170" target="n451">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">PECASE: New Techniques for Optimizing the Quality and Capacity of Wireless Communication Systems</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">4096</data>
      <data key="e_label">49089</data>
      <data key="e_expirationDate">2007-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">49089</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n458" target="n459">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Digital Government: REGBASE: A Distributed Information Infrastructure for Regulation Management and Compliance Checking</data>
      <data key="e_abstract">EIA-9983368&lt;br/&gt;Law, Kincho H.&lt;br/&gt;Stanford University&lt;br/&gt;&lt;br/&gt;Digital Government: REGBASE: A Distributed Information Infrastructure for &lt;br/&gt;Regulation Management and Compliance Checking&lt;br/&gt;&lt;br/&gt;This research project proposes to develop a formal, but practical infrastructure (REGBASE) to support national efforts to make governmental regulations publicly and beneficially available. The objective is to enhance the access and retrieval of government regulations by the public, as well as to provide support for the users, framers and critics of the regulations. It has been well recognized that the complexity, diversity, and volume of regulations are detrimental to business and also hinder public understanding of government. The means to improve the situation is to understand the issues in depth, and then develop tools that can support the interaction and collaboration among different parties involved. In the proposed distributed information service framework, governmental regulatory information will be made available on-line for use by designers, inspectors and regulatory policy bodies. The infrastructure includes repositories as a base and, more importantly, tools to locate, merge, compare, and analyze the information. This application infrastructure will exploit in an intelligent way the communication and computational resources that are now widely available to the public. New network access, search and analysis tools will help disseminate regulatory data and allow finding and comparing multiple sources of related data. Partial support for this project is provided by the Information Technology and Infrastructure Systems program of NSF&apos;s Directorate for Engineering.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98337e+06</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98337e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n460" target="n461">
      <data key="e_effectiveDate">2000-07-15</data>
      <data key="e_title">Adaptive Systems-on-a-Chip for Low-Power Signal Processing</data>
      <data key="e_abstract">Future signal processing systems will use a heterogeneous System-on-a-Chip architecture for performance, flexibility, cost and power efficiency. Components will include programmable DSP and RISC cores, memories, dedicated hardware blocks and embedded field programmable logic cores for flexible and late-bound functionality. Adaptation of the system will be required to track time-varying aspects of the environment. Dynamic reconfigurability can be used to adapt algorithms and architectures to time-varying computations in signal processing applications, thus dramatically reducing average power consumption. The objective of this project is to develop theory, methodology, tools, and demonstration of several adaptive signal processing systems using parameterized macros which can be dynamically configured to adapt to the computation and save power. Improvements to embedded field programmable architectures and circuits will be developed to make them better suited for low-power signal processing applications. This includes hardware and software mechanisms at the chip and system level to support the dynamic reconfiguration of parameterized macros to save power. CAD techniques will leverage recent work in incremental place-and-route, macro-based floorplanning, and statically scheduled communication. New design methods and tools will be evaluated in four realistic and complex DSP systems which all have time-varying computations and tight power budgets: 1) an MPEG2 encoder, 2) an FFT-based wireless receiver, 3) a radar for unmanned aircraft, and 4) a wireless LAN prototype. Collaboration with ENST/Paris is anticipated.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">9.98824e+06</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98824e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n463" target="n464">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Digital Government: COPLINK Center: Information and Knowledge Management for Law Enforcement</data>
      <data key="e_abstract">EIA-9983304&lt;br/&gt;Chen, Hsinchun&lt;br/&gt;University of Arizona&lt;br/&gt;&lt;br/&gt;Digital Government: COPLINK Center: Information and Knowledge Management for Law Enforcement&lt;br/&gt;&lt;br/&gt;The information and knowledge management problems facing many government agencies stem from barriers to access and utilization resulting from the content and format of information. These problems make information (and eventually knowledge) creation and utilization and complex and daunting process. New knowledge management technologies have started to emerge in a number of different applications and organizations, such as virtual enterprising, joint ventures, aerospace engineering and digital libraries. However, there has been no systematic attempt to study the technical, social and managerial foundation, theory and methodology of knowledge management that can be adopted in various social and industrial contexts. The COPLINK Center for Excellence aims to achieve the following two goals: 1) Develop knowledge management systems technologies and methodology that are appropriate for capturing, analyzing, visualizing and sharing law enforcement related information in social and organizational contexts. The basis of such research will be grounded in information retrieval, computational linguistics, information visualization, artificial intelligence, multimedia systems, multi-agent systems, and telecommunications. 2) Study of organizational, social, cultural and methodological impacts and changes that organizations need to make to maximize and leverage on a law enforcement agency&apos;s investments in information and knowledge management. The academic foundation for such research will be based on social informatics, decision theory, communication theory, cognitive psychology and managerial and organizational research.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.9833e+06</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.9833e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n463" target="n465">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Digital Government: COPLINK Center: Information and Knowledge Management for Law Enforcement</data>
      <data key="e_abstract">EIA-9983304&lt;br/&gt;Chen, Hsinchun&lt;br/&gt;University of Arizona&lt;br/&gt;&lt;br/&gt;Digital Government: COPLINK Center: Information and Knowledge Management for Law Enforcement&lt;br/&gt;&lt;br/&gt;The information and knowledge management problems facing many government agencies stem from barriers to access and utilization resulting from the content and format of information. These problems make information (and eventually knowledge) creation and utilization and complex and daunting process. New knowledge management technologies have started to emerge in a number of different applications and organizations, such as virtual enterprising, joint ventures, aerospace engineering and digital libraries. However, there has been no systematic attempt to study the technical, social and managerial foundation, theory and methodology of knowledge management that can be adopted in various social and industrial contexts. The COPLINK Center for Excellence aims to achieve the following two goals: 1) Develop knowledge management systems technologies and methodology that are appropriate for capturing, analyzing, visualizing and sharing law enforcement related information in social and organizational contexts. The basis of such research will be grounded in information retrieval, computational linguistics, information visualization, artificial intelligence, multimedia systems, multi-agent systems, and telecommunications. 2) Study of organizational, social, cultural and methodological impacts and changes that organizations need to make to maximize and leverage on a law enforcement agency&apos;s investments in information and knowledge management. The academic foundation for such research will be based on social informatics, decision theory, communication theory, cognitive psychology and managerial and organizational research.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.9833e+06</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.9833e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n464" target="n465">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Digital Government: COPLINK Center: Information and Knowledge Management for Law Enforcement</data>
      <data key="e_abstract">EIA-9983304&lt;br/&gt;Chen, Hsinchun&lt;br/&gt;University of Arizona&lt;br/&gt;&lt;br/&gt;Digital Government: COPLINK Center: Information and Knowledge Management for Law Enforcement&lt;br/&gt;&lt;br/&gt;The information and knowledge management problems facing many government agencies stem from barriers to access and utilization resulting from the content and format of information. These problems make information (and eventually knowledge) creation and utilization and complex and daunting process. New knowledge management technologies have started to emerge in a number of different applications and organizations, such as virtual enterprising, joint ventures, aerospace engineering and digital libraries. However, there has been no systematic attempt to study the technical, social and managerial foundation, theory and methodology of knowledge management that can be adopted in various social and industrial contexts. The COPLINK Center for Excellence aims to achieve the following two goals: 1) Develop knowledge management systems technologies and methodology that are appropriate for capturing, analyzing, visualizing and sharing law enforcement related information in social and organizational contexts. The basis of such research will be grounded in information retrieval, computational linguistics, information visualization, artificial intelligence, multimedia systems, multi-agent systems, and telecommunications. 2) Study of organizational, social, cultural and methodological impacts and changes that organizations need to make to maximize and leverage on a law enforcement agency&apos;s investments in information and knowledge management. The academic foundation for such research will be based on social informatics, decision theory, communication theory, cognitive psychology and managerial and organizational research.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.9833e+06</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.9833e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n467" target="n468">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Uncovering and Exploiting Memory Parellelism in Pointer-Chasing Applications</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;Proposal: C-CR 0000988&lt;br/&gt;PI: Donald Yeung, University of Maryland&lt;br/&gt;&lt;br/&gt;Conventional memory latency tolerance techniques are limited on pointer-intensive applications because pointer-chasing memory references must perform sequentially and prevent the overlap of multiple cache misses. Pointer-chasing computations, however, traverse several independent pointer chains. Such independent traversals provide a source of memory parallelism that has remained untapped by the existing latency tolerance techniques.&lt;br/&gt;&lt;br/&gt;This research develops novel pointer prefetching techniques to exploit &quot;inter-chain&quot; memory parallelism. Compared to existing techniques, the new techniques address more effectively the memory bottleneck for pointer-chasing computations commonly found in non-numeric applications. The research consists of three major thrusts. First, techniques are developed to schedule prefetches across multiple independent pointer-chain traversals simultaneously; thus overlapping cache misses from separate pointer-chasing loops or recursive function calls. Both compile-time and run-time scheduling techniques are investigated. Second, architectural support is developed to issue prefetch requests according to the required prefetch schedules. Initially, a prefetch engine capable of traversing pointer-based data structures is studied. The research also investigates into lightweight microthreads to perform prefetching inside a multithreaded CPU. Finally, compiler support is developed to automatically extract program information for computing the prefetch schedules and for generating the prefetch requests at runtime.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">988</data>
      <data key="e_expirationDate">2003-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">988</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n469" target="n470">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Identifying and Implementing a Coordinated Strategy for Voice over IP in Higher Education</data>
      <data key="e_abstract">This workshop by the Net@EDU Project within EDUCAUSE and the follow-on information dissemination activities are designed to generate an analysis of voice-over-IP (VoIP) options, strategies, and best practices. The results will be made available to the community of universities and parties involved in NGI, Internet2, and other related initiatives. VoIP promises to be an important technology on emerging, advanced networks. One of Net@EDU&apos;s special interest groups has already been working these issues. They will be &lt;br/&gt;involved in these workshops and already have some useful work done that should help to jump start and accelerate this workshop activity and the quality of the proceedings.</data>
      <data key="e_pgm">4090</data>
      <data key="e_label">83358</data>
      <data key="e_expirationDate">2002-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">83358</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n471" target="n472">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">System Support for Distributed Information Change Monitoring</data>
      <data key="e_abstract">The project investigates the design and implementation issues in the mointoring of information changes in large distributed networks such as the Internet and World Wide Web. One of the objectives is to develop efficient and scalable strategies, techniques, and systems support for distributed control of large numbers of information change monitoring requests. Research questions to be addressed include the following. What software techniques and tools are able to extract, integrate, and query streams of data over semi-structured or unstructured data? Which distributed trigger and data processing techniques are most scalable and yet efficient in the presence of millions of information change monitoring requests? How do change monitors adapt to wide system parameter variations in runtime environments such as the Internet? How do they scale up as the number of information sources reach millions? Critical system components include change detection algorithms (e.g., tree comparison for web pages), trigger and query grouping, indexing, and caching, as well as parallel processing. Appropriate components will be implemented and evaluated through simulation and measurements on the Internet. These experiments will emphasize the efficiency and scalability of Internet-scale information change monitoring systems. The research results will aid in the engineering, implementation and evaluation of systems and middleware software support for scalable and efficient processing of distributed triggers and queries, as well as effective detection and notification of information changes.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">9.98845e+06</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98845e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n170" target="n477">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Research at the Frontier of the Physical Layer</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">4096</data>
      <data key="e_label">49085</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">49085</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n284" target="n480">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Scalable Multilevel Algorithms in Computational Sciences</data>
      <data key="e_abstract">Solvers for Partial Differential Equations (PDEs) are the backbone of much of scientific computing. In particular, they are the basis of Computational Fluid Dynamics (CFD), the modeling of liquid and gas flows. This project studies new, efficient methods for solving PDEs and implements those methods on modern high-performance parallel computers. These solvers are useful in areas other than their original CFD home - in particular, surprising applications to diverse areas such as image restoration and VLSI placement will be studied as well.&lt;br/&gt;&lt;br/&gt;Technically, this project will investigate efficient algebraic multiscale algorithms for elliptic and non-elliptic PDE and CFD problems on arbitrary unstructured meshes which are suitable for distributed and shared memory parallel computing architectures. In addition, it will study how these algorithms can be extended to other large scale non-PDE problems, including image restoration and VLSI placement problems. Three aspects of these multiscale algorithms will be emphasized in this work: (1) Issues arising from making these algorithms more algebraic (for ease of use) including robustness to anisotropy, jumps and oscillations in coefficients, homogenization, etc. (2) Extension of these algorithms from their normal elliptic setting to non-elliptic and more generally non-PDE, graph-based settings. (3) Performance on modern high performance computer architectures with particular attention paid to communication and cache memory latency. Particular attention will be placed on algorithms appropriate for solving discretization matrices arising from a variety of large scale scientific computing problems such as CFD for advection dominated problems, VLSI placement an image processing. The non-elliptic behavior of these practical problems renders the known multilevel theory inadequate and serves to motivate a balanced effort consisting of algorithmic development, theoretical analysis, and practical application.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">72112</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">72112</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n284" target="n286">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Scalable Multilevel Algorithms in Computational Sciences</data>
      <data key="e_abstract">Solvers for Partial Differential Equations (PDEs) are the backbone of much of scientific computing. In particular, they are the basis of Computational Fluid Dynamics (CFD), the modeling of liquid and gas flows. This project studies new, efficient methods for solving PDEs and implements those methods on modern high-performance parallel computers. These solvers are useful in areas other than their original CFD home - in particular, surprising applications to diverse areas such as image restoration and VLSI placement will be studied as well.&lt;br/&gt;&lt;br/&gt;Technically, this project will investigate efficient algebraic multiscale algorithms for elliptic and non-elliptic PDE and CFD problems on arbitrary unstructured meshes which are suitable for distributed and shared memory parallel computing architectures. In addition, it will study how these algorithms can be extended to other large scale non-PDE problems, including image restoration and VLSI placement problems. Three aspects of these multiscale algorithms will be emphasized in this work: (1) Issues arising from making these algorithms more algebraic (for ease of use) including robustness to anisotropy, jumps and oscillations in coefficients, homogenization, etc. (2) Extension of these algorithms from their normal elliptic setting to non-elliptic and more generally non-PDE, graph-based settings. (3) Performance on modern high performance computer architectures with particular attention paid to communication and cache memory latency. Particular attention will be placed on algorithms appropriate for solving discretization matrices arising from a variety of large scale scientific computing problems such as CFD for advection dominated problems, VLSI placement an image processing. The non-elliptic behavior of these practical problems renders the known multilevel theory inadequate and serves to motivate a balanced effort consisting of algorithmic development, theoretical analysis, and practical application.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">72112</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">72112</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n284" target="n482">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Scalable Multilevel Algorithms in Computational Sciences</data>
      <data key="e_abstract">Solvers for Partial Differential Equations (PDEs) are the backbone of much of scientific computing. In particular, they are the basis of Computational Fluid Dynamics (CFD), the modeling of liquid and gas flows. This project studies new, efficient methods for solving PDEs and implements those methods on modern high-performance parallel computers. These solvers are useful in areas other than their original CFD home - in particular, surprising applications to diverse areas such as image restoration and VLSI placement will be studied as well.&lt;br/&gt;&lt;br/&gt;Technically, this project will investigate efficient algebraic multiscale algorithms for elliptic and non-elliptic PDE and CFD problems on arbitrary unstructured meshes which are suitable for distributed and shared memory parallel computing architectures. In addition, it will study how these algorithms can be extended to other large scale non-PDE problems, including image restoration and VLSI placement problems. Three aspects of these multiscale algorithms will be emphasized in this work: (1) Issues arising from making these algorithms more algebraic (for ease of use) including robustness to anisotropy, jumps and oscillations in coefficients, homogenization, etc. (2) Extension of these algorithms from their normal elliptic setting to non-elliptic and more generally non-PDE, graph-based settings. (3) Performance on modern high performance computer architectures with particular attention paid to communication and cache memory latency. Particular attention will be placed on algorithms appropriate for solving discretization matrices arising from a variety of large scale scientific computing problems such as CFD for advection dominated problems, VLSI placement an image processing. The non-elliptic behavior of these practical problems renders the known multilevel theory inadequate and serves to motivate a balanced effort consisting of algorithmic development, theoretical analysis, and practical application.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">72112</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">72112</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n286" target="n480">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Scalable Multilevel Algorithms in Computational Sciences</data>
      <data key="e_abstract">Solvers for Partial Differential Equations (PDEs) are the backbone of much of scientific computing. In particular, they are the basis of Computational Fluid Dynamics (CFD), the modeling of liquid and gas flows. This project studies new, efficient methods for solving PDEs and implements those methods on modern high-performance parallel computers. These solvers are useful in areas other than their original CFD home - in particular, surprising applications to diverse areas such as image restoration and VLSI placement will be studied as well.&lt;br/&gt;&lt;br/&gt;Technically, this project will investigate efficient algebraic multiscale algorithms for elliptic and non-elliptic PDE and CFD problems on arbitrary unstructured meshes which are suitable for distributed and shared memory parallel computing architectures. In addition, it will study how these algorithms can be extended to other large scale non-PDE problems, including image restoration and VLSI placement problems. Three aspects of these multiscale algorithms will be emphasized in this work: (1) Issues arising from making these algorithms more algebraic (for ease of use) including robustness to anisotropy, jumps and oscillations in coefficients, homogenization, etc. (2) Extension of these algorithms from their normal elliptic setting to non-elliptic and more generally non-PDE, graph-based settings. (3) Performance on modern high performance computer architectures with particular attention paid to communication and cache memory latency. Particular attention will be placed on algorithms appropriate for solving discretization matrices arising from a variety of large scale scientific computing problems such as CFD for advection dominated problems, VLSI placement an image processing. The non-elliptic behavior of these practical problems renders the known multilevel theory inadequate and serves to motivate a balanced effort consisting of algorithmic development, theoretical analysis, and practical application.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">72112</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">72112</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n480" target="n482">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Scalable Multilevel Algorithms in Computational Sciences</data>
      <data key="e_abstract">Solvers for Partial Differential Equations (PDEs) are the backbone of much of scientific computing. In particular, they are the basis of Computational Fluid Dynamics (CFD), the modeling of liquid and gas flows. This project studies new, efficient methods for solving PDEs and implements those methods on modern high-performance parallel computers. These solvers are useful in areas other than their original CFD home - in particular, surprising applications to diverse areas such as image restoration and VLSI placement will be studied as well.&lt;br/&gt;&lt;br/&gt;Technically, this project will investigate efficient algebraic multiscale algorithms for elliptic and non-elliptic PDE and CFD problems on arbitrary unstructured meshes which are suitable for distributed and shared memory parallel computing architectures. In addition, it will study how these algorithms can be extended to other large scale non-PDE problems, including image restoration and VLSI placement problems. Three aspects of these multiscale algorithms will be emphasized in this work: (1) Issues arising from making these algorithms more algebraic (for ease of use) including robustness to anisotropy, jumps and oscillations in coefficients, homogenization, etc. (2) Extension of these algorithms from their normal elliptic setting to non-elliptic and more generally non-PDE, graph-based settings. (3) Performance on modern high performance computer architectures with particular attention paid to communication and cache memory latency. Particular attention will be placed on algorithms appropriate for solving discretization matrices arising from a variety of large scale scientific computing problems such as CFD for advection dominated problems, VLSI placement an image processing. The non-elliptic behavior of these practical problems renders the known multilevel theory inadequate and serves to motivate a balanced effort consisting of algorithmic development, theoretical analysis, and practical application.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">72112</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">72112</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n286" target="n482">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Scalable Multilevel Algorithms in Computational Sciences</data>
      <data key="e_abstract">Solvers for Partial Differential Equations (PDEs) are the backbone of much of scientific computing. In particular, they are the basis of Computational Fluid Dynamics (CFD), the modeling of liquid and gas flows. This project studies new, efficient methods for solving PDEs and implements those methods on modern high-performance parallel computers. These solvers are useful in areas other than their original CFD home - in particular, surprising applications to diverse areas such as image restoration and VLSI placement will be studied as well.&lt;br/&gt;&lt;br/&gt;Technically, this project will investigate efficient algebraic multiscale algorithms for elliptic and non-elliptic PDE and CFD problems on arbitrary unstructured meshes which are suitable for distributed and shared memory parallel computing architectures. In addition, it will study how these algorithms can be extended to other large scale non-PDE problems, including image restoration and VLSI placement problems. Three aspects of these multiscale algorithms will be emphasized in this work: (1) Issues arising from making these algorithms more algebraic (for ease of use) including robustness to anisotropy, jumps and oscillations in coefficients, homogenization, etc. (2) Extension of these algorithms from their normal elliptic setting to non-elliptic and more generally non-PDE, graph-based settings. (3) Performance on modern high performance computer architectures with particular attention paid to communication and cache memory latency. Particular attention will be placed on algorithms appropriate for solving discretization matrices arising from a variety of large scale scientific computing problems such as CFD for advection dominated problems, VLSI placement an image processing. The non-elliptic behavior of these practical problems renders the known multilevel theory inadequate and serves to motivate a balanced effort consisting of algorithmic development, theoretical analysis, and practical application.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">72112</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">72112</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n483" target="n484">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">High-Quality Tests for Synchronous Sequential Circuits</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">4710</data>
      <data key="e_label">49081</data>
      <data key="e_expirationDate">2002-01-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">49081</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n490" target="n491">
      <data key="e_effectiveDate">2000-07-15</data>
      <data key="e_title">Digital Government: A Language Modeling Approach to Metadata for Cross-Database Linkage and Search</data>
      <data key="e_abstract">EIA-9983215&lt;br/&gt;Croft, W. Bruce&lt;br/&gt;University of Massachusetts Ahmerst&lt;br/&gt;&lt;br/&gt;Digital Government: A Language Modeling Approach to Metadata for Cross-Databse Linkage and Search&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;A crucial problem in government on-line services is that related information is often maintained in different systems across different agencies, thus making it difficult to integrate that information. Most attempts at &quot;one-stop shopping&quot; sites in the government fail for that reason; they must be maintained manually which does not work over time. On the other hand, data warehousing, where the data is cleaned, made conformant and housed in a central location, will not work due to the distributed nature of government-held information.&lt;br/&gt;&lt;br/&gt;One promising avenue to approach this problem would be through the use of automated systems which generate metadata (data about data) for various datasets, allowing them to be integrated more easily, while the original data remains in place at its home site. The work of this proposal will address this problem in a new way - language modeling. The government collaborators are the US Geological Survey, the US Department of Commerce, the General Services Administration and the US Library of Congress.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98322e+06</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98322e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n490" target="n492">
      <data key="e_effectiveDate">2000-07-15</data>
      <data key="e_title">Digital Government: A Language Modeling Approach to Metadata for Cross-Database Linkage and Search</data>
      <data key="e_abstract">EIA-9983215&lt;br/&gt;Croft, W. Bruce&lt;br/&gt;University of Massachusetts Ahmerst&lt;br/&gt;&lt;br/&gt;Digital Government: A Language Modeling Approach to Metadata for Cross-Databse Linkage and Search&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;A crucial problem in government on-line services is that related information is often maintained in different systems across different agencies, thus making it difficult to integrate that information. Most attempts at &quot;one-stop shopping&quot; sites in the government fail for that reason; they must be maintained manually which does not work over time. On the other hand, data warehousing, where the data is cleaned, made conformant and housed in a central location, will not work due to the distributed nature of government-held information.&lt;br/&gt;&lt;br/&gt;One promising avenue to approach this problem would be through the use of automated systems which generate metadata (data about data) for various datasets, allowing them to be integrated more easily, while the original data remains in place at its home site. The work of this proposal will address this problem in a new way - language modeling. The government collaborators are the US Geological Survey, the US Department of Commerce, the General Services Administration and the US Library of Congress.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98322e+06</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98322e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n491" target="n492">
      <data key="e_effectiveDate">2000-07-15</data>
      <data key="e_title">Digital Government: A Language Modeling Approach to Metadata for Cross-Database Linkage and Search</data>
      <data key="e_abstract">EIA-9983215&lt;br/&gt;Croft, W. Bruce&lt;br/&gt;University of Massachusetts Ahmerst&lt;br/&gt;&lt;br/&gt;Digital Government: A Language Modeling Approach to Metadata for Cross-Databse Linkage and Search&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;A crucial problem in government on-line services is that related information is often maintained in different systems across different agencies, thus making it difficult to integrate that information. Most attempts at &quot;one-stop shopping&quot; sites in the government fail for that reason; they must be maintained manually which does not work over time. On the other hand, data warehousing, where the data is cleaned, made conformant and housed in a central location, will not work due to the distributed nature of government-held information.&lt;br/&gt;&lt;br/&gt;One promising avenue to approach this problem would be through the use of automated systems which generate metadata (data about data) for various datasets, allowing them to be integrated more easily, while the original data remains in place at its home site. The work of this proposal will address this problem in a new way - language modeling. The government collaborators are the US Geological Survey, the US Department of Commerce, the General Services Administration and the US Library of Congress.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98322e+06</data>
      <data key="e_expirationDate">2004-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98322e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n495" target="n496">
      <data key="e_effectiveDate">2000-07-15</data>
      <data key="e_title">Development of Geocasting Protocols for a MANET</data>
      <data key="e_abstract">Ad hoc networking involves computers, typically wireless mobile nodes, that cooperatively form a network without specific user administration or configuration. In other words, ad hoc networking allows an arbitrary collection of mobile nodes to create a network on demand. This capability has numerous applications in tactical missions and rescue operations, as well as for educational and commercial use. Due to the number of applications desiring the formation of an ad hoc network infrastructure, development of this technology is currently a research priority at NSF. The work in this proposal concerns the development and performance evaluation of protocols that offer geocast communication in an ad hoc network. The goal of a geocasting protocol is to deliver a packet to a set of nodes within a specified geographical area, i.e., the geocast region. The researchers will develop and evaluate protocols that offer geocast communication to both explicityly defined groups (i.e., geocast to those mobile nodes in the geocast region that have registered with the group) and implicitly defined groups (i.e., geocast to all mobile nodes in the geocast region). The researchers will also consider both fixed geocast regions (i.e., defined by a specific geographical location) and dynamic geocast regions (i.e., defined by a dangerous moving target). The researchers will develop and evaluate (via simulation) three different approaches to geocasting: a directed flooding approach, a tree-based approach, and a mesh-based approach. They also propose to develop a hybrid approach that is based on the nature of the message to be transmitted and/or performance considerations (e.g., if the mobile nodes have been moving slowly in the recent past, then the redundancy offered by a mesh-based approach may not be needed). Lastly, the researchers plan to incorporate predictable and controllable end-to-end quality of service when a shared tree or shared mesh approach is used. Throughout the project, they will conduct extensive simulations to determine the conditions under which each of the geocast algorithms is most effective. The researchers will vary the essential parameters in the simulations to avoid biased results, e.g., network size, connectivity, topological rate of change, and various movement/calling patterns. In addition, the simulations will take into account some realistic limitations that have not always been addressed in prior performance evaluations, e.g., location errors that exist in widely available consumer Global Positioning Systems. The proposed project is a collaboration between two investigators who have the experience and capability necessary to complete the project successfully. The expertise of one investigator is in the mobile computing and networking area. The expertise of the other investigator is in the development of simulation models and the evaluation fo a protocol&apos;s performance. Throughout the investigation, the approach will be from a practical sense. The researchers will attend Internet Engineering Task Force meetings to obtain the current status of ad hoc protocols and to share the results.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">73699</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">73699</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n501" target="n502">
      <data key="e_effectiveDate">2000-07-15</data>
      <data key="e_title">New Statistical Challenges Posed by Multiscale and Adaptive Representations</data>
      <data key="e_abstract">NEW STATISTICAL CHALLENGES POSED BY MULTISCALE AND ADAPTIVE&lt;br/&gt;REPRESENTATIONS D.L. Donoho &amp; I.M. Johnstone, PI&apos;s.&lt;br/&gt;Project period: 07/01/00 -- 06/30/05&lt;br/&gt;&lt;br/&gt;The project will address the following specific topics:&lt;br/&gt;1. Estimation in tomography: the curvelet tight frame of&lt;br/&gt;representation seems, according to preliminary calculations, to&lt;br/&gt;achieve faster rates of convergence than traditional tomographic&lt;br/&gt;methods.&lt;br/&gt;2. Estimation and testing in time-frequency analysis: a new tight&lt;br/&gt;frame of chirplets has been built by deploying curvelets in the&lt;br/&gt;time-frequency plane. The new representation has promise for detection and&lt;br/&gt;estimation of chirps in the presence of noise.&lt;br/&gt;3. Recent work in computational vision asks ``what is the best basis&lt;br/&gt;for representing natural images?&apos;&apos; It is proposed that many empirical&lt;br/&gt;results in the rapidly growing literature on this topic might be&lt;br/&gt;explained and improved using analytically-constructed representations:&lt;br/&gt;ridgelets and curvelets.&lt;br/&gt;4. Geometry-driven diffusions are popular in applied image processing&lt;br/&gt;-- but their quantitative performance is not well-understood. It is&lt;br/&gt;proposed to develop a quantitative statistical theory using recent&lt;br/&gt;tools such as multiscale ridgelets.&lt;br/&gt;5. Extensions of Sparsity-Based ideas. Problems of finding `edgels in&lt;br/&gt;white noise&apos; and `subspaces in white noise&apos; offer challenging and&lt;br/&gt;timely directions in which to generalize existing sparsity ideas.&lt;br/&gt;6. Testing Sparse Means. An adaptive approach is suggested for&lt;br/&gt;testing if the mean of a random vector is nonzero, when the vector&lt;br/&gt;might exhibit an unknown degree of sparsity.&lt;br/&gt;7. Asymptotics of top eigenvalues of large covariance matrices.&lt;br/&gt;A program is set out to develop statistical theory for principal&lt;br/&gt;components of large data matrices based on recent progress in&lt;br/&gt;random matrix theory.&lt;br/&gt;Consistent with the principle of ``reproducible research&apos;&apos;, software&lt;br/&gt;and figures from this project will be made available in future&lt;br/&gt;releases of the public domain WaveLab system.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;In recent years, research in wavelets and time-frequency methods has&lt;br/&gt;broadened to construct new systems of representation, including&lt;br/&gt;systems custom-tailored for specific phenomena. Examples include&lt;br/&gt;wavelet packets and cosine packets, and very recently, systems like&lt;br/&gt;edgelets, ridgelets, chirplets, warplets, and curvelets. In parallel,&lt;br/&gt;research in statistical analysis and cognate fields allows data&lt;br/&gt;themselves to dictate the design of their own optimal systems of&lt;br/&gt;representation. Principal components (i.e. Karhunen-Loeve&lt;br/&gt;decomposition) is the oldest example of such data-adaptive&lt;br/&gt;representation; many newer ideas have been proposed recently, such as&lt;br/&gt;independent components analysis. The proposers have been&lt;br/&gt;active in both domains, creating new image and signal&lt;br/&gt;representations and developing statistical theory to underpin adaptive&lt;br/&gt;signal representations. The current project will (a) pursue two&lt;br/&gt;opportunities arising from the recent introduction of curvelets, (b)&lt;br/&gt;address two active applied research areas, computational vision and&lt;br/&gt;geometry driven diffusions, and (c) attack some issues which are&lt;br/&gt;argued to be at the core of new developments in statistical decision&lt;br/&gt;theory. Topic (a) may have implications for applied work in&lt;br/&gt;tomography, image and signal processing, and (c) may impact applied&lt;br/&gt;uses of principal components in domains such as climate and global&lt;br/&gt;change studies.</data>
      <data key="e_pgm">1269</data>
      <data key="e_label">72661</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0304</data>
      <data key="e_awardID">72661</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n507" target="n508">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Design, Theoretical Validation, and Empirical Evaluation of a Class of Multicast Congestion Control Schemes: Marriage of Feedback Control Theory and Multicast Congestion Control</data>
      <data key="e_abstract">The researchers have observed an explosive growth in transporting continuous media applications to multiple recipients on the Internet. Examples include continuous media servers, digital libraries, remote medical diagnosis, and distance learning. As most Internet continuous media based multicast applications do not support end-to-end congestion control, wide deployment of these applications have severe negative impact, ranging from starvation of self-controlled TCP flows to the potential for congestion collapse. The main intent of this proposal is thus to design, implement, analytically validate, and empirically evaluate a class of congestion control schemes for multicasts with the following design objectives: scalability, capability to adjust source sending rates to achieve TCP-friendliness and (weighted) fairness in an analytically provable manner, capability to handle independent losses of the same packet, capability to deal with dynamic traffic/membership changes, and minimal router support.&lt;br/&gt; Specifically, the researchers will lay out an analytical framework for rate-based multicast congestion control. In particular, T1. The researchers propose a novel approach for ACK aggregation so as to provide the sender with a simple but comprehensive view of congestion conditions in the multicast tree.&lt;br/&gt; T2. The researchers characterize the congestion status with three phases: congestion free, congestion alert, and congestion avoidance, and will devise a simple, yet effective mechanism for a sender to diagnose, based on the parameters in the aggregated acknowledgment received and parameters locally kept, which congestion phase its connection is in and whether or not independent losses of the same packet has occurred.&lt;br/&gt; T3. The researchers will devise, based on robust feedback control theory, a class of rate adjustment schemes that are (i) TCP-friendly, (ii) robustly stable, and (iii) achieve (weighted) fairness among competing multicast sessions, feature (ii) is especially important for systems with feedback loops, but has not been extensively addressed in most of the multicast congestion control work perhaps except [10]. The proposed control theoretic work will hence center around robust stability and disturbance attenuation problems that appear in rate-based flow control for multicasts.&lt;br/&gt; T4. To promote the wide deployment of proposed schemes, the researchers will investigate whether or not, and how, the operations (perhaps except acknowledgment aggregation) can be realized at end hosts. If some of the operations cannot be accomplished without router support, the researchers will look into possible light-weight implementation methods.&lt;br/&gt; T5. To empirically evaluate the performance of proposed multicast congestion control schemes, the researchers will conduct extensive simulation in ns-2 [81] to test their TCP-friendliness, fairness, and scalability properties and compare them against existing work. With the help of OARnet (the Ohio GigaPOP manager of Internet2) personnel,the researchers will also implement the proposed schemes in FreeBSD, and use a database server centered on large amounts of patient images acquired from MR and CT as a representative multicast application to empirically measure the key functional, scalable, and adaptive characteristics of the prototype software over the Internet2.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">73725</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">73725</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n509" target="n510">
      <data key="e_effectiveDate">2000-07-01</data>
      <data key="e_title">Tight Frames of Rational Splines and Application to CAD/CAM and Computer Graphics</data>
      <data key="e_abstract">Cardinal splines (i.e. splines with equally-spaced knots) provide a &lt;br/&gt;canonical example for demonstrating the mathematical structure of &lt;br/&gt;multiresolution analysis (MRA), which is a very powerful tool for the &lt;br/&gt;construction of orthogonal, semi-orthogonal, and bi-orthogonal &lt;br/&gt;wavelets. However, for practical applications, splines with arbitrary &lt;br/&gt;knots are often needed; and although the multi-level (ML) structure of &lt;br/&gt;spline subspaces is still valid, the Fourier approach to the study of &lt;br/&gt;cardinal splines and their corresponding wavelets no longer applies to &lt;br/&gt;this more general study. In addition, even for the cardinal setting, &lt;br/&gt;orthogonal and semi-orthogonal splines must be replaced by tight affine &lt;br/&gt;(or wavelet) frames when compact support (or finite time-duration) is &lt;br/&gt;needed for both spline-wavelet analysis and spline-wavelet synthesis. &lt;br/&gt;Furthermore, for CAD/CAM applications, there is also the need of &lt;br/&gt;rational B-splines (or NURBS) for precise representation of various &lt;br/&gt;geometric objects such as conic sections.&lt;br/&gt;&lt;br/&gt;This proposal is based on our recent results on tight frames of &lt;br/&gt;cardinal splines and on our newly developed complete ML structure of &lt;br/&gt;NURBS in terms of their weights. We propose to introduce non-stationary &lt;br/&gt;time-domain techniques to build a unified mathematical foundation of &lt;br/&gt;tight affine frames of compactly supported splines with arbitrary &lt;br/&gt;knots, and more generally, of NURBS. This will be followed by &lt;br/&gt;development of efficient algorithms and computational schemes &lt;br/&gt;associated with these new basic functions, which will be used as &lt;br/&gt;analysis and synthesis tools for CAD/CAM and computer graphics &lt;br/&gt;applications. We also propose to develop software libraries of these &lt;br/&gt;analysis and design tools that will be fully compatible with the &lt;br/&gt;industrial standards such as IGES, STEP, and PHIGS, for the CAD/CAM/CAE &lt;br/&gt;industries.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">9.98829e+06</data>
      <data key="e_expirationDate">2003-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98829e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n405" target="n518">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Microengines for Programmable Self-timed Control</data>
      <data key="e_abstract">This project explores practical architectures, circuits, and tool support for asynchronous digital systems. In particular, it seeks to fill a design niche that requires both high-performance in a specific problem domain and some flexibility in how the computation proceeds. One focus is on an asynchronous microprogrammed control style called a microengine. In addition to the flexibility that comes from a microcoded execution style, the asynchronous microengine approach embodies many advantages related directly to the asynchronous execution model of the controller, and of the circuits that it controls. This project explores the classes of design problems that can benefit from asynchronous microengines, explores the use of specialized circuit structures to enhance performance, and constructs a tool for the automated synthesis of microengines within the ACK high-level synthesis framework.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">9.98833e+06</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98833e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n521" target="n522">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Compositional Techniques for Verification and Performance Analysis of Reactive Probabilistic Systems</data>
      <data key="e_abstract">CCR-9988155&lt;br/&gt;Stark, Eugene W.&lt;br/&gt;Smolka, Scott A.&lt;br/&gt;State University of New York at Stony Brook&lt;br/&gt;&quot;Compositional Techniques for Verification and Performance Analysis&lt;br/&gt; of Reactive, Probabilistic Systems&quot;&lt;br/&gt;&lt;br/&gt;The research proposed here has as its objectives: (1) to develop&lt;br/&gt;significantly improved techniques for the formal specification,&lt;br/&gt;verification, and performance analysis of systems that exhibit&lt;br/&gt;concurrent, reactive, probabilistic, and real-time behavior; (2) to&lt;br/&gt;construct implementations of these techniques in the form of efficient&lt;br/&gt;computer-aided design tools; and (3) to assess these tools and&lt;br/&gt;techniques and guide their further development by applying them to&lt;br/&gt;problems drawn from actual real-world engineering practice. In this&lt;br/&gt;research, attention will be focused primarily on finite-state system&lt;br/&gt;models. A key characteristic of the approach is compositionality,&lt;br/&gt;which permits a large system to be described as the composition of a&lt;br/&gt;collection of simpler components and to be analyzed in a&lt;br/&gt;component-by-component fashion. This research will build on previous&lt;br/&gt;work of the PIs, in which they developed a model, called&lt;br/&gt;&quot;probabilistic I/O automata&quot; (PIOA), for the compositional&lt;br/&gt;specification of reactive, probabilistic systems, together with&lt;br/&gt;compositional performance analysis algorithms for this model. The&lt;br/&gt;proposed project would advance the state of the art in the area of&lt;br/&gt;formal specification and verification techniques for finite state&lt;br/&gt;systems, which holds promise for enabling more efficient production of&lt;br/&gt;more reliable software systems.</data>
      <data key="e_pgm">2880</data>
      <data key="e_label">9.98816e+06</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98816e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n524" target="n525">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">ITR: Fast Superconducting Qubit and Qugate for Quantum Computing</data>
      <data key="e_abstract">EIA-0082499&lt;br/&gt;Siyuan, Han&lt;br/&gt;University of Kansas&lt;br/&gt;&lt;br/&gt;Title: ITR: Fast Superconducting Qubit and Qugate for Quantum Computing&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;An interdisciplinary team of experimentalists and theoreticians from the University of Kansas and the Japanese Ministry of Posts and Telecommunications are collaborating to explore the use of superconducting interference devices for quantum computing. The SQUID devices under investigation consist of Josephson junctions embedded in a superconducting loop, and will be used to implement quantum bits and quantum gates. This work concentrates on two problems:&lt;br/&gt;&lt;br/&gt;1. To increase the decoherence time of qubits using niobium nitride (NbN) as the superconducting material. NbN has much lower intrinsic damming than the commonly used niobium SQUID. The Japanese collaborators, who have the best NbN junction fabrication facilities in the world, will make samples of tunnel junctions and SQUIDs. These will be sent to Kansas for measurement of decoherence times.&lt;br/&gt;&lt;br/&gt;2. To conduct an integrated theoretical and experimental study of Controlled-Not gates using SQUID.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82499</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82499</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n526" target="n527">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Multi-Lingual Digital Library for West African Sources</data>
      <data key="e_abstract">This project is entitled &quot;Multi-Lingual Digital Library for West African Sources&quot;. It seeks to build a multi-media digital library of West African sources in multiple languages. Collaborating &lt;br/&gt;organizations include several at Michigan State, the Institut Fondemental d&apos;Afrique Noire (IFAN) and West African Research Center (WARC) in Dakar, Senegal. Four types of material will be archived &lt;br/&gt;including historical manuscripts from the 19th and early 20th centuries (primarily in Arabic and French), oral sources (primarily in Wolof, Pulaar, and French), photographs and African language print &lt;br/&gt;materials. The project will give careful attention to identifying and providing metadata that describe the provenance and content of the digital materials. A multiple-languages interface will allow &lt;br/&gt;users to gain access to content through the use of standardized subject headings in the original languages. Caching proxy server technology will be used a local site servers in Senegal to address &lt;br/&gt;low-speed international network connections common in African countries. It is hoped that this project will serve as a model for larger-scale efforts by African and US researchers to create new repositories for the broader research and education communities.</data>
      <data key="e_pgm">5976</data>
      <data key="e_label">9.90583e+06</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">9.90583e+06</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n526" target="n528">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Multi-Lingual Digital Library for West African Sources</data>
      <data key="e_abstract">This project is entitled &quot;Multi-Lingual Digital Library for West African Sources&quot;. It seeks to build a multi-media digital library of West African sources in multiple languages. Collaborating &lt;br/&gt;organizations include several at Michigan State, the Institut Fondemental d&apos;Afrique Noire (IFAN) and West African Research Center (WARC) in Dakar, Senegal. Four types of material will be archived &lt;br/&gt;including historical manuscripts from the 19th and early 20th centuries (primarily in Arabic and French), oral sources (primarily in Wolof, Pulaar, and French), photographs and African language print &lt;br/&gt;materials. The project will give careful attention to identifying and providing metadata that describe the provenance and content of the digital materials. A multiple-languages interface will allow &lt;br/&gt;users to gain access to content through the use of standardized subject headings in the original languages. Caching proxy server technology will be used a local site servers in Senegal to address &lt;br/&gt;low-speed international network connections common in African countries. It is hoped that this project will serve as a model for larger-scale efforts by African and US researchers to create new repositories for the broader research and education communities.</data>
      <data key="e_pgm">5976</data>
      <data key="e_label">9.90583e+06</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">9.90583e+06</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n526" target="n529">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Multi-Lingual Digital Library for West African Sources</data>
      <data key="e_abstract">This project is entitled &quot;Multi-Lingual Digital Library for West African Sources&quot;. It seeks to build a multi-media digital library of West African sources in multiple languages. Collaborating &lt;br/&gt;organizations include several at Michigan State, the Institut Fondemental d&apos;Afrique Noire (IFAN) and West African Research Center (WARC) in Dakar, Senegal. Four types of material will be archived &lt;br/&gt;including historical manuscripts from the 19th and early 20th centuries (primarily in Arabic and French), oral sources (primarily in Wolof, Pulaar, and French), photographs and African language print &lt;br/&gt;materials. The project will give careful attention to identifying and providing metadata that describe the provenance and content of the digital materials. A multiple-languages interface will allow &lt;br/&gt;users to gain access to content through the use of standardized subject headings in the original languages. Caching proxy server technology will be used a local site servers in Senegal to address &lt;br/&gt;low-speed international network connections common in African countries. It is hoped that this project will serve as a model for larger-scale efforts by African and US researchers to create new repositories for the broader research and education communities.</data>
      <data key="e_pgm">5976</data>
      <data key="e_label">9.90583e+06</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">9.90583e+06</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n527" target="n528">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Multi-Lingual Digital Library for West African Sources</data>
      <data key="e_abstract">This project is entitled &quot;Multi-Lingual Digital Library for West African Sources&quot;. It seeks to build a multi-media digital library of West African sources in multiple languages. Collaborating &lt;br/&gt;organizations include several at Michigan State, the Institut Fondemental d&apos;Afrique Noire (IFAN) and West African Research Center (WARC) in Dakar, Senegal. Four types of material will be archived &lt;br/&gt;including historical manuscripts from the 19th and early 20th centuries (primarily in Arabic and French), oral sources (primarily in Wolof, Pulaar, and French), photographs and African language print &lt;br/&gt;materials. The project will give careful attention to identifying and providing metadata that describe the provenance and content of the digital materials. A multiple-languages interface will allow &lt;br/&gt;users to gain access to content through the use of standardized subject headings in the original languages. Caching proxy server technology will be used a local site servers in Senegal to address &lt;br/&gt;low-speed international network connections common in African countries. It is hoped that this project will serve as a model for larger-scale efforts by African and US researchers to create new repositories for the broader research and education communities.</data>
      <data key="e_pgm">5976</data>
      <data key="e_label">9.90583e+06</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">9.90583e+06</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n527" target="n529">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Multi-Lingual Digital Library for West African Sources</data>
      <data key="e_abstract">This project is entitled &quot;Multi-Lingual Digital Library for West African Sources&quot;. It seeks to build a multi-media digital library of West African sources in multiple languages. Collaborating &lt;br/&gt;organizations include several at Michigan State, the Institut Fondemental d&apos;Afrique Noire (IFAN) and West African Research Center (WARC) in Dakar, Senegal. Four types of material will be archived &lt;br/&gt;including historical manuscripts from the 19th and early 20th centuries (primarily in Arabic and French), oral sources (primarily in Wolof, Pulaar, and French), photographs and African language print &lt;br/&gt;materials. The project will give careful attention to identifying and providing metadata that describe the provenance and content of the digital materials. A multiple-languages interface will allow &lt;br/&gt;users to gain access to content through the use of standardized subject headings in the original languages. Caching proxy server technology will be used a local site servers in Senegal to address &lt;br/&gt;low-speed international network connections common in African countries. It is hoped that this project will serve as a model for larger-scale efforts by African and US researchers to create new repositories for the broader research and education communities.</data>
      <data key="e_pgm">5976</data>
      <data key="e_label">9.90583e+06</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">9.90583e+06</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n528" target="n529">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Multi-Lingual Digital Library for West African Sources</data>
      <data key="e_abstract">This project is entitled &quot;Multi-Lingual Digital Library for West African Sources&quot;. It seeks to build a multi-media digital library of West African sources in multiple languages. Collaborating &lt;br/&gt;organizations include several at Michigan State, the Institut Fondemental d&apos;Afrique Noire (IFAN) and West African Research Center (WARC) in Dakar, Senegal. Four types of material will be archived &lt;br/&gt;including historical manuscripts from the 19th and early 20th centuries (primarily in Arabic and French), oral sources (primarily in Wolof, Pulaar, and French), photographs and African language print &lt;br/&gt;materials. The project will give careful attention to identifying and providing metadata that describe the provenance and content of the digital materials. A multiple-languages interface will allow &lt;br/&gt;users to gain access to content through the use of standardized subject headings in the original languages. Caching proxy server technology will be used a local site servers in Senegal to address &lt;br/&gt;low-speed international network connections common in African countries. It is hoped that this project will serve as a model for larger-scale efforts by African and US researchers to create new repositories for the broader research and education communities.</data>
      <data key="e_pgm">5976</data>
      <data key="e_label">9.90583e+06</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">9.90583e+06</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n531" target="n532">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Modeling Run-Time Reconfiguration</data>
      <data key="e_abstract">Run-time reconfiguration (RTR) is a method of computing in which the hardware, usually field-programmable gate arrays (FPGAs), changes structure from one phase to the next of a computation. It has applications in image processing, signal processing, encryption, networking, and other areas. The research develops the necessary foundations to open up RTR to wider usage.&lt;br/&gt;&lt;br/&gt;The research addresses directions: (1) Development of a layered model of FPGAs suitable for broad exploration of RTR solution approaches, and porting of reconfiguration techniques developed by the investigators and others in the study of reconfigurable meshes to the RTR arena. (2) Design of fault tolerant RTR solutions in both an algorithm-specific context and in a general computational context. (3) Development of ``hardware operating system&apos;&apos; support for users to distance themselves from FPGA details.&lt;br/&gt;&lt;br/&gt;The research leads to benefits of less hardware, because of reuse of reconfigurable hardware, and faster computation, because of specialization to input. It develops a fundamental understanding of RTR, its power and&lt;br/&gt;limitations, and to establish a framework for using it efficiently.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">73429</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">73429</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n531" target="n533">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Modeling Run-Time Reconfiguration</data>
      <data key="e_abstract">Run-time reconfiguration (RTR) is a method of computing in which the hardware, usually field-programmable gate arrays (FPGAs), changes structure from one phase to the next of a computation. It has applications in image processing, signal processing, encryption, networking, and other areas. The research develops the necessary foundations to open up RTR to wider usage.&lt;br/&gt;&lt;br/&gt;The research addresses directions: (1) Development of a layered model of FPGAs suitable for broad exploration of RTR solution approaches, and porting of reconfiguration techniques developed by the investigators and others in the study of reconfigurable meshes to the RTR arena. (2) Design of fault tolerant RTR solutions in both an algorithm-specific context and in a general computational context. (3) Development of ``hardware operating system&apos;&apos; support for users to distance themselves from FPGA details.&lt;br/&gt;&lt;br/&gt;The research leads to benefits of less hardware, because of reuse of reconfigurable hardware, and faster computation, because of specialization to input. It develops a fundamental understanding of RTR, its power and&lt;br/&gt;limitations, and to establish a framework for using it efficiently.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">73429</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">73429</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n532" target="n533">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Modeling Run-Time Reconfiguration</data>
      <data key="e_abstract">Run-time reconfiguration (RTR) is a method of computing in which the hardware, usually field-programmable gate arrays (FPGAs), changes structure from one phase to the next of a computation. It has applications in image processing, signal processing, encryption, networking, and other areas. The research develops the necessary foundations to open up RTR to wider usage.&lt;br/&gt;&lt;br/&gt;The research addresses directions: (1) Development of a layered model of FPGAs suitable for broad exploration of RTR solution approaches, and porting of reconfiguration techniques developed by the investigators and others in the study of reconfigurable meshes to the RTR arena. (2) Design of fault tolerant RTR solutions in both an algorithm-specific context and in a general computational context. (3) Development of ``hardware operating system&apos;&apos; support for users to distance themselves from FPGA details.&lt;br/&gt;&lt;br/&gt;The research leads to benefits of less hardware, because of reuse of reconfigurable hardware, and faster computation, because of specialization to input. It develops a fundamental understanding of RTR, its power and&lt;br/&gt;limitations, and to establish a framework for using it efficiently.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">73429</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">73429</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n534" target="n535">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">MRI: Acquisition of a Computational Mini-Grid Supercomputing Facility</data>
      <data key="e_abstract">EIA-0079734&lt;br/&gt;Ligon, Walter B.&lt;br/&gt;Clemson University&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Computational Mini-Grid Supercomputing Facility&lt;br/&gt;&lt;br/&gt;The objective of this CISE/MRI proposal is to acquire computational equipment to support research in genomics, polymeric fibers and films, and high-performance computing systems. This enabling technology will be used to support programs in the Clemson University Genomics Institute, the Center for Advanced Engineering Fibers and Films, and the Parallel Architecture Research Laboratory. The requested equipment will help these three groups meet their current goals, increase their computational capacity, and permit expansion into new research areas. It will foster collaboration, interdisciplinary work, and integration of research and education.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79734</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79734</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n125" target="n534">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">MRI: Acquisition of a Computational Mini-Grid Supercomputing Facility</data>
      <data key="e_abstract">EIA-0079734&lt;br/&gt;Ligon, Walter B.&lt;br/&gt;Clemson University&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Computational Mini-Grid Supercomputing Facility&lt;br/&gt;&lt;br/&gt;The objective of this CISE/MRI proposal is to acquire computational equipment to support research in genomics, polymeric fibers and films, and high-performance computing systems. This enabling technology will be used to support programs in the Clemson University Genomics Institute, the Center for Advanced Engineering Fibers and Films, and the Parallel Architecture Research Laboratory. The requested equipment will help these three groups meet their current goals, increase their computational capacity, and permit expansion into new research areas. It will foster collaboration, interdisciplinary work, and integration of research and education.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79734</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79734</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n534" target="n537">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">MRI: Acquisition of a Computational Mini-Grid Supercomputing Facility</data>
      <data key="e_abstract">EIA-0079734&lt;br/&gt;Ligon, Walter B.&lt;br/&gt;Clemson University&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Computational Mini-Grid Supercomputing Facility&lt;br/&gt;&lt;br/&gt;The objective of this CISE/MRI proposal is to acquire computational equipment to support research in genomics, polymeric fibers and films, and high-performance computing systems. This enabling technology will be used to support programs in the Clemson University Genomics Institute, the Center for Advanced Engineering Fibers and Films, and the Parallel Architecture Research Laboratory. The requested equipment will help these three groups meet their current goals, increase their computational capacity, and permit expansion into new research areas. It will foster collaboration, interdisciplinary work, and integration of research and education.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79734</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79734</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n534" target="n538">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">MRI: Acquisition of a Computational Mini-Grid Supercomputing Facility</data>
      <data key="e_abstract">EIA-0079734&lt;br/&gt;Ligon, Walter B.&lt;br/&gt;Clemson University&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Computational Mini-Grid Supercomputing Facility&lt;br/&gt;&lt;br/&gt;The objective of this CISE/MRI proposal is to acquire computational equipment to support research in genomics, polymeric fibers and films, and high-performance computing systems. This enabling technology will be used to support programs in the Clemson University Genomics Institute, the Center for Advanced Engineering Fibers and Films, and the Parallel Architecture Research Laboratory. The requested equipment will help these three groups meet their current goals, increase their computational capacity, and permit expansion into new research areas. It will foster collaboration, interdisciplinary work, and integration of research and education.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79734</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79734</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n125" target="n535">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">MRI: Acquisition of a Computational Mini-Grid Supercomputing Facility</data>
      <data key="e_abstract">EIA-0079734&lt;br/&gt;Ligon, Walter B.&lt;br/&gt;Clemson University&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Computational Mini-Grid Supercomputing Facility&lt;br/&gt;&lt;br/&gt;The objective of this CISE/MRI proposal is to acquire computational equipment to support research in genomics, polymeric fibers and films, and high-performance computing systems. This enabling technology will be used to support programs in the Clemson University Genomics Institute, the Center for Advanced Engineering Fibers and Films, and the Parallel Architecture Research Laboratory. The requested equipment will help these three groups meet their current goals, increase their computational capacity, and permit expansion into new research areas. It will foster collaboration, interdisciplinary work, and integration of research and education.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79734</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79734</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n535" target="n537">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">MRI: Acquisition of a Computational Mini-Grid Supercomputing Facility</data>
      <data key="e_abstract">EIA-0079734&lt;br/&gt;Ligon, Walter B.&lt;br/&gt;Clemson University&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Computational Mini-Grid Supercomputing Facility&lt;br/&gt;&lt;br/&gt;The objective of this CISE/MRI proposal is to acquire computational equipment to support research in genomics, polymeric fibers and films, and high-performance computing systems. This enabling technology will be used to support programs in the Clemson University Genomics Institute, the Center for Advanced Engineering Fibers and Films, and the Parallel Architecture Research Laboratory. The requested equipment will help these three groups meet their current goals, increase their computational capacity, and permit expansion into new research areas. It will foster collaboration, interdisciplinary work, and integration of research and education.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79734</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79734</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n535" target="n538">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">MRI: Acquisition of a Computational Mini-Grid Supercomputing Facility</data>
      <data key="e_abstract">EIA-0079734&lt;br/&gt;Ligon, Walter B.&lt;br/&gt;Clemson University&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Computational Mini-Grid Supercomputing Facility&lt;br/&gt;&lt;br/&gt;The objective of this CISE/MRI proposal is to acquire computational equipment to support research in genomics, polymeric fibers and films, and high-performance computing systems. This enabling technology will be used to support programs in the Clemson University Genomics Institute, the Center for Advanced Engineering Fibers and Films, and the Parallel Architecture Research Laboratory. The requested equipment will help these three groups meet their current goals, increase their computational capacity, and permit expansion into new research areas. It will foster collaboration, interdisciplinary work, and integration of research and education.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79734</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79734</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n125" target="n537">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">MRI: Acquisition of a Computational Mini-Grid Supercomputing Facility</data>
      <data key="e_abstract">EIA-0079734&lt;br/&gt;Ligon, Walter B.&lt;br/&gt;Clemson University&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Computational Mini-Grid Supercomputing Facility&lt;br/&gt;&lt;br/&gt;The objective of this CISE/MRI proposal is to acquire computational equipment to support research in genomics, polymeric fibers and films, and high-performance computing systems. This enabling technology will be used to support programs in the Clemson University Genomics Institute, the Center for Advanced Engineering Fibers and Films, and the Parallel Architecture Research Laboratory. The requested equipment will help these three groups meet their current goals, increase their computational capacity, and permit expansion into new research areas. It will foster collaboration, interdisciplinary work, and integration of research and education.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79734</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79734</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n125" target="n538">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">MRI: Acquisition of a Computational Mini-Grid Supercomputing Facility</data>
      <data key="e_abstract">EIA-0079734&lt;br/&gt;Ligon, Walter B.&lt;br/&gt;Clemson University&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Computational Mini-Grid Supercomputing Facility&lt;br/&gt;&lt;br/&gt;The objective of this CISE/MRI proposal is to acquire computational equipment to support research in genomics, polymeric fibers and films, and high-performance computing systems. This enabling technology will be used to support programs in the Clemson University Genomics Institute, the Center for Advanced Engineering Fibers and Films, and the Parallel Architecture Research Laboratory. The requested equipment will help these three groups meet their current goals, increase their computational capacity, and permit expansion into new research areas. It will foster collaboration, interdisciplinary work, and integration of research and education.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79734</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79734</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n537" target="n538">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">MRI: Acquisition of a Computational Mini-Grid Supercomputing Facility</data>
      <data key="e_abstract">EIA-0079734&lt;br/&gt;Ligon, Walter B.&lt;br/&gt;Clemson University&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Computational Mini-Grid Supercomputing Facility&lt;br/&gt;&lt;br/&gt;The objective of this CISE/MRI proposal is to acquire computational equipment to support research in genomics, polymeric fibers and films, and high-performance computing systems. This enabling technology will be used to support programs in the Clemson University Genomics Institute, the Center for Advanced Engineering Fibers and Films, and the Parallel Architecture Research Laboratory. The requested equipment will help these three groups meet their current goals, increase their computational capacity, and permit expansion into new research areas. It will foster collaboration, interdisciplinary work, and integration of research and education.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79734</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79734</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n541" target="n542">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Collaborative Simulation for Education and Research</data>
      <data key="e_abstract">This USC-ISI award supports work on a Collaborative Simulation for Education and Research (CONSER) project. The CONSER system involves development of infrastructure that addresses two purposes: (a) support for research activities in the development and evaluation of networking protocols, and (b) teaching in the areas of networking protocols and networking concepts. Based on a collaborative simulation environment, CONSER is intended to aid understanding of protocol behavior in complex operational environments, over a range of traffic conditions, and on a variety of media. The system will involve simulation and visualization support intended to aid researcher and student understanding, thus &lt;br/&gt;providing benefits in both research and teaching settings. Based on ns and nam, the work is expected to be well received by the research community and industry.</data>
      <data key="e_pgm">V979</data>
      <data key="e_label">9.98621e+06</data>
      <data key="e_expirationDate">2006-10-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98621e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n541" target="n543">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Collaborative Simulation for Education and Research</data>
      <data key="e_abstract">This USC-ISI award supports work on a Collaborative Simulation for Education and Research (CONSER) project. The CONSER system involves development of infrastructure that addresses two purposes: (a) support for research activities in the development and evaluation of networking protocols, and (b) teaching in the areas of networking protocols and networking concepts. Based on a collaborative simulation environment, CONSER is intended to aid understanding of protocol behavior in complex operational environments, over a range of traffic conditions, and on a variety of media. The system will involve simulation and visualization support intended to aid researcher and student understanding, thus &lt;br/&gt;providing benefits in both research and teaching settings. Based on ns and nam, the work is expected to be well received by the research community and industry.</data>
      <data key="e_pgm">V979</data>
      <data key="e_label">9.98621e+06</data>
      <data key="e_expirationDate">2006-10-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98621e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n542" target="n543">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Collaborative Simulation for Education and Research</data>
      <data key="e_abstract">This USC-ISI award supports work on a Collaborative Simulation for Education and Research (CONSER) project. The CONSER system involves development of infrastructure that addresses two purposes: (a) support for research activities in the development and evaluation of networking protocols, and (b) teaching in the areas of networking protocols and networking concepts. Based on a collaborative simulation environment, CONSER is intended to aid understanding of protocol behavior in complex operational environments, over a range of traffic conditions, and on a variety of media. The system will involve simulation and visualization support intended to aid researcher and student understanding, thus &lt;br/&gt;providing benefits in both research and teaching settings. Based on ns and nam, the work is expected to be well received by the research community and industry.</data>
      <data key="e_pgm">V979</data>
      <data key="e_label">9.98621e+06</data>
      <data key="e_expirationDate">2006-10-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98621e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n547" target="n548">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">MRI: Development of a Comprehensive Real-Time Instrument for MIMO Wireless Channel Measurement and Space-Time Coding Implementation</data>
      <data key="e_abstract">EIA-0079799&lt;br/&gt;Swindlehurst, Arnold&lt;br/&gt;Brigham Young University&lt;br/&gt;&lt;br/&gt;MRI: Development of a Comprehensive Real-Time Instrument for MIMO Wireless Channel Measurement and Space-Time Coding Implementation&lt;br/&gt;&lt;br/&gt;The primary goal of this proposal is the design, construction, and testing of a general purpose wireless communications platform that can be used for multiple transmit and receive antennas channel measurement and space-time coding algorithm assessment. The proposed instrumentation will be extremely flexible, allowing for variable array geometry, antenna spacing, bandwidth (up to 20MHz), carrier frequency (500MHz-20GHz), modulation format, polarization, and path distance. Two system configurations will be constructed, a tethered system for short range (&lt;300m) indoor experiments, and another that permits longer range (300-5000m) outdoor experiments using GPS receivers for carrier phase lock.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79799</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79799</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n547" target="n549">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">MRI: Development of a Comprehensive Real-Time Instrument for MIMO Wireless Channel Measurement and Space-Time Coding Implementation</data>
      <data key="e_abstract">EIA-0079799&lt;br/&gt;Swindlehurst, Arnold&lt;br/&gt;Brigham Young University&lt;br/&gt;&lt;br/&gt;MRI: Development of a Comprehensive Real-Time Instrument for MIMO Wireless Channel Measurement and Space-Time Coding Implementation&lt;br/&gt;&lt;br/&gt;The primary goal of this proposal is the design, construction, and testing of a general purpose wireless communications platform that can be used for multiple transmit and receive antennas channel measurement and space-time coding algorithm assessment. The proposed instrumentation will be extremely flexible, allowing for variable array geometry, antenna spacing, bandwidth (up to 20MHz), carrier frequency (500MHz-20GHz), modulation format, polarization, and path distance. Two system configurations will be constructed, a tethered system for short range (&lt;300m) indoor experiments, and another that permits longer range (300-5000m) outdoor experiments using GPS receivers for carrier phase lock.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79799</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79799</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n547" target="n550">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">MRI: Development of a Comprehensive Real-Time Instrument for MIMO Wireless Channel Measurement and Space-Time Coding Implementation</data>
      <data key="e_abstract">EIA-0079799&lt;br/&gt;Swindlehurst, Arnold&lt;br/&gt;Brigham Young University&lt;br/&gt;&lt;br/&gt;MRI: Development of a Comprehensive Real-Time Instrument for MIMO Wireless Channel Measurement and Space-Time Coding Implementation&lt;br/&gt;&lt;br/&gt;The primary goal of this proposal is the design, construction, and testing of a general purpose wireless communications platform that can be used for multiple transmit and receive antennas channel measurement and space-time coding algorithm assessment. The proposed instrumentation will be extremely flexible, allowing for variable array geometry, antenna spacing, bandwidth (up to 20MHz), carrier frequency (500MHz-20GHz), modulation format, polarization, and path distance. Two system configurations will be constructed, a tethered system for short range (&lt;300m) indoor experiments, and another that permits longer range (300-5000m) outdoor experiments using GPS receivers for carrier phase lock.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79799</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79799</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n548" target="n549">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">MRI: Development of a Comprehensive Real-Time Instrument for MIMO Wireless Channel Measurement and Space-Time Coding Implementation</data>
      <data key="e_abstract">EIA-0079799&lt;br/&gt;Swindlehurst, Arnold&lt;br/&gt;Brigham Young University&lt;br/&gt;&lt;br/&gt;MRI: Development of a Comprehensive Real-Time Instrument for MIMO Wireless Channel Measurement and Space-Time Coding Implementation&lt;br/&gt;&lt;br/&gt;The primary goal of this proposal is the design, construction, and testing of a general purpose wireless communications platform that can be used for multiple transmit and receive antennas channel measurement and space-time coding algorithm assessment. The proposed instrumentation will be extremely flexible, allowing for variable array geometry, antenna spacing, bandwidth (up to 20MHz), carrier frequency (500MHz-20GHz), modulation format, polarization, and path distance. Two system configurations will be constructed, a tethered system for short range (&lt;300m) indoor experiments, and another that permits longer range (300-5000m) outdoor experiments using GPS receivers for carrier phase lock.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79799</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79799</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n548" target="n550">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">MRI: Development of a Comprehensive Real-Time Instrument for MIMO Wireless Channel Measurement and Space-Time Coding Implementation</data>
      <data key="e_abstract">EIA-0079799&lt;br/&gt;Swindlehurst, Arnold&lt;br/&gt;Brigham Young University&lt;br/&gt;&lt;br/&gt;MRI: Development of a Comprehensive Real-Time Instrument for MIMO Wireless Channel Measurement and Space-Time Coding Implementation&lt;br/&gt;&lt;br/&gt;The primary goal of this proposal is the design, construction, and testing of a general purpose wireless communications platform that can be used for multiple transmit and receive antennas channel measurement and space-time coding algorithm assessment. The proposed instrumentation will be extremely flexible, allowing for variable array geometry, antenna spacing, bandwidth (up to 20MHz), carrier frequency (500MHz-20GHz), modulation format, polarization, and path distance. Two system configurations will be constructed, a tethered system for short range (&lt;300m) indoor experiments, and another that permits longer range (300-5000m) outdoor experiments using GPS receivers for carrier phase lock.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79799</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79799</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n549" target="n550">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">MRI: Development of a Comprehensive Real-Time Instrument for MIMO Wireless Channel Measurement and Space-Time Coding Implementation</data>
      <data key="e_abstract">EIA-0079799&lt;br/&gt;Swindlehurst, Arnold&lt;br/&gt;Brigham Young University&lt;br/&gt;&lt;br/&gt;MRI: Development of a Comprehensive Real-Time Instrument for MIMO Wireless Channel Measurement and Space-Time Coding Implementation&lt;br/&gt;&lt;br/&gt;The primary goal of this proposal is the design, construction, and testing of a general purpose wireless communications platform that can be used for multiple transmit and receive antennas channel measurement and space-time coding algorithm assessment. The proposed instrumentation will be extremely flexible, allowing for variable array geometry, antenna spacing, bandwidth (up to 20MHz), carrier frequency (500MHz-20GHz), modulation format, polarization, and path distance. Two system configurations will be constructed, a tethered system for short range (&lt;300m) indoor experiments, and another that permits longer range (300-5000m) outdoor experiments using GPS receivers for carrier phase lock.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79799</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79799</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n551" target="n552">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">MRI: LIVE: Laboratory for Immersive Virtual Experiments</data>
      <data key="e_abstract">EIA-0079800&lt;br/&gt;Huang, Thomas S.&lt;br/&gt;University of Illinois Urbana-Champaign&lt;br/&gt;&lt;br/&gt;MRI: LIVE: Laboratory for Immersive Virtual Experiments&lt;br/&gt;&lt;br/&gt;This proposal establishes the Laboratory for Immersive Virtual Environments (LIVE) which will provide a testbed for engineering and computer science research on distributed graphics, sensor integration, audio display, and human face, speech, and gesture recognition. Psychologists will use LIVE to determine how visual perception drives information extraction, navigation, and human movement. Scientists will use LIVE to explore and analyze their three dimensional datasets. Twenty-five faculty members from nine departments will use LIVE to build and understand virtual worlds and human interfaces. The heart of LIVE will be a fully enclosed, 6-sided, 10&apos;x10&apos;x10&apos;, backprojected virtual reality environment to be housed at the Beckman Institute and the University of Illinois.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79800</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79800</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n551" target="n553">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">MRI: LIVE: Laboratory for Immersive Virtual Experiments</data>
      <data key="e_abstract">EIA-0079800&lt;br/&gt;Huang, Thomas S.&lt;br/&gt;University of Illinois Urbana-Champaign&lt;br/&gt;&lt;br/&gt;MRI: LIVE: Laboratory for Immersive Virtual Experiments&lt;br/&gt;&lt;br/&gt;This proposal establishes the Laboratory for Immersive Virtual Environments (LIVE) which will provide a testbed for engineering and computer science research on distributed graphics, sensor integration, audio display, and human face, speech, and gesture recognition. Psychologists will use LIVE to determine how visual perception drives information extraction, navigation, and human movement. Scientists will use LIVE to explore and analyze their three dimensional datasets. Twenty-five faculty members from nine departments will use LIVE to build and understand virtual worlds and human interfaces. The heart of LIVE will be a fully enclosed, 6-sided, 10&apos;x10&apos;x10&apos;, backprojected virtual reality environment to be housed at the Beckman Institute and the University of Illinois.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79800</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79800</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n552" target="n553">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">MRI: LIVE: Laboratory for Immersive Virtual Experiments</data>
      <data key="e_abstract">EIA-0079800&lt;br/&gt;Huang, Thomas S.&lt;br/&gt;University of Illinois Urbana-Champaign&lt;br/&gt;&lt;br/&gt;MRI: LIVE: Laboratory for Immersive Virtual Experiments&lt;br/&gt;&lt;br/&gt;This proposal establishes the Laboratory for Immersive Virtual Environments (LIVE) which will provide a testbed for engineering and computer science research on distributed graphics, sensor integration, audio display, and human face, speech, and gesture recognition. Psychologists will use LIVE to determine how visual perception drives information extraction, navigation, and human movement. Scientists will use LIVE to explore and analyze their three dimensional datasets. Twenty-five faculty members from nine departments will use LIVE to build and understand virtual worlds and human interfaces. The heart of LIVE will be a fully enclosed, 6-sided, 10&apos;x10&apos;x10&apos;, backprojected virtual reality environment to be housed at the Beckman Institute and the University of Illinois.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79800</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79800</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n558" target="n559">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">ITR: Intelligent Memory Architectures and Algorithms to Crack the Protein Folding Problem</data>
      <data key="e_abstract">EIA-0081307&lt;br/&gt;Torrellas, Josep&lt;br/&gt;University of Illinois&lt;br/&gt;&lt;br/&gt;ITR: Intelligent Memory Architectures and Algorithms to Crack the Protein Folding Problem&lt;br/&gt;&lt;br/&gt;This project is a multidisciplinary effort to design fundamentally improved algorithms, hardware, and software to solve the protein folding problem. The project, which teams experts in hardware, software, and computational biology, promises advances in applications of protein folding such as drug design and understanding of diseases. In addition, the project will investigate new hardware and software based on advancing IC technology. The architecture under examination will use increased integration of processors and memory in a single chip, and software will take advantage of the proximity of memory and processing. This work is tightly coupled to the IBM Blue Gene effort, but will investigate complementary issues. In particular, simulation-based studies will investigate the use of next-generation intelligent architectures.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81307</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81307</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n558" target="n560">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">ITR: Intelligent Memory Architectures and Algorithms to Crack the Protein Folding Problem</data>
      <data key="e_abstract">EIA-0081307&lt;br/&gt;Torrellas, Josep&lt;br/&gt;University of Illinois&lt;br/&gt;&lt;br/&gt;ITR: Intelligent Memory Architectures and Algorithms to Crack the Protein Folding Problem&lt;br/&gt;&lt;br/&gt;This project is a multidisciplinary effort to design fundamentally improved algorithms, hardware, and software to solve the protein folding problem. The project, which teams experts in hardware, software, and computational biology, promises advances in applications of protein folding such as drug design and understanding of diseases. In addition, the project will investigate new hardware and software based on advancing IC technology. The architecture under examination will use increased integration of processors and memory in a single chip, and software will take advantage of the proximity of memory and processing. This work is tightly coupled to the IBM Blue Gene effort, but will investigate complementary issues. In particular, simulation-based studies will investigate the use of next-generation intelligent architectures.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81307</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81307</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n558" target="n561">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">ITR: Intelligent Memory Architectures and Algorithms to Crack the Protein Folding Problem</data>
      <data key="e_abstract">EIA-0081307&lt;br/&gt;Torrellas, Josep&lt;br/&gt;University of Illinois&lt;br/&gt;&lt;br/&gt;ITR: Intelligent Memory Architectures and Algorithms to Crack the Protein Folding Problem&lt;br/&gt;&lt;br/&gt;This project is a multidisciplinary effort to design fundamentally improved algorithms, hardware, and software to solve the protein folding problem. The project, which teams experts in hardware, software, and computational biology, promises advances in applications of protein folding such as drug design and understanding of diseases. In addition, the project will investigate new hardware and software based on advancing IC technology. The architecture under examination will use increased integration of processors and memory in a single chip, and software will take advantage of the proximity of memory and processing. This work is tightly coupled to the IBM Blue Gene effort, but will investigate complementary issues. In particular, simulation-based studies will investigate the use of next-generation intelligent architectures.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81307</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81307</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n558" target="n562">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">ITR: Intelligent Memory Architectures and Algorithms to Crack the Protein Folding Problem</data>
      <data key="e_abstract">EIA-0081307&lt;br/&gt;Torrellas, Josep&lt;br/&gt;University of Illinois&lt;br/&gt;&lt;br/&gt;ITR: Intelligent Memory Architectures and Algorithms to Crack the Protein Folding Problem&lt;br/&gt;&lt;br/&gt;This project is a multidisciplinary effort to design fundamentally improved algorithms, hardware, and software to solve the protein folding problem. The project, which teams experts in hardware, software, and computational biology, promises advances in applications of protein folding such as drug design and understanding of diseases. In addition, the project will investigate new hardware and software based on advancing IC technology. The architecture under examination will use increased integration of processors and memory in a single chip, and software will take advantage of the proximity of memory and processing. This work is tightly coupled to the IBM Blue Gene effort, but will investigate complementary issues. In particular, simulation-based studies will investigate the use of next-generation intelligent architectures.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81307</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81307</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n559" target="n560">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">ITR: Intelligent Memory Architectures and Algorithms to Crack the Protein Folding Problem</data>
      <data key="e_abstract">EIA-0081307&lt;br/&gt;Torrellas, Josep&lt;br/&gt;University of Illinois&lt;br/&gt;&lt;br/&gt;ITR: Intelligent Memory Architectures and Algorithms to Crack the Protein Folding Problem&lt;br/&gt;&lt;br/&gt;This project is a multidisciplinary effort to design fundamentally improved algorithms, hardware, and software to solve the protein folding problem. The project, which teams experts in hardware, software, and computational biology, promises advances in applications of protein folding such as drug design and understanding of diseases. In addition, the project will investigate new hardware and software based on advancing IC technology. The architecture under examination will use increased integration of processors and memory in a single chip, and software will take advantage of the proximity of memory and processing. This work is tightly coupled to the IBM Blue Gene effort, but will investigate complementary issues. In particular, simulation-based studies will investigate the use of next-generation intelligent architectures.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81307</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81307</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n559" target="n561">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">ITR: Intelligent Memory Architectures and Algorithms to Crack the Protein Folding Problem</data>
      <data key="e_abstract">EIA-0081307&lt;br/&gt;Torrellas, Josep&lt;br/&gt;University of Illinois&lt;br/&gt;&lt;br/&gt;ITR: Intelligent Memory Architectures and Algorithms to Crack the Protein Folding Problem&lt;br/&gt;&lt;br/&gt;This project is a multidisciplinary effort to design fundamentally improved algorithms, hardware, and software to solve the protein folding problem. The project, which teams experts in hardware, software, and computational biology, promises advances in applications of protein folding such as drug design and understanding of diseases. In addition, the project will investigate new hardware and software based on advancing IC technology. The architecture under examination will use increased integration of processors and memory in a single chip, and software will take advantage of the proximity of memory and processing. This work is tightly coupled to the IBM Blue Gene effort, but will investigate complementary issues. In particular, simulation-based studies will investigate the use of next-generation intelligent architectures.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81307</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81307</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n559" target="n562">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">ITR: Intelligent Memory Architectures and Algorithms to Crack the Protein Folding Problem</data>
      <data key="e_abstract">EIA-0081307&lt;br/&gt;Torrellas, Josep&lt;br/&gt;University of Illinois&lt;br/&gt;&lt;br/&gt;ITR: Intelligent Memory Architectures and Algorithms to Crack the Protein Folding Problem&lt;br/&gt;&lt;br/&gt;This project is a multidisciplinary effort to design fundamentally improved algorithms, hardware, and software to solve the protein folding problem. The project, which teams experts in hardware, software, and computational biology, promises advances in applications of protein folding such as drug design and understanding of diseases. In addition, the project will investigate new hardware and software based on advancing IC technology. The architecture under examination will use increased integration of processors and memory in a single chip, and software will take advantage of the proximity of memory and processing. This work is tightly coupled to the IBM Blue Gene effort, but will investigate complementary issues. In particular, simulation-based studies will investigate the use of next-generation intelligent architectures.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81307</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81307</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n560" target="n561">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">ITR: Intelligent Memory Architectures and Algorithms to Crack the Protein Folding Problem</data>
      <data key="e_abstract">EIA-0081307&lt;br/&gt;Torrellas, Josep&lt;br/&gt;University of Illinois&lt;br/&gt;&lt;br/&gt;ITR: Intelligent Memory Architectures and Algorithms to Crack the Protein Folding Problem&lt;br/&gt;&lt;br/&gt;This project is a multidisciplinary effort to design fundamentally improved algorithms, hardware, and software to solve the protein folding problem. The project, which teams experts in hardware, software, and computational biology, promises advances in applications of protein folding such as drug design and understanding of diseases. In addition, the project will investigate new hardware and software based on advancing IC technology. The architecture under examination will use increased integration of processors and memory in a single chip, and software will take advantage of the proximity of memory and processing. This work is tightly coupled to the IBM Blue Gene effort, but will investigate complementary issues. In particular, simulation-based studies will investigate the use of next-generation intelligent architectures.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81307</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81307</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n560" target="n562">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">ITR: Intelligent Memory Architectures and Algorithms to Crack the Protein Folding Problem</data>
      <data key="e_abstract">EIA-0081307&lt;br/&gt;Torrellas, Josep&lt;br/&gt;University of Illinois&lt;br/&gt;&lt;br/&gt;ITR: Intelligent Memory Architectures and Algorithms to Crack the Protein Folding Problem&lt;br/&gt;&lt;br/&gt;This project is a multidisciplinary effort to design fundamentally improved algorithms, hardware, and software to solve the protein folding problem. The project, which teams experts in hardware, software, and computational biology, promises advances in applications of protein folding such as drug design and understanding of diseases. In addition, the project will investigate new hardware and software based on advancing IC technology. The architecture under examination will use increased integration of processors and memory in a single chip, and software will take advantage of the proximity of memory and processing. This work is tightly coupled to the IBM Blue Gene effort, but will investigate complementary issues. In particular, simulation-based studies will investigate the use of next-generation intelligent architectures.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81307</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81307</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n561" target="n562">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">ITR: Intelligent Memory Architectures and Algorithms to Crack the Protein Folding Problem</data>
      <data key="e_abstract">EIA-0081307&lt;br/&gt;Torrellas, Josep&lt;br/&gt;University of Illinois&lt;br/&gt;&lt;br/&gt;ITR: Intelligent Memory Architectures and Algorithms to Crack the Protein Folding Problem&lt;br/&gt;&lt;br/&gt;This project is a multidisciplinary effort to design fundamentally improved algorithms, hardware, and software to solve the protein folding problem. The project, which teams experts in hardware, software, and computational biology, promises advances in applications of protein folding such as drug design and understanding of diseases. In addition, the project will investigate new hardware and software based on advancing IC technology. The architecture under examination will use increased integration of processors and memory in a single chip, and software will take advantage of the proximity of memory and processing. This work is tightly coupled to the IBM Blue Gene effort, but will investigate complementary issues. In particular, simulation-based studies will investigate the use of next-generation intelligent architectures.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81307</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81307</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n567" target="n568">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">ITR: Quantum Information Processing with Ultracold Rydberg Atoms</data>
      <data key="e_abstract">EIA-0082913&lt;br/&gt;Cote, Robin&lt;br/&gt;University of Connecticut&lt;br/&gt;&lt;br/&gt;ITR: Quantum Information Processing with Ultracold Rydberg Atoms&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Ultracold Rydberg atoms have long lifetimes and interact strongly at large distances, which may make them ideal for quantum information processing. This project is investigating this possibility by studying and experimentally implementing a universal two-qubit logic gate, in which a two-atom state acquires a phase that depends on the time the atoms are interacting. The project is investigating sparsely populated optical lattices containing Rydberg atoms, which will permit the preparation of well-identified atoms in arbitrary states. The properties of atom-atom interactions, including the effect of external electric and magnetic fields will be investigated. Finally, methods of reading the information stored in Rydberg atoms will be investigated, including imaged fluorescence and selective ionization. Scaling to large numbers of qubits, as well as the effects of the optical lattice on decoherence, will be explored.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82913</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82913</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n567" target="n569">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">ITR: Quantum Information Processing with Ultracold Rydberg Atoms</data>
      <data key="e_abstract">EIA-0082913&lt;br/&gt;Cote, Robin&lt;br/&gt;University of Connecticut&lt;br/&gt;&lt;br/&gt;ITR: Quantum Information Processing with Ultracold Rydberg Atoms&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Ultracold Rydberg atoms have long lifetimes and interact strongly at large distances, which may make them ideal for quantum information processing. This project is investigating this possibility by studying and experimentally implementing a universal two-qubit logic gate, in which a two-atom state acquires a phase that depends on the time the atoms are interacting. The project is investigating sparsely populated optical lattices containing Rydberg atoms, which will permit the preparation of well-identified atoms in arbitrary states. The properties of atom-atom interactions, including the effect of external electric and magnetic fields will be investigated. Finally, methods of reading the information stored in Rydberg atoms will be investigated, including imaged fluorescence and selective ionization. Scaling to large numbers of qubits, as well as the effects of the optical lattice on decoherence, will be explored.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82913</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82913</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n568" target="n569">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">ITR: Quantum Information Processing with Ultracold Rydberg Atoms</data>
      <data key="e_abstract">EIA-0082913&lt;br/&gt;Cote, Robin&lt;br/&gt;University of Connecticut&lt;br/&gt;&lt;br/&gt;ITR: Quantum Information Processing with Ultracold Rydberg Atoms&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Ultracold Rydberg atoms have long lifetimes and interact strongly at large distances, which may make them ideal for quantum information processing. This project is investigating this possibility by studying and experimentally implementing a universal two-qubit logic gate, in which a two-atom state acquires a phase that depends on the time the atoms are interacting. The project is investigating sparsely populated optical lattices containing Rydberg atoms, which will permit the preparation of well-identified atoms in arbitrary states. The properties of atom-atom interactions, including the effect of external electric and magnetic fields will be investigated. Finally, methods of reading the information stored in Rydberg atoms will be investigated, including imaged fluorescence and selective ionization. Scaling to large numbers of qubits, as well as the effects of the optical lattice on decoherence, will be explored.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82913</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82913</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n572" target="n573">
      <data key="e_effectiveDate">2000-08-31</data>
      <data key="e_title">Lumigraphs and Manifolds</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">2865</data>
      <data key="e_label">196213</data>
      <data key="e_expirationDate">2002-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">196213</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n574" target="n575">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Collaborative Research: Biocomplexity of Aquatic Microbial Systems: Relating Diversity of Microorganisms to Ecosystem Function</data>
      <data key="e_abstract">BIOCOMPLEXITY: Collaborative Research: Biocomplexity of aquatic microbial systems -- relating diversity of microorganisms to ecosystem function&lt;br/&gt;&lt;br/&gt;Microbial biogeochemical cycling of the elements regulates a dynamic environment in which the cycles of different elements are linked through the physiology of microorganisms. While a certain degree of understanding can be gained through physical/chemical approaches to measurement and modeling of the net transformations, these approaches necessarily rely on gross simplifications about the role and regulation of the various functional groups (guilds) involved. Recent advances in molecular microbial ecology have shown the microbial world to contain immense diversity and complexity at every level: redundancy and duplication of functional genes within a single organism; molecular diversity among functional genes that encode the same process in different organisms; large genetic diversity among different organisms apparently engaged in the same biogeochemical function within single communities; great variability in the species composition of different communities that apparently perform equally well. The goal of this project is to investigate the functional relationship between complexity in microbial communities and the physical/chemical environment at a range of biological and ecological scales. Previously, such analysis was technologically limited by the inability to assay large numbers of samples simultaneously for a large number of genes and phylotypes. Using gene array technology, the researchers will be able to detect the distribution and differential expression of functional genes in natural systems. The results of this study will constitute the first step towards application of DNA chip technology for gene expression of &quot;exotic&quot; (i.e., not of biomedical importance) processes and organisms in the environment. The gene arrays, along with a full suite of ecosystem process measurements, will be deployed along a transect that spans the eutrophic - oligotrophic gradient from the inland waters of the Chesapeake Bay out to the Sargasso Sea. Experiments and functional gene studies will focus on key transformations in the carbon and nitrogen cycles (C fixation, N fixation, nitrification, denitrification, urea assimilation). The diversity of guilds will be interpreted in terms of ecosystem function, assessed using geochemical data and tracer experiments. In addition to field studies designed to investigate and dissect the natural system, the group of collaborating scientists will also perform perturbation experiments using mesocosms. The goal of these experiments is to determine how microbial species diversity affects the major energy and nutrient flows within ecosystems, and to assess the degree of stability or instability associated with changes in redundancy within guilds of microorganisms responsible for major nitrogen and carbon pathways.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">9.98148e+06</data>
      <data key="e_expirationDate">2007-07-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">9.98148e+06</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n580" target="n581">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Inductance Control for On-Chip Signal Integrity</data>
      <data key="e_abstract">On-chip inductance becomes an important design issue at transistor geometries of 0.25 mm and below. The primary goal of this project is to generate computer aided design tools and on-chip measurement techniques that will enable IC designers to cope with inductance related issues without the need for highly specialized knowledge or experimentation. The project has three thrusts. In thrust one, modeling and model reduction techniques will be developed that enable complex on-chip structures to be modeled and reduced so that they will be tractable for simulation. The techniques are based on rule-based extensions to PEEC and wavelet-based extraction. Thrust two is concerned with establishing an automated methodology to generate layout and performance margin guidance for use by designers. At the center of this effort is the identification of a set of generalized parameterized interconnect topologies that capture most of the important issues related to inductance control. Thrust three is concerned with measurement verification of the results of the other two thrusts and the determination of new automated in-situ measurement techniques.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">9.98833e+06</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98833e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n582" target="n583">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">A New Framework for Adaptive Subspace Filtering: Theory and Algorithms for Application to Wireless Communications</data>
      <data key="e_abstract">This proposal is a continuation of the research on Reduced-rank Adaptive Subspace Filtering for Spread Spectrum Communications, Array Signal Processing and Detection, a research grant funded by the National Science Foundation under Contract No. MIP-9706215. During the current grant, the theoretical foundations of the cross-spectral metric (CSM) for reduced-rank were studied. The practical processing algorithms to apply the CSM to several related research areas are developed. Our research shows that the CSM method results in a better rank reduction than the principal components method in the sense of the minimum mean square error for filtering. The remaining problem with the CSM method is that the eigenvectors of the data covariance matrix, that are used to span the full-rank space, are unknown and have to be estimated from the observation data in most applications. The expensive computation needed for obtaining these eigenvectors will hinder the use of the CSM method in real-time processing. In addition, the strategy used by the CSM method for searching a desired rank-reducing subspace is not efficient: one has to compute all of the eigenvectors and their corresponding eigenvalues in order to rank order the cross-spectral items, but only a certain number of them are needed for the subspace filtering. &lt;br/&gt;A new framework for reduced rank subspace filtering, built on a non-eigenvector-based subspace representation, is now proposed to address these issues. In this new framework, a set of orthonormal vectors, which tridiagonalizes, rather than diagonalizes, the covariance matrix, is used to replace a set of eigenvectors as a basis of the full-rank space. Advantages of this replacement are (1) the computation of the tridiagonalization has a much lower complexity as compared to that of the diagonalization; (2) the rank reduction from the full-rank N to the lower rank K only requires computing K or less desired orthonormal basis vectors, instead of computing all N of them, and (3) the resulting subspace remains optimal in the sense of maximum signal-to-interference plus noise ratio. The proposed effort for this non-eigenvector-based subspace filtering framework includes studying the theoretical foundations, developing adaptive processing algorithms and their computation architectures, deriving a rank reduction optimization metric, and evaluating theoretical and implementation performances as compared to the eigenvector based approaches. &lt;br/&gt;New results in rank reduced adaptive filtering will be directly applicable to several new research areas currently under consideration such as space and time wireless systems and code addressed multiple access signaling. In these applications, and similar extensions, low dimension addressing signals are superimposed in larger 2-dimensional(frequency/time or space/time) signal spaces. Signal crosstalk in the form of address overlap must be removed by processing over the entire 2-D observation space. The ability to rank reduce data for efficient crosstalk rejection will be a major step in the development of practical processing algorithms. In addition, overlap interference will be time varying due to continual data modulation (in the multiple accessing case) and due to the spatial fading (in the space/time case). Hence adaptive updating processors will be necessary for maximum efficiency. The research developed in the newer study proposed here will significantly influence practical filtering solutions for these two dimensional cases.</data>
      <data key="e_pgm">4720</data>
      <data key="e_label">73559</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">73559</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n585" target="n586">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Research on Interconnect-Dominated Floorplanning</data>
      <data key="e_abstract">The project researches interconnect-dominated floorplanning. The floorplanning organizes the topology of wires and the assignment of buffers while placement and routing are structured as supporting tools to fulfill the design requirements. The project includes exploration of the following. Technology Floorplanning: Given a circuit design project, the technology floorplanning identifies the technology that provides the best interconnect capability in terms of cost and performance. Logic Floorplanning with a Hierarchical Design Framework: A logic floorplanning takes advantage of the design hierarchy and derives the effect of interconnect delay. Physical Layout Floorplanning: Physical layout floorplanning serves as a foundation to support the interconnect optimization operations.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">9.98768e+06</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98768e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n587" target="n588">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Digital Government: Collaborative Research: Quality Graphics for Federal Statistics</data>
      <data key="e_abstract">EIA-9983451&lt;br/&gt;MacEachren, Alan M.&lt;br/&gt;Pennsylvania State University&lt;br/&gt;&lt;br/&gt;Digital Government: Collaborative Research: Quality Graphics for Federal Statistical Summaries&lt;br/&gt;&lt;br/&gt;The Federal government distributes a vast quantity of statistical summaries in printed and electronic form. The full wealth of information that might be derived from these summaries is not being realized because limited attention is paid to disseminating summaries in understandable forms. This research will develop and assess quality graphics for federal statistical summaries. The development and assessment process will consider perceptual and cognitive factors in reading, interacting with and interpreting statistical graphs, maps and metadata representatives. The purposes of the quality graphics include exploration by agency users evaluating data quality and looking for emergent trends, decision making by public policy makers and communication of statistical summaries to the public. The proposed research addresses four topic areas: converting tables to graphs, representing metadata, interacting with both graphs and maps, and conveying multivariate spatial and temporal relationships. The research features use of Web-based &quot;middleware&quot; components to provide rapid development of graphics for usability testing. The featured middleware is a Java Graphics Component Library (GPL). A long history of research has recently culminated in a rigorous graph algebra that is the foundation for the library. The library also benefits from the collective intellectual effort being poured into Java. Principles of human perception and cognition will be used to guide the construction of statistical graphics, and usability tests will help to refine those principles in the context of federal statistical summaries. The research team has been carefully selected to bring together outstanding and compatible investigators who will provide expertise in statistical graphics, cartography, human perception and cognition, and software development and is composed of university, industrial and federal agency personnel.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98345e+06</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98345e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n592" target="n593">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Center for Ultracold Atoms</data>
      <data key="e_abstract">This award provides infrastructure and research funding for the Center for Ultracold Atoms, a joint effort between the Massachusetts Institute of Technology and Harvard University designed to take advantage of the recent dramatic progress in atom optics on the one hand and in methods for creating coherent atoms on the other to explore opportunities for scientific advances and new technologies that would be difficult to implement within the context of individual investigators working in isolation. The goals of the Center include developing coherent atom sources of unprecedented intensity, devising techniques for manipulating the atoms for scientific and technological applications, developing methods for trapping and cooling new species of atoms, studying surface interactions with coherent atoms, loading atoms into surface waveguide structures, and developing methods for new types of spectroscopy. The Center will reach out to the scientific community with a substantial visitors program that will encourage senior and junior scientists to participate in the research and carry the Center&apos;s expertise elsewhere. A companion educational project is also being undertaken with the Museum of Science in Boston and Wellesley College. This project is jointly supported by the Atomic, Molecular, Optical, and Plasma Physics Program, the Condensed Matter Physics Program, and the Office of Multidisciplinary Activities in the Mathematical and Physical Sciences Directorate.</data>
      <data key="e_pgm">1248</data>
      <data key="e_label">71311</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">71311</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n592" target="n594">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Center for Ultracold Atoms</data>
      <data key="e_abstract">This award provides infrastructure and research funding for the Center for Ultracold Atoms, a joint effort between the Massachusetts Institute of Technology and Harvard University designed to take advantage of the recent dramatic progress in atom optics on the one hand and in methods for creating coherent atoms on the other to explore opportunities for scientific advances and new technologies that would be difficult to implement within the context of individual investigators working in isolation. The goals of the Center include developing coherent atom sources of unprecedented intensity, devising techniques for manipulating the atoms for scientific and technological applications, developing methods for trapping and cooling new species of atoms, studying surface interactions with coherent atoms, loading atoms into surface waveguide structures, and developing methods for new types of spectroscopy. The Center will reach out to the scientific community with a substantial visitors program that will encourage senior and junior scientists to participate in the research and carry the Center&apos;s expertise elsewhere. A companion educational project is also being undertaken with the Museum of Science in Boston and Wellesley College. This project is jointly supported by the Atomic, Molecular, Optical, and Plasma Physics Program, the Condensed Matter Physics Program, and the Office of Multidisciplinary Activities in the Mathematical and Physical Sciences Directorate.</data>
      <data key="e_pgm">1248</data>
      <data key="e_label">71311</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">71311</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n592" target="n595">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Center for Ultracold Atoms</data>
      <data key="e_abstract">This award provides infrastructure and research funding for the Center for Ultracold Atoms, a joint effort between the Massachusetts Institute of Technology and Harvard University designed to take advantage of the recent dramatic progress in atom optics on the one hand and in methods for creating coherent atoms on the other to explore opportunities for scientific advances and new technologies that would be difficult to implement within the context of individual investigators working in isolation. The goals of the Center include developing coherent atom sources of unprecedented intensity, devising techniques for manipulating the atoms for scientific and technological applications, developing methods for trapping and cooling new species of atoms, studying surface interactions with coherent atoms, loading atoms into surface waveguide structures, and developing methods for new types of spectroscopy. The Center will reach out to the scientific community with a substantial visitors program that will encourage senior and junior scientists to participate in the research and carry the Center&apos;s expertise elsewhere. A companion educational project is also being undertaken with the Museum of Science in Boston and Wellesley College. This project is jointly supported by the Atomic, Molecular, Optical, and Plasma Physics Program, the Condensed Matter Physics Program, and the Office of Multidisciplinary Activities in the Mathematical and Physical Sciences Directorate.</data>
      <data key="e_pgm">1248</data>
      <data key="e_label">71311</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">71311</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n593" target="n594">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Center for Ultracold Atoms</data>
      <data key="e_abstract">This award provides infrastructure and research funding for the Center for Ultracold Atoms, a joint effort between the Massachusetts Institute of Technology and Harvard University designed to take advantage of the recent dramatic progress in atom optics on the one hand and in methods for creating coherent atoms on the other to explore opportunities for scientific advances and new technologies that would be difficult to implement within the context of individual investigators working in isolation. The goals of the Center include developing coherent atom sources of unprecedented intensity, devising techniques for manipulating the atoms for scientific and technological applications, developing methods for trapping and cooling new species of atoms, studying surface interactions with coherent atoms, loading atoms into surface waveguide structures, and developing methods for new types of spectroscopy. The Center will reach out to the scientific community with a substantial visitors program that will encourage senior and junior scientists to participate in the research and carry the Center&apos;s expertise elsewhere. A companion educational project is also being undertaken with the Museum of Science in Boston and Wellesley College. This project is jointly supported by the Atomic, Molecular, Optical, and Plasma Physics Program, the Condensed Matter Physics Program, and the Office of Multidisciplinary Activities in the Mathematical and Physical Sciences Directorate.</data>
      <data key="e_pgm">1248</data>
      <data key="e_label">71311</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">71311</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n593" target="n595">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Center for Ultracold Atoms</data>
      <data key="e_abstract">This award provides infrastructure and research funding for the Center for Ultracold Atoms, a joint effort between the Massachusetts Institute of Technology and Harvard University designed to take advantage of the recent dramatic progress in atom optics on the one hand and in methods for creating coherent atoms on the other to explore opportunities for scientific advances and new technologies that would be difficult to implement within the context of individual investigators working in isolation. The goals of the Center include developing coherent atom sources of unprecedented intensity, devising techniques for manipulating the atoms for scientific and technological applications, developing methods for trapping and cooling new species of atoms, studying surface interactions with coherent atoms, loading atoms into surface waveguide structures, and developing methods for new types of spectroscopy. The Center will reach out to the scientific community with a substantial visitors program that will encourage senior and junior scientists to participate in the research and carry the Center&apos;s expertise elsewhere. A companion educational project is also being undertaken with the Museum of Science in Boston and Wellesley College. This project is jointly supported by the Atomic, Molecular, Optical, and Plasma Physics Program, the Condensed Matter Physics Program, and the Office of Multidisciplinary Activities in the Mathematical and Physical Sciences Directorate.</data>
      <data key="e_pgm">1248</data>
      <data key="e_label">71311</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">71311</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n594" target="n595">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Center for Ultracold Atoms</data>
      <data key="e_abstract">This award provides infrastructure and research funding for the Center for Ultracold Atoms, a joint effort between the Massachusetts Institute of Technology and Harvard University designed to take advantage of the recent dramatic progress in atom optics on the one hand and in methods for creating coherent atoms on the other to explore opportunities for scientific advances and new technologies that would be difficult to implement within the context of individual investigators working in isolation. The goals of the Center include developing coherent atom sources of unprecedented intensity, devising techniques for manipulating the atoms for scientific and technological applications, developing methods for trapping and cooling new species of atoms, studying surface interactions with coherent atoms, loading atoms into surface waveguide structures, and developing methods for new types of spectroscopy. The Center will reach out to the scientific community with a substantial visitors program that will encourage senior and junior scientists to participate in the research and carry the Center&apos;s expertise elsewhere. A companion educational project is also being undertaken with the Museum of Science in Boston and Wellesley College. This project is jointly supported by the Atomic, Molecular, Optical, and Plasma Physics Program, the Condensed Matter Physics Program, and the Office of Multidisciplinary Activities in the Mathematical and Physical Sciences Directorate.</data>
      <data key="e_pgm">1248</data>
      <data key="e_label">71311</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">71311</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n597" target="n598">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">General-purpose Process Migration</data>
      <data key="e_abstract">Vijay Karamcheti, Zvi M. Kedem&lt;br/&gt;&lt;br/&gt;The process migration problem has received a lot of attention but has produced only mixed results. Traditionally, migration mechanisms have been used to balance the load among processors in a distributed system. However, most mechanisms proposed to date for general-purpose applications have required extensive operating system kernel modifications. Hence, conventional wisdom has held that the costs of process migration are not justified by resulting benefits. This research takes a new look at process migration, recognizing both its utility for an increasing number of emerging applications that can tolerate higher overheads, as well as the possibility of implementing necessary mechanisms using novel user-level techniques. Process&lt;br/&gt;migration enables a host of capabilities such as mobility, collaborative work, distributed systems management, automatic reconfigurability, and fault-tolerance. Moreover, API interception&lt;br/&gt;technologies, which permit insertion of arbitrary functionality in the execution path of an application&apos;s interactions with the underlying operating system, enable process migration to be implemented without kernel modification. This research investigates a virtualization&lt;br/&gt;scheme that decouples an application from dependencies in the operating system and the physical environment. Virtualization also monitors and controls the interactions and the resulting side effects between processes and operating systems, making process migration&lt;br/&gt;possible.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">9.98818e+06</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98818e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n599" target="n600">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">The Effects of Collaborative Skills and Culture in International Computer-Supported Collaborations</data>
      <data key="e_abstract">This research project proposes to study the programming productivity of global software teams from the dual perspectives of cultural variability and collaborative skills. Based on these two dynamics, the PIs will develop a framework, determine its validity and then establish the relative importance of cultural forces and collaborative skills on global team performance. An international research team of faculty at the Middle East Technical University in Ankara, Turkey and Sejong University in Seoul, South Korea are participating in this project. The three universities will set up global teams of undergraduate students doing joint programming tasks. Students will use a synchronous groupware system (with asynchronous communications capabilities, also) for their tasks. This research should provide useful insights into how to manage globally dispersed software development projects and the specific dynamics affecting global team performance, contributing to the theoretically motivated empirical studies on these topics.</data>
      <data key="e_pgm">5976</data>
      <data key="e_label">9.9886e+06</data>
      <data key="e_expirationDate">2006-07-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">9.9886e+06</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n599" target="n601">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">The Effects of Collaborative Skills and Culture in International Computer-Supported Collaborations</data>
      <data key="e_abstract">This research project proposes to study the programming productivity of global software teams from the dual perspectives of cultural variability and collaborative skills. Based on these two dynamics, the PIs will develop a framework, determine its validity and then establish the relative importance of cultural forces and collaborative skills on global team performance. An international research team of faculty at the Middle East Technical University in Ankara, Turkey and Sejong University in Seoul, South Korea are participating in this project. The three universities will set up global teams of undergraduate students doing joint programming tasks. Students will use a synchronous groupware system (with asynchronous communications capabilities, also) for their tasks. This research should provide useful insights into how to manage globally dispersed software development projects and the specific dynamics affecting global team performance, contributing to the theoretically motivated empirical studies on these topics.</data>
      <data key="e_pgm">5976</data>
      <data key="e_label">9.9886e+06</data>
      <data key="e_expirationDate">2006-07-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">9.9886e+06</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n600" target="n601">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">The Effects of Collaborative Skills and Culture in International Computer-Supported Collaborations</data>
      <data key="e_abstract">This research project proposes to study the programming productivity of global software teams from the dual perspectives of cultural variability and collaborative skills. Based on these two dynamics, the PIs will develop a framework, determine its validity and then establish the relative importance of cultural forces and collaborative skills on global team performance. An international research team of faculty at the Middle East Technical University in Ankara, Turkey and Sejong University in Seoul, South Korea are participating in this project. The three universities will set up global teams of undergraduate students doing joint programming tasks. Students will use a synchronous groupware system (with asynchronous communications capabilities, also) for their tasks. This research should provide useful insights into how to manage globally dispersed software development projects and the specific dynamics affecting global team performance, contributing to the theoretically motivated empirical studies on these topics.</data>
      <data key="e_pgm">5976</data>
      <data key="e_label">9.9886e+06</data>
      <data key="e_expirationDate">2006-07-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">9.9886e+06</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n603" target="n604">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Digital Government: Very Large Scale Multidimensional Data Management and Retrieval for USGS and NIMA Imagery</data>
      <data key="e_abstract">EIA-9983430&lt;br/&gt;Zhang, Aidong&lt;br/&gt;SUNY Buffalo&lt;br/&gt;&lt;br/&gt;Digital Government: Very Large Scale Multidimensional Data Management and Retrieval for USGS and NIMA Imagery&lt;br/&gt;&lt;br/&gt;This project will conduct research on managing and retrieving multidimensional data held by partner government agencies (the United States Geological Survey (USGS) and the National Imaging and Mapping Agency (NIMA)). In such applications, geographic image databases distributed at remote locations must be made available at other locations for purpose of data retrieval. With the advent of content-based retrieval of image data, traditional methods for database design and query search will not be suitable in developing distributed image retrieval systems.&lt;br/&gt;&lt;br/&gt;The objective of the proposed research is to investigate novel approaches to supporting effective and efficient access to the integrated geographic image databases over the Internet. These approaches will establish a foundation for the design of distributed geographic image retrieval systems. The technical challenge relating to the design of such integration is the creation of meta-level system on top of the image databases. The specific research goals are : (1) identify multi-scale representation (multidimensional) methods for geographic images, (2) investigate novel clustering approaches that can detect clusters of arbitrary shape of multidimensional image data, (3) construct a metadata model that formulates the metadata that are needed for the integrated system to direct a visual query to relevant databases, (4) develop the theoretical foundation of database selection approaches based on the metadata, and (5) design novel visual query processing approaches that integrate heterogeneous features extracted from the content of visual data.&lt;br/&gt;&lt;br/&gt;During the project period, USGS and NIMA will provide all necessary imagery data sets and their categories. A system will be disseminated at both USGS and NIMA sites at the end of the project period. Through the above research activities, the fundamental understanding and novel techniques will be provided to support the design of distributed volume of multidimensional data distributed over the Internet and will find broad applications as a template for the development of distributed image database system in other government agencies, such as NASA. The experimental results to be generated can be used to establish effective benchmarks for assessing the performance of distributed image data retrieval systems.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98343e+06</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98343e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n605" target="n606">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">ITR: High Density Analog Computing Arrays</data>
      <data key="e_abstract">EIA-0083172&lt;br/&gt;Hasler, Paul&lt;br/&gt;Georgia Institute of Technology&lt;br/&gt;&lt;br/&gt;ITR: High Density Analog Computing Arrays&lt;br/&gt;&lt;br/&gt;This project is investigating an alternate model of real-time sensory signal processing based upon a mixture of analog and digital computation. The arrays under study perform analog computations directly in modified EEPROM memory cells; each cell acts as a multiplier that multiplies the input signal by an analog value stored on a floating gate. Potential advantages of analog computing arrays include lower latency than digital computers, lower power consumption, and parallel processing of many analog inputs. The project encompasses experimental demonstration of analog computing arrays, simulation and design tools to make the technology accessable to DSP designers, and investigation of the robustness and reliability of the technology. Several single-chip signal processing systems will be developed, including adaptive matched filters, a cepstrum calculater, systems for hidden Markov models, and a multi-directional filter array.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">83172</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">83172</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n608" target="n609">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Digital Government: Knowledge Management Over Time-Varying Geospatial Datasets</data>
      <data key="e_abstract">EIA-9983445&lt;br/&gt;Agouris, Peggy&lt;br/&gt;University of Maine&lt;br/&gt;&lt;br/&gt;Digital Government: Knowledge Management over Time-Varying Geospatial Datasets&lt;br/&gt;&lt;br/&gt;Spatially-related data is collected by many government agencies in various formats and for various uses. This project seeks to facilitate the integration of these data, thus providing new uses. This will require the development of a knowledge management framework to provide syntax, context, and semantics, as well as exploring the introduction of time-varying data into the framework. Education and outreach will be part of the project through the development of an on-line short courses related to data integration in the area of geographical information systems. The grantees will be working with government partners (National Imagery and Mapping Agency, the National Agricultural Statistics Service, and the US Army Topographic Engineering Center), as well as an industrial organization, Base Systems, and the non-profit OpenGIS Consortium, which works closely with vendors of GIS products.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.98344e+06</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98344e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n608" target="n610">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Digital Government: Knowledge Management Over Time-Varying Geospatial Datasets</data>
      <data key="e_abstract">EIA-9983445&lt;br/&gt;Agouris, Peggy&lt;br/&gt;University of Maine&lt;br/&gt;&lt;br/&gt;Digital Government: Knowledge Management over Time-Varying Geospatial Datasets&lt;br/&gt;&lt;br/&gt;Spatially-related data is collected by many government agencies in various formats and for various uses. This project seeks to facilitate the integration of these data, thus providing new uses. This will require the development of a knowledge management framework to provide syntax, context, and semantics, as well as exploring the introduction of time-varying data into the framework. Education and outreach will be part of the project through the development of an on-line short courses related to data integration in the area of geographical information systems. The grantees will be working with government partners (National Imagery and Mapping Agency, the National Agricultural Statistics Service, and the US Army Topographic Engineering Center), as well as an industrial organization, Base Systems, and the non-profit OpenGIS Consortium, which works closely with vendors of GIS products.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.98344e+06</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98344e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n609" target="n610">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Digital Government: Knowledge Management Over Time-Varying Geospatial Datasets</data>
      <data key="e_abstract">EIA-9983445&lt;br/&gt;Agouris, Peggy&lt;br/&gt;University of Maine&lt;br/&gt;&lt;br/&gt;Digital Government: Knowledge Management over Time-Varying Geospatial Datasets&lt;br/&gt;&lt;br/&gt;Spatially-related data is collected by many government agencies in various formats and for various uses. This project seeks to facilitate the integration of these data, thus providing new uses. This will require the development of a knowledge management framework to provide syntax, context, and semantics, as well as exploring the introduction of time-varying data into the framework. Education and outreach will be part of the project through the development of an on-line short courses related to data integration in the area of geographical information systems. The grantees will be working with government partners (National Imagery and Mapping Agency, the National Agricultural Statistics Service, and the US Army Topographic Engineering Center), as well as an industrial organization, Base Systems, and the non-profit OpenGIS Consortium, which works closely with vendors of GIS products.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.98344e+06</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98344e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n62" target="n613">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Towards an Automated Development Environment for Parallel Computing with Reconfigurable Processing Elements</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;Proposal: 0075792&lt;br/&gt;PI: Michael Langston&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;An adaptive computing system (ACS) offers a revolutionary combination of the performance of custom hardware and the flexibility of software by employing reconfigurable technology. A key feature of an ACS is the reconfigurable processing element, which, in the current generation, is a Field-Programmable Gate Array (FPGA) chip. This research project investigates the impact of an ACS in the context of a high-performance computational grid with clusters-of-workstations, shared memory multi-processors and rapid interconnects. Suites of fast estimators are devised using approximation algorithms for FPGA mapping and partitioning. An assortment of algorithmic methods is applied. A major focus is on new heuristic and optimization strategies designed to exploit emergent mathematical techniques. Supporting software tools are also developed, with an emphasis placed on portability. Implementation testbeds are built around edge-based segmentation and related problems common to a variety of image processing applications.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">75792</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">75792</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n613" target="n615">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Towards an Automated Development Environment for Parallel Computing with Reconfigurable Processing Elements</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;Proposal: 0075792&lt;br/&gt;PI: Michael Langston&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;An adaptive computing system (ACS) offers a revolutionary combination of the performance of custom hardware and the flexibility of software by employing reconfigurable technology. A key feature of an ACS is the reconfigurable processing element, which, in the current generation, is a Field-Programmable Gate Array (FPGA) chip. This research project investigates the impact of an ACS in the context of a high-performance computational grid with clusters-of-workstations, shared memory multi-processors and rapid interconnects. Suites of fast estimators are devised using approximation algorithms for FPGA mapping and partitioning. An assortment of algorithmic methods is applied. A major focus is on new heuristic and optimization strategies designed to exploit emergent mathematical techniques. Supporting software tools are also developed, with an emphasis placed on portability. Implementation testbeds are built around edge-based segmentation and related problems common to a variety of image processing applications.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">75792</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">75792</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n613" target="n616">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Towards an Automated Development Environment for Parallel Computing with Reconfigurable Processing Elements</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;Proposal: 0075792&lt;br/&gt;PI: Michael Langston&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;An adaptive computing system (ACS) offers a revolutionary combination of the performance of custom hardware and the flexibility of software by employing reconfigurable technology. A key feature of an ACS is the reconfigurable processing element, which, in the current generation, is a Field-Programmable Gate Array (FPGA) chip. This research project investigates the impact of an ACS in the context of a high-performance computational grid with clusters-of-workstations, shared memory multi-processors and rapid interconnects. Suites of fast estimators are devised using approximation algorithms for FPGA mapping and partitioning. An assortment of algorithmic methods is applied. A major focus is on new heuristic and optimization strategies designed to exploit emergent mathematical techniques. Supporting software tools are also developed, with an emphasis placed on portability. Implementation testbeds are built around edge-based segmentation and related problems common to a variety of image processing applications.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">75792</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">75792</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n613" target="n617">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Towards an Automated Development Environment for Parallel Computing with Reconfigurable Processing Elements</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;Proposal: 0075792&lt;br/&gt;PI: Michael Langston&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;An adaptive computing system (ACS) offers a revolutionary combination of the performance of custom hardware and the flexibility of software by employing reconfigurable technology. A key feature of an ACS is the reconfigurable processing element, which, in the current generation, is a Field-Programmable Gate Array (FPGA) chip. This research project investigates the impact of an ACS in the context of a high-performance computational grid with clusters-of-workstations, shared memory multi-processors and rapid interconnects. Suites of fast estimators are devised using approximation algorithms for FPGA mapping and partitioning. An assortment of algorithmic methods is applied. A major focus is on new heuristic and optimization strategies designed to exploit emergent mathematical techniques. Supporting software tools are also developed, with an emphasis placed on portability. Implementation testbeds are built around edge-based segmentation and related problems common to a variety of image processing applications.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">75792</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">75792</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n62" target="n615">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Towards an Automated Development Environment for Parallel Computing with Reconfigurable Processing Elements</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;Proposal: 0075792&lt;br/&gt;PI: Michael Langston&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;An adaptive computing system (ACS) offers a revolutionary combination of the performance of custom hardware and the flexibility of software by employing reconfigurable technology. A key feature of an ACS is the reconfigurable processing element, which, in the current generation, is a Field-Programmable Gate Array (FPGA) chip. This research project investigates the impact of an ACS in the context of a high-performance computational grid with clusters-of-workstations, shared memory multi-processors and rapid interconnects. Suites of fast estimators are devised using approximation algorithms for FPGA mapping and partitioning. An assortment of algorithmic methods is applied. A major focus is on new heuristic and optimization strategies designed to exploit emergent mathematical techniques. Supporting software tools are also developed, with an emphasis placed on portability. Implementation testbeds are built around edge-based segmentation and related problems common to a variety of image processing applications.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">75792</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">75792</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n62" target="n616">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Towards an Automated Development Environment for Parallel Computing with Reconfigurable Processing Elements</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;Proposal: 0075792&lt;br/&gt;PI: Michael Langston&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;An adaptive computing system (ACS) offers a revolutionary combination of the performance of custom hardware and the flexibility of software by employing reconfigurable technology. A key feature of an ACS is the reconfigurable processing element, which, in the current generation, is a Field-Programmable Gate Array (FPGA) chip. This research project investigates the impact of an ACS in the context of a high-performance computational grid with clusters-of-workstations, shared memory multi-processors and rapid interconnects. Suites of fast estimators are devised using approximation algorithms for FPGA mapping and partitioning. An assortment of algorithmic methods is applied. A major focus is on new heuristic and optimization strategies designed to exploit emergent mathematical techniques. Supporting software tools are also developed, with an emphasis placed on portability. Implementation testbeds are built around edge-based segmentation and related problems common to a variety of image processing applications.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">75792</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">75792</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n62" target="n617">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Towards an Automated Development Environment for Parallel Computing with Reconfigurable Processing Elements</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;Proposal: 0075792&lt;br/&gt;PI: Michael Langston&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;An adaptive computing system (ACS) offers a revolutionary combination of the performance of custom hardware and the flexibility of software by employing reconfigurable technology. A key feature of an ACS is the reconfigurable processing element, which, in the current generation, is a Field-Programmable Gate Array (FPGA) chip. This research project investigates the impact of an ACS in the context of a high-performance computational grid with clusters-of-workstations, shared memory multi-processors and rapid interconnects. Suites of fast estimators are devised using approximation algorithms for FPGA mapping and partitioning. An assortment of algorithmic methods is applied. A major focus is on new heuristic and optimization strategies designed to exploit emergent mathematical techniques. Supporting software tools are also developed, with an emphasis placed on portability. Implementation testbeds are built around edge-based segmentation and related problems common to a variety of image processing applications.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">75792</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">75792</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n615" target="n616">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Towards an Automated Development Environment for Parallel Computing with Reconfigurable Processing Elements</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;Proposal: 0075792&lt;br/&gt;PI: Michael Langston&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;An adaptive computing system (ACS) offers a revolutionary combination of the performance of custom hardware and the flexibility of software by employing reconfigurable technology. A key feature of an ACS is the reconfigurable processing element, which, in the current generation, is a Field-Programmable Gate Array (FPGA) chip. This research project investigates the impact of an ACS in the context of a high-performance computational grid with clusters-of-workstations, shared memory multi-processors and rapid interconnects. Suites of fast estimators are devised using approximation algorithms for FPGA mapping and partitioning. An assortment of algorithmic methods is applied. A major focus is on new heuristic and optimization strategies designed to exploit emergent mathematical techniques. Supporting software tools are also developed, with an emphasis placed on portability. Implementation testbeds are built around edge-based segmentation and related problems common to a variety of image processing applications.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">75792</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">75792</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n615" target="n617">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Towards an Automated Development Environment for Parallel Computing with Reconfigurable Processing Elements</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;Proposal: 0075792&lt;br/&gt;PI: Michael Langston&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;An adaptive computing system (ACS) offers a revolutionary combination of the performance of custom hardware and the flexibility of software by employing reconfigurable technology. A key feature of an ACS is the reconfigurable processing element, which, in the current generation, is a Field-Programmable Gate Array (FPGA) chip. This research project investigates the impact of an ACS in the context of a high-performance computational grid with clusters-of-workstations, shared memory multi-processors and rapid interconnects. Suites of fast estimators are devised using approximation algorithms for FPGA mapping and partitioning. An assortment of algorithmic methods is applied. A major focus is on new heuristic and optimization strategies designed to exploit emergent mathematical techniques. Supporting software tools are also developed, with an emphasis placed on portability. Implementation testbeds are built around edge-based segmentation and related problems common to a variety of image processing applications.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">75792</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">75792</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n616" target="n617">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Towards an Automated Development Environment for Parallel Computing with Reconfigurable Processing Elements</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;Proposal: 0075792&lt;br/&gt;PI: Michael Langston&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;An adaptive computing system (ACS) offers a revolutionary combination of the performance of custom hardware and the flexibility of software by employing reconfigurable technology. A key feature of an ACS is the reconfigurable processing element, which, in the current generation, is a Field-Programmable Gate Array (FPGA) chip. This research project investigates the impact of an ACS in the context of a high-performance computational grid with clusters-of-workstations, shared memory multi-processors and rapid interconnects. Suites of fast estimators are devised using approximation algorithms for FPGA mapping and partitioning. An assortment of algorithmic methods is applied. A major focus is on new heuristic and optimization strategies designed to exploit emergent mathematical techniques. Supporting software tools are also developed, with an emphasis placed on portability. Implementation testbeds are built around edge-based segmentation and related problems common to a variety of image processing applications.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">75792</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">75792</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n621" target="n622">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Digital Government: Representation and Distribution of Geospatial Knowledge</data>
      <data key="e_abstract">EIA-9983267&lt;br/&gt;Malyankar, Raphael M.&lt;br/&gt;Arizona State University&lt;br/&gt;&lt;br/&gt;Digital Government: Representation and Distribution of Geospatial Knowledge&lt;br/&gt;&lt;br/&gt;This project will work within the domain of maritime navigation, in partnership primarily with the Coast Guard, but with contacts to NOAA, NIMA, and the Navy. The ontology developed will include such representations as shorelines, hazards, aids to navigation, tide tables, sea lanes, etc. and a related markup language. This will be of benefit as these agencies begin to deploy web-based Q&amp;A systems and automated mission planning, intelligent navigation aids, and route planning tools. The research challenges in this work revolve around representing a domain that is highly dynamic, time-critical and rich in factors and entities.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98327e+06</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98327e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n627" target="n628">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Low-Power System-on-a-Chip Design for Minimum Transient Energy</data>
      <data key="e_abstract">In this project, the original theory for hazard elimination in combinational circuits is being expanded to also accelerate circuit speed while reducing power consumption. This method relies on transistor timing adjustments to eliminate hazards, and also on transistor resizing to speed up multiple circuit paths. One method being investigated is to enumerate path delays in the circuit, and use them as non-linear programming constraints. This will be feasible if the circuit is partitioned. Another method being investigated is a different formulation that does not require path enumeration. Experiments on both formulations are being conducted using linear and non-linear programming solvers. Since it is not clear which method is superior, both will be tried using AMPL, which is a widely used mathematical programming package.&lt;br/&gt; Hazards account for 60% of the power used in certain arithmetic circuits. Hazards are eliminated with either the balanced path delay method or by an original method of increasing logic gate inertial delays so that gates do not respond to hazard-creating input conditions. Agrawal and Bushnell have proven the minimum transient energy conditions necessary for a circuit to use minimal energy, and developed the first optimal linear programming method for transistor resizing to minimize transient energy. They use a simultaneous application of both the balanced path delay method, and the method of increasing logic gate inertial delays to make logic gates filter out hazards. The balanced path delay method frequently requires the insertion of buffers to balance path delays, or decreased transistor sizes on fast paths to achieve the same effect. Hazard filtering, instead, merely requires that logic gates be slowed down so that they cannot react to hazard-producing conditions at their inputs. The project is developing a non-linear programming method to adjust path delays by transistor resizing to simultaneously lower power and speed up the circuit, while limiting the increase in chip area. This beneficial method substantially reduces power while speeding up the circuit, which lowers chip packaging, cooling, and silicon costs.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">9.98824e+06</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98824e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n635" target="n636">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Assessing the Role of Technology in the Work of Modern Engineers</data>
      <data key="e_abstract">Researchers know very little about the tasks engineers perform, the ways in which they employ technology, how their knowledge and tasks are embodied in technology, and what skills are likely to be required in the years ahead. In this project, the role of technology in the work of modern engineers will be assessed. Using ethnographic techniques of observation and interviewing, two types of engineers will be studied: structural engineers who design buildings and electrical engineers who design computer chips. In the course of this research, what engineers do, what technologies they employ, the extent to which, and the manner in which, these technologies embody, augment, and alter engineering knowledge and tasks will be investigated. This study augments current knowledge about engineering work and will have significant implications for organizations as they consider how to manage and support their engineering workforce. In addition, the answers to these research questions will inform the education and training of engineers, as well as their broader study in the social sciences. Our results will lend insight into the design of engineering workspaces, tools, and supporting technologies. More generally, they will further add to our understanding of how technologies are employed in the workplace.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">70468</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">70468</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n635" target="n637">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Assessing the Role of Technology in the Work of Modern Engineers</data>
      <data key="e_abstract">Researchers know very little about the tasks engineers perform, the ways in which they employ technology, how their knowledge and tasks are embodied in technology, and what skills are likely to be required in the years ahead. In this project, the role of technology in the work of modern engineers will be assessed. Using ethnographic techniques of observation and interviewing, two types of engineers will be studied: structural engineers who design buildings and electrical engineers who design computer chips. In the course of this research, what engineers do, what technologies they employ, the extent to which, and the manner in which, these technologies embody, augment, and alter engineering knowledge and tasks will be investigated. This study augments current knowledge about engineering work and will have significant implications for organizations as they consider how to manage and support their engineering workforce. In addition, the answers to these research questions will inform the education and training of engineers, as well as their broader study in the social sciences. Our results will lend insight into the design of engineering workspaces, tools, and supporting technologies. More generally, they will further add to our understanding of how technologies are employed in the workplace.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">70468</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">70468</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n635" target="n638">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Assessing the Role of Technology in the Work of Modern Engineers</data>
      <data key="e_abstract">Researchers know very little about the tasks engineers perform, the ways in which they employ technology, how their knowledge and tasks are embodied in technology, and what skills are likely to be required in the years ahead. In this project, the role of technology in the work of modern engineers will be assessed. Using ethnographic techniques of observation and interviewing, two types of engineers will be studied: structural engineers who design buildings and electrical engineers who design computer chips. In the course of this research, what engineers do, what technologies they employ, the extent to which, and the manner in which, these technologies embody, augment, and alter engineering knowledge and tasks will be investigated. This study augments current knowledge about engineering work and will have significant implications for organizations as they consider how to manage and support their engineering workforce. In addition, the answers to these research questions will inform the education and training of engineers, as well as their broader study in the social sciences. Our results will lend insight into the design of engineering workspaces, tools, and supporting technologies. More generally, they will further add to our understanding of how technologies are employed in the workplace.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">70468</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">70468</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n636" target="n637">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Assessing the Role of Technology in the Work of Modern Engineers</data>
      <data key="e_abstract">Researchers know very little about the tasks engineers perform, the ways in which they employ technology, how their knowledge and tasks are embodied in technology, and what skills are likely to be required in the years ahead. In this project, the role of technology in the work of modern engineers will be assessed. Using ethnographic techniques of observation and interviewing, two types of engineers will be studied: structural engineers who design buildings and electrical engineers who design computer chips. In the course of this research, what engineers do, what technologies they employ, the extent to which, and the manner in which, these technologies embody, augment, and alter engineering knowledge and tasks will be investigated. This study augments current knowledge about engineering work and will have significant implications for organizations as they consider how to manage and support their engineering workforce. In addition, the answers to these research questions will inform the education and training of engineers, as well as their broader study in the social sciences. Our results will lend insight into the design of engineering workspaces, tools, and supporting technologies. More generally, they will further add to our understanding of how technologies are employed in the workplace.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">70468</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">70468</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n636" target="n638">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Assessing the Role of Technology in the Work of Modern Engineers</data>
      <data key="e_abstract">Researchers know very little about the tasks engineers perform, the ways in which they employ technology, how their knowledge and tasks are embodied in technology, and what skills are likely to be required in the years ahead. In this project, the role of technology in the work of modern engineers will be assessed. Using ethnographic techniques of observation and interviewing, two types of engineers will be studied: structural engineers who design buildings and electrical engineers who design computer chips. In the course of this research, what engineers do, what technologies they employ, the extent to which, and the manner in which, these technologies embody, augment, and alter engineering knowledge and tasks will be investigated. This study augments current knowledge about engineering work and will have significant implications for organizations as they consider how to manage and support their engineering workforce. In addition, the answers to these research questions will inform the education and training of engineers, as well as their broader study in the social sciences. Our results will lend insight into the design of engineering workspaces, tools, and supporting technologies. More generally, they will further add to our understanding of how technologies are employed in the workplace.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">70468</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">70468</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n637" target="n638">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Assessing the Role of Technology in the Work of Modern Engineers</data>
      <data key="e_abstract">Researchers know very little about the tasks engineers perform, the ways in which they employ technology, how their knowledge and tasks are embodied in technology, and what skills are likely to be required in the years ahead. In this project, the role of technology in the work of modern engineers will be assessed. Using ethnographic techniques of observation and interviewing, two types of engineers will be studied: structural engineers who design buildings and electrical engineers who design computer chips. In the course of this research, what engineers do, what technologies they employ, the extent to which, and the manner in which, these technologies embody, augment, and alter engineering knowledge and tasks will be investigated. This study augments current knowledge about engineering work and will have significant implications for organizations as they consider how to manage and support their engineering workforce. In addition, the answers to these research questions will inform the education and training of engineers, as well as their broader study in the social sciences. Our results will lend insight into the design of engineering workspaces, tools, and supporting technologies. More generally, they will further add to our understanding of how technologies are employed in the workplace.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">70468</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">70468</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n603" target="n641">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Metadata Model, Resource Discovery, and Querying on large-scale Multidimensional Datasets</data>
      <data key="e_abstract">The objective of the proposed research is to investigate novel approaches to supporting effective and efficient access to various geographic image databases over the Internet leading to design of &lt;br/&gt;distributed geographic image retrieval systems. The research involves three research teams from: SUNY Buffalo, National Center for Science Information Systems (NACSIS) in Japan, and the University of Nantes in France. (IRESTE) The technical challenges are the creation of a new meta-level system for geographic image databases. To achieve this, research issues concerned with representation models for geographic data, relationships between metadata and resource discovery and efficient query processing in a distributed environment must be addressed. The problems addressed are critical to retrieval on large volume, multidimensional, distributed data over the Internet. Results can be broadly applied to numerous domain and interdisciplinary research areas.</data>
      <data key="e_pgm">5978</data>
      <data key="e_label">9.9056e+06</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">9.9056e+06</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n604" target="n641">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Metadata Model, Resource Discovery, and Querying on large-scale Multidimensional Datasets</data>
      <data key="e_abstract">The objective of the proposed research is to investigate novel approaches to supporting effective and efficient access to various geographic image databases over the Internet leading to design of &lt;br/&gt;distributed geographic image retrieval systems. The research involves three research teams from: SUNY Buffalo, National Center for Science Information Systems (NACSIS) in Japan, and the University of Nantes in France. (IRESTE) The technical challenges are the creation of a new meta-level system for geographic image databases. To achieve this, research issues concerned with representation models for geographic data, relationships between metadata and resource discovery and efficient query processing in a distributed environment must be addressed. The problems addressed are critical to retrieval on large volume, multidimensional, distributed data over the Internet. Results can be broadly applied to numerous domain and interdisciplinary research areas.</data>
      <data key="e_pgm">5978</data>
      <data key="e_label">9.9056e+06</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">9.9056e+06</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n603" target="n604">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Metadata Model, Resource Discovery, and Querying on large-scale Multidimensional Datasets</data>
      <data key="e_abstract">The objective of the proposed research is to investigate novel approaches to supporting effective and efficient access to various geographic image databases over the Internet leading to design of &lt;br/&gt;distributed geographic image retrieval systems. The research involves three research teams from: SUNY Buffalo, National Center for Science Information Systems (NACSIS) in Japan, and the University of Nantes in France. (IRESTE) The technical challenges are the creation of a new meta-level system for geographic image databases. To achieve this, research issues concerned with representation models for geographic data, relationships between metadata and resource discovery and efficient query processing in a distributed environment must be addressed. The problems addressed are critical to retrieval on large volume, multidimensional, distributed data over the Internet. Results can be broadly applied to numerous domain and interdisciplinary research areas.</data>
      <data key="e_pgm">5978</data>
      <data key="e_label">9.9056e+06</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">9.9056e+06</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n648" target="n649">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">High-Performance Decoding of Algebraic Codes Beyond their Packing Radii</data>
      <data key="e_abstract">Abstract&lt;br/&gt; This research will improve the decoding of error-control codes, in&lt;br/&gt;particular, Reed-Solomon codes, which are essential in most advanced modern&lt;br/&gt;communication and storage systems, ranging from deep space communication to &lt;br/&gt;optical and magnetic storage systems, such as the compact disk and CD-ROM.&lt;br/&gt;By developing decoders that operate at lower signal to noise ratios, this&lt;br/&gt;research will allow the recovery of data and records that currently are&lt;br/&gt;unreadable.&lt;br/&gt; This research will develop an efficient soft decoding algorithm that &lt;br/&gt;will significantly outperform GMD-based decoding algorithms. The algorithm&lt;br/&gt;is based on Sudan&apos;s list decoding technique that produces a list of &lt;br/&gt;tentative codewords and it shows a number of very interesting features.&lt;br/&gt;Among the most intriguing characteristics of the algorithm is a complexity&lt;br/&gt;and performance that can be traded freely within certain fundamental&lt;br/&gt;limits. Hence, the coding gain provided by the Reed-Solomon code can be&lt;br/&gt;traded on the fly for complexity in any application with the maximal&lt;br/&gt;list size changing accordingly.</data>
      <data key="e_pgm">4096</data>
      <data key="e_label">73490</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">73490</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n224" target="n648">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">High-Performance Decoding of Algebraic Codes Beyond their Packing Radii</data>
      <data key="e_abstract">Abstract&lt;br/&gt; This research will improve the decoding of error-control codes, in&lt;br/&gt;particular, Reed-Solomon codes, which are essential in most advanced modern&lt;br/&gt;communication and storage systems, ranging from deep space communication to &lt;br/&gt;optical and magnetic storage systems, such as the compact disk and CD-ROM.&lt;br/&gt;By developing decoders that operate at lower signal to noise ratios, this&lt;br/&gt;research will allow the recovery of data and records that currently are&lt;br/&gt;unreadable.&lt;br/&gt; This research will develop an efficient soft decoding algorithm that &lt;br/&gt;will significantly outperform GMD-based decoding algorithms. The algorithm&lt;br/&gt;is based on Sudan&apos;s list decoding technique that produces a list of &lt;br/&gt;tentative codewords and it shows a number of very interesting features.&lt;br/&gt;Among the most intriguing characteristics of the algorithm is a complexity&lt;br/&gt;and performance that can be traded freely within certain fundamental&lt;br/&gt;limits. Hence, the coding gain provided by the Reed-Solomon code can be&lt;br/&gt;traded on the fly for complexity in any application with the maximal&lt;br/&gt;list size changing accordingly.</data>
      <data key="e_pgm">4096</data>
      <data key="e_label">73490</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">73490</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n224" target="n649">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">High-Performance Decoding of Algebraic Codes Beyond their Packing Radii</data>
      <data key="e_abstract">Abstract&lt;br/&gt; This research will improve the decoding of error-control codes, in&lt;br/&gt;particular, Reed-Solomon codes, which are essential in most advanced modern&lt;br/&gt;communication and storage systems, ranging from deep space communication to &lt;br/&gt;optical and magnetic storage systems, such as the compact disk and CD-ROM.&lt;br/&gt;By developing decoders that operate at lower signal to noise ratios, this&lt;br/&gt;research will allow the recovery of data and records that currently are&lt;br/&gt;unreadable.&lt;br/&gt; This research will develop an efficient soft decoding algorithm that &lt;br/&gt;will significantly outperform GMD-based decoding algorithms. The algorithm&lt;br/&gt;is based on Sudan&apos;s list decoding technique that produces a list of &lt;br/&gt;tentative codewords and it shows a number of very interesting features.&lt;br/&gt;Among the most intriguing characteristics of the algorithm is a complexity&lt;br/&gt;and performance that can be traded freely within certain fundamental&lt;br/&gt;limits. Hence, the coding gain provided by the Reed-Solomon code can be&lt;br/&gt;traded on the fly for complexity in any application with the maximal&lt;br/&gt;list size changing accordingly.</data>
      <data key="e_pgm">4096</data>
      <data key="e_label">73490</data>
      <data key="e_expirationDate">2004-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">73490</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n652" target="n653">
      <data key="e_effectiveDate">2000-08-15</data>
      <data key="e_title">Collaborative Research: Biocomplexity of Aquatic Microbial Systems: Relating Diversity of Microorganisms to Ecosystem Function</data>
      <data key="e_abstract">BIOCOMPLEXITY: Collaborative Research: Biocomplexity of aquatic microbial systems -- relating diversity of microorganisms to ecosystem function&lt;br/&gt;&lt;br/&gt;Microbial biogeochemical cycling of the elements regulates a dynamic environment in which the cycles of different elements are linked through the physiology of microorganisms. While a certain degree of understanding can be gained through physical/chemical approaches to measurement and modeling of the net transformations, these approaches necessarily rely on gross simplifications about the role and regulation of the various functional groups (guilds) involved. Recent advances in molecular microbial ecology have shown the microbial world to contain immense diversity and complexity at every level: redundancy and duplication of functional genes within a single organism; molecular diversity among functional genes that encode the same process in different organisms; large genetic diversity among different organisms apparently engaged in the same biogeochemical function within single communities; great variability in the species composition of different communities that apparently perform equally well. The goal of this project is to investigate the functional relationship between complexity in microbial communities and the physical/chemical environment at a range of biological and ecological scales. Previously, such analysis was technologically limited by the inability to assay large numbers of samples simultaneously for a large number of genes and phylotypes. Using gene array technology, the researchers will be able to detect the distribution and differential expression of functional genes in natural systems. The results of this study will constitute the first step towards application of DNA chip technology for gene expression of &quot;exotic&quot; (i.e., not of biomedical importance) processes and organisms in the environment. The gene arrays, along with a full suite of ecosystem process measurements, will be deployed along a transect that spans the eutrophic - oligotrophic gradient from the inland waters of the Chesapeake Bay out to the Sargasso Sea. Experiments and functional gene studies will focus on key transformations in the carbon and nitrogen cycles (C fixation, N fixation, nitrification, denitrification, urea assimilation). The diversity of guilds will be interpreted in terms of ecosystem function, assessed using geochemical data and tracer experiments. In addition to field studies designed to investigate and dissect the natural system, the group of collaborating scientists will also perform perturbation experiments using mesocosms. The goal of these experiments is to determine how microbial species diversity affects the major energy and nutrient flows within ecosystems, and to assess the degree of stability or instability associated with changes in redundancy within guilds of microorganisms responsible for major nitrogen and carbon pathways.</data>
      <data key="e_pgm">1253</data>
      <data key="e_label">9.98162e+06</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0309</data>
      <data key="e_awardID">9.98162e+06</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n655" target="n656">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Particle Tracking in High Reynolds Number Turbulent Flows</data>
      <data key="e_abstract">In this research program the silicon strip technology from High Energy Physics detectors is further developed for three dimensional particle tracking. Fundamental questions about Lagrangian properties of turbulence are unanswered, and our ability to test theoretical models is very limited. Important unsolved problems include statistics of the fluid particle acceleration, inertial range scaling of the Lagrangian structure functions, and n-particle turbulent dispersion. This new technology allows to follow up to four particles in three dimensions at sufficient spatial and temporal resolution to provide the facility to measure single and multiple particle Langrangian statistics in the dissipation and interial range of high Reynolds number turbulence. Simultaneously, new ways are developed to generate very high Reynolds number isotropic and homogeneous turbulence in flows with no mean velocity. These measurements will provide much needed data to compare with theoretical predictions and may trigger new directions in theoretical research. In addition, they will give important information needed to address problems of turbulent diffusion in many processes ranging from turbulent combustion to pollutant transport in the atmosphere. It also promises the continued development of high speed strip detectors as a general purpose tool which can be used for many measurements where a weak light streak has to be followed in real time.</data>
      <data key="e_pgm">9134</data>
      <data key="e_label">9.98876e+06</data>
      <data key="e_expirationDate">2006-07-31</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">9.98876e+06</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n657" target="n658">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">CISE Research Instrumentation: Establishing a Laboratory for Research in Parallel Computing and Signal Processing</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">2890</data>
      <data key="e_label">196324</data>
      <data key="e_expirationDate">2005-10-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">196324</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n661" target="n662">
      <data key="e_effectiveDate">2000-08-22</data>
      <data key="e_title">Multimodal Interaction with Biological Molecules in Virtual Environments</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">4080</data>
      <data key="e_label">296148</data>
      <data key="e_expirationDate">2003-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">296148</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n661" target="n663">
      <data key="e_effectiveDate">2000-08-22</data>
      <data key="e_title">Multimodal Interaction with Biological Molecules in Virtual Environments</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">4080</data>
      <data key="e_label">296148</data>
      <data key="e_expirationDate">2003-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">296148</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n662" target="n663">
      <data key="e_effectiveDate">2000-08-22</data>
      <data key="e_title">Multimodal Interaction with Biological Molecules in Virtual Environments</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">4080</data>
      <data key="e_label">296148</data>
      <data key="e_expirationDate">2003-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">296148</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n668" target="n669">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Digital Government: Harvesting Information to Sustain Our Forests</data>
      <data key="e_abstract">EIA-99083518&lt;br/&gt;Delcambre, Lois M.&lt;br/&gt;Oregon Graduate School&lt;br/&gt;&lt;br/&gt;Digital Government: Harvesting Information to Sustain our Forests&lt;br/&gt;&lt;br/&gt;Historically, forest management practices have aimed to maximize harvests, put out fires, and create plots of a common age and species. More recently, there is an increasing awareness that forests must be managed to reflect a broad range of local, regional and national objectives, including ecological, economic, social and recreational. This research will be a collaboration between the university research team and the Adaptive Management Areas program, a multi-agency initiative (USDA Forest Service, and two agencies of the US Department of Interior, Bureau of Land Management and Fish and Wildlife Service) to design and prototype an &quot;Adaptive Management Portal&quot; to make Adaptive Management information available in an open, natural and useful way to all parties interested in forest lands. A key technical question is to what extent &quot;superimposed information&quot; (information overlaid on the base set of resources) can help realize these goals. There is particular interest in thematic organizations based on terminology in common use in the forest practices domain, and in the ability for arbitrary users to augment the system with their own annotations, linkages and collations that might be of use to others.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98352e+06</data>
      <data key="e_expirationDate">2005-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98352e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n668" target="n670">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Digital Government: Harvesting Information to Sustain Our Forests</data>
      <data key="e_abstract">EIA-99083518&lt;br/&gt;Delcambre, Lois M.&lt;br/&gt;Oregon Graduate School&lt;br/&gt;&lt;br/&gt;Digital Government: Harvesting Information to Sustain our Forests&lt;br/&gt;&lt;br/&gt;Historically, forest management practices have aimed to maximize harvests, put out fires, and create plots of a common age and species. More recently, there is an increasing awareness that forests must be managed to reflect a broad range of local, regional and national objectives, including ecological, economic, social and recreational. This research will be a collaboration between the university research team and the Adaptive Management Areas program, a multi-agency initiative (USDA Forest Service, and two agencies of the US Department of Interior, Bureau of Land Management and Fish and Wildlife Service) to design and prototype an &quot;Adaptive Management Portal&quot; to make Adaptive Management information available in an open, natural and useful way to all parties interested in forest lands. A key technical question is to what extent &quot;superimposed information&quot; (information overlaid on the base set of resources) can help realize these goals. There is particular interest in thematic organizations based on terminology in common use in the forest practices domain, and in the ability for arbitrary users to augment the system with their own annotations, linkages and collations that might be of use to others.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98352e+06</data>
      <data key="e_expirationDate">2005-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98352e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n668" target="n671">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Digital Government: Harvesting Information to Sustain Our Forests</data>
      <data key="e_abstract">EIA-99083518&lt;br/&gt;Delcambre, Lois M.&lt;br/&gt;Oregon Graduate School&lt;br/&gt;&lt;br/&gt;Digital Government: Harvesting Information to Sustain our Forests&lt;br/&gt;&lt;br/&gt;Historically, forest management practices have aimed to maximize harvests, put out fires, and create plots of a common age and species. More recently, there is an increasing awareness that forests must be managed to reflect a broad range of local, regional and national objectives, including ecological, economic, social and recreational. This research will be a collaboration between the university research team and the Adaptive Management Areas program, a multi-agency initiative (USDA Forest Service, and two agencies of the US Department of Interior, Bureau of Land Management and Fish and Wildlife Service) to design and prototype an &quot;Adaptive Management Portal&quot; to make Adaptive Management information available in an open, natural and useful way to all parties interested in forest lands. A key technical question is to what extent &quot;superimposed information&quot; (information overlaid on the base set of resources) can help realize these goals. There is particular interest in thematic organizations based on terminology in common use in the forest practices domain, and in the ability for arbitrary users to augment the system with their own annotations, linkages and collations that might be of use to others.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98352e+06</data>
      <data key="e_expirationDate">2005-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98352e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n669" target="n670">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Digital Government: Harvesting Information to Sustain Our Forests</data>
      <data key="e_abstract">EIA-99083518&lt;br/&gt;Delcambre, Lois M.&lt;br/&gt;Oregon Graduate School&lt;br/&gt;&lt;br/&gt;Digital Government: Harvesting Information to Sustain our Forests&lt;br/&gt;&lt;br/&gt;Historically, forest management practices have aimed to maximize harvests, put out fires, and create plots of a common age and species. More recently, there is an increasing awareness that forests must be managed to reflect a broad range of local, regional and national objectives, including ecological, economic, social and recreational. This research will be a collaboration between the university research team and the Adaptive Management Areas program, a multi-agency initiative (USDA Forest Service, and two agencies of the US Department of Interior, Bureau of Land Management and Fish and Wildlife Service) to design and prototype an &quot;Adaptive Management Portal&quot; to make Adaptive Management information available in an open, natural and useful way to all parties interested in forest lands. A key technical question is to what extent &quot;superimposed information&quot; (information overlaid on the base set of resources) can help realize these goals. There is particular interest in thematic organizations based on terminology in common use in the forest practices domain, and in the ability for arbitrary users to augment the system with their own annotations, linkages and collations that might be of use to others.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98352e+06</data>
      <data key="e_expirationDate">2005-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98352e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n669" target="n671">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Digital Government: Harvesting Information to Sustain Our Forests</data>
      <data key="e_abstract">EIA-99083518&lt;br/&gt;Delcambre, Lois M.&lt;br/&gt;Oregon Graduate School&lt;br/&gt;&lt;br/&gt;Digital Government: Harvesting Information to Sustain our Forests&lt;br/&gt;&lt;br/&gt;Historically, forest management practices have aimed to maximize harvests, put out fires, and create plots of a common age and species. More recently, there is an increasing awareness that forests must be managed to reflect a broad range of local, regional and national objectives, including ecological, economic, social and recreational. This research will be a collaboration between the university research team and the Adaptive Management Areas program, a multi-agency initiative (USDA Forest Service, and two agencies of the US Department of Interior, Bureau of Land Management and Fish and Wildlife Service) to design and prototype an &quot;Adaptive Management Portal&quot; to make Adaptive Management information available in an open, natural and useful way to all parties interested in forest lands. A key technical question is to what extent &quot;superimposed information&quot; (information overlaid on the base set of resources) can help realize these goals. There is particular interest in thematic organizations based on terminology in common use in the forest practices domain, and in the ability for arbitrary users to augment the system with their own annotations, linkages and collations that might be of use to others.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98352e+06</data>
      <data key="e_expirationDate">2005-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98352e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n670" target="n671">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Digital Government: Harvesting Information to Sustain Our Forests</data>
      <data key="e_abstract">EIA-99083518&lt;br/&gt;Delcambre, Lois M.&lt;br/&gt;Oregon Graduate School&lt;br/&gt;&lt;br/&gt;Digital Government: Harvesting Information to Sustain our Forests&lt;br/&gt;&lt;br/&gt;Historically, forest management practices have aimed to maximize harvests, put out fires, and create plots of a common age and species. More recently, there is an increasing awareness that forests must be managed to reflect a broad range of local, regional and national objectives, including ecological, economic, social and recreational. This research will be a collaboration between the university research team and the Adaptive Management Areas program, a multi-agency initiative (USDA Forest Service, and two agencies of the US Department of Interior, Bureau of Land Management and Fish and Wildlife Service) to design and prototype an &quot;Adaptive Management Portal&quot; to make Adaptive Management information available in an open, natural and useful way to all parties interested in forest lands. A key technical question is to what extent &quot;superimposed information&quot; (information overlaid on the base set of resources) can help realize these goals. There is particular interest in thematic organizations based on terminology in common use in the forest practices domain, and in the ability for arbitrary users to augment the system with their own annotations, linkages and collations that might be of use to others.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98352e+06</data>
      <data key="e_expirationDate">2005-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98352e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n674" target="n675">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">CISE Research Instrumentation: Research in Computational Multimedia</data>
      <data key="e_abstract">EIA-9986057&lt;br/&gt;Kenneth Rose&lt;br/&gt;Univ. of California-SB&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Research in Computational Multimedia&lt;br/&gt;&lt;br/&gt;The departments of Computer Science and Electrical Engineering at the University of California, Santa Barbara will purchase the following infrastructure equipment: an SGI 2100 multimedia server with three client SFI 02 workstations to manage, process and stream multimedia data onto a wireless local area network with several PC&apos;s on the backbone Ethernet and mobile computers (laptop computers and handheld devices); an MPEG-2 encoder from Optivision, still and video cameras as input devices, and high resolution monitors and a color laser printer for display and presentation. This equipment will be dedicated to support research in computer and information science and engineering with the main focus on computational multimedia research related to multimedia signal processing, databases, networking and communications. The equipment will be used by several interrelated research projects. These projects include: clustering, indexing, and data mining in high dimensional feature spaces; scalable audio and video over IP and wireless networks; data warehousing in mobile environments; and shared multimedia objects in mobile environments.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98606e+06</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98606e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n674" target="n676">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">CISE Research Instrumentation: Research in Computational Multimedia</data>
      <data key="e_abstract">EIA-9986057&lt;br/&gt;Kenneth Rose&lt;br/&gt;Univ. of California-SB&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Research in Computational Multimedia&lt;br/&gt;&lt;br/&gt;The departments of Computer Science and Electrical Engineering at the University of California, Santa Barbara will purchase the following infrastructure equipment: an SGI 2100 multimedia server with three client SFI 02 workstations to manage, process and stream multimedia data onto a wireless local area network with several PC&apos;s on the backbone Ethernet and mobile computers (laptop computers and handheld devices); an MPEG-2 encoder from Optivision, still and video cameras as input devices, and high resolution monitors and a color laser printer for display and presentation. This equipment will be dedicated to support research in computer and information science and engineering with the main focus on computational multimedia research related to multimedia signal processing, databases, networking and communications. The equipment will be used by several interrelated research projects. These projects include: clustering, indexing, and data mining in high dimensional feature spaces; scalable audio and video over IP and wireless networks; data warehousing in mobile environments; and shared multimedia objects in mobile environments.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98606e+06</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98606e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n674" target="n677">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">CISE Research Instrumentation: Research in Computational Multimedia</data>
      <data key="e_abstract">EIA-9986057&lt;br/&gt;Kenneth Rose&lt;br/&gt;Univ. of California-SB&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Research in Computational Multimedia&lt;br/&gt;&lt;br/&gt;The departments of Computer Science and Electrical Engineering at the University of California, Santa Barbara will purchase the following infrastructure equipment: an SGI 2100 multimedia server with three client SFI 02 workstations to manage, process and stream multimedia data onto a wireless local area network with several PC&apos;s on the backbone Ethernet and mobile computers (laptop computers and handheld devices); an MPEG-2 encoder from Optivision, still and video cameras as input devices, and high resolution monitors and a color laser printer for display and presentation. This equipment will be dedicated to support research in computer and information science and engineering with the main focus on computational multimedia research related to multimedia signal processing, databases, networking and communications. The equipment will be used by several interrelated research projects. These projects include: clustering, indexing, and data mining in high dimensional feature spaces; scalable audio and video over IP and wireless networks; data warehousing in mobile environments; and shared multimedia objects in mobile environments.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98606e+06</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98606e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n674" target="n678">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">CISE Research Instrumentation: Research in Computational Multimedia</data>
      <data key="e_abstract">EIA-9986057&lt;br/&gt;Kenneth Rose&lt;br/&gt;Univ. of California-SB&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Research in Computational Multimedia&lt;br/&gt;&lt;br/&gt;The departments of Computer Science and Electrical Engineering at the University of California, Santa Barbara will purchase the following infrastructure equipment: an SGI 2100 multimedia server with three client SFI 02 workstations to manage, process and stream multimedia data onto a wireless local area network with several PC&apos;s on the backbone Ethernet and mobile computers (laptop computers and handheld devices); an MPEG-2 encoder from Optivision, still and video cameras as input devices, and high resolution monitors and a color laser printer for display and presentation. This equipment will be dedicated to support research in computer and information science and engineering with the main focus on computational multimedia research related to multimedia signal processing, databases, networking and communications. The equipment will be used by several interrelated research projects. These projects include: clustering, indexing, and data mining in high dimensional feature spaces; scalable audio and video over IP and wireless networks; data warehousing in mobile environments; and shared multimedia objects in mobile environments.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98606e+06</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98606e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n675" target="n676">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">CISE Research Instrumentation: Research in Computational Multimedia</data>
      <data key="e_abstract">EIA-9986057&lt;br/&gt;Kenneth Rose&lt;br/&gt;Univ. of California-SB&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Research in Computational Multimedia&lt;br/&gt;&lt;br/&gt;The departments of Computer Science and Electrical Engineering at the University of California, Santa Barbara will purchase the following infrastructure equipment: an SGI 2100 multimedia server with three client SFI 02 workstations to manage, process and stream multimedia data onto a wireless local area network with several PC&apos;s on the backbone Ethernet and mobile computers (laptop computers and handheld devices); an MPEG-2 encoder from Optivision, still and video cameras as input devices, and high resolution monitors and a color laser printer for display and presentation. This equipment will be dedicated to support research in computer and information science and engineering with the main focus on computational multimedia research related to multimedia signal processing, databases, networking and communications. The equipment will be used by several interrelated research projects. These projects include: clustering, indexing, and data mining in high dimensional feature spaces; scalable audio and video over IP and wireless networks; data warehousing in mobile environments; and shared multimedia objects in mobile environments.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98606e+06</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98606e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n675" target="n677">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">CISE Research Instrumentation: Research in Computational Multimedia</data>
      <data key="e_abstract">EIA-9986057&lt;br/&gt;Kenneth Rose&lt;br/&gt;Univ. of California-SB&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Research in Computational Multimedia&lt;br/&gt;&lt;br/&gt;The departments of Computer Science and Electrical Engineering at the University of California, Santa Barbara will purchase the following infrastructure equipment: an SGI 2100 multimedia server with three client SFI 02 workstations to manage, process and stream multimedia data onto a wireless local area network with several PC&apos;s on the backbone Ethernet and mobile computers (laptop computers and handheld devices); an MPEG-2 encoder from Optivision, still and video cameras as input devices, and high resolution monitors and a color laser printer for display and presentation. This equipment will be dedicated to support research in computer and information science and engineering with the main focus on computational multimedia research related to multimedia signal processing, databases, networking and communications. The equipment will be used by several interrelated research projects. These projects include: clustering, indexing, and data mining in high dimensional feature spaces; scalable audio and video over IP and wireless networks; data warehousing in mobile environments; and shared multimedia objects in mobile environments.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98606e+06</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98606e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n675" target="n678">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">CISE Research Instrumentation: Research in Computational Multimedia</data>
      <data key="e_abstract">EIA-9986057&lt;br/&gt;Kenneth Rose&lt;br/&gt;Univ. of California-SB&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Research in Computational Multimedia&lt;br/&gt;&lt;br/&gt;The departments of Computer Science and Electrical Engineering at the University of California, Santa Barbara will purchase the following infrastructure equipment: an SGI 2100 multimedia server with three client SFI 02 workstations to manage, process and stream multimedia data onto a wireless local area network with several PC&apos;s on the backbone Ethernet and mobile computers (laptop computers and handheld devices); an MPEG-2 encoder from Optivision, still and video cameras as input devices, and high resolution monitors and a color laser printer for display and presentation. This equipment will be dedicated to support research in computer and information science and engineering with the main focus on computational multimedia research related to multimedia signal processing, databases, networking and communications. The equipment will be used by several interrelated research projects. These projects include: clustering, indexing, and data mining in high dimensional feature spaces; scalable audio and video over IP and wireless networks; data warehousing in mobile environments; and shared multimedia objects in mobile environments.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98606e+06</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98606e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n676" target="n677">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">CISE Research Instrumentation: Research in Computational Multimedia</data>
      <data key="e_abstract">EIA-9986057&lt;br/&gt;Kenneth Rose&lt;br/&gt;Univ. of California-SB&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Research in Computational Multimedia&lt;br/&gt;&lt;br/&gt;The departments of Computer Science and Electrical Engineering at the University of California, Santa Barbara will purchase the following infrastructure equipment: an SGI 2100 multimedia server with three client SFI 02 workstations to manage, process and stream multimedia data onto a wireless local area network with several PC&apos;s on the backbone Ethernet and mobile computers (laptop computers and handheld devices); an MPEG-2 encoder from Optivision, still and video cameras as input devices, and high resolution monitors and a color laser printer for display and presentation. This equipment will be dedicated to support research in computer and information science and engineering with the main focus on computational multimedia research related to multimedia signal processing, databases, networking and communications. The equipment will be used by several interrelated research projects. These projects include: clustering, indexing, and data mining in high dimensional feature spaces; scalable audio and video over IP and wireless networks; data warehousing in mobile environments; and shared multimedia objects in mobile environments.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98606e+06</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98606e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n676" target="n678">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">CISE Research Instrumentation: Research in Computational Multimedia</data>
      <data key="e_abstract">EIA-9986057&lt;br/&gt;Kenneth Rose&lt;br/&gt;Univ. of California-SB&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Research in Computational Multimedia&lt;br/&gt;&lt;br/&gt;The departments of Computer Science and Electrical Engineering at the University of California, Santa Barbara will purchase the following infrastructure equipment: an SGI 2100 multimedia server with three client SFI 02 workstations to manage, process and stream multimedia data onto a wireless local area network with several PC&apos;s on the backbone Ethernet and mobile computers (laptop computers and handheld devices); an MPEG-2 encoder from Optivision, still and video cameras as input devices, and high resolution monitors and a color laser printer for display and presentation. This equipment will be dedicated to support research in computer and information science and engineering with the main focus on computational multimedia research related to multimedia signal processing, databases, networking and communications. The equipment will be used by several interrelated research projects. These projects include: clustering, indexing, and data mining in high dimensional feature spaces; scalable audio and video over IP and wireless networks; data warehousing in mobile environments; and shared multimedia objects in mobile environments.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98606e+06</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98606e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n677" target="n678">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">CISE Research Instrumentation: Research in Computational Multimedia</data>
      <data key="e_abstract">EIA-9986057&lt;br/&gt;Kenneth Rose&lt;br/&gt;Univ. of California-SB&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Research in Computational Multimedia&lt;br/&gt;&lt;br/&gt;The departments of Computer Science and Electrical Engineering at the University of California, Santa Barbara will purchase the following infrastructure equipment: an SGI 2100 multimedia server with three client SFI 02 workstations to manage, process and stream multimedia data onto a wireless local area network with several PC&apos;s on the backbone Ethernet and mobile computers (laptop computers and handheld devices); an MPEG-2 encoder from Optivision, still and video cameras as input devices, and high resolution monitors and a color laser printer for display and presentation. This equipment will be dedicated to support research in computer and information science and engineering with the main focus on computational multimedia research related to multimedia signal processing, databases, networking and communications. The equipment will be used by several interrelated research projects. These projects include: clustering, indexing, and data mining in high dimensional feature spaces; scalable audio and video over IP and wireless networks; data warehousing in mobile environments; and shared multimedia objects in mobile environments.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">9.98606e+06</data>
      <data key="e_expirationDate">2003-07-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98606e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n306" target="n682">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Augmenting the Periphery with Large-Scale Visual Displays</data>
      <data key="e_abstract">The goal of this research is to explore and evaluate the addition of visual peripheral displays to human-computer interfaces. Through a series of laboratory experiments, in conjunction with usability evaluations and field trials of system prototypes, the PI will conduct an in-depth exploration of how peripheral information can be used effectively in an office setting. The PI and her team will design and build an interface that uses projected visual displays to extend the user&apos;s virtual workspace past the confines of his/her desktop monitor, and then use it to augment an office environment in a way that supports current working practices. The emphasis on supporting current work practice, with its resulting inclusion of desktop computers, is in sharp contrast to visions of tomorrow&apos;s world where the desktop computer is replaced by either a plethora of computing devices (ubiquitous computing) or a single personal head-worn display that superimposes information over the wearer&apos;s view of the world (wearable augmented reality). The PI posits that her approach, coupled with a set of carefully designed laboratory experiments, will lead to a deep understanding of the issues involved in deploying peripheral displays in realistic environments. This research is expected to have several important outcomes. It will lead to techniques for determining and modeling a user&apos;s working contexts, especially the ranking and clustering of documents within those contexts; these will enable the user to easily shift between tasks, quickly regaining the last state of previous efforts and maintaining an awareness of documents used in the past. This research will also lead to techniques for visually presenting peripheral information, perhaps as visual montages designed to take advantage of peripheral perception mechanisms by conveying the salient portions of a piece of information, including its surrounding context, while remaining unobtrusive. Finally, the research will provide insight into the design of a 3D peripheral space that augments the use of traditional desktop terminals, with emphasis on the division of this space into logical groups based on the relevance and familiarity of the material, and the dynamic behavior of the space as material becomes more or less relevant to the current task.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.98871e+06</data>
      <data key="e_expirationDate">2004-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98871e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n682" target="n684">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Augmenting the Periphery with Large-Scale Visual Displays</data>
      <data key="e_abstract">The goal of this research is to explore and evaluate the addition of visual peripheral displays to human-computer interfaces. Through a series of laboratory experiments, in conjunction with usability evaluations and field trials of system prototypes, the PI will conduct an in-depth exploration of how peripheral information can be used effectively in an office setting. The PI and her team will design and build an interface that uses projected visual displays to extend the user&apos;s virtual workspace past the confines of his/her desktop monitor, and then use it to augment an office environment in a way that supports current working practices. The emphasis on supporting current work practice, with its resulting inclusion of desktop computers, is in sharp contrast to visions of tomorrow&apos;s world where the desktop computer is replaced by either a plethora of computing devices (ubiquitous computing) or a single personal head-worn display that superimposes information over the wearer&apos;s view of the world (wearable augmented reality). The PI posits that her approach, coupled with a set of carefully designed laboratory experiments, will lead to a deep understanding of the issues involved in deploying peripheral displays in realistic environments. This research is expected to have several important outcomes. It will lead to techniques for determining and modeling a user&apos;s working contexts, especially the ranking and clustering of documents within those contexts; these will enable the user to easily shift between tasks, quickly regaining the last state of previous efforts and maintaining an awareness of documents used in the past. This research will also lead to techniques for visually presenting peripheral information, perhaps as visual montages designed to take advantage of peripheral perception mechanisms by conveying the salient portions of a piece of information, including its surrounding context, while remaining unobtrusive. Finally, the research will provide insight into the design of a 3D peripheral space that augments the use of traditional desktop terminals, with emphasis on the division of this space into logical groups based on the relevance and familiarity of the material, and the dynamic behavior of the space as material becomes more or less relevant to the current task.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.98871e+06</data>
      <data key="e_expirationDate">2004-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98871e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n306" target="n684">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Augmenting the Periphery with Large-Scale Visual Displays</data>
      <data key="e_abstract">The goal of this research is to explore and evaluate the addition of visual peripheral displays to human-computer interfaces. Through a series of laboratory experiments, in conjunction with usability evaluations and field trials of system prototypes, the PI will conduct an in-depth exploration of how peripheral information can be used effectively in an office setting. The PI and her team will design and build an interface that uses projected visual displays to extend the user&apos;s virtual workspace past the confines of his/her desktop monitor, and then use it to augment an office environment in a way that supports current working practices. The emphasis on supporting current work practice, with its resulting inclusion of desktop computers, is in sharp contrast to visions of tomorrow&apos;s world where the desktop computer is replaced by either a plethora of computing devices (ubiquitous computing) or a single personal head-worn display that superimposes information over the wearer&apos;s view of the world (wearable augmented reality). The PI posits that her approach, coupled with a set of carefully designed laboratory experiments, will lead to a deep understanding of the issues involved in deploying peripheral displays in realistic environments. This research is expected to have several important outcomes. It will lead to techniques for determining and modeling a user&apos;s working contexts, especially the ranking and clustering of documents within those contexts; these will enable the user to easily shift between tasks, quickly regaining the last state of previous efforts and maintaining an awareness of documents used in the past. This research will also lead to techniques for visually presenting peripheral information, perhaps as visual montages designed to take advantage of peripheral perception mechanisms by conveying the salient portions of a piece of information, including its surrounding context, while remaining unobtrusive. Finally, the research will provide insight into the design of a 3D peripheral space that augments the use of traditional desktop terminals, with emphasis on the division of this space into logical groups based on the relevance and familiarity of the material, and the dynamic behavior of the space as material becomes more or less relevant to the current task.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.98871e+06</data>
      <data key="e_expirationDate">2004-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98871e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n689" target="n690">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Decentralized Decision-Making in Complex Network Systems</data>
      <data key="e_abstract">Network systems provide the infrastructure and foundation for the functioning of today&apos;s societies and economies. They come in many forms and include physical networks such as transportation, communication, and power networks, as well as more abstract networks such as a variety of economic and financial networks and social and knowledge networks. Such network systems are characterized by decentralized decision-making, a large-scale nature, different time scales, and distinct system equilibrium concepts. This project studies complex network systems, consisting of the foundational systems of transportation and communication networks whose interplay is becoming increasingly important in the case of telecommuting, intelligent transportation systems, electronic commerce, and knowledge networks. The project aims to: develop a theory of such complex network systems, which emphasizes the individual behavior of the decision-makers and allows for the treatment of multiple criteria; construct algorithms for the solution of such systems, along with convergence analysis and computer implementation and numerical experimentation; develop visualization techniques to understand and depict the behavior, and apply the theory to complex knowledge networks; and conduct an empirical analysis. The results of the research are expected to have broad cross-disciplinary reach and to significantly add to the understanding of fundamental networks systems underlying our societies and economies for the purpose of both prediction &lt;br/&gt;and network management purposes.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">2647</data>
      <data key="e_expirationDate">2007-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">2647</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n689" target="n691">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Decentralized Decision-Making in Complex Network Systems</data>
      <data key="e_abstract">Network systems provide the infrastructure and foundation for the functioning of today&apos;s societies and economies. They come in many forms and include physical networks such as transportation, communication, and power networks, as well as more abstract networks such as a variety of economic and financial networks and social and knowledge networks. Such network systems are characterized by decentralized decision-making, a large-scale nature, different time scales, and distinct system equilibrium concepts. This project studies complex network systems, consisting of the foundational systems of transportation and communication networks whose interplay is becoming increasingly important in the case of telecommuting, intelligent transportation systems, electronic commerce, and knowledge networks. The project aims to: develop a theory of such complex network systems, which emphasizes the individual behavior of the decision-makers and allows for the treatment of multiple criteria; construct algorithms for the solution of such systems, along with convergence analysis and computer implementation and numerical experimentation; develop visualization techniques to understand and depict the behavior, and apply the theory to complex knowledge networks; and conduct an empirical analysis. The results of the research are expected to have broad cross-disciplinary reach and to significantly add to the understanding of fundamental networks systems underlying our societies and economies for the purpose of both prediction &lt;br/&gt;and network management purposes.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">2647</data>
      <data key="e_expirationDate">2007-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">2647</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n690" target="n691">
      <data key="e_effectiveDate">2000-08-01</data>
      <data key="e_title">Decentralized Decision-Making in Complex Network Systems</data>
      <data key="e_abstract">Network systems provide the infrastructure and foundation for the functioning of today&apos;s societies and economies. They come in many forms and include physical networks such as transportation, communication, and power networks, as well as more abstract networks such as a variety of economic and financial networks and social and knowledge networks. Such network systems are characterized by decentralized decision-making, a large-scale nature, different time scales, and distinct system equilibrium concepts. This project studies complex network systems, consisting of the foundational systems of transportation and communication networks whose interplay is becoming increasingly important in the case of telecommuting, intelligent transportation systems, electronic commerce, and knowledge networks. The project aims to: develop a theory of such complex network systems, which emphasizes the individual behavior of the decision-makers and allows for the treatment of multiple criteria; construct algorithms for the solution of such systems, along with convergence analysis and computer implementation and numerical experimentation; develop visualization techniques to understand and depict the behavior, and apply the theory to complex knowledge networks; and conduct an empirical analysis. The results of the research are expected to have broad cross-disciplinary reach and to significantly add to the understanding of fundamental networks systems underlying our societies and economies for the purpose of both prediction &lt;br/&gt;and network management purposes.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">2647</data>
      <data key="e_expirationDate">2007-07-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">2647</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n692" target="n693">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Technologies for Sensor-based Wireless Networks of Toys for Smart Developmental Problem-solving Environments</data>
      <data key="e_abstract">Despite enormous progress in networking and computing technologies, their application has remained restricted to conventional person-to-person and person-to-computer communication. However, the Moore&apos;s Law driven continual reduction in cost and form factor is now making it possible to imbed networking - even wireless networking - and computing capabilities not just in our PCs and laptops but also other objects. Further, a marriage of these ever tinier and cheaper processors and wireless network interfaces with emerging micro-sensors based on MEMS technology is allowing cheap sensing, processing, and communication capabilities to be unobtrusively embedded in familiar physical objects. The result is an emerging paradigm shift where the primary role of information technology would be to enhance or assist in &quot;person to physical world&quot; communication via familiar physical objects with embedded (a) micro-sensors to react to external stimuli, and (b) wireless networking and computing engines for tetherless communication with compute servers and other networked embedded objects. &lt;br/&gt;&lt;br/&gt;The proposed research seeks to explore wireless networking, middleware, and data management technologies for realizing the above vision. The problems of ad hoc structure, distributed nature, unreliable sensing, large scale/density, and novel sensor data types are characteristic of such deeply instrumented physical environments with inter-networked physical objects. This requires one to rethink current architectures, protocols, algorithms, and formalisms that were developed for different needs. Further, to provide a concrete problem domain, we propose to use and evaluate our technologies in a &quot;smart kindergarten&quot; driver application targeted at developmental problem-solving environments for early childhood education. This is a natural application as young children learn by exploring and interacting with objects such as toys in their environment. Our envisioned system would enhance the education process by providing a childhood learning environment that is individualized to each child, adapts to the context, coordinates activities of multiple children, and allows unobtrusive evaluation of the learning process by the teacher. This would be done by wirelessly-networked, sensor-enhanced toys with back-end middleware services and database techniques. &lt;br/&gt;&lt;br/&gt;The main information technology contributions of this research would be:&lt;br/&gt; Wireless protocols for networks using short-range radios, with focus on highly unstructured, dynamic, and dense networks of embedded devices, and problems of energy efficiency and quality of service needs of sensor data.&lt;br/&gt; Network architectures designed for naming, addressing, and routing by object capabilities and attributes, as opposed to id based approaches in conventional networks.&lt;br/&gt; Efficient techniques and algorithms for identifying, locating, and tracking users and objects in instrumented environments, particularly indoors.&lt;br/&gt; Middleware architecture providing services such as special communication patterns, context-aware network resource allocation and scheduling under attribute and capacity constraints, power-aware operation, media processing using shared background servers, and context discovery, tracking, and change notification.&lt;br/&gt; Data management methods to handle data from multiple heterogeneous, unreliable, noisy sensors in a highly dynamic environment, with support for real-time sensor data interpretation and fusion, and off-line mining.&lt;br/&gt; Automated mining of user profiles from sensor data, and their use in task planning and execution of actions in the instrumented environment &lt;br/&gt; Techniques for sensor-assisted automatic speech recognition of children&apos;s speech.&lt;br/&gt;&lt;br/&gt;Complementing the above will be the driver application where a Smart Kindergarten for developmental problem solving will be prototyped based on the above ideas, and evaluated in a real classroom setting. Various objects, particularly toys, will be wirelessly networked and have sensing and perhaps actuator capabilities. A wireless network, with radios and protocols suitable for handling a high density of proximate objects, will interconnect the toys to each other and to database and compute servers using a toy network middleware API. Sensors embedded in toys and worn by children will allow the database servers to discover and track context and configuration information about the children and the toys, and also orchestrate aural, visual, motion, tactile and other feedback. The system will enhance the developmental process by providing a problem-solving environment that is individualized, context adaptive, and coordinated among multiple children. It will also allow monitoring and logging for unobtrusive paper-free assessment by teacher or parent.&lt;br/&gt;&lt;br/&gt;The project team is interdisciplinary, with researchers from UCLA&apos;s CS and EE Departments for the technology component of the project, and from UCLA&apos;s Graduate </data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85773</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85773</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n320" target="n692">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Technologies for Sensor-based Wireless Networks of Toys for Smart Developmental Problem-solving Environments</data>
      <data key="e_abstract">Despite enormous progress in networking and computing technologies, their application has remained restricted to conventional person-to-person and person-to-computer communication. However, the Moore&apos;s Law driven continual reduction in cost and form factor is now making it possible to imbed networking - even wireless networking - and computing capabilities not just in our PCs and laptops but also other objects. Further, a marriage of these ever tinier and cheaper processors and wireless network interfaces with emerging micro-sensors based on MEMS technology is allowing cheap sensing, processing, and communication capabilities to be unobtrusively embedded in familiar physical objects. The result is an emerging paradigm shift where the primary role of information technology would be to enhance or assist in &quot;person to physical world&quot; communication via familiar physical objects with embedded (a) micro-sensors to react to external stimuli, and (b) wireless networking and computing engines for tetherless communication with compute servers and other networked embedded objects. &lt;br/&gt;&lt;br/&gt;The proposed research seeks to explore wireless networking, middleware, and data management technologies for realizing the above vision. The problems of ad hoc structure, distributed nature, unreliable sensing, large scale/density, and novel sensor data types are characteristic of such deeply instrumented physical environments with inter-networked physical objects. This requires one to rethink current architectures, protocols, algorithms, and formalisms that were developed for different needs. Further, to provide a concrete problem domain, we propose to use and evaluate our technologies in a &quot;smart kindergarten&quot; driver application targeted at developmental problem-solving environments for early childhood education. This is a natural application as young children learn by exploring and interacting with objects such as toys in their environment. Our envisioned system would enhance the education process by providing a childhood learning environment that is individualized to each child, adapts to the context, coordinates activities of multiple children, and allows unobtrusive evaluation of the learning process by the teacher. This would be done by wirelessly-networked, sensor-enhanced toys with back-end middleware services and database techniques. &lt;br/&gt;&lt;br/&gt;The main information technology contributions of this research would be:&lt;br/&gt; Wireless protocols for networks using short-range radios, with focus on highly unstructured, dynamic, and dense networks of embedded devices, and problems of energy efficiency and quality of service needs of sensor data.&lt;br/&gt; Network architectures designed for naming, addressing, and routing by object capabilities and attributes, as opposed to id based approaches in conventional networks.&lt;br/&gt; Efficient techniques and algorithms for identifying, locating, and tracking users and objects in instrumented environments, particularly indoors.&lt;br/&gt; Middleware architecture providing services such as special communication patterns, context-aware network resource allocation and scheduling under attribute and capacity constraints, power-aware operation, media processing using shared background servers, and context discovery, tracking, and change notification.&lt;br/&gt; Data management methods to handle data from multiple heterogeneous, unreliable, noisy sensors in a highly dynamic environment, with support for real-time sensor data interpretation and fusion, and off-line mining.&lt;br/&gt; Automated mining of user profiles from sensor data, and their use in task planning and execution of actions in the instrumented environment &lt;br/&gt; Techniques for sensor-assisted automatic speech recognition of children&apos;s speech.&lt;br/&gt;&lt;br/&gt;Complementing the above will be the driver application where a Smart Kindergarten for developmental problem solving will be prototyped based on the above ideas, and evaluated in a real classroom setting. Various objects, particularly toys, will be wirelessly networked and have sensing and perhaps actuator capabilities. A wireless network, with radios and protocols suitable for handling a high density of proximate objects, will interconnect the toys to each other and to database and compute servers using a toy network middleware API. Sensors embedded in toys and worn by children will allow the database servers to discover and track context and configuration information about the children and the toys, and also orchestrate aural, visual, motion, tactile and other feedback. The system will enhance the developmental process by providing a problem-solving environment that is individualized, context adaptive, and coordinated among multiple children. It will also allow monitoring and logging for unobtrusive paper-free assessment by teacher or parent.&lt;br/&gt;&lt;br/&gt;The project team is interdisciplinary, with researchers from UCLA&apos;s CS and EE Departments for the technology component of the project, and from UCLA&apos;s Graduate </data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85773</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85773</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n692" target="n695">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Technologies for Sensor-based Wireless Networks of Toys for Smart Developmental Problem-solving Environments</data>
      <data key="e_abstract">Despite enormous progress in networking and computing technologies, their application has remained restricted to conventional person-to-person and person-to-computer communication. However, the Moore&apos;s Law driven continual reduction in cost and form factor is now making it possible to imbed networking - even wireless networking - and computing capabilities not just in our PCs and laptops but also other objects. Further, a marriage of these ever tinier and cheaper processors and wireless network interfaces with emerging micro-sensors based on MEMS technology is allowing cheap sensing, processing, and communication capabilities to be unobtrusively embedded in familiar physical objects. The result is an emerging paradigm shift where the primary role of information technology would be to enhance or assist in &quot;person to physical world&quot; communication via familiar physical objects with embedded (a) micro-sensors to react to external stimuli, and (b) wireless networking and computing engines for tetherless communication with compute servers and other networked embedded objects. &lt;br/&gt;&lt;br/&gt;The proposed research seeks to explore wireless networking, middleware, and data management technologies for realizing the above vision. The problems of ad hoc structure, distributed nature, unreliable sensing, large scale/density, and novel sensor data types are characteristic of such deeply instrumented physical environments with inter-networked physical objects. This requires one to rethink current architectures, protocols, algorithms, and formalisms that were developed for different needs. Further, to provide a concrete problem domain, we propose to use and evaluate our technologies in a &quot;smart kindergarten&quot; driver application targeted at developmental problem-solving environments for early childhood education. This is a natural application as young children learn by exploring and interacting with objects such as toys in their environment. Our envisioned system would enhance the education process by providing a childhood learning environment that is individualized to each child, adapts to the context, coordinates activities of multiple children, and allows unobtrusive evaluation of the learning process by the teacher. This would be done by wirelessly-networked, sensor-enhanced toys with back-end middleware services and database techniques. &lt;br/&gt;&lt;br/&gt;The main information technology contributions of this research would be:&lt;br/&gt; Wireless protocols for networks using short-range radios, with focus on highly unstructured, dynamic, and dense networks of embedded devices, and problems of energy efficiency and quality of service needs of sensor data.&lt;br/&gt; Network architectures designed for naming, addressing, and routing by object capabilities and attributes, as opposed to id based approaches in conventional networks.&lt;br/&gt; Efficient techniques and algorithms for identifying, locating, and tracking users and objects in instrumented environments, particularly indoors.&lt;br/&gt; Middleware architecture providing services such as special communication patterns, context-aware network resource allocation and scheduling under attribute and capacity constraints, power-aware operation, media processing using shared background servers, and context discovery, tracking, and change notification.&lt;br/&gt; Data management methods to handle data from multiple heterogeneous, unreliable, noisy sensors in a highly dynamic environment, with support for real-time sensor data interpretation and fusion, and off-line mining.&lt;br/&gt; Automated mining of user profiles from sensor data, and their use in task planning and execution of actions in the instrumented environment &lt;br/&gt; Techniques for sensor-assisted automatic speech recognition of children&apos;s speech.&lt;br/&gt;&lt;br/&gt;Complementing the above will be the driver application where a Smart Kindergarten for developmental problem solving will be prototyped based on the above ideas, and evaluated in a real classroom setting. Various objects, particularly toys, will be wirelessly networked and have sensing and perhaps actuator capabilities. A wireless network, with radios and protocols suitable for handling a high density of proximate objects, will interconnect the toys to each other and to database and compute servers using a toy network middleware API. Sensors embedded in toys and worn by children will allow the database servers to discover and track context and configuration information about the children and the toys, and also orchestrate aural, visual, motion, tactile and other feedback. The system will enhance the developmental process by providing a problem-solving environment that is individualized, context adaptive, and coordinated among multiple children. It will also allow monitoring and logging for unobtrusive paper-free assessment by teacher or parent.&lt;br/&gt;&lt;br/&gt;The project team is interdisciplinary, with researchers from UCLA&apos;s CS and EE Departments for the technology component of the project, and from UCLA&apos;s Graduate </data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85773</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85773</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n692" target="n696">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Technologies for Sensor-based Wireless Networks of Toys for Smart Developmental Problem-solving Environments</data>
      <data key="e_abstract">Despite enormous progress in networking and computing technologies, their application has remained restricted to conventional person-to-person and person-to-computer communication. However, the Moore&apos;s Law driven continual reduction in cost and form factor is now making it possible to imbed networking - even wireless networking - and computing capabilities not just in our PCs and laptops but also other objects. Further, a marriage of these ever tinier and cheaper processors and wireless network interfaces with emerging micro-sensors based on MEMS technology is allowing cheap sensing, processing, and communication capabilities to be unobtrusively embedded in familiar physical objects. The result is an emerging paradigm shift where the primary role of information technology would be to enhance or assist in &quot;person to physical world&quot; communication via familiar physical objects with embedded (a) micro-sensors to react to external stimuli, and (b) wireless networking and computing engines for tetherless communication with compute servers and other networked embedded objects. &lt;br/&gt;&lt;br/&gt;The proposed research seeks to explore wireless networking, middleware, and data management technologies for realizing the above vision. The problems of ad hoc structure, distributed nature, unreliable sensing, large scale/density, and novel sensor data types are characteristic of such deeply instrumented physical environments with inter-networked physical objects. This requires one to rethink current architectures, protocols, algorithms, and formalisms that were developed for different needs. Further, to provide a concrete problem domain, we propose to use and evaluate our technologies in a &quot;smart kindergarten&quot; driver application targeted at developmental problem-solving environments for early childhood education. This is a natural application as young children learn by exploring and interacting with objects such as toys in their environment. Our envisioned system would enhance the education process by providing a childhood learning environment that is individualized to each child, adapts to the context, coordinates activities of multiple children, and allows unobtrusive evaluation of the learning process by the teacher. This would be done by wirelessly-networked, sensor-enhanced toys with back-end middleware services and database techniques. &lt;br/&gt;&lt;br/&gt;The main information technology contributions of this research would be:&lt;br/&gt; Wireless protocols for networks using short-range radios, with focus on highly unstructured, dynamic, and dense networks of embedded devices, and problems of energy efficiency and quality of service needs of sensor data.&lt;br/&gt; Network architectures designed for naming, addressing, and routing by object capabilities and attributes, as opposed to id based approaches in conventional networks.&lt;br/&gt; Efficient techniques and algorithms for identifying, locating, and tracking users and objects in instrumented environments, particularly indoors.&lt;br/&gt; Middleware architecture providing services such as special communication patterns, context-aware network resource allocation and scheduling under attribute and capacity constraints, power-aware operation, media processing using shared background servers, and context discovery, tracking, and change notification.&lt;br/&gt; Data management methods to handle data from multiple heterogeneous, unreliable, noisy sensors in a highly dynamic environment, with support for real-time sensor data interpretation and fusion, and off-line mining.&lt;br/&gt; Automated mining of user profiles from sensor data, and their use in task planning and execution of actions in the instrumented environment &lt;br/&gt; Techniques for sensor-assisted automatic speech recognition of children&apos;s speech.&lt;br/&gt;&lt;br/&gt;Complementing the above will be the driver application where a Smart Kindergarten for developmental problem solving will be prototyped based on the above ideas, and evaluated in a real classroom setting. Various objects, particularly toys, will be wirelessly networked and have sensing and perhaps actuator capabilities. A wireless network, with radios and protocols suitable for handling a high density of proximate objects, will interconnect the toys to each other and to database and compute servers using a toy network middleware API. Sensors embedded in toys and worn by children will allow the database servers to discover and track context and configuration information about the children and the toys, and also orchestrate aural, visual, motion, tactile and other feedback. The system will enhance the developmental process by providing a problem-solving environment that is individualized, context adaptive, and coordinated among multiple children. It will also allow monitoring and logging for unobtrusive paper-free assessment by teacher or parent.&lt;br/&gt;&lt;br/&gt;The project team is interdisciplinary, with researchers from UCLA&apos;s CS and EE Departments for the technology component of the project, and from UCLA&apos;s Graduate </data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85773</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85773</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n320" target="n693">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Technologies for Sensor-based Wireless Networks of Toys for Smart Developmental Problem-solving Environments</data>
      <data key="e_abstract">Despite enormous progress in networking and computing technologies, their application has remained restricted to conventional person-to-person and person-to-computer communication. However, the Moore&apos;s Law driven continual reduction in cost and form factor is now making it possible to imbed networking - even wireless networking - and computing capabilities not just in our PCs and laptops but also other objects. Further, a marriage of these ever tinier and cheaper processors and wireless network interfaces with emerging micro-sensors based on MEMS technology is allowing cheap sensing, processing, and communication capabilities to be unobtrusively embedded in familiar physical objects. The result is an emerging paradigm shift where the primary role of information technology would be to enhance or assist in &quot;person to physical world&quot; communication via familiar physical objects with embedded (a) micro-sensors to react to external stimuli, and (b) wireless networking and computing engines for tetherless communication with compute servers and other networked embedded objects. &lt;br/&gt;&lt;br/&gt;The proposed research seeks to explore wireless networking, middleware, and data management technologies for realizing the above vision. The problems of ad hoc structure, distributed nature, unreliable sensing, large scale/density, and novel sensor data types are characteristic of such deeply instrumented physical environments with inter-networked physical objects. This requires one to rethink current architectures, protocols, algorithms, and formalisms that were developed for different needs. Further, to provide a concrete problem domain, we propose to use and evaluate our technologies in a &quot;smart kindergarten&quot; driver application targeted at developmental problem-solving environments for early childhood education. This is a natural application as young children learn by exploring and interacting with objects such as toys in their environment. Our envisioned system would enhance the education process by providing a childhood learning environment that is individualized to each child, adapts to the context, coordinates activities of multiple children, and allows unobtrusive evaluation of the learning process by the teacher. This would be done by wirelessly-networked, sensor-enhanced toys with back-end middleware services and database techniques. &lt;br/&gt;&lt;br/&gt;The main information technology contributions of this research would be:&lt;br/&gt; Wireless protocols for networks using short-range radios, with focus on highly unstructured, dynamic, and dense networks of embedded devices, and problems of energy efficiency and quality of service needs of sensor data.&lt;br/&gt; Network architectures designed for naming, addressing, and routing by object capabilities and attributes, as opposed to id based approaches in conventional networks.&lt;br/&gt; Efficient techniques and algorithms for identifying, locating, and tracking users and objects in instrumented environments, particularly indoors.&lt;br/&gt; Middleware architecture providing services such as special communication patterns, context-aware network resource allocation and scheduling under attribute and capacity constraints, power-aware operation, media processing using shared background servers, and context discovery, tracking, and change notification.&lt;br/&gt; Data management methods to handle data from multiple heterogeneous, unreliable, noisy sensors in a highly dynamic environment, with support for real-time sensor data interpretation and fusion, and off-line mining.&lt;br/&gt; Automated mining of user profiles from sensor data, and their use in task planning and execution of actions in the instrumented environment &lt;br/&gt; Techniques for sensor-assisted automatic speech recognition of children&apos;s speech.&lt;br/&gt;&lt;br/&gt;Complementing the above will be the driver application where a Smart Kindergarten for developmental problem solving will be prototyped based on the above ideas, and evaluated in a real classroom setting. Various objects, particularly toys, will be wirelessly networked and have sensing and perhaps actuator capabilities. A wireless network, with radios and protocols suitable for handling a high density of proximate objects, will interconnect the toys to each other and to database and compute servers using a toy network middleware API. Sensors embedded in toys and worn by children will allow the database servers to discover and track context and configuration information about the children and the toys, and also orchestrate aural, visual, motion, tactile and other feedback. The system will enhance the developmental process by providing a problem-solving environment that is individualized, context adaptive, and coordinated among multiple children. It will also allow monitoring and logging for unobtrusive paper-free assessment by teacher or parent.&lt;br/&gt;&lt;br/&gt;The project team is interdisciplinary, with researchers from UCLA&apos;s CS and EE Departments for the technology component of the project, and from UCLA&apos;s Graduate </data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85773</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85773</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n693" target="n695">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Technologies for Sensor-based Wireless Networks of Toys for Smart Developmental Problem-solving Environments</data>
      <data key="e_abstract">Despite enormous progress in networking and computing technologies, their application has remained restricted to conventional person-to-person and person-to-computer communication. However, the Moore&apos;s Law driven continual reduction in cost and form factor is now making it possible to imbed networking - even wireless networking - and computing capabilities not just in our PCs and laptops but also other objects. Further, a marriage of these ever tinier and cheaper processors and wireless network interfaces with emerging micro-sensors based on MEMS technology is allowing cheap sensing, processing, and communication capabilities to be unobtrusively embedded in familiar physical objects. The result is an emerging paradigm shift where the primary role of information technology would be to enhance or assist in &quot;person to physical world&quot; communication via familiar physical objects with embedded (a) micro-sensors to react to external stimuli, and (b) wireless networking and computing engines for tetherless communication with compute servers and other networked embedded objects. &lt;br/&gt;&lt;br/&gt;The proposed research seeks to explore wireless networking, middleware, and data management technologies for realizing the above vision. The problems of ad hoc structure, distributed nature, unreliable sensing, large scale/density, and novel sensor data types are characteristic of such deeply instrumented physical environments with inter-networked physical objects. This requires one to rethink current architectures, protocols, algorithms, and formalisms that were developed for different needs. Further, to provide a concrete problem domain, we propose to use and evaluate our technologies in a &quot;smart kindergarten&quot; driver application targeted at developmental problem-solving environments for early childhood education. This is a natural application as young children learn by exploring and interacting with objects such as toys in their environment. Our envisioned system would enhance the education process by providing a childhood learning environment that is individualized to each child, adapts to the context, coordinates activities of multiple children, and allows unobtrusive evaluation of the learning process by the teacher. This would be done by wirelessly-networked, sensor-enhanced toys with back-end middleware services and database techniques. &lt;br/&gt;&lt;br/&gt;The main information technology contributions of this research would be:&lt;br/&gt; Wireless protocols for networks using short-range radios, with focus on highly unstructured, dynamic, and dense networks of embedded devices, and problems of energy efficiency and quality of service needs of sensor data.&lt;br/&gt; Network architectures designed for naming, addressing, and routing by object capabilities and attributes, as opposed to id based approaches in conventional networks.&lt;br/&gt; Efficient techniques and algorithms for identifying, locating, and tracking users and objects in instrumented environments, particularly indoors.&lt;br/&gt; Middleware architecture providing services such as special communication patterns, context-aware network resource allocation and scheduling under attribute and capacity constraints, power-aware operation, media processing using shared background servers, and context discovery, tracking, and change notification.&lt;br/&gt; Data management methods to handle data from multiple heterogeneous, unreliable, noisy sensors in a highly dynamic environment, with support for real-time sensor data interpretation and fusion, and off-line mining.&lt;br/&gt; Automated mining of user profiles from sensor data, and their use in task planning and execution of actions in the instrumented environment &lt;br/&gt; Techniques for sensor-assisted automatic speech recognition of children&apos;s speech.&lt;br/&gt;&lt;br/&gt;Complementing the above will be the driver application where a Smart Kindergarten for developmental problem solving will be prototyped based on the above ideas, and evaluated in a real classroom setting. Various objects, particularly toys, will be wirelessly networked and have sensing and perhaps actuator capabilities. A wireless network, with radios and protocols suitable for handling a high density of proximate objects, will interconnect the toys to each other and to database and compute servers using a toy network middleware API. Sensors embedded in toys and worn by children will allow the database servers to discover and track context and configuration information about the children and the toys, and also orchestrate aural, visual, motion, tactile and other feedback. The system will enhance the developmental process by providing a problem-solving environment that is individualized, context adaptive, and coordinated among multiple children. It will also allow monitoring and logging for unobtrusive paper-free assessment by teacher or parent.&lt;br/&gt;&lt;br/&gt;The project team is interdisciplinary, with researchers from UCLA&apos;s CS and EE Departments for the technology component of the project, and from UCLA&apos;s Graduate </data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85773</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85773</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n693" target="n696">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Technologies for Sensor-based Wireless Networks of Toys for Smart Developmental Problem-solving Environments</data>
      <data key="e_abstract">Despite enormous progress in networking and computing technologies, their application has remained restricted to conventional person-to-person and person-to-computer communication. However, the Moore&apos;s Law driven continual reduction in cost and form factor is now making it possible to imbed networking - even wireless networking - and computing capabilities not just in our PCs and laptops but also other objects. Further, a marriage of these ever tinier and cheaper processors and wireless network interfaces with emerging micro-sensors based on MEMS technology is allowing cheap sensing, processing, and communication capabilities to be unobtrusively embedded in familiar physical objects. The result is an emerging paradigm shift where the primary role of information technology would be to enhance or assist in &quot;person to physical world&quot; communication via familiar physical objects with embedded (a) micro-sensors to react to external stimuli, and (b) wireless networking and computing engines for tetherless communication with compute servers and other networked embedded objects. &lt;br/&gt;&lt;br/&gt;The proposed research seeks to explore wireless networking, middleware, and data management technologies for realizing the above vision. The problems of ad hoc structure, distributed nature, unreliable sensing, large scale/density, and novel sensor data types are characteristic of such deeply instrumented physical environments with inter-networked physical objects. This requires one to rethink current architectures, protocols, algorithms, and formalisms that were developed for different needs. Further, to provide a concrete problem domain, we propose to use and evaluate our technologies in a &quot;smart kindergarten&quot; driver application targeted at developmental problem-solving environments for early childhood education. This is a natural application as young children learn by exploring and interacting with objects such as toys in their environment. Our envisioned system would enhance the education process by providing a childhood learning environment that is individualized to each child, adapts to the context, coordinates activities of multiple children, and allows unobtrusive evaluation of the learning process by the teacher. This would be done by wirelessly-networked, sensor-enhanced toys with back-end middleware services and database techniques. &lt;br/&gt;&lt;br/&gt;The main information technology contributions of this research would be:&lt;br/&gt; Wireless protocols for networks using short-range radios, with focus on highly unstructured, dynamic, and dense networks of embedded devices, and problems of energy efficiency and quality of service needs of sensor data.&lt;br/&gt; Network architectures designed for naming, addressing, and routing by object capabilities and attributes, as opposed to id based approaches in conventional networks.&lt;br/&gt; Efficient techniques and algorithms for identifying, locating, and tracking users and objects in instrumented environments, particularly indoors.&lt;br/&gt; Middleware architecture providing services such as special communication patterns, context-aware network resource allocation and scheduling under attribute and capacity constraints, power-aware operation, media processing using shared background servers, and context discovery, tracking, and change notification.&lt;br/&gt; Data management methods to handle data from multiple heterogeneous, unreliable, noisy sensors in a highly dynamic environment, with support for real-time sensor data interpretation and fusion, and off-line mining.&lt;br/&gt; Automated mining of user profiles from sensor data, and their use in task planning and execution of actions in the instrumented environment &lt;br/&gt; Techniques for sensor-assisted automatic speech recognition of children&apos;s speech.&lt;br/&gt;&lt;br/&gt;Complementing the above will be the driver application where a Smart Kindergarten for developmental problem solving will be prototyped based on the above ideas, and evaluated in a real classroom setting. Various objects, particularly toys, will be wirelessly networked and have sensing and perhaps actuator capabilities. A wireless network, with radios and protocols suitable for handling a high density of proximate objects, will interconnect the toys to each other and to database and compute servers using a toy network middleware API. Sensors embedded in toys and worn by children will allow the database servers to discover and track context and configuration information about the children and the toys, and also orchestrate aural, visual, motion, tactile and other feedback. The system will enhance the developmental process by providing a problem-solving environment that is individualized, context adaptive, and coordinated among multiple children. It will also allow monitoring and logging for unobtrusive paper-free assessment by teacher or parent.&lt;br/&gt;&lt;br/&gt;The project team is interdisciplinary, with researchers from UCLA&apos;s CS and EE Departments for the technology component of the project, and from UCLA&apos;s Graduate </data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85773</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85773</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n320" target="n695">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Technologies for Sensor-based Wireless Networks of Toys for Smart Developmental Problem-solving Environments</data>
      <data key="e_abstract">Despite enormous progress in networking and computing technologies, their application has remained restricted to conventional person-to-person and person-to-computer communication. However, the Moore&apos;s Law driven continual reduction in cost and form factor is now making it possible to imbed networking - even wireless networking - and computing capabilities not just in our PCs and laptops but also other objects. Further, a marriage of these ever tinier and cheaper processors and wireless network interfaces with emerging micro-sensors based on MEMS technology is allowing cheap sensing, processing, and communication capabilities to be unobtrusively embedded in familiar physical objects. The result is an emerging paradigm shift where the primary role of information technology would be to enhance or assist in &quot;person to physical world&quot; communication via familiar physical objects with embedded (a) micro-sensors to react to external stimuli, and (b) wireless networking and computing engines for tetherless communication with compute servers and other networked embedded objects. &lt;br/&gt;&lt;br/&gt;The proposed research seeks to explore wireless networking, middleware, and data management technologies for realizing the above vision. The problems of ad hoc structure, distributed nature, unreliable sensing, large scale/density, and novel sensor data types are characteristic of such deeply instrumented physical environments with inter-networked physical objects. This requires one to rethink current architectures, protocols, algorithms, and formalisms that were developed for different needs. Further, to provide a concrete problem domain, we propose to use and evaluate our technologies in a &quot;smart kindergarten&quot; driver application targeted at developmental problem-solving environments for early childhood education. This is a natural application as young children learn by exploring and interacting with objects such as toys in their environment. Our envisioned system would enhance the education process by providing a childhood learning environment that is individualized to each child, adapts to the context, coordinates activities of multiple children, and allows unobtrusive evaluation of the learning process by the teacher. This would be done by wirelessly-networked, sensor-enhanced toys with back-end middleware services and database techniques. &lt;br/&gt;&lt;br/&gt;The main information technology contributions of this research would be:&lt;br/&gt; Wireless protocols for networks using short-range radios, with focus on highly unstructured, dynamic, and dense networks of embedded devices, and problems of energy efficiency and quality of service needs of sensor data.&lt;br/&gt; Network architectures designed for naming, addressing, and routing by object capabilities and attributes, as opposed to id based approaches in conventional networks.&lt;br/&gt; Efficient techniques and algorithms for identifying, locating, and tracking users and objects in instrumented environments, particularly indoors.&lt;br/&gt; Middleware architecture providing services such as special communication patterns, context-aware network resource allocation and scheduling under attribute and capacity constraints, power-aware operation, media processing using shared background servers, and context discovery, tracking, and change notification.&lt;br/&gt; Data management methods to handle data from multiple heterogeneous, unreliable, noisy sensors in a highly dynamic environment, with support for real-time sensor data interpretation and fusion, and off-line mining.&lt;br/&gt; Automated mining of user profiles from sensor data, and their use in task planning and execution of actions in the instrumented environment &lt;br/&gt; Techniques for sensor-assisted automatic speech recognition of children&apos;s speech.&lt;br/&gt;&lt;br/&gt;Complementing the above will be the driver application where a Smart Kindergarten for developmental problem solving will be prototyped based on the above ideas, and evaluated in a real classroom setting. Various objects, particularly toys, will be wirelessly networked and have sensing and perhaps actuator capabilities. A wireless network, with radios and protocols suitable for handling a high density of proximate objects, will interconnect the toys to each other and to database and compute servers using a toy network middleware API. Sensors embedded in toys and worn by children will allow the database servers to discover and track context and configuration information about the children and the toys, and also orchestrate aural, visual, motion, tactile and other feedback. The system will enhance the developmental process by providing a problem-solving environment that is individualized, context adaptive, and coordinated among multiple children. It will also allow monitoring and logging for unobtrusive paper-free assessment by teacher or parent.&lt;br/&gt;&lt;br/&gt;The project team is interdisciplinary, with researchers from UCLA&apos;s CS and EE Departments for the technology component of the project, and from UCLA&apos;s Graduate </data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85773</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85773</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n320" target="n696">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Technologies for Sensor-based Wireless Networks of Toys for Smart Developmental Problem-solving Environments</data>
      <data key="e_abstract">Despite enormous progress in networking and computing technologies, their application has remained restricted to conventional person-to-person and person-to-computer communication. However, the Moore&apos;s Law driven continual reduction in cost and form factor is now making it possible to imbed networking - even wireless networking - and computing capabilities not just in our PCs and laptops but also other objects. Further, a marriage of these ever tinier and cheaper processors and wireless network interfaces with emerging micro-sensors based on MEMS technology is allowing cheap sensing, processing, and communication capabilities to be unobtrusively embedded in familiar physical objects. The result is an emerging paradigm shift where the primary role of information technology would be to enhance or assist in &quot;person to physical world&quot; communication via familiar physical objects with embedded (a) micro-sensors to react to external stimuli, and (b) wireless networking and computing engines for tetherless communication with compute servers and other networked embedded objects. &lt;br/&gt;&lt;br/&gt;The proposed research seeks to explore wireless networking, middleware, and data management technologies for realizing the above vision. The problems of ad hoc structure, distributed nature, unreliable sensing, large scale/density, and novel sensor data types are characteristic of such deeply instrumented physical environments with inter-networked physical objects. This requires one to rethink current architectures, protocols, algorithms, and formalisms that were developed for different needs. Further, to provide a concrete problem domain, we propose to use and evaluate our technologies in a &quot;smart kindergarten&quot; driver application targeted at developmental problem-solving environments for early childhood education. This is a natural application as young children learn by exploring and interacting with objects such as toys in their environment. Our envisioned system would enhance the education process by providing a childhood learning environment that is individualized to each child, adapts to the context, coordinates activities of multiple children, and allows unobtrusive evaluation of the learning process by the teacher. This would be done by wirelessly-networked, sensor-enhanced toys with back-end middleware services and database techniques. &lt;br/&gt;&lt;br/&gt;The main information technology contributions of this research would be:&lt;br/&gt; Wireless protocols for networks using short-range radios, with focus on highly unstructured, dynamic, and dense networks of embedded devices, and problems of energy efficiency and quality of service needs of sensor data.&lt;br/&gt; Network architectures designed for naming, addressing, and routing by object capabilities and attributes, as opposed to id based approaches in conventional networks.&lt;br/&gt; Efficient techniques and algorithms for identifying, locating, and tracking users and objects in instrumented environments, particularly indoors.&lt;br/&gt; Middleware architecture providing services such as special communication patterns, context-aware network resource allocation and scheduling under attribute and capacity constraints, power-aware operation, media processing using shared background servers, and context discovery, tracking, and change notification.&lt;br/&gt; Data management methods to handle data from multiple heterogeneous, unreliable, noisy sensors in a highly dynamic environment, with support for real-time sensor data interpretation and fusion, and off-line mining.&lt;br/&gt; Automated mining of user profiles from sensor data, and their use in task planning and execution of actions in the instrumented environment &lt;br/&gt; Techniques for sensor-assisted automatic speech recognition of children&apos;s speech.&lt;br/&gt;&lt;br/&gt;Complementing the above will be the driver application where a Smart Kindergarten for developmental problem solving will be prototyped based on the above ideas, and evaluated in a real classroom setting. Various objects, particularly toys, will be wirelessly networked and have sensing and perhaps actuator capabilities. A wireless network, with radios and protocols suitable for handling a high density of proximate objects, will interconnect the toys to each other and to database and compute servers using a toy network middleware API. Sensors embedded in toys and worn by children will allow the database servers to discover and track context and configuration information about the children and the toys, and also orchestrate aural, visual, motion, tactile and other feedback. The system will enhance the developmental process by providing a problem-solving environment that is individualized, context adaptive, and coordinated among multiple children. It will also allow monitoring and logging for unobtrusive paper-free assessment by teacher or parent.&lt;br/&gt;&lt;br/&gt;The project team is interdisciplinary, with researchers from UCLA&apos;s CS and EE Departments for the technology component of the project, and from UCLA&apos;s Graduate </data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85773</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85773</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n695" target="n696">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Technologies for Sensor-based Wireless Networks of Toys for Smart Developmental Problem-solving Environments</data>
      <data key="e_abstract">Despite enormous progress in networking and computing technologies, their application has remained restricted to conventional person-to-person and person-to-computer communication. However, the Moore&apos;s Law driven continual reduction in cost and form factor is now making it possible to imbed networking - even wireless networking - and computing capabilities not just in our PCs and laptops but also other objects. Further, a marriage of these ever tinier and cheaper processors and wireless network interfaces with emerging micro-sensors based on MEMS technology is allowing cheap sensing, processing, and communication capabilities to be unobtrusively embedded in familiar physical objects. The result is an emerging paradigm shift where the primary role of information technology would be to enhance or assist in &quot;person to physical world&quot; communication via familiar physical objects with embedded (a) micro-sensors to react to external stimuli, and (b) wireless networking and computing engines for tetherless communication with compute servers and other networked embedded objects. &lt;br/&gt;&lt;br/&gt;The proposed research seeks to explore wireless networking, middleware, and data management technologies for realizing the above vision. The problems of ad hoc structure, distributed nature, unreliable sensing, large scale/density, and novel sensor data types are characteristic of such deeply instrumented physical environments with inter-networked physical objects. This requires one to rethink current architectures, protocols, algorithms, and formalisms that were developed for different needs. Further, to provide a concrete problem domain, we propose to use and evaluate our technologies in a &quot;smart kindergarten&quot; driver application targeted at developmental problem-solving environments for early childhood education. This is a natural application as young children learn by exploring and interacting with objects such as toys in their environment. Our envisioned system would enhance the education process by providing a childhood learning environment that is individualized to each child, adapts to the context, coordinates activities of multiple children, and allows unobtrusive evaluation of the learning process by the teacher. This would be done by wirelessly-networked, sensor-enhanced toys with back-end middleware services and database techniques. &lt;br/&gt;&lt;br/&gt;The main information technology contributions of this research would be:&lt;br/&gt; Wireless protocols for networks using short-range radios, with focus on highly unstructured, dynamic, and dense networks of embedded devices, and problems of energy efficiency and quality of service needs of sensor data.&lt;br/&gt; Network architectures designed for naming, addressing, and routing by object capabilities and attributes, as opposed to id based approaches in conventional networks.&lt;br/&gt; Efficient techniques and algorithms for identifying, locating, and tracking users and objects in instrumented environments, particularly indoors.&lt;br/&gt; Middleware architecture providing services such as special communication patterns, context-aware network resource allocation and scheduling under attribute and capacity constraints, power-aware operation, media processing using shared background servers, and context discovery, tracking, and change notification.&lt;br/&gt; Data management methods to handle data from multiple heterogeneous, unreliable, noisy sensors in a highly dynamic environment, with support for real-time sensor data interpretation and fusion, and off-line mining.&lt;br/&gt; Automated mining of user profiles from sensor data, and their use in task planning and execution of actions in the instrumented environment &lt;br/&gt; Techniques for sensor-assisted automatic speech recognition of children&apos;s speech.&lt;br/&gt;&lt;br/&gt;Complementing the above will be the driver application where a Smart Kindergarten for developmental problem solving will be prototyped based on the above ideas, and evaluated in a real classroom setting. Various objects, particularly toys, will be wirelessly networked and have sensing and perhaps actuator capabilities. A wireless network, with radios and protocols suitable for handling a high density of proximate objects, will interconnect the toys to each other and to database and compute servers using a toy network middleware API. Sensors embedded in toys and worn by children will allow the database servers to discover and track context and configuration information about the children and the toys, and also orchestrate aural, visual, motion, tactile and other feedback. The system will enhance the developmental process by providing a problem-solving environment that is individualized, context adaptive, and coordinated among multiple children. It will also allow monitoring and logging for unobtrusive paper-free assessment by teacher or parent.&lt;br/&gt;&lt;br/&gt;The project team is interdisciplinary, with researchers from UCLA&apos;s CS and EE Departments for the technology component of the project, and from UCLA&apos;s Graduate </data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85773</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85773</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n136" target="n700">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Networking Data Centers</data>
      <data key="e_abstract">This project will initiate collaboration between ELDA/ELRA and LDC that includes networking and cross-agreements between the two organizations for the production, acquisition, normalization, certification and distribution of novel language data resources for research, education and technology development. A reciprocity agreement to be negotiated between LDC and ELRA will take into account both European and American constraints on the distribution of data. The parties will implement the legal agreement in a concrete manner through the production of a large-scale broadcast news corpus encompassing data in over 45 languages, where the legal, technical, and distribution issues will be sorted out in accordance with the cross-agreement. The production process will be conducted in accordance with best practices in this area as defined by ELRA&apos;s and LDC&apos;s previous work, and in particular will take advantage of LDC&apos;s previous experience in the collection of single language broadcast news collections and Internet-based distribution of language resources. The joint undertaking will provide a concrete test case for transatlantic cooperation between the two organizations while simultaneously developing a unique resource to facilitate the other projects sponsored under this joint EC/NSF initiative.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">9.9822e+06</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.9822e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n702" target="n703">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research: Resource Allocation and Denial of Service Prevention in Active Networks</data>
      <data key="e_abstract">The Internet is used by a rapidly expanding and changing set of applications. The need for the network to evolve and even to provide application specific processing is significant. However the current network infrastructure is hard to evolve and does not readily support customizability. The goal of Active Networking [21, 3, 2] is to facilitate this evolution and customization by making the network infrastructure programmable. One way of adding programability is to allow code to be down-loaded into the routers, thus enabling the addition or modification of services. A more radical approach is to allow the packets themselves to carry programs to be executed selectively on the network&apos;s routers. Among other issues, these two approaches increase the possibility of denial of service attacks whereby a user places excessive demands on network resources in order to deny access to another user. However, they also enable new approaches to handling such attacks and to addressing the general problem of allocating resources within the network.&lt;br/&gt;&lt;br/&gt;The proposed research focuses on issues involving programmable, or active, packets. Active packets facilitate denial of service attacks in several ways. First, unlike conventional data transport packets, an active packet may require processor cycles and memory at the routers beyond those needed to simply forward the packet. Second, in general, the execution of an active packet at a router may cause more than one active packet to be transmitted from the router. Such behavior is useful, since it allows a packet to fan out across the network, but it is potentially dangerous since it can lead to an exponential growth in the resources used by a single initial packet. Experience with active packet-based systems [9, 8, 23, 22, 24] suggests that denial of service is the single biggest obstacle which must be overcome before such systems are feasible.&lt;br/&gt;&lt;br/&gt;The proposed research tackles this problem along various fronts. First, the researchers propose to design packet programming languages that make some types of behavior intrinsically impossible. For example, in PLAN [9], packet programs are guaranteed to terminate and thus can never use an un-bounded number of router cycles. The researchers will explore tradeoffs between restricting behavior in terms of resource requirements and limiting the expressibility and thus the flexibility of active packets. However, not all potentially harmful behaviors can be eliminated in this manner. Thus, on a second front, the researchers will consider mechanisms that explicitly account for a packet&apos;s resource usage in the network. For example, each packet may carry a resource bound, which is decremented as resources are used, and which triggers termination when the bound is used up. The proposed research combines both implicit and explicit mechanisms for controlling resource usage, with algorithms to control the flow of traffic into the network to decrease the likelihood of denial of service. More generally, one can envisage assessing costs to active packets that execute on congested resources. Thus, on a third front, the researchers propose to investigate mechanisms based on congestion costs to achieve more efficient resource allocations and how they can be facilitated via active packets.&lt;br/&gt;&lt;br/&gt;Three methodologies will be used to validate proposed solutions. First, the researchers will draw on mathematical modeling to motivate the benefits and investigate the characteristics of the proposed solutions. Second, the researchers will leverage expertise and past work on implementing active networks to demonstrate what is feasible to build, and explore the constraints each solution will place on eventual applications. Finally, the researchers will use network simulation to investigate systems on a scale not achievable on the experimental testbeds.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81360</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">81360</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n704" target="n705">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Support for Video Traffic in Ad Hoc Networks</data>
      <data key="e_abstract">Ad-hoc networks are a new networking paradigm, in which the network nodes create a network &quot;on demand.&quot; There is a number of characteristic attributes associated with the ad-hoc networking approach: highly dynamic network architecture (nodes join and leave the network often and without warning), totally distributed architecture, and multi-hop routing.&lt;br/&gt; Due to the high reconfiguration rate of the ad-hoc network architectures, many, if not most of the conventional routing protocols do not perform well in this type of environment. Consequently, a number of novel routing protocols, specifically suited for the ad-hoc networks, were proposed. A new Internet Engineering Task Force working group, the MANET Working Group, has been established to address the issues of routing in ad-hoc networks.&lt;br/&gt; One topic that has not been adequately covered in the context of routing in ad-hoc networks is the issue of QoS-routing for multimedia applications, in general, and the issue of routing for real-time traffic, in particular. More specifically, an ad-hoc network may undergo frequent and unpredictable changes in the network topology, which results in relatively short lifetime of the network paths. Thus, paths become frequently invalid, and, what is more of a problem, there may be little warning of a path going down. Although this might not be a substantial problem for non real-time applications, such frequent path invalidation will most often lead to severe degradation of real-time communication. Thus, a mechanism is needed that will compensate for this behavior of ad-hoc networks. &lt;br/&gt; A number of approaches have been previously proposed in the technical literature. For instance, maintaining a secondary route, so that when the primary route fails, the system can fall back onto the secondary route as soon as the failure is detected, has proven a good strategy. However, the secondary route mechanism is insufficient in many cases, as the state of paths in the network is usually highly correlated. Thus, failure of the primary path usually means that the secondary path may not be available as well. Also, the change-over time may last too long , so as to cause a perceptible degradation of the signal quality during this period.&lt;br/&gt; One characteristic of the ad-hoc networks is that there are many paths between a source and a destination. Thus, a mechanism that takes advantage of these multitude of paths is bound to perform better (i.e., in supporting QoS for real-time traffic) than the above two-path approach. Moreover, rather than selecting a single path at any time to use for a specific connection, a better scheme would be to always distribute the information among multiple paths, possibly with some correlation between the information on the various paths, so as to protect against failure of some subset of the paths. The proposed mechanism thus consists of four steps: i) discovery of multiple paths between the source and the destination nodes and evaluation of the correlation in the paths&apos; availability; ii) selection of a subset of the paths into an Active Path Set (APS), based on some &quot;goodness&apos;&apos; measures (such as the expected availability of the path, the capacity of the path, the delay and jitter of the path, etc), and a scheme that allows to evaluate these measures for the network paths; iii) a method of coding and spreading the information among the paths&lt;br/&gt;(including matching the paths with the specific requirements of a traffic type); iv) a scheme to monitor the APS paths, estimate their QoS parameters, and update the APS based on the state of the paths and their correlation.&lt;br/&gt; The above approach is general and can be applied to a variety of real-time traffic types. However, to make the study more realistic, the researchers chose video communication as the real-time test application. Thus, the researchers intend to propose a specific set of algorithms/protocols that addresses the four steps as outlined above, in the context of video communication. For instance, they will determine what are the parameters relevant to transmission of compressed video traffic over unstable paths and propose schemes to code video source into multiple correlated descriptions that can be spread over multiple paths.&lt;br/&gt; The researchers intend to integrate the above multi-path transport scheme for video traffic into a comprehensive simulation of the ad-hoc networking environment, that will include a radio propagation model, nodal mobility model, MAC protocol, and a routing algorithm (to discover the network paths). The researchers will gather performance measures from the simulation that will allow them to determine the quality of video at the application level depending on the parameters of the models used. The researchers expect to be able to answer questions, ranging from the very basic issue of viability of supporting real-time traffic in an ad-hoc networking environment to what type of routing protocol is most suitable for real-t</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81357</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">81357</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n360" target="n704">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Support for Video Traffic in Ad Hoc Networks</data>
      <data key="e_abstract">Ad-hoc networks are a new networking paradigm, in which the network nodes create a network &quot;on demand.&quot; There is a number of characteristic attributes associated with the ad-hoc networking approach: highly dynamic network architecture (nodes join and leave the network often and without warning), totally distributed architecture, and multi-hop routing.&lt;br/&gt; Due to the high reconfiguration rate of the ad-hoc network architectures, many, if not most of the conventional routing protocols do not perform well in this type of environment. Consequently, a number of novel routing protocols, specifically suited for the ad-hoc networks, were proposed. A new Internet Engineering Task Force working group, the MANET Working Group, has been established to address the issues of routing in ad-hoc networks.&lt;br/&gt; One topic that has not been adequately covered in the context of routing in ad-hoc networks is the issue of QoS-routing for multimedia applications, in general, and the issue of routing for real-time traffic, in particular. More specifically, an ad-hoc network may undergo frequent and unpredictable changes in the network topology, which results in relatively short lifetime of the network paths. Thus, paths become frequently invalid, and, what is more of a problem, there may be little warning of a path going down. Although this might not be a substantial problem for non real-time applications, such frequent path invalidation will most often lead to severe degradation of real-time communication. Thus, a mechanism is needed that will compensate for this behavior of ad-hoc networks. &lt;br/&gt; A number of approaches have been previously proposed in the technical literature. For instance, maintaining a secondary route, so that when the primary route fails, the system can fall back onto the secondary route as soon as the failure is detected, has proven a good strategy. However, the secondary route mechanism is insufficient in many cases, as the state of paths in the network is usually highly correlated. Thus, failure of the primary path usually means that the secondary path may not be available as well. Also, the change-over time may last too long , so as to cause a perceptible degradation of the signal quality during this period.&lt;br/&gt; One characteristic of the ad-hoc networks is that there are many paths between a source and a destination. Thus, a mechanism that takes advantage of these multitude of paths is bound to perform better (i.e., in supporting QoS for real-time traffic) than the above two-path approach. Moreover, rather than selecting a single path at any time to use for a specific connection, a better scheme would be to always distribute the information among multiple paths, possibly with some correlation between the information on the various paths, so as to protect against failure of some subset of the paths. The proposed mechanism thus consists of four steps: i) discovery of multiple paths between the source and the destination nodes and evaluation of the correlation in the paths&apos; availability; ii) selection of a subset of the paths into an Active Path Set (APS), based on some &quot;goodness&apos;&apos; measures (such as the expected availability of the path, the capacity of the path, the delay and jitter of the path, etc), and a scheme that allows to evaluate these measures for the network paths; iii) a method of coding and spreading the information among the paths&lt;br/&gt;(including matching the paths with the specific requirements of a traffic type); iv) a scheme to monitor the APS paths, estimate their QoS parameters, and update the APS based on the state of the paths and their correlation.&lt;br/&gt; The above approach is general and can be applied to a variety of real-time traffic types. However, to make the study more realistic, the researchers chose video communication as the real-time test application. Thus, the researchers intend to propose a specific set of algorithms/protocols that addresses the four steps as outlined above, in the context of video communication. For instance, they will determine what are the parameters relevant to transmission of compressed video traffic over unstable paths and propose schemes to code video source into multiple correlated descriptions that can be spread over multiple paths.&lt;br/&gt; The researchers intend to integrate the above multi-path transport scheme for video traffic into a comprehensive simulation of the ad-hoc networking environment, that will include a radio propagation model, nodal mobility model, MAC protocol, and a routing algorithm (to discover the network paths). The researchers will gather performance measures from the simulation that will allow them to determine the quality of video at the application level depending on the parameters of the models used. The researchers expect to be able to answer questions, ranging from the very basic issue of viability of supporting real-time traffic in an ad-hoc networking environment to what type of routing protocol is most suitable for real-t</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81357</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">81357</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n360" target="n705">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Support for Video Traffic in Ad Hoc Networks</data>
      <data key="e_abstract">Ad-hoc networks are a new networking paradigm, in which the network nodes create a network &quot;on demand.&quot; There is a number of characteristic attributes associated with the ad-hoc networking approach: highly dynamic network architecture (nodes join and leave the network often and without warning), totally distributed architecture, and multi-hop routing.&lt;br/&gt; Due to the high reconfiguration rate of the ad-hoc network architectures, many, if not most of the conventional routing protocols do not perform well in this type of environment. Consequently, a number of novel routing protocols, specifically suited for the ad-hoc networks, were proposed. A new Internet Engineering Task Force working group, the MANET Working Group, has been established to address the issues of routing in ad-hoc networks.&lt;br/&gt; One topic that has not been adequately covered in the context of routing in ad-hoc networks is the issue of QoS-routing for multimedia applications, in general, and the issue of routing for real-time traffic, in particular. More specifically, an ad-hoc network may undergo frequent and unpredictable changes in the network topology, which results in relatively short lifetime of the network paths. Thus, paths become frequently invalid, and, what is more of a problem, there may be little warning of a path going down. Although this might not be a substantial problem for non real-time applications, such frequent path invalidation will most often lead to severe degradation of real-time communication. Thus, a mechanism is needed that will compensate for this behavior of ad-hoc networks. &lt;br/&gt; A number of approaches have been previously proposed in the technical literature. For instance, maintaining a secondary route, so that when the primary route fails, the system can fall back onto the secondary route as soon as the failure is detected, has proven a good strategy. However, the secondary route mechanism is insufficient in many cases, as the state of paths in the network is usually highly correlated. Thus, failure of the primary path usually means that the secondary path may not be available as well. Also, the change-over time may last too long , so as to cause a perceptible degradation of the signal quality during this period.&lt;br/&gt; One characteristic of the ad-hoc networks is that there are many paths between a source and a destination. Thus, a mechanism that takes advantage of these multitude of paths is bound to perform better (i.e., in supporting QoS for real-time traffic) than the above two-path approach. Moreover, rather than selecting a single path at any time to use for a specific connection, a better scheme would be to always distribute the information among multiple paths, possibly with some correlation between the information on the various paths, so as to protect against failure of some subset of the paths. The proposed mechanism thus consists of four steps: i) discovery of multiple paths between the source and the destination nodes and evaluation of the correlation in the paths&apos; availability; ii) selection of a subset of the paths into an Active Path Set (APS), based on some &quot;goodness&apos;&apos; measures (such as the expected availability of the path, the capacity of the path, the delay and jitter of the path, etc), and a scheme that allows to evaluate these measures for the network paths; iii) a method of coding and spreading the information among the paths&lt;br/&gt;(including matching the paths with the specific requirements of a traffic type); iv) a scheme to monitor the APS paths, estimate their QoS parameters, and update the APS based on the state of the paths and their correlation.&lt;br/&gt; The above approach is general and can be applied to a variety of real-time traffic types. However, to make the study more realistic, the researchers chose video communication as the real-time test application. Thus, the researchers intend to propose a specific set of algorithms/protocols that addresses the four steps as outlined above, in the context of video communication. For instance, they will determine what are the parameters relevant to transmission of compressed video traffic over unstable paths and propose schemes to code video source into multiple correlated descriptions that can be spread over multiple paths.&lt;br/&gt; The researchers intend to integrate the above multi-path transport scheme for video traffic into a comprehensive simulation of the ad-hoc networking environment, that will include a radio propagation model, nodal mobility model, MAC protocol, and a routing algorithm (to discover the network paths). The researchers will gather performance measures from the simulation that will allow them to determine the quality of video at the application level depending on the parameters of the models used. The researchers expect to be able to answer questions, ranging from the very basic issue of viability of supporting real-time traffic in an ad-hoc networking environment to what type of routing protocol is most suitable for real-t</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81357</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">81357</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n707" target="n708">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Exploiting Remote Infrastructure for Mobile Information Access</data>
      <data key="e_abstract">The researchers propose to design, implement, analyze and evaluate mechanisms that will enable mobile clients to opportunistically exploit computing, data and communication resources in the fixed infrastructure to facilitate information access. A mobile client typically faces many resource challenges. These include unpredictable variation in wireless network quality; wide disparity in the availability of remote services; limitations imposed by weight and size constraints on CPU power, memory size and disk capacity; and concern for battery power consumption. The goal is to exploit remote infrastructure, such as transient caching sites or compute servers, to overcome these challenges. When such infrastructure is unavailable or would be too expensive or slow to access in terms of battery power or bandwidth, the researchers wish to be able to continue operation relying solely on local resources. The challenge is to come up with the adaptive mechanisms, algorithms and policies that can be effectively implemented on a resource-limited mobile client, yet function smoothly and seamlessly from a user&apos;s viewpoint. &lt;br/&gt; The research will span both experimentation and analysis. On the one hand, the researchers plan to design, implement, and evaluate mobile computing systems embodying innovative solutions to the challenges described above. This will give the researchers the hands-on experience and insights needed to elucidate sound design principles for this domain. In parallel, the researchers plan to conduct an analytical and modelling effort to obtain a deeper understanding of fundamental tradeoffs in adaptation, and to explore a much broader region of the design space than feasible experimentally. &lt;br/&gt; This effort will be organized into three major thrusts:&lt;br/&gt; 1.Efficient file cache management for mobile clients&lt;br/&gt; 2.Offloading computation for energy and bandwidth savings&lt;br/&gt; 3.Modelling and analysis of adaptation tradeoffs&lt;br/&gt; In the first thrust, the researchers plan to explore the use of intermediate caching sites in the infrastructure to improve the efficiency of cache management in mobile file systems. The importance of caching for mobile information access is widely acknowledged, but what specific form it should take in the mobile systems of the future remains an open question. In the second thrust, the researchers will explore techniques for mobile clients to adaptatively offload computation on servers in the infrastructure. Such offloading may be useful for improved performance as well as enhanced battery life. In the third thrust, the researchers will analytically investigate a wide range of policies for the mechanisms developed in the other thrusts. The mobile computing systems of the future are likely to encounter considerable variability in environmental conditions, user preferences, and application characteristics. Characterizing this large multi-dimensional space analytically, and understanding its implications for alternative policies will be critical to the development of successful systems.&lt;br/&gt; If successful, the research will bring mobile computing one step closer to reality. Although commercial deployment of mobile computing systems is an enormous potential market for industry, many difficult technical problems remain to be solved. Incremental efforts by industry to extend today&apos;s widely-deployed commercial off-the-shelf systems to mobile environments will not solve these problems. The researchers proposed effort, combining experimentation and analysis on a key set of challenging problems, thus represents a high-risk, high-payoff venture.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81396</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">81396</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n709" target="n710">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Algorithms for Irregular Discrete Computations on SMPs</data>
      <data key="e_abstract">Parallel computing has promised very high performance, but has delivered it only in a narrow range of applications. Exploiting parallelism at the level of large distributed-memory systems is hampered by the cost of message-passing, while shared-memory systems remain mostly small-scale. With the advent of symmetric multiprocessors (SMPs), however, shared-memory on a modest scale is becoming the standard for desktop scientific and engineering applications. Yet very little has been done yet to support effective parallel computing on an SMP beyond basic linear algebra and fast Fourier transforms. Fortunately, preliminary work by the investigators indicates that it is possible to design and implement algorithms for irregular (i.e. graph-based) computations that provide efficient, scalable performance on SMPs. This project will further develop, implement, assess, and refine those algorithms.&lt;br/&gt;&lt;br/&gt;Technically, the project will focus on science-driven problems that are graph- or geometry-based (e.g. phylogeny tees and watershed modeling) and thus involve irregular computation. It will provide three main benefits for these problems. First, it will develop new algorithms and a library of basic routines to support graph and computational geometry algorithms, leveraging insights from nearly twenty years of theoretical work on Parallel Random Access Memory (PRAM). Second, it will produce an assessment methodology for SMP-based computations using both generated test instances and real-world data, extending recent developments in algorithmic engineering and experimental algorithmics. Finally, it will provide a practical demonstration of high performance computing on desktop SMPs for scientific problems.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81404</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81404</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n405" target="n711">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Formal Methods for Robust Embedded Software</data>
      <data key="e_abstract">To overcome the limitations of current development methods for embedded software, which include limited use of secure and type-safe languages and the lack of support for formal validation techniques, this work explores the use of the modern secure language Java coupled with light-weight formal methods to establish correctness. It tailors the Java technology to fit the domain of embedded systems by placing restrictions on the Java subset, supporting provably correct-by-construction synthesis techniques for reactive control skeletons, and analysis techniques based on algorithmic as well as deductive formal verification techniques. A key goal is also to support advanced features such as dynamic software upgrades and process migration through safe implementation methods that are formally verified. It develops a new set of model-checking and program analysis tools that are demonstrated on prototype hardware/software co-designs of embedded Java processors.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81406</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81406</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n714" target="n715">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">SGER: Software Engineering Technology Watch</data>
      <data key="e_abstract">This project, carried out in close conjunction with the Software Engineering Research Center (SERC, an NSF-sponsored IUCRC Center), establishes a Software Engineering Technology Watch to track trends in Research, Technology, and the Market as relates to software engineering. The results of the these Watch activities-reports and studies-inform the respective communities about the state of the art, promising directions, and apparent failures. As part of laying the foundation for the modeling of trends, data is compiled on various aspects of the national software engineering enterprise. Example case studies would be the Internet, the Web, Java, Unix, Ada, and Linux. The activity also serves to define and refine the best strategies for collecting, analyzing, and disseminating the information, thus providing a feasibility study to determine the process to follow. Issues to address include: What cost factors guide trends, What lifecycle do technologies follow, and What aspects of technology evolution are controllable? The research develops a synthetic model of a &quot;technology trend,&quot; a synthetic comparative survey of other existing tech-watch initiatives in software engineering, an identification of relevant indices and sources of information, and a report including factual information pertaining to the evolution of the field.</data>
      <data key="e_pgm">2880</data>
      <data key="e_label">86226</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86226</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n719" target="n720">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Active Queue Management for Scalable Network Services: Theory and Internet Practice</data>
      <data key="e_abstract">Two of the largest issues facing the Internet today are the problems of (1) providing quality-of-service to applications that require some form of &quot;guarantee&quot; of bandwidth availability and/or end-to-end delay, and (2) the problem of avoiding congestion between traditional best-effort flows. Packet scheduling has been the mechanism traditionally used for quality-of-service guarantees while end-system adaptations in the transport layer have been the dominant form of congestion control. Typically, the problems of congestion control and quality-of-service have been addressed as largely independent concerns and separate mechanisms have been developed for each .&lt;br/&gt; The essence of this project is to study the interplay between congestion control and quality-of-service and the mechanisms that have been employed for realizing each. The starting point is an investigation of using active queue management(AQM) techniques in network routers to provide both congestion control and quality-of-service for IP flows. Active queue management refers to the practice of manipulating the queue of packets at an outbound network interface on a router to bias the performance of network flows. For example, discarding packets from the queue is an active queue management mechanism that is used in the RED (random early detection) congestion avoidance mechanism.&lt;br/&gt; There does not yet exist any fundamental theoretical understanding of how individual AQM mechanisms effect the performance of network flows. From the perspective of Internet service providers this problem is compounded by the fact that there is little, if any, understanding of how AQM mechanisms can be tuned to realize specific performance goals. The impact of AQM mechanisms on broader measures of network performance is unknown because a framework for analyzing AQM performance does not yet exist. This is in contrast to other approaches for quality-of-service such as packet scheduling. Packet schedulers have been carefully analyzed and frameworks exist for computing bounds on performance metrics such as delay, delay-jitter, packet loss, etc. &lt;br/&gt; The main goal of this project is to develop the theoretical underpinnings and engineering principles to guide the construction and deployment of AQM for the Internet. This requires an analytic and empirical study of AQM mechanisms for the purpose of understanding the costs, benefits, and scalability limitations of using AQM for congestion control and quality-of-service. The project also contributes to the overall understanding of principles of resource management in routers. In addition, novel AQM schemes that provide new, scalable solutions for quality of service and congestion control are developed.&lt;br/&gt;Specific objectives include the following:&lt;br/&gt; 1. Develop an analysis framework for understanding the performance of AQM mechanisms. A novel aspect is consideration of both network-centric performance metrics such as link utilization, and end-system&lt;br/&gt;or user-centric measures such as response time.&lt;br/&gt; 2. Develop and analyze novel AQM and hybrid AQM-packet scheduling schemes that realize a spectrum of quality-of-service and congestion control services. A key contribution here is a demonstration of the costs&lt;br/&gt;and benefits of using packet scheduling v. AQM for scalable implementations of services.&lt;br/&gt; 3. Demonstrate the effectiveness of the AQM schemes through a case study in managing a research network and supporting an advanced, real-time, distributed-virtual-environment application.&lt;br/&gt; The results of this project can have a significant impact on the evolution of the Internet to support new levels of service quality and control congestion while ensuring scalability for very large numbers of users and devices.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82870</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82870</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n697" target="n722">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">A QoS-based Approach to Clustering and Interclustering with a Unified Methodology for Scalability, Security, Performance, Fault-handling and Co-scheduling</data>
      <data key="e_abstract">Because improved parallel computing requires introduction of quality of service and security into cluster computing environments, the following study and research is incorporated in this effort: QoS for intracluster scheduling is studies and prototyped; Intrusion Detection Integration with a single cluster environment is studied and prototyped. Furthermore, since ubiquitous middleware offers a powerful paradigm for integrating resources in common operating environments, Proposers study CORBA interoperation with QoS and parallel computing.&lt;br/&gt;&lt;br/&gt;Several specific themes facilitate new science in this work. Background study on existing QoS-oriented research for LANs and clusters are brought to bear on the current work, including previous work of Proposers involving RSVP and cluster admission controls. Furthermore, lower middleware for delivering admission-based messaging for clusters is prototyped on a Myrinet SAN (single cluster configuration). A high-quality implementation of the MPI/RT standard messaging middleware is integrated with QoS-based middleware. CORBA is studied as a possible parallel processing notation and environment, including potential for peer-to-peer constructs such as in MPI. Intrusion detection methodologies are converted for use in SAN-based settings, such as Myrinet. Finally, CORBA uses in hard realtime settings are considered. New middleware prototyped in this effort are promulgated through the worldwide web.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">9.98852e+06</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98852e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n719" target="n725">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Rate-based Scheduling Technology for Latency-Sensitive Graphics Applications</data>
      <data key="e_abstract">Highly interactive graphics applications have stringent latency requirements to ensure a compelling experience. These requirements are usually met by overprovisioning hardware resources and are validated through ad-hoc testing. Recent advances in real-time scheduling technology could serve as the&lt;br/&gt;basis for a more analytical approach. Unfortunately, conventional real-time scheduling disciplines are quite rigid and are usually implemented only within special-purpose real-time operating systems. For these reasons, real-time scheduling techniques have been largely ignored within the graphics community.&lt;br/&gt;&lt;br/&gt;In this project, a real-time scheduling and analysis framework will be developed for latency-sensitive graphics applications. This framework will extend recent work on rate-based processor scheduling. The proposed framework will allow timing requirements to be specified as average rates and will be highly tolerant of any deviance from specified rates. &lt;br/&gt;&lt;br/&gt;The proposed rate-based framework will be evaluated through research involving the nanoManipulator system (www.cs.unc.edu/Research/nano). The nanoManipulator couples a scanning tunneling microscope or atomic force microscope to a virtual-reality graphics display and a haptic interface to provide a telepresence system. The existing nanoManipulator is not multi-threaded, and thus the system is difficult to modify and analyze. In this project, a fully multi-threaded version of the nanoManipulator will be developed using the&lt;br/&gt;proposed framework.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82866</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82866</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n719" target="n726">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Rate-based Scheduling Technology for Latency-Sensitive Graphics Applications</data>
      <data key="e_abstract">Highly interactive graphics applications have stringent latency requirements to ensure a compelling experience. These requirements are usually met by overprovisioning hardware resources and are validated through ad-hoc testing. Recent advances in real-time scheduling technology could serve as the&lt;br/&gt;basis for a more analytical approach. Unfortunately, conventional real-time scheduling disciplines are quite rigid and are usually implemented only within special-purpose real-time operating systems. For these reasons, real-time scheduling techniques have been largely ignored within the graphics community.&lt;br/&gt;&lt;br/&gt;In this project, a real-time scheduling and analysis framework will be developed for latency-sensitive graphics applications. This framework will extend recent work on rate-based processor scheduling. The proposed framework will allow timing requirements to be specified as average rates and will be highly tolerant of any deviance from specified rates. &lt;br/&gt;&lt;br/&gt;The proposed rate-based framework will be evaluated through research involving the nanoManipulator system (www.cs.unc.edu/Research/nano). The nanoManipulator couples a scanning tunneling microscope or atomic force microscope to a virtual-reality graphics display and a haptic interface to provide a telepresence system. The existing nanoManipulator is not multi-threaded, and thus the system is difficult to modify and analyze. In this project, a fully multi-threaded version of the nanoManipulator will be developed using the&lt;br/&gt;proposed framework.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82866</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82866</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n719" target="n727">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Rate-based Scheduling Technology for Latency-Sensitive Graphics Applications</data>
      <data key="e_abstract">Highly interactive graphics applications have stringent latency requirements to ensure a compelling experience. These requirements are usually met by overprovisioning hardware resources and are validated through ad-hoc testing. Recent advances in real-time scheduling technology could serve as the&lt;br/&gt;basis for a more analytical approach. Unfortunately, conventional real-time scheduling disciplines are quite rigid and are usually implemented only within special-purpose real-time operating systems. For these reasons, real-time scheduling techniques have been largely ignored within the graphics community.&lt;br/&gt;&lt;br/&gt;In this project, a real-time scheduling and analysis framework will be developed for latency-sensitive graphics applications. This framework will extend recent work on rate-based processor scheduling. The proposed framework will allow timing requirements to be specified as average rates and will be highly tolerant of any deviance from specified rates. &lt;br/&gt;&lt;br/&gt;The proposed rate-based framework will be evaluated through research involving the nanoManipulator system (www.cs.unc.edu/Research/nano). The nanoManipulator couples a scanning tunneling microscope or atomic force microscope to a virtual-reality graphics display and a haptic interface to provide a telepresence system. The existing nanoManipulator is not multi-threaded, and thus the system is difficult to modify and analyze. In this project, a fully multi-threaded version of the nanoManipulator will be developed using the&lt;br/&gt;proposed framework.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82866</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82866</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n725" target="n726">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Rate-based Scheduling Technology for Latency-Sensitive Graphics Applications</data>
      <data key="e_abstract">Highly interactive graphics applications have stringent latency requirements to ensure a compelling experience. These requirements are usually met by overprovisioning hardware resources and are validated through ad-hoc testing. Recent advances in real-time scheduling technology could serve as the&lt;br/&gt;basis for a more analytical approach. Unfortunately, conventional real-time scheduling disciplines are quite rigid and are usually implemented only within special-purpose real-time operating systems. For these reasons, real-time scheduling techniques have been largely ignored within the graphics community.&lt;br/&gt;&lt;br/&gt;In this project, a real-time scheduling and analysis framework will be developed for latency-sensitive graphics applications. This framework will extend recent work on rate-based processor scheduling. The proposed framework will allow timing requirements to be specified as average rates and will be highly tolerant of any deviance from specified rates. &lt;br/&gt;&lt;br/&gt;The proposed rate-based framework will be evaluated through research involving the nanoManipulator system (www.cs.unc.edu/Research/nano). The nanoManipulator couples a scanning tunneling microscope or atomic force microscope to a virtual-reality graphics display and a haptic interface to provide a telepresence system. The existing nanoManipulator is not multi-threaded, and thus the system is difficult to modify and analyze. In this project, a fully multi-threaded version of the nanoManipulator will be developed using the&lt;br/&gt;proposed framework.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82866</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82866</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n725" target="n727">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Rate-based Scheduling Technology for Latency-Sensitive Graphics Applications</data>
      <data key="e_abstract">Highly interactive graphics applications have stringent latency requirements to ensure a compelling experience. These requirements are usually met by overprovisioning hardware resources and are validated through ad-hoc testing. Recent advances in real-time scheduling technology could serve as the&lt;br/&gt;basis for a more analytical approach. Unfortunately, conventional real-time scheduling disciplines are quite rigid and are usually implemented only within special-purpose real-time operating systems. For these reasons, real-time scheduling techniques have been largely ignored within the graphics community.&lt;br/&gt;&lt;br/&gt;In this project, a real-time scheduling and analysis framework will be developed for latency-sensitive graphics applications. This framework will extend recent work on rate-based processor scheduling. The proposed framework will allow timing requirements to be specified as average rates and will be highly tolerant of any deviance from specified rates. &lt;br/&gt;&lt;br/&gt;The proposed rate-based framework will be evaluated through research involving the nanoManipulator system (www.cs.unc.edu/Research/nano). The nanoManipulator couples a scanning tunneling microscope or atomic force microscope to a virtual-reality graphics display and a haptic interface to provide a telepresence system. The existing nanoManipulator is not multi-threaded, and thus the system is difficult to modify and analyze. In this project, a fully multi-threaded version of the nanoManipulator will be developed using the&lt;br/&gt;proposed framework.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82866</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82866</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n726" target="n727">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Rate-based Scheduling Technology for Latency-Sensitive Graphics Applications</data>
      <data key="e_abstract">Highly interactive graphics applications have stringent latency requirements to ensure a compelling experience. These requirements are usually met by overprovisioning hardware resources and are validated through ad-hoc testing. Recent advances in real-time scheduling technology could serve as the&lt;br/&gt;basis for a more analytical approach. Unfortunately, conventional real-time scheduling disciplines are quite rigid and are usually implemented only within special-purpose real-time operating systems. For these reasons, real-time scheduling techniques have been largely ignored within the graphics community.&lt;br/&gt;&lt;br/&gt;In this project, a real-time scheduling and analysis framework will be developed for latency-sensitive graphics applications. This framework will extend recent work on rate-based processor scheduling. The proposed framework will allow timing requirements to be specified as average rates and will be highly tolerant of any deviance from specified rates. &lt;br/&gt;&lt;br/&gt;The proposed rate-based framework will be evaluated through research involving the nanoManipulator system (www.cs.unc.edu/Research/nano). The nanoManipulator couples a scanning tunneling microscope or atomic force microscope to a virtual-reality graphics display and a haptic interface to provide a telepresence system. The existing nanoManipulator is not multi-threaded, and thus the system is difficult to modify and analyze. In this project, a fully multi-threaded version of the nanoManipulator will be developed using the&lt;br/&gt;proposed framework.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82866</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82866</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n729" target="n730">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">A Paradigm Shift in Program Analysis and Transformation via Intersection and Union Types</data>
      <data key="e_abstract">PI: Kfoury, Assaf J.&lt;br/&gt;Proposal Number: 9988529&lt;br/&gt;Institution: Boston University&lt;br/&gt;&lt;br/&gt;The proposed research is, broadly speaking, in type theory and rewriting theory, separately and in com-bination, motivated by issues of design and implementation of programming languages. Topics of special interest include: automated type inference, expressiveness of language features, efficiency of implementations, reliability, and analysis of tradeoffs between all of the preceding.&lt;br/&gt;&lt;br/&gt;Building on past research by the 2 principal investigators and their collaborators in typed lambda calculi, functional languages and rewriting systems, the proposed research will extend to richer typed lambda calculi, mostly based on intersection and union types, towards supporting better design and implementation of higher-order typed languages. This is a shift from the use of universal and existential types in addressing the same issues in the past. The proposed shift is justified by several complications resulting from the use of universal types, discovered by the 2 principal investigators and many other researchers since the early 1990&apos;s; by contrast, very recent research shows that these complications are not encountered in typed lambda calculi based on intersection types.&lt;br/&gt;&lt;br/&gt;The ultimate goal of the proposed research is to provide a rigorous and formal foundation for the imple-mentation of a type-directed and flow-directed compiler using an explicitly typed intermediate language. Such a compiler will observe the invariant that at each stage of the compilation process the intermediate representation of the program will be well typed. A typed intermediate language is currently developed, based on a design to facilitate both flow-directed and type-directed optimization.</data>
      <data key="e_pgm">2880</data>
      <data key="e_label">9.98853e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98853e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n731" target="n732">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: On a Virtual Laboratory for Network Engineering Educational Programs</data>
      <data key="e_abstract">EIA-0081761&lt;br/&gt;Zhao, Wei&lt;br/&gt;Texas Engineering Experiment Station&lt;br/&gt;&lt;br/&gt;ITR: On a Virtual Laboratory for Network Engineering Educational Programs&lt;br/&gt;&lt;br/&gt;The primary objective of this proposed research is to investigate issues&lt;br/&gt;involved in building a virtual laboratory for network engineering&lt;br/&gt;educational programs. Different from a conventional teaching laboratory,&lt;br/&gt;the proposed virtual laboratory will not require students to physically&lt;br/&gt;present in the laboratory rooms, rather they conduct their experiments via&lt;br/&gt;remote network connections, such as the World Wide Web. The intent is to&lt;br/&gt;demonstrate that this kind of virtual laboratory facilitates the training&lt;br/&gt;of IT workers in a significant way: it will reduce the cost, increase the&lt;br/&gt;facility utilization, and improve the quality of IT courses that are&lt;br/&gt;experiment-oriented. This project will focus on addressing technical&lt;br/&gt;challenges that are critical for successful implementation and operation of&lt;br/&gt;such a virtual laboratory. Configuration and scheduling techniques will be&lt;br/&gt;studied that provide students with remote but real time access to the&lt;br/&gt;equipment during their experiments. These kinds of configurations must be&lt;br/&gt;carried out in an efficient and flexible manner so that a large number of&lt;br/&gt;students can do their experiments simultaneously. Access control methods&lt;br/&gt;will also be investigated that deal with the degree of control over network&lt;br/&gt;components given to the user. The efficacy of diverse access control&lt;br/&gt;methods will be examined over a range of implemented exercises. The aim is&lt;br/&gt;to build a virtual laboratory that allows an instructor to choose the&lt;br/&gt;interplay of operating system support, experiment semantics, and network&lt;br/&gt;support to best suit the learning environment.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81761</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81761</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n731" target="n733">
      <data key="e_effectiveDate">None</data>
      <data key="e_title">None</data>
      <data key="e_abstract">None</data>
      <data key="e_pgm">None</data>
      <data key="e_expirationDate">None</data>
      <data key="e_div">None</data>
      <data key="e_dir">None</data>
    </edge>
    <edge source="n731" target="n733">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: On a Virtual Laboratory for Network Engineering Educational Programs</data>
      <data key="e_abstract">EIA-0081761&lt;br/&gt;Zhao, Wei&lt;br/&gt;Texas Engineering Experiment Station&lt;br/&gt;&lt;br/&gt;ITR: On a Virtual Laboratory for Network Engineering Educational Programs&lt;br/&gt;&lt;br/&gt;The primary objective of this proposed research is to investigate issues&lt;br/&gt;involved in building a virtual laboratory for network engineering&lt;br/&gt;educational programs. Different from a conventional teaching laboratory,&lt;br/&gt;the proposed virtual laboratory will not require students to physically&lt;br/&gt;present in the laboratory rooms, rather they conduct their experiments via&lt;br/&gt;remote network connections, such as the World Wide Web. The intent is to&lt;br/&gt;demonstrate that this kind of virtual laboratory facilitates the training&lt;br/&gt;of IT workers in a significant way: it will reduce the cost, increase the&lt;br/&gt;facility utilization, and improve the quality of IT courses that are&lt;br/&gt;experiment-oriented. This project will focus on addressing technical&lt;br/&gt;challenges that are critical for successful implementation and operation of&lt;br/&gt;such a virtual laboratory. Configuration and scheduling techniques will be&lt;br/&gt;studied that provide students with remote but real time access to the&lt;br/&gt;equipment during their experiments. These kinds of configurations must be&lt;br/&gt;carried out in an efficient and flexible manner so that a large number of&lt;br/&gt;students can do their experiments simultaneously. Access control methods&lt;br/&gt;will also be investigated that deal with the degree of control over network&lt;br/&gt;components given to the user. The efficacy of diverse access control&lt;br/&gt;methods will be examined over a range of implemented exercises. The aim is&lt;br/&gt;to build a virtual laboratory that allows an instructor to choose the&lt;br/&gt;interplay of operating system support, experiment semantics, and network&lt;br/&gt;support to best suit the learning environment.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81761</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81761</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n732" target="n733">
      <data key="e_effectiveDate">None</data>
      <data key="e_title">None</data>
      <data key="e_abstract">None</data>
      <data key="e_pgm">None</data>
      <data key="e_expirationDate">None</data>
      <data key="e_div">None</data>
      <data key="e_dir">None</data>
    </edge>
    <edge source="n732" target="n733">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: On a Virtual Laboratory for Network Engineering Educational Programs</data>
      <data key="e_abstract">EIA-0081761&lt;br/&gt;Zhao, Wei&lt;br/&gt;Texas Engineering Experiment Station&lt;br/&gt;&lt;br/&gt;ITR: On a Virtual Laboratory for Network Engineering Educational Programs&lt;br/&gt;&lt;br/&gt;The primary objective of this proposed research is to investigate issues&lt;br/&gt;involved in building a virtual laboratory for network engineering&lt;br/&gt;educational programs. Different from a conventional teaching laboratory,&lt;br/&gt;the proposed virtual laboratory will not require students to physically&lt;br/&gt;present in the laboratory rooms, rather they conduct their experiments via&lt;br/&gt;remote network connections, such as the World Wide Web. The intent is to&lt;br/&gt;demonstrate that this kind of virtual laboratory facilitates the training&lt;br/&gt;of IT workers in a significant way: it will reduce the cost, increase the&lt;br/&gt;facility utilization, and improve the quality of IT courses that are&lt;br/&gt;experiment-oriented. This project will focus on addressing technical&lt;br/&gt;challenges that are critical for successful implementation and operation of&lt;br/&gt;such a virtual laboratory. Configuration and scheduling techniques will be&lt;br/&gt;studied that provide students with remote but real time access to the&lt;br/&gt;equipment during their experiments. These kinds of configurations must be&lt;br/&gt;carried out in an efficient and flexible manner so that a large number of&lt;br/&gt;students can do their experiments simultaneously. Access control methods&lt;br/&gt;will also be investigated that deal with the degree of control over network&lt;br/&gt;components given to the user. The efficacy of diverse access control&lt;br/&gt;methods will be examined over a range of implemented exercises. The aim is&lt;br/&gt;to build a virtual laboratory that allows an instructor to choose the&lt;br/&gt;interplay of operating system support, experiment semantics, and network&lt;br/&gt;support to best suit the learning environment.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81761</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81761</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n733" target="n733">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: On a Virtual Laboratory for Network Engineering Educational Programs</data>
      <data key="e_abstract">EIA-0081761&lt;br/&gt;Zhao, Wei&lt;br/&gt;Texas Engineering Experiment Station&lt;br/&gt;&lt;br/&gt;ITR: On a Virtual Laboratory for Network Engineering Educational Programs&lt;br/&gt;&lt;br/&gt;The primary objective of this proposed research is to investigate issues&lt;br/&gt;involved in building a virtual laboratory for network engineering&lt;br/&gt;educational programs. Different from a conventional teaching laboratory,&lt;br/&gt;the proposed virtual laboratory will not require students to physically&lt;br/&gt;present in the laboratory rooms, rather they conduct their experiments via&lt;br/&gt;remote network connections, such as the World Wide Web. The intent is to&lt;br/&gt;demonstrate that this kind of virtual laboratory facilitates the training&lt;br/&gt;of IT workers in a significant way: it will reduce the cost, increase the&lt;br/&gt;facility utilization, and improve the quality of IT courses that are&lt;br/&gt;experiment-oriented. This project will focus on addressing technical&lt;br/&gt;challenges that are critical for successful implementation and operation of&lt;br/&gt;such a virtual laboratory. Configuration and scheduling techniques will be&lt;br/&gt;studied that provide students with remote but real time access to the&lt;br/&gt;equipment during their experiments. These kinds of configurations must be&lt;br/&gt;carried out in an efficient and flexible manner so that a large number of&lt;br/&gt;students can do their experiments simultaneously. Access control methods&lt;br/&gt;will also be investigated that deal with the degree of control over network&lt;br/&gt;components given to the user. The efficacy of diverse access control&lt;br/&gt;methods will be examined over a range of implemented exercises. The aim is&lt;br/&gt;to build a virtual laboratory that allows an instructor to choose the&lt;br/&gt;interplay of operating system support, experiment semantics, and network&lt;br/&gt;support to best suit the learning environment.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81761</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81761</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n740" target="n741">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Interactions of Multithreaded Computing and Operating System Design</data>
      <data key="e_abstract">Proposal Number: 0085670&lt;br/&gt;Title: Interactions of Multithreaded Computing and Operating System Design&lt;br/&gt;PIs: Henry M. Levy and Susan J. Eggers&lt;br/&gt;&lt;br/&gt;Abstract:&lt;br/&gt;&lt;br/&gt;Over the next decade, highly scalable computer systems consisting of&lt;br/&gt;hundreds or thousands of nodes will become necessary to meet the&lt;br/&gt;demands of emerging global applications, such as Web and database&lt;br/&gt;servers used for Internet commerce. These applications must be&lt;br/&gt;multithreaded to handle the thousands of simultaneous requests. One&lt;br/&gt;promising technology for this scalable server domain is the use of&lt;br/&gt;multithreaded processors; for example, Compaq has announced&lt;br/&gt;Simultaneous Multithreading (SMT) support for its Alpha processor &lt;br/&gt;in the 2002/2003 time frame.&lt;br/&gt;&lt;br/&gt;Unfortunately, no research has been conducted on multithreading&lt;br/&gt;support for operating systems or on how to structure operating systems&lt;br/&gt;for multithreaded CPUs. Yet the OS may be crucial in this environment;&lt;br/&gt;e.g., measurements show that a loaded web server can spend 75% of its&lt;br/&gt;time in the OS. The proposed research will develop the software&lt;br/&gt;environment to complement the multithreaded hardware environment&lt;br/&gt;provided by SMT and other fine-grained parallel processors. The&lt;br/&gt;research sits squarely between architecture and operating systems,&lt;br/&gt;examining (1) the design and performance of multithreaded processors&lt;br/&gt;to support OS needs, and (2) the structure of operating systems in&lt;br/&gt;light of the capabilities of multithreaded processors. The research&lt;br/&gt;will focus on these questions in the domain of highly parallel,&lt;br/&gt;request-driven workloads, such as Web servers and database servers.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85670</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85670</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n742" target="n743">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n742" target="n744">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n742" target="n745">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n742" target="n746">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n742" target="n747">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n742" target="n748">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n743" target="n744">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n743" target="n745">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n743" target="n746">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n743" target="n747">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n743" target="n748">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n744" target="n745">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n744" target="n746">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n744" target="n747">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n744" target="n748">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n745" target="n746">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n745" target="n747">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n745" target="n748">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n746" target="n747">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n746" target="n748">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n747" target="n748">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Infrastructure for Intelligent Mobile Information Systems</data>
      <data key="e_abstract">EIA-0002217&lt;br/&gt;Kim, Jung H.&lt;br/&gt;North Carolina A&amp;T State University&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for Intelligent Mobile Information Systems&lt;br/&gt;&lt;br/&gt;This proposal involves the creation of an infrastructure for research and graduate education in Intelligent Mobile Information Systems (IMIS) at North Carolina A&amp;T State University. The infrastructure will support the efforts by the Department of Computer Science, the Department of Electrical Engineering and t he College of Engineering to enhance the effectiveness of North Carolina A&amp;T State University as a pipeline to graduate study by African-Americans in computer science and computer engineering. The proposal outlines an aggressive mentoring program oriented toward encouraging African-American students to continue to graduate school. This will involve identifying undergraduate students with high academic performance and recruiting them to participate in the IMIS research program.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2217</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">2217</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n749" target="n750">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Digital Government: Testbed for High-Speed &apos;End-to-End&apos; Communications in Support of Comprehensive Emergency Management</data>
      <data key="e_abstract">EIA-9983463&lt;br/&gt;Bostian Charles W.&lt;br/&gt;Virginia Polytechnic Institute&lt;br/&gt;&lt;br/&gt;Digital Government: Testbed for High Speed &quot;End to End&quot; Communications in Support of Comprehensive Emergency Management&lt;br/&gt;&lt;br/&gt;This project will explore the use of rapidly deployable &quot;last mile&quot; aerostat-based wireless high-speed communications in the environment of emergency management. The technology base will be Local Multipoint Distribution Service radio. An important element of the work will be rapid deployment, so equipment must be portable and able to access remote databases and GIS engines; a base station and two field units will be constructed. The primary partner agency initially will be the National Response Center, which is responsible for responding to chemical spills, toxic agent release, and related environmental problems. Other agencies, such as the Federal Emergency Management Agency, will be targets for future partnerships</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98346e+06</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98346e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n752" target="n753">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: RONEE - An Integrated Engineering Method and Tool Environment for Reliable Object-oriented Real-time Distributed Systems</data>
      <data key="e_abstract">Two major goals are:&lt;br/&gt;(1) To establish a new-generation integrated methodology for real- time (RT) distributed programming and software engineering on the basis of the recent developments in object-oriented (OO) RT&lt;br/&gt; distributed programming; and &lt;br/&gt;(2) To develop an integrated tool-set named the Real-time Object Network Engineering Environment (RONEE), which will bring about a quantum jump in the system engineers&apos; productivity in&lt;br/&gt; constructing distributed RT application systems.&lt;br/&gt;&lt;br/&gt;The research base of this multi-nation researcher team includes (1) a new high-level style of RT OO distributed programming called the TMO (Time-triggered Message-triggered Object) programming, (2)&lt;br/&gt;middleware supporting RT objects such as TAO object request broker and TMOSM, (3) OS and middleware support for fault-tolerant RT distributed computing such as ROAFTS and TTP OS, (4) frameworks&lt;br/&gt;such as RT UML and ACSR for formal specification, etc.&lt;br/&gt;&lt;br/&gt;Specific research tasks include integrations of cornerstone&lt;br/&gt;techniques and development of RONEE for:&lt;br/&gt;(1) High-level distributed RT programming based on C++ and JAVA;&lt;br/&gt;(2) Formal multi-level specification of requirements and designs;&lt;br/&gt;(3) Efficient execution of RT objects, stationary and mobile;&lt;br/&gt;(4) Fault-tolerant computing with distributed RT objects;&lt;br/&gt;(5) Verification of timing designs;&lt;br/&gt;(6) Distributed RT simulation and 3D visualization of application&lt;br/&gt;environments to support software validation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86147</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86147</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n752" target="n754">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: RONEE - An Integrated Engineering Method and Tool Environment for Reliable Object-oriented Real-time Distributed Systems</data>
      <data key="e_abstract">Two major goals are:&lt;br/&gt;(1) To establish a new-generation integrated methodology for real- time (RT) distributed programming and software engineering on the basis of the recent developments in object-oriented (OO) RT&lt;br/&gt; distributed programming; and &lt;br/&gt;(2) To develop an integrated tool-set named the Real-time Object Network Engineering Environment (RONEE), which will bring about a quantum jump in the system engineers&apos; productivity in&lt;br/&gt; constructing distributed RT application systems.&lt;br/&gt;&lt;br/&gt;The research base of this multi-nation researcher team includes (1) a new high-level style of RT OO distributed programming called the TMO (Time-triggered Message-triggered Object) programming, (2)&lt;br/&gt;middleware supporting RT objects such as TAO object request broker and TMOSM, (3) OS and middleware support for fault-tolerant RT distributed computing such as ROAFTS and TTP OS, (4) frameworks&lt;br/&gt;such as RT UML and ACSR for formal specification, etc.&lt;br/&gt;&lt;br/&gt;Specific research tasks include integrations of cornerstone&lt;br/&gt;techniques and development of RONEE for:&lt;br/&gt;(1) High-level distributed RT programming based on C++ and JAVA;&lt;br/&gt;(2) Formal multi-level specification of requirements and designs;&lt;br/&gt;(3) Efficient execution of RT objects, stationary and mobile;&lt;br/&gt;(4) Fault-tolerant computing with distributed RT objects;&lt;br/&gt;(5) Verification of timing designs;&lt;br/&gt;(6) Distributed RT simulation and 3D visualization of application&lt;br/&gt;environments to support software validation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86147</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86147</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n752" target="n755">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: RONEE - An Integrated Engineering Method and Tool Environment for Reliable Object-oriented Real-time Distributed Systems</data>
      <data key="e_abstract">Two major goals are:&lt;br/&gt;(1) To establish a new-generation integrated methodology for real- time (RT) distributed programming and software engineering on the basis of the recent developments in object-oriented (OO) RT&lt;br/&gt; distributed programming; and &lt;br/&gt;(2) To develop an integrated tool-set named the Real-time Object Network Engineering Environment (RONEE), which will bring about a quantum jump in the system engineers&apos; productivity in&lt;br/&gt; constructing distributed RT application systems.&lt;br/&gt;&lt;br/&gt;The research base of this multi-nation researcher team includes (1) a new high-level style of RT OO distributed programming called the TMO (Time-triggered Message-triggered Object) programming, (2)&lt;br/&gt;middleware supporting RT objects such as TAO object request broker and TMOSM, (3) OS and middleware support for fault-tolerant RT distributed computing such as ROAFTS and TTP OS, (4) frameworks&lt;br/&gt;such as RT UML and ACSR for formal specification, etc.&lt;br/&gt;&lt;br/&gt;Specific research tasks include integrations of cornerstone&lt;br/&gt;techniques and development of RONEE for:&lt;br/&gt;(1) High-level distributed RT programming based on C++ and JAVA;&lt;br/&gt;(2) Formal multi-level specification of requirements and designs;&lt;br/&gt;(3) Efficient execution of RT objects, stationary and mobile;&lt;br/&gt;(4) Fault-tolerant computing with distributed RT objects;&lt;br/&gt;(5) Verification of timing designs;&lt;br/&gt;(6) Distributed RT simulation and 3D visualization of application&lt;br/&gt;environments to support software validation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86147</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86147</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n753" target="n754">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: RONEE - An Integrated Engineering Method and Tool Environment for Reliable Object-oriented Real-time Distributed Systems</data>
      <data key="e_abstract">Two major goals are:&lt;br/&gt;(1) To establish a new-generation integrated methodology for real- time (RT) distributed programming and software engineering on the basis of the recent developments in object-oriented (OO) RT&lt;br/&gt; distributed programming; and &lt;br/&gt;(2) To develop an integrated tool-set named the Real-time Object Network Engineering Environment (RONEE), which will bring about a quantum jump in the system engineers&apos; productivity in&lt;br/&gt; constructing distributed RT application systems.&lt;br/&gt;&lt;br/&gt;The research base of this multi-nation researcher team includes (1) a new high-level style of RT OO distributed programming called the TMO (Time-triggered Message-triggered Object) programming, (2)&lt;br/&gt;middleware supporting RT objects such as TAO object request broker and TMOSM, (3) OS and middleware support for fault-tolerant RT distributed computing such as ROAFTS and TTP OS, (4) frameworks&lt;br/&gt;such as RT UML and ACSR for formal specification, etc.&lt;br/&gt;&lt;br/&gt;Specific research tasks include integrations of cornerstone&lt;br/&gt;techniques and development of RONEE for:&lt;br/&gt;(1) High-level distributed RT programming based on C++ and JAVA;&lt;br/&gt;(2) Formal multi-level specification of requirements and designs;&lt;br/&gt;(3) Efficient execution of RT objects, stationary and mobile;&lt;br/&gt;(4) Fault-tolerant computing with distributed RT objects;&lt;br/&gt;(5) Verification of timing designs;&lt;br/&gt;(6) Distributed RT simulation and 3D visualization of application&lt;br/&gt;environments to support software validation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86147</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86147</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n753" target="n755">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: RONEE - An Integrated Engineering Method and Tool Environment for Reliable Object-oriented Real-time Distributed Systems</data>
      <data key="e_abstract">Two major goals are:&lt;br/&gt;(1) To establish a new-generation integrated methodology for real- time (RT) distributed programming and software engineering on the basis of the recent developments in object-oriented (OO) RT&lt;br/&gt; distributed programming; and &lt;br/&gt;(2) To develop an integrated tool-set named the Real-time Object Network Engineering Environment (RONEE), which will bring about a quantum jump in the system engineers&apos; productivity in&lt;br/&gt; constructing distributed RT application systems.&lt;br/&gt;&lt;br/&gt;The research base of this multi-nation researcher team includes (1) a new high-level style of RT OO distributed programming called the TMO (Time-triggered Message-triggered Object) programming, (2)&lt;br/&gt;middleware supporting RT objects such as TAO object request broker and TMOSM, (3) OS and middleware support for fault-tolerant RT distributed computing such as ROAFTS and TTP OS, (4) frameworks&lt;br/&gt;such as RT UML and ACSR for formal specification, etc.&lt;br/&gt;&lt;br/&gt;Specific research tasks include integrations of cornerstone&lt;br/&gt;techniques and development of RONEE for:&lt;br/&gt;(1) High-level distributed RT programming based on C++ and JAVA;&lt;br/&gt;(2) Formal multi-level specification of requirements and designs;&lt;br/&gt;(3) Efficient execution of RT objects, stationary and mobile;&lt;br/&gt;(4) Fault-tolerant computing with distributed RT objects;&lt;br/&gt;(5) Verification of timing designs;&lt;br/&gt;(6) Distributed RT simulation and 3D visualization of application&lt;br/&gt;environments to support software validation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86147</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86147</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n754" target="n755">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: RONEE - An Integrated Engineering Method and Tool Environment for Reliable Object-oriented Real-time Distributed Systems</data>
      <data key="e_abstract">Two major goals are:&lt;br/&gt;(1) To establish a new-generation integrated methodology for real- time (RT) distributed programming and software engineering on the basis of the recent developments in object-oriented (OO) RT&lt;br/&gt; distributed programming; and &lt;br/&gt;(2) To develop an integrated tool-set named the Real-time Object Network Engineering Environment (RONEE), which will bring about a quantum jump in the system engineers&apos; productivity in&lt;br/&gt; constructing distributed RT application systems.&lt;br/&gt;&lt;br/&gt;The research base of this multi-nation researcher team includes (1) a new high-level style of RT OO distributed programming called the TMO (Time-triggered Message-triggered Object) programming, (2)&lt;br/&gt;middleware supporting RT objects such as TAO object request broker and TMOSM, (3) OS and middleware support for fault-tolerant RT distributed computing such as ROAFTS and TTP OS, (4) frameworks&lt;br/&gt;such as RT UML and ACSR for formal specification, etc.&lt;br/&gt;&lt;br/&gt;Specific research tasks include integrations of cornerstone&lt;br/&gt;techniques and development of RONEE for:&lt;br/&gt;(1) High-level distributed RT programming based on C++ and JAVA;&lt;br/&gt;(2) Formal multi-level specification of requirements and designs;&lt;br/&gt;(3) Efficient execution of RT objects, stationary and mobile;&lt;br/&gt;(4) Fault-tolerant computing with distributed RT objects;&lt;br/&gt;(5) Verification of timing designs;&lt;br/&gt;(6) Distributed RT simulation and 3D visualization of application&lt;br/&gt;environments to support software validation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86147</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86147</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n758" target="n759">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: From the Web to the Global InfoBase</data>
      <data key="e_abstract">The project is driven by the vision of a Global InfoBase (GIB): a ubiquitous and universal information resource, simple to use, up to date, and comprehensive. Towards this vision, the project is developing technologies needed to transform today&apos;s World-Wide Web into the GIB. The project consists of four interrelated thrusts:&lt;br/&gt;&lt;br/&gt;* Integration of existing technologies into a ``universal&apos;&apos; information model and query language.&lt;br/&gt;&lt;br/&gt;* Personalized information management, so users obtain more relevant and timely information.&lt;br/&gt;&lt;br/&gt;* Sophisticated semantic-analysis tools.&lt;br/&gt;&lt;br/&gt;* Algorithms for mining information to synthesize new knowledge.&lt;br/&gt;&lt;br/&gt;The Web has created a resource comprising much of the world&apos;s knowledge. Yet today our ability to use the Web as an information resource is in a primitive state. The GIB project is developing technology that will allow society far more effective and efficient use of the dramatically growing amount of information available online.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85896</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85896</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n758" target="n760">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: From the Web to the Global InfoBase</data>
      <data key="e_abstract">The project is driven by the vision of a Global InfoBase (GIB): a ubiquitous and universal information resource, simple to use, up to date, and comprehensive. Towards this vision, the project is developing technologies needed to transform today&apos;s World-Wide Web into the GIB. The project consists of four interrelated thrusts:&lt;br/&gt;&lt;br/&gt;* Integration of existing technologies into a ``universal&apos;&apos; information model and query language.&lt;br/&gt;&lt;br/&gt;* Personalized information management, so users obtain more relevant and timely information.&lt;br/&gt;&lt;br/&gt;* Sophisticated semantic-analysis tools.&lt;br/&gt;&lt;br/&gt;* Algorithms for mining information to synthesize new knowledge.&lt;br/&gt;&lt;br/&gt;The Web has created a resource comprising much of the world&apos;s knowledge. Yet today our ability to use the Web as an information resource is in a primitive state. The GIB project is developing technology that will allow society far more effective and efficient use of the dramatically growing amount of information available online.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85896</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85896</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n758" target="n761">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: From the Web to the Global InfoBase</data>
      <data key="e_abstract">The project is driven by the vision of a Global InfoBase (GIB): a ubiquitous and universal information resource, simple to use, up to date, and comprehensive. Towards this vision, the project is developing technologies needed to transform today&apos;s World-Wide Web into the GIB. The project consists of four interrelated thrusts:&lt;br/&gt;&lt;br/&gt;* Integration of existing technologies into a ``universal&apos;&apos; information model and query language.&lt;br/&gt;&lt;br/&gt;* Personalized information management, so users obtain more relevant and timely information.&lt;br/&gt;&lt;br/&gt;* Sophisticated semantic-analysis tools.&lt;br/&gt;&lt;br/&gt;* Algorithms for mining information to synthesize new knowledge.&lt;br/&gt;&lt;br/&gt;The Web has created a resource comprising much of the world&apos;s knowledge. Yet today our ability to use the Web as an information resource is in a primitive state. The GIB project is developing technology that will allow society far more effective and efficient use of the dramatically growing amount of information available online.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85896</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85896</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n759" target="n760">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: From the Web to the Global InfoBase</data>
      <data key="e_abstract">The project is driven by the vision of a Global InfoBase (GIB): a ubiquitous and universal information resource, simple to use, up to date, and comprehensive. Towards this vision, the project is developing technologies needed to transform today&apos;s World-Wide Web into the GIB. The project consists of four interrelated thrusts:&lt;br/&gt;&lt;br/&gt;* Integration of existing technologies into a ``universal&apos;&apos; information model and query language.&lt;br/&gt;&lt;br/&gt;* Personalized information management, so users obtain more relevant and timely information.&lt;br/&gt;&lt;br/&gt;* Sophisticated semantic-analysis tools.&lt;br/&gt;&lt;br/&gt;* Algorithms for mining information to synthesize new knowledge.&lt;br/&gt;&lt;br/&gt;The Web has created a resource comprising much of the world&apos;s knowledge. Yet today our ability to use the Web as an information resource is in a primitive state. The GIB project is developing technology that will allow society far more effective and efficient use of the dramatically growing amount of information available online.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85896</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85896</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n759" target="n761">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: From the Web to the Global InfoBase</data>
      <data key="e_abstract">The project is driven by the vision of a Global InfoBase (GIB): a ubiquitous and universal information resource, simple to use, up to date, and comprehensive. Towards this vision, the project is developing technologies needed to transform today&apos;s World-Wide Web into the GIB. The project consists of four interrelated thrusts:&lt;br/&gt;&lt;br/&gt;* Integration of existing technologies into a ``universal&apos;&apos; information model and query language.&lt;br/&gt;&lt;br/&gt;* Personalized information management, so users obtain more relevant and timely information.&lt;br/&gt;&lt;br/&gt;* Sophisticated semantic-analysis tools.&lt;br/&gt;&lt;br/&gt;* Algorithms for mining information to synthesize new knowledge.&lt;br/&gt;&lt;br/&gt;The Web has created a resource comprising much of the world&apos;s knowledge. Yet today our ability to use the Web as an information resource is in a primitive state. The GIB project is developing technology that will allow society far more effective and efficient use of the dramatically growing amount of information available online.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85896</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85896</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n760" target="n761">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: From the Web to the Global InfoBase</data>
      <data key="e_abstract">The project is driven by the vision of a Global InfoBase (GIB): a ubiquitous and universal information resource, simple to use, up to date, and comprehensive. Towards this vision, the project is developing technologies needed to transform today&apos;s World-Wide Web into the GIB. The project consists of four interrelated thrusts:&lt;br/&gt;&lt;br/&gt;* Integration of existing technologies into a ``universal&apos;&apos; information model and query language.&lt;br/&gt;&lt;br/&gt;* Personalized information management, so users obtain more relevant and timely information.&lt;br/&gt;&lt;br/&gt;* Sophisticated semantic-analysis tools.&lt;br/&gt;&lt;br/&gt;* Algorithms for mining information to synthesize new knowledge.&lt;br/&gt;&lt;br/&gt;The Web has created a resource comprising much of the world&apos;s knowledge. Yet today our ability to use the Web as an information resource is in a primitive state. The GIB project is developing technology that will allow society far more effective and efficient use of the dramatically growing amount of information available online.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85896</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85896</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n762" target="n763">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Statistical Pattern Recognition in Environmental Observation and Forecasting Systems</data>
      <data key="e_abstract">Project Summary&lt;br/&gt;Environmental observation and forecasting systems (EOFS) are emerging new technologies with unparalleled&lt;br/&gt;potential to impact sustainable development. EOFS are expected to foster and support new paradigms for&lt;br/&gt;generation, transfer and social application of knowledge in domains that range from the global earth to its&lt;br/&gt;regional and local sub-systems.&lt;br/&gt;At the core of EOFS is the timely and customized acquisition, generation, processing and delivery of reliable,&lt;br/&gt;relevant information to many and very diverse audiences. Multiple challenges need to be met to implement&lt;br/&gt;this concept.&lt;br/&gt;A critical challenge is the development of automated procedures to verify the quality of the huge amounts&lt;br/&gt;of observational and simulation data that are generated by EOFS both in real-time and off-line. Process-&lt;br/&gt;based strategies for quality control of scientific data, while effective for moderate-size archival data are too&lt;br/&gt;labor-intensive to map well into EOFS-scale data sets. Strategies based on pattern recognition and machine&lt;br/&gt;learning hold significant promise as an alternative or complement.&lt;br/&gt;Under the proposed project, we will develop approaches based in statistical pattern recognition and signal&lt;br/&gt;processing, on-line adaptive systems, datamining, and advanced search to address critical quality control&lt;br/&gt;issues including: 1) Detecting sensor corruption in non-stationary, spatial-temporal systems, 2) Estimating&lt;br/&gt;true signals from corrupted sensor data, and 3) Detecting and characterizing regimes where model anomalies&lt;br/&gt;are likely.&lt;br/&gt;These quality control techniques will be developed and exercised on CORIE, a pilot EOFS for the COlumbia&lt;br/&gt;RIver Estuary and adjacent coastal waters (http://www.ccalmr.ogi.edu/CORIE)&lt;br/&gt;The project will have strong social impact, through the role of EOFS (and, specifically, CORIE) on regional&lt;br/&gt;and national sustainable development issues. The project will also include cross-disciplinary educational&lt;br/&gt;opportunities at multiple levels.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82736</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82736</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n137" target="n140">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Wireless Client-server Technologies for Ubiquitous Multimedia Computing</data>
      <data key="e_abstract">The Computer Science and Engineering, and Telecommunication Departments at Michigan State University (MSU) requested support to conduct research involving the categories: Human-Computer Interface, Information Management, and Scalable Information Infrastructure. Three existing research laboratories will be connected and used as a testbed to facilitate this collaborative interdisciplinary research: High-Speed Networking &amp; Performance Lab (HSNP), Media and Entertainment Technologies Lab (MET), and Media Interface and Network Design Lab (MIND). A primary objective of the proposed research is to further advance the client-server technologies for a ubiquitous multimedia computing environment. Since ubiquitous computing capability may take the form of advanced personal digitized assistants (PDAs), wearable computer systems featuring heads up displays, or argumented reality systems, this proposal addresses key issues which include: (1) personal environment data capture, (2) environmental data salience, (3) transport of multimedia data over wireless and heterogeneous network systems in a client-server environment, (4) media access and retrieval, and (5) human-computer-interface (HCI) design and analysis. Some of the significant questions that are addressed in the project are: (a) given continuous audio and video capture, what is the best choice of content to archive in a personal multimedia database? (b) how can audio and video capture of unconstrained content be used as an effective query mechanism for a multimedia database? (c) how can perceived latency and communications performance be optimized in a wireless environment where bandwidth, error rates, and latency are continuously variable and dependent upon many environmental conditions?, (d) how can the quality-of-service for the transport of real-time multimedia traffic be guaranteed using heterogeneous networks and wireless network systems?, and (e) what should be the design principles to support a fully mobile user engaged in hand-free high-bandwidth, collaborative work and communication? New algorithms and techniques will be developed and tested to address these questions either via simulation, emulation, or experimentation. It is anticipated that the existing facilities will be significantly enhanced via a pending NSF research equipment proposal. This proposed equipment (e.g., VoD servers, multimedia lap tops, WaveLan products, wearable argumented reality interfaces) will be the defining infrastructure to support a sophisticated testbed, connecting three research laboratories (HSNP, MET, MIND) via wireless links. The multi-lab testbed will be used for the experimental phase of the project. As an effort to broaden the participation in our research, the PIs will invite interested faculty (via two-week summer workshops) and students (via ten-week summer research experience) from HBCUs (Historically Black Colleges and Universities) to work with them during the three summers. Additionally, the PIs will invite a few interested professionals from industry to participate in the research initiatives.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82743</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82743</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n137" target="n138">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Wireless Client-server Technologies for Ubiquitous Multimedia Computing</data>
      <data key="e_abstract">The Computer Science and Engineering, and Telecommunication Departments at Michigan State University (MSU) requested support to conduct research involving the categories: Human-Computer Interface, Information Management, and Scalable Information Infrastructure. Three existing research laboratories will be connected and used as a testbed to facilitate this collaborative interdisciplinary research: High-Speed Networking &amp; Performance Lab (HSNP), Media and Entertainment Technologies Lab (MET), and Media Interface and Network Design Lab (MIND). A primary objective of the proposed research is to further advance the client-server technologies for a ubiquitous multimedia computing environment. Since ubiquitous computing capability may take the form of advanced personal digitized assistants (PDAs), wearable computer systems featuring heads up displays, or argumented reality systems, this proposal addresses key issues which include: (1) personal environment data capture, (2) environmental data salience, (3) transport of multimedia data over wireless and heterogeneous network systems in a client-server environment, (4) media access and retrieval, and (5) human-computer-interface (HCI) design and analysis. Some of the significant questions that are addressed in the project are: (a) given continuous audio and video capture, what is the best choice of content to archive in a personal multimedia database? (b) how can audio and video capture of unconstrained content be used as an effective query mechanism for a multimedia database? (c) how can perceived latency and communications performance be optimized in a wireless environment where bandwidth, error rates, and latency are continuously variable and dependent upon many environmental conditions?, (d) how can the quality-of-service for the transport of real-time multimedia traffic be guaranteed using heterogeneous networks and wireless network systems?, and (e) what should be the design principles to support a fully mobile user engaged in hand-free high-bandwidth, collaborative work and communication? New algorithms and techniques will be developed and tested to address these questions either via simulation, emulation, or experimentation. It is anticipated that the existing facilities will be significantly enhanced via a pending NSF research equipment proposal. This proposed equipment (e.g., VoD servers, multimedia lap tops, WaveLan products, wearable argumented reality interfaces) will be the defining infrastructure to support a sophisticated testbed, connecting three research laboratories (HSNP, MET, MIND) via wireless links. The multi-lab testbed will be used for the experimental phase of the project. As an effort to broaden the participation in our research, the PIs will invite interested faculty (via two-week summer workshops) and students (via ten-week summer research experience) from HBCUs (Historically Black Colleges and Universities) to work with them during the three summers. Additionally, the PIs will invite a few interested professionals from industry to participate in the research initiatives.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82743</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82743</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n138" target="n140">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Wireless Client-server Technologies for Ubiquitous Multimedia Computing</data>
      <data key="e_abstract">The Computer Science and Engineering, and Telecommunication Departments at Michigan State University (MSU) requested support to conduct research involving the categories: Human-Computer Interface, Information Management, and Scalable Information Infrastructure. Three existing research laboratories will be connected and used as a testbed to facilitate this collaborative interdisciplinary research: High-Speed Networking &amp; Performance Lab (HSNP), Media and Entertainment Technologies Lab (MET), and Media Interface and Network Design Lab (MIND). A primary objective of the proposed research is to further advance the client-server technologies for a ubiquitous multimedia computing environment. Since ubiquitous computing capability may take the form of advanced personal digitized assistants (PDAs), wearable computer systems featuring heads up displays, or argumented reality systems, this proposal addresses key issues which include: (1) personal environment data capture, (2) environmental data salience, (3) transport of multimedia data over wireless and heterogeneous network systems in a client-server environment, (4) media access and retrieval, and (5) human-computer-interface (HCI) design and analysis. Some of the significant questions that are addressed in the project are: (a) given continuous audio and video capture, what is the best choice of content to archive in a personal multimedia database? (b) how can audio and video capture of unconstrained content be used as an effective query mechanism for a multimedia database? (c) how can perceived latency and communications performance be optimized in a wireless environment where bandwidth, error rates, and latency are continuously variable and dependent upon many environmental conditions?, (d) how can the quality-of-service for the transport of real-time multimedia traffic be guaranteed using heterogeneous networks and wireless network systems?, and (e) what should be the design principles to support a fully mobile user engaged in hand-free high-bandwidth, collaborative work and communication? New algorithms and techniques will be developed and tested to address these questions either via simulation, emulation, or experimentation. It is anticipated that the existing facilities will be significantly enhanced via a pending NSF research equipment proposal. This proposed equipment (e.g., VoD servers, multimedia lap tops, WaveLan products, wearable argumented reality interfaces) will be the defining infrastructure to support a sophisticated testbed, connecting three research laboratories (HSNP, MET, MIND) via wireless links. The multi-lab testbed will be used for the experimental phase of the project. As an effort to broaden the participation in our research, the PIs will invite interested faculty (via two-week summer workshops) and students (via ten-week summer research experience) from HBCUs (Historically Black Colleges and Universities) to work with them during the three summers. Additionally, the PIs will invite a few interested professionals from industry to participate in the research initiatives.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82743</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82743</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n767" target="n768">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Creating the Next Generation of Intelligent Animated Conversational Agents</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. The goal of this project is to improve reading achievement of children with reading problems by designing computer-based interactive reading tutors that incorporate new speech and language technologies. The reading tutors will help English- and Spanish-speaking children learn to read by providing classroom teachers and reading specialists with tools to instruct and exercise the set of auditory, visual and linguistic skills needed to read, speech discrimination, speech production, phonological awareness, sound-to-letter mappings, vocabulary, fluency and comprehension. The tutors will be designed, tested and refined in collaboration with reading specialists and instructional designers, and tested with children in special education programs in elementary schools in Boulder Colorado. &lt;br/&gt;&lt;br/&gt;The tutors will incorporate new and improved auditory and visual speech recognition and facial animation technologies. Five partner sites - Oregon Graduate Institute (OGI), Universidad de las Americas, Puebla (UDLA), University of California, Santa Cruz (UCSC), University of California, San Diego (UCSD) and the University of Colorado (CU), will develop speech and language technologies. Research and development of children&apos;s speech recognizers will be conducted at UDLA for Spanish and at OGI for English. In addition, these sites will design and develop speech corpora to enable recognition research. UCSD will conduct research leading to development of head tracking and speech reading systems, and design and develop video corpora to enable this research. UCSC will conduct research leading to development of new animated faces with improved animation capabilities. System integration will be conducted at OGI, which will integrate auditory and visual recognition systems and facial animation systems into the CSLU Toolkit. CU will develop English reading tutors in collaboration with teachers, instructional designers and students, and conduct evaluations of project outcomes. UDLA will also develop and test Spanish versions of the tutors.&lt;br/&gt;&lt;br/&gt;The project is expected to produce significant advances in auditory and visual recognition technologies, including accurate recognition of children&apos;s speech, accurate recognition of visual features of speech, and the first real-time integration of auditory and visual speech recognition in language training applications. In addition, the PI and his team will achieve a new level of understanding of the structure of children&apos;s speech, and the processing of auditory and visual information in reading. Facial animation is expected to play a major role in engaging children, enabling them to enjoy the learning experience more and therefore spend more time on task. The PI expects to demonstrate that facial animation using visible articulators will improve speech discrimination and speech production skills, improved phonological awareness and improved reading. By integrating auditory and visual speech recognition and speech generation technologies into animated agents, and designing reading tutors that incorporate these agents in a well designed reading program, the PI hopes to improve reading achievement in schools. To optimize this outcome, the PI is working closely with reading specialists to incorporate their experience and best practices; and by developing formative and summative evaluation plans that assure fair and accurate assessment of the outcomes of the planned interventions.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86107</data>
      <data key="e_expirationDate">2007-10-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86107</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n767" target="n769">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Creating the Next Generation of Intelligent Animated Conversational Agents</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. The goal of this project is to improve reading achievement of children with reading problems by designing computer-based interactive reading tutors that incorporate new speech and language technologies. The reading tutors will help English- and Spanish-speaking children learn to read by providing classroom teachers and reading specialists with tools to instruct and exercise the set of auditory, visual and linguistic skills needed to read, speech discrimination, speech production, phonological awareness, sound-to-letter mappings, vocabulary, fluency and comprehension. The tutors will be designed, tested and refined in collaboration with reading specialists and instructional designers, and tested with children in special education programs in elementary schools in Boulder Colorado. &lt;br/&gt;&lt;br/&gt;The tutors will incorporate new and improved auditory and visual speech recognition and facial animation technologies. Five partner sites - Oregon Graduate Institute (OGI), Universidad de las Americas, Puebla (UDLA), University of California, Santa Cruz (UCSC), University of California, San Diego (UCSD) and the University of Colorado (CU), will develop speech and language technologies. Research and development of children&apos;s speech recognizers will be conducted at UDLA for Spanish and at OGI for English. In addition, these sites will design and develop speech corpora to enable recognition research. UCSD will conduct research leading to development of head tracking and speech reading systems, and design and develop video corpora to enable this research. UCSC will conduct research leading to development of new animated faces with improved animation capabilities. System integration will be conducted at OGI, which will integrate auditory and visual recognition systems and facial animation systems into the CSLU Toolkit. CU will develop English reading tutors in collaboration with teachers, instructional designers and students, and conduct evaluations of project outcomes. UDLA will also develop and test Spanish versions of the tutors.&lt;br/&gt;&lt;br/&gt;The project is expected to produce significant advances in auditory and visual recognition technologies, including accurate recognition of children&apos;s speech, accurate recognition of visual features of speech, and the first real-time integration of auditory and visual speech recognition in language training applications. In addition, the PI and his team will achieve a new level of understanding of the structure of children&apos;s speech, and the processing of auditory and visual information in reading. Facial animation is expected to play a major role in engaging children, enabling them to enjoy the learning experience more and therefore spend more time on task. The PI expects to demonstrate that facial animation using visible articulators will improve speech discrimination and speech production skills, improved phonological awareness and improved reading. By integrating auditory and visual speech recognition and speech generation technologies into animated agents, and designing reading tutors that incorporate these agents in a well designed reading program, the PI hopes to improve reading achievement in schools. To optimize this outcome, the PI is working closely with reading specialists to incorporate their experience and best practices; and by developing formative and summative evaluation plans that assure fair and accurate assessment of the outcomes of the planned interventions.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86107</data>
      <data key="e_expirationDate">2007-10-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86107</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n767" target="n770">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Creating the Next Generation of Intelligent Animated Conversational Agents</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. The goal of this project is to improve reading achievement of children with reading problems by designing computer-based interactive reading tutors that incorporate new speech and language technologies. The reading tutors will help English- and Spanish-speaking children learn to read by providing classroom teachers and reading specialists with tools to instruct and exercise the set of auditory, visual and linguistic skills needed to read, speech discrimination, speech production, phonological awareness, sound-to-letter mappings, vocabulary, fluency and comprehension. The tutors will be designed, tested and refined in collaboration with reading specialists and instructional designers, and tested with children in special education programs in elementary schools in Boulder Colorado. &lt;br/&gt;&lt;br/&gt;The tutors will incorporate new and improved auditory and visual speech recognition and facial animation technologies. Five partner sites - Oregon Graduate Institute (OGI), Universidad de las Americas, Puebla (UDLA), University of California, Santa Cruz (UCSC), University of California, San Diego (UCSD) and the University of Colorado (CU), will develop speech and language technologies. Research and development of children&apos;s speech recognizers will be conducted at UDLA for Spanish and at OGI for English. In addition, these sites will design and develop speech corpora to enable recognition research. UCSD will conduct research leading to development of head tracking and speech reading systems, and design and develop video corpora to enable this research. UCSC will conduct research leading to development of new animated faces with improved animation capabilities. System integration will be conducted at OGI, which will integrate auditory and visual recognition systems and facial animation systems into the CSLU Toolkit. CU will develop English reading tutors in collaboration with teachers, instructional designers and students, and conduct evaluations of project outcomes. UDLA will also develop and test Spanish versions of the tutors.&lt;br/&gt;&lt;br/&gt;The project is expected to produce significant advances in auditory and visual recognition technologies, including accurate recognition of children&apos;s speech, accurate recognition of visual features of speech, and the first real-time integration of auditory and visual speech recognition in language training applications. In addition, the PI and his team will achieve a new level of understanding of the structure of children&apos;s speech, and the processing of auditory and visual information in reading. Facial animation is expected to play a major role in engaging children, enabling them to enjoy the learning experience more and therefore spend more time on task. The PI expects to demonstrate that facial animation using visible articulators will improve speech discrimination and speech production skills, improved phonological awareness and improved reading. By integrating auditory and visual speech recognition and speech generation technologies into animated agents, and designing reading tutors that incorporate these agents in a well designed reading program, the PI hopes to improve reading achievement in schools. To optimize this outcome, the PI is working closely with reading specialists to incorporate their experience and best practices; and by developing formative and summative evaluation plans that assure fair and accurate assessment of the outcomes of the planned interventions.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86107</data>
      <data key="e_expirationDate">2007-10-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86107</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n768" target="n769">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Creating the Next Generation of Intelligent Animated Conversational Agents</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. The goal of this project is to improve reading achievement of children with reading problems by designing computer-based interactive reading tutors that incorporate new speech and language technologies. The reading tutors will help English- and Spanish-speaking children learn to read by providing classroom teachers and reading specialists with tools to instruct and exercise the set of auditory, visual and linguistic skills needed to read, speech discrimination, speech production, phonological awareness, sound-to-letter mappings, vocabulary, fluency and comprehension. The tutors will be designed, tested and refined in collaboration with reading specialists and instructional designers, and tested with children in special education programs in elementary schools in Boulder Colorado. &lt;br/&gt;&lt;br/&gt;The tutors will incorporate new and improved auditory and visual speech recognition and facial animation technologies. Five partner sites - Oregon Graduate Institute (OGI), Universidad de las Americas, Puebla (UDLA), University of California, Santa Cruz (UCSC), University of California, San Diego (UCSD) and the University of Colorado (CU), will develop speech and language technologies. Research and development of children&apos;s speech recognizers will be conducted at UDLA for Spanish and at OGI for English. In addition, these sites will design and develop speech corpora to enable recognition research. UCSD will conduct research leading to development of head tracking and speech reading systems, and design and develop video corpora to enable this research. UCSC will conduct research leading to development of new animated faces with improved animation capabilities. System integration will be conducted at OGI, which will integrate auditory and visual recognition systems and facial animation systems into the CSLU Toolkit. CU will develop English reading tutors in collaboration with teachers, instructional designers and students, and conduct evaluations of project outcomes. UDLA will also develop and test Spanish versions of the tutors.&lt;br/&gt;&lt;br/&gt;The project is expected to produce significant advances in auditory and visual recognition technologies, including accurate recognition of children&apos;s speech, accurate recognition of visual features of speech, and the first real-time integration of auditory and visual speech recognition in language training applications. In addition, the PI and his team will achieve a new level of understanding of the structure of children&apos;s speech, and the processing of auditory and visual information in reading. Facial animation is expected to play a major role in engaging children, enabling them to enjoy the learning experience more and therefore spend more time on task. The PI expects to demonstrate that facial animation using visible articulators will improve speech discrimination and speech production skills, improved phonological awareness and improved reading. By integrating auditory and visual speech recognition and speech generation technologies into animated agents, and designing reading tutors that incorporate these agents in a well designed reading program, the PI hopes to improve reading achievement in schools. To optimize this outcome, the PI is working closely with reading specialists to incorporate their experience and best practices; and by developing formative and summative evaluation plans that assure fair and accurate assessment of the outcomes of the planned interventions.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86107</data>
      <data key="e_expirationDate">2007-10-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86107</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n768" target="n770">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Creating the Next Generation of Intelligent Animated Conversational Agents</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. The goal of this project is to improve reading achievement of children with reading problems by designing computer-based interactive reading tutors that incorporate new speech and language technologies. The reading tutors will help English- and Spanish-speaking children learn to read by providing classroom teachers and reading specialists with tools to instruct and exercise the set of auditory, visual and linguistic skills needed to read, speech discrimination, speech production, phonological awareness, sound-to-letter mappings, vocabulary, fluency and comprehension. The tutors will be designed, tested and refined in collaboration with reading specialists and instructional designers, and tested with children in special education programs in elementary schools in Boulder Colorado. &lt;br/&gt;&lt;br/&gt;The tutors will incorporate new and improved auditory and visual speech recognition and facial animation technologies. Five partner sites - Oregon Graduate Institute (OGI), Universidad de las Americas, Puebla (UDLA), University of California, Santa Cruz (UCSC), University of California, San Diego (UCSD) and the University of Colorado (CU), will develop speech and language technologies. Research and development of children&apos;s speech recognizers will be conducted at UDLA for Spanish and at OGI for English. In addition, these sites will design and develop speech corpora to enable recognition research. UCSD will conduct research leading to development of head tracking and speech reading systems, and design and develop video corpora to enable this research. UCSC will conduct research leading to development of new animated faces with improved animation capabilities. System integration will be conducted at OGI, which will integrate auditory and visual recognition systems and facial animation systems into the CSLU Toolkit. CU will develop English reading tutors in collaboration with teachers, instructional designers and students, and conduct evaluations of project outcomes. UDLA will also develop and test Spanish versions of the tutors.&lt;br/&gt;&lt;br/&gt;The project is expected to produce significant advances in auditory and visual recognition technologies, including accurate recognition of children&apos;s speech, accurate recognition of visual features of speech, and the first real-time integration of auditory and visual speech recognition in language training applications. In addition, the PI and his team will achieve a new level of understanding of the structure of children&apos;s speech, and the processing of auditory and visual information in reading. Facial animation is expected to play a major role in engaging children, enabling them to enjoy the learning experience more and therefore spend more time on task. The PI expects to demonstrate that facial animation using visible articulators will improve speech discrimination and speech production skills, improved phonological awareness and improved reading. By integrating auditory and visual speech recognition and speech generation technologies into animated agents, and designing reading tutors that incorporate these agents in a well designed reading program, the PI hopes to improve reading achievement in schools. To optimize this outcome, the PI is working closely with reading specialists to incorporate their experience and best practices; and by developing formative and summative evaluation plans that assure fair and accurate assessment of the outcomes of the planned interventions.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86107</data>
      <data key="e_expirationDate">2007-10-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86107</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n769" target="n770">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Creating the Next Generation of Intelligent Animated Conversational Agents</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. The goal of this project is to improve reading achievement of children with reading problems by designing computer-based interactive reading tutors that incorporate new speech and language technologies. The reading tutors will help English- and Spanish-speaking children learn to read by providing classroom teachers and reading specialists with tools to instruct and exercise the set of auditory, visual and linguistic skills needed to read, speech discrimination, speech production, phonological awareness, sound-to-letter mappings, vocabulary, fluency and comprehension. The tutors will be designed, tested and refined in collaboration with reading specialists and instructional designers, and tested with children in special education programs in elementary schools in Boulder Colorado. &lt;br/&gt;&lt;br/&gt;The tutors will incorporate new and improved auditory and visual speech recognition and facial animation technologies. Five partner sites - Oregon Graduate Institute (OGI), Universidad de las Americas, Puebla (UDLA), University of California, Santa Cruz (UCSC), University of California, San Diego (UCSD) and the University of Colorado (CU), will develop speech and language technologies. Research and development of children&apos;s speech recognizers will be conducted at UDLA for Spanish and at OGI for English. In addition, these sites will design and develop speech corpora to enable recognition research. UCSD will conduct research leading to development of head tracking and speech reading systems, and design and develop video corpora to enable this research. UCSC will conduct research leading to development of new animated faces with improved animation capabilities. System integration will be conducted at OGI, which will integrate auditory and visual recognition systems and facial animation systems into the CSLU Toolkit. CU will develop English reading tutors in collaboration with teachers, instructional designers and students, and conduct evaluations of project outcomes. UDLA will also develop and test Spanish versions of the tutors.&lt;br/&gt;&lt;br/&gt;The project is expected to produce significant advances in auditory and visual recognition technologies, including accurate recognition of children&apos;s speech, accurate recognition of visual features of speech, and the first real-time integration of auditory and visual speech recognition in language training applications. In addition, the PI and his team will achieve a new level of understanding of the structure of children&apos;s speech, and the processing of auditory and visual information in reading. Facial animation is expected to play a major role in engaging children, enabling them to enjoy the learning experience more and therefore spend more time on task. The PI expects to demonstrate that facial animation using visible articulators will improve speech discrimination and speech production skills, improved phonological awareness and improved reading. By integrating auditory and visual speech recognition and speech generation technologies into animated agents, and designing reading tutors that incorporate these agents in a well designed reading program, the PI hopes to improve reading achievement in schools. To optimize this outcome, the PI is working closely with reading specialists to incorporate their experience and best practices; and by developing formative and summative evaluation plans that assure fair and accurate assessment of the outcomes of the planned interventions.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86107</data>
      <data key="e_expirationDate">2007-10-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86107</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n770" target="n771">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Modeling Degree of Articulation for Speech Synthesis</data>
      <data key="e_abstract">The automatic conversion of text to speech provides a means to achieve universal access to on-line information. However, except for simple messages, speech generated by current synthesizers is both unpleasant and hard to understand: even though words presented individually are quite intelligible, listeners are generally unable to comprehend longer or more complex messages without intense concentration. A key reason for this &quot;incomprehensibility&quot; is the lack of proper prosody in synthetic speech. Prosody refers to the rhythmic and melodic characteristics of speech, which are used by the speaker to structure information for the listener. That is, prosody conveys to the listener which words or phrases are important prominence, and which words belong together in some semantic or syntactic sense (phrasing). Prosody involves a host of acoustic features, such as variations in fundamental frequency (F0), timing, and features that are related to the speaker&apos;s level of effort. Current synthesizers have poor prosody for two main reasons: (i) accurate prediction from text of timing and F0 is intrinsically difficult, and (ii) they can neither predict nor control features in speech that correspond to the speaker&apos;s articulatory effort. While many techniques exist for control of segmental duration (one aspect of timing) and F0 characteristics of speech, little attention has been paid to control of this second category of effects, and the quality of current synthesizers is poor as a result.&lt;br/&gt;&lt;br/&gt;The PI has defined a concept of &quot;degree of articulation&quot; to refer to the fact that, at a given speaking rate, speakers can control the precision and speed of the motions of their tongue, lips, velum, etc. with varying degrees of effort, from &quot;hypo-articulate&quot; (sloppy) to &quot;hyper-articulate&quot; (precise). Acoustic correlates of degree of articulation have been shown to covary with linguistic factors such as word emphasis and syllabic stress. While clearly important, this concept is nevertheless vague and its static and dynamic acoustic correlates have not been well established. Moreover, no quantitative models exist that predict degree of articulation from text or that provide a sufficiently precise quantitative description of these acoustic correlates for implementation in a synthesizer. The overarching goal of this project is to develop principled quantitative models for the prediction of acoustic features associated with degree-of-articulation, and to implement these results in a speech synthesizer. The strategy will be (a) to use text materials that systematically vary in prominence-related factors in order to elicit varying levels of degree of articulation in read speech; (b) to analyze speech signal, laryngograph signal, and jaw/lip articulatory data; and (c) to use the analysis results to generate mathematical descriptions of the relationship between prosodic structure and spectral features of the speech signal.&lt;br/&gt;&lt;br/&gt;The outcomes of this project will include the following: Improved understanding of the acoustic, glottal, and articulatory correlates of degree of articulation, including both static and dynamic features. This knowledge will impact not only basic science, but also technologies like speech synthesis and automatic speech recognition; Accurate prediction of spectral features of the speech signal from prosodic structure, based on a principled model that incorporates both acoustic and articulatory knowledge; Techniques for more natural-sounding speech synthesis that requires a lower attentional demand on the listener. This will lead to greater user acceptance of synthesized speech in applications including voice-based information access, language training, and tools for visually or vocally disabled persons.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82718</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82718</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n773" target="n774">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Experimental Validation of Large-Scale Networked Software Systems</data>
      <data key="e_abstract">Large-scale networked software systems are hard to design, and even more&lt;br/&gt;difficult to validate. Validation of such systems is increasingly&lt;br/&gt;important, since they are more and more being called on to perform critical&lt;br/&gt;functions. This validation difficulty stems from the inherent complexity of&lt;br/&gt;these systems, and often is due to the fact that they are often designed to&lt;br/&gt;adapt to variable workloads and operating conditions at the process, node,&lt;br/&gt;and network levels. Incorrect operation during periods of dynamic&lt;br/&gt;adaptation can lead to unpredictable and potentially hazardous&lt;br/&gt;consequences. In order to ensure that such systems operate correctly in&lt;br/&gt;critical environments, one must perform validations to confirm that they&lt;br/&gt;will function reliably in the presence of faults/failures, have predictable&lt;br/&gt;performance, and will continue to operate when intrusions occur. Validation&lt;br/&gt;of multiple behavior dimensions (e.g., reliability/availability,&lt;br/&gt;performance, and survivability) is also critical. This research will&lt;br/&gt;develop the theory, methodology, and tools necessary to experimentally&lt;br/&gt;validate the reliability/availability, performance, and survivability of&lt;br/&gt;large-scale networked software systems. The intention is to develop a&lt;br/&gt;comprehensive framework for experimentally validating large-scale networked&lt;br/&gt;software systems. Taken as a whole, this work will provide a sound and&lt;br/&gt;fundamental approach to validation of networked software and applications.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86096</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86096</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n773" target="n775">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Experimental Validation of Large-Scale Networked Software Systems</data>
      <data key="e_abstract">Large-scale networked software systems are hard to design, and even more&lt;br/&gt;difficult to validate. Validation of such systems is increasingly&lt;br/&gt;important, since they are more and more being called on to perform critical&lt;br/&gt;functions. This validation difficulty stems from the inherent complexity of&lt;br/&gt;these systems, and often is due to the fact that they are often designed to&lt;br/&gt;adapt to variable workloads and operating conditions at the process, node,&lt;br/&gt;and network levels. Incorrect operation during periods of dynamic&lt;br/&gt;adaptation can lead to unpredictable and potentially hazardous&lt;br/&gt;consequences. In order to ensure that such systems operate correctly in&lt;br/&gt;critical environments, one must perform validations to confirm that they&lt;br/&gt;will function reliably in the presence of faults/failures, have predictable&lt;br/&gt;performance, and will continue to operate when intrusions occur. Validation&lt;br/&gt;of multiple behavior dimensions (e.g., reliability/availability,&lt;br/&gt;performance, and survivability) is also critical. This research will&lt;br/&gt;develop the theory, methodology, and tools necessary to experimentally&lt;br/&gt;validate the reliability/availability, performance, and survivability of&lt;br/&gt;large-scale networked software systems. The intention is to develop a&lt;br/&gt;comprehensive framework for experimentally validating large-scale networked&lt;br/&gt;software systems. Taken as a whole, this work will provide a sound and&lt;br/&gt;fundamental approach to validation of networked software and applications.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86096</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86096</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n618" target="n773">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Experimental Validation of Large-Scale Networked Software Systems</data>
      <data key="e_abstract">Large-scale networked software systems are hard to design, and even more&lt;br/&gt;difficult to validate. Validation of such systems is increasingly&lt;br/&gt;important, since they are more and more being called on to perform critical&lt;br/&gt;functions. This validation difficulty stems from the inherent complexity of&lt;br/&gt;these systems, and often is due to the fact that they are often designed to&lt;br/&gt;adapt to variable workloads and operating conditions at the process, node,&lt;br/&gt;and network levels. Incorrect operation during periods of dynamic&lt;br/&gt;adaptation can lead to unpredictable and potentially hazardous&lt;br/&gt;consequences. In order to ensure that such systems operate correctly in&lt;br/&gt;critical environments, one must perform validations to confirm that they&lt;br/&gt;will function reliably in the presence of faults/failures, have predictable&lt;br/&gt;performance, and will continue to operate when intrusions occur. Validation&lt;br/&gt;of multiple behavior dimensions (e.g., reliability/availability,&lt;br/&gt;performance, and survivability) is also critical. This research will&lt;br/&gt;develop the theory, methodology, and tools necessary to experimentally&lt;br/&gt;validate the reliability/availability, performance, and survivability of&lt;br/&gt;large-scale networked software systems. The intention is to develop a&lt;br/&gt;comprehensive framework for experimentally validating large-scale networked&lt;br/&gt;software systems. Taken as a whole, this work will provide a sound and&lt;br/&gt;fundamental approach to validation of networked software and applications.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86096</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86096</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n774" target="n775">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Experimental Validation of Large-Scale Networked Software Systems</data>
      <data key="e_abstract">Large-scale networked software systems are hard to design, and even more&lt;br/&gt;difficult to validate. Validation of such systems is increasingly&lt;br/&gt;important, since they are more and more being called on to perform critical&lt;br/&gt;functions. This validation difficulty stems from the inherent complexity of&lt;br/&gt;these systems, and often is due to the fact that they are often designed to&lt;br/&gt;adapt to variable workloads and operating conditions at the process, node,&lt;br/&gt;and network levels. Incorrect operation during periods of dynamic&lt;br/&gt;adaptation can lead to unpredictable and potentially hazardous&lt;br/&gt;consequences. In order to ensure that such systems operate correctly in&lt;br/&gt;critical environments, one must perform validations to confirm that they&lt;br/&gt;will function reliably in the presence of faults/failures, have predictable&lt;br/&gt;performance, and will continue to operate when intrusions occur. Validation&lt;br/&gt;of multiple behavior dimensions (e.g., reliability/availability,&lt;br/&gt;performance, and survivability) is also critical. This research will&lt;br/&gt;develop the theory, methodology, and tools necessary to experimentally&lt;br/&gt;validate the reliability/availability, performance, and survivability of&lt;br/&gt;large-scale networked software systems. The intention is to develop a&lt;br/&gt;comprehensive framework for experimentally validating large-scale networked&lt;br/&gt;software systems. Taken as a whole, this work will provide a sound and&lt;br/&gt;fundamental approach to validation of networked software and applications.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86096</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86096</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n618" target="n774">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Experimental Validation of Large-Scale Networked Software Systems</data>
      <data key="e_abstract">Large-scale networked software systems are hard to design, and even more&lt;br/&gt;difficult to validate. Validation of such systems is increasingly&lt;br/&gt;important, since they are more and more being called on to perform critical&lt;br/&gt;functions. This validation difficulty stems from the inherent complexity of&lt;br/&gt;these systems, and often is due to the fact that they are often designed to&lt;br/&gt;adapt to variable workloads and operating conditions at the process, node,&lt;br/&gt;and network levels. Incorrect operation during periods of dynamic&lt;br/&gt;adaptation can lead to unpredictable and potentially hazardous&lt;br/&gt;consequences. In order to ensure that such systems operate correctly in&lt;br/&gt;critical environments, one must perform validations to confirm that they&lt;br/&gt;will function reliably in the presence of faults/failures, have predictable&lt;br/&gt;performance, and will continue to operate when intrusions occur. Validation&lt;br/&gt;of multiple behavior dimensions (e.g., reliability/availability,&lt;br/&gt;performance, and survivability) is also critical. This research will&lt;br/&gt;develop the theory, methodology, and tools necessary to experimentally&lt;br/&gt;validate the reliability/availability, performance, and survivability of&lt;br/&gt;large-scale networked software systems. The intention is to develop a&lt;br/&gt;comprehensive framework for experimentally validating large-scale networked&lt;br/&gt;software systems. Taken as a whole, this work will provide a sound and&lt;br/&gt;fundamental approach to validation of networked software and applications.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86096</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86096</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n618" target="n775">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Experimental Validation of Large-Scale Networked Software Systems</data>
      <data key="e_abstract">Large-scale networked software systems are hard to design, and even more&lt;br/&gt;difficult to validate. Validation of such systems is increasingly&lt;br/&gt;important, since they are more and more being called on to perform critical&lt;br/&gt;functions. This validation difficulty stems from the inherent complexity of&lt;br/&gt;these systems, and often is due to the fact that they are often designed to&lt;br/&gt;adapt to variable workloads and operating conditions at the process, node,&lt;br/&gt;and network levels. Incorrect operation during periods of dynamic&lt;br/&gt;adaptation can lead to unpredictable and potentially hazardous&lt;br/&gt;consequences. In order to ensure that such systems operate correctly in&lt;br/&gt;critical environments, one must perform validations to confirm that they&lt;br/&gt;will function reliably in the presence of faults/failures, have predictable&lt;br/&gt;performance, and will continue to operate when intrusions occur. Validation&lt;br/&gt;of multiple behavior dimensions (e.g., reliability/availability,&lt;br/&gt;performance, and survivability) is also critical. This research will&lt;br/&gt;develop the theory, methodology, and tools necessary to experimentally&lt;br/&gt;validate the reliability/availability, performance, and survivability of&lt;br/&gt;large-scale networked software systems. The intention is to develop a&lt;br/&gt;comprehensive framework for experimentally validating large-scale networked&lt;br/&gt;software systems. Taken as a whole, this work will provide a sound and&lt;br/&gt;fundamental approach to validation of networked software and applications.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86096</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86096</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n777" target="n778">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Framenet++: An On-Line Lexical Semantic Resource and its Application to Speech and Language Understanding</data>
      <data key="e_abstract">This is the first year funding of a three-year continuing award. Robust domain-independent language understanding is essential for multilingual information extraction, summarization, question answering, and automatic translation. With pervasive computing environments soon to come, language understanding will become even more indispensable for interacting with artifacts of widely different functionalities. The field of natural language understanding has made significant progress in the last fifteen years. A large part of this gain is due to the sophisticated combination of statistical algorithms with template-based algorithms tailored to specific domains like air-traffic information, travel scheduling, and business news. But any real solution to the problem of domain-independent understanding will require moving beyond template-based monolingual systems to more flexible, general purpose HCI systems via three key innovations: (1) a domain-independent semantic language as the back end for these understanding systems, replacing the current domain-restricted templates and slots; (2) rich semantic lexical databases which are broad enough to cover the necessary words for language engineering tasks, and deep enough in usable semantic information to support true domain-independent understanding; and (3) sophisticated techniques for performing this mapping.&lt;br/&gt;&lt;br/&gt;This project will develop these three components: a very large lexical database FrameNet++, a semantic language designed for domain-independent understanding tasks, and the tools for applying it to and evaluating it on key NLU applications. The semantic language and lexical database are based on formalizing the semantic frames and the semantic and syntactic combinatory properties - the valences - of a significant portion of the English lexicon. FrameNet++ will offer significantly richer semantic information than is available in current databases like COMLEX and WordNet, by characterizing the conceptual frames within which words are defined and identifying the semantic roles which the arguments of these words can take. These roles and frames are key to building domain-independent language understanding applications. The project will focus from the start on specific NLU applications: word sense disambiguation, information extraction, multilingual information extraction, and an eventual extension to text data mining. For each application, the PI and his team will apply the FrameNet++ system to improve the domain independence of the semantic components, using statistical algorithms for semantic annotation that we have already begun to implement. These applications will in turn provide a rich and realistic evaluation framework to guide FrameNet++ development, and will encourage potential users to apply it to a wide variety of tasks.&lt;br/&gt;&lt;br/&gt;The FrameNet++ database will be capable of serving many purposes. Provided with statistical information about frequencies of words, word/sense mappings, and combinatorial patterns linked to word senses, it will be usable in various automatic language understanding processes, including word sense disambiguation and information extraction. Since the formal semantic annotations are keyed to conceptual structures which are independent of any individual language, they are available for the creation of parallel lexicon databases of other languages. The semantic structures in the databases will facilitate matches from one language to another, in machine translation and machine-assisted translation, while the syntactic structures allow the production of appropriate grammatical sentences in the target language.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86132</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86132</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n777" target="n779">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Framenet++: An On-Line Lexical Semantic Resource and its Application to Speech and Language Understanding</data>
      <data key="e_abstract">This is the first year funding of a three-year continuing award. Robust domain-independent language understanding is essential for multilingual information extraction, summarization, question answering, and automatic translation. With pervasive computing environments soon to come, language understanding will become even more indispensable for interacting with artifacts of widely different functionalities. The field of natural language understanding has made significant progress in the last fifteen years. A large part of this gain is due to the sophisticated combination of statistical algorithms with template-based algorithms tailored to specific domains like air-traffic information, travel scheduling, and business news. But any real solution to the problem of domain-independent understanding will require moving beyond template-based monolingual systems to more flexible, general purpose HCI systems via three key innovations: (1) a domain-independent semantic language as the back end for these understanding systems, replacing the current domain-restricted templates and slots; (2) rich semantic lexical databases which are broad enough to cover the necessary words for language engineering tasks, and deep enough in usable semantic information to support true domain-independent understanding; and (3) sophisticated techniques for performing this mapping.&lt;br/&gt;&lt;br/&gt;This project will develop these three components: a very large lexical database FrameNet++, a semantic language designed for domain-independent understanding tasks, and the tools for applying it to and evaluating it on key NLU applications. The semantic language and lexical database are based on formalizing the semantic frames and the semantic and syntactic combinatory properties - the valences - of a significant portion of the English lexicon. FrameNet++ will offer significantly richer semantic information than is available in current databases like COMLEX and WordNet, by characterizing the conceptual frames within which words are defined and identifying the semantic roles which the arguments of these words can take. These roles and frames are key to building domain-independent language understanding applications. The project will focus from the start on specific NLU applications: word sense disambiguation, information extraction, multilingual information extraction, and an eventual extension to text data mining. For each application, the PI and his team will apply the FrameNet++ system to improve the domain independence of the semantic components, using statistical algorithms for semantic annotation that we have already begun to implement. These applications will in turn provide a rich and realistic evaluation framework to guide FrameNet++ development, and will encourage potential users to apply it to a wide variety of tasks.&lt;br/&gt;&lt;br/&gt;The FrameNet++ database will be capable of serving many purposes. Provided with statistical information about frequencies of words, word/sense mappings, and combinatorial patterns linked to word senses, it will be usable in various automatic language understanding processes, including word sense disambiguation and information extraction. Since the formal semantic annotations are keyed to conceptual structures which are independent of any individual language, they are available for the creation of parallel lexicon databases of other languages. The semantic structures in the databases will facilitate matches from one language to another, in machine translation and machine-assisted translation, while the syntactic structures allow the production of appropriate grammatical sentences in the target language.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86132</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86132</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n777" target="n780">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Framenet++: An On-Line Lexical Semantic Resource and its Application to Speech and Language Understanding</data>
      <data key="e_abstract">This is the first year funding of a three-year continuing award. Robust domain-independent language understanding is essential for multilingual information extraction, summarization, question answering, and automatic translation. With pervasive computing environments soon to come, language understanding will become even more indispensable for interacting with artifacts of widely different functionalities. The field of natural language understanding has made significant progress in the last fifteen years. A large part of this gain is due to the sophisticated combination of statistical algorithms with template-based algorithms tailored to specific domains like air-traffic information, travel scheduling, and business news. But any real solution to the problem of domain-independent understanding will require moving beyond template-based monolingual systems to more flexible, general purpose HCI systems via three key innovations: (1) a domain-independent semantic language as the back end for these understanding systems, replacing the current domain-restricted templates and slots; (2) rich semantic lexical databases which are broad enough to cover the necessary words for language engineering tasks, and deep enough in usable semantic information to support true domain-independent understanding; and (3) sophisticated techniques for performing this mapping.&lt;br/&gt;&lt;br/&gt;This project will develop these three components: a very large lexical database FrameNet++, a semantic language designed for domain-independent understanding tasks, and the tools for applying it to and evaluating it on key NLU applications. The semantic language and lexical database are based on formalizing the semantic frames and the semantic and syntactic combinatory properties - the valences - of a significant portion of the English lexicon. FrameNet++ will offer significantly richer semantic information than is available in current databases like COMLEX and WordNet, by characterizing the conceptual frames within which words are defined and identifying the semantic roles which the arguments of these words can take. These roles and frames are key to building domain-independent language understanding applications. The project will focus from the start on specific NLU applications: word sense disambiguation, information extraction, multilingual information extraction, and an eventual extension to text data mining. For each application, the PI and his team will apply the FrameNet++ system to improve the domain independence of the semantic components, using statistical algorithms for semantic annotation that we have already begun to implement. These applications will in turn provide a rich and realistic evaluation framework to guide FrameNet++ development, and will encourage potential users to apply it to a wide variety of tasks.&lt;br/&gt;&lt;br/&gt;The FrameNet++ database will be capable of serving many purposes. Provided with statistical information about frequencies of words, word/sense mappings, and combinatorial patterns linked to word senses, it will be usable in various automatic language understanding processes, including word sense disambiguation and information extraction. Since the formal semantic annotations are keyed to conceptual structures which are independent of any individual language, they are available for the creation of parallel lexicon databases of other languages. The semantic structures in the databases will facilitate matches from one language to another, in machine translation and machine-assisted translation, while the syntactic structures allow the production of appropriate grammatical sentences in the target language.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86132</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86132</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n778" target="n779">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Framenet++: An On-Line Lexical Semantic Resource and its Application to Speech and Language Understanding</data>
      <data key="e_abstract">This is the first year funding of a three-year continuing award. Robust domain-independent language understanding is essential for multilingual information extraction, summarization, question answering, and automatic translation. With pervasive computing environments soon to come, language understanding will become even more indispensable for interacting with artifacts of widely different functionalities. The field of natural language understanding has made significant progress in the last fifteen years. A large part of this gain is due to the sophisticated combination of statistical algorithms with template-based algorithms tailored to specific domains like air-traffic information, travel scheduling, and business news. But any real solution to the problem of domain-independent understanding will require moving beyond template-based monolingual systems to more flexible, general purpose HCI systems via three key innovations: (1) a domain-independent semantic language as the back end for these understanding systems, replacing the current domain-restricted templates and slots; (2) rich semantic lexical databases which are broad enough to cover the necessary words for language engineering tasks, and deep enough in usable semantic information to support true domain-independent understanding; and (3) sophisticated techniques for performing this mapping.&lt;br/&gt;&lt;br/&gt;This project will develop these three components: a very large lexical database FrameNet++, a semantic language designed for domain-independent understanding tasks, and the tools for applying it to and evaluating it on key NLU applications. The semantic language and lexical database are based on formalizing the semantic frames and the semantic and syntactic combinatory properties - the valences - of a significant portion of the English lexicon. FrameNet++ will offer significantly richer semantic information than is available in current databases like COMLEX and WordNet, by characterizing the conceptual frames within which words are defined and identifying the semantic roles which the arguments of these words can take. These roles and frames are key to building domain-independent language understanding applications. The project will focus from the start on specific NLU applications: word sense disambiguation, information extraction, multilingual information extraction, and an eventual extension to text data mining. For each application, the PI and his team will apply the FrameNet++ system to improve the domain independence of the semantic components, using statistical algorithms for semantic annotation that we have already begun to implement. These applications will in turn provide a rich and realistic evaluation framework to guide FrameNet++ development, and will encourage potential users to apply it to a wide variety of tasks.&lt;br/&gt;&lt;br/&gt;The FrameNet++ database will be capable of serving many purposes. Provided with statistical information about frequencies of words, word/sense mappings, and combinatorial patterns linked to word senses, it will be usable in various automatic language understanding processes, including word sense disambiguation and information extraction. Since the formal semantic annotations are keyed to conceptual structures which are independent of any individual language, they are available for the creation of parallel lexicon databases of other languages. The semantic structures in the databases will facilitate matches from one language to another, in machine translation and machine-assisted translation, while the syntactic structures allow the production of appropriate grammatical sentences in the target language.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86132</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86132</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n778" target="n780">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Framenet++: An On-Line Lexical Semantic Resource and its Application to Speech and Language Understanding</data>
      <data key="e_abstract">This is the first year funding of a three-year continuing award. Robust domain-independent language understanding is essential for multilingual information extraction, summarization, question answering, and automatic translation. With pervasive computing environments soon to come, language understanding will become even more indispensable for interacting with artifacts of widely different functionalities. The field of natural language understanding has made significant progress in the last fifteen years. A large part of this gain is due to the sophisticated combination of statistical algorithms with template-based algorithms tailored to specific domains like air-traffic information, travel scheduling, and business news. But any real solution to the problem of domain-independent understanding will require moving beyond template-based monolingual systems to more flexible, general purpose HCI systems via three key innovations: (1) a domain-independent semantic language as the back end for these understanding systems, replacing the current domain-restricted templates and slots; (2) rich semantic lexical databases which are broad enough to cover the necessary words for language engineering tasks, and deep enough in usable semantic information to support true domain-independent understanding; and (3) sophisticated techniques for performing this mapping.&lt;br/&gt;&lt;br/&gt;This project will develop these three components: a very large lexical database FrameNet++, a semantic language designed for domain-independent understanding tasks, and the tools for applying it to and evaluating it on key NLU applications. The semantic language and lexical database are based on formalizing the semantic frames and the semantic and syntactic combinatory properties - the valences - of a significant portion of the English lexicon. FrameNet++ will offer significantly richer semantic information than is available in current databases like COMLEX and WordNet, by characterizing the conceptual frames within which words are defined and identifying the semantic roles which the arguments of these words can take. These roles and frames are key to building domain-independent language understanding applications. The project will focus from the start on specific NLU applications: word sense disambiguation, information extraction, multilingual information extraction, and an eventual extension to text data mining. For each application, the PI and his team will apply the FrameNet++ system to improve the domain independence of the semantic components, using statistical algorithms for semantic annotation that we have already begun to implement. These applications will in turn provide a rich and realistic evaluation framework to guide FrameNet++ development, and will encourage potential users to apply it to a wide variety of tasks.&lt;br/&gt;&lt;br/&gt;The FrameNet++ database will be capable of serving many purposes. Provided with statistical information about frequencies of words, word/sense mappings, and combinatorial patterns linked to word senses, it will be usable in various automatic language understanding processes, including word sense disambiguation and information extraction. Since the formal semantic annotations are keyed to conceptual structures which are independent of any individual language, they are available for the creation of parallel lexicon databases of other languages. The semantic structures in the databases will facilitate matches from one language to another, in machine translation and machine-assisted translation, while the syntactic structures allow the production of appropriate grammatical sentences in the target language.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86132</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86132</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n779" target="n780">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Framenet++: An On-Line Lexical Semantic Resource and its Application to Speech and Language Understanding</data>
      <data key="e_abstract">This is the first year funding of a three-year continuing award. Robust domain-independent language understanding is essential for multilingual information extraction, summarization, question answering, and automatic translation. With pervasive computing environments soon to come, language understanding will become even more indispensable for interacting with artifacts of widely different functionalities. The field of natural language understanding has made significant progress in the last fifteen years. A large part of this gain is due to the sophisticated combination of statistical algorithms with template-based algorithms tailored to specific domains like air-traffic information, travel scheduling, and business news. But any real solution to the problem of domain-independent understanding will require moving beyond template-based monolingual systems to more flexible, general purpose HCI systems via three key innovations: (1) a domain-independent semantic language as the back end for these understanding systems, replacing the current domain-restricted templates and slots; (2) rich semantic lexical databases which are broad enough to cover the necessary words for language engineering tasks, and deep enough in usable semantic information to support true domain-independent understanding; and (3) sophisticated techniques for performing this mapping.&lt;br/&gt;&lt;br/&gt;This project will develop these three components: a very large lexical database FrameNet++, a semantic language designed for domain-independent understanding tasks, and the tools for applying it to and evaluating it on key NLU applications. The semantic language and lexical database are based on formalizing the semantic frames and the semantic and syntactic combinatory properties - the valences - of a significant portion of the English lexicon. FrameNet++ will offer significantly richer semantic information than is available in current databases like COMLEX and WordNet, by characterizing the conceptual frames within which words are defined and identifying the semantic roles which the arguments of these words can take. These roles and frames are key to building domain-independent language understanding applications. The project will focus from the start on specific NLU applications: word sense disambiguation, information extraction, multilingual information extraction, and an eventual extension to text data mining. For each application, the PI and his team will apply the FrameNet++ system to improve the domain independence of the semantic components, using statistical algorithms for semantic annotation that we have already begun to implement. These applications will in turn provide a rich and realistic evaluation framework to guide FrameNet++ development, and will encourage potential users to apply it to a wide variety of tasks.&lt;br/&gt;&lt;br/&gt;The FrameNet++ database will be capable of serving many purposes. Provided with statistical information about frequencies of words, word/sense mappings, and combinatorial patterns linked to word senses, it will be usable in various automatic language understanding processes, including word sense disambiguation and information extraction. Since the formal semantic annotations are keyed to conceptual structures which are independent of any individual language, they are available for the creation of parallel lexicon databases of other languages. The semantic structures in the databases will facilitate matches from one language to another, in machine translation and machine-assisted translation, while the syntactic structures allow the production of appropriate grammatical sentences in the target language.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86132</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86132</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n692" target="n784">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research: Real-time Capture, Management and Reconstruction of Spatio-Temporal Events</data>
      <data key="e_abstract">With the advances in embedded processors, low cost sensor technologies, and wireless communication, unprecedented amounts of diverse types of information about the real world and its activities are being generated. Much of the information is spatio-temporal in nature; concerning objects dispersed in space and time, and interacting and communicating with each other and their surroundings. An infrastructure that facilitates real-time capture, storage, processing, display, and analysis of the information generated will truly revolutionize a wide variety of application domains. Examples of domains that will benefit from this technology include avionics, ground traffic, commercial applications such as ship-ping and transportation, emergency response and disaster relief operations, physical phenomenon such as weather and storm tracking, forest fire tracking, migration patterns of animals/birds, command and control, smart environments, etc. Applications in the above domains require real-time monitoring, tracking and analysis of objects/events/phenomena in space and time. &lt;br/&gt;&lt;br/&gt;An integral component of such sensor enriched communication and information infrastructure is a database management technology that allows seamless access to information dispersed across a hierarchy of storage, communication and processing units - from sensor devices, where data originates, to large data banks where the information generated is stored for analysis and mining. This research will explore next generation database management system technology that provides effective support for information processing in highly distributed and dynamic sensor-enriched environments. The approach taken will be end-to-end - that is, research will be conducted on all aspects of the system ranging from representation, data modeling, query languages, data structures, query optimization, query processing, distribution, and concurrent accesses. A prototype database management infrastructure that supports highly dynamic geographically dispersed spatio-temporal data, multi-resolution representation of data, and provides effective support for visualization and analysis will be developed.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86116</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86116</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n787" target="n788">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Collaborative Research: Progress-Based Resource Management Using Control</data>
      <data key="e_abstract">The next generation of computing will be dominated by applications which fall between the existing categories &quot;real-time&quot; and &quot;general purpose&quot;. These &quot;real-rate&quot; applications have performance contraints, such as throughput, smoothness, and bounded latency that are looser than real-time but stricter than general purpose applications. This project will further the state-of-the-art in resource management for real-rate applications, aiming to provide more predictable, accurate, and dynamic resource allocation. In particular, it will develop mechanisms and policies that can manage resources accurately for variable rate, high performance applications such as video streaming, sensor networks, or routers. It will also develop modeling and analysis techniques based on control theory that support reasoning about the dynamic behavior of applications and computer systems. Activities included in the project include: evaluating control algorithms for suitability for different application classes, inventing implementation techniques for building low overhead highly accurate controllers, developing techniques to analyze complex systems composed of multiple adaptive applications and resource managers, and defining notions such as quality-of-service or correctness that incorporate dynamics.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">9.98844e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98844e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n787" target="n789">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Collaborative Research: Progress-Based Resource Management Using Control</data>
      <data key="e_abstract">The next generation of computing will be dominated by applications which fall between the existing categories &quot;real-time&quot; and &quot;general purpose&quot;. These &quot;real-rate&quot; applications have performance contraints, such as throughput, smoothness, and bounded latency that are looser than real-time but stricter than general purpose applications. This project will further the state-of-the-art in resource management for real-rate applications, aiming to provide more predictable, accurate, and dynamic resource allocation. In particular, it will develop mechanisms and policies that can manage resources accurately for variable rate, high performance applications such as video streaming, sensor networks, or routers. It will also develop modeling and analysis techniques based on control theory that support reasoning about the dynamic behavior of applications and computer systems. Activities included in the project include: evaluating control algorithms for suitability for different application classes, inventing implementation techniques for building low overhead highly accurate controllers, developing techniques to analyze complex systems composed of multiple adaptive applications and resource managers, and defining notions such as quality-of-service or correctness that incorporate dynamics.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">9.98844e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98844e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n788" target="n789">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Collaborative Research: Progress-Based Resource Management Using Control</data>
      <data key="e_abstract">The next generation of computing will be dominated by applications which fall between the existing categories &quot;real-time&quot; and &quot;general purpose&quot;. These &quot;real-rate&quot; applications have performance contraints, such as throughput, smoothness, and bounded latency that are looser than real-time but stricter than general purpose applications. This project will further the state-of-the-art in resource management for real-rate applications, aiming to provide more predictable, accurate, and dynamic resource allocation. In particular, it will develop mechanisms and policies that can manage resources accurately for variable rate, high performance applications such as video streaming, sensor networks, or routers. It will also develop modeling and analysis techniques based on control theory that support reasoning about the dynamic behavior of applications and computer systems. Activities included in the project include: evaluating control algorithms for suitability for different application classes, inventing implementation techniques for building low overhead highly accurate controllers, developing techniques to analyze complex systems composed of multiple adaptive applications and resource managers, and defining notions such as quality-of-service or correctness that incorporate dynamics.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">9.98844e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98844e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n793" target="n794">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">How to Generate Random Topologies with Internet-like Characteristics</data>
      <data key="e_abstract">The exponential growth of the number of Internet hosts has been well documented in the trade press. The research community, however, has not seen many systematic empirical studies of how the Internet topology evolves over time and in space. Most recently, the authors of [FFF99] report on several power-law relationships observed on Autonomous Systems&apos; (AS) connectivity degree, degree frequencies, and the neighborhood size within any given hop count from an AS. This pioneering work represents a first important step toward a better understanding of the dynamic nature of the actual Internet topology.&lt;br/&gt; The need for realistic random topologies in simulations has long been recognized by researchers working on routing and multicast protocols, e.g. [BE90, ZGLA91, WE94]; more recently, the need for realistic random topologies has also been voiced by researchers studying traffic dynamics and protocol behavior [MS94, FGHW99, F+00]. In recognition of this, several topology generators have been proposed in the literature. The most recent one, proposed in [J+00] and called Inet, takes advantage of the power-law relationships reported in [FFF99] in its construction of random topologies.&lt;br/&gt; The preliminary work conducted as part of this research and reported in this proposal shows exponential growth over time in frequency of every outdegree, the outdegree of every rank, and the neighborhood size within any given hop count from an AS. The preliminary results also show that only the random topologies generated by the Inet model have the power-law relationships similar to those of the Internet. Unfortunately, these random topologies do not exhibit the exponential growth observed of the Internet. A new Inet topology generator, called New Inet,was constructed to generate topologies exhibiting both the power law relationships and exponential growth rates over time.&lt;br/&gt;&lt;br/&gt;The research proposed here consists of three parts:&lt;br/&gt;1. To investigate whether or not proposed topology models are in fact capturing the essence of certain underlying network design mechanisms or engineering constraints that result in random topologies that perforce exhibit many of the empirically observed phenomena. To this end the PIs focus on two particular approaches concerning the emergence of scaling phenomena associated with Internet-like graph structures in the context of the models of Barabasi and Albert [BAJ99, BA99] and of Carlson and Doyle [CD99a, CD99b].&lt;br/&gt;2. An in-depth analysis of the properties of connectivity graphs at the intra-AS level. In particular, the Pis are interested in large ASs spanning wide geographic area.&lt;br/&gt;3. To determine if trees constructed from a given graph can serve as a &quot;fingerprint&quot; of the graph from which various properties of the graph can be derived; and to model policy routing on the Internet as such trees. In particular, the PIs want to check whether these tree structures exhibit scaling laws that have been found ubiquitous in the context of river networks such as the Horton-Strahler laws [Hor45, Str57].&lt;br/&gt; The PIs propose to continue studying AS-level connectivity data made available by the National Laboratory for Applied Network Research (NLANR) for better understanding of how ASs connect to each other, how this connectivity changes over time, and how each individual AS can be modeled in long-running simulations. Additionally, the PIs also propose to model router-level connectivity within ASs. For this, the PIs have access to intra-AS connectivity information from the AT&amp;T WorldNet backbone.&lt;br/&gt; Funding for two graduate student research assistants was requested in this proposal. Both students will spend their academic year at the University of Michigan under the supervision of the PI, Sugih Jamin. They will help the PIs carry out research on studying AS-level connectivity, intra-AS connectivity, source-rooted trees constructed from traceroute data, and improve the New Inet random topology generator. During summers, one or both of the students will visit AT&amp;T Labs-Research in Florham Park under the supervision of the co-PI, Walter Willinger, to collect and study data on AT&amp;T WorldNet backbone. To improve the collaboration, the PI and co-PI may also visit each other&apos;ssite during the project.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">82287</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82287</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n795" target="n796">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: End-User Software Engineering</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;The objective is to improve the reliability of software produced by&lt;br/&gt;end-user programming languages in general, and by spreadsheet&lt;br/&gt;languages in particular. The approach is to address software&lt;br/&gt;engineering issues as an integrated whole in ways that incrementally&lt;br/&gt;interact with each other and with the user. For example, the system&lt;br/&gt;will notice the user&apos;s reactions to sample values she tests, and will&lt;br/&gt;then suggest general principles about the spreadsheet, encouraging the&lt;br/&gt;user to refine or modify the suggestions. This collaboration between&lt;br/&gt;the system and user will incrementally generate formal specifications&lt;br/&gt;as the spreadsheet evolves. These specifications can in turn be fed&lt;br/&gt;back to enhance reliability -- by automatically suggesting appropriate&lt;br/&gt;test values, by helping locate faults, and by ensuring continuing&lt;br/&gt;consistency with the specifications. The research involves three&lt;br/&gt;facets: developing an interactive mechanism for user-system&lt;br/&gt;collaboration, developing algorithms for the system&apos;s part of the&lt;br/&gt;collaboration, and conducting experiments to evaluate effectiveness.&lt;br/&gt;This is the first research attempting to bring fundamental software&lt;br/&gt;engineering principles to bear on end-user programming. Since the use&lt;br/&gt;of end-user-written programs and spreadsheets is very widespread and&lt;br/&gt;their lack of reliability is pervasive, improved reliability will&lt;br/&gt;impact a potentially huge number of business and personal computer&lt;br/&gt;users.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82265</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82265</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n795" target="n797">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: End-User Software Engineering</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;The objective is to improve the reliability of software produced by&lt;br/&gt;end-user programming languages in general, and by spreadsheet&lt;br/&gt;languages in particular. The approach is to address software&lt;br/&gt;engineering issues as an integrated whole in ways that incrementally&lt;br/&gt;interact with each other and with the user. For example, the system&lt;br/&gt;will notice the user&apos;s reactions to sample values she tests, and will&lt;br/&gt;then suggest general principles about the spreadsheet, encouraging the&lt;br/&gt;user to refine or modify the suggestions. This collaboration between&lt;br/&gt;the system and user will incrementally generate formal specifications&lt;br/&gt;as the spreadsheet evolves. These specifications can in turn be fed&lt;br/&gt;back to enhance reliability -- by automatically suggesting appropriate&lt;br/&gt;test values, by helping locate faults, and by ensuring continuing&lt;br/&gt;consistency with the specifications. The research involves three&lt;br/&gt;facets: developing an interactive mechanism for user-system&lt;br/&gt;collaboration, developing algorithms for the system&apos;s part of the&lt;br/&gt;collaboration, and conducting experiments to evaluate effectiveness.&lt;br/&gt;This is the first research attempting to bring fundamental software&lt;br/&gt;engineering principles to bear on end-user programming. Since the use&lt;br/&gt;of end-user-written programs and spreadsheets is very widespread and&lt;br/&gt;their lack of reliability is pervasive, improved reliability will&lt;br/&gt;impact a potentially huge number of business and personal computer&lt;br/&gt;users.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82265</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82265</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n796" target="n797">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: End-User Software Engineering</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;The objective is to improve the reliability of software produced by&lt;br/&gt;end-user programming languages in general, and by spreadsheet&lt;br/&gt;languages in particular. The approach is to address software&lt;br/&gt;engineering issues as an integrated whole in ways that incrementally&lt;br/&gt;interact with each other and with the user. For example, the system&lt;br/&gt;will notice the user&apos;s reactions to sample values she tests, and will&lt;br/&gt;then suggest general principles about the spreadsheet, encouraging the&lt;br/&gt;user to refine or modify the suggestions. This collaboration between&lt;br/&gt;the system and user will incrementally generate formal specifications&lt;br/&gt;as the spreadsheet evolves. These specifications can in turn be fed&lt;br/&gt;back to enhance reliability -- by automatically suggesting appropriate&lt;br/&gt;test values, by helping locate faults, and by ensuring continuing&lt;br/&gt;consistency with the specifications. The research involves three&lt;br/&gt;facets: developing an interactive mechanism for user-system&lt;br/&gt;collaboration, developing algorithms for the system&apos;s part of the&lt;br/&gt;collaboration, and conducting experiments to evaluate effectiveness.&lt;br/&gt;This is the first research attempting to bring fundamental software&lt;br/&gt;engineering principles to bear on end-user programming. Since the use&lt;br/&gt;of end-user-written programs and spreadsheets is very widespread and&lt;br/&gt;their lack of reliability is pervasive, improved reliability will&lt;br/&gt;impact a potentially huge number of business and personal computer&lt;br/&gt;users.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82265</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82265</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n224" target="n801">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Theory and Design of Watermarking Codes</data>
      <data key="e_abstract">With the widespread dissemination of digital video, images, audio and other data on the Internet and other media, information protection and copyright protection have become areas of vital importance. These applications have generated an extraordinary level of interest in digital watermarking techniques in recent years. Whereas novel watermarking algorithms and novel ways to defeat them have been developed, fundamental principles of information theory have barely skimmed the surface of this field, and recent results by the PI have shown that existing schemes operate far below the ultimate achievable limits. We intend to develop our ideas for a theory of watermarking codes and to construct codes that approach capacity. We also plan to develop a closely related thrust of research on robust watermarking and authentication techniques for images and video. Our research will be guided by extensive body of knowledge (much of it developed in the last decade) developed in the context of modern communication systems on one hand, and image analysis on the other hand. Our plan is to explore the following topics:&lt;br/&gt;&lt;br/&gt;Codes for Gaussian channels. We have recently derived closed-form solutions for the hiding capacity of channels involving Gaussian sources and squared error distortion metrics. Our first goal in this research is to develop codes whose performance approaches capacity for such channels. This model, in addition to being useful in the context of practical watermarking applications, will shed light on the fundamental issues involved in constructing watermarking codes. Hence its implications go beyond the particular model studied.&lt;br/&gt;&lt;br/&gt;Estimation of Attack Channel Parameters. In blind watermarking applications, the decoder does not have access to the original data, and does not know the particular attack that may have been used to corrupt the data. Desynchronization attacks such as scaling, shifting, rotating or warping of image data can then be deadly. We will explore fundamental mechanisms for the decoder to estimate the parameters of such attacks.&lt;br/&gt;&lt;br/&gt;Codes for arbitrary channels. While Gaussian channels are the worst channels under certain conditions, watermarking codes need to be robust against a variety of attacks. One of our goals is to develop codes and decoding techniques that perform well not only against Gaussian noise attacks, but also against desynchronization attacks, erasures, and other attacks.&lt;br/&gt;&lt;br/&gt;Application to Image and Video Watermarking. While information theory and coding theory provide fundamental guiding principles to the design of watermarking systems, application of these principles presents unique challenges in specific situations such as image and video watermarking. These two applications will be investigated in detail.&lt;br/&gt;&lt;br/&gt;Fingerprinting and Authentication Codes. There are several information-hiding problems closely related to watermarking that we intend to explore.&lt;br/&gt;&lt;br/&gt; Many graduate students are interested in moving into such an attractive research area, which combines breadth and strong emphasis on fundamentals with practical relevance. We intend to train these students for leadership roles in information technology. We also plan to involve undergraduate students, as they find this subject to be a truly enjoyable learning experience.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81268</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81268</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n803" target="n804">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: An Optimizing Compiler for Languages with Programmable Memory Models</data>
      <data key="e_abstract">The memory model for a programming language specifies&lt;br/&gt;the relationship between the order in which data accesses&lt;br/&gt;appear in a program and the order seen during execution by&lt;br/&gt;the different program components With the advent of parallel&lt;br/&gt;programming environments like Posix threads, Java, and OpenMP, multi-&lt;br/&gt;threaded explicitly parallel programs have become much more&lt;br/&gt;frequent. This increases the need for memory models that&lt;br/&gt;are easy to understand and efficient so that correct pro-&lt;br/&gt;grams can be developed and still give good performance.&lt;br/&gt;Unfortunately, the usability of memory models, their impact&lt;br/&gt;on performance, and the compiler technology needed to per-&lt;br/&gt;form optimizations of parallel programs are poorly under-&lt;br/&gt;stood. The result is that current memory models tend to&lt;br/&gt;favor performance over usability by restricting the programs&lt;br/&gt;that can be written with them, or by being difficult to&lt;br/&gt;understand. The objective of this project is to study com-&lt;br/&gt;piler techniques to optimize explicitly parallel programs by&lt;br/&gt;using optimizations and analysis algorithms structured to&lt;br/&gt;handle a broad class of consistency models. The techniques&lt;br/&gt;studied will be implemented in a compiler that will serve&lt;br/&gt;as a testbed for prototyping and studying programming&lt;br/&gt;language memory models and for studying the optimization&lt;br/&gt;and analysis of explicitly parallel programs.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81265</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81265</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n805" target="n806">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research: Optimization and Integrated Control of Low Power Wireless Multimedia Networks</data>
      <data key="e_abstract">Recent years have seen an explosive growth in the number of multimedia devices and communication tech-nologies. While these multimedia technologies still depend on wireline links, the future of multimedia com-munication is in wireless. Mobile multimedia networks will provide seamless communication between roam-ing users. Applications such as video conferencing or internet access will become available from any where in the world. Widely varying classes of network traffic (traffic heterogeneity) that change over time will be generated by multimedia applications. The network itself will consist of both wireline and wireless links (network heterogeneity). We are already beginning to feel this change.&lt;br/&gt;A major constraint in multimedia wireless technology is the limited power budget. Mobile devices such as laptops and personal digital assistants have limited battery life. The re-chargeable battery technology has resulted only in a small increase in battery efficiency. So, it is important to optimize network protocols for minimum power to ensure the continued growth of wireless based multimedia communication. Many attempts have been made towards this goal at individual layers of the network protocol stack. Most of these efforts have concentrated only at the physical and link layers. In this proposal, we deal with the wireless network protocol stack as a whole and provide a unified framework to optimize both computation and transmission power at various layers of the protocol stack. A power manager serves as the central core of our framework. The&lt;br/&gt;different layers of the protocol stack communicate with each other through the power manager that makes adaptive policy decisions based on the network, traffic, and power limitations. This is a uniqueness of the proposal. In the conventional model, the network layers have minimal or no communication at all leading to poor performance compared to the proposed framework. The proposed methods strive to optimize the quality of service (QoS) with minimum power consumption at the mobile wireless nodes. Towards achieving our objectives we consider the following research issues :&lt;br/&gt;&lt;br/&gt;Adaptive source coding and modulation strategies at the physical layer that minimize power for a desired reliability. The adaptation is done based on channel state estimates produced by the link layer.&lt;br/&gt;&lt;br/&gt;New error-resilient coding methods that do not have the drawbacks of the conventional forward error correcting codes. Little overhead, graceful degradation, and simple encoding and decoding that reduce the power requirements for computations are the main strengths of the coder. The coders are especially well-suited for state-of-the-art compression standards like JPEG, MPEG, and H.263. Radically different on-line channel state estimators are also proposed.&lt;br/&gt;&lt;br/&gt;Power-aware transport control protocols that distinguish between various channel and battery state.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Application layer adaptation mechanisms to conserve power. The mechanisms use techniques from networking and signal processing areas to improve the performance.&lt;br/&gt;&lt;br/&gt;Adaptive power manager (integrated controller) policies that control the entire wireless network from a global point of view. Reliability, bandwidth, robustness and power considerations are taken into account for policy decisions.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;A simulation framework to experiment with different hardware configurations and network topologies and constraints. The developed framework will be help to validate and update our theories. This tool that will be made available through the web will be invaluable for other researchers in designing future energy-efficient wireless multimedia systems.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82064</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82064</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n805" target="n807">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research: Optimization and Integrated Control of Low Power Wireless Multimedia Networks</data>
      <data key="e_abstract">Recent years have seen an explosive growth in the number of multimedia devices and communication tech-nologies. While these multimedia technologies still depend on wireline links, the future of multimedia com-munication is in wireless. Mobile multimedia networks will provide seamless communication between roam-ing users. Applications such as video conferencing or internet access will become available from any where in the world. Widely varying classes of network traffic (traffic heterogeneity) that change over time will be generated by multimedia applications. The network itself will consist of both wireline and wireless links (network heterogeneity). We are already beginning to feel this change.&lt;br/&gt;A major constraint in multimedia wireless technology is the limited power budget. Mobile devices such as laptops and personal digital assistants have limited battery life. The re-chargeable battery technology has resulted only in a small increase in battery efficiency. So, it is important to optimize network protocols for minimum power to ensure the continued growth of wireless based multimedia communication. Many attempts have been made towards this goal at individual layers of the network protocol stack. Most of these efforts have concentrated only at the physical and link layers. In this proposal, we deal with the wireless network protocol stack as a whole and provide a unified framework to optimize both computation and transmission power at various layers of the protocol stack. A power manager serves as the central core of our framework. The&lt;br/&gt;different layers of the protocol stack communicate with each other through the power manager that makes adaptive policy decisions based on the network, traffic, and power limitations. This is a uniqueness of the proposal. In the conventional model, the network layers have minimal or no communication at all leading to poor performance compared to the proposed framework. The proposed methods strive to optimize the quality of service (QoS) with minimum power consumption at the mobile wireless nodes. Towards achieving our objectives we consider the following research issues :&lt;br/&gt;&lt;br/&gt;Adaptive source coding and modulation strategies at the physical layer that minimize power for a desired reliability. The adaptation is done based on channel state estimates produced by the link layer.&lt;br/&gt;&lt;br/&gt;New error-resilient coding methods that do not have the drawbacks of the conventional forward error correcting codes. Little overhead, graceful degradation, and simple encoding and decoding that reduce the power requirements for computations are the main strengths of the coder. The coders are especially well-suited for state-of-the-art compression standards like JPEG, MPEG, and H.263. Radically different on-line channel state estimators are also proposed.&lt;br/&gt;&lt;br/&gt;Power-aware transport control protocols that distinguish between various channel and battery state.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Application layer adaptation mechanisms to conserve power. The mechanisms use techniques from networking and signal processing areas to improve the performance.&lt;br/&gt;&lt;br/&gt;Adaptive power manager (integrated controller) policies that control the entire wireless network from a global point of view. Reliability, bandwidth, robustness and power considerations are taken into account for policy decisions.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;A simulation framework to experiment with different hardware configurations and network topologies and constraints. The developed framework will be help to validate and update our theories. This tool that will be made available through the web will be invaluable for other researchers in designing future energy-efficient wireless multimedia systems.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82064</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82064</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n806" target="n807">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research: Optimization and Integrated Control of Low Power Wireless Multimedia Networks</data>
      <data key="e_abstract">Recent years have seen an explosive growth in the number of multimedia devices and communication tech-nologies. While these multimedia technologies still depend on wireline links, the future of multimedia com-munication is in wireless. Mobile multimedia networks will provide seamless communication between roam-ing users. Applications such as video conferencing or internet access will become available from any where in the world. Widely varying classes of network traffic (traffic heterogeneity) that change over time will be generated by multimedia applications. The network itself will consist of both wireline and wireless links (network heterogeneity). We are already beginning to feel this change.&lt;br/&gt;A major constraint in multimedia wireless technology is the limited power budget. Mobile devices such as laptops and personal digital assistants have limited battery life. The re-chargeable battery technology has resulted only in a small increase in battery efficiency. So, it is important to optimize network protocols for minimum power to ensure the continued growth of wireless based multimedia communication. Many attempts have been made towards this goal at individual layers of the network protocol stack. Most of these efforts have concentrated only at the physical and link layers. In this proposal, we deal with the wireless network protocol stack as a whole and provide a unified framework to optimize both computation and transmission power at various layers of the protocol stack. A power manager serves as the central core of our framework. The&lt;br/&gt;different layers of the protocol stack communicate with each other through the power manager that makes adaptive policy decisions based on the network, traffic, and power limitations. This is a uniqueness of the proposal. In the conventional model, the network layers have minimal or no communication at all leading to poor performance compared to the proposed framework. The proposed methods strive to optimize the quality of service (QoS) with minimum power consumption at the mobile wireless nodes. Towards achieving our objectives we consider the following research issues :&lt;br/&gt;&lt;br/&gt;Adaptive source coding and modulation strategies at the physical layer that minimize power for a desired reliability. The adaptation is done based on channel state estimates produced by the link layer.&lt;br/&gt;&lt;br/&gt;New error-resilient coding methods that do not have the drawbacks of the conventional forward error correcting codes. Little overhead, graceful degradation, and simple encoding and decoding that reduce the power requirements for computations are the main strengths of the coder. The coders are especially well-suited for state-of-the-art compression standards like JPEG, MPEG, and H.263. Radically different on-line channel state estimators are also proposed.&lt;br/&gt;&lt;br/&gt;Power-aware transport control protocols that distinguish between various channel and battery state.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Application layer adaptation mechanisms to conserve power. The mechanisms use techniques from networking and signal processing areas to improve the performance.&lt;br/&gt;&lt;br/&gt;Adaptive power manager (integrated controller) policies that control the entire wireless network from a global point of view. Reliability, bandwidth, robustness and power considerations are taken into account for policy decisions.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;A simulation framework to experiment with different hardware configurations and network topologies and constraints. The developed framework will be help to validate and update our theories. This tool that will be made available through the web will be invaluable for other researchers in designing future energy-efficient wireless multimedia systems.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82064</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82064</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n808" target="n809">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Multi-Scale Interaction with 3D Data Environments</data>
      <data key="e_abstract">This is the first year funding of a three-year continuing award. This multi-disciplinary study will explore a novel approach based on 3D magnifying &quot;lenses&quot; to building improved highly interactive 3D interfaces to scientific and engineering data that allow users to easily interact at multiple scales and with multiple coordinate systems. To be effective it is necessary that interfaces to large data bases be highly interactive allowing users to &quot;drill down&quot; on demand to see detail as necessary, or to &quot;zoom out&quot; to get contextual information. The P1 will extend human spatial cognition theory to deal with common problems in interactive 3D visualization systems, and in particular to cover perceptual issues relating to eye-hand coordination in multi-scale environments using stereo and head tracking VR technologies to improve visualization and interaction. The PI will develop techniques to link views so that users do not become disoriented while interacting with data at different scales. A proof-of-concept prototype that enables scientists and engineers to monitor and control remotely operated vehicles or autonomous undersea vehicles will be used to conduct experimental user evaluations to provide feedback and measure success.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81292</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81292</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n808" target="n810">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Multi-Scale Interaction with 3D Data Environments</data>
      <data key="e_abstract">This is the first year funding of a three-year continuing award. This multi-disciplinary study will explore a novel approach based on 3D magnifying &quot;lenses&quot; to building improved highly interactive 3D interfaces to scientific and engineering data that allow users to easily interact at multiple scales and with multiple coordinate systems. To be effective it is necessary that interfaces to large data bases be highly interactive allowing users to &quot;drill down&quot; on demand to see detail as necessary, or to &quot;zoom out&quot; to get contextual information. The P1 will extend human spatial cognition theory to deal with common problems in interactive 3D visualization systems, and in particular to cover perceptual issues relating to eye-hand coordination in multi-scale environments using stereo and head tracking VR technologies to improve visualization and interaction. The PI will develop techniques to link views so that users do not become disoriented while interacting with data at different scales. A proof-of-concept prototype that enables scientists and engineers to monitor and control remotely operated vehicles or autonomous undersea vehicles will be used to conduct experimental user evaluations to provide feedback and measure success.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81292</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81292</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n809" target="n810">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Multi-Scale Interaction with 3D Data Environments</data>
      <data key="e_abstract">This is the first year funding of a three-year continuing award. This multi-disciplinary study will explore a novel approach based on 3D magnifying &quot;lenses&quot; to building improved highly interactive 3D interfaces to scientific and engineering data that allow users to easily interact at multiple scales and with multiple coordinate systems. To be effective it is necessary that interfaces to large data bases be highly interactive allowing users to &quot;drill down&quot; on demand to see detail as necessary, or to &quot;zoom out&quot; to get contextual information. The P1 will extend human spatial cognition theory to deal with common problems in interactive 3D visualization systems, and in particular to cover perceptual issues relating to eye-hand coordination in multi-scale environments using stereo and head tracking VR technologies to improve visualization and interaction. The PI will develop techniques to link views so that users do not become disoriented while interacting with data at different scales. A proof-of-concept prototype that enables scientists and engineers to monitor and control remotely operated vehicles or autonomous undersea vehicles will be used to conduct experimental user evaluations to provide feedback and measure success.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81292</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81292</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n811" target="n812">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Educational Innovation: Integrating Agent Technology into CISE Curriculum Using Lecturelets</data>
      <data key="e_abstract">EIA- 0086230&lt;br/&gt;Chen, Su-Shing&lt;br/&gt;University of Missouri - Columbia&lt;br/&gt;&lt;br/&gt;CISE Educational Innovation: Integrating Agent Technology into CISE Curriculum Using Lecturelets&lt;br/&gt;&lt;br/&gt;This project integrates research results from agent technology into both course delivery mechanisms and course content in computing curricula. Specific &quot;smart&quot; learning materials for individual subjects, including definition/description, examples, exercises, quizzes, projects, and supplemental information are developed and implemented into lecturelets (active XML documents with Java code) for customized interactive presentation of subjects. The lecturelets are self-contained and can be integrated into a wide range of courses. They contain both the XML documents and the instructions (templates/agents) on how documents should be processed or displayed according to the profile and model of a student. In addition to developing the lecturelets delivery system, the project also provides course materials (i.e. a collection of lecturlets) on agent technology that can be used as a stand-alone course or integrated into existing courses in such fields as software engineering, distributed systems, communication systems, and AI-related courses.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">86230</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86230</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n811" target="n813">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Educational Innovation: Integrating Agent Technology into CISE Curriculum Using Lecturelets</data>
      <data key="e_abstract">EIA- 0086230&lt;br/&gt;Chen, Su-Shing&lt;br/&gt;University of Missouri - Columbia&lt;br/&gt;&lt;br/&gt;CISE Educational Innovation: Integrating Agent Technology into CISE Curriculum Using Lecturelets&lt;br/&gt;&lt;br/&gt;This project integrates research results from agent technology into both course delivery mechanisms and course content in computing curricula. Specific &quot;smart&quot; learning materials for individual subjects, including definition/description, examples, exercises, quizzes, projects, and supplemental information are developed and implemented into lecturelets (active XML documents with Java code) for customized interactive presentation of subjects. The lecturelets are self-contained and can be integrated into a wide range of courses. They contain both the XML documents and the instructions (templates/agents) on how documents should be processed or displayed according to the profile and model of a student. In addition to developing the lecturelets delivery system, the project also provides course materials (i.e. a collection of lecturlets) on agent technology that can be used as a stand-alone course or integrated into existing courses in such fields as software engineering, distributed systems, communication systems, and AI-related courses.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">86230</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86230</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n812" target="n813">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Educational Innovation: Integrating Agent Technology into CISE Curriculum Using Lecturelets</data>
      <data key="e_abstract">EIA- 0086230&lt;br/&gt;Chen, Su-Shing&lt;br/&gt;University of Missouri - Columbia&lt;br/&gt;&lt;br/&gt;CISE Educational Innovation: Integrating Agent Technology into CISE Curriculum Using Lecturelets&lt;br/&gt;&lt;br/&gt;This project integrates research results from agent technology into both course delivery mechanisms and course content in computing curricula. Specific &quot;smart&quot; learning materials for individual subjects, including definition/description, examples, exercises, quizzes, projects, and supplemental information are developed and implemented into lecturelets (active XML documents with Java code) for customized interactive presentation of subjects. The lecturelets are self-contained and can be integrated into a wide range of courses. They contain both the XML documents and the instructions (templates/agents) on how documents should be processed or displayed according to the profile and model of a student. In addition to developing the lecturelets delivery system, the project also provides course materials (i.e. a collection of lecturlets) on agent technology that can be used as a stand-alone course or integrated into existing courses in such fields as software engineering, distributed systems, communication systems, and AI-related courses.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">86230</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86230</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n400" target="n815">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Mechanisms for Securing Emerging Applications</data>
      <data key="e_abstract">As computers become pervasive in the home and community, new applications&lt;br/&gt;will emerge that will make daily living easier by automating &lt;br/&gt;or assisting in a variety of human activities. Such&lt;br/&gt;applications will be information rich and they will create and manipulate&lt;br/&gt;sensitive information about the activities of their users, and &lt;br/&gt;the environment in which they live and work. At the Georgia &lt;br/&gt;Institute of Technology, an information rich &quot;Aware Home&apos;&apos; has&lt;br/&gt;been built to explore many such applications. Clearly, it is important&lt;br/&gt;that such applications be secured if they are to be deployed successfully.&lt;br/&gt;This project will undertake a range of research activities to&lt;br/&gt;secure future applications. These include new security policies &lt;br/&gt;for such applications, and intuitive and flexible access &lt;br/&gt;control models. A variety of automatic user identification &lt;br/&gt;techniques will also be investigated to authenticate sources&lt;br/&gt;of requests without requiring burdensome participation from the users &lt;br/&gt;making the requests. New notions of integrity for information accessed from &lt;br/&gt;outside sources will be developed. The authorization, authentication and &lt;br/&gt;integrity mechanisms will be used to build security services&lt;br/&gt;for emerging applications. The use of formal models &lt;br/&gt;will be explored to study important properties of the new security &lt;br/&gt;policies and access control models.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81276</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">81276</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n400" target="n512">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Mechanisms for Securing Emerging Applications</data>
      <data key="e_abstract">As computers become pervasive in the home and community, new applications&lt;br/&gt;will emerge that will make daily living easier by automating &lt;br/&gt;or assisting in a variety of human activities. Such&lt;br/&gt;applications will be information rich and they will create and manipulate&lt;br/&gt;sensitive information about the activities of their users, and &lt;br/&gt;the environment in which they live and work. At the Georgia &lt;br/&gt;Institute of Technology, an information rich &quot;Aware Home&apos;&apos; has&lt;br/&gt;been built to explore many such applications. Clearly, it is important&lt;br/&gt;that such applications be secured if they are to be deployed successfully.&lt;br/&gt;This project will undertake a range of research activities to&lt;br/&gt;secure future applications. These include new security policies &lt;br/&gt;for such applications, and intuitive and flexible access &lt;br/&gt;control models. A variety of automatic user identification &lt;br/&gt;techniques will also be investigated to authenticate sources&lt;br/&gt;of requests without requiring burdensome participation from the users &lt;br/&gt;making the requests. New notions of integrity for information accessed from &lt;br/&gt;outside sources will be developed. The authorization, authentication and &lt;br/&gt;integrity mechanisms will be used to build security services&lt;br/&gt;for emerging applications. The use of formal models &lt;br/&gt;will be explored to study important properties of the new security &lt;br/&gt;policies and access control models.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81276</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">81276</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n512" target="n815">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Mechanisms for Securing Emerging Applications</data>
      <data key="e_abstract">As computers become pervasive in the home and community, new applications&lt;br/&gt;will emerge that will make daily living easier by automating &lt;br/&gt;or assisting in a variety of human activities. Such&lt;br/&gt;applications will be information rich and they will create and manipulate&lt;br/&gt;sensitive information about the activities of their users, and &lt;br/&gt;the environment in which they live and work. At the Georgia &lt;br/&gt;Institute of Technology, an information rich &quot;Aware Home&apos;&apos; has&lt;br/&gt;been built to explore many such applications. Clearly, it is important&lt;br/&gt;that such applications be secured if they are to be deployed successfully.&lt;br/&gt;This project will undertake a range of research activities to&lt;br/&gt;secure future applications. These include new security policies &lt;br/&gt;for such applications, and intuitive and flexible access &lt;br/&gt;control models. A variety of automatic user identification &lt;br/&gt;techniques will also be investigated to authenticate sources&lt;br/&gt;of requests without requiring burdensome participation from the users &lt;br/&gt;making the requests. New notions of integrity for information accessed from &lt;br/&gt;outside sources will be developed. The authorization, authentication and &lt;br/&gt;integrity mechanisms will be used to build security services&lt;br/&gt;for emerging applications. The use of formal models &lt;br/&gt;will be explored to study important properties of the new security &lt;br/&gt;policies and access control models.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81276</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">81276</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n817" target="n818">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR/ACS :Collaborative Research: Advanced Algorithms for Visualizing Sources of Noise and Vibrations of Complex Structures</data>
      <data key="e_abstract">The goal of this project is to develop advanced algorithms for visualizing the sources of noise and vibrations of complex vibrating structures based on simple measurements of acoustic pressures. The software developed will provide an efficient, robust, and user-friendly tool to practicing engineers concerned with identification and control of noise and vibration. For example, this will allow aircraft designers to reduce the amount of noise inside the passenger cabin of an airplane, or make quieter cars. &lt;br/&gt;&lt;br/&gt;This project will address mathematical, computational, and engineering issues related to finding the source of acoustic radiation from a vibrating structure, also known as acoustic holography. This process involves determining the vibrational patterns in a structure based on simple acoustic pressure measurements from an array of microphones near the structure. The mathematical part of acoustic holography is an inverse problem, the direct problem being to determine the radiated acoustic pressure field in the fluid medium, given the vibration responses of the structure. Inverse problems such as this are ill-posed, thus requiring effective regularization techniques as filters. In this project, the researchers will pool their expertise and experience in advanced computational science, mathematical analysis, and mechanical engineering to elevate the acoustic holography to a higher level for solving engineering noise and vibration problems in a cost-effective manner. The algorithms and regularization techniques will be firmly based on current work in numerical linear algebra, mathematical analysis, and engineering practice. Iterative methods will be developed to provide fast computational techniques for both the single layer and Helmholtz-Kirchhoff integral equation methods. Estimates of stability and accuracy will be established to provide guidance for optimal regularization strategies, measurement locations, and number of expansion functions for the Helmholtz Equation Least Squares (HELS) method and combined HELS (CHELS) method. Experimental validation of the methods will be carried out for both interior and exterior regions on an aircraft cabin and a vehicle front end.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81270</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81270</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n817" target="n819">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR/ACS :Collaborative Research: Advanced Algorithms for Visualizing Sources of Noise and Vibrations of Complex Structures</data>
      <data key="e_abstract">The goal of this project is to develop advanced algorithms for visualizing the sources of noise and vibrations of complex vibrating structures based on simple measurements of acoustic pressures. The software developed will provide an efficient, robust, and user-friendly tool to practicing engineers concerned with identification and control of noise and vibration. For example, this will allow aircraft designers to reduce the amount of noise inside the passenger cabin of an airplane, or make quieter cars. &lt;br/&gt;&lt;br/&gt;This project will address mathematical, computational, and engineering issues related to finding the source of acoustic radiation from a vibrating structure, also known as acoustic holography. This process involves determining the vibrational patterns in a structure based on simple acoustic pressure measurements from an array of microphones near the structure. The mathematical part of acoustic holography is an inverse problem, the direct problem being to determine the radiated acoustic pressure field in the fluid medium, given the vibration responses of the structure. Inverse problems such as this are ill-posed, thus requiring effective regularization techniques as filters. In this project, the researchers will pool their expertise and experience in advanced computational science, mathematical analysis, and mechanical engineering to elevate the acoustic holography to a higher level for solving engineering noise and vibration problems in a cost-effective manner. The algorithms and regularization techniques will be firmly based on current work in numerical linear algebra, mathematical analysis, and engineering practice. Iterative methods will be developed to provide fast computational techniques for both the single layer and Helmholtz-Kirchhoff integral equation methods. Estimates of stability and accuracy will be established to provide guidance for optimal regularization strategies, measurement locations, and number of expansion functions for the Helmholtz Equation Least Squares (HELS) method and combined HELS (CHELS) method. Experimental validation of the methods will be carried out for both interior and exterior regions on an aircraft cabin and a vehicle front end.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81270</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81270</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n817" target="n820">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR/ACS :Collaborative Research: Advanced Algorithms for Visualizing Sources of Noise and Vibrations of Complex Structures</data>
      <data key="e_abstract">The goal of this project is to develop advanced algorithms for visualizing the sources of noise and vibrations of complex vibrating structures based on simple measurements of acoustic pressures. The software developed will provide an efficient, robust, and user-friendly tool to practicing engineers concerned with identification and control of noise and vibration. For example, this will allow aircraft designers to reduce the amount of noise inside the passenger cabin of an airplane, or make quieter cars. &lt;br/&gt;&lt;br/&gt;This project will address mathematical, computational, and engineering issues related to finding the source of acoustic radiation from a vibrating structure, also known as acoustic holography. This process involves determining the vibrational patterns in a structure based on simple acoustic pressure measurements from an array of microphones near the structure. The mathematical part of acoustic holography is an inverse problem, the direct problem being to determine the radiated acoustic pressure field in the fluid medium, given the vibration responses of the structure. Inverse problems such as this are ill-posed, thus requiring effective regularization techniques as filters. In this project, the researchers will pool their expertise and experience in advanced computational science, mathematical analysis, and mechanical engineering to elevate the acoustic holography to a higher level for solving engineering noise and vibration problems in a cost-effective manner. The algorithms and regularization techniques will be firmly based on current work in numerical linear algebra, mathematical analysis, and engineering practice. Iterative methods will be developed to provide fast computational techniques for both the single layer and Helmholtz-Kirchhoff integral equation methods. Estimates of stability and accuracy will be established to provide guidance for optimal regularization strategies, measurement locations, and number of expansion functions for the Helmholtz Equation Least Squares (HELS) method and combined HELS (CHELS) method. Experimental validation of the methods will be carried out for both interior and exterior regions on an aircraft cabin and a vehicle front end.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81270</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81270</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n818" target="n819">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR/ACS :Collaborative Research: Advanced Algorithms for Visualizing Sources of Noise and Vibrations of Complex Structures</data>
      <data key="e_abstract">The goal of this project is to develop advanced algorithms for visualizing the sources of noise and vibrations of complex vibrating structures based on simple measurements of acoustic pressures. The software developed will provide an efficient, robust, and user-friendly tool to practicing engineers concerned with identification and control of noise and vibration. For example, this will allow aircraft designers to reduce the amount of noise inside the passenger cabin of an airplane, or make quieter cars. &lt;br/&gt;&lt;br/&gt;This project will address mathematical, computational, and engineering issues related to finding the source of acoustic radiation from a vibrating structure, also known as acoustic holography. This process involves determining the vibrational patterns in a structure based on simple acoustic pressure measurements from an array of microphones near the structure. The mathematical part of acoustic holography is an inverse problem, the direct problem being to determine the radiated acoustic pressure field in the fluid medium, given the vibration responses of the structure. Inverse problems such as this are ill-posed, thus requiring effective regularization techniques as filters. In this project, the researchers will pool their expertise and experience in advanced computational science, mathematical analysis, and mechanical engineering to elevate the acoustic holography to a higher level for solving engineering noise and vibration problems in a cost-effective manner. The algorithms and regularization techniques will be firmly based on current work in numerical linear algebra, mathematical analysis, and engineering practice. Iterative methods will be developed to provide fast computational techniques for both the single layer and Helmholtz-Kirchhoff integral equation methods. Estimates of stability and accuracy will be established to provide guidance for optimal regularization strategies, measurement locations, and number of expansion functions for the Helmholtz Equation Least Squares (HELS) method and combined HELS (CHELS) method. Experimental validation of the methods will be carried out for both interior and exterior regions on an aircraft cabin and a vehicle front end.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81270</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81270</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n818" target="n820">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR/ACS :Collaborative Research: Advanced Algorithms for Visualizing Sources of Noise and Vibrations of Complex Structures</data>
      <data key="e_abstract">The goal of this project is to develop advanced algorithms for visualizing the sources of noise and vibrations of complex vibrating structures based on simple measurements of acoustic pressures. The software developed will provide an efficient, robust, and user-friendly tool to practicing engineers concerned with identification and control of noise and vibration. For example, this will allow aircraft designers to reduce the amount of noise inside the passenger cabin of an airplane, or make quieter cars. &lt;br/&gt;&lt;br/&gt;This project will address mathematical, computational, and engineering issues related to finding the source of acoustic radiation from a vibrating structure, also known as acoustic holography. This process involves determining the vibrational patterns in a structure based on simple acoustic pressure measurements from an array of microphones near the structure. The mathematical part of acoustic holography is an inverse problem, the direct problem being to determine the radiated acoustic pressure field in the fluid medium, given the vibration responses of the structure. Inverse problems such as this are ill-posed, thus requiring effective regularization techniques as filters. In this project, the researchers will pool their expertise and experience in advanced computational science, mathematical analysis, and mechanical engineering to elevate the acoustic holography to a higher level for solving engineering noise and vibration problems in a cost-effective manner. The algorithms and regularization techniques will be firmly based on current work in numerical linear algebra, mathematical analysis, and engineering practice. Iterative methods will be developed to provide fast computational techniques for both the single layer and Helmholtz-Kirchhoff integral equation methods. Estimates of stability and accuracy will be established to provide guidance for optimal regularization strategies, measurement locations, and number of expansion functions for the Helmholtz Equation Least Squares (HELS) method and combined HELS (CHELS) method. Experimental validation of the methods will be carried out for both interior and exterior regions on an aircraft cabin and a vehicle front end.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81270</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81270</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n819" target="n820">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR/ACS :Collaborative Research: Advanced Algorithms for Visualizing Sources of Noise and Vibrations of Complex Structures</data>
      <data key="e_abstract">The goal of this project is to develop advanced algorithms for visualizing the sources of noise and vibrations of complex vibrating structures based on simple measurements of acoustic pressures. The software developed will provide an efficient, robust, and user-friendly tool to practicing engineers concerned with identification and control of noise and vibration. For example, this will allow aircraft designers to reduce the amount of noise inside the passenger cabin of an airplane, or make quieter cars. &lt;br/&gt;&lt;br/&gt;This project will address mathematical, computational, and engineering issues related to finding the source of acoustic radiation from a vibrating structure, also known as acoustic holography. This process involves determining the vibrational patterns in a structure based on simple acoustic pressure measurements from an array of microphones near the structure. The mathematical part of acoustic holography is an inverse problem, the direct problem being to determine the radiated acoustic pressure field in the fluid medium, given the vibration responses of the structure. Inverse problems such as this are ill-posed, thus requiring effective regularization techniques as filters. In this project, the researchers will pool their expertise and experience in advanced computational science, mathematical analysis, and mechanical engineering to elevate the acoustic holography to a higher level for solving engineering noise and vibration problems in a cost-effective manner. The algorithms and regularization techniques will be firmly based on current work in numerical linear algebra, mathematical analysis, and engineering practice. Iterative methods will be developed to provide fast computational techniques for both the single layer and Helmholtz-Kirchhoff integral equation methods. Estimates of stability and accuracy will be established to provide guidance for optimal regularization strategies, measurement locations, and number of expansion functions for the Helmholtz Equation Least Squares (HELS) method and combined HELS (CHELS) method. Experimental validation of the methods will be carried out for both interior and exterior regions on an aircraft cabin and a vehicle front end.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81270</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81270</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n822" target="n823">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Ectropic Design: Intelligent Collaboration Spaces for Open-Source Software</data>
      <data key="e_abstract">Open-Source software has proven to be an exciting and productive&lt;br/&gt;alternative to traditional, process-driven software development. But&lt;br/&gt;Open Source has an inherent limitation--it has been most successful in&lt;br/&gt;those situations where a strong individual has served as a unifying&lt;br/&gt;force. Ectropic software design is proposed as a way of sustaining&lt;br/&gt;conceptual integrity in the absence of such a unifying force;&lt;br/&gt;ectrospaces are proposed as a collaboration technology to support&lt;br/&gt;ectropic design. The term ectropic is intended to describe software&lt;br/&gt;systems that maintain or increase their structuredness, as opposed to&lt;br/&gt;the more typical, entropic, case in which the structure and conceptual&lt;br/&gt;integrity of a system degrade as it evolves. Besides precisely&lt;br/&gt;defining the ectropic design process and building a prototype&lt;br/&gt;ectrospace, this project studies the power and practicality of&lt;br/&gt;ectropic software. It also investigates it in the setting of a&lt;br/&gt;sophomore-level, object-oriented development class. The class will&lt;br/&gt;undertake an Open-Source development effort using ectropic design and&lt;br/&gt;supported by an ectrospace. The resulting software and documentation&lt;br/&gt;will be distributed in Open Source form to the Internet community.</data>
      <data key="e_pgm">2880</data>
      <data key="e_label">9.98824e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98824e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n344" target="n825">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">WORKSHOP: Lake Tahoe Workshop on Hierarchical Visualization Methods - Oct. 15-17, 2000</data>
      <data key="e_abstract">This project supports a two-day workshop addressing topics in the general area of hierarchical scientific visualization. This includes topics such as hierarchical data representation and approximation; using hierarchical data representations for the purposes of interactive visualization emphasizing level-of-detail, adaptive, and view- dependent strategies; hierarchical data representations as they relate to distributed, remote, and collaborative visualization applications; and hierarchical visualization in immersive and virtual reality environments. The workshop will take place at Lake Tahoe, NV, on October 15-17, 2000.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">84843</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">84843</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n418" target="n419">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: An Internet Robotics Learning Testbed</data>
      <data key="e_abstract">EIA-0082041&lt;br/&gt;Kitts, Christopher&lt;br/&gt;Santa Clara University&lt;br/&gt;&lt;br/&gt;ITR: An Internet Robotics Learning Testbed&lt;br/&gt;&lt;br/&gt;This research involves the implementation of a project-based learning strategy through the development and use of an Internet Robotics Learning Testbed (IRLT). The IRLT will consist of a network of robots and control centers. The robotic devices will be used for exploratory missions ranging from ocean archeology to space science and the control centers will provide wireless communication with the remote robotic devices in order to send commands and receive data. Several IT learning challenges that emerge include human-computer interfaces, task planning, resource scheduling, fault detection and diagnosis, and goal-directed commanding. The IRLT will enable students to understand the need for specific IT applications, iteratively develop such applications, test application functionality in an operational environment, and experience application value in a mission setting. The IRLT will include a suite of services to support collaborating IT educators. These services will include an educator support program to train faculty in the use of the IRLT as an educational resource, several pre-packaged educational modules permitting the rapid and low-cost use of the testbed, and Web-based documentation providing engineering details, user manuals, guides for new experiments, and libraries of past IT applications. The IRLT will be centered in an undergraduate engineering curriculum and its high school outreach program. It will draw stakeholders from multiple universities and research laboratories who will serve as advisors and mentors.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82041</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82041</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n508" target="n831">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Multiple Time Scale Traffic Control for Next Generation Internets</data>
      <data key="e_abstract">The Internet is governed by a compendium of protocols spanning a number of functionalities including congestion control, routing, multicast, label switching, resource reservation and admission control, error control, and network management. These protocols engage in control actions at multiple time scales, from microseconds for routing and label switching, to milliseconds for congestion control, and the several second range for multicast, routing table updates, and network management. The behavior and performance of a network system is influenced by the workload that drive the protocols - reliable transport of heavy-tailed files, QoS-sensitive streaming of VBR video and audio, burstiness of connection arrivals, skewed make-up of short- and long-lived flows, and self-similar burstiness of multiplexed traffic - with Internet workload exhibiting variability and correlation at multiple time scales, from multiplicative scaling observed for IP flows in the millisecond range, to long-range correlations present in the second range.&lt;br/&gt; With QoS emerging as a unifying umbrella under which the various network subsystems can&lt;br/&gt;be organized and viewed as provisioning user-specified services, the need arises to coordinate and&lt;br/&gt;integrate the control activities - many of whom have direct impact on end-to-end QoS - such that&lt;br/&gt;both effective and efficient services can be rendered. In tandem with the QoS integration challenge, the PIs are presented with the opportunity to explicitly exploit the multiple time scale nature of network protocols and Internet workload to affect seemless and predictable services. In the proposed project, the PIs plan to address the following two-pronged problem: (1) exploit multiple time scale property of network protocols to facilitate effective coordination and integration of disjoint network controls for end-to-end QoS, and (2) exploit multiple time scale nature of Internet workload to achieve workload-sensitive traffic controls.&lt;br/&gt; Problem (1), in turn, is comprised of two key issues: sufficiency or separation conditions under which two network controls - e.g., routing and congestion control-acting at different time scales can be integrated without causing harmful effects with respect to stability and efficiency, and the effective coupling of protocols when time scale separation is not available - e.g., label control and congestion control. Problem (2) consists of three key issues: short-lived connection management using lightweight optimistic control, long-lived connection management using connection duration prediction and multi-layered feedback control, and QoS amplification through workload-sensitive, end-to-end and per-hop control. The two-fold challenge that the PIs plan to attack is grounded in well-defined technical problems, and at the same time, represents an innovative traffic control dimension with broader implications to the next generation Internet.&lt;br/&gt; The PIs bring expertise in core traffic control areas spanning congestion control (Park), QoS&lt;br/&gt;routing (Hou), multicast (Hou), label switching (Park), and traffic modeling (Park). Both PIs&lt;br/&gt;have significant experience in performance analysis and protocol design, and they complement each other&apos;s strengths with respect to simulation (Hou) and implementation (Park) based performance evaluation. The PIs will leverage existing benchmarking platforms - the Purdue Infobahn QoS testbed comprised of Cisco 7206 routers (Park) and the NetSim performance evaluation environment (Hou) - to implement, test, and benchmark the protocols. The research results, technology demonstration, and software prototypes will be made available through the Web to academic institutions, industrial affiliates, and the networking community at large.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82861</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82861</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n835" target="n836">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Parallel Algorithm Development on an upgraded Beowulf Cluster</data>
      <data key="e_abstract">EIA-0079710&lt;br/&gt;Fulton, Charles T.&lt;br/&gt;Florida Institute of Technology&lt;br/&gt;&lt;br/&gt;MRI: Parallel Algorithm Development on an Upgraded Beowulf Cluster&lt;br/&gt;&lt;br/&gt;It is proposed to update an existing 5 processor BEOWULF system to 32-workstations, each having two processors, for a total of 64 processors. The upgrade is motivated by the needs of the radiative heat transfer group at Florida Tech for a more powerful computing capability, and the development of parallel numerical algorithms for various large scale numerical linear algebra problems which have been under investigation by the applied mathematics and computer science groups for more than 10 years. Both research activities require the parallel capability and the large amount of RAM proposed on each node.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79710</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79710</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n835" target="n837">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Parallel Algorithm Development on an upgraded Beowulf Cluster</data>
      <data key="e_abstract">EIA-0079710&lt;br/&gt;Fulton, Charles T.&lt;br/&gt;Florida Institute of Technology&lt;br/&gt;&lt;br/&gt;MRI: Parallel Algorithm Development on an Upgraded Beowulf Cluster&lt;br/&gt;&lt;br/&gt;It is proposed to update an existing 5 processor BEOWULF system to 32-workstations, each having two processors, for a total of 64 processors. The upgrade is motivated by the needs of the radiative heat transfer group at Florida Tech for a more powerful computing capability, and the development of parallel numerical algorithms for various large scale numerical linear algebra problems which have been under investigation by the applied mathematics and computer science groups for more than 10 years. Both research activities require the parallel capability and the large amount of RAM proposed on each node.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79710</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79710</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n835" target="n838">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Parallel Algorithm Development on an upgraded Beowulf Cluster</data>
      <data key="e_abstract">EIA-0079710&lt;br/&gt;Fulton, Charles T.&lt;br/&gt;Florida Institute of Technology&lt;br/&gt;&lt;br/&gt;MRI: Parallel Algorithm Development on an Upgraded Beowulf Cluster&lt;br/&gt;&lt;br/&gt;It is proposed to update an existing 5 processor BEOWULF system to 32-workstations, each having two processors, for a total of 64 processors. The upgrade is motivated by the needs of the radiative heat transfer group at Florida Tech for a more powerful computing capability, and the development of parallel numerical algorithms for various large scale numerical linear algebra problems which have been under investigation by the applied mathematics and computer science groups for more than 10 years. Both research activities require the parallel capability and the large amount of RAM proposed on each node.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79710</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79710</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n836" target="n837">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Parallel Algorithm Development on an upgraded Beowulf Cluster</data>
      <data key="e_abstract">EIA-0079710&lt;br/&gt;Fulton, Charles T.&lt;br/&gt;Florida Institute of Technology&lt;br/&gt;&lt;br/&gt;MRI: Parallel Algorithm Development on an Upgraded Beowulf Cluster&lt;br/&gt;&lt;br/&gt;It is proposed to update an existing 5 processor BEOWULF system to 32-workstations, each having two processors, for a total of 64 processors. The upgrade is motivated by the needs of the radiative heat transfer group at Florida Tech for a more powerful computing capability, and the development of parallel numerical algorithms for various large scale numerical linear algebra problems which have been under investigation by the applied mathematics and computer science groups for more than 10 years. Both research activities require the parallel capability and the large amount of RAM proposed on each node.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79710</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79710</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n836" target="n838">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Parallel Algorithm Development on an upgraded Beowulf Cluster</data>
      <data key="e_abstract">EIA-0079710&lt;br/&gt;Fulton, Charles T.&lt;br/&gt;Florida Institute of Technology&lt;br/&gt;&lt;br/&gt;MRI: Parallel Algorithm Development on an Upgraded Beowulf Cluster&lt;br/&gt;&lt;br/&gt;It is proposed to update an existing 5 processor BEOWULF system to 32-workstations, each having two processors, for a total of 64 processors. The upgrade is motivated by the needs of the radiative heat transfer group at Florida Tech for a more powerful computing capability, and the development of parallel numerical algorithms for various large scale numerical linear algebra problems which have been under investigation by the applied mathematics and computer science groups for more than 10 years. Both research activities require the parallel capability and the large amount of RAM proposed on each node.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79710</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79710</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n837" target="n838">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Parallel Algorithm Development on an upgraded Beowulf Cluster</data>
      <data key="e_abstract">EIA-0079710&lt;br/&gt;Fulton, Charles T.&lt;br/&gt;Florida Institute of Technology&lt;br/&gt;&lt;br/&gt;MRI: Parallel Algorithm Development on an Upgraded Beowulf Cluster&lt;br/&gt;&lt;br/&gt;It is proposed to update an existing 5 processor BEOWULF system to 32-workstations, each having two processors, for a total of 64 processors. The upgrade is motivated by the needs of the radiative heat transfer group at Florida Tech for a more powerful computing capability, and the development of parallel numerical algorithms for various large scale numerical linear algebra problems which have been under investigation by the applied mathematics and computer science groups for more than 10 years. Both research activities require the parallel capability and the large amount of RAM proposed on each node.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79710</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79710</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n375" target="n839">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Interacting with the Visual World: Capturing, Understanding, and Predicting Appearance</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This research program is geared towards making significant advances to the science and engineering of visual information processing, and addresses fundamental problems in the fields of computational vision, computer graphics, and human-machine interactions. Today, images and video clips are ubiquitous on the internet, digital video is changing the way entertainment is produced, distance learning is used in various facets of education, and advanced visual interfaces to machines are around the corner. However, at present there are severe limits to the extent to which a user can benefit from visual information, because virtually all of this information is presented in its raw form, that is, the way it was captured. The goal of this project is to develop the technical tools needed to achieve a variety of complex manipulations of visual data. These tools will enable a user to freely explore, interact with, and create variations of the physical world being presented. For instance, a user may remove and add objects to an image of a scene, vary lighting conditions, change the materials of surfaces, or view the scene from a novel perspective.&lt;br/&gt;&lt;br/&gt;This project encompasses a comprehensive research program for creating the science and technology base required to enable such advanced manipulations of visual data. The general research problem may be stated as follows: Capturing, understanding, and predicting the appearance of our everyday world. Success in this domain of research necessitates a unified approach to open problems in two fields: computational vision and computer graphics. The research effort will focus on five pertinent areas: sensing, modeling, estimation, generation, and evaluation. The tangible contributions will be in the form of sensors that provide new types of visual information; complex models of materials, reflectances and textures; estimation algorithms that use the team&apos;s new models to recover scene properties from minimal data; advanced rendering techniques; and a set of comprehensive image/video databases for evaluation of work in this field. The results will impact numerous application domains, including digital imaging, entertainment, virtual environments, distance learning, e-commerce, interactive product design, art restoration, architectural modeling, restorative surgery, and surface inspection.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85864</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85864</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n251" target="n839">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Interacting with the Visual World: Capturing, Understanding, and Predicting Appearance</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This research program is geared towards making significant advances to the science and engineering of visual information processing, and addresses fundamental problems in the fields of computational vision, computer graphics, and human-machine interactions. Today, images and video clips are ubiquitous on the internet, digital video is changing the way entertainment is produced, distance learning is used in various facets of education, and advanced visual interfaces to machines are around the corner. However, at present there are severe limits to the extent to which a user can benefit from visual information, because virtually all of this information is presented in its raw form, that is, the way it was captured. The goal of this project is to develop the technical tools needed to achieve a variety of complex manipulations of visual data. These tools will enable a user to freely explore, interact with, and create variations of the physical world being presented. For instance, a user may remove and add objects to an image of a scene, vary lighting conditions, change the materials of surfaces, or view the scene from a novel perspective.&lt;br/&gt;&lt;br/&gt;This project encompasses a comprehensive research program for creating the science and technology base required to enable such advanced manipulations of visual data. The general research problem may be stated as follows: Capturing, understanding, and predicting the appearance of our everyday world. Success in this domain of research necessitates a unified approach to open problems in two fields: computational vision and computer graphics. The research effort will focus on five pertinent areas: sensing, modeling, estimation, generation, and evaluation. The tangible contributions will be in the form of sensors that provide new types of visual information; complex models of materials, reflectances and textures; estimation algorithms that use the team&apos;s new models to recover scene properties from minimal data; advanced rendering techniques; and a set of comprehensive image/video databases for evaluation of work in this field. The results will impact numerous application domains, including digital imaging, entertainment, virtual environments, distance learning, e-commerce, interactive product design, art restoration, architectural modeling, restorative surgery, and surface inspection.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85864</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85864</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n839" target="n842">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Interacting with the Visual World: Capturing, Understanding, and Predicting Appearance</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This research program is geared towards making significant advances to the science and engineering of visual information processing, and addresses fundamental problems in the fields of computational vision, computer graphics, and human-machine interactions. Today, images and video clips are ubiquitous on the internet, digital video is changing the way entertainment is produced, distance learning is used in various facets of education, and advanced visual interfaces to machines are around the corner. However, at present there are severe limits to the extent to which a user can benefit from visual information, because virtually all of this information is presented in its raw form, that is, the way it was captured. The goal of this project is to develop the technical tools needed to achieve a variety of complex manipulations of visual data. These tools will enable a user to freely explore, interact with, and create variations of the physical world being presented. For instance, a user may remove and add objects to an image of a scene, vary lighting conditions, change the materials of surfaces, or view the scene from a novel perspective.&lt;br/&gt;&lt;br/&gt;This project encompasses a comprehensive research program for creating the science and technology base required to enable such advanced manipulations of visual data. The general research problem may be stated as follows: Capturing, understanding, and predicting the appearance of our everyday world. Success in this domain of research necessitates a unified approach to open problems in two fields: computational vision and computer graphics. The research effort will focus on five pertinent areas: sensing, modeling, estimation, generation, and evaluation. The tangible contributions will be in the form of sensors that provide new types of visual information; complex models of materials, reflectances and textures; estimation algorithms that use the team&apos;s new models to recover scene properties from minimal data; advanced rendering techniques; and a set of comprehensive image/video databases for evaluation of work in this field. The results will impact numerous application domains, including digital imaging, entertainment, virtual environments, distance learning, e-commerce, interactive product design, art restoration, architectural modeling, restorative surgery, and surface inspection.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85864</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85864</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n839" target="n843">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Interacting with the Visual World: Capturing, Understanding, and Predicting Appearance</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This research program is geared towards making significant advances to the science and engineering of visual information processing, and addresses fundamental problems in the fields of computational vision, computer graphics, and human-machine interactions. Today, images and video clips are ubiquitous on the internet, digital video is changing the way entertainment is produced, distance learning is used in various facets of education, and advanced visual interfaces to machines are around the corner. However, at present there are severe limits to the extent to which a user can benefit from visual information, because virtually all of this information is presented in its raw form, that is, the way it was captured. The goal of this project is to develop the technical tools needed to achieve a variety of complex manipulations of visual data. These tools will enable a user to freely explore, interact with, and create variations of the physical world being presented. For instance, a user may remove and add objects to an image of a scene, vary lighting conditions, change the materials of surfaces, or view the scene from a novel perspective.&lt;br/&gt;&lt;br/&gt;This project encompasses a comprehensive research program for creating the science and technology base required to enable such advanced manipulations of visual data. The general research problem may be stated as follows: Capturing, understanding, and predicting the appearance of our everyday world. Success in this domain of research necessitates a unified approach to open problems in two fields: computational vision and computer graphics. The research effort will focus on five pertinent areas: sensing, modeling, estimation, generation, and evaluation. The tangible contributions will be in the form of sensors that provide new types of visual information; complex models of materials, reflectances and textures; estimation algorithms that use the team&apos;s new models to recover scene properties from minimal data; advanced rendering techniques; and a set of comprehensive image/video databases for evaluation of work in this field. The results will impact numerous application domains, including digital imaging, entertainment, virtual environments, distance learning, e-commerce, interactive product design, art restoration, architectural modeling, restorative surgery, and surface inspection.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85864</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85864</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n251" target="n375">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Interacting with the Visual World: Capturing, Understanding, and Predicting Appearance</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This research program is geared towards making significant advances to the science and engineering of visual information processing, and addresses fundamental problems in the fields of computational vision, computer graphics, and human-machine interactions. Today, images and video clips are ubiquitous on the internet, digital video is changing the way entertainment is produced, distance learning is used in various facets of education, and advanced visual interfaces to machines are around the corner. However, at present there are severe limits to the extent to which a user can benefit from visual information, because virtually all of this information is presented in its raw form, that is, the way it was captured. The goal of this project is to develop the technical tools needed to achieve a variety of complex manipulations of visual data. These tools will enable a user to freely explore, interact with, and create variations of the physical world being presented. For instance, a user may remove and add objects to an image of a scene, vary lighting conditions, change the materials of surfaces, or view the scene from a novel perspective.&lt;br/&gt;&lt;br/&gt;This project encompasses a comprehensive research program for creating the science and technology base required to enable such advanced manipulations of visual data. The general research problem may be stated as follows: Capturing, understanding, and predicting the appearance of our everyday world. Success in this domain of research necessitates a unified approach to open problems in two fields: computational vision and computer graphics. The research effort will focus on five pertinent areas: sensing, modeling, estimation, generation, and evaluation. The tangible contributions will be in the form of sensors that provide new types of visual information; complex models of materials, reflectances and textures; estimation algorithms that use the team&apos;s new models to recover scene properties from minimal data; advanced rendering techniques; and a set of comprehensive image/video databases for evaluation of work in this field. The results will impact numerous application domains, including digital imaging, entertainment, virtual environments, distance learning, e-commerce, interactive product design, art restoration, architectural modeling, restorative surgery, and surface inspection.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85864</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85864</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n375" target="n842">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Interacting with the Visual World: Capturing, Understanding, and Predicting Appearance</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This research program is geared towards making significant advances to the science and engineering of visual information processing, and addresses fundamental problems in the fields of computational vision, computer graphics, and human-machine interactions. Today, images and video clips are ubiquitous on the internet, digital video is changing the way entertainment is produced, distance learning is used in various facets of education, and advanced visual interfaces to machines are around the corner. However, at present there are severe limits to the extent to which a user can benefit from visual information, because virtually all of this information is presented in its raw form, that is, the way it was captured. The goal of this project is to develop the technical tools needed to achieve a variety of complex manipulations of visual data. These tools will enable a user to freely explore, interact with, and create variations of the physical world being presented. For instance, a user may remove and add objects to an image of a scene, vary lighting conditions, change the materials of surfaces, or view the scene from a novel perspective.&lt;br/&gt;&lt;br/&gt;This project encompasses a comprehensive research program for creating the science and technology base required to enable such advanced manipulations of visual data. The general research problem may be stated as follows: Capturing, understanding, and predicting the appearance of our everyday world. Success in this domain of research necessitates a unified approach to open problems in two fields: computational vision and computer graphics. The research effort will focus on five pertinent areas: sensing, modeling, estimation, generation, and evaluation. The tangible contributions will be in the form of sensors that provide new types of visual information; complex models of materials, reflectances and textures; estimation algorithms that use the team&apos;s new models to recover scene properties from minimal data; advanced rendering techniques; and a set of comprehensive image/video databases for evaluation of work in this field. The results will impact numerous application domains, including digital imaging, entertainment, virtual environments, distance learning, e-commerce, interactive product design, art restoration, architectural modeling, restorative surgery, and surface inspection.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85864</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85864</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n375" target="n843">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Interacting with the Visual World: Capturing, Understanding, and Predicting Appearance</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This research program is geared towards making significant advances to the science and engineering of visual information processing, and addresses fundamental problems in the fields of computational vision, computer graphics, and human-machine interactions. Today, images and video clips are ubiquitous on the internet, digital video is changing the way entertainment is produced, distance learning is used in various facets of education, and advanced visual interfaces to machines are around the corner. However, at present there are severe limits to the extent to which a user can benefit from visual information, because virtually all of this information is presented in its raw form, that is, the way it was captured. The goal of this project is to develop the technical tools needed to achieve a variety of complex manipulations of visual data. These tools will enable a user to freely explore, interact with, and create variations of the physical world being presented. For instance, a user may remove and add objects to an image of a scene, vary lighting conditions, change the materials of surfaces, or view the scene from a novel perspective.&lt;br/&gt;&lt;br/&gt;This project encompasses a comprehensive research program for creating the science and technology base required to enable such advanced manipulations of visual data. The general research problem may be stated as follows: Capturing, understanding, and predicting the appearance of our everyday world. Success in this domain of research necessitates a unified approach to open problems in two fields: computational vision and computer graphics. The research effort will focus on five pertinent areas: sensing, modeling, estimation, generation, and evaluation. The tangible contributions will be in the form of sensors that provide new types of visual information; complex models of materials, reflectances and textures; estimation algorithms that use the team&apos;s new models to recover scene properties from minimal data; advanced rendering techniques; and a set of comprehensive image/video databases for evaluation of work in this field. The results will impact numerous application domains, including digital imaging, entertainment, virtual environments, distance learning, e-commerce, interactive product design, art restoration, architectural modeling, restorative surgery, and surface inspection.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85864</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85864</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n251" target="n842">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Interacting with the Visual World: Capturing, Understanding, and Predicting Appearance</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This research program is geared towards making significant advances to the science and engineering of visual information processing, and addresses fundamental problems in the fields of computational vision, computer graphics, and human-machine interactions. Today, images and video clips are ubiquitous on the internet, digital video is changing the way entertainment is produced, distance learning is used in various facets of education, and advanced visual interfaces to machines are around the corner. However, at present there are severe limits to the extent to which a user can benefit from visual information, because virtually all of this information is presented in its raw form, that is, the way it was captured. The goal of this project is to develop the technical tools needed to achieve a variety of complex manipulations of visual data. These tools will enable a user to freely explore, interact with, and create variations of the physical world being presented. For instance, a user may remove and add objects to an image of a scene, vary lighting conditions, change the materials of surfaces, or view the scene from a novel perspective.&lt;br/&gt;&lt;br/&gt;This project encompasses a comprehensive research program for creating the science and technology base required to enable such advanced manipulations of visual data. The general research problem may be stated as follows: Capturing, understanding, and predicting the appearance of our everyday world. Success in this domain of research necessitates a unified approach to open problems in two fields: computational vision and computer graphics. The research effort will focus on five pertinent areas: sensing, modeling, estimation, generation, and evaluation. The tangible contributions will be in the form of sensors that provide new types of visual information; complex models of materials, reflectances and textures; estimation algorithms that use the team&apos;s new models to recover scene properties from minimal data; advanced rendering techniques; and a set of comprehensive image/video databases for evaluation of work in this field. The results will impact numerous application domains, including digital imaging, entertainment, virtual environments, distance learning, e-commerce, interactive product design, art restoration, architectural modeling, restorative surgery, and surface inspection.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85864</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85864</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n251" target="n843">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Interacting with the Visual World: Capturing, Understanding, and Predicting Appearance</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This research program is geared towards making significant advances to the science and engineering of visual information processing, and addresses fundamental problems in the fields of computational vision, computer graphics, and human-machine interactions. Today, images and video clips are ubiquitous on the internet, digital video is changing the way entertainment is produced, distance learning is used in various facets of education, and advanced visual interfaces to machines are around the corner. However, at present there are severe limits to the extent to which a user can benefit from visual information, because virtually all of this information is presented in its raw form, that is, the way it was captured. The goal of this project is to develop the technical tools needed to achieve a variety of complex manipulations of visual data. These tools will enable a user to freely explore, interact with, and create variations of the physical world being presented. For instance, a user may remove and add objects to an image of a scene, vary lighting conditions, change the materials of surfaces, or view the scene from a novel perspective.&lt;br/&gt;&lt;br/&gt;This project encompasses a comprehensive research program for creating the science and technology base required to enable such advanced manipulations of visual data. The general research problem may be stated as follows: Capturing, understanding, and predicting the appearance of our everyday world. Success in this domain of research necessitates a unified approach to open problems in two fields: computational vision and computer graphics. The research effort will focus on five pertinent areas: sensing, modeling, estimation, generation, and evaluation. The tangible contributions will be in the form of sensors that provide new types of visual information; complex models of materials, reflectances and textures; estimation algorithms that use the team&apos;s new models to recover scene properties from minimal data; advanced rendering techniques; and a set of comprehensive image/video databases for evaluation of work in this field. The results will impact numerous application domains, including digital imaging, entertainment, virtual environments, distance learning, e-commerce, interactive product design, art restoration, architectural modeling, restorative surgery, and surface inspection.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85864</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85864</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n842" target="n843">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Interacting with the Visual World: Capturing, Understanding, and Predicting Appearance</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This research program is geared towards making significant advances to the science and engineering of visual information processing, and addresses fundamental problems in the fields of computational vision, computer graphics, and human-machine interactions. Today, images and video clips are ubiquitous on the internet, digital video is changing the way entertainment is produced, distance learning is used in various facets of education, and advanced visual interfaces to machines are around the corner. However, at present there are severe limits to the extent to which a user can benefit from visual information, because virtually all of this information is presented in its raw form, that is, the way it was captured. The goal of this project is to develop the technical tools needed to achieve a variety of complex manipulations of visual data. These tools will enable a user to freely explore, interact with, and create variations of the physical world being presented. For instance, a user may remove and add objects to an image of a scene, vary lighting conditions, change the materials of surfaces, or view the scene from a novel perspective.&lt;br/&gt;&lt;br/&gt;This project encompasses a comprehensive research program for creating the science and technology base required to enable such advanced manipulations of visual data. The general research problem may be stated as follows: Capturing, understanding, and predicting the appearance of our everyday world. Success in this domain of research necessitates a unified approach to open problems in two fields: computational vision and computer graphics. The research effort will focus on five pertinent areas: sensing, modeling, estimation, generation, and evaluation. The tangible contributions will be in the form of sensors that provide new types of visual information; complex models of materials, reflectances and textures; estimation algorithms that use the team&apos;s new models to recover scene properties from minimal data; advanced rendering techniques; and a set of comprehensive image/video databases for evaluation of work in this field. The results will impact numerous application domains, including digital imaging, entertainment, virtual environments, distance learning, e-commerce, interactive product design, art restoration, architectural modeling, restorative surgery, and surface inspection.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85864</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85864</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n845" target="n846">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques</data>
      <data key="e_abstract">EIA-0080123&lt;br/&gt;Andrews, Gregory R.&lt;br/&gt;University of Arizona&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques&lt;br/&gt;&lt;br/&gt;This project is exploring a variety of complementary techniques for optimizing distributed and networked systems ---from client interfaces, through middleware and servers, to the communication infrastructure---and they examine a variety of optimization criteria---including time, space, power, quality of service, and utility.&lt;br/&gt;&lt;br/&gt;Client-level projects are investigating ways to reduce power consumption and memory requirements, and ways to protect clients and mobile code from each other. The middleware projects are examining ways to support interactive visualization of geographical databases and ways to optimize evaluation of temporal queries. The server projects focus on optimizing the computational and I/O behavior of large, typically parallel server systems connected to many disks; applications include database servers&lt;br/&gt;and parallel scientific programs. The final set of projects examines ways to optimize network performance; topics include routing and forwarding in high-speed networks, more efficient protocols for wireless networks and mobile computations, and a new approach called network-resident storage.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80123</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80123</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n845" target="n847">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques</data>
      <data key="e_abstract">EIA-0080123&lt;br/&gt;Andrews, Gregory R.&lt;br/&gt;University of Arizona&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques&lt;br/&gt;&lt;br/&gt;This project is exploring a variety of complementary techniques for optimizing distributed and networked systems ---from client interfaces, through middleware and servers, to the communication infrastructure---and they examine a variety of optimization criteria---including time, space, power, quality of service, and utility.&lt;br/&gt;&lt;br/&gt;Client-level projects are investigating ways to reduce power consumption and memory requirements, and ways to protect clients and mobile code from each other. The middleware projects are examining ways to support interactive visualization of geographical databases and ways to optimize evaluation of temporal queries. The server projects focus on optimizing the computational and I/O behavior of large, typically parallel server systems connected to many disks; applications include database servers&lt;br/&gt;and parallel scientific programs. The final set of projects examines ways to optimize network performance; topics include routing and forwarding in high-speed networks, more efficient protocols for wireless networks and mobile computations, and a new approach called network-resident storage.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80123</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80123</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n845" target="n848">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques</data>
      <data key="e_abstract">EIA-0080123&lt;br/&gt;Andrews, Gregory R.&lt;br/&gt;University of Arizona&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques&lt;br/&gt;&lt;br/&gt;This project is exploring a variety of complementary techniques for optimizing distributed and networked systems ---from client interfaces, through middleware and servers, to the communication infrastructure---and they examine a variety of optimization criteria---including time, space, power, quality of service, and utility.&lt;br/&gt;&lt;br/&gt;Client-level projects are investigating ways to reduce power consumption and memory requirements, and ways to protect clients and mobile code from each other. The middleware projects are examining ways to support interactive visualization of geographical databases and ways to optimize evaluation of temporal queries. The server projects focus on optimizing the computational and I/O behavior of large, typically parallel server systems connected to many disks; applications include database servers&lt;br/&gt;and parallel scientific programs. The final set of projects examines ways to optimize network performance; topics include routing and forwarding in high-speed networks, more efficient protocols for wireless networks and mobile computations, and a new approach called network-resident storage.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80123</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80123</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n845" target="n849">
      <data key="e_effectiveDate">None</data>
      <data key="e_title">None</data>
      <data key="e_abstract">None</data>
      <data key="e_pgm">None</data>
      <data key="e_expirationDate">None</data>
      <data key="e_div">None</data>
      <data key="e_dir">None</data>
    </edge>
    <edge source="n845" target="n849">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques</data>
      <data key="e_abstract">EIA-0080123&lt;br/&gt;Andrews, Gregory R.&lt;br/&gt;University of Arizona&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques&lt;br/&gt;&lt;br/&gt;This project is exploring a variety of complementary techniques for optimizing distributed and networked systems ---from client interfaces, through middleware and servers, to the communication infrastructure---and they examine a variety of optimization criteria---including time, space, power, quality of service, and utility.&lt;br/&gt;&lt;br/&gt;Client-level projects are investigating ways to reduce power consumption and memory requirements, and ways to protect clients and mobile code from each other. The middleware projects are examining ways to support interactive visualization of geographical databases and ways to optimize evaluation of temporal queries. The server projects focus on optimizing the computational and I/O behavior of large, typically parallel server systems connected to many disks; applications include database servers&lt;br/&gt;and parallel scientific programs. The final set of projects examines ways to optimize network performance; topics include routing and forwarding in high-speed networks, more efficient protocols for wireless networks and mobile computations, and a new approach called network-resident storage.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80123</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80123</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n846" target="n847">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques</data>
      <data key="e_abstract">EIA-0080123&lt;br/&gt;Andrews, Gregory R.&lt;br/&gt;University of Arizona&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques&lt;br/&gt;&lt;br/&gt;This project is exploring a variety of complementary techniques for optimizing distributed and networked systems ---from client interfaces, through middleware and servers, to the communication infrastructure---and they examine a variety of optimization criteria---including time, space, power, quality of service, and utility.&lt;br/&gt;&lt;br/&gt;Client-level projects are investigating ways to reduce power consumption and memory requirements, and ways to protect clients and mobile code from each other. The middleware projects are examining ways to support interactive visualization of geographical databases and ways to optimize evaluation of temporal queries. The server projects focus on optimizing the computational and I/O behavior of large, typically parallel server systems connected to many disks; applications include database servers&lt;br/&gt;and parallel scientific programs. The final set of projects examines ways to optimize network performance; topics include routing and forwarding in high-speed networks, more efficient protocols for wireless networks and mobile computations, and a new approach called network-resident storage.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80123</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80123</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n846" target="n848">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques</data>
      <data key="e_abstract">EIA-0080123&lt;br/&gt;Andrews, Gregory R.&lt;br/&gt;University of Arizona&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques&lt;br/&gt;&lt;br/&gt;This project is exploring a variety of complementary techniques for optimizing distributed and networked systems ---from client interfaces, through middleware and servers, to the communication infrastructure---and they examine a variety of optimization criteria---including time, space, power, quality of service, and utility.&lt;br/&gt;&lt;br/&gt;Client-level projects are investigating ways to reduce power consumption and memory requirements, and ways to protect clients and mobile code from each other. The middleware projects are examining ways to support interactive visualization of geographical databases and ways to optimize evaluation of temporal queries. The server projects focus on optimizing the computational and I/O behavior of large, typically parallel server systems connected to many disks; applications include database servers&lt;br/&gt;and parallel scientific programs. The final set of projects examines ways to optimize network performance; topics include routing and forwarding in high-speed networks, more efficient protocols for wireless networks and mobile computations, and a new approach called network-resident storage.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80123</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80123</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n846" target="n849">
      <data key="e_effectiveDate">None</data>
      <data key="e_title">None</data>
      <data key="e_abstract">None</data>
      <data key="e_pgm">None</data>
      <data key="e_expirationDate">None</data>
      <data key="e_div">None</data>
      <data key="e_dir">None</data>
    </edge>
    <edge source="n846" target="n849">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques</data>
      <data key="e_abstract">EIA-0080123&lt;br/&gt;Andrews, Gregory R.&lt;br/&gt;University of Arizona&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques&lt;br/&gt;&lt;br/&gt;This project is exploring a variety of complementary techniques for optimizing distributed and networked systems ---from client interfaces, through middleware and servers, to the communication infrastructure---and they examine a variety of optimization criteria---including time, space, power, quality of service, and utility.&lt;br/&gt;&lt;br/&gt;Client-level projects are investigating ways to reduce power consumption and memory requirements, and ways to protect clients and mobile code from each other. The middleware projects are examining ways to support interactive visualization of geographical databases and ways to optimize evaluation of temporal queries. The server projects focus on optimizing the computational and I/O behavior of large, typically parallel server systems connected to many disks; applications include database servers&lt;br/&gt;and parallel scientific programs. The final set of projects examines ways to optimize network performance; topics include routing and forwarding in high-speed networks, more efficient protocols for wireless networks and mobile computations, and a new approach called network-resident storage.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80123</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80123</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n847" target="n848">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques</data>
      <data key="e_abstract">EIA-0080123&lt;br/&gt;Andrews, Gregory R.&lt;br/&gt;University of Arizona&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques&lt;br/&gt;&lt;br/&gt;This project is exploring a variety of complementary techniques for optimizing distributed and networked systems ---from client interfaces, through middleware and servers, to the communication infrastructure---and they examine a variety of optimization criteria---including time, space, power, quality of service, and utility.&lt;br/&gt;&lt;br/&gt;Client-level projects are investigating ways to reduce power consumption and memory requirements, and ways to protect clients and mobile code from each other. The middleware projects are examining ways to support interactive visualization of geographical databases and ways to optimize evaluation of temporal queries. The server projects focus on optimizing the computational and I/O behavior of large, typically parallel server systems connected to many disks; applications include database servers&lt;br/&gt;and parallel scientific programs. The final set of projects examines ways to optimize network performance; topics include routing and forwarding in high-speed networks, more efficient protocols for wireless networks and mobile computations, and a new approach called network-resident storage.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80123</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80123</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n847" target="n849">
      <data key="e_effectiveDate">None</data>
      <data key="e_title">None</data>
      <data key="e_abstract">None</data>
      <data key="e_pgm">None</data>
      <data key="e_expirationDate">None</data>
      <data key="e_div">None</data>
      <data key="e_dir">None</data>
    </edge>
    <edge source="n847" target="n849">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques</data>
      <data key="e_abstract">EIA-0080123&lt;br/&gt;Andrews, Gregory R.&lt;br/&gt;University of Arizona&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques&lt;br/&gt;&lt;br/&gt;This project is exploring a variety of complementary techniques for optimizing distributed and networked systems ---from client interfaces, through middleware and servers, to the communication infrastructure---and they examine a variety of optimization criteria---including time, space, power, quality of service, and utility.&lt;br/&gt;&lt;br/&gt;Client-level projects are investigating ways to reduce power consumption and memory requirements, and ways to protect clients and mobile code from each other. The middleware projects are examining ways to support interactive visualization of geographical databases and ways to optimize evaluation of temporal queries. The server projects focus on optimizing the computational and I/O behavior of large, typically parallel server systems connected to many disks; applications include database servers&lt;br/&gt;and parallel scientific programs. The final set of projects examines ways to optimize network performance; topics include routing and forwarding in high-speed networks, more efficient protocols for wireless networks and mobile computations, and a new approach called network-resident storage.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80123</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80123</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n848" target="n849">
      <data key="e_effectiveDate">None</data>
      <data key="e_title">None</data>
      <data key="e_abstract">None</data>
      <data key="e_pgm">None</data>
      <data key="e_expirationDate">None</data>
      <data key="e_div">None</data>
      <data key="e_dir">None</data>
    </edge>
    <edge source="n848" target="n849">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques</data>
      <data key="e_abstract">EIA-0080123&lt;br/&gt;Andrews, Gregory R.&lt;br/&gt;University of Arizona&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques&lt;br/&gt;&lt;br/&gt;This project is exploring a variety of complementary techniques for optimizing distributed and networked systems ---from client interfaces, through middleware and servers, to the communication infrastructure---and they examine a variety of optimization criteria---including time, space, power, quality of service, and utility.&lt;br/&gt;&lt;br/&gt;Client-level projects are investigating ways to reduce power consumption and memory requirements, and ways to protect clients and mobile code from each other. The middleware projects are examining ways to support interactive visualization of geographical databases and ways to optimize evaluation of temporal queries. The server projects focus on optimizing the computational and I/O behavior of large, typically parallel server systems connected to many disks; applications include database servers&lt;br/&gt;and parallel scientific programs. The final set of projects examines ways to optimize network performance; topics include routing and forwarding in high-speed networks, more efficient protocols for wireless networks and mobile computations, and a new approach called network-resident storage.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80123</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80123</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n849" target="n849">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques</data>
      <data key="e_abstract">EIA-0080123&lt;br/&gt;Andrews, Gregory R.&lt;br/&gt;University of Arizona&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Optimization of Distributed and Networked Systems: A Spectrum of Techniques&lt;br/&gt;&lt;br/&gt;This project is exploring a variety of complementary techniques for optimizing distributed and networked systems ---from client interfaces, through middleware and servers, to the communication infrastructure---and they examine a variety of optimization criteria---including time, space, power, quality of service, and utility.&lt;br/&gt;&lt;br/&gt;Client-level projects are investigating ways to reduce power consumption and memory requirements, and ways to protect clients and mobile code from each other. The middleware projects are examining ways to support interactive visualization of geographical databases and ways to optimize evaluation of temporal queries. The server projects focus on optimizing the computational and I/O behavior of large, typically parallel server systems connected to many disks; applications include database servers&lt;br/&gt;and parallel scientific programs. The final set of projects examines ways to optimize network performance; topics include routing and forwarding in high-speed networks, more efficient protocols for wireless networks and mobile computations, and a new approach called network-resident storage.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80123</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80123</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n411" target="n851">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Spatial Intelligence for Computer-Enhanced Interaction with Physical Environments</data>
      <data key="e_abstract">EIA-0080124&lt;br/&gt;Nelson, Randal C.&lt;br/&gt;University of Rochester&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Spatial Intelligence for Computer-Enhanced Interaction with Physical Environments&lt;br/&gt;&lt;br/&gt;Intelligent computer systems have considerable potential to augment human abilities, not only in accessing abstracted information, but also in dealing with physical environments (both real and virtual). A canonical example of such a system, though certainly not the only one, is a robot with which one can converse. To mediate between people and a physical environment, an intelligent system must perceive spatial structure of various sorts and competently execute physical actions. At the same time, it must communicate with human users to provide information, accept instruction, or assist interactively with complex tasks. &lt;br/&gt;&lt;br/&gt;The term &quot;spatial intelligence&quot; can be used to capture the overarching ability to perceive, act in, and communicate about a physical environment. Implementing spatial intelligence depends on integrating a variety of enabling technologies in AI, distributed systems, and human interfaces. Some of the most critical of these technologies, particularly in machine perception and natural language communication are currently crossing a threshold that promises to make useful, end-to-end, spatially intelligent systems viable for the first time.&lt;br/&gt;&lt;br/&gt;The overall goal of the project is to enable creation of flexible spatial intelligence with which human users can interact naturally to carry out a variety of collaborative tasks. The project will create and equip a laboratory resource specifically designed to advance the state of the art in the various enabling technologies, and facilitate and demonstrate their integration into end-to-end systems.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80124</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80124</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n851" target="n853">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Spatial Intelligence for Computer-Enhanced Interaction with Physical Environments</data>
      <data key="e_abstract">EIA-0080124&lt;br/&gt;Nelson, Randal C.&lt;br/&gt;University of Rochester&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Spatial Intelligence for Computer-Enhanced Interaction with Physical Environments&lt;br/&gt;&lt;br/&gt;Intelligent computer systems have considerable potential to augment human abilities, not only in accessing abstracted information, but also in dealing with physical environments (both real and virtual). A canonical example of such a system, though certainly not the only one, is a robot with which one can converse. To mediate between people and a physical environment, an intelligent system must perceive spatial structure of various sorts and competently execute physical actions. At the same time, it must communicate with human users to provide information, accept instruction, or assist interactively with complex tasks. &lt;br/&gt;&lt;br/&gt;The term &quot;spatial intelligence&quot; can be used to capture the overarching ability to perceive, act in, and communicate about a physical environment. Implementing spatial intelligence depends on integrating a variety of enabling technologies in AI, distributed systems, and human interfaces. Some of the most critical of these technologies, particularly in machine perception and natural language communication are currently crossing a threshold that promises to make useful, end-to-end, spatially intelligent systems viable for the first time.&lt;br/&gt;&lt;br/&gt;The overall goal of the project is to enable creation of flexible spatial intelligence with which human users can interact naturally to carry out a variety of collaborative tasks. The project will create and equip a laboratory resource specifically designed to advance the state of the art in the various enabling technologies, and facilitate and demonstrate their integration into end-to-end systems.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80124</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80124</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n851" target="n854">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Spatial Intelligence for Computer-Enhanced Interaction with Physical Environments</data>
      <data key="e_abstract">EIA-0080124&lt;br/&gt;Nelson, Randal C.&lt;br/&gt;University of Rochester&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Spatial Intelligence for Computer-Enhanced Interaction with Physical Environments&lt;br/&gt;&lt;br/&gt;Intelligent computer systems have considerable potential to augment human abilities, not only in accessing abstracted information, but also in dealing with physical environments (both real and virtual). A canonical example of such a system, though certainly not the only one, is a robot with which one can converse. To mediate between people and a physical environment, an intelligent system must perceive spatial structure of various sorts and competently execute physical actions. At the same time, it must communicate with human users to provide information, accept instruction, or assist interactively with complex tasks. &lt;br/&gt;&lt;br/&gt;The term &quot;spatial intelligence&quot; can be used to capture the overarching ability to perceive, act in, and communicate about a physical environment. Implementing spatial intelligence depends on integrating a variety of enabling technologies in AI, distributed systems, and human interfaces. Some of the most critical of these technologies, particularly in machine perception and natural language communication are currently crossing a threshold that promises to make useful, end-to-end, spatially intelligent systems viable for the first time.&lt;br/&gt;&lt;br/&gt;The overall goal of the project is to enable creation of flexible spatial intelligence with which human users can interact naturally to carry out a variety of collaborative tasks. The project will create and equip a laboratory resource specifically designed to advance the state of the art in the various enabling technologies, and facilitate and demonstrate their integration into end-to-end systems.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80124</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80124</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n851" target="n855">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Spatial Intelligence for Computer-Enhanced Interaction with Physical Environments</data>
      <data key="e_abstract">EIA-0080124&lt;br/&gt;Nelson, Randal C.&lt;br/&gt;University of Rochester&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Spatial Intelligence for Computer-Enhanced Interaction with Physical Environments&lt;br/&gt;&lt;br/&gt;Intelligent computer systems have considerable potential to augment human abilities, not only in accessing abstracted information, but also in dealing with physical environments (both real and virtual). A canonical example of such a system, though certainly not the only one, is a robot with which one can converse. To mediate between people and a physical environment, an intelligent system must perceive spatial structure of various sorts and competently execute physical actions. At the same time, it must communicate with human users to provide information, accept instruction, or assist interactively with complex tasks. &lt;br/&gt;&lt;br/&gt;The term &quot;spatial intelligence&quot; can be used to capture the overarching ability to perceive, act in, and communicate about a physical environment. Implementing spatial intelligence depends on integrating a variety of enabling technologies in AI, distributed systems, and human interfaces. Some of the most critical of these technologies, particularly in machine perception and natural language communication are currently crossing a threshold that promises to make useful, end-to-end, spatially intelligent systems viable for the first time.&lt;br/&gt;&lt;br/&gt;The overall goal of the project is to enable creation of flexible spatial intelligence with which human users can interact naturally to carry out a variety of collaborative tasks. The project will create and equip a laboratory resource specifically designed to advance the state of the art in the various enabling technologies, and facilitate and demonstrate their integration into end-to-end systems.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80124</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80124</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n411" target="n853">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Spatial Intelligence for Computer-Enhanced Interaction with Physical Environments</data>
      <data key="e_abstract">EIA-0080124&lt;br/&gt;Nelson, Randal C.&lt;br/&gt;University of Rochester&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Spatial Intelligence for Computer-Enhanced Interaction with Physical Environments&lt;br/&gt;&lt;br/&gt;Intelligent computer systems have considerable potential to augment human abilities, not only in accessing abstracted information, but also in dealing with physical environments (both real and virtual). A canonical example of such a system, though certainly not the only one, is a robot with which one can converse. To mediate between people and a physical environment, an intelligent system must perceive spatial structure of various sorts and competently execute physical actions. At the same time, it must communicate with human users to provide information, accept instruction, or assist interactively with complex tasks. &lt;br/&gt;&lt;br/&gt;The term &quot;spatial intelligence&quot; can be used to capture the overarching ability to perceive, act in, and communicate about a physical environment. Implementing spatial intelligence depends on integrating a variety of enabling technologies in AI, distributed systems, and human interfaces. Some of the most critical of these technologies, particularly in machine perception and natural language communication are currently crossing a threshold that promises to make useful, end-to-end, spatially intelligent systems viable for the first time.&lt;br/&gt;&lt;br/&gt;The overall goal of the project is to enable creation of flexible spatial intelligence with which human users can interact naturally to carry out a variety of collaborative tasks. The project will create and equip a laboratory resource specifically designed to advance the state of the art in the various enabling technologies, and facilitate and demonstrate their integration into end-to-end systems.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80124</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80124</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n411" target="n854">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Spatial Intelligence for Computer-Enhanced Interaction with Physical Environments</data>
      <data key="e_abstract">EIA-0080124&lt;br/&gt;Nelson, Randal C.&lt;br/&gt;University of Rochester&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Spatial Intelligence for Computer-Enhanced Interaction with Physical Environments&lt;br/&gt;&lt;br/&gt;Intelligent computer systems have considerable potential to augment human abilities, not only in accessing abstracted information, but also in dealing with physical environments (both real and virtual). A canonical example of such a system, though certainly not the only one, is a robot with which one can converse. To mediate between people and a physical environment, an intelligent system must perceive spatial structure of various sorts and competently execute physical actions. At the same time, it must communicate with human users to provide information, accept instruction, or assist interactively with complex tasks. &lt;br/&gt;&lt;br/&gt;The term &quot;spatial intelligence&quot; can be used to capture the overarching ability to perceive, act in, and communicate about a physical environment. Implementing spatial intelligence depends on integrating a variety of enabling technologies in AI, distributed systems, and human interfaces. Some of the most critical of these technologies, particularly in machine perception and natural language communication are currently crossing a threshold that promises to make useful, end-to-end, spatially intelligent systems viable for the first time.&lt;br/&gt;&lt;br/&gt;The overall goal of the project is to enable creation of flexible spatial intelligence with which human users can interact naturally to carry out a variety of collaborative tasks. The project will create and equip a laboratory resource specifically designed to advance the state of the art in the various enabling technologies, and facilitate and demonstrate their integration into end-to-end systems.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80124</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80124</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n411" target="n855">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Spatial Intelligence for Computer-Enhanced Interaction with Physical Environments</data>
      <data key="e_abstract">EIA-0080124&lt;br/&gt;Nelson, Randal C.&lt;br/&gt;University of Rochester&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Spatial Intelligence for Computer-Enhanced Interaction with Physical Environments&lt;br/&gt;&lt;br/&gt;Intelligent computer systems have considerable potential to augment human abilities, not only in accessing abstracted information, but also in dealing with physical environments (both real and virtual). A canonical example of such a system, though certainly not the only one, is a robot with which one can converse. To mediate between people and a physical environment, an intelligent system must perceive spatial structure of various sorts and competently execute physical actions. At the same time, it must communicate with human users to provide information, accept instruction, or assist interactively with complex tasks. &lt;br/&gt;&lt;br/&gt;The term &quot;spatial intelligence&quot; can be used to capture the overarching ability to perceive, act in, and communicate about a physical environment. Implementing spatial intelligence depends on integrating a variety of enabling technologies in AI, distributed systems, and human interfaces. Some of the most critical of these technologies, particularly in machine perception and natural language communication are currently crossing a threshold that promises to make useful, end-to-end, spatially intelligent systems viable for the first time.&lt;br/&gt;&lt;br/&gt;The overall goal of the project is to enable creation of flexible spatial intelligence with which human users can interact naturally to carry out a variety of collaborative tasks. The project will create and equip a laboratory resource specifically designed to advance the state of the art in the various enabling technologies, and facilitate and demonstrate their integration into end-to-end systems.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80124</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80124</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n853" target="n854">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Spatial Intelligence for Computer-Enhanced Interaction with Physical Environments</data>
      <data key="e_abstract">EIA-0080124&lt;br/&gt;Nelson, Randal C.&lt;br/&gt;University of Rochester&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Spatial Intelligence for Computer-Enhanced Interaction with Physical Environments&lt;br/&gt;&lt;br/&gt;Intelligent computer systems have considerable potential to augment human abilities, not only in accessing abstracted information, but also in dealing with physical environments (both real and virtual). A canonical example of such a system, though certainly not the only one, is a robot with which one can converse. To mediate between people and a physical environment, an intelligent system must perceive spatial structure of various sorts and competently execute physical actions. At the same time, it must communicate with human users to provide information, accept instruction, or assist interactively with complex tasks. &lt;br/&gt;&lt;br/&gt;The term &quot;spatial intelligence&quot; can be used to capture the overarching ability to perceive, act in, and communicate about a physical environment. Implementing spatial intelligence depends on integrating a variety of enabling technologies in AI, distributed systems, and human interfaces. Some of the most critical of these technologies, particularly in machine perception and natural language communication are currently crossing a threshold that promises to make useful, end-to-end, spatially intelligent systems viable for the first time.&lt;br/&gt;&lt;br/&gt;The overall goal of the project is to enable creation of flexible spatial intelligence with which human users can interact naturally to carry out a variety of collaborative tasks. The project will create and equip a laboratory resource specifically designed to advance the state of the art in the various enabling technologies, and facilitate and demonstrate their integration into end-to-end systems.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80124</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80124</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n853" target="n855">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Spatial Intelligence for Computer-Enhanced Interaction with Physical Environments</data>
      <data key="e_abstract">EIA-0080124&lt;br/&gt;Nelson, Randal C.&lt;br/&gt;University of Rochester&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Spatial Intelligence for Computer-Enhanced Interaction with Physical Environments&lt;br/&gt;&lt;br/&gt;Intelligent computer systems have considerable potential to augment human abilities, not only in accessing abstracted information, but also in dealing with physical environments (both real and virtual). A canonical example of such a system, though certainly not the only one, is a robot with which one can converse. To mediate between people and a physical environment, an intelligent system must perceive spatial structure of various sorts and competently execute physical actions. At the same time, it must communicate with human users to provide information, accept instruction, or assist interactively with complex tasks. &lt;br/&gt;&lt;br/&gt;The term &quot;spatial intelligence&quot; can be used to capture the overarching ability to perceive, act in, and communicate about a physical environment. Implementing spatial intelligence depends on integrating a variety of enabling technologies in AI, distributed systems, and human interfaces. Some of the most critical of these technologies, particularly in machine perception and natural language communication are currently crossing a threshold that promises to make useful, end-to-end, spatially intelligent systems viable for the first time.&lt;br/&gt;&lt;br/&gt;The overall goal of the project is to enable creation of flexible spatial intelligence with which human users can interact naturally to carry out a variety of collaborative tasks. The project will create and equip a laboratory resource specifically designed to advance the state of the art in the various enabling technologies, and facilitate and demonstrate their integration into end-to-end systems.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80124</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80124</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n854" target="n855">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Spatial Intelligence for Computer-Enhanced Interaction with Physical Environments</data>
      <data key="e_abstract">EIA-0080124&lt;br/&gt;Nelson, Randal C.&lt;br/&gt;University of Rochester&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Spatial Intelligence for Computer-Enhanced Interaction with Physical Environments&lt;br/&gt;&lt;br/&gt;Intelligent computer systems have considerable potential to augment human abilities, not only in accessing abstracted information, but also in dealing with physical environments (both real and virtual). A canonical example of such a system, though certainly not the only one, is a robot with which one can converse. To mediate between people and a physical environment, an intelligent system must perceive spatial structure of various sorts and competently execute physical actions. At the same time, it must communicate with human users to provide information, accept instruction, or assist interactively with complex tasks. &lt;br/&gt;&lt;br/&gt;The term &quot;spatial intelligence&quot; can be used to capture the overarching ability to perceive, act in, and communicate about a physical environment. Implementing spatial intelligence depends on integrating a variety of enabling technologies in AI, distributed systems, and human interfaces. Some of the most critical of these technologies, particularly in machine perception and natural language communication are currently crossing a threshold that promises to make useful, end-to-end, spatially intelligent systems viable for the first time.&lt;br/&gt;&lt;br/&gt;The overall goal of the project is to enable creation of flexible spatial intelligence with which human users can interact naturally to carry out a variety of collaborative tasks. The project will create and equip a laboratory resource specifically designed to advance the state of the art in the various enabling technologies, and facilitate and demonstrate their integration into end-to-end systems.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80124</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80124</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n856" target="n857">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Formal Methods Education and Programming Effectiveness: Are They Related?</data>
      <data key="e_abstract">EIA-0082849&lt;br/&gt;Page, Rex&lt;br/&gt;University of Oklahoma&lt;br/&gt;&lt;br/&gt;ITR: Formal Methods Education and Programming Effectiveness: Are They&lt;br/&gt;Related?&lt;br/&gt;&lt;br/&gt;Software is often laced with coding errors which result in loss of life and&lt;br/&gt;productivity. These problems have been addressed by improved tools,&lt;br/&gt;disciplined processes, more effective design and analysis, and software&lt;br/&gt;product inspections. This research addresses the problem through better&lt;br/&gt;education. This work seeks evidence for the integration of mathematical&lt;br/&gt;reasoning in computing education as leading to improved programming skills.&lt;br/&gt; The project will measure and compare the programming effectiveness of two&lt;br/&gt;cohorts: one that studies core concepts of mathematical reasoning within&lt;br/&gt;the context of programming and another studying these core concepts in a&lt;br/&gt;pure mathematical context. The comparison measure of programming skills&lt;br/&gt;will be software development projects in a subsequent course. Learning&lt;br/&gt;more about any relationships between programming ability and mathematical&lt;br/&gt;reasoning can be an important step in improving curricula that educate&lt;br/&gt;students for the critical information technology positions of the future</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">82849</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82849</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n675" target="n859">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Digital Campus: Scalable Information Services on a Campus-wide Wireless Network</data>
      <data key="e_abstract">EIA-0080134&lt;br/&gt;Singh, Ambuj&lt;br/&gt;University of California - Santa Barbara&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Digital Campus: Scalable Information Services on a Campus-Wide Wireless Network&lt;br/&gt;&lt;br/&gt;Researchers at the University of California at Santa Barbara will implement a wireless-networked, distributed heterogeneous environment on campus and use it to conduct research in databases, networking, distributed systems, and multimedia. The PIs will focus on large-scale systems in which data is the critical resource and system services are based on various data manipulation functions including data collection, movement/delivery, aggregation/processing, and presentation. A significant part of the research will be conducted using a digital classroom, a remote classroom, and individual and team kiosks. Services such as lecture on demand, virtual offices, and remote learning will be provided using this infrastructure. Specific research issues that will be investigated include content-based access, personalized views, multi-dimensional indexing, smart end-to-end applications, joint source-network coding, scalable storage, reliable network service, information summarization, distributed collaboration, multimedia annotation, and interactivity.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80134</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80134</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n675" target="n677">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Digital Campus: Scalable Information Services on a Campus-wide Wireless Network</data>
      <data key="e_abstract">EIA-0080134&lt;br/&gt;Singh, Ambuj&lt;br/&gt;University of California - Santa Barbara&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Digital Campus: Scalable Information Services on a Campus-Wide Wireless Network&lt;br/&gt;&lt;br/&gt;Researchers at the University of California at Santa Barbara will implement a wireless-networked, distributed heterogeneous environment on campus and use it to conduct research in databases, networking, distributed systems, and multimedia. The PIs will focus on large-scale systems in which data is the critical resource and system services are based on various data manipulation functions including data collection, movement/delivery, aggregation/processing, and presentation. A significant part of the research will be conducted using a digital classroom, a remote classroom, and individual and team kiosks. Services such as lecture on demand, virtual offices, and remote learning will be provided using this infrastructure. Specific research issues that will be investigated include content-based access, personalized views, multi-dimensional indexing, smart end-to-end applications, joint source-network coding, scalable storage, reliable network service, information summarization, distributed collaboration, multimedia annotation, and interactivity.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80134</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80134</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n675" target="n861">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Digital Campus: Scalable Information Services on a Campus-wide Wireless Network</data>
      <data key="e_abstract">EIA-0080134&lt;br/&gt;Singh, Ambuj&lt;br/&gt;University of California - Santa Barbara&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Digital Campus: Scalable Information Services on a Campus-Wide Wireless Network&lt;br/&gt;&lt;br/&gt;Researchers at the University of California at Santa Barbara will implement a wireless-networked, distributed heterogeneous environment on campus and use it to conduct research in databases, networking, distributed systems, and multimedia. The PIs will focus on large-scale systems in which data is the critical resource and system services are based on various data manipulation functions including data collection, movement/delivery, aggregation/processing, and presentation. A significant part of the research will be conducted using a digital classroom, a remote classroom, and individual and team kiosks. Services such as lecture on demand, virtual offices, and remote learning will be provided using this infrastructure. Specific research issues that will be investigated include content-based access, personalized views, multi-dimensional indexing, smart end-to-end applications, joint source-network coding, scalable storage, reliable network service, information summarization, distributed collaboration, multimedia annotation, and interactivity.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80134</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80134</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n193" target="n675">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Digital Campus: Scalable Information Services on a Campus-wide Wireless Network</data>
      <data key="e_abstract">EIA-0080134&lt;br/&gt;Singh, Ambuj&lt;br/&gt;University of California - Santa Barbara&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Digital Campus: Scalable Information Services on a Campus-Wide Wireless Network&lt;br/&gt;&lt;br/&gt;Researchers at the University of California at Santa Barbara will implement a wireless-networked, distributed heterogeneous environment on campus and use it to conduct research in databases, networking, distributed systems, and multimedia. The PIs will focus on large-scale systems in which data is the critical resource and system services are based on various data manipulation functions including data collection, movement/delivery, aggregation/processing, and presentation. A significant part of the research will be conducted using a digital classroom, a remote classroom, and individual and team kiosks. Services such as lecture on demand, virtual offices, and remote learning will be provided using this infrastructure. Specific research issues that will be investigated include content-based access, personalized views, multi-dimensional indexing, smart end-to-end applications, joint source-network coding, scalable storage, reliable network service, information summarization, distributed collaboration, multimedia annotation, and interactivity.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80134</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80134</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n677" target="n859">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Digital Campus: Scalable Information Services on a Campus-wide Wireless Network</data>
      <data key="e_abstract">EIA-0080134&lt;br/&gt;Singh, Ambuj&lt;br/&gt;University of California - Santa Barbara&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Digital Campus: Scalable Information Services on a Campus-Wide Wireless Network&lt;br/&gt;&lt;br/&gt;Researchers at the University of California at Santa Barbara will implement a wireless-networked, distributed heterogeneous environment on campus and use it to conduct research in databases, networking, distributed systems, and multimedia. The PIs will focus on large-scale systems in which data is the critical resource and system services are based on various data manipulation functions including data collection, movement/delivery, aggregation/processing, and presentation. A significant part of the research will be conducted using a digital classroom, a remote classroom, and individual and team kiosks. Services such as lecture on demand, virtual offices, and remote learning will be provided using this infrastructure. Specific research issues that will be investigated include content-based access, personalized views, multi-dimensional indexing, smart end-to-end applications, joint source-network coding, scalable storage, reliable network service, information summarization, distributed collaboration, multimedia annotation, and interactivity.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80134</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80134</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n859" target="n861">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Digital Campus: Scalable Information Services on a Campus-wide Wireless Network</data>
      <data key="e_abstract">EIA-0080134&lt;br/&gt;Singh, Ambuj&lt;br/&gt;University of California - Santa Barbara&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Digital Campus: Scalable Information Services on a Campus-Wide Wireless Network&lt;br/&gt;&lt;br/&gt;Researchers at the University of California at Santa Barbara will implement a wireless-networked, distributed heterogeneous environment on campus and use it to conduct research in databases, networking, distributed systems, and multimedia. The PIs will focus on large-scale systems in which data is the critical resource and system services are based on various data manipulation functions including data collection, movement/delivery, aggregation/processing, and presentation. A significant part of the research will be conducted using a digital classroom, a remote classroom, and individual and team kiosks. Services such as lecture on demand, virtual offices, and remote learning will be provided using this infrastructure. Specific research issues that will be investigated include content-based access, personalized views, multi-dimensional indexing, smart end-to-end applications, joint source-network coding, scalable storage, reliable network service, information summarization, distributed collaboration, multimedia annotation, and interactivity.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80134</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80134</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n193" target="n859">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Digital Campus: Scalable Information Services on a Campus-wide Wireless Network</data>
      <data key="e_abstract">EIA-0080134&lt;br/&gt;Singh, Ambuj&lt;br/&gt;University of California - Santa Barbara&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Digital Campus: Scalable Information Services on a Campus-Wide Wireless Network&lt;br/&gt;&lt;br/&gt;Researchers at the University of California at Santa Barbara will implement a wireless-networked, distributed heterogeneous environment on campus and use it to conduct research in databases, networking, distributed systems, and multimedia. The PIs will focus on large-scale systems in which data is the critical resource and system services are based on various data manipulation functions including data collection, movement/delivery, aggregation/processing, and presentation. A significant part of the research will be conducted using a digital classroom, a remote classroom, and individual and team kiosks. Services such as lecture on demand, virtual offices, and remote learning will be provided using this infrastructure. Specific research issues that will be investigated include content-based access, personalized views, multi-dimensional indexing, smart end-to-end applications, joint source-network coding, scalable storage, reliable network service, information summarization, distributed collaboration, multimedia annotation, and interactivity.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80134</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80134</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n677" target="n861">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Digital Campus: Scalable Information Services on a Campus-wide Wireless Network</data>
      <data key="e_abstract">EIA-0080134&lt;br/&gt;Singh, Ambuj&lt;br/&gt;University of California - Santa Barbara&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Digital Campus: Scalable Information Services on a Campus-Wide Wireless Network&lt;br/&gt;&lt;br/&gt;Researchers at the University of California at Santa Barbara will implement a wireless-networked, distributed heterogeneous environment on campus and use it to conduct research in databases, networking, distributed systems, and multimedia. The PIs will focus on large-scale systems in which data is the critical resource and system services are based on various data manipulation functions including data collection, movement/delivery, aggregation/processing, and presentation. A significant part of the research will be conducted using a digital classroom, a remote classroom, and individual and team kiosks. Services such as lecture on demand, virtual offices, and remote learning will be provided using this infrastructure. Specific research issues that will be investigated include content-based access, personalized views, multi-dimensional indexing, smart end-to-end applications, joint source-network coding, scalable storage, reliable network service, information summarization, distributed collaboration, multimedia annotation, and interactivity.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80134</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80134</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n193" target="n677">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Digital Campus: Scalable Information Services on a Campus-wide Wireless Network</data>
      <data key="e_abstract">EIA-0080134&lt;br/&gt;Singh, Ambuj&lt;br/&gt;University of California - Santa Barbara&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Digital Campus: Scalable Information Services on a Campus-Wide Wireless Network&lt;br/&gt;&lt;br/&gt;Researchers at the University of California at Santa Barbara will implement a wireless-networked, distributed heterogeneous environment on campus and use it to conduct research in databases, networking, distributed systems, and multimedia. The PIs will focus on large-scale systems in which data is the critical resource and system services are based on various data manipulation functions including data collection, movement/delivery, aggregation/processing, and presentation. A significant part of the research will be conducted using a digital classroom, a remote classroom, and individual and team kiosks. Services such as lecture on demand, virtual offices, and remote learning will be provided using this infrastructure. Specific research issues that will be investigated include content-based access, personalized views, multi-dimensional indexing, smart end-to-end applications, joint source-network coding, scalable storage, reliable network service, information summarization, distributed collaboration, multimedia annotation, and interactivity.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80134</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80134</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n193" target="n861">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Digital Campus: Scalable Information Services on a Campus-wide Wireless Network</data>
      <data key="e_abstract">EIA-0080134&lt;br/&gt;Singh, Ambuj&lt;br/&gt;University of California - Santa Barbara&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Digital Campus: Scalable Information Services on a Campus-Wide Wireless Network&lt;br/&gt;&lt;br/&gt;Researchers at the University of California at Santa Barbara will implement a wireless-networked, distributed heterogeneous environment on campus and use it to conduct research in databases, networking, distributed systems, and multimedia. The PIs will focus on large-scale systems in which data is the critical resource and system services are based on various data manipulation functions including data collection, movement/delivery, aggregation/processing, and presentation. A significant part of the research will be conducted using a digital classroom, a remote classroom, and individual and team kiosks. Services such as lecture on demand, virtual offices, and remote learning will be provided using this infrastructure. Specific research issues that will be investigated include content-based access, personalized views, multi-dimensional indexing, smart end-to-end applications, joint source-network coding, scalable storage, reliable network service, information summarization, distributed collaboration, multimedia annotation, and interactivity.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80134</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80134</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n863" target="n864">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: The Digital Commonspace</data>
      <data key="e_abstract">EIA-0080146&lt;br/&gt;Nutt, Gary&lt;br/&gt;University of Colorado - Boulder&lt;br/&gt;&lt;br/&gt;CISE Reserach Infrastructure: The Digital CommonSpace&lt;br/&gt;&lt;br/&gt;This research program addresses symbiotic computing environments in which people use small, communicating computers (SCCs) to collect and use information and tools in their personal information space. This information space, which we call a Digital CommonSpace, is managed by a broad spectrum of computing and network equipment: SCCs, laptops, personal computers and workstations, and servers all interconnected by a spectrum of networks. This research addresses devices, systems, and networks; dynamic interconnection facilities; and general applications. There is an emerging computing model for symbiotic computing environments in which traditional computer science domains have become blurred: The OS must be tailored to meet the needs of application support tools and the applications themselves. The application support software must exploit the OS design and while providing custom support to the applications. Applications must use new software paradigms to take advantage of the application support tools and OS. The individuals for this research program have conducted research in each of these traditional domains, and they are now focusing on the Digital CommonSpace as the mechanism to advance the state-of-the-art symbiotic computing.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80146</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80146</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n863" target="n865">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: The Digital Commonspace</data>
      <data key="e_abstract">EIA-0080146&lt;br/&gt;Nutt, Gary&lt;br/&gt;University of Colorado - Boulder&lt;br/&gt;&lt;br/&gt;CISE Reserach Infrastructure: The Digital CommonSpace&lt;br/&gt;&lt;br/&gt;This research program addresses symbiotic computing environments in which people use small, communicating computers (SCCs) to collect and use information and tools in their personal information space. This information space, which we call a Digital CommonSpace, is managed by a broad spectrum of computing and network equipment: SCCs, laptops, personal computers and workstations, and servers all interconnected by a spectrum of networks. This research addresses devices, systems, and networks; dynamic interconnection facilities; and general applications. There is an emerging computing model for symbiotic computing environments in which traditional computer science domains have become blurred: The OS must be tailored to meet the needs of application support tools and the applications themselves. The application support software must exploit the OS design and while providing custom support to the applications. Applications must use new software paradigms to take advantage of the application support tools and OS. The individuals for this research program have conducted research in each of these traditional domains, and they are now focusing on the Digital CommonSpace as the mechanism to advance the state-of-the-art symbiotic computing.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80146</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80146</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n863" target="n866">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: The Digital Commonspace</data>
      <data key="e_abstract">EIA-0080146&lt;br/&gt;Nutt, Gary&lt;br/&gt;University of Colorado - Boulder&lt;br/&gt;&lt;br/&gt;CISE Reserach Infrastructure: The Digital CommonSpace&lt;br/&gt;&lt;br/&gt;This research program addresses symbiotic computing environments in which people use small, communicating computers (SCCs) to collect and use information and tools in their personal information space. This information space, which we call a Digital CommonSpace, is managed by a broad spectrum of computing and network equipment: SCCs, laptops, personal computers and workstations, and servers all interconnected by a spectrum of networks. This research addresses devices, systems, and networks; dynamic interconnection facilities; and general applications. There is an emerging computing model for symbiotic computing environments in which traditional computer science domains have become blurred: The OS must be tailored to meet the needs of application support tools and the applications themselves. The application support software must exploit the OS design and while providing custom support to the applications. Applications must use new software paradigms to take advantage of the application support tools and OS. The individuals for this research program have conducted research in each of these traditional domains, and they are now focusing on the Digital CommonSpace as the mechanism to advance the state-of-the-art symbiotic computing.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80146</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80146</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n756" target="n863">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: The Digital Commonspace</data>
      <data key="e_abstract">EIA-0080146&lt;br/&gt;Nutt, Gary&lt;br/&gt;University of Colorado - Boulder&lt;br/&gt;&lt;br/&gt;CISE Reserach Infrastructure: The Digital CommonSpace&lt;br/&gt;&lt;br/&gt;This research program addresses symbiotic computing environments in which people use small, communicating computers (SCCs) to collect and use information and tools in their personal information space. This information space, which we call a Digital CommonSpace, is managed by a broad spectrum of computing and network equipment: SCCs, laptops, personal computers and workstations, and servers all interconnected by a spectrum of networks. This research addresses devices, systems, and networks; dynamic interconnection facilities; and general applications. There is an emerging computing model for symbiotic computing environments in which traditional computer science domains have become blurred: The OS must be tailored to meet the needs of application support tools and the applications themselves. The application support software must exploit the OS design and while providing custom support to the applications. Applications must use new software paradigms to take advantage of the application support tools and OS. The individuals for this research program have conducted research in each of these traditional domains, and they are now focusing on the Digital CommonSpace as the mechanism to advance the state-of-the-art symbiotic computing.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80146</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80146</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n864" target="n865">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: The Digital Commonspace</data>
      <data key="e_abstract">EIA-0080146&lt;br/&gt;Nutt, Gary&lt;br/&gt;University of Colorado - Boulder&lt;br/&gt;&lt;br/&gt;CISE Reserach Infrastructure: The Digital CommonSpace&lt;br/&gt;&lt;br/&gt;This research program addresses symbiotic computing environments in which people use small, communicating computers (SCCs) to collect and use information and tools in their personal information space. This information space, which we call a Digital CommonSpace, is managed by a broad spectrum of computing and network equipment: SCCs, laptops, personal computers and workstations, and servers all interconnected by a spectrum of networks. This research addresses devices, systems, and networks; dynamic interconnection facilities; and general applications. There is an emerging computing model for symbiotic computing environments in which traditional computer science domains have become blurred: The OS must be tailored to meet the needs of application support tools and the applications themselves. The application support software must exploit the OS design and while providing custom support to the applications. Applications must use new software paradigms to take advantage of the application support tools and OS. The individuals for this research program have conducted research in each of these traditional domains, and they are now focusing on the Digital CommonSpace as the mechanism to advance the state-of-the-art symbiotic computing.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80146</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80146</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n864" target="n866">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: The Digital Commonspace</data>
      <data key="e_abstract">EIA-0080146&lt;br/&gt;Nutt, Gary&lt;br/&gt;University of Colorado - Boulder&lt;br/&gt;&lt;br/&gt;CISE Reserach Infrastructure: The Digital CommonSpace&lt;br/&gt;&lt;br/&gt;This research program addresses symbiotic computing environments in which people use small, communicating computers (SCCs) to collect and use information and tools in their personal information space. This information space, which we call a Digital CommonSpace, is managed by a broad spectrum of computing and network equipment: SCCs, laptops, personal computers and workstations, and servers all interconnected by a spectrum of networks. This research addresses devices, systems, and networks; dynamic interconnection facilities; and general applications. There is an emerging computing model for symbiotic computing environments in which traditional computer science domains have become blurred: The OS must be tailored to meet the needs of application support tools and the applications themselves. The application support software must exploit the OS design and while providing custom support to the applications. Applications must use new software paradigms to take advantage of the application support tools and OS. The individuals for this research program have conducted research in each of these traditional domains, and they are now focusing on the Digital CommonSpace as the mechanism to advance the state-of-the-art symbiotic computing.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80146</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80146</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n756" target="n864">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: The Digital Commonspace</data>
      <data key="e_abstract">EIA-0080146&lt;br/&gt;Nutt, Gary&lt;br/&gt;University of Colorado - Boulder&lt;br/&gt;&lt;br/&gt;CISE Reserach Infrastructure: The Digital CommonSpace&lt;br/&gt;&lt;br/&gt;This research program addresses symbiotic computing environments in which people use small, communicating computers (SCCs) to collect and use information and tools in their personal information space. This information space, which we call a Digital CommonSpace, is managed by a broad spectrum of computing and network equipment: SCCs, laptops, personal computers and workstations, and servers all interconnected by a spectrum of networks. This research addresses devices, systems, and networks; dynamic interconnection facilities; and general applications. There is an emerging computing model for symbiotic computing environments in which traditional computer science domains have become blurred: The OS must be tailored to meet the needs of application support tools and the applications themselves. The application support software must exploit the OS design and while providing custom support to the applications. Applications must use new software paradigms to take advantage of the application support tools and OS. The individuals for this research program have conducted research in each of these traditional domains, and they are now focusing on the Digital CommonSpace as the mechanism to advance the state-of-the-art symbiotic computing.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80146</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80146</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n865" target="n866">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: The Digital Commonspace</data>
      <data key="e_abstract">EIA-0080146&lt;br/&gt;Nutt, Gary&lt;br/&gt;University of Colorado - Boulder&lt;br/&gt;&lt;br/&gt;CISE Reserach Infrastructure: The Digital CommonSpace&lt;br/&gt;&lt;br/&gt;This research program addresses symbiotic computing environments in which people use small, communicating computers (SCCs) to collect and use information and tools in their personal information space. This information space, which we call a Digital CommonSpace, is managed by a broad spectrum of computing and network equipment: SCCs, laptops, personal computers and workstations, and servers all interconnected by a spectrum of networks. This research addresses devices, systems, and networks; dynamic interconnection facilities; and general applications. There is an emerging computing model for symbiotic computing environments in which traditional computer science domains have become blurred: The OS must be tailored to meet the needs of application support tools and the applications themselves. The application support software must exploit the OS design and while providing custom support to the applications. Applications must use new software paradigms to take advantage of the application support tools and OS. The individuals for this research program have conducted research in each of these traditional domains, and they are now focusing on the Digital CommonSpace as the mechanism to advance the state-of-the-art symbiotic computing.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80146</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80146</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n756" target="n865">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: The Digital Commonspace</data>
      <data key="e_abstract">EIA-0080146&lt;br/&gt;Nutt, Gary&lt;br/&gt;University of Colorado - Boulder&lt;br/&gt;&lt;br/&gt;CISE Reserach Infrastructure: The Digital CommonSpace&lt;br/&gt;&lt;br/&gt;This research program addresses symbiotic computing environments in which people use small, communicating computers (SCCs) to collect and use information and tools in their personal information space. This information space, which we call a Digital CommonSpace, is managed by a broad spectrum of computing and network equipment: SCCs, laptops, personal computers and workstations, and servers all interconnected by a spectrum of networks. This research addresses devices, systems, and networks; dynamic interconnection facilities; and general applications. There is an emerging computing model for symbiotic computing environments in which traditional computer science domains have become blurred: The OS must be tailored to meet the needs of application support tools and the applications themselves. The application support software must exploit the OS design and while providing custom support to the applications. Applications must use new software paradigms to take advantage of the application support tools and OS. The individuals for this research program have conducted research in each of these traditional domains, and they are now focusing on the Digital CommonSpace as the mechanism to advance the state-of-the-art symbiotic computing.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80146</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80146</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n756" target="n866">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: The Digital Commonspace</data>
      <data key="e_abstract">EIA-0080146&lt;br/&gt;Nutt, Gary&lt;br/&gt;University of Colorado - Boulder&lt;br/&gt;&lt;br/&gt;CISE Reserach Infrastructure: The Digital CommonSpace&lt;br/&gt;&lt;br/&gt;This research program addresses symbiotic computing environments in which people use small, communicating computers (SCCs) to collect and use information and tools in their personal information space. This information space, which we call a Digital CommonSpace, is managed by a broad spectrum of computing and network equipment: SCCs, laptops, personal computers and workstations, and servers all interconnected by a spectrum of networks. This research addresses devices, systems, and networks; dynamic interconnection facilities; and general applications. There is an emerging computing model for symbiotic computing environments in which traditional computer science domains have become blurred: The OS must be tailored to meet the needs of application support tools and the applications themselves. The application support software must exploit the OS design and while providing custom support to the applications. Applications must use new software paradigms to take advantage of the application support tools and OS. The individuals for this research program have conducted research in each of these traditional domains, and they are now focusing on the Digital CommonSpace as the mechanism to advance the state-of-the-art symbiotic computing.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80146</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80146</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n872" target="n873">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Learning and recognition of objects in sensory data.</data>
      <data key="e_abstract">Humans can recognize objects and scenes using their senses. The ability of learning the appearance of a great number of objects, organizing them into categories, and quickly recognizing them later is an important skill for survival. Replicating such ability in machines would be extremely useful in a great number of scientific and industrial applications such as automatic exploration of databases of medical images, diagnostics and quality control in industrial plants, automatic classification of images and sounds on the web.&lt;br/&gt;&lt;br/&gt;The aim of this study is to develop a theory of recognition that is applicable any type of sensory data and where no supervision is required for learning and categorization.&lt;br/&gt;&lt;br/&gt;The approach is probabilistic: object categories are modeled by probability density functions on part appearance and object shape. Detection and recognition are formulated as statistical inference problems. Unsupervised learning of object categories is approached using maximum likelihood. In order to motivate and test the theory the investigators will engage in three applications: automatic classification and retrieval of objects from image databases, of human actions from movies, and of neuronal signals associated with perceptual tasks.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82830</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82830</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n878" target="n879">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: A CISE Planning Proposal for Inter American University of Puerto Rico</data>
      <data key="e_abstract">EIA-0002197&lt;br/&gt;Aldebol, Sylvia&lt;br/&gt;Inter American University of Puerto Rico&lt;br/&gt;&lt;br/&gt;CISE Minority Institutions Infrastructure: A CISE Planning Proposal for Inter American University of Puerto Rico&lt;br/&gt;&lt;br/&gt;The purpose of this project is to prepare a five-year plan that will improve the quality of learning experiences, increase research activities, and attract more students to the computer science program. In this five-year plan, activities will be developed to promote active student learning through inquiry and project-based laboratory and classroom experiences. The strategy used in developing the activities will be to identify and develop focus areas in computer science that will impact course and curriculum, research activities, laboratory improvement, faculty enhancement, and industry partnership. The two main focus areas to be developed are (1) computing on a network and (2) data acquisition.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2197</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">2197</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n878" target="n880">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: A CISE Planning Proposal for Inter American University of Puerto Rico</data>
      <data key="e_abstract">EIA-0002197&lt;br/&gt;Aldebol, Sylvia&lt;br/&gt;Inter American University of Puerto Rico&lt;br/&gt;&lt;br/&gt;CISE Minority Institutions Infrastructure: A CISE Planning Proposal for Inter American University of Puerto Rico&lt;br/&gt;&lt;br/&gt;The purpose of this project is to prepare a five-year plan that will improve the quality of learning experiences, increase research activities, and attract more students to the computer science program. In this five-year plan, activities will be developed to promote active student learning through inquiry and project-based laboratory and classroom experiences. The strategy used in developing the activities will be to identify and develop focus areas in computer science that will impact course and curriculum, research activities, laboratory improvement, faculty enhancement, and industry partnership. The two main focus areas to be developed are (1) computing on a network and (2) data acquisition.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2197</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">2197</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n879" target="n880">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: A CISE Planning Proposal for Inter American University of Puerto Rico</data>
      <data key="e_abstract">EIA-0002197&lt;br/&gt;Aldebol, Sylvia&lt;br/&gt;Inter American University of Puerto Rico&lt;br/&gt;&lt;br/&gt;CISE Minority Institutions Infrastructure: A CISE Planning Proposal for Inter American University of Puerto Rico&lt;br/&gt;&lt;br/&gt;The purpose of this project is to prepare a five-year plan that will improve the quality of learning experiences, increase research activities, and attract more students to the computer science program. In this five-year plan, activities will be developed to promote active student learning through inquiry and project-based laboratory and classroom experiences. The strategy used in developing the activities will be to identify and develop focus areas in computer science that will impact course and curriculum, research activities, laboratory improvement, faculty enhancement, and industry partnership. The two main focus areas to be developed are (1) computing on a network and (2) data acquisition.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">2197</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">2197</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n881" target="n882">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Dynamic Code Enhancement and Scheduling Techniques for Complex Simulations</data>
      <data key="e_abstract">One of the most important classes of programs today is large-scale adaptive and time-dependent simulations. These are increasingly important for solving important scientific problems such as particle dynamics and boundary element problems. An equally important environment for running any code is a cluster of workstations. Such a system may include single-processor and SMP nodes supporting a hybrid message passing/shared address space programming paradigm. This project will provide automated and semi-automated tools for optimizing serial performance, parallel performance, and overall resource utilization when those complex codes are run in such complex environments.&lt;br/&gt;&lt;br/&gt;Technically, the goal of the project is to develop a comprehensive dynamic code enhancement, resource management, scheduling, and performance monitoring framework. This is accomplished by relegating a number of code optimization and scheduling decisions to run-time, where they can rely on performance traces. The framework generalizes the process-thread model to a schedulable entity model in which processes and threads are treated as free and bound entities respectively. Either the programmer or the compiler can create these entities. In addition, the dynamic code enhancer performs optimizations at run-time granularity control for threads, and transformations between free and bound entities to improve performance. The overhead of dynamic code enhancement is amortized over several computation steps. An aggregate scheduler/resource manager maps the specified entities to hosts, using performance data to optimize its decisions for CPU, memory system, network, and parallel code performance. The dynamic code enhancer/scheduler framework is triggered by online performance monitoring that is automatically instrumented into the code.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82834</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82834</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n886" target="n887">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Design Automation Tools for Micro-Scale Mixed Technology Systems</data>
      <data key="e_abstract">This research is on developing models and algorithms for design and analysis of optoelectronic systems to encompass micro-scale systems that integrate optical, electrical, and mechanical components. It is focused on five main topics: light propagation models for micro-optics, mixed-domain component models, a corresponding mixed-domain simulation environment, a 3D user interface, and design and analysis techniques that directly incorporate the additional performance concerns of mixed-technology systems. The algorithms being developed include new hybrid methods to model optical propagation in the micro-scale environment, techniques for creating consistent component level models across domains, and multi-time base simulation techniques. For the methodologies for mixed-technology design, the definition of performance is expanded beyond speed, power and area to include the additional concerns of mixed-technology systems such as noise, crosstalk and tolerancing. The research team is creating software tools, which will be made available to other research groups in both industry and academia.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">9.98832e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98832e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n725" target="n726">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Real-time Scheduling on Heterogeneous Multiprocessors</data>
      <data key="e_abstract">As real-time computer application systems become larger and more complex, it is becoming imperative that such applications be implemented on multi-processor platforms rather than on uniprocessor ones. Furthermore it is often the case that the implementation platforms are not completely homogeneous, but are comprised of several different kinds of processors and other resources. This project will different kinds of processors and other resources. This project will study both the fundamental scheduling-theoretic questions, and the pragmatic implementation issues, that arise when periodic task systems are to be implemented on such heterogeneous multi-resource platforms. This research will build upon previous research, performed by both investigators, on fair scheduling in real-time system. The theoretical findings resulting from this research will be validated by applying them to some implementation projects currently under development at the University of North Carolina.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">9.98833e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98833e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n847" target="n891">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Compiler Techniques for Code Compression</data>
      <data key="e_abstract">The goal of this project is to develop techniques to reduce the memory footprint of executable code, so as to allow more and more sophisticated applications to be executed on limited-memory devices, such as hand-held computers, personal digital assistants, and embedded processors. Recent years have seen the incorporation of computers and computational devices into many aspects of our everyday lives. In many cases, the amount of memory available for such processors is limited by considerations such as space, weight, and power consumption. At the same time, there is a desire to run more and more sophisticated applications on such processors. Since an application that occupies more memory than is available on such a processor will not be able to run on that processor, it is desirable to develop techniques to reduce the memory footprint of applications. Moreover, it is necessary that the compressed applications remain executable, since for the application domains under consideration it is not feasible to decompress the executable in order to execute it. This project investigates the construction of tools and techniques for code compression in a manner that preserves executability.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">73394</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">73394</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n494" target="n893">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Experimental Partnership - Meridian: An Integrated Toolkit for Developing Interactive Distributed Applications</data>
      <data key="e_abstract">EIA-0000433&lt;br/&gt;Betty H. Cheng&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;TITLE: Meridian: An Integrated Toolkit for Developing Interactive Distributed Applications&lt;br/&gt;&lt;br/&gt;The proposed project involves the integration and validation of MERIDIAN, a collection of tools designed to help automate the development of IDAs. Collectively, these tools will support diagram-based modeling, rigorous correctness analysis, software reuse, automated code generation, and software visualization. Moreover, they will interact with one another through explicit design representations with formally defined semantics, enabling requirements to be traced from high-level models to low-level code.&lt;br/&gt;&lt;br/&gt;Interactive distributed applications (IDAs) are those that involve direct interaction with users and whose processing and data components are distributed across a network. Examples of IDAs include distributed data management systems, on-board driver/pilot navigation assistance systems, computer-supported cooperative work environments, distance education tools, and a variety of public safety systems. The increasing interest in IDAs is fueled by several factors, including the advent of the World-Wide Web, the development of new middleware technologies, the introduction of scripting languages for graphical user interfaces, and the availability of new network services and protocols.</data>
      <data key="e_pgm">2880</data>
      <data key="e_label">433</data>
      <data key="e_expirationDate">2007-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">433</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n893" target="n895">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Experimental Partnership - Meridian: An Integrated Toolkit for Developing Interactive Distributed Applications</data>
      <data key="e_abstract">EIA-0000433&lt;br/&gt;Betty H. Cheng&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;TITLE: Meridian: An Integrated Toolkit for Developing Interactive Distributed Applications&lt;br/&gt;&lt;br/&gt;The proposed project involves the integration and validation of MERIDIAN, a collection of tools designed to help automate the development of IDAs. Collectively, these tools will support diagram-based modeling, rigorous correctness analysis, software reuse, automated code generation, and software visualization. Moreover, they will interact with one another through explicit design representations with formally defined semantics, enabling requirements to be traced from high-level models to low-level code.&lt;br/&gt;&lt;br/&gt;Interactive distributed applications (IDAs) are those that involve direct interaction with users and whose processing and data components are distributed across a network. Examples of IDAs include distributed data management systems, on-board driver/pilot navigation assistance systems, computer-supported cooperative work environments, distance education tools, and a variety of public safety systems. The increasing interest in IDAs is fueled by several factors, including the advent of the World-Wide Web, the development of new middleware technologies, the introduction of scripting languages for graphical user interfaces, and the availability of new network services and protocols.</data>
      <data key="e_pgm">2880</data>
      <data key="e_label">433</data>
      <data key="e_expirationDate">2007-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">433</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n250" target="n893">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Experimental Partnership - Meridian: An Integrated Toolkit for Developing Interactive Distributed Applications</data>
      <data key="e_abstract">EIA-0000433&lt;br/&gt;Betty H. Cheng&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;TITLE: Meridian: An Integrated Toolkit for Developing Interactive Distributed Applications&lt;br/&gt;&lt;br/&gt;The proposed project involves the integration and validation of MERIDIAN, a collection of tools designed to help automate the development of IDAs. Collectively, these tools will support diagram-based modeling, rigorous correctness analysis, software reuse, automated code generation, and software visualization. Moreover, they will interact with one another through explicit design representations with formally defined semantics, enabling requirements to be traced from high-level models to low-level code.&lt;br/&gt;&lt;br/&gt;Interactive distributed applications (IDAs) are those that involve direct interaction with users and whose processing and data components are distributed across a network. Examples of IDAs include distributed data management systems, on-board driver/pilot navigation assistance systems, computer-supported cooperative work environments, distance education tools, and a variety of public safety systems. The increasing interest in IDAs is fueled by several factors, including the advent of the World-Wide Web, the development of new middleware technologies, the introduction of scripting languages for graphical user interfaces, and the availability of new network services and protocols.</data>
      <data key="e_pgm">2880</data>
      <data key="e_label">433</data>
      <data key="e_expirationDate">2007-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">433</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n494" target="n895">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Experimental Partnership - Meridian: An Integrated Toolkit for Developing Interactive Distributed Applications</data>
      <data key="e_abstract">EIA-0000433&lt;br/&gt;Betty H. Cheng&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;TITLE: Meridian: An Integrated Toolkit for Developing Interactive Distributed Applications&lt;br/&gt;&lt;br/&gt;The proposed project involves the integration and validation of MERIDIAN, a collection of tools designed to help automate the development of IDAs. Collectively, these tools will support diagram-based modeling, rigorous correctness analysis, software reuse, automated code generation, and software visualization. Moreover, they will interact with one another through explicit design representations with formally defined semantics, enabling requirements to be traced from high-level models to low-level code.&lt;br/&gt;&lt;br/&gt;Interactive distributed applications (IDAs) are those that involve direct interaction with users and whose processing and data components are distributed across a network. Examples of IDAs include distributed data management systems, on-board driver/pilot navigation assistance systems, computer-supported cooperative work environments, distance education tools, and a variety of public safety systems. The increasing interest in IDAs is fueled by several factors, including the advent of the World-Wide Web, the development of new middleware technologies, the introduction of scripting languages for graphical user interfaces, and the availability of new network services and protocols.</data>
      <data key="e_pgm">2880</data>
      <data key="e_label">433</data>
      <data key="e_expirationDate">2007-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">433</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n250" target="n494">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Experimental Partnership - Meridian: An Integrated Toolkit for Developing Interactive Distributed Applications</data>
      <data key="e_abstract">EIA-0000433&lt;br/&gt;Betty H. Cheng&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;TITLE: Meridian: An Integrated Toolkit for Developing Interactive Distributed Applications&lt;br/&gt;&lt;br/&gt;The proposed project involves the integration and validation of MERIDIAN, a collection of tools designed to help automate the development of IDAs. Collectively, these tools will support diagram-based modeling, rigorous correctness analysis, software reuse, automated code generation, and software visualization. Moreover, they will interact with one another through explicit design representations with formally defined semantics, enabling requirements to be traced from high-level models to low-level code.&lt;br/&gt;&lt;br/&gt;Interactive distributed applications (IDAs) are those that involve direct interaction with users and whose processing and data components are distributed across a network. Examples of IDAs include distributed data management systems, on-board driver/pilot navigation assistance systems, computer-supported cooperative work environments, distance education tools, and a variety of public safety systems. The increasing interest in IDAs is fueled by several factors, including the advent of the World-Wide Web, the development of new middleware technologies, the introduction of scripting languages for graphical user interfaces, and the availability of new network services and protocols.</data>
      <data key="e_pgm">2880</data>
      <data key="e_label">433</data>
      <data key="e_expirationDate">2007-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">433</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n250" target="n895">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Experimental Partnership - Meridian: An Integrated Toolkit for Developing Interactive Distributed Applications</data>
      <data key="e_abstract">EIA-0000433&lt;br/&gt;Betty H. Cheng&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;TITLE: Meridian: An Integrated Toolkit for Developing Interactive Distributed Applications&lt;br/&gt;&lt;br/&gt;The proposed project involves the integration and validation of MERIDIAN, a collection of tools designed to help automate the development of IDAs. Collectively, these tools will support diagram-based modeling, rigorous correctness analysis, software reuse, automated code generation, and software visualization. Moreover, they will interact with one another through explicit design representations with formally defined semantics, enabling requirements to be traced from high-level models to low-level code.&lt;br/&gt;&lt;br/&gt;Interactive distributed applications (IDAs) are those that involve direct interaction with users and whose processing and data components are distributed across a network. Examples of IDAs include distributed data management systems, on-board driver/pilot navigation assistance systems, computer-supported cooperative work environments, distance education tools, and a variety of public safety systems. The increasing interest in IDAs is fueled by several factors, including the advent of the World-Wide Web, the development of new middleware technologies, the introduction of scripting languages for graphical user interfaces, and the availability of new network services and protocols.</data>
      <data key="e_pgm">2880</data>
      <data key="e_label">433</data>
      <data key="e_expirationDate">2007-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">433</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n900" target="n901">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Parallel Computer Equipment for the Levich Institute of CCNY</data>
      <data key="e_abstract">EIA-0078826&lt;br/&gt;Koplik, Joel&lt;br/&gt;CUNY City College&lt;br/&gt;&lt;br/&gt;MRI: Parallel Computer Equipment for the Levich Institute of CCNY&lt;br/&gt;&lt;br/&gt;The faculty of the Levich Institute of the City College of New York requests funding for the purchase of a medium-scale parallel computer system for research in fluid mechanics. The equipment will support research activities in (1) shear-induced diffusion in suspensions, (2) polymer blends and instabilities, (3) complex multiphase flows, (4) molecular fluid mechanics and (5) interfacial and surfactant dynamics. In addition, the proposed system will be invaluable for broad educational purposes: developing the conceptual familiarity, hands-on experience, and hardware infrastructure required thinking and working computationally in parallel terms</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">78826</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">78826</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n900" target="n902">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Parallel Computer Equipment for the Levich Institute of CCNY</data>
      <data key="e_abstract">EIA-0078826&lt;br/&gt;Koplik, Joel&lt;br/&gt;CUNY City College&lt;br/&gt;&lt;br/&gt;MRI: Parallel Computer Equipment for the Levich Institute of CCNY&lt;br/&gt;&lt;br/&gt;The faculty of the Levich Institute of the City College of New York requests funding for the purchase of a medium-scale parallel computer system for research in fluid mechanics. The equipment will support research activities in (1) shear-induced diffusion in suspensions, (2) polymer blends and instabilities, (3) complex multiphase flows, (4) molecular fluid mechanics and (5) interfacial and surfactant dynamics. In addition, the proposed system will be invaluable for broad educational purposes: developing the conceptual familiarity, hands-on experience, and hardware infrastructure required thinking and working computationally in parallel terms</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">78826</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">78826</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n900" target="n903">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Parallel Computer Equipment for the Levich Institute of CCNY</data>
      <data key="e_abstract">EIA-0078826&lt;br/&gt;Koplik, Joel&lt;br/&gt;CUNY City College&lt;br/&gt;&lt;br/&gt;MRI: Parallel Computer Equipment for the Levich Institute of CCNY&lt;br/&gt;&lt;br/&gt;The faculty of the Levich Institute of the City College of New York requests funding for the purchase of a medium-scale parallel computer system for research in fluid mechanics. The equipment will support research activities in (1) shear-induced diffusion in suspensions, (2) polymer blends and instabilities, (3) complex multiphase flows, (4) molecular fluid mechanics and (5) interfacial and surfactant dynamics. In addition, the proposed system will be invaluable for broad educational purposes: developing the conceptual familiarity, hands-on experience, and hardware infrastructure required thinking and working computationally in parallel terms</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">78826</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">78826</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n900" target="n904">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Parallel Computer Equipment for the Levich Institute of CCNY</data>
      <data key="e_abstract">EIA-0078826&lt;br/&gt;Koplik, Joel&lt;br/&gt;CUNY City College&lt;br/&gt;&lt;br/&gt;MRI: Parallel Computer Equipment for the Levich Institute of CCNY&lt;br/&gt;&lt;br/&gt;The faculty of the Levich Institute of the City College of New York requests funding for the purchase of a medium-scale parallel computer system for research in fluid mechanics. The equipment will support research activities in (1) shear-induced diffusion in suspensions, (2) polymer blends and instabilities, (3) complex multiphase flows, (4) molecular fluid mechanics and (5) interfacial and surfactant dynamics. In addition, the proposed system will be invaluable for broad educational purposes: developing the conceptual familiarity, hands-on experience, and hardware infrastructure required thinking and working computationally in parallel terms</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">78826</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">78826</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n901" target="n902">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Parallel Computer Equipment for the Levich Institute of CCNY</data>
      <data key="e_abstract">EIA-0078826&lt;br/&gt;Koplik, Joel&lt;br/&gt;CUNY City College&lt;br/&gt;&lt;br/&gt;MRI: Parallel Computer Equipment for the Levich Institute of CCNY&lt;br/&gt;&lt;br/&gt;The faculty of the Levich Institute of the City College of New York requests funding for the purchase of a medium-scale parallel computer system for research in fluid mechanics. The equipment will support research activities in (1) shear-induced diffusion in suspensions, (2) polymer blends and instabilities, (3) complex multiphase flows, (4) molecular fluid mechanics and (5) interfacial and surfactant dynamics. In addition, the proposed system will be invaluable for broad educational purposes: developing the conceptual familiarity, hands-on experience, and hardware infrastructure required thinking and working computationally in parallel terms</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">78826</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">78826</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n901" target="n903">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Parallel Computer Equipment for the Levich Institute of CCNY</data>
      <data key="e_abstract">EIA-0078826&lt;br/&gt;Koplik, Joel&lt;br/&gt;CUNY City College&lt;br/&gt;&lt;br/&gt;MRI: Parallel Computer Equipment for the Levich Institute of CCNY&lt;br/&gt;&lt;br/&gt;The faculty of the Levich Institute of the City College of New York requests funding for the purchase of a medium-scale parallel computer system for research in fluid mechanics. The equipment will support research activities in (1) shear-induced diffusion in suspensions, (2) polymer blends and instabilities, (3) complex multiphase flows, (4) molecular fluid mechanics and (5) interfacial and surfactant dynamics. In addition, the proposed system will be invaluable for broad educational purposes: developing the conceptual familiarity, hands-on experience, and hardware infrastructure required thinking and working computationally in parallel terms</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">78826</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">78826</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n901" target="n904">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Parallel Computer Equipment for the Levich Institute of CCNY</data>
      <data key="e_abstract">EIA-0078826&lt;br/&gt;Koplik, Joel&lt;br/&gt;CUNY City College&lt;br/&gt;&lt;br/&gt;MRI: Parallel Computer Equipment for the Levich Institute of CCNY&lt;br/&gt;&lt;br/&gt;The faculty of the Levich Institute of the City College of New York requests funding for the purchase of a medium-scale parallel computer system for research in fluid mechanics. The equipment will support research activities in (1) shear-induced diffusion in suspensions, (2) polymer blends and instabilities, (3) complex multiphase flows, (4) molecular fluid mechanics and (5) interfacial and surfactant dynamics. In addition, the proposed system will be invaluable for broad educational purposes: developing the conceptual familiarity, hands-on experience, and hardware infrastructure required thinking and working computationally in parallel terms</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">78826</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">78826</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n902" target="n903">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Parallel Computer Equipment for the Levich Institute of CCNY</data>
      <data key="e_abstract">EIA-0078826&lt;br/&gt;Koplik, Joel&lt;br/&gt;CUNY City College&lt;br/&gt;&lt;br/&gt;MRI: Parallel Computer Equipment for the Levich Institute of CCNY&lt;br/&gt;&lt;br/&gt;The faculty of the Levich Institute of the City College of New York requests funding for the purchase of a medium-scale parallel computer system for research in fluid mechanics. The equipment will support research activities in (1) shear-induced diffusion in suspensions, (2) polymer blends and instabilities, (3) complex multiphase flows, (4) molecular fluid mechanics and (5) interfacial and surfactant dynamics. In addition, the proposed system will be invaluable for broad educational purposes: developing the conceptual familiarity, hands-on experience, and hardware infrastructure required thinking and working computationally in parallel terms</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">78826</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">78826</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n902" target="n904">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Parallel Computer Equipment for the Levich Institute of CCNY</data>
      <data key="e_abstract">EIA-0078826&lt;br/&gt;Koplik, Joel&lt;br/&gt;CUNY City College&lt;br/&gt;&lt;br/&gt;MRI: Parallel Computer Equipment for the Levich Institute of CCNY&lt;br/&gt;&lt;br/&gt;The faculty of the Levich Institute of the City College of New York requests funding for the purchase of a medium-scale parallel computer system for research in fluid mechanics. The equipment will support research activities in (1) shear-induced diffusion in suspensions, (2) polymer blends and instabilities, (3) complex multiphase flows, (4) molecular fluid mechanics and (5) interfacial and surfactant dynamics. In addition, the proposed system will be invaluable for broad educational purposes: developing the conceptual familiarity, hands-on experience, and hardware infrastructure required thinking and working computationally in parallel terms</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">78826</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">78826</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n903" target="n904">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Parallel Computer Equipment for the Levich Institute of CCNY</data>
      <data key="e_abstract">EIA-0078826&lt;br/&gt;Koplik, Joel&lt;br/&gt;CUNY City College&lt;br/&gt;&lt;br/&gt;MRI: Parallel Computer Equipment for the Levich Institute of CCNY&lt;br/&gt;&lt;br/&gt;The faculty of the Levich Institute of the City College of New York requests funding for the purchase of a medium-scale parallel computer system for research in fluid mechanics. The equipment will support research activities in (1) shear-induced diffusion in suspensions, (2) polymer blends and instabilities, (3) complex multiphase flows, (4) molecular fluid mechanics and (5) interfacial and surfactant dynamics. In addition, the proposed system will be invaluable for broad educational purposes: developing the conceptual familiarity, hands-on experience, and hardware infrastructure required thinking and working computationally in parallel terms</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">78826</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">78826</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n909" target="n910">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Dynamic Cooperative Performance Optimization</data>
      <data key="e_abstract">Object-oriented programming languages, notably Java, are gaining broad use&lt;br/&gt;because of their benefits, which come largely from their flexibility. But&lt;br/&gt;this same flexibility makes Java programs more difficult to optimize in&lt;br/&gt;advance. One sample collection of programs, optimized in advance, spent&lt;br/&gt;40-95% of their time on an aggressive processor waiting for the memory to&lt;br/&gt;provide data, illustrating the need to improve memory behavior. Future&lt;br/&gt;processors will only make the problem worse.&lt;br/&gt;&lt;br/&gt;The project makes an integrated attack on this problem, incorporating new&lt;br/&gt;program analyses and optimizations, profile feedback, run-time techniques&lt;br/&gt;including adaptive garbage collection algorithms, and ways of communicating&lt;br/&gt;high-level predictions and observations of program behavior to the&lt;br/&gt;hardware.&lt;br/&gt;&lt;br/&gt;The project aims to design and build a compiler, run-time system, and&lt;br/&gt;enhanced architectural and operating system features that react quickly and&lt;br/&gt;gracefully to compiler predictions and actual run-time behavior to achieve&lt;br/&gt;high performance. The goal is synergy via cooperation between the system&lt;br/&gt;components versus solving each problem within an individual component.&lt;br/&gt;&lt;br/&gt;While the research focuses on improving memory performance, the project&apos;s&lt;br/&gt;envisioned framework is suited to a wide range of performance optimization&lt;br/&gt;techniques, so the expected research results and software products have&lt;br/&gt;broader impact.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85792</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85792</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n909" target="n911">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Dynamic Cooperative Performance Optimization</data>
      <data key="e_abstract">Object-oriented programming languages, notably Java, are gaining broad use&lt;br/&gt;because of their benefits, which come largely from their flexibility. But&lt;br/&gt;this same flexibility makes Java programs more difficult to optimize in&lt;br/&gt;advance. One sample collection of programs, optimized in advance, spent&lt;br/&gt;40-95% of their time on an aggressive processor waiting for the memory to&lt;br/&gt;provide data, illustrating the need to improve memory behavior. Future&lt;br/&gt;processors will only make the problem worse.&lt;br/&gt;&lt;br/&gt;The project makes an integrated attack on this problem, incorporating new&lt;br/&gt;program analyses and optimizations, profile feedback, run-time techniques&lt;br/&gt;including adaptive garbage collection algorithms, and ways of communicating&lt;br/&gt;high-level predictions and observations of program behavior to the&lt;br/&gt;hardware.&lt;br/&gt;&lt;br/&gt;The project aims to design and build a compiler, run-time system, and&lt;br/&gt;enhanced architectural and operating system features that react quickly and&lt;br/&gt;gracefully to compiler predictions and actual run-time behavior to achieve&lt;br/&gt;high performance. The goal is synergy via cooperation between the system&lt;br/&gt;components versus solving each problem within an individual component.&lt;br/&gt;&lt;br/&gt;While the research focuses on improving memory performance, the project&apos;s&lt;br/&gt;envisioned framework is suited to a wide range of performance optimization&lt;br/&gt;techniques, so the expected research results and software products have&lt;br/&gt;broader impact.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85792</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85792</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n910" target="n911">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Dynamic Cooperative Performance Optimization</data>
      <data key="e_abstract">Object-oriented programming languages, notably Java, are gaining broad use&lt;br/&gt;because of their benefits, which come largely from their flexibility. But&lt;br/&gt;this same flexibility makes Java programs more difficult to optimize in&lt;br/&gt;advance. One sample collection of programs, optimized in advance, spent&lt;br/&gt;40-95% of their time on an aggressive processor waiting for the memory to&lt;br/&gt;provide data, illustrating the need to improve memory behavior. Future&lt;br/&gt;processors will only make the problem worse.&lt;br/&gt;&lt;br/&gt;The project makes an integrated attack on this problem, incorporating new&lt;br/&gt;program analyses and optimizations, profile feedback, run-time techniques&lt;br/&gt;including adaptive garbage collection algorithms, and ways of communicating&lt;br/&gt;high-level predictions and observations of program behavior to the&lt;br/&gt;hardware.&lt;br/&gt;&lt;br/&gt;The project aims to design and build a compiler, run-time system, and&lt;br/&gt;enhanced architectural and operating system features that react quickly and&lt;br/&gt;gracefully to compiler predictions and actual run-time behavior to achieve&lt;br/&gt;high performance. The goal is synergy via cooperation between the system&lt;br/&gt;components versus solving each problem within an individual component.&lt;br/&gt;&lt;br/&gt;While the research focuses on improving memory performance, the project&apos;s&lt;br/&gt;envisioned framework is suited to a wide range of performance optimization&lt;br/&gt;techniques, so the expected research results and software products have&lt;br/&gt;broader impact.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85792</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85792</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n914" target="n915">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Acquisition of Immersive WorkWall System for Campus-wide Visualization Research</data>
      <data key="e_abstract">EIA-0079557&lt;br/&gt;Smith, Scott R.&lt;br/&gt;Southern Illinois University Edwardsville&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of Immersive WorkWall System for Campus-wide Visualization Research&lt;br/&gt;&lt;br/&gt;This proposal requests funds to provide Southern Illinois University at Edwardsville with the visualization capabilities of an Immersive WorkWall System. The particular Immersive WorkWall consists of a single screen stereoscopic projection system and a workstation rendering dual graphic images onto a six by eight foot screen. Viewers wearing special glasses experience real depth effects. The new equipment capability will significantly enhance the on-going research activities concerning animated simulation models of scientific and engineering concepts.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79557</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">79557</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n579" target="n918">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Architectures and Algorithms to Exploit Probe-Based Storage</data>
      <data key="e_abstract">The storage density of rotating magnetic recording is approaching its&lt;br/&gt;theoretical maximum. Magnetic probe-based technology avoids these limitations by using techniques such as orthogonal recording which promise very high density storage within the next five to ten years. Probe-based storage devices promise improved access times, enormous potential parallelism gains, and remarkable storage densities. However, because of the unique characteristics of these devices there is a high&lt;br/&gt;probability that existing file system architectures and algorithms will be suboptimal. By reexamining these basic structures in the context of probe-based storage, it is likely that significant performance gains can be achieved.&lt;br/&gt;&lt;br/&gt;The proposed work comprises fundamental research in four areas: simulation of probe-based storage devices, architectural issues such as parallelism and caching, storage allocation and file layout, and request scheduling. In reexamining these basic issues for this new technology, this research creates a body of work that will lead the way in the development of secondary storage systems for such devices. This research is likely to result in a better understanding of the implementation details associated with probe-based storage devices to provide a set of algorithms and structures that can be used in systems implementations employing them.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">73509</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">73509</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n579" target="n919">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Architectures and Algorithms to Exploit Probe-Based Storage</data>
      <data key="e_abstract">The storage density of rotating magnetic recording is approaching its&lt;br/&gt;theoretical maximum. Magnetic probe-based technology avoids these limitations by using techniques such as orthogonal recording which promise very high density storage within the next five to ten years. Probe-based storage devices promise improved access times, enormous potential parallelism gains, and remarkable storage densities. However, because of the unique characteristics of these devices there is a high&lt;br/&gt;probability that existing file system architectures and algorithms will be suboptimal. By reexamining these basic structures in the context of probe-based storage, it is likely that significant performance gains can be achieved.&lt;br/&gt;&lt;br/&gt;The proposed work comprises fundamental research in four areas: simulation of probe-based storage devices, architectural issues such as parallelism and caching, storage allocation and file layout, and request scheduling. In reexamining these basic issues for this new technology, this research creates a body of work that will lead the way in the development of secondary storage systems for such devices. This research is likely to result in a better understanding of the implementation details associated with probe-based storage devices to provide a set of algorithms and structures that can be used in systems implementations employing them.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">73509</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">73509</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n918" target="n919">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Architectures and Algorithms to Exploit Probe-Based Storage</data>
      <data key="e_abstract">The storage density of rotating magnetic recording is approaching its&lt;br/&gt;theoretical maximum. Magnetic probe-based technology avoids these limitations by using techniques such as orthogonal recording which promise very high density storage within the next five to ten years. Probe-based storage devices promise improved access times, enormous potential parallelism gains, and remarkable storage densities. However, because of the unique characteristics of these devices there is a high&lt;br/&gt;probability that existing file system architectures and algorithms will be suboptimal. By reexamining these basic structures in the context of probe-based storage, it is likely that significant performance gains can be achieved.&lt;br/&gt;&lt;br/&gt;The proposed work comprises fundamental research in four areas: simulation of probe-based storage devices, architectural issues such as parallelism and caching, storage allocation and file layout, and request scheduling. In reexamining these basic issues for this new technology, this research creates a body of work that will lead the way in the development of secondary storage systems for such devices. This research is likely to result in a better understanding of the implementation details associated with probe-based storage devices to provide a set of algorithms and structures that can be used in systems implementations employing them.</data>
      <data key="e_pgm">4715</data>
      <data key="e_label">73509</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">73509</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n923" target="n924">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research in Novel Dynamic Scheduling Methods with Application to Computation of Quantum Trajectories</data>
      <data key="e_abstract">This project will discover novel scheduling techniques for large scale applications on parallel computers, and will develop new parallel algorithms using those scheduling techniques to calculate quantum trajectories for electron scattering problems. Such research is important to help enable theoretical models as close as possible to real events, so that simulations of physical phenomenal derive accurate predictions. Since many application problems in science and engineering are irregular, large and computationally intensive, finding their best solution in terms of numerical properties and parallel performance represents an important contribution to the development of advanced computational science. &lt;br/&gt;&lt;br/&gt;Technically, this project will develop new models for dynamic scheduling strategies used in scientific computing based on probabalistic and statistical analysis, and will evaluate their effectiveness on an analytical and experimental basis. The project specifcally addresses the following general issues: (1) to develop novel dynamic scheduling strategies that can accommodate applications with unpredictable behavior in load distribution, and evaluate their competitiveness with respect to existing technology; (2) to develop new parallel numerical algorithms using those strategies for the study of scattering from an Eckart potential barrier in one dimension and electron scattering in three dimensions from the ground state hydrogen atom and one, two, three and four electrons bound to a hydrogen-like one and two dimensional multicharged ion; (3) to analyze the performance of this parallel application via new and predictive performance metrics. In addition, visualization methods will be applied to the calculation of quantum trajectories, leading to the possible identification of visual and quantified signatures of &quot;quantum chaos.&quot; This is chaos in quantum mechanical systems defined in terms of the behavior of quantum trajectories.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81303</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81303</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n630" target="n923">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research in Novel Dynamic Scheduling Methods with Application to Computation of Quantum Trajectories</data>
      <data key="e_abstract">This project will discover novel scheduling techniques for large scale applications on parallel computers, and will develop new parallel algorithms using those scheduling techniques to calculate quantum trajectories for electron scattering problems. Such research is important to help enable theoretical models as close as possible to real events, so that simulations of physical phenomenal derive accurate predictions. Since many application problems in science and engineering are irregular, large and computationally intensive, finding their best solution in terms of numerical properties and parallel performance represents an important contribution to the development of advanced computational science. &lt;br/&gt;&lt;br/&gt;Technically, this project will develop new models for dynamic scheduling strategies used in scientific computing based on probabalistic and statistical analysis, and will evaluate their effectiveness on an analytical and experimental basis. The project specifcally addresses the following general issues: (1) to develop novel dynamic scheduling strategies that can accommodate applications with unpredictable behavior in load distribution, and evaluate their competitiveness with respect to existing technology; (2) to develop new parallel numerical algorithms using those strategies for the study of scattering from an Eckart potential barrier in one dimension and electron scattering in three dimensions from the ground state hydrogen atom and one, two, three and four electrons bound to a hydrogen-like one and two dimensional multicharged ion; (3) to analyze the performance of this parallel application via new and predictive performance metrics. In addition, visualization methods will be applied to the calculation of quantum trajectories, leading to the possible identification of visual and quantified signatures of &quot;quantum chaos.&quot; This is chaos in quantum mechanical systems defined in terms of the behavior of quantum trajectories.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81303</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81303</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n630" target="n924">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research in Novel Dynamic Scheduling Methods with Application to Computation of Quantum Trajectories</data>
      <data key="e_abstract">This project will discover novel scheduling techniques for large scale applications on parallel computers, and will develop new parallel algorithms using those scheduling techniques to calculate quantum trajectories for electron scattering problems. Such research is important to help enable theoretical models as close as possible to real events, so that simulations of physical phenomenal derive accurate predictions. Since many application problems in science and engineering are irregular, large and computationally intensive, finding their best solution in terms of numerical properties and parallel performance represents an important contribution to the development of advanced computational science. &lt;br/&gt;&lt;br/&gt;Technically, this project will develop new models for dynamic scheduling strategies used in scientific computing based on probabalistic and statistical analysis, and will evaluate their effectiveness on an analytical and experimental basis. The project specifcally addresses the following general issues: (1) to develop novel dynamic scheduling strategies that can accommodate applications with unpredictable behavior in load distribution, and evaluate their competitiveness with respect to existing technology; (2) to develop new parallel numerical algorithms using those strategies for the study of scattering from an Eckart potential barrier in one dimension and electron scattering in three dimensions from the ground state hydrogen atom and one, two, three and four electrons bound to a hydrogen-like one and two dimensional multicharged ion; (3) to analyze the performance of this parallel application via new and predictive performance metrics. In addition, visualization methods will be applied to the calculation of quantum trajectories, leading to the possible identification of visual and quantified signatures of &quot;quantum chaos.&quot; This is chaos in quantum mechanical systems defined in terms of the behavior of quantum trajectories.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81303</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81303</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n926" target="n927">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Educating a Wireless Information Systems Workforce</data>
      <data key="e_abstract">EIA-0081327&lt;br/&gt;Krishnamurthy, Prashant&lt;br/&gt;University of Pittsburgh&lt;br/&gt;&lt;br/&gt;ITR: Educating a Wireless Information Systems Workforce&lt;br/&gt;&lt;br/&gt;The primary objective of this project is to develop and implement a&lt;br/&gt;wireless information systems degree track that provides a unique education&lt;br/&gt;in the development, design, and deployment of wireless information systems&lt;br/&gt;with an emphasis on emerging wireless data technology. The goal is to&lt;br/&gt;produce information technology (IT) professionals with the knowledge to&lt;br/&gt;address the special challenges (e.g. user mobility, adverse communications&lt;br/&gt;channels, limited battery life) posed by emerging wireless information&lt;br/&gt;systems. A secondary objective is to develop innovative instructional&lt;br/&gt;methods and tools using wireless devices in the classroom and laboratory&lt;br/&gt;that extend through K-16 education. The research and coursework associated&lt;br/&gt;with this educational track are needed to help meet the explosive demand&lt;br/&gt;for IT professionals from wireless service providers, wireless equipment&lt;br/&gt;manufacturers, applications developers using wireless systems, and wireless&lt;br/&gt;information systems users.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">81327</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81327</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n926" target="n928">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Educating a Wireless Information Systems Workforce</data>
      <data key="e_abstract">EIA-0081327&lt;br/&gt;Krishnamurthy, Prashant&lt;br/&gt;University of Pittsburgh&lt;br/&gt;&lt;br/&gt;ITR: Educating a Wireless Information Systems Workforce&lt;br/&gt;&lt;br/&gt;The primary objective of this project is to develop and implement a&lt;br/&gt;wireless information systems degree track that provides a unique education&lt;br/&gt;in the development, design, and deployment of wireless information systems&lt;br/&gt;with an emphasis on emerging wireless data technology. The goal is to&lt;br/&gt;produce information technology (IT) professionals with the knowledge to&lt;br/&gt;address the special challenges (e.g. user mobility, adverse communications&lt;br/&gt;channels, limited battery life) posed by emerging wireless information&lt;br/&gt;systems. A secondary objective is to develop innovative instructional&lt;br/&gt;methods and tools using wireless devices in the classroom and laboratory&lt;br/&gt;that extend through K-16 education. The research and coursework associated&lt;br/&gt;with this educational track are needed to help meet the explosive demand&lt;br/&gt;for IT professionals from wireless service providers, wireless equipment&lt;br/&gt;manufacturers, applications developers using wireless systems, and wireless&lt;br/&gt;information systems users.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">81327</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81327</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n927" target="n928">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Educating a Wireless Information Systems Workforce</data>
      <data key="e_abstract">EIA-0081327&lt;br/&gt;Krishnamurthy, Prashant&lt;br/&gt;University of Pittsburgh&lt;br/&gt;&lt;br/&gt;ITR: Educating a Wireless Information Systems Workforce&lt;br/&gt;&lt;br/&gt;The primary objective of this project is to develop and implement a&lt;br/&gt;wireless information systems degree track that provides a unique education&lt;br/&gt;in the development, design, and deployment of wireless information systems&lt;br/&gt;with an emphasis on emerging wireless data technology. The goal is to&lt;br/&gt;produce information technology (IT) professionals with the knowledge to&lt;br/&gt;address the special challenges (e.g. user mobility, adverse communications&lt;br/&gt;channels, limited battery life) posed by emerging wireless information&lt;br/&gt;systems. A secondary objective is to develop innovative instructional&lt;br/&gt;methods and tools using wireless devices in the classroom and laboratory&lt;br/&gt;that extend through K-16 education. The research and coursework associated&lt;br/&gt;with this educational track are needed to help meet the explosive demand&lt;br/&gt;for IT professionals from wireless service providers, wireless equipment&lt;br/&gt;manufacturers, applications developers using wireless systems, and wireless&lt;br/&gt;information systems users.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">81327</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81327</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n929" target="n930">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Cluster Based Computational Techniques for the Modelling of Problems Involving Bifurcations</data>
      <data key="e_abstract">This project will address the needs of researchers involved in solving large nonlinear problems, which exhibit multiple solution paths, or large-scale minimization problems with multiple local minima. It will develop a library of visualization and computational steering tools to address path following problems efficiently on Beowulf computer clusters. It will also provide tools for optimization of the cluster communication. It will produce tools that are integrated with the CUMULVS steering and visualization environment. The software libraries will be open source and provided to the community of researchers and users.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81324</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81324</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n929" target="n931">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Cluster Based Computational Techniques for the Modelling of Problems Involving Bifurcations</data>
      <data key="e_abstract">This project will address the needs of researchers involved in solving large nonlinear problems, which exhibit multiple solution paths, or large-scale minimization problems with multiple local minima. It will develop a library of visualization and computational steering tools to address path following problems efficiently on Beowulf computer clusters. It will also provide tools for optimization of the cluster communication. It will produce tools that are integrated with the CUMULVS steering and visualization environment. The software libraries will be open source and provided to the community of researchers and users.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81324</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81324</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n930" target="n931">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Cluster Based Computational Techniques for the Modelling of Problems Involving Bifurcations</data>
      <data key="e_abstract">This project will address the needs of researchers involved in solving large nonlinear problems, which exhibit multiple solution paths, or large-scale minimization problems with multiple local minima. It will develop a library of visualization and computational steering tools to address path following problems efficiently on Beowulf computer clusters. It will also provide tools for optimization of the cluster communication. It will produce tools that are integrated with the CUMULVS steering and visualization environment. The software libraries will be open source and provided to the community of researchers and users.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81324</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81324</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n505" target="n932">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: The Construction and Analysis of Information Networks</data>
      <data key="e_abstract">Sources of on-line information are becoming increasingly decentralized, heterogeneous, and complex even as they become correspondingly richer and more valuable. Determining the structure of these information sources is becoming key to extracting and managing the knowledge they contain. Some of these sources exhibit an explicit network structure --- the hyperlinks of the World Wide Web form an excellent example. In other domains, ranging from electronic communication to informal human social networks, subtle hidden linkage relations play a large role in determining the information flow within and between communities. The link structures of both types of environments can yield a surprising wealth of latent information about their content, making their complexity manageable.&lt;br/&gt;&lt;br/&gt;The proposed research seeks effective mechanisms for eliciting a global understanding of link structures in information networks. A key component of this effort is the design of techniques and tools enabling a richer level of interaction with on-line information. The research focuses on the development and integration of new techniques in three areas: natural language understanding methods to uncover implicit relationships in on-line content, efficient algorithms to analyze complex networks of inter-connections, and mathematical models of the dynamics and social processes by which networked information evolves.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81334</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81334</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n932" target="n934">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: The Construction and Analysis of Information Networks</data>
      <data key="e_abstract">Sources of on-line information are becoming increasingly decentralized, heterogeneous, and complex even as they become correspondingly richer and more valuable. Determining the structure of these information sources is becoming key to extracting and managing the knowledge they contain. Some of these sources exhibit an explicit network structure --- the hyperlinks of the World Wide Web form an excellent example. In other domains, ranging from electronic communication to informal human social networks, subtle hidden linkage relations play a large role in determining the information flow within and between communities. The link structures of both types of environments can yield a surprising wealth of latent information about their content, making their complexity manageable.&lt;br/&gt;&lt;br/&gt;The proposed research seeks effective mechanisms for eliciting a global understanding of link structures in information networks. A key component of this effort is the design of techniques and tools enabling a richer level of interaction with on-line information. The research focuses on the development and integration of new techniques in three areas: natural language understanding methods to uncover implicit relationships in on-line content, efficient algorithms to analyze complex networks of inter-connections, and mathematical models of the dynamics and social processes by which networked information evolves.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81334</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81334</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n932" target="n935">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: The Construction and Analysis of Information Networks</data>
      <data key="e_abstract">Sources of on-line information are becoming increasingly decentralized, heterogeneous, and complex even as they become correspondingly richer and more valuable. Determining the structure of these information sources is becoming key to extracting and managing the knowledge they contain. Some of these sources exhibit an explicit network structure --- the hyperlinks of the World Wide Web form an excellent example. In other domains, ranging from electronic communication to informal human social networks, subtle hidden linkage relations play a large role in determining the information flow within and between communities. The link structures of both types of environments can yield a surprising wealth of latent information about their content, making their complexity manageable.&lt;br/&gt;&lt;br/&gt;The proposed research seeks effective mechanisms for eliciting a global understanding of link structures in information networks. A key component of this effort is the design of techniques and tools enabling a richer level of interaction with on-line information. The research focuses on the development and integration of new techniques in three areas: natural language understanding methods to uncover implicit relationships in on-line content, efficient algorithms to analyze complex networks of inter-connections, and mathematical models of the dynamics and social processes by which networked information evolves.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81334</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81334</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n932" target="n936">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: The Construction and Analysis of Information Networks</data>
      <data key="e_abstract">Sources of on-line information are becoming increasingly decentralized, heterogeneous, and complex even as they become correspondingly richer and more valuable. Determining the structure of these information sources is becoming key to extracting and managing the knowledge they contain. Some of these sources exhibit an explicit network structure --- the hyperlinks of the World Wide Web form an excellent example. In other domains, ranging from electronic communication to informal human social networks, subtle hidden linkage relations play a large role in determining the information flow within and between communities. The link structures of both types of environments can yield a surprising wealth of latent information about their content, making their complexity manageable.&lt;br/&gt;&lt;br/&gt;The proposed research seeks effective mechanisms for eliciting a global understanding of link structures in information networks. A key component of this effort is the design of techniques and tools enabling a richer level of interaction with on-line information. The research focuses on the development and integration of new techniques in three areas: natural language understanding methods to uncover implicit relationships in on-line content, efficient algorithms to analyze complex networks of inter-connections, and mathematical models of the dynamics and social processes by which networked information evolves.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81334</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81334</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n505" target="n934">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: The Construction and Analysis of Information Networks</data>
      <data key="e_abstract">Sources of on-line information are becoming increasingly decentralized, heterogeneous, and complex even as they become correspondingly richer and more valuable. Determining the structure of these information sources is becoming key to extracting and managing the knowledge they contain. Some of these sources exhibit an explicit network structure --- the hyperlinks of the World Wide Web form an excellent example. In other domains, ranging from electronic communication to informal human social networks, subtle hidden linkage relations play a large role in determining the information flow within and between communities. The link structures of both types of environments can yield a surprising wealth of latent information about their content, making their complexity manageable.&lt;br/&gt;&lt;br/&gt;The proposed research seeks effective mechanisms for eliciting a global understanding of link structures in information networks. A key component of this effort is the design of techniques and tools enabling a richer level of interaction with on-line information. The research focuses on the development and integration of new techniques in three areas: natural language understanding methods to uncover implicit relationships in on-line content, efficient algorithms to analyze complex networks of inter-connections, and mathematical models of the dynamics and social processes by which networked information evolves.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81334</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81334</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n505" target="n935">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: The Construction and Analysis of Information Networks</data>
      <data key="e_abstract">Sources of on-line information are becoming increasingly decentralized, heterogeneous, and complex even as they become correspondingly richer and more valuable. Determining the structure of these information sources is becoming key to extracting and managing the knowledge they contain. Some of these sources exhibit an explicit network structure --- the hyperlinks of the World Wide Web form an excellent example. In other domains, ranging from electronic communication to informal human social networks, subtle hidden linkage relations play a large role in determining the information flow within and between communities. The link structures of both types of environments can yield a surprising wealth of latent information about their content, making their complexity manageable.&lt;br/&gt;&lt;br/&gt;The proposed research seeks effective mechanisms for eliciting a global understanding of link structures in information networks. A key component of this effort is the design of techniques and tools enabling a richer level of interaction with on-line information. The research focuses on the development and integration of new techniques in three areas: natural language understanding methods to uncover implicit relationships in on-line content, efficient algorithms to analyze complex networks of inter-connections, and mathematical models of the dynamics and social processes by which networked information evolves.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81334</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81334</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n505" target="n936">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: The Construction and Analysis of Information Networks</data>
      <data key="e_abstract">Sources of on-line information are becoming increasingly decentralized, heterogeneous, and complex even as they become correspondingly richer and more valuable. Determining the structure of these information sources is becoming key to extracting and managing the knowledge they contain. Some of these sources exhibit an explicit network structure --- the hyperlinks of the World Wide Web form an excellent example. In other domains, ranging from electronic communication to informal human social networks, subtle hidden linkage relations play a large role in determining the information flow within and between communities. The link structures of both types of environments can yield a surprising wealth of latent information about their content, making their complexity manageable.&lt;br/&gt;&lt;br/&gt;The proposed research seeks effective mechanisms for eliciting a global understanding of link structures in information networks. A key component of this effort is the design of techniques and tools enabling a richer level of interaction with on-line information. The research focuses on the development and integration of new techniques in three areas: natural language understanding methods to uncover implicit relationships in on-line content, efficient algorithms to analyze complex networks of inter-connections, and mathematical models of the dynamics and social processes by which networked information evolves.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81334</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81334</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n934" target="n935">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: The Construction and Analysis of Information Networks</data>
      <data key="e_abstract">Sources of on-line information are becoming increasingly decentralized, heterogeneous, and complex even as they become correspondingly richer and more valuable. Determining the structure of these information sources is becoming key to extracting and managing the knowledge they contain. Some of these sources exhibit an explicit network structure --- the hyperlinks of the World Wide Web form an excellent example. In other domains, ranging from electronic communication to informal human social networks, subtle hidden linkage relations play a large role in determining the information flow within and between communities. The link structures of both types of environments can yield a surprising wealth of latent information about their content, making their complexity manageable.&lt;br/&gt;&lt;br/&gt;The proposed research seeks effective mechanisms for eliciting a global understanding of link structures in information networks. A key component of this effort is the design of techniques and tools enabling a richer level of interaction with on-line information. The research focuses on the development and integration of new techniques in three areas: natural language understanding methods to uncover implicit relationships in on-line content, efficient algorithms to analyze complex networks of inter-connections, and mathematical models of the dynamics and social processes by which networked information evolves.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81334</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81334</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n934" target="n936">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: The Construction and Analysis of Information Networks</data>
      <data key="e_abstract">Sources of on-line information are becoming increasingly decentralized, heterogeneous, and complex even as they become correspondingly richer and more valuable. Determining the structure of these information sources is becoming key to extracting and managing the knowledge they contain. Some of these sources exhibit an explicit network structure --- the hyperlinks of the World Wide Web form an excellent example. In other domains, ranging from electronic communication to informal human social networks, subtle hidden linkage relations play a large role in determining the information flow within and between communities. The link structures of both types of environments can yield a surprising wealth of latent information about their content, making their complexity manageable.&lt;br/&gt;&lt;br/&gt;The proposed research seeks effective mechanisms for eliciting a global understanding of link structures in information networks. A key component of this effort is the design of techniques and tools enabling a richer level of interaction with on-line information. The research focuses on the development and integration of new techniques in three areas: natural language understanding methods to uncover implicit relationships in on-line content, efficient algorithms to analyze complex networks of inter-connections, and mathematical models of the dynamics and social processes by which networked information evolves.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81334</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81334</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n935" target="n936">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: The Construction and Analysis of Information Networks</data>
      <data key="e_abstract">Sources of on-line information are becoming increasingly decentralized, heterogeneous, and complex even as they become correspondingly richer and more valuable. Determining the structure of these information sources is becoming key to extracting and managing the knowledge they contain. Some of these sources exhibit an explicit network structure --- the hyperlinks of the World Wide Web form an excellent example. In other domains, ranging from electronic communication to informal human social networks, subtle hidden linkage relations play a large role in determining the information flow within and between communities. The link structures of both types of environments can yield a surprising wealth of latent information about their content, making their complexity manageable.&lt;br/&gt;&lt;br/&gt;The proposed research seeks effective mechanisms for eliciting a global understanding of link structures in information networks. A key component of this effort is the design of techniques and tools enabling a richer level of interaction with on-line information. The research focuses on the development and integration of new techniques in three areas: natural language understanding methods to uncover implicit relationships in on-line content, efficient algorithms to analyze complex networks of inter-connections, and mathematical models of the dynamics and social processes by which networked information evolves.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81334</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81334</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n938" target="n939">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Self-Scheduling N-Body Algorithms</data>
      <data key="e_abstract">The n-body problem - i.e. simulating the motion of many particles that attract or repel each other - is a classic one with many applications. N-body algorithms are the computational means of solving this problem. The many users of these algorithms include biophysiologists and biochemists studying biological phenomena, pharmaceutical researchers dealing with drug structures and interactions, astrophysicists studying the structure of the cosmos, and engineering researchers studying hydrodynamics. The algorithms are also interesting in their own right to computer scientists. This project will develop an innovative approach to n-body algorithms called self-scheduling n-body algorithms. This family of algorithms promises not only reduced computational complexity, but also a straightforward implementation.&lt;br/&gt;&lt;br/&gt;Technically, the research will use a multiple time step method where each pairwise interaction is evaluated using a dynamic schedule that attempts to equalize the error of each interaction, drastically reducing the computational cost. The fundamental new idea is to equalize the impulses for all interactions, rather than equalizing the time steps for all interactions (which is too conservative in most cases). Mathematically, the constant time step t is traded for a constant impulse I, defined as Fij tij, where Fij is the force between particles i and j and tij becomes the time step used to re-evaluate Fij. This leads to an expected execution complexity of O(n4/3) per simulation step. Algorithmic improvements that rely on the first and second derivatives of force reduce the per step computational complexity to O(n log n) and O(n), respectively. The project will fully explore these algorithms, analyze their error bounds and computational complexity, implement prototype versions, and explore additional topics (such as efficient parallelization and cache-efficient memory layouts) as time permits.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82931</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82931</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n938" target="n940">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Self-Scheduling N-Body Algorithms</data>
      <data key="e_abstract">The n-body problem - i.e. simulating the motion of many particles that attract or repel each other - is a classic one with many applications. N-body algorithms are the computational means of solving this problem. The many users of these algorithms include biophysiologists and biochemists studying biological phenomena, pharmaceutical researchers dealing with drug structures and interactions, astrophysicists studying the structure of the cosmos, and engineering researchers studying hydrodynamics. The algorithms are also interesting in their own right to computer scientists. This project will develop an innovative approach to n-body algorithms called self-scheduling n-body algorithms. This family of algorithms promises not only reduced computational complexity, but also a straightforward implementation.&lt;br/&gt;&lt;br/&gt;Technically, the research will use a multiple time step method where each pairwise interaction is evaluated using a dynamic schedule that attempts to equalize the error of each interaction, drastically reducing the computational cost. The fundamental new idea is to equalize the impulses for all interactions, rather than equalizing the time steps for all interactions (which is too conservative in most cases). Mathematically, the constant time step t is traded for a constant impulse I, defined as Fij tij, where Fij is the force between particles i and j and tij becomes the time step used to re-evaluate Fij. This leads to an expected execution complexity of O(n4/3) per simulation step. Algorithmic improvements that rely on the first and second derivatives of force reduce the per step computational complexity to O(n log n) and O(n), respectively. The project will fully explore these algorithms, analyze their error bounds and computational complexity, implement prototype versions, and explore additional topics (such as efficient parallelization and cache-efficient memory layouts) as time permits.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82931</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82931</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n938" target="n941">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Self-Scheduling N-Body Algorithms</data>
      <data key="e_abstract">The n-body problem - i.e. simulating the motion of many particles that attract or repel each other - is a classic one with many applications. N-body algorithms are the computational means of solving this problem. The many users of these algorithms include biophysiologists and biochemists studying biological phenomena, pharmaceutical researchers dealing with drug structures and interactions, astrophysicists studying the structure of the cosmos, and engineering researchers studying hydrodynamics. The algorithms are also interesting in their own right to computer scientists. This project will develop an innovative approach to n-body algorithms called self-scheduling n-body algorithms. This family of algorithms promises not only reduced computational complexity, but also a straightforward implementation.&lt;br/&gt;&lt;br/&gt;Technically, the research will use a multiple time step method where each pairwise interaction is evaluated using a dynamic schedule that attempts to equalize the error of each interaction, drastically reducing the computational cost. The fundamental new idea is to equalize the impulses for all interactions, rather than equalizing the time steps for all interactions (which is too conservative in most cases). Mathematically, the constant time step t is traded for a constant impulse I, defined as Fij tij, where Fij is the force between particles i and j and tij becomes the time step used to re-evaluate Fij. This leads to an expected execution complexity of O(n4/3) per simulation step. Algorithmic improvements that rely on the first and second derivatives of force reduce the per step computational complexity to O(n log n) and O(n), respectively. The project will fully explore these algorithms, analyze their error bounds and computational complexity, implement prototype versions, and explore additional topics (such as efficient parallelization and cache-efficient memory layouts) as time permits.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82931</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82931</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n938" target="n942">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Self-Scheduling N-Body Algorithms</data>
      <data key="e_abstract">The n-body problem - i.e. simulating the motion of many particles that attract or repel each other - is a classic one with many applications. N-body algorithms are the computational means of solving this problem. The many users of these algorithms include biophysiologists and biochemists studying biological phenomena, pharmaceutical researchers dealing with drug structures and interactions, astrophysicists studying the structure of the cosmos, and engineering researchers studying hydrodynamics. The algorithms are also interesting in their own right to computer scientists. This project will develop an innovative approach to n-body algorithms called self-scheduling n-body algorithms. This family of algorithms promises not only reduced computational complexity, but also a straightforward implementation.&lt;br/&gt;&lt;br/&gt;Technically, the research will use a multiple time step method where each pairwise interaction is evaluated using a dynamic schedule that attempts to equalize the error of each interaction, drastically reducing the computational cost. The fundamental new idea is to equalize the impulses for all interactions, rather than equalizing the time steps for all interactions (which is too conservative in most cases). Mathematically, the constant time step t is traded for a constant impulse I, defined as Fij tij, where Fij is the force between particles i and j and tij becomes the time step used to re-evaluate Fij. This leads to an expected execution complexity of O(n4/3) per simulation step. Algorithmic improvements that rely on the first and second derivatives of force reduce the per step computational complexity to O(n log n) and O(n), respectively. The project will fully explore these algorithms, analyze their error bounds and computational complexity, implement prototype versions, and explore additional topics (such as efficient parallelization and cache-efficient memory layouts) as time permits.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82931</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82931</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n939" target="n940">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Self-Scheduling N-Body Algorithms</data>
      <data key="e_abstract">The n-body problem - i.e. simulating the motion of many particles that attract or repel each other - is a classic one with many applications. N-body algorithms are the computational means of solving this problem. The many users of these algorithms include biophysiologists and biochemists studying biological phenomena, pharmaceutical researchers dealing with drug structures and interactions, astrophysicists studying the structure of the cosmos, and engineering researchers studying hydrodynamics. The algorithms are also interesting in their own right to computer scientists. This project will develop an innovative approach to n-body algorithms called self-scheduling n-body algorithms. This family of algorithms promises not only reduced computational complexity, but also a straightforward implementation.&lt;br/&gt;&lt;br/&gt;Technically, the research will use a multiple time step method where each pairwise interaction is evaluated using a dynamic schedule that attempts to equalize the error of each interaction, drastically reducing the computational cost. The fundamental new idea is to equalize the impulses for all interactions, rather than equalizing the time steps for all interactions (which is too conservative in most cases). Mathematically, the constant time step t is traded for a constant impulse I, defined as Fij tij, where Fij is the force between particles i and j and tij becomes the time step used to re-evaluate Fij. This leads to an expected execution complexity of O(n4/3) per simulation step. Algorithmic improvements that rely on the first and second derivatives of force reduce the per step computational complexity to O(n log n) and O(n), respectively. The project will fully explore these algorithms, analyze their error bounds and computational complexity, implement prototype versions, and explore additional topics (such as efficient parallelization and cache-efficient memory layouts) as time permits.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82931</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82931</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n939" target="n941">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Self-Scheduling N-Body Algorithms</data>
      <data key="e_abstract">The n-body problem - i.e. simulating the motion of many particles that attract or repel each other - is a classic one with many applications. N-body algorithms are the computational means of solving this problem. The many users of these algorithms include biophysiologists and biochemists studying biological phenomena, pharmaceutical researchers dealing with drug structures and interactions, astrophysicists studying the structure of the cosmos, and engineering researchers studying hydrodynamics. The algorithms are also interesting in their own right to computer scientists. This project will develop an innovative approach to n-body algorithms called self-scheduling n-body algorithms. This family of algorithms promises not only reduced computational complexity, but also a straightforward implementation.&lt;br/&gt;&lt;br/&gt;Technically, the research will use a multiple time step method where each pairwise interaction is evaluated using a dynamic schedule that attempts to equalize the error of each interaction, drastically reducing the computational cost. The fundamental new idea is to equalize the impulses for all interactions, rather than equalizing the time steps for all interactions (which is too conservative in most cases). Mathematically, the constant time step t is traded for a constant impulse I, defined as Fij tij, where Fij is the force between particles i and j and tij becomes the time step used to re-evaluate Fij. This leads to an expected execution complexity of O(n4/3) per simulation step. Algorithmic improvements that rely on the first and second derivatives of force reduce the per step computational complexity to O(n log n) and O(n), respectively. The project will fully explore these algorithms, analyze their error bounds and computational complexity, implement prototype versions, and explore additional topics (such as efficient parallelization and cache-efficient memory layouts) as time permits.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82931</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82931</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n939" target="n942">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Self-Scheduling N-Body Algorithms</data>
      <data key="e_abstract">The n-body problem - i.e. simulating the motion of many particles that attract or repel each other - is a classic one with many applications. N-body algorithms are the computational means of solving this problem. The many users of these algorithms include biophysiologists and biochemists studying biological phenomena, pharmaceutical researchers dealing with drug structures and interactions, astrophysicists studying the structure of the cosmos, and engineering researchers studying hydrodynamics. The algorithms are also interesting in their own right to computer scientists. This project will develop an innovative approach to n-body algorithms called self-scheduling n-body algorithms. This family of algorithms promises not only reduced computational complexity, but also a straightforward implementation.&lt;br/&gt;&lt;br/&gt;Technically, the research will use a multiple time step method where each pairwise interaction is evaluated using a dynamic schedule that attempts to equalize the error of each interaction, drastically reducing the computational cost. The fundamental new idea is to equalize the impulses for all interactions, rather than equalizing the time steps for all interactions (which is too conservative in most cases). Mathematically, the constant time step t is traded for a constant impulse I, defined as Fij tij, where Fij is the force between particles i and j and tij becomes the time step used to re-evaluate Fij. This leads to an expected execution complexity of O(n4/3) per simulation step. Algorithmic improvements that rely on the first and second derivatives of force reduce the per step computational complexity to O(n log n) and O(n), respectively. The project will fully explore these algorithms, analyze their error bounds and computational complexity, implement prototype versions, and explore additional topics (such as efficient parallelization and cache-efficient memory layouts) as time permits.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82931</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82931</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n940" target="n941">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Self-Scheduling N-Body Algorithms</data>
      <data key="e_abstract">The n-body problem - i.e. simulating the motion of many particles that attract or repel each other - is a classic one with many applications. N-body algorithms are the computational means of solving this problem. The many users of these algorithms include biophysiologists and biochemists studying biological phenomena, pharmaceutical researchers dealing with drug structures and interactions, astrophysicists studying the structure of the cosmos, and engineering researchers studying hydrodynamics. The algorithms are also interesting in their own right to computer scientists. This project will develop an innovative approach to n-body algorithms called self-scheduling n-body algorithms. This family of algorithms promises not only reduced computational complexity, but also a straightforward implementation.&lt;br/&gt;&lt;br/&gt;Technically, the research will use a multiple time step method where each pairwise interaction is evaluated using a dynamic schedule that attempts to equalize the error of each interaction, drastically reducing the computational cost. The fundamental new idea is to equalize the impulses for all interactions, rather than equalizing the time steps for all interactions (which is too conservative in most cases). Mathematically, the constant time step t is traded for a constant impulse I, defined as Fij tij, where Fij is the force between particles i and j and tij becomes the time step used to re-evaluate Fij. This leads to an expected execution complexity of O(n4/3) per simulation step. Algorithmic improvements that rely on the first and second derivatives of force reduce the per step computational complexity to O(n log n) and O(n), respectively. The project will fully explore these algorithms, analyze their error bounds and computational complexity, implement prototype versions, and explore additional topics (such as efficient parallelization and cache-efficient memory layouts) as time permits.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82931</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82931</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n940" target="n942">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Self-Scheduling N-Body Algorithms</data>
      <data key="e_abstract">The n-body problem - i.e. simulating the motion of many particles that attract or repel each other - is a classic one with many applications. N-body algorithms are the computational means of solving this problem. The many users of these algorithms include biophysiologists and biochemists studying biological phenomena, pharmaceutical researchers dealing with drug structures and interactions, astrophysicists studying the structure of the cosmos, and engineering researchers studying hydrodynamics. The algorithms are also interesting in their own right to computer scientists. This project will develop an innovative approach to n-body algorithms called self-scheduling n-body algorithms. This family of algorithms promises not only reduced computational complexity, but also a straightforward implementation.&lt;br/&gt;&lt;br/&gt;Technically, the research will use a multiple time step method where each pairwise interaction is evaluated using a dynamic schedule that attempts to equalize the error of each interaction, drastically reducing the computational cost. The fundamental new idea is to equalize the impulses for all interactions, rather than equalizing the time steps for all interactions (which is too conservative in most cases). Mathematically, the constant time step t is traded for a constant impulse I, defined as Fij tij, where Fij is the force between particles i and j and tij becomes the time step used to re-evaluate Fij. This leads to an expected execution complexity of O(n4/3) per simulation step. Algorithmic improvements that rely on the first and second derivatives of force reduce the per step computational complexity to O(n log n) and O(n), respectively. The project will fully explore these algorithms, analyze their error bounds and computational complexity, implement prototype versions, and explore additional topics (such as efficient parallelization and cache-efficient memory layouts) as time permits.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82931</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82931</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n941" target="n942">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Self-Scheduling N-Body Algorithms</data>
      <data key="e_abstract">The n-body problem - i.e. simulating the motion of many particles that attract or repel each other - is a classic one with many applications. N-body algorithms are the computational means of solving this problem. The many users of these algorithms include biophysiologists and biochemists studying biological phenomena, pharmaceutical researchers dealing with drug structures and interactions, astrophysicists studying the structure of the cosmos, and engineering researchers studying hydrodynamics. The algorithms are also interesting in their own right to computer scientists. This project will develop an innovative approach to n-body algorithms called self-scheduling n-body algorithms. This family of algorithms promises not only reduced computational complexity, but also a straightforward implementation.&lt;br/&gt;&lt;br/&gt;Technically, the research will use a multiple time step method where each pairwise interaction is evaluated using a dynamic schedule that attempts to equalize the error of each interaction, drastically reducing the computational cost. The fundamental new idea is to equalize the impulses for all interactions, rather than equalizing the time steps for all interactions (which is too conservative in most cases). Mathematically, the constant time step t is traded for a constant impulse I, defined as Fij tij, where Fij is the force between particles i and j and tij becomes the time step used to re-evaluate Fij. This leads to an expected execution complexity of O(n4/3) per simulation step. Algorithmic improvements that rely on the first and second derivatives of force reduce the per step computational complexity to O(n log n) and O(n), respectively. The project will fully explore these algorithms, analyze their error bounds and computational complexity, implement prototype versions, and explore additional topics (such as efficient parallelization and cache-efficient memory layouts) as time permits.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82931</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82931</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n943" target="n944">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Mining Text for General World Knowledge</data>
      <data key="e_abstract">Despite significant advances in recent years in speech recognition generation technology and statistical language modeling, existing natural language systems are still limited to very specific, narrow domains, and totally lack common sense - the ability to &quot;see the obvious&quot; when interacting with a user. A major reason for this is the lack of a broad base of general world knowledge in current AI systems - knowledge such as that a sandwich is food (for. humans), while dinnerware is not; that dwellings usually have doors and walls; or, that when one person is killed by another, it is often with a gun; etc. This project will use previous work on mining linguistic knowledge from text as a springboard for tackling the problem of mining general world knowledge from texts. The methodology depends neither on &quot;deep&quot; text understanding nor on explicit occurrence of the desired general facts in the targeted corpora. Rather, the PI&apos;s approach elaborates on the idea that regularities observed in patterns of predication in texts generally reflect regularities in the world, particularly regularities in the way certain types of entities jointly participate in various events and relationships. While absolute statistical frequencies of such patterns can be severely misleading (people do not commit crimes, or have accidents or hold public office nearly as often as scanning of newspapers might suggest), the techniques that will be employed rely on conditional frequencies to obtain factually reliable hypotheses. The knowledge extracted will be cast in a formally interpretable propositional form, lending itself to certain and uncertain inference. This in turn will help &quot;sanitize&quot; the extracted knowledge, by revealing and helping to remedy apparent contradictions. Suitable corpora for this work include not only newspapers and other factual sources, but also realistic novels and writings for children - in fact, almost all electronically accessible texts are potentially useful, and no annotation will be required. While not all kinds of common-sense knowledge can be acquired in this way, the knowledge that can be acquired is very extensive, is essential to language understanding and common-sense reasoning, and is relatively close at hand. The kind of general knowledge to be mined from text corpora is not only useful, but essential in the long run for intelligent systems with some general linguistic competence and a modicum of common sense. Thus the work will bring a step closer the prospect of computers that genuinely understand their users.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82928</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82928</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n943" target="n945">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Mining Text for General World Knowledge</data>
      <data key="e_abstract">Despite significant advances in recent years in speech recognition generation technology and statistical language modeling, existing natural language systems are still limited to very specific, narrow domains, and totally lack common sense - the ability to &quot;see the obvious&quot; when interacting with a user. A major reason for this is the lack of a broad base of general world knowledge in current AI systems - knowledge such as that a sandwich is food (for. humans), while dinnerware is not; that dwellings usually have doors and walls; or, that when one person is killed by another, it is often with a gun; etc. This project will use previous work on mining linguistic knowledge from text as a springboard for tackling the problem of mining general world knowledge from texts. The methodology depends neither on &quot;deep&quot; text understanding nor on explicit occurrence of the desired general facts in the targeted corpora. Rather, the PI&apos;s approach elaborates on the idea that regularities observed in patterns of predication in texts generally reflect regularities in the world, particularly regularities in the way certain types of entities jointly participate in various events and relationships. While absolute statistical frequencies of such patterns can be severely misleading (people do not commit crimes, or have accidents or hold public office nearly as often as scanning of newspapers might suggest), the techniques that will be employed rely on conditional frequencies to obtain factually reliable hypotheses. The knowledge extracted will be cast in a formally interpretable propositional form, lending itself to certain and uncertain inference. This in turn will help &quot;sanitize&quot; the extracted knowledge, by revealing and helping to remedy apparent contradictions. Suitable corpora for this work include not only newspapers and other factual sources, but also realistic novels and writings for children - in fact, almost all electronically accessible texts are potentially useful, and no annotation will be required. While not all kinds of common-sense knowledge can be acquired in this way, the knowledge that can be acquired is very extensive, is essential to language understanding and common-sense reasoning, and is relatively close at hand. The kind of general knowledge to be mined from text corpora is not only useful, but essential in the long run for intelligent systems with some general linguistic competence and a modicum of common sense. Thus the work will bring a step closer the prospect of computers that genuinely understand their users.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82928</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82928</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n944" target="n945">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Mining Text for General World Knowledge</data>
      <data key="e_abstract">Despite significant advances in recent years in speech recognition generation technology and statistical language modeling, existing natural language systems are still limited to very specific, narrow domains, and totally lack common sense - the ability to &quot;see the obvious&quot; when interacting with a user. A major reason for this is the lack of a broad base of general world knowledge in current AI systems - knowledge such as that a sandwich is food (for. humans), while dinnerware is not; that dwellings usually have doors and walls; or, that when one person is killed by another, it is often with a gun; etc. This project will use previous work on mining linguistic knowledge from text as a springboard for tackling the problem of mining general world knowledge from texts. The methodology depends neither on &quot;deep&quot; text understanding nor on explicit occurrence of the desired general facts in the targeted corpora. Rather, the PI&apos;s approach elaborates on the idea that regularities observed in patterns of predication in texts generally reflect regularities in the world, particularly regularities in the way certain types of entities jointly participate in various events and relationships. While absolute statistical frequencies of such patterns can be severely misleading (people do not commit crimes, or have accidents or hold public office nearly as often as scanning of newspapers might suggest), the techniques that will be employed rely on conditional frequencies to obtain factually reliable hypotheses. The knowledge extracted will be cast in a formally interpretable propositional form, lending itself to certain and uncertain inference. This in turn will help &quot;sanitize&quot; the extracted knowledge, by revealing and helping to remedy apparent contradictions. Suitable corpora for this work include not only newspapers and other factual sources, but also realistic novels and writings for children - in fact, almost all electronically accessible texts are potentially useful, and no annotation will be required. While not all kinds of common-sense knowledge can be acquired in this way, the knowledge that can be acquired is very extensive, is essential to language understanding and common-sense reasoning, and is relatively close at hand. The kind of general knowledge to be mined from text corpora is not only useful, but essential in the long run for intelligent systems with some general linguistic competence and a modicum of common sense. Thus the work will bring a step closer the prospect of computers that genuinely understand their users.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82928</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82928</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n281" target="n946">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: System Support for Automatic and Consistent Replication of Internet Services</data>
      <data key="e_abstract">This project explores two complementary techniques for addressing&lt;br/&gt;fundamental limitations in replicating network services. The first aspect&lt;br/&gt;of this proposal seeks to automatically replicate service programs and&lt;br/&gt;state information to allow transparent caching or replication of dynamic&lt;br/&gt;services. The goal of the research is to allow transparent caching or&lt;br/&gt;replication of dynamic services, a key step toward automatically converting&lt;br/&gt;unscalable service implementations into scalable ones. The second thrust of&lt;br/&gt;this work is to allow network services to dynamically trade replica&lt;br/&gt;consistency for increased system availability and performance. The TACT&lt;br/&gt;(Tunable Availability and Consistency Tradeoffs) toolkit allows Internet&lt;br/&gt;services to flexibly and dynamically choose their own&lt;br/&gt;availability/consistency tradeoffs. We use three consistency metrics,&lt;br/&gt;Numerical Error, Order Error and Staleness to capture application-specific&lt;br/&gt;consistency requirements of Internet services. Applications use these&lt;br/&gt;metrics in addition to application-specific parameters to assign a numeric&lt;br/&gt;value to system consistency, e.g., the percentage of user requests that&lt;br/&gt;must eventually be rolled back because of underlying replica&lt;br/&gt;inconsistency. Finally, TACT allows consistency to be specified on a&lt;br/&gt;per-user, client, and replica basis, enabling differentiated quality of&lt;br/&gt;service.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82912</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82912</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n949" target="n950">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Perturbation Theoretic Approach to the Electron Correlation Cusp Problem</data>
      <data key="e_abstract">The goal of this research is the production of an improved computer code for molecular electronic structure computation that permits the practical computation of the energy of a molecule with an accuracy at least ten times better than that currently possible. The project will apply this code to the computation of energy barriers and potential energy surfaces for elementary gas phase chemical reactions. Once it becomes aailable to the general scientific community, the code will find application to a wide range of other problems in chemistry and materials science. The computed potential energy surfaces will have immediate application for reactive scattering computations. In this way the project contributes significantly to the infrastructure that will eventually enable the computation of chemical rate constants from first principles. These constants are widely applicable. For example, they are essential input for computer models of atmospheric chemistry and combustion.&lt;br/&gt;&lt;br/&gt;The new feature of this method is the use of Rayleigh-Schroedinger perturbation theory to treat the electron correlation cusp problem. Existing highly-developed orbital-based methods are well suited to description of the global behavior of the wave function. However, they are not well suited to the description of the cusp, a secondary feature that eventually limits the accuracy of large scale electronic structure computations. The new method solves the first-order wave function by use of explicitly correlated geminal basis functions.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82899</data>
      <data key="e_expirationDate">2004-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82899</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n949" target="n951">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Perturbation Theoretic Approach to the Electron Correlation Cusp Problem</data>
      <data key="e_abstract">The goal of this research is the production of an improved computer code for molecular electronic structure computation that permits the practical computation of the energy of a molecule with an accuracy at least ten times better than that currently possible. The project will apply this code to the computation of energy barriers and potential energy surfaces for elementary gas phase chemical reactions. Once it becomes aailable to the general scientific community, the code will find application to a wide range of other problems in chemistry and materials science. The computed potential energy surfaces will have immediate application for reactive scattering computations. In this way the project contributes significantly to the infrastructure that will eventually enable the computation of chemical rate constants from first principles. These constants are widely applicable. For example, they are essential input for computer models of atmospheric chemistry and combustion.&lt;br/&gt;&lt;br/&gt;The new feature of this method is the use of Rayleigh-Schroedinger perturbation theory to treat the electron correlation cusp problem. Existing highly-developed orbital-based methods are well suited to description of the global behavior of the wave function. However, they are not well suited to the description of the cusp, a secondary feature that eventually limits the accuracy of large scale electronic structure computations. The new method solves the first-order wave function by use of explicitly correlated geminal basis functions.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82899</data>
      <data key="e_expirationDate">2004-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82899</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n950" target="n951">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Perturbation Theoretic Approach to the Electron Correlation Cusp Problem</data>
      <data key="e_abstract">The goal of this research is the production of an improved computer code for molecular electronic structure computation that permits the practical computation of the energy of a molecule with an accuracy at least ten times better than that currently possible. The project will apply this code to the computation of energy barriers and potential energy surfaces for elementary gas phase chemical reactions. Once it becomes aailable to the general scientific community, the code will find application to a wide range of other problems in chemistry and materials science. The computed potential energy surfaces will have immediate application for reactive scattering computations. In this way the project contributes significantly to the infrastructure that will eventually enable the computation of chemical rate constants from first principles. These constants are widely applicable. For example, they are essential input for computer models of atmospheric chemistry and combustion.&lt;br/&gt;&lt;br/&gt;The new feature of this method is the use of Rayleigh-Schroedinger perturbation theory to treat the electron correlation cusp problem. Existing highly-developed orbital-based methods are well suited to description of the global behavior of the wave function. However, they are not well suited to the description of the cusp, a secondary feature that eventually limits the accuracy of large scale electronic structure computations. The new method solves the first-order wave function by use of explicitly correlated geminal basis functions.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82899</data>
      <data key="e_expirationDate">2004-03-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82899</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n952" target="n953">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: A New Generation Wireless System with Integrated Cellular and Mobile Relaying Technologies</data>
      <data key="e_abstract">With the advent of Internet, it is now anticipated that wireless data services will within a few years surpass the demand for voice services. The explosive demand for wireless data services is forcing the telecom and networking research communities to rethink some of the well-known solutions to the limited capacity problem which is fundamental in such networks.&lt;br/&gt; In this proposal, a new concept for solving the limited capacity problem in current wireless systems is formulated, and a cost-effective approach to implement the concept is described. The researchers main concept is to circumvent congestions in cellular networks by &quot;hopping&quot; to frequencies in the unlicensed band, such as the Industrial, Scientific, and Medical Band (ISM) band (2.4 - 2.5 GHZ) and by diverting traffic to noncongested areas. The researchers propose to integrate cellular and mobile relaying technologies by deploying new devices called mobile relaying stations (MRS&apos;s). With MRS&apos;s, a mobile host can communicate with the base transceiver stations (BTS&apos;s) located in a different cell by switching (or &quot;hopping&quot;) to the unlicensed frequency band. In this way, a mobile host moving into a congested cell can continue its call that would otherwise be dropped; likewise, a new call that would normally be blocked in a congested cell could be served by borrowing channels from a nearby cell via relaying. As a result, handoff-dropping probability and new call blocking probability of existing cellular systems can be substantially improved. This, in turn, leads to an unprecedented increase in the number of users (i.e., capacity) of these systems if one utilizes the proposed new concept in conjunction with cell splitting. While the hardware complexity of the proposed next-generation wireless systems will be slightly higher than that of the conventional systems, the anticipated benefits in terms of improved network performance, capacity, and connectivity at all times will far outweigh this slight increase in hardware complexity.&lt;br/&gt; The success of this project could lead to a major paradigm shift in the next-generation wireless network standards. It is also anticipated that this project will enhance the telecommunications and networking education and research programs at the State University of New York at Buffalo, and could pave the way for a Center for Advanced Telecommunications and Networking that houses faculty and graduate students from the Department of Electrical Engineering, Department of Computer Science and Engineering, as well as other departments.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82916</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82916</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n954" target="n955">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Understanding and Surviving Computation in the Wild</data>
      <data key="e_abstract">The marriage of the personal computer and the Internet has led to explosive growth in the contacts between separately administered computing resources, creating new opportunities and risks. Applets, agents, viruses, email attachments, and downloadable software are escaping the confines of their original systems and spreading through communications networks. Computers are disabled by network-borne infections; browsers crash due to unforeseen interactions between an applet and a language implementation; application programs are broken by operating system upgrades. This computation in the wild is a far cry from the carefully isolated, controlled, and managed computer systems of the past. The connections between computer systems and living systems are superficially obvious and yet deep in their implications. This proposal argues that networked computer systems can be better understood, controlled, and developed when viewed from the perspective of living systems, treating the rich and dynamic network computer environment formed by diverse benign and malicious software collectively as a software ecosystem. &lt;br/&gt;&lt;br/&gt;Taking seriously the analogy between computer systems and living systems encourages rethinking many aspects of current computing practice ranging from operating system design to communications mechanisms to computer security. This project is aimed at developing design strategies from biology, constructing software that can survive in the wild, and developing a better understanding of the current and emerging software ecosystems. Like many researchers, we believe that the current crisis of software development is unlikely to improve significantly through incremental research using traditional methods. New approaches must be tried, and new ways of thinking about computing must be developed. Biological principles stand to unify many scattered current research efforts addressing robust operation, survivability and security, while also suggesting new avenues for research. In addition, as the size and scope of software systems continues to grow, and global computer networks continue to expand, tools and methodologies from biological research will be increasingly relevant for understanding and monitoring the results.&lt;br/&gt;&lt;br/&gt;The proposal takes the investigators&apos; ideas and insights about living systems online, emphasizing concrete implementations that demonstrate new approaches to solving real problems. Specifically, the following projects centered on and exploring computation in the wild are proposed: Homeostasis for improved operating system survivability, Beyond network intrusion detection, Toward a living networked operating system, Software diversity for species survivability, and Tools and techniques from biology. The homeostasis project will augment a computer operating system with mechanisms similar in spirit to those of biological systems, which maintain homeostasis, by coupling system-call based process monitoring to feedback mechanisms. The second project will generalized and extend an existing prototype TCP-based intrustion-detection systems based on immunological principles. The third project differs from the first two by relaxing the constraint of staying within the bounds of operating system architectures and technology. Rather, it will pursue promising directions for robust operation and computer security based on living systems principles, even where they involve possibly significant levels of incompatibility with existing software and designs. The software diversity project will investigate methods by which software can be made more diverse, and the fifth project will apply quantitative methods from biology to the study of the current software ecosystem.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">9.98656e+06</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98656e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n956" target="n957">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Customized Interfaces for Assistive Technology</data>
      <data key="e_abstract">This project seeks to develop a novel class of computer interfaces centered on a vision-based interaction paradigm, and human augmentation using a range of panoramic sensors and intelligent controllers to provide assistive technology to disabled users. The goal of such interfaces is to enable people with physical disabilities such as impaired limbs, paralysis, or tremors to overcome difficulties associated with accessing computers and products with embedded computers such as wheelchairs, household and office electronic equipment, and robotic aids with traditional input devices. The goal is to create the framework, architecture, scientific algorithms, and augmentative hardware and software to facilitate (a) interaction; (b) control and tasking; and (c) programming of computers and computer-controlled smart devices. There are two main sets of research problems that need to be solved: (a) the development of novel, flexible, portable, adaptable interfaces that allow users with physical disabilities to interact with computers and computer controlled devices by touching and feeling; and (b) human augmentation via a combination of inexpensive sensors and controllers, along with a set of algorithms and software for computer mediated control. This research will result in the next generation of interfaces for users to interact with computers and robot assistants, and more generally, devices with embedded controllers. Although the immediate goal is to develop the basic framework, methods, and algorithms using the smart wheelchair as a test product, the basic ideas will be applicable to a wider range of products.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">83240</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83240</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n956" target="n958">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Customized Interfaces for Assistive Technology</data>
      <data key="e_abstract">This project seeks to develop a novel class of computer interfaces centered on a vision-based interaction paradigm, and human augmentation using a range of panoramic sensors and intelligent controllers to provide assistive technology to disabled users. The goal of such interfaces is to enable people with physical disabilities such as impaired limbs, paralysis, or tremors to overcome difficulties associated with accessing computers and products with embedded computers such as wheelchairs, household and office electronic equipment, and robotic aids with traditional input devices. The goal is to create the framework, architecture, scientific algorithms, and augmentative hardware and software to facilitate (a) interaction; (b) control and tasking; and (c) programming of computers and computer-controlled smart devices. There are two main sets of research problems that need to be solved: (a) the development of novel, flexible, portable, adaptable interfaces that allow users with physical disabilities to interact with computers and computer controlled devices by touching and feeling; and (b) human augmentation via a combination of inexpensive sensors and controllers, along with a set of algorithms and software for computer mediated control. This research will result in the next generation of interfaces for users to interact with computers and robot assistants, and more generally, devices with embedded controllers. Although the immediate goal is to develop the basic framework, methods, and algorithms using the smart wheelchair as a test product, the basic ideas will be applicable to a wider range of products.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">83240</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83240</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n957" target="n958">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Customized Interfaces for Assistive Technology</data>
      <data key="e_abstract">This project seeks to develop a novel class of computer interfaces centered on a vision-based interaction paradigm, and human augmentation using a range of panoramic sensors and intelligent controllers to provide assistive technology to disabled users. The goal of such interfaces is to enable people with physical disabilities such as impaired limbs, paralysis, or tremors to overcome difficulties associated with accessing computers and products with embedded computers such as wheelchairs, household and office electronic equipment, and robotic aids with traditional input devices. The goal is to create the framework, architecture, scientific algorithms, and augmentative hardware and software to facilitate (a) interaction; (b) control and tasking; and (c) programming of computers and computer-controlled smart devices. There are two main sets of research problems that need to be solved: (a) the development of novel, flexible, portable, adaptable interfaces that allow users with physical disabilities to interact with computers and computer controlled devices by touching and feeling; and (b) human augmentation via a combination of inexpensive sensors and controllers, along with a set of algorithms and software for computer mediated control. This research will result in the next generation of interfaces for users to interact with computers and robot assistants, and more generally, devices with embedded controllers. Although the immediate goal is to develop the basic framework, methods, and algorithms using the smart wheelchair as a test product, the basic ideas will be applicable to a wider range of products.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">83240</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83240</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n472" target="n961">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Advanced Algorithms for Spatial-Temporal Interactions in Distributed GIS</data>
      <data key="e_abstract">This research focuses on the analysis of spatial interactions in distributed GIS environments. Data related to systems of spatial interactions includes spatial flows and locational and attribute data pertaining to origins and destinations. Given that these datasets are distributed across multiple nodes of a computer network, this research aims to: 1) study mechanisms of data partitioning and develop a metadata structure that describes the partition; 2) develop decomposable algorithms for gravity modeling that minimize communication cost; and 3) develop efficient algorithms for learning and classifying flow patterns using distributed data sources. The approach is to let the databases reside at their native sites. The algorithms dynamically decompose themselves into partial computations that are executed at individual database sites, and local results are composed to obtain the same global results that would have been obtained if the databases had been moved to a common site. The algorithms can find the decompositions to match the distribution of data across the network. This research will have significant impact on many problems that need to process distributed data. For example, it can enable GIS systems to analyze spatial-temporal datasets distributed over the Internet without having to move the databases to a common site.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81434</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81434</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n964" target="n965">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: The Cervantes Project--Advances from Computer Science Research to Update and Enhance Traditional Scholarship Practices</data>
      <data key="e_abstract">The project will construct a Virtual Variorum Edition (VVE) of Cervantes&apos; (1547-1616) influential &quot;Don Quixote de la Mancha&quot;. The VVE, based on available-quality microfilmed images of the original editions published during Cervantes&apos; life, will contain the important editions of the work in image and text; annotation of the variances present among the editions to enable comparison; derivative editions, generated as the result of scholarly analysis of the variances and bearing supporting reasoning; and commentary by experts that illuminates elements of texts and of the comparisons among editions. The project&apos;s focus is on the digital representation, interlinking, and dissemination of the many computer-based manifestations of the documents and commentary. It will prototype an end-to-end effort; from front-end acquisition of materials through flexible back-end presentation to readers. The VVE, even in prototype form, will be an important artifact, with potential to become the standard research tool used by Cervantes scholars. It will, for the first time, make the resource of multiple rare editions of the &quot;Quixote&quot; easily available in primary-source form. The project will have implications for Digital Library projects involving the humanities, both as an demonstration of what can be achieved today but also through development and validation of techniques.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81420</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81420</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n967" target="n968">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Computational Environment Infrastructure with Applications to Mid Ocean Ridge Research: The &apos;Virtual Research Vessel&apos; Prototype</data>
      <data key="e_abstract">The East Pacific Rise (EPR) is currently our best-studied section of fast-spreading mid-ocean ridge, yielding a wealth of observational data and results spanning many scientific fields. However, this information has not yet been fully utilized. Most of it exists in noninteractive form (e.g. journal publications) or as incompatible datasets and models. To make the most of this data, scientists will need a wide range of sophisticated programming support to coordinate the use of data, computational tools, and numerical models across distributed networks of computers. This project will develop that support.&lt;br/&gt;&lt;br/&gt;Technically, this project will develop the computational infrastructure needed to support data sharing, tool composition, and model coupling for the use of large scale, interdisciplinary data archives. It will then apply that infrastructure to build a domain-specific environment called the Virtual Research Vessel 1 (VRV-1). The VRV-1 will facilitate the use of data, maps, and models related to the EPR. The project will address fundamental issues of integrated middleware for scientific data management and computational science. In particular, it will support data sharing by merging three technologies: geographic information systems, database management systems, and electronic notebooks. It will support tool composition through an extension of an existing electronic notebook. Finally, it will support model coupling by developing support for exploring model correlations and relationships at a very high, domain-specific level in a fast prototyping environment.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81487</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81487</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n515" target="n967">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Computational Environment Infrastructure with Applications to Mid Ocean Ridge Research: The &apos;Virtual Research Vessel&apos; Prototype</data>
      <data key="e_abstract">The East Pacific Rise (EPR) is currently our best-studied section of fast-spreading mid-ocean ridge, yielding a wealth of observational data and results spanning many scientific fields. However, this information has not yet been fully utilized. Most of it exists in noninteractive form (e.g. journal publications) or as incompatible datasets and models. To make the most of this data, scientists will need a wide range of sophisticated programming support to coordinate the use of data, computational tools, and numerical models across distributed networks of computers. This project will develop that support.&lt;br/&gt;&lt;br/&gt;Technically, this project will develop the computational infrastructure needed to support data sharing, tool composition, and model coupling for the use of large scale, interdisciplinary data archives. It will then apply that infrastructure to build a domain-specific environment called the Virtual Research Vessel 1 (VRV-1). The VRV-1 will facilitate the use of data, maps, and models related to the EPR. The project will address fundamental issues of integrated middleware for scientific data management and computational science. In particular, it will support data sharing by merging three technologies: geographic information systems, database management systems, and electronic notebooks. It will support tool composition through an extension of an existing electronic notebook. Finally, it will support model coupling by developing support for exploring model correlations and relationships at a very high, domain-specific level in a fast prototyping environment.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81487</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81487</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n967" target="n970">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Computational Environment Infrastructure with Applications to Mid Ocean Ridge Research: The &apos;Virtual Research Vessel&apos; Prototype</data>
      <data key="e_abstract">The East Pacific Rise (EPR) is currently our best-studied section of fast-spreading mid-ocean ridge, yielding a wealth of observational data and results spanning many scientific fields. However, this information has not yet been fully utilized. Most of it exists in noninteractive form (e.g. journal publications) or as incompatible datasets and models. To make the most of this data, scientists will need a wide range of sophisticated programming support to coordinate the use of data, computational tools, and numerical models across distributed networks of computers. This project will develop that support.&lt;br/&gt;&lt;br/&gt;Technically, this project will develop the computational infrastructure needed to support data sharing, tool composition, and model coupling for the use of large scale, interdisciplinary data archives. It will then apply that infrastructure to build a domain-specific environment called the Virtual Research Vessel 1 (VRV-1). The VRV-1 will facilitate the use of data, maps, and models related to the EPR. The project will address fundamental issues of integrated middleware for scientific data management and computational science. In particular, it will support data sharing by merging three technologies: geographic information systems, database management systems, and electronic notebooks. It will support tool composition through an extension of an existing electronic notebook. Finally, it will support model coupling by developing support for exploring model correlations and relationships at a very high, domain-specific level in a fast prototyping environment.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81487</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81487</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n515" target="n968">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Computational Environment Infrastructure with Applications to Mid Ocean Ridge Research: The &apos;Virtual Research Vessel&apos; Prototype</data>
      <data key="e_abstract">The East Pacific Rise (EPR) is currently our best-studied section of fast-spreading mid-ocean ridge, yielding a wealth of observational data and results spanning many scientific fields. However, this information has not yet been fully utilized. Most of it exists in noninteractive form (e.g. journal publications) or as incompatible datasets and models. To make the most of this data, scientists will need a wide range of sophisticated programming support to coordinate the use of data, computational tools, and numerical models across distributed networks of computers. This project will develop that support.&lt;br/&gt;&lt;br/&gt;Technically, this project will develop the computational infrastructure needed to support data sharing, tool composition, and model coupling for the use of large scale, interdisciplinary data archives. It will then apply that infrastructure to build a domain-specific environment called the Virtual Research Vessel 1 (VRV-1). The VRV-1 will facilitate the use of data, maps, and models related to the EPR. The project will address fundamental issues of integrated middleware for scientific data management and computational science. In particular, it will support data sharing by merging three technologies: geographic information systems, database management systems, and electronic notebooks. It will support tool composition through an extension of an existing electronic notebook. Finally, it will support model coupling by developing support for exploring model correlations and relationships at a very high, domain-specific level in a fast prototyping environment.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81487</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81487</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n968" target="n970">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Computational Environment Infrastructure with Applications to Mid Ocean Ridge Research: The &apos;Virtual Research Vessel&apos; Prototype</data>
      <data key="e_abstract">The East Pacific Rise (EPR) is currently our best-studied section of fast-spreading mid-ocean ridge, yielding a wealth of observational data and results spanning many scientific fields. However, this information has not yet been fully utilized. Most of it exists in noninteractive form (e.g. journal publications) or as incompatible datasets and models. To make the most of this data, scientists will need a wide range of sophisticated programming support to coordinate the use of data, computational tools, and numerical models across distributed networks of computers. This project will develop that support.&lt;br/&gt;&lt;br/&gt;Technically, this project will develop the computational infrastructure needed to support data sharing, tool composition, and model coupling for the use of large scale, interdisciplinary data archives. It will then apply that infrastructure to build a domain-specific environment called the Virtual Research Vessel 1 (VRV-1). The VRV-1 will facilitate the use of data, maps, and models related to the EPR. The project will address fundamental issues of integrated middleware for scientific data management and computational science. In particular, it will support data sharing by merging three technologies: geographic information systems, database management systems, and electronic notebooks. It will support tool composition through an extension of an existing electronic notebook. Finally, it will support model coupling by developing support for exploring model correlations and relationships at a very high, domain-specific level in a fast prototyping environment.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81487</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81487</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n515" target="n970">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Computational Environment Infrastructure with Applications to Mid Ocean Ridge Research: The &apos;Virtual Research Vessel&apos; Prototype</data>
      <data key="e_abstract">The East Pacific Rise (EPR) is currently our best-studied section of fast-spreading mid-ocean ridge, yielding a wealth of observational data and results spanning many scientific fields. However, this information has not yet been fully utilized. Most of it exists in noninteractive form (e.g. journal publications) or as incompatible datasets and models. To make the most of this data, scientists will need a wide range of sophisticated programming support to coordinate the use of data, computational tools, and numerical models across distributed networks of computers. This project will develop that support.&lt;br/&gt;&lt;br/&gt;Technically, this project will develop the computational infrastructure needed to support data sharing, tool composition, and model coupling for the use of large scale, interdisciplinary data archives. It will then apply that infrastructure to build a domain-specific environment called the Virtual Research Vessel 1 (VRV-1). The VRV-1 will facilitate the use of data, maps, and models related to the EPR. The project will address fundamental issues of integrated middleware for scientific data management and computational science. In particular, it will support data sharing by merging three technologies: geographic information systems, database management systems, and electronic notebooks. It will support tool composition through an extension of an existing electronic notebook. Finally, it will support model coupling by developing support for exploring model correlations and relationships at a very high, domain-specific level in a fast prototyping environment.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81487</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81487</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n971" target="n972">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">IT Transfer to Egypt: Process Model for Developing Countries</data>
      <data key="e_abstract">Research on Information Technology Transfer (ITT) to developing countries has suffered due to a lack of knowledge about how the process works. Studies have generally been done from a distance and have assumed an acultural environment in which national IT policy plays an unspecified role. Implementation factors have also been missing from the explanatory models. The goals of this project are to test a theoretical model that explains the ITT process and examine the effectiveness of national IT policy in one developing country -- Egypt. Egyptian users of urban/rural information centers, cybercafes, and ISPs, private and public sector knowledge workers, and national IT policy makers will be sampled. Research methods will include ethnography, interviews, systematic observation, and questionnaires. Drawing from anthropological, economic, and technology innovation and diffusion theories, this research integrates technical and socio-cultural factors within a national setting. Policy makers in developing countries can learn why some policies encourage the process of ITT while others hinder it. Managers in transnational firms charged with introducing IT in foreign subsidiaries, offices, and plants can learn how to better implement IT. This research will result in new knowledge, theories and methods for assessing Information Technology Transfer in other developing countries.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">82473</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82473</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n971" target="n973">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">IT Transfer to Egypt: Process Model for Developing Countries</data>
      <data key="e_abstract">Research on Information Technology Transfer (ITT) to developing countries has suffered due to a lack of knowledge about how the process works. Studies have generally been done from a distance and have assumed an acultural environment in which national IT policy plays an unspecified role. Implementation factors have also been missing from the explanatory models. The goals of this project are to test a theoretical model that explains the ITT process and examine the effectiveness of national IT policy in one developing country -- Egypt. Egyptian users of urban/rural information centers, cybercafes, and ISPs, private and public sector knowledge workers, and national IT policy makers will be sampled. Research methods will include ethnography, interviews, systematic observation, and questionnaires. Drawing from anthropological, economic, and technology innovation and diffusion theories, this research integrates technical and socio-cultural factors within a national setting. Policy makers in developing countries can learn why some policies encourage the process of ITT while others hinder it. Managers in transnational firms charged with introducing IT in foreign subsidiaries, offices, and plants can learn how to better implement IT. This research will result in new knowledge, theories and methods for assessing Information Technology Transfer in other developing countries.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">82473</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82473</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n971" target="n974">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">IT Transfer to Egypt: Process Model for Developing Countries</data>
      <data key="e_abstract">Research on Information Technology Transfer (ITT) to developing countries has suffered due to a lack of knowledge about how the process works. Studies have generally been done from a distance and have assumed an acultural environment in which national IT policy plays an unspecified role. Implementation factors have also been missing from the explanatory models. The goals of this project are to test a theoretical model that explains the ITT process and examine the effectiveness of national IT policy in one developing country -- Egypt. Egyptian users of urban/rural information centers, cybercafes, and ISPs, private and public sector knowledge workers, and national IT policy makers will be sampled. Research methods will include ethnography, interviews, systematic observation, and questionnaires. Drawing from anthropological, economic, and technology innovation and diffusion theories, this research integrates technical and socio-cultural factors within a national setting. Policy makers in developing countries can learn why some policies encourage the process of ITT while others hinder it. Managers in transnational firms charged with introducing IT in foreign subsidiaries, offices, and plants can learn how to better implement IT. This research will result in new knowledge, theories and methods for assessing Information Technology Transfer in other developing countries.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">82473</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82473</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n972" target="n973">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">IT Transfer to Egypt: Process Model for Developing Countries</data>
      <data key="e_abstract">Research on Information Technology Transfer (ITT) to developing countries has suffered due to a lack of knowledge about how the process works. Studies have generally been done from a distance and have assumed an acultural environment in which national IT policy plays an unspecified role. Implementation factors have also been missing from the explanatory models. The goals of this project are to test a theoretical model that explains the ITT process and examine the effectiveness of national IT policy in one developing country -- Egypt. Egyptian users of urban/rural information centers, cybercafes, and ISPs, private and public sector knowledge workers, and national IT policy makers will be sampled. Research methods will include ethnography, interviews, systematic observation, and questionnaires. Drawing from anthropological, economic, and technology innovation and diffusion theories, this research integrates technical and socio-cultural factors within a national setting. Policy makers in developing countries can learn why some policies encourage the process of ITT while others hinder it. Managers in transnational firms charged with introducing IT in foreign subsidiaries, offices, and plants can learn how to better implement IT. This research will result in new knowledge, theories and methods for assessing Information Technology Transfer in other developing countries.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">82473</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82473</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n972" target="n974">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">IT Transfer to Egypt: Process Model for Developing Countries</data>
      <data key="e_abstract">Research on Information Technology Transfer (ITT) to developing countries has suffered due to a lack of knowledge about how the process works. Studies have generally been done from a distance and have assumed an acultural environment in which national IT policy plays an unspecified role. Implementation factors have also been missing from the explanatory models. The goals of this project are to test a theoretical model that explains the ITT process and examine the effectiveness of national IT policy in one developing country -- Egypt. Egyptian users of urban/rural information centers, cybercafes, and ISPs, private and public sector knowledge workers, and national IT policy makers will be sampled. Research methods will include ethnography, interviews, systematic observation, and questionnaires. Drawing from anthropological, economic, and technology innovation and diffusion theories, this research integrates technical and socio-cultural factors within a national setting. Policy makers in developing countries can learn why some policies encourage the process of ITT while others hinder it. Managers in transnational firms charged with introducing IT in foreign subsidiaries, offices, and plants can learn how to better implement IT. This research will result in new knowledge, theories and methods for assessing Information Technology Transfer in other developing countries.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">82473</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82473</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n973" target="n974">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">IT Transfer to Egypt: Process Model for Developing Countries</data>
      <data key="e_abstract">Research on Information Technology Transfer (ITT) to developing countries has suffered due to a lack of knowledge about how the process works. Studies have generally been done from a distance and have assumed an acultural environment in which national IT policy plays an unspecified role. Implementation factors have also been missing from the explanatory models. The goals of this project are to test a theoretical model that explains the ITT process and examine the effectiveness of national IT policy in one developing country -- Egypt. Egyptian users of urban/rural information centers, cybercafes, and ISPs, private and public sector knowledge workers, and national IT policy makers will be sampled. Research methods will include ethnography, interviews, systematic observation, and questionnaires. Drawing from anthropological, economic, and technology innovation and diffusion theories, this research integrates technical and socio-cultural factors within a national setting. Policy makers in developing countries can learn why some policies encourage the process of ITT while others hinder it. Managers in transnational firms charged with introducing IT in foreign subsidiaries, offices, and plants can learn how to better implement IT. This research will result in new knowledge, theories and methods for assessing Information Technology Transfer in other developing countries.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">82473</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82473</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n975" target="n976">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Advanced Application Service Provider Technologies for Large-Scale Optimization</data>
      <data key="e_abstract">Application Service Providers (ASPs) - which distribute the use of software, rather than the software itself, via the Internet - are increasingly coming to be seen as one of the next major elements in the development of network services. This project will study and test the application of the ASP concept in an area where it is likely to have a particularly great influence: software for large-scale optimization. The complexity and variety of optimization software present challenging obstacles to the development of network services, but the exceptional variety and modularity of this software insure that successful efforts will have a broad impact in science, education, and business.&lt;br/&gt;&lt;br/&gt;Technically, the project will build on previous work by the Optimization Technology Center of Northwestern University and Argonne National Laboratory. One subproject will investigate resource allocation for optimization requests whose needs cannot be easily forecast. Others will involve the design and testing of both client-side and server-side object class libraries that have no models in current mathematical computation. The project will also include the automation of algorithm choice and solver scheduling, through the construction and use of a database to learn from results of previous optimization requests and from human experts. The project thus brings together methods, software, and systems for computational large-scale optimization, with the aim of making the impressive technology in this area more accessible for applications, algorithm development, and education.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82807</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82807</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n977" target="n978">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: The Educational Consequences of the Digital Divide</data>
      <data key="e_abstract">This project examines the educational impact of computing and information technologies, both at school and in the home, for middle and high-school students. It investigates a second stage of the Digital Divide, one that goes beyond lack of access to information technology (IT). It examines whether there are different educational benefits to computing when comparing whites to minorities, boys to girls, and more affluent to poorer children. The research combines statistical analyses of survey data with fieldwork observations of children using computers at school, after-school programs, and at home. The goal of the research is to gain insight into the processes by which some children gain more than others in terms of educational benefits from computing. It seeks to document the educational benefits that are occurring, to identify what kinds of students are missing out on these benefits and why, and to identify what kinds of educational applications yield greatest benefits. This research is intended to aid policymakers who are concerned with equity issues in education, educators involved in providing effective information technology in schools, and to inform citizens about the educational consequences of the Digital Divide.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">79976</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">79976</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n981" target="n982">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research: Generating an Accurate Sense of Depth and Size Using Computer Graphics</data>
      <data key="e_abstract">Despite impressive gains in realism over the last decade, computer graphics is currently unable to effectively generate images of objects and environments that look large. This is mostly because computer graphics is poor at conveying information about absolute depth. The goal of this project is to demonstrate that it is possible to significantly improve the sense of depth and scale in computer graphics if rendering methods are developed with specific attention to the need to convey cues for absolute depth. Accomplishing this goal will require new insights into the 3D information extractable from 2D images, modifications to graphics algorithms in order to better render salient information, and sophisticated perceptual experimentation to validate that people can actually see the intended 3D space. The PI&apos;s approach will be to draw upon the results and methods of computational vision in ways that have not previously been done in the computer graphics community. Computational vision provides insights into the intrinsic constraints on how information about 3D space can be recovered from 2D images. In particular, the computational analysis of vision points out the important distinction between relative depth judgments and absolute depth judgments. Surprisingly few of the commonly studied image cues are in fact sufficient to provide information about absolute depth. Of those that do, several cannot be exploited in computer graphics due to fundamental limitations in display technology and our inability to precisely control viewing conditions except in immersive environments. The research will impact a broad range of graphics applications in which accurate spatial information needs to be conveyed, including education and training, design and prototyping, and telepresence.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">80999</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">80999</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n990" target="n991">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research: Development of Head-mounted Projective Display for Distance Collaborative Environments</data>
      <data key="e_abstract">This is the first year funding of a three-year continuing award. A demanding application area of virtual or augmented environment is multi-user collaborative environment where multiple users at either a local site or remote sites communicate with each other and interact with a synthetic or virtual scene. Among the necessary sensors and devices, an effective visualization device and a real-time image acquisition system are two main challenges. The objective of this project is to develop a novel visualization device referred to as head-mounted projective display (HMPD), build a multi-user interactive workbench by integrating the developed HMPD technology with a unique real-time image acquisition system known as an omni-focus camera, and evaluate and quantify the system as an effective tool for remote collaboration. The head-mounted projective display (HMPD) proposed is coupled with a supple, non-distorting and durable projection surface as an alternative to current visualization devices. Its novel concept suggests solutions to part of the problems of state-of-art visualization devices, such as large distortion with wide field of view, occlusion contradiction between virtual and real objects, and brightness conflict with background illumination. Several properties of the proposed HMPD make it extremely suitable for multiple-user collaborative applications. Research efforts will be made to design and implement a lightweight and compact head-mounted prototype by introducing diffractive optical element (DOE) and plastic materials, and investigate approaches to optimize the illumination of the display and retro-reflective material properties for imaging purpose. At one site, a multi-user interactive bench prototype with tele-presence capability will be built by using the HMPD concept and adding an image acquisition system developed from a unique omni-focus camera system. At the other site, a mural display system will be built with conventional stereoscopic video system located near the mural display, where one or several collaborators will also gather. Tele-collaborative work will be tested between the Beckman Institute at the University of Illinois--Urbana Champaign and the School of Optics-CREOL at the University of Central Florida through the Internet II connection linking our laboratories. Finally, the PIs will quantify the depth and size representation and perception accuracy, evaluate occlusion perception aspects, and set up a comprehensive calibration procedure for the HMPD and the workbench and mural prototypes. The results are expected to impact a wide range of applications such as collaboration/tele-collaboration, tele-presence, tele-manipulation, and visualized education/tele-education.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">83037</data>
      <data key="e_expirationDate">2003-05-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83037</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n993" target="n994">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research: &quot;Free&quot; Bits: The Challenge of the Wireless Internet</data>
      <data key="e_abstract">Second and third generation wireless systems have been designed primarily for voice, so they are con-nection oriented, delay sensitive, and provide fixed bit rates. Additionally, since service is desired &quot;any-time/anywhere,&quot; these systems must provide ubiquitous coverage. This coverage is achieved through rela-tively uniform grids of cell sites, which are placed to control interference and minimize outage rather than to maximize throughout. As a result, such systems deliver low bit rates and are relatively expensive when used for large amounts of information. At the same time, wireline connection to the Internet has encouraged uses (and users) that depend upon bits being virtually &quot;free.&quot; We contend that this &quot;economic&quot; mismatch between wired and wireless access is the primary obstacle to the dramatic growth of a wireless Internet. The solution may lie in designing systems specifically for wireless data, recognizing that data services are often connectionless, delay insensitive and have no specific bit rate requirements. These differences&lt;br/&gt;suggest that ubiquitous (anytime/anywhere) coverage is not a strict requirement for wireless data networks and makes possible systems in which small, separated coverage areas facilitate transfers of megabytes of data in fractions of a second, and for a fraction of the cost associated with conventional ubiquitous coverage. Communication theory and simple link budget calculations tell us that it is possible to build such sys-tems, but the signal processing challenges are numerous and distinct from the historical challenges offered by connection-oriented wireless services. When a mobile user passes an Infostation, there will be a window of opportunity, perhaps as short as a fraction of a second, in which the user will have access to a high-rate communication channel. A key task is to identify that window and transmit at an appropriate rate. The mobile must make these decisions based on measurements of a wideband radio channel in which there is frequency selectivity and time variation in the fading as well as in the interference. In the specific context&lt;br/&gt;of an Infostations system, we plan to divide our research into four components: &lt;br/&gt;&lt;br/&gt;Radio Channel Modeling: The characterization of typical Infostation radio channels. &lt;br/&gt;&lt;br/&gt;Transceiver Design: The analysis and performance evaluation of transmitters and receivers for both single carrier and multicarrier systems.&lt;br/&gt;&lt;br/&gt;Radio Resource Management Transmitter power and rate adaptation policies derived from receiver measurements.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Algorithm Development Testbed A platform employing DSP and FPGA technology for the practical evaluation of transmitter and receiver algorithms. &lt;br/&gt;&lt;br/&gt;The activities of this project will encompass three research institutions in New Jersey (New Jersey In-stitute of Technology, Princeton University, Rutgers University) under the auspices of the N.J. Center for Wireless Telecommunications (NJCWT). The NJCWT is an inter-institutional research and educational or-ganization sponsored and funded by the N.J. Commission on Science and Technology. The focus of the center is a multi-year effort in Digital Radio Technology for Computing, Communications and Information Systems. This effort is supportive of and will enhance the present proposed project in wireless networks.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85846</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85846</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n993" target="n995">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research: &quot;Free&quot; Bits: The Challenge of the Wireless Internet</data>
      <data key="e_abstract">Second and third generation wireless systems have been designed primarily for voice, so they are con-nection oriented, delay sensitive, and provide fixed bit rates. Additionally, since service is desired &quot;any-time/anywhere,&quot; these systems must provide ubiquitous coverage. This coverage is achieved through rela-tively uniform grids of cell sites, which are placed to control interference and minimize outage rather than to maximize throughout. As a result, such systems deliver low bit rates and are relatively expensive when used for large amounts of information. At the same time, wireline connection to the Internet has encouraged uses (and users) that depend upon bits being virtually &quot;free.&quot; We contend that this &quot;economic&quot; mismatch between wired and wireless access is the primary obstacle to the dramatic growth of a wireless Internet. The solution may lie in designing systems specifically for wireless data, recognizing that data services are often connectionless, delay insensitive and have no specific bit rate requirements. These differences&lt;br/&gt;suggest that ubiquitous (anytime/anywhere) coverage is not a strict requirement for wireless data networks and makes possible systems in which small, separated coverage areas facilitate transfers of megabytes of data in fractions of a second, and for a fraction of the cost associated with conventional ubiquitous coverage. Communication theory and simple link budget calculations tell us that it is possible to build such sys-tems, but the signal processing challenges are numerous and distinct from the historical challenges offered by connection-oriented wireless services. When a mobile user passes an Infostation, there will be a window of opportunity, perhaps as short as a fraction of a second, in which the user will have access to a high-rate communication channel. A key task is to identify that window and transmit at an appropriate rate. The mobile must make these decisions based on measurements of a wideband radio channel in which there is frequency selectivity and time variation in the fading as well as in the interference. In the specific context&lt;br/&gt;of an Infostations system, we plan to divide our research into four components: &lt;br/&gt;&lt;br/&gt;Radio Channel Modeling: The characterization of typical Infostation radio channels. &lt;br/&gt;&lt;br/&gt;Transceiver Design: The analysis and performance evaluation of transmitters and receivers for both single carrier and multicarrier systems.&lt;br/&gt;&lt;br/&gt;Radio Resource Management Transmitter power and rate adaptation policies derived from receiver measurements.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Algorithm Development Testbed A platform employing DSP and FPGA technology for the practical evaluation of transmitter and receiver algorithms. &lt;br/&gt;&lt;br/&gt;The activities of this project will encompass three research institutions in New Jersey (New Jersey In-stitute of Technology, Princeton University, Rutgers University) under the auspices of the N.J. Center for Wireless Telecommunications (NJCWT). The NJCWT is an inter-institutional research and educational or-ganization sponsored and funded by the N.J. Commission on Science and Technology. The focus of the center is a multi-year effort in Digital Radio Technology for Computing, Communications and Information Systems. This effort is supportive of and will enhance the present proposed project in wireless networks.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85846</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85846</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n994" target="n995">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research: &quot;Free&quot; Bits: The Challenge of the Wireless Internet</data>
      <data key="e_abstract">Second and third generation wireless systems have been designed primarily for voice, so they are con-nection oriented, delay sensitive, and provide fixed bit rates. Additionally, since service is desired &quot;any-time/anywhere,&quot; these systems must provide ubiquitous coverage. This coverage is achieved through rela-tively uniform grids of cell sites, which are placed to control interference and minimize outage rather than to maximize throughout. As a result, such systems deliver low bit rates and are relatively expensive when used for large amounts of information. At the same time, wireline connection to the Internet has encouraged uses (and users) that depend upon bits being virtually &quot;free.&quot; We contend that this &quot;economic&quot; mismatch between wired and wireless access is the primary obstacle to the dramatic growth of a wireless Internet. The solution may lie in designing systems specifically for wireless data, recognizing that data services are often connectionless, delay insensitive and have no specific bit rate requirements. These differences&lt;br/&gt;suggest that ubiquitous (anytime/anywhere) coverage is not a strict requirement for wireless data networks and makes possible systems in which small, separated coverage areas facilitate transfers of megabytes of data in fractions of a second, and for a fraction of the cost associated with conventional ubiquitous coverage. Communication theory and simple link budget calculations tell us that it is possible to build such sys-tems, but the signal processing challenges are numerous and distinct from the historical challenges offered by connection-oriented wireless services. When a mobile user passes an Infostation, there will be a window of opportunity, perhaps as short as a fraction of a second, in which the user will have access to a high-rate communication channel. A key task is to identify that window and transmit at an appropriate rate. The mobile must make these decisions based on measurements of a wideband radio channel in which there is frequency selectivity and time variation in the fading as well as in the interference. In the specific context&lt;br/&gt;of an Infostations system, we plan to divide our research into four components: &lt;br/&gt;&lt;br/&gt;Radio Channel Modeling: The characterization of typical Infostation radio channels. &lt;br/&gt;&lt;br/&gt;Transceiver Design: The analysis and performance evaluation of transmitters and receivers for both single carrier and multicarrier systems.&lt;br/&gt;&lt;br/&gt;Radio Resource Management Transmitter power and rate adaptation policies derived from receiver measurements.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Algorithm Development Testbed A platform employing DSP and FPGA technology for the practical evaluation of transmitter and receiver algorithms. &lt;br/&gt;&lt;br/&gt;The activities of this project will encompass three research institutions in New Jersey (New Jersey In-stitute of Technology, Princeton University, Rutgers University) under the auspices of the N.J. Center for Wireless Telecommunications (NJCWT). The NJCWT is an inter-institutional research and educational or-ganization sponsored and funded by the N.J. Commission on Science and Technology. The focus of the center is a multi-year effort in Digital Radio Technology for Computing, Communications and Information Systems. This effort is supportive of and will enhance the present proposed project in wireless networks.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85846</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85846</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n22" target="n996">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">iMASH: Adaptive Middleware and Networking Support for the Nomadic Healer</data>
      <data key="e_abstract">The researchers envision that within the next few years mobile and wireless access to the Internet will very likely become the norm, rather than the exception as is seen today. This proposal describes the plans to develop and deploy iMASH, a network system that supports anytime, anywhere, on any platform access to the electronic patient records database for healthcare providers. The objective is to provide the capability for real-time, multimedia communication, so that a physician may access, on the move, the patients record and other relevant information as filtered by the physician&apos;s user profile, and may migrate ongoing application sessions seamlessly to different platforms that range from a high performance diagnostic workstation in the physician&apos;s office to hand held PDAs in the examination room. While the proposed techniques are general and extend to a range of mobile applications, the specific target of this project is healthcare applications. To this end, the researchers will develop a clinical testbed, which will serve as a laboratory for developing, testing, and evaluating advanced information technology in the context of patient care. The testbed will provide the user requirements to drive the iMASH architecture design, and will permit direct, realistic validation of the research results.&lt;br/&gt; The researchers expect to make the following contributions from this research and development effort:&lt;br/&gt; 1) Development of a middleware infrastructure that provides support for anytime, anywhere, on any platform access to the Internet&lt;br/&gt; 2) A suite of wireless networking protocols and algorithms that provide quality of service support in a mobile, heterogeneous networking environment&lt;br/&gt; 3) A deployment of iMASH within the UCLA Medial School and a controlled study to evaluate its effectiveness in reducing healthcare costs and improving physician effectiveness&lt;br/&gt; 4) A system emulation capability that can be used to evaluate the performance and scalability of the middleware services and protocols across multiple dimensions including number of users, number of devices, types of applications, and geographical area. The emulator will be used to &apos;test drive&apos; novel protocols and applications prior to deployment on the physical testbed.&lt;br/&gt; The researchers have assembled a strong research and development team to undertake the iMASH effort. The team possesses the necessary expertise in the related areas of networking (Zhang, Gerla), wireless communications (Gerla, Lu), parallel and distributed systems (Bagrodia, Gerla), performance evaluation (Bagrodia), computerized medicine (Valentino, McCoy), clinical evaluation of technological innovations in improving heath care (Fiske), and campus computing and communication technology (Solomon).&lt;br/&gt; A longer term goal of this effort is to deploy iMASH-like technology widely within the UCLA campus to support ubiquitous multimedia access for students and faculty, and to support wireless distance education. To enable appropriate technology transition, the team also includes two key members from the university administration: the CIO for the medical school (McCoy) and the Associate Vice-chancellor of Administrative Services with line responsibility over campus telecommunications (Solomon). The UCLA Hospital has recently embarked on a historical reconstruction with a $1 billion endowment. An integral part of the reconstruction is availability of complete wireless connectivity within the hospital. The UCLA campus is also engaged in a project to upgrade the network connectivity throughout the campus with the aim of providing a minimum of 10Mbps bandwidth from desktop to desktop within any two locations on campus. Planning is underway to further enhance this capability with wireless connectivity. These two technology initiatives provide a unique opportunity to insert the iMASH technology in widespread use within the UCLA campus, and subsequently to other locations.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">9.98668e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98668e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n996" target="n998">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">iMASH: Adaptive Middleware and Networking Support for the Nomadic Healer</data>
      <data key="e_abstract">The researchers envision that within the next few years mobile and wireless access to the Internet will very likely become the norm, rather than the exception as is seen today. This proposal describes the plans to develop and deploy iMASH, a network system that supports anytime, anywhere, on any platform access to the electronic patient records database for healthcare providers. The objective is to provide the capability for real-time, multimedia communication, so that a physician may access, on the move, the patients record and other relevant information as filtered by the physician&apos;s user profile, and may migrate ongoing application sessions seamlessly to different platforms that range from a high performance diagnostic workstation in the physician&apos;s office to hand held PDAs in the examination room. While the proposed techniques are general and extend to a range of mobile applications, the specific target of this project is healthcare applications. To this end, the researchers will develop a clinical testbed, which will serve as a laboratory for developing, testing, and evaluating advanced information technology in the context of patient care. The testbed will provide the user requirements to drive the iMASH architecture design, and will permit direct, realistic validation of the research results.&lt;br/&gt; The researchers expect to make the following contributions from this research and development effort:&lt;br/&gt; 1) Development of a middleware infrastructure that provides support for anytime, anywhere, on any platform access to the Internet&lt;br/&gt; 2) A suite of wireless networking protocols and algorithms that provide quality of service support in a mobile, heterogeneous networking environment&lt;br/&gt; 3) A deployment of iMASH within the UCLA Medial School and a controlled study to evaluate its effectiveness in reducing healthcare costs and improving physician effectiveness&lt;br/&gt; 4) A system emulation capability that can be used to evaluate the performance and scalability of the middleware services and protocols across multiple dimensions including number of users, number of devices, types of applications, and geographical area. The emulator will be used to &apos;test drive&apos; novel protocols and applications prior to deployment on the physical testbed.&lt;br/&gt; The researchers have assembled a strong research and development team to undertake the iMASH effort. The team possesses the necessary expertise in the related areas of networking (Zhang, Gerla), wireless communications (Gerla, Lu), parallel and distributed systems (Bagrodia, Gerla), performance evaluation (Bagrodia), computerized medicine (Valentino, McCoy), clinical evaluation of technological innovations in improving heath care (Fiske), and campus computing and communication technology (Solomon).&lt;br/&gt; A longer term goal of this effort is to deploy iMASH-like technology widely within the UCLA campus to support ubiquitous multimedia access for students and faculty, and to support wireless distance education. To enable appropriate technology transition, the team also includes two key members from the university administration: the CIO for the medical school (McCoy) and the Associate Vice-chancellor of Administrative Services with line responsibility over campus telecommunications (Solomon). The UCLA Hospital has recently embarked on a historical reconstruction with a $1 billion endowment. An integral part of the reconstruction is availability of complete wireless connectivity within the hospital. The UCLA campus is also engaged in a project to upgrade the network connectivity throughout the campus with the aim of providing a minimum of 10Mbps bandwidth from desktop to desktop within any two locations on campus. Planning is underway to further enhance this capability with wireless connectivity. These two technology initiatives provide a unique opportunity to insert the iMASH technology in widespread use within the UCLA campus, and subsequently to other locations.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">9.98668e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98668e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n996" target="n999">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">iMASH: Adaptive Middleware and Networking Support for the Nomadic Healer</data>
      <data key="e_abstract">The researchers envision that within the next few years mobile and wireless access to the Internet will very likely become the norm, rather than the exception as is seen today. This proposal describes the plans to develop and deploy iMASH, a network system that supports anytime, anywhere, on any platform access to the electronic patient records database for healthcare providers. The objective is to provide the capability for real-time, multimedia communication, so that a physician may access, on the move, the patients record and other relevant information as filtered by the physician&apos;s user profile, and may migrate ongoing application sessions seamlessly to different platforms that range from a high performance diagnostic workstation in the physician&apos;s office to hand held PDAs in the examination room. While the proposed techniques are general and extend to a range of mobile applications, the specific target of this project is healthcare applications. To this end, the researchers will develop a clinical testbed, which will serve as a laboratory for developing, testing, and evaluating advanced information technology in the context of patient care. The testbed will provide the user requirements to drive the iMASH architecture design, and will permit direct, realistic validation of the research results.&lt;br/&gt; The researchers expect to make the following contributions from this research and development effort:&lt;br/&gt; 1) Development of a middleware infrastructure that provides support for anytime, anywhere, on any platform access to the Internet&lt;br/&gt; 2) A suite of wireless networking protocols and algorithms that provide quality of service support in a mobile, heterogeneous networking environment&lt;br/&gt; 3) A deployment of iMASH within the UCLA Medial School and a controlled study to evaluate its effectiveness in reducing healthcare costs and improving physician effectiveness&lt;br/&gt; 4) A system emulation capability that can be used to evaluate the performance and scalability of the middleware services and protocols across multiple dimensions including number of users, number of devices, types of applications, and geographical area. The emulator will be used to &apos;test drive&apos; novel protocols and applications prior to deployment on the physical testbed.&lt;br/&gt; The researchers have assembled a strong research and development team to undertake the iMASH effort. The team possesses the necessary expertise in the related areas of networking (Zhang, Gerla), wireless communications (Gerla, Lu), parallel and distributed systems (Bagrodia, Gerla), performance evaluation (Bagrodia), computerized medicine (Valentino, McCoy), clinical evaluation of technological innovations in improving heath care (Fiske), and campus computing and communication technology (Solomon).&lt;br/&gt; A longer term goal of this effort is to deploy iMASH-like technology widely within the UCLA campus to support ubiquitous multimedia access for students and faculty, and to support wireless distance education. To enable appropriate technology transition, the team also includes two key members from the university administration: the CIO for the medical school (McCoy) and the Associate Vice-chancellor of Administrative Services with line responsibility over campus telecommunications (Solomon). The UCLA Hospital has recently embarked on a historical reconstruction with a $1 billion endowment. An integral part of the reconstruction is availability of complete wireless connectivity within the hospital. The UCLA campus is also engaged in a project to upgrade the network connectivity throughout the campus with the aim of providing a minimum of 10Mbps bandwidth from desktop to desktop within any two locations on campus. Planning is underway to further enhance this capability with wireless connectivity. These two technology initiatives provide a unique opportunity to insert the iMASH technology in widespread use within the UCLA campus, and subsequently to other locations.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">9.98668e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98668e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n996" target="n1000">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">iMASH: Adaptive Middleware and Networking Support for the Nomadic Healer</data>
      <data key="e_abstract">The researchers envision that within the next few years mobile and wireless access to the Internet will very likely become the norm, rather than the exception as is seen today. This proposal describes the plans to develop and deploy iMASH, a network system that supports anytime, anywhere, on any platform access to the electronic patient records database for healthcare providers. The objective is to provide the capability for real-time, multimedia communication, so that a physician may access, on the move, the patients record and other relevant information as filtered by the physician&apos;s user profile, and may migrate ongoing application sessions seamlessly to different platforms that range from a high performance diagnostic workstation in the physician&apos;s office to hand held PDAs in the examination room. While the proposed techniques are general and extend to a range of mobile applications, the specific target of this project is healthcare applications. To this end, the researchers will develop a clinical testbed, which will serve as a laboratory for developing, testing, and evaluating advanced information technology in the context of patient care. The testbed will provide the user requirements to drive the iMASH architecture design, and will permit direct, realistic validation of the research results.&lt;br/&gt; The researchers expect to make the following contributions from this research and development effort:&lt;br/&gt; 1) Development of a middleware infrastructure that provides support for anytime, anywhere, on any platform access to the Internet&lt;br/&gt; 2) A suite of wireless networking protocols and algorithms that provide quality of service support in a mobile, heterogeneous networking environment&lt;br/&gt; 3) A deployment of iMASH within the UCLA Medial School and a controlled study to evaluate its effectiveness in reducing healthcare costs and improving physician effectiveness&lt;br/&gt; 4) A system emulation capability that can be used to evaluate the performance and scalability of the middleware services and protocols across multiple dimensions including number of users, number of devices, types of applications, and geographical area. The emulator will be used to &apos;test drive&apos; novel protocols and applications prior to deployment on the physical testbed.&lt;br/&gt; The researchers have assembled a strong research and development team to undertake the iMASH effort. The team possesses the necessary expertise in the related areas of networking (Zhang, Gerla), wireless communications (Gerla, Lu), parallel and distributed systems (Bagrodia, Gerla), performance evaluation (Bagrodia), computerized medicine (Valentino, McCoy), clinical evaluation of technological innovations in improving heath care (Fiske), and campus computing and communication technology (Solomon).&lt;br/&gt; A longer term goal of this effort is to deploy iMASH-like technology widely within the UCLA campus to support ubiquitous multimedia access for students and faculty, and to support wireless distance education. To enable appropriate technology transition, the team also includes two key members from the university administration: the CIO for the medical school (McCoy) and the Associate Vice-chancellor of Administrative Services with line responsibility over campus telecommunications (Solomon). The UCLA Hospital has recently embarked on a historical reconstruction with a $1 billion endowment. An integral part of the reconstruction is availability of complete wireless connectivity within the hospital. The UCLA campus is also engaged in a project to upgrade the network connectivity throughout the campus with the aim of providing a minimum of 10Mbps bandwidth from desktop to desktop within any two locations on campus. Planning is underway to further enhance this capability with wireless connectivity. These two technology initiatives provide a unique opportunity to insert the iMASH technology in widespread use within the UCLA campus, and subsequently to other locations.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">9.98668e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98668e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n22" target="n998">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">iMASH: Adaptive Middleware and Networking Support for the Nomadic Healer</data>
      <data key="e_abstract">The researchers envision that within the next few years mobile and wireless access to the Internet will very likely become the norm, rather than the exception as is seen today. This proposal describes the plans to develop and deploy iMASH, a network system that supports anytime, anywhere, on any platform access to the electronic patient records database for healthcare providers. The objective is to provide the capability for real-time, multimedia communication, so that a physician may access, on the move, the patients record and other relevant information as filtered by the physician&apos;s user profile, and may migrate ongoing application sessions seamlessly to different platforms that range from a high performance diagnostic workstation in the physician&apos;s office to hand held PDAs in the examination room. While the proposed techniques are general and extend to a range of mobile applications, the specific target of this project is healthcare applications. To this end, the researchers will develop a clinical testbed, which will serve as a laboratory for developing, testing, and evaluating advanced information technology in the context of patient care. The testbed will provide the user requirements to drive the iMASH architecture design, and will permit direct, realistic validation of the research results.&lt;br/&gt; The researchers expect to make the following contributions from this research and development effort:&lt;br/&gt; 1) Development of a middleware infrastructure that provides support for anytime, anywhere, on any platform access to the Internet&lt;br/&gt; 2) A suite of wireless networking protocols and algorithms that provide quality of service support in a mobile, heterogeneous networking environment&lt;br/&gt; 3) A deployment of iMASH within the UCLA Medial School and a controlled study to evaluate its effectiveness in reducing healthcare costs and improving physician effectiveness&lt;br/&gt; 4) A system emulation capability that can be used to evaluate the performance and scalability of the middleware services and protocols across multiple dimensions including number of users, number of devices, types of applications, and geographical area. The emulator will be used to &apos;test drive&apos; novel protocols and applications prior to deployment on the physical testbed.&lt;br/&gt; The researchers have assembled a strong research and development team to undertake the iMASH effort. The team possesses the necessary expertise in the related areas of networking (Zhang, Gerla), wireless communications (Gerla, Lu), parallel and distributed systems (Bagrodia, Gerla), performance evaluation (Bagrodia), computerized medicine (Valentino, McCoy), clinical evaluation of technological innovations in improving heath care (Fiske), and campus computing and communication technology (Solomon).&lt;br/&gt; A longer term goal of this effort is to deploy iMASH-like technology widely within the UCLA campus to support ubiquitous multimedia access for students and faculty, and to support wireless distance education. To enable appropriate technology transition, the team also includes two key members from the university administration: the CIO for the medical school (McCoy) and the Associate Vice-chancellor of Administrative Services with line responsibility over campus telecommunications (Solomon). The UCLA Hospital has recently embarked on a historical reconstruction with a $1 billion endowment. An integral part of the reconstruction is availability of complete wireless connectivity within the hospital. The UCLA campus is also engaged in a project to upgrade the network connectivity throughout the campus with the aim of providing a minimum of 10Mbps bandwidth from desktop to desktop within any two locations on campus. Planning is underway to further enhance this capability with wireless connectivity. These two technology initiatives provide a unique opportunity to insert the iMASH technology in widespread use within the UCLA campus, and subsequently to other locations.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">9.98668e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98668e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n22" target="n999">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">iMASH: Adaptive Middleware and Networking Support for the Nomadic Healer</data>
      <data key="e_abstract">The researchers envision that within the next few years mobile and wireless access to the Internet will very likely become the norm, rather than the exception as is seen today. This proposal describes the plans to develop and deploy iMASH, a network system that supports anytime, anywhere, on any platform access to the electronic patient records database for healthcare providers. The objective is to provide the capability for real-time, multimedia communication, so that a physician may access, on the move, the patients record and other relevant information as filtered by the physician&apos;s user profile, and may migrate ongoing application sessions seamlessly to different platforms that range from a high performance diagnostic workstation in the physician&apos;s office to hand held PDAs in the examination room. While the proposed techniques are general and extend to a range of mobile applications, the specific target of this project is healthcare applications. To this end, the researchers will develop a clinical testbed, which will serve as a laboratory for developing, testing, and evaluating advanced information technology in the context of patient care. The testbed will provide the user requirements to drive the iMASH architecture design, and will permit direct, realistic validation of the research results.&lt;br/&gt; The researchers expect to make the following contributions from this research and development effort:&lt;br/&gt; 1) Development of a middleware infrastructure that provides support for anytime, anywhere, on any platform access to the Internet&lt;br/&gt; 2) A suite of wireless networking protocols and algorithms that provide quality of service support in a mobile, heterogeneous networking environment&lt;br/&gt; 3) A deployment of iMASH within the UCLA Medial School and a controlled study to evaluate its effectiveness in reducing healthcare costs and improving physician effectiveness&lt;br/&gt; 4) A system emulation capability that can be used to evaluate the performance and scalability of the middleware services and protocols across multiple dimensions including number of users, number of devices, types of applications, and geographical area. The emulator will be used to &apos;test drive&apos; novel protocols and applications prior to deployment on the physical testbed.&lt;br/&gt; The researchers have assembled a strong research and development team to undertake the iMASH effort. The team possesses the necessary expertise in the related areas of networking (Zhang, Gerla), wireless communications (Gerla, Lu), parallel and distributed systems (Bagrodia, Gerla), performance evaluation (Bagrodia), computerized medicine (Valentino, McCoy), clinical evaluation of technological innovations in improving heath care (Fiske), and campus computing and communication technology (Solomon).&lt;br/&gt; A longer term goal of this effort is to deploy iMASH-like technology widely within the UCLA campus to support ubiquitous multimedia access for students and faculty, and to support wireless distance education. To enable appropriate technology transition, the team also includes two key members from the university administration: the CIO for the medical school (McCoy) and the Associate Vice-chancellor of Administrative Services with line responsibility over campus telecommunications (Solomon). The UCLA Hospital has recently embarked on a historical reconstruction with a $1 billion endowment. An integral part of the reconstruction is availability of complete wireless connectivity within the hospital. The UCLA campus is also engaged in a project to upgrade the network connectivity throughout the campus with the aim of providing a minimum of 10Mbps bandwidth from desktop to desktop within any two locations on campus. Planning is underway to further enhance this capability with wireless connectivity. These two technology initiatives provide a unique opportunity to insert the iMASH technology in widespread use within the UCLA campus, and subsequently to other locations.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">9.98668e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98668e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n22" target="n1000">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">iMASH: Adaptive Middleware and Networking Support for the Nomadic Healer</data>
      <data key="e_abstract">The researchers envision that within the next few years mobile and wireless access to the Internet will very likely become the norm, rather than the exception as is seen today. This proposal describes the plans to develop and deploy iMASH, a network system that supports anytime, anywhere, on any platform access to the electronic patient records database for healthcare providers. The objective is to provide the capability for real-time, multimedia communication, so that a physician may access, on the move, the patients record and other relevant information as filtered by the physician&apos;s user profile, and may migrate ongoing application sessions seamlessly to different platforms that range from a high performance diagnostic workstation in the physician&apos;s office to hand held PDAs in the examination room. While the proposed techniques are general and extend to a range of mobile applications, the specific target of this project is healthcare applications. To this end, the researchers will develop a clinical testbed, which will serve as a laboratory for developing, testing, and evaluating advanced information technology in the context of patient care. The testbed will provide the user requirements to drive the iMASH architecture design, and will permit direct, realistic validation of the research results.&lt;br/&gt; The researchers expect to make the following contributions from this research and development effort:&lt;br/&gt; 1) Development of a middleware infrastructure that provides support for anytime, anywhere, on any platform access to the Internet&lt;br/&gt; 2) A suite of wireless networking protocols and algorithms that provide quality of service support in a mobile, heterogeneous networking environment&lt;br/&gt; 3) A deployment of iMASH within the UCLA Medial School and a controlled study to evaluate its effectiveness in reducing healthcare costs and improving physician effectiveness&lt;br/&gt; 4) A system emulation capability that can be used to evaluate the performance and scalability of the middleware services and protocols across multiple dimensions including number of users, number of devices, types of applications, and geographical area. The emulator will be used to &apos;test drive&apos; novel protocols and applications prior to deployment on the physical testbed.&lt;br/&gt; The researchers have assembled a strong research and development team to undertake the iMASH effort. The team possesses the necessary expertise in the related areas of networking (Zhang, Gerla), wireless communications (Gerla, Lu), parallel and distributed systems (Bagrodia, Gerla), performance evaluation (Bagrodia), computerized medicine (Valentino, McCoy), clinical evaluation of technological innovations in improving heath care (Fiske), and campus computing and communication technology (Solomon).&lt;br/&gt; A longer term goal of this effort is to deploy iMASH-like technology widely within the UCLA campus to support ubiquitous multimedia access for students and faculty, and to support wireless distance education. To enable appropriate technology transition, the team also includes two key members from the university administration: the CIO for the medical school (McCoy) and the Associate Vice-chancellor of Administrative Services with line responsibility over campus telecommunications (Solomon). The UCLA Hospital has recently embarked on a historical reconstruction with a $1 billion endowment. An integral part of the reconstruction is availability of complete wireless connectivity within the hospital. The UCLA campus is also engaged in a project to upgrade the network connectivity throughout the campus with the aim of providing a minimum of 10Mbps bandwidth from desktop to desktop within any two locations on campus. Planning is underway to further enhance this capability with wireless connectivity. These two technology initiatives provide a unique opportunity to insert the iMASH technology in widespread use within the UCLA campus, and subsequently to other locations.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">9.98668e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98668e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n998" target="n999">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">iMASH: Adaptive Middleware and Networking Support for the Nomadic Healer</data>
      <data key="e_abstract">The researchers envision that within the next few years mobile and wireless access to the Internet will very likely become the norm, rather than the exception as is seen today. This proposal describes the plans to develop and deploy iMASH, a network system that supports anytime, anywhere, on any platform access to the electronic patient records database for healthcare providers. The objective is to provide the capability for real-time, multimedia communication, so that a physician may access, on the move, the patients record and other relevant information as filtered by the physician&apos;s user profile, and may migrate ongoing application sessions seamlessly to different platforms that range from a high performance diagnostic workstation in the physician&apos;s office to hand held PDAs in the examination room. While the proposed techniques are general and extend to a range of mobile applications, the specific target of this project is healthcare applications. To this end, the researchers will develop a clinical testbed, which will serve as a laboratory for developing, testing, and evaluating advanced information technology in the context of patient care. The testbed will provide the user requirements to drive the iMASH architecture design, and will permit direct, realistic validation of the research results.&lt;br/&gt; The researchers expect to make the following contributions from this research and development effort:&lt;br/&gt; 1) Development of a middleware infrastructure that provides support for anytime, anywhere, on any platform access to the Internet&lt;br/&gt; 2) A suite of wireless networking protocols and algorithms that provide quality of service support in a mobile, heterogeneous networking environment&lt;br/&gt; 3) A deployment of iMASH within the UCLA Medial School and a controlled study to evaluate its effectiveness in reducing healthcare costs and improving physician effectiveness&lt;br/&gt; 4) A system emulation capability that can be used to evaluate the performance and scalability of the middleware services and protocols across multiple dimensions including number of users, number of devices, types of applications, and geographical area. The emulator will be used to &apos;test drive&apos; novel protocols and applications prior to deployment on the physical testbed.&lt;br/&gt; The researchers have assembled a strong research and development team to undertake the iMASH effort. The team possesses the necessary expertise in the related areas of networking (Zhang, Gerla), wireless communications (Gerla, Lu), parallel and distributed systems (Bagrodia, Gerla), performance evaluation (Bagrodia), computerized medicine (Valentino, McCoy), clinical evaluation of technological innovations in improving heath care (Fiske), and campus computing and communication technology (Solomon).&lt;br/&gt; A longer term goal of this effort is to deploy iMASH-like technology widely within the UCLA campus to support ubiquitous multimedia access for students and faculty, and to support wireless distance education. To enable appropriate technology transition, the team also includes two key members from the university administration: the CIO for the medical school (McCoy) and the Associate Vice-chancellor of Administrative Services with line responsibility over campus telecommunications (Solomon). The UCLA Hospital has recently embarked on a historical reconstruction with a $1 billion endowment. An integral part of the reconstruction is availability of complete wireless connectivity within the hospital. The UCLA campus is also engaged in a project to upgrade the network connectivity throughout the campus with the aim of providing a minimum of 10Mbps bandwidth from desktop to desktop within any two locations on campus. Planning is underway to further enhance this capability with wireless connectivity. These two technology initiatives provide a unique opportunity to insert the iMASH technology in widespread use within the UCLA campus, and subsequently to other locations.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">9.98668e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98668e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n998" target="n1000">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">iMASH: Adaptive Middleware and Networking Support for the Nomadic Healer</data>
      <data key="e_abstract">The researchers envision that within the next few years mobile and wireless access to the Internet will very likely become the norm, rather than the exception as is seen today. This proposal describes the plans to develop and deploy iMASH, a network system that supports anytime, anywhere, on any platform access to the electronic patient records database for healthcare providers. The objective is to provide the capability for real-time, multimedia communication, so that a physician may access, on the move, the patients record and other relevant information as filtered by the physician&apos;s user profile, and may migrate ongoing application sessions seamlessly to different platforms that range from a high performance diagnostic workstation in the physician&apos;s office to hand held PDAs in the examination room. While the proposed techniques are general and extend to a range of mobile applications, the specific target of this project is healthcare applications. To this end, the researchers will develop a clinical testbed, which will serve as a laboratory for developing, testing, and evaluating advanced information technology in the context of patient care. The testbed will provide the user requirements to drive the iMASH architecture design, and will permit direct, realistic validation of the research results.&lt;br/&gt; The researchers expect to make the following contributions from this research and development effort:&lt;br/&gt; 1) Development of a middleware infrastructure that provides support for anytime, anywhere, on any platform access to the Internet&lt;br/&gt; 2) A suite of wireless networking protocols and algorithms that provide quality of service support in a mobile, heterogeneous networking environment&lt;br/&gt; 3) A deployment of iMASH within the UCLA Medial School and a controlled study to evaluate its effectiveness in reducing healthcare costs and improving physician effectiveness&lt;br/&gt; 4) A system emulation capability that can be used to evaluate the performance and scalability of the middleware services and protocols across multiple dimensions including number of users, number of devices, types of applications, and geographical area. The emulator will be used to &apos;test drive&apos; novel protocols and applications prior to deployment on the physical testbed.&lt;br/&gt; The researchers have assembled a strong research and development team to undertake the iMASH effort. The team possesses the necessary expertise in the related areas of networking (Zhang, Gerla), wireless communications (Gerla, Lu), parallel and distributed systems (Bagrodia, Gerla), performance evaluation (Bagrodia), computerized medicine (Valentino, McCoy), clinical evaluation of technological innovations in improving heath care (Fiske), and campus computing and communication technology (Solomon).&lt;br/&gt; A longer term goal of this effort is to deploy iMASH-like technology widely within the UCLA campus to support ubiquitous multimedia access for students and faculty, and to support wireless distance education. To enable appropriate technology transition, the team also includes two key members from the university administration: the CIO for the medical school (McCoy) and the Associate Vice-chancellor of Administrative Services with line responsibility over campus telecommunications (Solomon). The UCLA Hospital has recently embarked on a historical reconstruction with a $1 billion endowment. An integral part of the reconstruction is availability of complete wireless connectivity within the hospital. The UCLA campus is also engaged in a project to upgrade the network connectivity throughout the campus with the aim of providing a minimum of 10Mbps bandwidth from desktop to desktop within any two locations on campus. Planning is underway to further enhance this capability with wireless connectivity. These two technology initiatives provide a unique opportunity to insert the iMASH technology in widespread use within the UCLA campus, and subsequently to other locations.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">9.98668e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98668e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n999" target="n1000">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">iMASH: Adaptive Middleware and Networking Support for the Nomadic Healer</data>
      <data key="e_abstract">The researchers envision that within the next few years mobile and wireless access to the Internet will very likely become the norm, rather than the exception as is seen today. This proposal describes the plans to develop and deploy iMASH, a network system that supports anytime, anywhere, on any platform access to the electronic patient records database for healthcare providers. The objective is to provide the capability for real-time, multimedia communication, so that a physician may access, on the move, the patients record and other relevant information as filtered by the physician&apos;s user profile, and may migrate ongoing application sessions seamlessly to different platforms that range from a high performance diagnostic workstation in the physician&apos;s office to hand held PDAs in the examination room. While the proposed techniques are general and extend to a range of mobile applications, the specific target of this project is healthcare applications. To this end, the researchers will develop a clinical testbed, which will serve as a laboratory for developing, testing, and evaluating advanced information technology in the context of patient care. The testbed will provide the user requirements to drive the iMASH architecture design, and will permit direct, realistic validation of the research results.&lt;br/&gt; The researchers expect to make the following contributions from this research and development effort:&lt;br/&gt; 1) Development of a middleware infrastructure that provides support for anytime, anywhere, on any platform access to the Internet&lt;br/&gt; 2) A suite of wireless networking protocols and algorithms that provide quality of service support in a mobile, heterogeneous networking environment&lt;br/&gt; 3) A deployment of iMASH within the UCLA Medial School and a controlled study to evaluate its effectiveness in reducing healthcare costs and improving physician effectiveness&lt;br/&gt; 4) A system emulation capability that can be used to evaluate the performance and scalability of the middleware services and protocols across multiple dimensions including number of users, number of devices, types of applications, and geographical area. The emulator will be used to &apos;test drive&apos; novel protocols and applications prior to deployment on the physical testbed.&lt;br/&gt; The researchers have assembled a strong research and development team to undertake the iMASH effort. The team possesses the necessary expertise in the related areas of networking (Zhang, Gerla), wireless communications (Gerla, Lu), parallel and distributed systems (Bagrodia, Gerla), performance evaluation (Bagrodia), computerized medicine (Valentino, McCoy), clinical evaluation of technological innovations in improving heath care (Fiske), and campus computing and communication technology (Solomon).&lt;br/&gt; A longer term goal of this effort is to deploy iMASH-like technology widely within the UCLA campus to support ubiquitous multimedia access for students and faculty, and to support wireless distance education. To enable appropriate technology transition, the team also includes two key members from the university administration: the CIO for the medical school (McCoy) and the Associate Vice-chancellor of Administrative Services with line responsibility over campus telecommunications (Solomon). The UCLA Hospital has recently embarked on a historical reconstruction with a $1 billion endowment. An integral part of the reconstruction is availability of complete wireless connectivity within the hospital. The UCLA campus is also engaged in a project to upgrade the network connectivity throughout the campus with the aim of providing a minimum of 10Mbps bandwidth from desktop to desktop within any two locations on campus. Planning is underway to further enhance this capability with wireless connectivity. These two technology initiatives provide a unique opportunity to insert the iMASH technology in widespread use within the UCLA campus, and subsequently to other locations.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">9.98668e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98668e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1001" target="n1002">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Scalable Enterprise Sytems: Theory and Methodologies to Support the Operation of Flexible Production Networks</data>
      <data key="e_abstract">0075407&lt;br/&gt;Hammer&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This grant provides funding for developing fundamental theory and methodologies to support integrated operation of decentralized networks of firms producing goods under short lead times and variable demand. Research under this grant is directed towards specifying a complete decision support and information architecture that enables automated negotiation among firms and improves overall performance of these Flexible Production Networks (FPNs). In support of this objective, the investigators have identified four high-impact research areas:&lt;br/&gt;1) Simulation, modeling, and analysis of FPN operation under imperfect information; 2) Capacity and cost models for firms that exist in multiple FPNs; 3) Generation of information &apos;wrappers&apos; that tie firms&apos; existing information systems to an information hub infrastructure; 4) Information models for representing complex FPN operations. Each of these four areas is critical to achieving a feasible computational infrastructure that represents the diverse production technologies within the FPN, while addressing issues of scalability, rapid deployment, and decentralized operation.&lt;br/&gt;&lt;br/&gt;Successful conclusion of this research will generate advances in operational support for firms existing within FPNs, as well as broader advances in theory concerning the deployment of information and decision support systems in a multi-firm environment. Current industrial initiatives in extended-enterprise resource planning systems are a positive step towards deployment and simplification of electronic transactions; however, they lack a framework for analysis and decision support for production involving a network of firms. Research under this grant provides such a framework, with a focus on low overhead, dynamic deployment of a computational, and flexible and scalable decision support for decentralized, multi-firm operations. Research in these areas will also broaden existing theory in decision support and computer science, where current models tend towards exact analysis and representation of limited or highly specific systems. In a broad sense, this work will re-focus current capabilities towards supporting more complex, flexible systems.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">75407</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">75407</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1001" target="n1003">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Scalable Enterprise Sytems: Theory and Methodologies to Support the Operation of Flexible Production Networks</data>
      <data key="e_abstract">0075407&lt;br/&gt;Hammer&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This grant provides funding for developing fundamental theory and methodologies to support integrated operation of decentralized networks of firms producing goods under short lead times and variable demand. Research under this grant is directed towards specifying a complete decision support and information architecture that enables automated negotiation among firms and improves overall performance of these Flexible Production Networks (FPNs). In support of this objective, the investigators have identified four high-impact research areas:&lt;br/&gt;1) Simulation, modeling, and analysis of FPN operation under imperfect information; 2) Capacity and cost models for firms that exist in multiple FPNs; 3) Generation of information &apos;wrappers&apos; that tie firms&apos; existing information systems to an information hub infrastructure; 4) Information models for representing complex FPN operations. Each of these four areas is critical to achieving a feasible computational infrastructure that represents the diverse production technologies within the FPN, while addressing issues of scalability, rapid deployment, and decentralized operation.&lt;br/&gt;&lt;br/&gt;Successful conclusion of this research will generate advances in operational support for firms existing within FPNs, as well as broader advances in theory concerning the deployment of information and decision support systems in a multi-firm environment. Current industrial initiatives in extended-enterprise resource planning systems are a positive step towards deployment and simplification of electronic transactions; however, they lack a framework for analysis and decision support for production involving a network of firms. Research under this grant provides such a framework, with a focus on low overhead, dynamic deployment of a computational, and flexible and scalable decision support for decentralized, multi-firm operations. Research in these areas will also broaden existing theory in decision support and computer science, where current models tend towards exact analysis and representation of limited or highly specific systems. In a broad sense, this work will re-focus current capabilities towards supporting more complex, flexible systems.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">75407</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">75407</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1001" target="n1004">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Scalable Enterprise Sytems: Theory and Methodologies to Support the Operation of Flexible Production Networks</data>
      <data key="e_abstract">0075407&lt;br/&gt;Hammer&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This grant provides funding for developing fundamental theory and methodologies to support integrated operation of decentralized networks of firms producing goods under short lead times and variable demand. Research under this grant is directed towards specifying a complete decision support and information architecture that enables automated negotiation among firms and improves overall performance of these Flexible Production Networks (FPNs). In support of this objective, the investigators have identified four high-impact research areas:&lt;br/&gt;1) Simulation, modeling, and analysis of FPN operation under imperfect information; 2) Capacity and cost models for firms that exist in multiple FPNs; 3) Generation of information &apos;wrappers&apos; that tie firms&apos; existing information systems to an information hub infrastructure; 4) Information models for representing complex FPN operations. Each of these four areas is critical to achieving a feasible computational infrastructure that represents the diverse production technologies within the FPN, while addressing issues of scalability, rapid deployment, and decentralized operation.&lt;br/&gt;&lt;br/&gt;Successful conclusion of this research will generate advances in operational support for firms existing within FPNs, as well as broader advances in theory concerning the deployment of information and decision support systems in a multi-firm environment. Current industrial initiatives in extended-enterprise resource planning systems are a positive step towards deployment and simplification of electronic transactions; however, they lack a framework for analysis and decision support for production involving a network of firms. Research under this grant provides such a framework, with a focus on low overhead, dynamic deployment of a computational, and flexible and scalable decision support for decentralized, multi-firm operations. Research in these areas will also broaden existing theory in decision support and computer science, where current models tend towards exact analysis and representation of limited or highly specific systems. In a broad sense, this work will re-focus current capabilities towards supporting more complex, flexible systems.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">75407</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">75407</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1001" target="n1005">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Scalable Enterprise Sytems: Theory and Methodologies to Support the Operation of Flexible Production Networks</data>
      <data key="e_abstract">0075407&lt;br/&gt;Hammer&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This grant provides funding for developing fundamental theory and methodologies to support integrated operation of decentralized networks of firms producing goods under short lead times and variable demand. Research under this grant is directed towards specifying a complete decision support and information architecture that enables automated negotiation among firms and improves overall performance of these Flexible Production Networks (FPNs). In support of this objective, the investigators have identified four high-impact research areas:&lt;br/&gt;1) Simulation, modeling, and analysis of FPN operation under imperfect information; 2) Capacity and cost models for firms that exist in multiple FPNs; 3) Generation of information &apos;wrappers&apos; that tie firms&apos; existing information systems to an information hub infrastructure; 4) Information models for representing complex FPN operations. Each of these four areas is critical to achieving a feasible computational infrastructure that represents the diverse production technologies within the FPN, while addressing issues of scalability, rapid deployment, and decentralized operation.&lt;br/&gt;&lt;br/&gt;Successful conclusion of this research will generate advances in operational support for firms existing within FPNs, as well as broader advances in theory concerning the deployment of information and decision support systems in a multi-firm environment. Current industrial initiatives in extended-enterprise resource planning systems are a positive step towards deployment and simplification of electronic transactions; however, they lack a framework for analysis and decision support for production involving a network of firms. Research under this grant provides such a framework, with a focus on low overhead, dynamic deployment of a computational, and flexible and scalable decision support for decentralized, multi-firm operations. Research in these areas will also broaden existing theory in decision support and computer science, where current models tend towards exact analysis and representation of limited or highly specific systems. In a broad sense, this work will re-focus current capabilities towards supporting more complex, flexible systems.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">75407</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">75407</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1002" target="n1003">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Scalable Enterprise Sytems: Theory and Methodologies to Support the Operation of Flexible Production Networks</data>
      <data key="e_abstract">0075407&lt;br/&gt;Hammer&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This grant provides funding for developing fundamental theory and methodologies to support integrated operation of decentralized networks of firms producing goods under short lead times and variable demand. Research under this grant is directed towards specifying a complete decision support and information architecture that enables automated negotiation among firms and improves overall performance of these Flexible Production Networks (FPNs). In support of this objective, the investigators have identified four high-impact research areas:&lt;br/&gt;1) Simulation, modeling, and analysis of FPN operation under imperfect information; 2) Capacity and cost models for firms that exist in multiple FPNs; 3) Generation of information &apos;wrappers&apos; that tie firms&apos; existing information systems to an information hub infrastructure; 4) Information models for representing complex FPN operations. Each of these four areas is critical to achieving a feasible computational infrastructure that represents the diverse production technologies within the FPN, while addressing issues of scalability, rapid deployment, and decentralized operation.&lt;br/&gt;&lt;br/&gt;Successful conclusion of this research will generate advances in operational support for firms existing within FPNs, as well as broader advances in theory concerning the deployment of information and decision support systems in a multi-firm environment. Current industrial initiatives in extended-enterprise resource planning systems are a positive step towards deployment and simplification of electronic transactions; however, they lack a framework for analysis and decision support for production involving a network of firms. Research under this grant provides such a framework, with a focus on low overhead, dynamic deployment of a computational, and flexible and scalable decision support for decentralized, multi-firm operations. Research in these areas will also broaden existing theory in decision support and computer science, where current models tend towards exact analysis and representation of limited or highly specific systems. In a broad sense, this work will re-focus current capabilities towards supporting more complex, flexible systems.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">75407</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">75407</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1002" target="n1004">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Scalable Enterprise Sytems: Theory and Methodologies to Support the Operation of Flexible Production Networks</data>
      <data key="e_abstract">0075407&lt;br/&gt;Hammer&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This grant provides funding for developing fundamental theory and methodologies to support integrated operation of decentralized networks of firms producing goods under short lead times and variable demand. Research under this grant is directed towards specifying a complete decision support and information architecture that enables automated negotiation among firms and improves overall performance of these Flexible Production Networks (FPNs). In support of this objective, the investigators have identified four high-impact research areas:&lt;br/&gt;1) Simulation, modeling, and analysis of FPN operation under imperfect information; 2) Capacity and cost models for firms that exist in multiple FPNs; 3) Generation of information &apos;wrappers&apos; that tie firms&apos; existing information systems to an information hub infrastructure; 4) Information models for representing complex FPN operations. Each of these four areas is critical to achieving a feasible computational infrastructure that represents the diverse production technologies within the FPN, while addressing issues of scalability, rapid deployment, and decentralized operation.&lt;br/&gt;&lt;br/&gt;Successful conclusion of this research will generate advances in operational support for firms existing within FPNs, as well as broader advances in theory concerning the deployment of information and decision support systems in a multi-firm environment. Current industrial initiatives in extended-enterprise resource planning systems are a positive step towards deployment and simplification of electronic transactions; however, they lack a framework for analysis and decision support for production involving a network of firms. Research under this grant provides such a framework, with a focus on low overhead, dynamic deployment of a computational, and flexible and scalable decision support for decentralized, multi-firm operations. Research in these areas will also broaden existing theory in decision support and computer science, where current models tend towards exact analysis and representation of limited or highly specific systems. In a broad sense, this work will re-focus current capabilities towards supporting more complex, flexible systems.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">75407</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">75407</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1002" target="n1005">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Scalable Enterprise Sytems: Theory and Methodologies to Support the Operation of Flexible Production Networks</data>
      <data key="e_abstract">0075407&lt;br/&gt;Hammer&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This grant provides funding for developing fundamental theory and methodologies to support integrated operation of decentralized networks of firms producing goods under short lead times and variable demand. Research under this grant is directed towards specifying a complete decision support and information architecture that enables automated negotiation among firms and improves overall performance of these Flexible Production Networks (FPNs). In support of this objective, the investigators have identified four high-impact research areas:&lt;br/&gt;1) Simulation, modeling, and analysis of FPN operation under imperfect information; 2) Capacity and cost models for firms that exist in multiple FPNs; 3) Generation of information &apos;wrappers&apos; that tie firms&apos; existing information systems to an information hub infrastructure; 4) Information models for representing complex FPN operations. Each of these four areas is critical to achieving a feasible computational infrastructure that represents the diverse production technologies within the FPN, while addressing issues of scalability, rapid deployment, and decentralized operation.&lt;br/&gt;&lt;br/&gt;Successful conclusion of this research will generate advances in operational support for firms existing within FPNs, as well as broader advances in theory concerning the deployment of information and decision support systems in a multi-firm environment. Current industrial initiatives in extended-enterprise resource planning systems are a positive step towards deployment and simplification of electronic transactions; however, they lack a framework for analysis and decision support for production involving a network of firms. Research under this grant provides such a framework, with a focus on low overhead, dynamic deployment of a computational, and flexible and scalable decision support for decentralized, multi-firm operations. Research in these areas will also broaden existing theory in decision support and computer science, where current models tend towards exact analysis and representation of limited or highly specific systems. In a broad sense, this work will re-focus current capabilities towards supporting more complex, flexible systems.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">75407</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">75407</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1003" target="n1004">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Scalable Enterprise Sytems: Theory and Methodologies to Support the Operation of Flexible Production Networks</data>
      <data key="e_abstract">0075407&lt;br/&gt;Hammer&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This grant provides funding for developing fundamental theory and methodologies to support integrated operation of decentralized networks of firms producing goods under short lead times and variable demand. Research under this grant is directed towards specifying a complete decision support and information architecture that enables automated negotiation among firms and improves overall performance of these Flexible Production Networks (FPNs). In support of this objective, the investigators have identified four high-impact research areas:&lt;br/&gt;1) Simulation, modeling, and analysis of FPN operation under imperfect information; 2) Capacity and cost models for firms that exist in multiple FPNs; 3) Generation of information &apos;wrappers&apos; that tie firms&apos; existing information systems to an information hub infrastructure; 4) Information models for representing complex FPN operations. Each of these four areas is critical to achieving a feasible computational infrastructure that represents the diverse production technologies within the FPN, while addressing issues of scalability, rapid deployment, and decentralized operation.&lt;br/&gt;&lt;br/&gt;Successful conclusion of this research will generate advances in operational support for firms existing within FPNs, as well as broader advances in theory concerning the deployment of information and decision support systems in a multi-firm environment. Current industrial initiatives in extended-enterprise resource planning systems are a positive step towards deployment and simplification of electronic transactions; however, they lack a framework for analysis and decision support for production involving a network of firms. Research under this grant provides such a framework, with a focus on low overhead, dynamic deployment of a computational, and flexible and scalable decision support for decentralized, multi-firm operations. Research in these areas will also broaden existing theory in decision support and computer science, where current models tend towards exact analysis and representation of limited or highly specific systems. In a broad sense, this work will re-focus current capabilities towards supporting more complex, flexible systems.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">75407</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">75407</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1003" target="n1005">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Scalable Enterprise Sytems: Theory and Methodologies to Support the Operation of Flexible Production Networks</data>
      <data key="e_abstract">0075407&lt;br/&gt;Hammer&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This grant provides funding for developing fundamental theory and methodologies to support integrated operation of decentralized networks of firms producing goods under short lead times and variable demand. Research under this grant is directed towards specifying a complete decision support and information architecture that enables automated negotiation among firms and improves overall performance of these Flexible Production Networks (FPNs). In support of this objective, the investigators have identified four high-impact research areas:&lt;br/&gt;1) Simulation, modeling, and analysis of FPN operation under imperfect information; 2) Capacity and cost models for firms that exist in multiple FPNs; 3) Generation of information &apos;wrappers&apos; that tie firms&apos; existing information systems to an information hub infrastructure; 4) Information models for representing complex FPN operations. Each of these four areas is critical to achieving a feasible computational infrastructure that represents the diverse production technologies within the FPN, while addressing issues of scalability, rapid deployment, and decentralized operation.&lt;br/&gt;&lt;br/&gt;Successful conclusion of this research will generate advances in operational support for firms existing within FPNs, as well as broader advances in theory concerning the deployment of information and decision support systems in a multi-firm environment. Current industrial initiatives in extended-enterprise resource planning systems are a positive step towards deployment and simplification of electronic transactions; however, they lack a framework for analysis and decision support for production involving a network of firms. Research under this grant provides such a framework, with a focus on low overhead, dynamic deployment of a computational, and flexible and scalable decision support for decentralized, multi-firm operations. Research in these areas will also broaden existing theory in decision support and computer science, where current models tend towards exact analysis and representation of limited or highly specific systems. In a broad sense, this work will re-focus current capabilities towards supporting more complex, flexible systems.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">75407</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">75407</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1004" target="n1005">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Scalable Enterprise Sytems: Theory and Methodologies to Support the Operation of Flexible Production Networks</data>
      <data key="e_abstract">0075407&lt;br/&gt;Hammer&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This grant provides funding for developing fundamental theory and methodologies to support integrated operation of decentralized networks of firms producing goods under short lead times and variable demand. Research under this grant is directed towards specifying a complete decision support and information architecture that enables automated negotiation among firms and improves overall performance of these Flexible Production Networks (FPNs). In support of this objective, the investigators have identified four high-impact research areas:&lt;br/&gt;1) Simulation, modeling, and analysis of FPN operation under imperfect information; 2) Capacity and cost models for firms that exist in multiple FPNs; 3) Generation of information &apos;wrappers&apos; that tie firms&apos; existing information systems to an information hub infrastructure; 4) Information models for representing complex FPN operations. Each of these four areas is critical to achieving a feasible computational infrastructure that represents the diverse production technologies within the FPN, while addressing issues of scalability, rapid deployment, and decentralized operation.&lt;br/&gt;&lt;br/&gt;Successful conclusion of this research will generate advances in operational support for firms existing within FPNs, as well as broader advances in theory concerning the deployment of information and decision support systems in a multi-firm environment. Current industrial initiatives in extended-enterprise resource planning systems are a positive step towards deployment and simplification of electronic transactions; however, they lack a framework for analysis and decision support for production involving a network of firms. Research under this grant provides such a framework, with a focus on low overhead, dynamic deployment of a computational, and flexible and scalable decision support for decentralized, multi-firm operations. Research in these areas will also broaden existing theory in decision support and computer science, where current models tend towards exact analysis and representation of limited or highly specific systems. In a broad sense, this work will re-focus current capabilities towards supporting more complex, flexible systems.</data>
      <data key="e_pgm">2890</data>
      <data key="e_label">75407</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">75407</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1006" target="n1007">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Protocol Agile Optical Networking for the Next Generation Internet</data>
      <data key="e_abstract">The researchers propose to investigate and to demonstrate an optical networking technique that offers extremely high performance and flexibility, targeting the goals of the Next Generation Internet and beyond. In particular, the researchers will study the Optical-Label Switching technique which offers the following unique properties: 1) Accommodates signals of any protocol and format, 2) Achieves ultra-low latency transport of high burst rate packets, 3) Requires no network or packet synchronization, 4) Interoperates with both circuit-switched and packet-switched traffic, 5) Automatically detects and restores network failures, and 6) Provides on-demand Quality of Service (QoS) and priority based differentiated services.&lt;br/&gt; The proposed effort makes comprehensive studies of the Next Generation Internet, and seeks&lt;br/&gt;innovations in both hardware and software. Specifically, the researchers will pursue an integrated effort of the following research activities: 1) NGI architecture and protocol studies, which will design and simulate an optimum network, architecture and protocol to transport optical packets over multi-wavelength signals, 2) NGI network control &amp; management (NC&amp;M), and signaling, which will pursue efficient and rapid routing of packets while intelligently reflecting the network traffic conditions, 3) NGI network element design and optical technologies, which will design, simulate, and prototype network elements that effectively route packets while resolving packet contentions, and 4) NGI Testbed Integration and Demonstration, which will bring network elements and NC&amp;M together to implement a testbed and to experimentally demonstrate broad aspects of networking research covered in this proposed effort. The designed architecture, protocol, and network elements will be put to test at this step and the simulation results will be experimentally verified.</data>
      <data key="e_pgm">5980</data>
      <data key="e_label">9.98666e+06</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">9.98666e+06</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n1006" target="n1008">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Protocol Agile Optical Networking for the Next Generation Internet</data>
      <data key="e_abstract">The researchers propose to investigate and to demonstrate an optical networking technique that offers extremely high performance and flexibility, targeting the goals of the Next Generation Internet and beyond. In particular, the researchers will study the Optical-Label Switching technique which offers the following unique properties: 1) Accommodates signals of any protocol and format, 2) Achieves ultra-low latency transport of high burst rate packets, 3) Requires no network or packet synchronization, 4) Interoperates with both circuit-switched and packet-switched traffic, 5) Automatically detects and restores network failures, and 6) Provides on-demand Quality of Service (QoS) and priority based differentiated services.&lt;br/&gt; The proposed effort makes comprehensive studies of the Next Generation Internet, and seeks&lt;br/&gt;innovations in both hardware and software. Specifically, the researchers will pursue an integrated effort of the following research activities: 1) NGI architecture and protocol studies, which will design and simulate an optimum network, architecture and protocol to transport optical packets over multi-wavelength signals, 2) NGI network control &amp; management (NC&amp;M), and signaling, which will pursue efficient and rapid routing of packets while intelligently reflecting the network traffic conditions, 3) NGI network element design and optical technologies, which will design, simulate, and prototype network elements that effectively route packets while resolving packet contentions, and 4) NGI Testbed Integration and Demonstration, which will bring network elements and NC&amp;M together to implement a testbed and to experimentally demonstrate broad aspects of networking research covered in this proposed effort. The designed architecture, protocol, and network elements will be put to test at this step and the simulation results will be experimentally verified.</data>
      <data key="e_pgm">5980</data>
      <data key="e_label">9.98666e+06</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">9.98666e+06</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n1007" target="n1008">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Protocol Agile Optical Networking for the Next Generation Internet</data>
      <data key="e_abstract">The researchers propose to investigate and to demonstrate an optical networking technique that offers extremely high performance and flexibility, targeting the goals of the Next Generation Internet and beyond. In particular, the researchers will study the Optical-Label Switching technique which offers the following unique properties: 1) Accommodates signals of any protocol and format, 2) Achieves ultra-low latency transport of high burst rate packets, 3) Requires no network or packet synchronization, 4) Interoperates with both circuit-switched and packet-switched traffic, 5) Automatically detects and restores network failures, and 6) Provides on-demand Quality of Service (QoS) and priority based differentiated services.&lt;br/&gt; The proposed effort makes comprehensive studies of the Next Generation Internet, and seeks&lt;br/&gt;innovations in both hardware and software. Specifically, the researchers will pursue an integrated effort of the following research activities: 1) NGI architecture and protocol studies, which will design and simulate an optimum network, architecture and protocol to transport optical packets over multi-wavelength signals, 2) NGI network control &amp; management (NC&amp;M), and signaling, which will pursue efficient and rapid routing of packets while intelligently reflecting the network traffic conditions, 3) NGI network element design and optical technologies, which will design, simulate, and prototype network elements that effectively route packets while resolving packet contentions, and 4) NGI Testbed Integration and Demonstration, which will bring network elements and NC&amp;M together to implement a testbed and to experimentally demonstrate broad aspects of networking research covered in this proposed effort. The designed architecture, protocol, and network elements will be put to test at this step and the simulation results will be experimentally verified.</data>
      <data key="e_pgm">5980</data>
      <data key="e_label">9.98666e+06</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">9.98666e+06</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n1012" target="n1013">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Synthesis of Adaptive Mission-Specific Processors</data>
      <data key="e_abstract">The objective of this project is to develop the design methodology, software tools and programming environment for the automated synthesis of Mission-specific Processors (MSPs) based on the Morphosys architecture developed at UC, Irvine. Because their hardware design is not tailored to the application(s), generic embedded processor architectures make inefficient use of area and power. ASICs on the other hand cannot be updated to accommodate different algorithm improvements. By allowing a set of target applications to dictate the architecture, MSP designs incorporate the necessary components and interconnections to optimize the performance of a specific suite of programs. The synthesized architecture is dynamically reconfigurable and supports a retargetable compiler to incorporate algorithm enhancements or changing task specifications. The proposed approach relies on the compilation of a program expressed in a high-level algorithmic language into a data-flow graph (DFG) format and from that format into VHDL. This technology has been developed as part of the Cameron Project at Colorado State University. A new MSP design framework will be developed to incorporate constraint-based MSP synthesis and estimation as well as compilation support for the synthesized architecture.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">83080</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">83080</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1012" target="n1014">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Synthesis of Adaptive Mission-Specific Processors</data>
      <data key="e_abstract">The objective of this project is to develop the design methodology, software tools and programming environment for the automated synthesis of Mission-specific Processors (MSPs) based on the Morphosys architecture developed at UC, Irvine. Because their hardware design is not tailored to the application(s), generic embedded processor architectures make inefficient use of area and power. ASICs on the other hand cannot be updated to accommodate different algorithm improvements. By allowing a set of target applications to dictate the architecture, MSP designs incorporate the necessary components and interconnections to optimize the performance of a specific suite of programs. The synthesized architecture is dynamically reconfigurable and supports a retargetable compiler to incorporate algorithm enhancements or changing task specifications. The proposed approach relies on the compilation of a program expressed in a high-level algorithmic language into a data-flow graph (DFG) format and from that format into VHDL. This technology has been developed as part of the Cameron Project at Colorado State University. A new MSP design framework will be developed to incorporate constraint-based MSP synthesis and estimation as well as compilation support for the synthesized architecture.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">83080</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">83080</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1013" target="n1014">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Synthesis of Adaptive Mission-Specific Processors</data>
      <data key="e_abstract">The objective of this project is to develop the design methodology, software tools and programming environment for the automated synthesis of Mission-specific Processors (MSPs) based on the Morphosys architecture developed at UC, Irvine. Because their hardware design is not tailored to the application(s), generic embedded processor architectures make inefficient use of area and power. ASICs on the other hand cannot be updated to accommodate different algorithm improvements. By allowing a set of target applications to dictate the architecture, MSP designs incorporate the necessary components and interconnections to optimize the performance of a specific suite of programs. The synthesized architecture is dynamically reconfigurable and supports a retargetable compiler to incorporate algorithm enhancements or changing task specifications. The proposed approach relies on the compilation of a program expressed in a high-level algorithmic language into a data-flow graph (DFG) format and from that format into VHDL. This technology has been developed as part of the Cameron Project at Colorado State University. A new MSP design framework will be developed to incorporate constraint-based MSP synthesis and estimation as well as compilation support for the synthesized architecture.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">83080</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">83080</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1015" target="n1016">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Understanding Open Software Communities, Processes and Practices: A Socio-Technical Approach</data>
      <data key="e_abstract">This study will develop empirically grounded models and theories of the social processes, technical system configurations, organizational contexts, and interrelationships that give rise to open software. &quot;Open software&quot;, or more narrowly, open source software, represents a new approach for communities of like-minded participants to develop software systems that are intended to be shared freely, rather than offered as commercial products. While there is a growing popular literature attesting to open software, there is little in the way of careful systematic empirical study that informs how such communities produce software; how they coordinate software development across different settings; and what social processes, work practices, and organizational contexts are necessary to their success. Thus, to the extent that science research communities and commercial enterprises seek the supposed efficacy of open software, they will need better grounded theories of use to allow effective investment of their resources. This study investigates four communities engaged in open software. Field study methods will be employed to examine each community from both a technical and social viewpoint. Studying both social and technical arrangements allows the examination of the continual emergence of both within a joint social-technical ecology. Case study methods will be used to compare across communities.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">83075</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">83075</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1018" target="n1019">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Realistic Illumination for Scalar Field and Vector Field Visualization</data>
      <data key="e_abstract">Computer graphics has made great advances in producing realism and detail in synthetic scenes, yet these techniques have not been applied to scientific visualization. A realistic-looking 3D scene is immediately and viscerally apprehended, which promotes understanding of its contents. Fidelity of shading is important to a person viewing an already-familiar architectural environment. Such fidelity is even more important for a person viewing the complicated and unfamiliar geometry of isosurfaces and field lines that reveal the features in a dataset resulting from computational simulation or from physical measurement. This project will develop global illumination techniques for isosurfaces and field lines in 3D data, with applications to medical data and engineering data.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">83898</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">83898</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1022" target="n1023">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Intelligent Storage Systems for Object-Oriented Programs</data>
      <data key="e_abstract">Object-oriented programming is the mechanism of choice for implementing high-end applications. However, the architectures supporting these applications continue to be biased toward array- rather than object-based&lt;br/&gt;paradigms, where proximity of storage layout does not necessarily imply contemporaneous access.&lt;br/&gt;Object-oriented programs do exhibit repeated patterns of storage access. Thus, a dynamic approach that can &lt;br/&gt;facilitate intelligent pre-fetching of data into Caches or TLB&apos;s can better support object-oriented programs.&lt;br/&gt;&lt;br/&gt;This research investigates the use of intelligent storage systems, such as Intelligent RAM (IRAM) and Processor in Memory (PIM) devices, to improve the performance of object-oriented programs in the following three ways. Storage management functions, such as allocation and garbage collection, are migrated to intelligent memory devices using algorithms that are not only efficient in execution time, but also simple in logic design. Storage prefetch functions, such as memory forwarding and jump-pointers, are migrated away from the CPU and its cache and into the intelligent storage system. Storage access idioms are captured, compressed, and sent to the intelligent storage system for execution.&lt;br/&gt;&lt;br/&gt;The result of this research is the liberation of the CPU and its data cache from the overhead associated with the dynamic, garbage-collected storage of modern object-oriented languages.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81214</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">81214</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1025" target="n1026">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">An Algorithmic Evaluation of Optical Interconnection Networks</data>
      <data key="e_abstract">Parallel architectures based on the optical interconnect technology are becoming more and more popular as reflected by the numerous optical computers that have been proposed in recent years. Optical computers have been shown to possess superior interconnect properties compared to their electrical counterparts. These architectures offer the potential of building affordable machines operating at extremely high speeds.&lt;br/&gt;&lt;br/&gt;The development of algorithms for the proposed optical architectures is complicated by the fact that some models support pipelined data transfer, other models have a heterogeneous interconnect topology, and yet other models use an asymmetric topology. Although algorithms have been designed for some of these models, this development is in its infancy. The developed algorithms are mainly for a limited set of fundamental problems, and even this level of development has been done for only a few of the proposed architectures. Further, the developed algorithms assume that the size of the architecture is a function of the problem size. This assumption is clearly invalid in practice. Typically, the problem size will be much larger than the size of the architecture. An important question is if the optical architecture algorithms that have been developed so far are scalable. That is, can they be efficiently extended to solve problems whose size is considerably larger than the machine size. In this project the base of known efficient algorithms for optical architectures will be significantly expanded. Special attention will be paid to scalable algorithms. A cross-architecture performance study from the algorithms point of view will be performed. This study will be conducted in the domains of fundamental data operations and image processing. Scalability study has been conducted in the past by various researchers on models such as the PRAM, meshes with buses, and so on. But little has been done for the optical models. Also, many of the past works (for example, on meshes with buses) have studied the scalability issue by simulating a machine of one size on a machine of different size. Such studies are restricted in their applicability. In this project the general scalability issue will be investigated and hence the scalability of algorithms will be explored directly. In particular, the following question will be addressed: As the size of the input increases arbitrarily, how do the speedup and efficiency of the algorithm under concern change?&lt;br/&gt;&lt;br/&gt;The problem domains of interest are fundamental data operations such as sorting, routing, selection, etc. and image processing operations such as clustering, template matching, histogram, FFT, etc. These operations have been chosen since they span a number of application domains. At least three optical architectures, namely, Arrays with Reconfigurable Optical Buses (AROBs), Optical Transpose Interconnection Systems (OTISs), and Partitionable Optical Passive Star (POPS) computers, will be considered. The algorithms and algorithmic techniques to be developed in this project can be expected to be applicable to other architectures as well.</data>
      <data key="e_pgm">2860</data>
      <data key="e_label">9.9124e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.9124e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1028" target="n1029">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Wafer Oriented Trend Analysis for VLSI Test Opitmazation</data>
      <data key="e_abstract">The mainstream approach to improving the quality of IC testing is to develop better tests that target more realistic failure mechanisms. This project is investigating an orthogonal approach that promises significant improvements in the effectiveness of any given testing procedure by employing wafer based spatial test information as additional input in interpreting test results. This spatial information is useful in making the best possible judgement about circuit quality because of the widely observed clustering of defects on semiconductor wafers, and the fact that circuit parameters track closely for adjacent dice on the same wafer. Early results indicate the potential for defect level improvements up to an order of magnitude in screening for high quality circuits, and the possibility of screening out potential early-life failures, without expensive burn-in tests. On-going research aims at validating these findings on new, more voluminous, industrial test data, and also developing analytical models to estimate the test quality improvements. The new approach is also being investigated for developing neighborhood-based thresholds for optimally interpreting test results in non-Boolean test approaches, such as IDDQ and very low voltage testing.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">9.91239e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.91239e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n180" target="n1031">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Multiscale Hierarchical Analysis of Protein Structure and Dynamics</data>
      <data key="e_abstract">The rapid growth in the number of experimentally determined protein structures has accentuated&lt;br/&gt;the need for theoretical and computational methods that can make this data useful. The proposed&lt;br/&gt;research aims to provide tools for important applications in computational molecular biology, such&lt;br/&gt;as protein engineering and structure-based drug design, as well as scientific explorations of&lt;br/&gt;structure-function relationships. This goal can be achieved through the ability to predict the&lt;br/&gt;conformational changes that occur when proteins and ligands interact. Protein engineering to alter&lt;br/&gt;enzyme specificity or to enhance stability under adverse conditions can also benefit from a&lt;br/&gt;predictive understanding of the motions involved in protein folding, unfolding, and catalysis. An&lt;br/&gt;attack on this wide range of challenging problems requires algorithmic breakthroughs, enormous&lt;br/&gt;computing power, and powerful techniques for visualization.&lt;br/&gt;&lt;br/&gt;Computational molecular biology uses information at the atomic level to study biologically&lt;br/&gt;relevant phenomena. Interpolating from the behavior of individual atoms to the collective behavior&lt;br/&gt;of complex biological molecules such as proteins presents a significant theoretical and&lt;br/&gt;computational challenge. Unfortunately, the available computational methods to study protein&lt;br/&gt;dynamics over long ranges of time are far from satisfactory due to a number of severe difficulties,&lt;br/&gt;including: (1) The time scales involved in theoretical/computational prediction span an enormous&lt;br/&gt;range: about fifteen orders of magnitude. (2) The conformation space of relevant protein shapes has&lt;br/&gt;very large dimension (&gt; 10000) and the corresponding energy functions are quite jagged, making&lt;br/&gt;calculations expensive and difficult. (3) Algorithms for comparing/manipulating molecular shape&lt;br/&gt;are expensive in terms of both implementation difficulty and computational time.&lt;br/&gt;&lt;br/&gt;To resolve these difficulties the investigators are applying a variety of ideas centered on the&lt;br/&gt;theme of multiscaling: for time scales, for energy landscapes, and for measuring structural&lt;br/&gt;similarity at different levels of resolution. Algorithm development in the proposed area requires a&lt;br/&gt;unique blend of interdisciplinary expertise that is found within this team of investigators. The team&lt;br/&gt;includes a theoretical chemist who is an expert in protein dynamics, a theoretical physicist who is an&lt;br/&gt;expert in molecular biology and biophysics, and three computer scientists who bring expertise in&lt;br/&gt;geometric and combinatorial algorithms at both theoretical and experimental levels. This combined&lt;br/&gt;expertise is reinforced by a supportive interdisciplinary environment and excellent parallel&lt;br/&gt;computing resources.&lt;br/&gt;&lt;br/&gt;This research has three primary goals: (1) to broaden the range of time-scales for which&lt;br/&gt;meaningful Molecular Dynamics simulations of proteins are possible; (2) to improve our&lt;br/&gt;understanding of protein conformation space and the associated energy functions through the use&lt;br/&gt;of hierarchical multiscaling techniques; and (3) to enable the processing of proteins-as-shapes to&lt;br/&gt;proceed as easily as proteins are now processed as strings (over the 20-symbol alphabet of amino&lt;br/&gt;acids). Advances in these areas will significantly improve protein understanding, allowing&lt;br/&gt;computational experiments involving protein dynamics over wide ranges of time. Such improved&lt;br/&gt;computational abilities can potentially lead to important advances in the understanding of biology&lt;br/&gt;and the design of medicinal drugs.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">9.98852e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98852e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1031" target="n1033">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Multiscale Hierarchical Analysis of Protein Structure and Dynamics</data>
      <data key="e_abstract">The rapid growth in the number of experimentally determined protein structures has accentuated&lt;br/&gt;the need for theoretical and computational methods that can make this data useful. The proposed&lt;br/&gt;research aims to provide tools for important applications in computational molecular biology, such&lt;br/&gt;as protein engineering and structure-based drug design, as well as scientific explorations of&lt;br/&gt;structure-function relationships. This goal can be achieved through the ability to predict the&lt;br/&gt;conformational changes that occur when proteins and ligands interact. Protein engineering to alter&lt;br/&gt;enzyme specificity or to enhance stability under adverse conditions can also benefit from a&lt;br/&gt;predictive understanding of the motions involved in protein folding, unfolding, and catalysis. An&lt;br/&gt;attack on this wide range of challenging problems requires algorithmic breakthroughs, enormous&lt;br/&gt;computing power, and powerful techniques for visualization.&lt;br/&gt;&lt;br/&gt;Computational molecular biology uses information at the atomic level to study biologically&lt;br/&gt;relevant phenomena. Interpolating from the behavior of individual atoms to the collective behavior&lt;br/&gt;of complex biological molecules such as proteins presents a significant theoretical and&lt;br/&gt;computational challenge. Unfortunately, the available computational methods to study protein&lt;br/&gt;dynamics over long ranges of time are far from satisfactory due to a number of severe difficulties,&lt;br/&gt;including: (1) The time scales involved in theoretical/computational prediction span an enormous&lt;br/&gt;range: about fifteen orders of magnitude. (2) The conformation space of relevant protein shapes has&lt;br/&gt;very large dimension (&gt; 10000) and the corresponding energy functions are quite jagged, making&lt;br/&gt;calculations expensive and difficult. (3) Algorithms for comparing/manipulating molecular shape&lt;br/&gt;are expensive in terms of both implementation difficulty and computational time.&lt;br/&gt;&lt;br/&gt;To resolve these difficulties the investigators are applying a variety of ideas centered on the&lt;br/&gt;theme of multiscaling: for time scales, for energy landscapes, and for measuring structural&lt;br/&gt;similarity at different levels of resolution. Algorithm development in the proposed area requires a&lt;br/&gt;unique blend of interdisciplinary expertise that is found within this team of investigators. The team&lt;br/&gt;includes a theoretical chemist who is an expert in protein dynamics, a theoretical physicist who is an&lt;br/&gt;expert in molecular biology and biophysics, and three computer scientists who bring expertise in&lt;br/&gt;geometric and combinatorial algorithms at both theoretical and experimental levels. This combined&lt;br/&gt;expertise is reinforced by a supportive interdisciplinary environment and excellent parallel&lt;br/&gt;computing resources.&lt;br/&gt;&lt;br/&gt;This research has three primary goals: (1) to broaden the range of time-scales for which&lt;br/&gt;meaningful Molecular Dynamics simulations of proteins are possible; (2) to improve our&lt;br/&gt;understanding of protein conformation space and the associated energy functions through the use&lt;br/&gt;of hierarchical multiscaling techniques; and (3) to enable the processing of proteins-as-shapes to&lt;br/&gt;proceed as easily as proteins are now processed as strings (over the 20-symbol alphabet of amino&lt;br/&gt;acids). Advances in these areas will significantly improve protein understanding, allowing&lt;br/&gt;computational experiments involving protein dynamics over wide ranges of time. Such improved&lt;br/&gt;computational abilities can potentially lead to important advances in the understanding of biology&lt;br/&gt;and the design of medicinal drugs.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">9.98852e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98852e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n934" target="n1031">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Multiscale Hierarchical Analysis of Protein Structure and Dynamics</data>
      <data key="e_abstract">The rapid growth in the number of experimentally determined protein structures has accentuated&lt;br/&gt;the need for theoretical and computational methods that can make this data useful. The proposed&lt;br/&gt;research aims to provide tools for important applications in computational molecular biology, such&lt;br/&gt;as protein engineering and structure-based drug design, as well as scientific explorations of&lt;br/&gt;structure-function relationships. This goal can be achieved through the ability to predict the&lt;br/&gt;conformational changes that occur when proteins and ligands interact. Protein engineering to alter&lt;br/&gt;enzyme specificity or to enhance stability under adverse conditions can also benefit from a&lt;br/&gt;predictive understanding of the motions involved in protein folding, unfolding, and catalysis. An&lt;br/&gt;attack on this wide range of challenging problems requires algorithmic breakthroughs, enormous&lt;br/&gt;computing power, and powerful techniques for visualization.&lt;br/&gt;&lt;br/&gt;Computational molecular biology uses information at the atomic level to study biologically&lt;br/&gt;relevant phenomena. Interpolating from the behavior of individual atoms to the collective behavior&lt;br/&gt;of complex biological molecules such as proteins presents a significant theoretical and&lt;br/&gt;computational challenge. Unfortunately, the available computational methods to study protein&lt;br/&gt;dynamics over long ranges of time are far from satisfactory due to a number of severe difficulties,&lt;br/&gt;including: (1) The time scales involved in theoretical/computational prediction span an enormous&lt;br/&gt;range: about fifteen orders of magnitude. (2) The conformation space of relevant protein shapes has&lt;br/&gt;very large dimension (&gt; 10000) and the corresponding energy functions are quite jagged, making&lt;br/&gt;calculations expensive and difficult. (3) Algorithms for comparing/manipulating molecular shape&lt;br/&gt;are expensive in terms of both implementation difficulty and computational time.&lt;br/&gt;&lt;br/&gt;To resolve these difficulties the investigators are applying a variety of ideas centered on the&lt;br/&gt;theme of multiscaling: for time scales, for energy landscapes, and for measuring structural&lt;br/&gt;similarity at different levels of resolution. Algorithm development in the proposed area requires a&lt;br/&gt;unique blend of interdisciplinary expertise that is found within this team of investigators. The team&lt;br/&gt;includes a theoretical chemist who is an expert in protein dynamics, a theoretical physicist who is an&lt;br/&gt;expert in molecular biology and biophysics, and three computer scientists who bring expertise in&lt;br/&gt;geometric and combinatorial algorithms at both theoretical and experimental levels. This combined&lt;br/&gt;expertise is reinforced by a supportive interdisciplinary environment and excellent parallel&lt;br/&gt;computing resources.&lt;br/&gt;&lt;br/&gt;This research has three primary goals: (1) to broaden the range of time-scales for which&lt;br/&gt;meaningful Molecular Dynamics simulations of proteins are possible; (2) to improve our&lt;br/&gt;understanding of protein conformation space and the associated energy functions through the use&lt;br/&gt;of hierarchical multiscaling techniques; and (3) to enable the processing of proteins-as-shapes to&lt;br/&gt;proceed as easily as proteins are now processed as strings (over the 20-symbol alphabet of amino&lt;br/&gt;acids). Advances in these areas will significantly improve protein understanding, allowing&lt;br/&gt;computational experiments involving protein dynamics over wide ranges of time. Such improved&lt;br/&gt;computational abilities can potentially lead to important advances in the understanding of biology&lt;br/&gt;and the design of medicinal drugs.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">9.98852e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98852e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n180" target="n1033">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Multiscale Hierarchical Analysis of Protein Structure and Dynamics</data>
      <data key="e_abstract">The rapid growth in the number of experimentally determined protein structures has accentuated&lt;br/&gt;the need for theoretical and computational methods that can make this data useful. The proposed&lt;br/&gt;research aims to provide tools for important applications in computational molecular biology, such&lt;br/&gt;as protein engineering and structure-based drug design, as well as scientific explorations of&lt;br/&gt;structure-function relationships. This goal can be achieved through the ability to predict the&lt;br/&gt;conformational changes that occur when proteins and ligands interact. Protein engineering to alter&lt;br/&gt;enzyme specificity or to enhance stability under adverse conditions can also benefit from a&lt;br/&gt;predictive understanding of the motions involved in protein folding, unfolding, and catalysis. An&lt;br/&gt;attack on this wide range of challenging problems requires algorithmic breakthroughs, enormous&lt;br/&gt;computing power, and powerful techniques for visualization.&lt;br/&gt;&lt;br/&gt;Computational molecular biology uses information at the atomic level to study biologically&lt;br/&gt;relevant phenomena. Interpolating from the behavior of individual atoms to the collective behavior&lt;br/&gt;of complex biological molecules such as proteins presents a significant theoretical and&lt;br/&gt;computational challenge. Unfortunately, the available computational methods to study protein&lt;br/&gt;dynamics over long ranges of time are far from satisfactory due to a number of severe difficulties,&lt;br/&gt;including: (1) The time scales involved in theoretical/computational prediction span an enormous&lt;br/&gt;range: about fifteen orders of magnitude. (2) The conformation space of relevant protein shapes has&lt;br/&gt;very large dimension (&gt; 10000) and the corresponding energy functions are quite jagged, making&lt;br/&gt;calculations expensive and difficult. (3) Algorithms for comparing/manipulating molecular shape&lt;br/&gt;are expensive in terms of both implementation difficulty and computational time.&lt;br/&gt;&lt;br/&gt;To resolve these difficulties the investigators are applying a variety of ideas centered on the&lt;br/&gt;theme of multiscaling: for time scales, for energy landscapes, and for measuring structural&lt;br/&gt;similarity at different levels of resolution. Algorithm development in the proposed area requires a&lt;br/&gt;unique blend of interdisciplinary expertise that is found within this team of investigators. The team&lt;br/&gt;includes a theoretical chemist who is an expert in protein dynamics, a theoretical physicist who is an&lt;br/&gt;expert in molecular biology and biophysics, and three computer scientists who bring expertise in&lt;br/&gt;geometric and combinatorial algorithms at both theoretical and experimental levels. This combined&lt;br/&gt;expertise is reinforced by a supportive interdisciplinary environment and excellent parallel&lt;br/&gt;computing resources.&lt;br/&gt;&lt;br/&gt;This research has three primary goals: (1) to broaden the range of time-scales for which&lt;br/&gt;meaningful Molecular Dynamics simulations of proteins are possible; (2) to improve our&lt;br/&gt;understanding of protein conformation space and the associated energy functions through the use&lt;br/&gt;of hierarchical multiscaling techniques; and (3) to enable the processing of proteins-as-shapes to&lt;br/&gt;proceed as easily as proteins are now processed as strings (over the 20-symbol alphabet of amino&lt;br/&gt;acids). Advances in these areas will significantly improve protein understanding, allowing&lt;br/&gt;computational experiments involving protein dynamics over wide ranges of time. Such improved&lt;br/&gt;computational abilities can potentially lead to important advances in the understanding of biology&lt;br/&gt;and the design of medicinal drugs.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">9.98852e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98852e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n180" target="n934">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Multiscale Hierarchical Analysis of Protein Structure and Dynamics</data>
      <data key="e_abstract">The rapid growth in the number of experimentally determined protein structures has accentuated&lt;br/&gt;the need for theoretical and computational methods that can make this data useful. The proposed&lt;br/&gt;research aims to provide tools for important applications in computational molecular biology, such&lt;br/&gt;as protein engineering and structure-based drug design, as well as scientific explorations of&lt;br/&gt;structure-function relationships. This goal can be achieved through the ability to predict the&lt;br/&gt;conformational changes that occur when proteins and ligands interact. Protein engineering to alter&lt;br/&gt;enzyme specificity or to enhance stability under adverse conditions can also benefit from a&lt;br/&gt;predictive understanding of the motions involved in protein folding, unfolding, and catalysis. An&lt;br/&gt;attack on this wide range of challenging problems requires algorithmic breakthroughs, enormous&lt;br/&gt;computing power, and powerful techniques for visualization.&lt;br/&gt;&lt;br/&gt;Computational molecular biology uses information at the atomic level to study biologically&lt;br/&gt;relevant phenomena. Interpolating from the behavior of individual atoms to the collective behavior&lt;br/&gt;of complex biological molecules such as proteins presents a significant theoretical and&lt;br/&gt;computational challenge. Unfortunately, the available computational methods to study protein&lt;br/&gt;dynamics over long ranges of time are far from satisfactory due to a number of severe difficulties,&lt;br/&gt;including: (1) The time scales involved in theoretical/computational prediction span an enormous&lt;br/&gt;range: about fifteen orders of magnitude. (2) The conformation space of relevant protein shapes has&lt;br/&gt;very large dimension (&gt; 10000) and the corresponding energy functions are quite jagged, making&lt;br/&gt;calculations expensive and difficult. (3) Algorithms for comparing/manipulating molecular shape&lt;br/&gt;are expensive in terms of both implementation difficulty and computational time.&lt;br/&gt;&lt;br/&gt;To resolve these difficulties the investigators are applying a variety of ideas centered on the&lt;br/&gt;theme of multiscaling: for time scales, for energy landscapes, and for measuring structural&lt;br/&gt;similarity at different levels of resolution. Algorithm development in the proposed area requires a&lt;br/&gt;unique blend of interdisciplinary expertise that is found within this team of investigators. The team&lt;br/&gt;includes a theoretical chemist who is an expert in protein dynamics, a theoretical physicist who is an&lt;br/&gt;expert in molecular biology and biophysics, and three computer scientists who bring expertise in&lt;br/&gt;geometric and combinatorial algorithms at both theoretical and experimental levels. This combined&lt;br/&gt;expertise is reinforced by a supportive interdisciplinary environment and excellent parallel&lt;br/&gt;computing resources.&lt;br/&gt;&lt;br/&gt;This research has three primary goals: (1) to broaden the range of time-scales for which&lt;br/&gt;meaningful Molecular Dynamics simulations of proteins are possible; (2) to improve our&lt;br/&gt;understanding of protein conformation space and the associated energy functions through the use&lt;br/&gt;of hierarchical multiscaling techniques; and (3) to enable the processing of proteins-as-shapes to&lt;br/&gt;proceed as easily as proteins are now processed as strings (over the 20-symbol alphabet of amino&lt;br/&gt;acids). Advances in these areas will significantly improve protein understanding, allowing&lt;br/&gt;computational experiments involving protein dynamics over wide ranges of time. Such improved&lt;br/&gt;computational abilities can potentially lead to important advances in the understanding of biology&lt;br/&gt;and the design of medicinal drugs.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">9.98852e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98852e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n934" target="n1033">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Multiscale Hierarchical Analysis of Protein Structure and Dynamics</data>
      <data key="e_abstract">The rapid growth in the number of experimentally determined protein structures has accentuated&lt;br/&gt;the need for theoretical and computational methods that can make this data useful. The proposed&lt;br/&gt;research aims to provide tools for important applications in computational molecular biology, such&lt;br/&gt;as protein engineering and structure-based drug design, as well as scientific explorations of&lt;br/&gt;structure-function relationships. This goal can be achieved through the ability to predict the&lt;br/&gt;conformational changes that occur when proteins and ligands interact. Protein engineering to alter&lt;br/&gt;enzyme specificity or to enhance stability under adverse conditions can also benefit from a&lt;br/&gt;predictive understanding of the motions involved in protein folding, unfolding, and catalysis. An&lt;br/&gt;attack on this wide range of challenging problems requires algorithmic breakthroughs, enormous&lt;br/&gt;computing power, and powerful techniques for visualization.&lt;br/&gt;&lt;br/&gt;Computational molecular biology uses information at the atomic level to study biologically&lt;br/&gt;relevant phenomena. Interpolating from the behavior of individual atoms to the collective behavior&lt;br/&gt;of complex biological molecules such as proteins presents a significant theoretical and&lt;br/&gt;computational challenge. Unfortunately, the available computational methods to study protein&lt;br/&gt;dynamics over long ranges of time are far from satisfactory due to a number of severe difficulties,&lt;br/&gt;including: (1) The time scales involved in theoretical/computational prediction span an enormous&lt;br/&gt;range: about fifteen orders of magnitude. (2) The conformation space of relevant protein shapes has&lt;br/&gt;very large dimension (&gt; 10000) and the corresponding energy functions are quite jagged, making&lt;br/&gt;calculations expensive and difficult. (3) Algorithms for comparing/manipulating molecular shape&lt;br/&gt;are expensive in terms of both implementation difficulty and computational time.&lt;br/&gt;&lt;br/&gt;To resolve these difficulties the investigators are applying a variety of ideas centered on the&lt;br/&gt;theme of multiscaling: for time scales, for energy landscapes, and for measuring structural&lt;br/&gt;similarity at different levels of resolution. Algorithm development in the proposed area requires a&lt;br/&gt;unique blend of interdisciplinary expertise that is found within this team of investigators. The team&lt;br/&gt;includes a theoretical chemist who is an expert in protein dynamics, a theoretical physicist who is an&lt;br/&gt;expert in molecular biology and biophysics, and three computer scientists who bring expertise in&lt;br/&gt;geometric and combinatorial algorithms at both theoretical and experimental levels. This combined&lt;br/&gt;expertise is reinforced by a supportive interdisciplinary environment and excellent parallel&lt;br/&gt;computing resources.&lt;br/&gt;&lt;br/&gt;This research has three primary goals: (1) to broaden the range of time-scales for which&lt;br/&gt;meaningful Molecular Dynamics simulations of proteins are possible; (2) to improve our&lt;br/&gt;understanding of protein conformation space and the associated energy functions through the use&lt;br/&gt;of hierarchical multiscaling techniques; and (3) to enable the processing of proteins-as-shapes to&lt;br/&gt;proceed as easily as proteins are now processed as strings (over the 20-symbol alphabet of amino&lt;br/&gt;acids). Advances in these areas will significantly improve protein understanding, allowing&lt;br/&gt;computational experiments involving protein dynamics over wide ranges of time. Such improved&lt;br/&gt;computational abilities can potentially lead to important advances in the understanding of biology&lt;br/&gt;and the design of medicinal drugs.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">9.98852e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98852e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1035" target="n1036">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Collaborative Research for a National Center for Empirical Software Engineering Research</data>
      <data key="e_abstract">A necessary step towards the goal of building more reliable software systems, on time and within budget, is to establish an institutionalized empirical discipline for understanding causal relationships among the processes, components, and technologies that affect the building of systems. As in the physical and natural sciences, experimentation in software engineering requires a community with support for collaboration, experimental replication and refinement, and sharing of experimental data and results.&lt;br/&gt;For these reasons the Center for Empirical Software Engineering Research (CESER) undertakes original empirical research and is developing a prototype system for sharing and evolving the results of such research with a community of affiliated researchers and practitioners. CESER develops and refines techniques to increase the descriptive and predictive power of empirical models, and studies specific software development technologies to enable industrial organizations to understand the benefits and drawbacks of those technologies in their specific context. The Center provides courses and symposia on empirical methodologies and results, and assists the use of empirical knowledge in software engineering education. The Center&apos;s initial focus is on empirical studies of software COTS integration and software quality improvement phenomenology.&lt;br/&gt;The center is initially organized as a collaborative effort among the University of Maryland, the Fraunhofer Center - Maryland, the University of Southern California, the University of Nebraska at Lincoln, and Mississippi State University.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86078</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86078</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1037" target="n1038">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Virtual Instruments: Scalable Software Instruments for the Grid</data>
      <data key="e_abstract">Ensembles of distributed communication, computation, and storage resources, also known as &quot;Computational Grids&quot;, are emerging as a critical platform for high-performance computing. Grids are used effectively to support runs of distributed applications at a large enough scale to provide new disciplinary results to their developers. Researchers in almost every field of science and engineering are particularly interested in a class of applications particularly well suited to the Grid, scientific simulations where many parameterized instances of a give computation are performed. The development of accessible, efficient, fault-tolerant Grid-enabled versions of simulation software will enable disciplinary scientists to investigate wide-ranging scenarios and to obtain new results orders of magnitude faster than is currently possible.&lt;br/&gt;&lt;br/&gt;Many scientists would like to view large-scale simulations as software instruments that support some level of user interaction. This would be effective only if simulations can be deployed easily and controlled dynamically, i.e. if the computation can be steered. A traditional scenario is for the user to steer the simulation based on partial results that evolve continuously during execution. The partial results provide an increasingly refined indicator of the final results of the simulation and can be used to identify mid-execution which parameter sets are most promising. Given the potential of wide-area, federated Grid environments to deliver the aggregate computational power, data storage and dissemination facilities for large-scale simulations, and the need for scientists to steer such computations, it is increasingly important to develop performance-efficient and steerable software instruments that target the Grid. This project will address the significant computer science problems that arise from the need to support steerable scientific simulations in large-scale Grid environments.&lt;br/&gt;&lt;br/&gt;The project will design, develop, and prototype a virtual software instrument as a vehicle for designing and prototyping scalable, steerable scientific simulations for the Grid. It will use a Monte Carlo simulation program, MCell, as a prototype application for development and testing of the virtual instrument. The virtual instrument itself will consist of a set of software modules, libraries, interfaces, and steering-sensitive scheduling algorithms. The project will have impact on both the computer science and disciplinary science communities. It will foster new research in computer science through the development of event models, performance models, data management strategies, and adaptive scheduling and steering algorithms. It will also enable domain scientists to obtain new results in neuroscience.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86092</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86092</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1037" target="n1039">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Virtual Instruments: Scalable Software Instruments for the Grid</data>
      <data key="e_abstract">Ensembles of distributed communication, computation, and storage resources, also known as &quot;Computational Grids&quot;, are emerging as a critical platform for high-performance computing. Grids are used effectively to support runs of distributed applications at a large enough scale to provide new disciplinary results to their developers. Researchers in almost every field of science and engineering are particularly interested in a class of applications particularly well suited to the Grid, scientific simulations where many parameterized instances of a give computation are performed. The development of accessible, efficient, fault-tolerant Grid-enabled versions of simulation software will enable disciplinary scientists to investigate wide-ranging scenarios and to obtain new results orders of magnitude faster than is currently possible.&lt;br/&gt;&lt;br/&gt;Many scientists would like to view large-scale simulations as software instruments that support some level of user interaction. This would be effective only if simulations can be deployed easily and controlled dynamically, i.e. if the computation can be steered. A traditional scenario is for the user to steer the simulation based on partial results that evolve continuously during execution. The partial results provide an increasingly refined indicator of the final results of the simulation and can be used to identify mid-execution which parameter sets are most promising. Given the potential of wide-area, federated Grid environments to deliver the aggregate computational power, data storage and dissemination facilities for large-scale simulations, and the need for scientists to steer such computations, it is increasingly important to develop performance-efficient and steerable software instruments that target the Grid. This project will address the significant computer science problems that arise from the need to support steerable scientific simulations in large-scale Grid environments.&lt;br/&gt;&lt;br/&gt;The project will design, develop, and prototype a virtual software instrument as a vehicle for designing and prototyping scalable, steerable scientific simulations for the Grid. It will use a Monte Carlo simulation program, MCell, as a prototype application for development and testing of the virtual instrument. The virtual instrument itself will consist of a set of software modules, libraries, interfaces, and steering-sensitive scheduling algorithms. The project will have impact on both the computer science and disciplinary science communities. It will foster new research in computer science through the development of event models, performance models, data management strategies, and adaptive scheduling and steering algorithms. It will also enable domain scientists to obtain new results in neuroscience.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86092</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86092</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1038" target="n1039">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Virtual Instruments: Scalable Software Instruments for the Grid</data>
      <data key="e_abstract">Ensembles of distributed communication, computation, and storage resources, also known as &quot;Computational Grids&quot;, are emerging as a critical platform for high-performance computing. Grids are used effectively to support runs of distributed applications at a large enough scale to provide new disciplinary results to their developers. Researchers in almost every field of science and engineering are particularly interested in a class of applications particularly well suited to the Grid, scientific simulations where many parameterized instances of a give computation are performed. The development of accessible, efficient, fault-tolerant Grid-enabled versions of simulation software will enable disciplinary scientists to investigate wide-ranging scenarios and to obtain new results orders of magnitude faster than is currently possible.&lt;br/&gt;&lt;br/&gt;Many scientists would like to view large-scale simulations as software instruments that support some level of user interaction. This would be effective only if simulations can be deployed easily and controlled dynamically, i.e. if the computation can be steered. A traditional scenario is for the user to steer the simulation based on partial results that evolve continuously during execution. The partial results provide an increasingly refined indicator of the final results of the simulation and can be used to identify mid-execution which parameter sets are most promising. Given the potential of wide-area, federated Grid environments to deliver the aggregate computational power, data storage and dissemination facilities for large-scale simulations, and the need for scientists to steer such computations, it is increasingly important to develop performance-efficient and steerable software instruments that target the Grid. This project will address the significant computer science problems that arise from the need to support steerable scientific simulations in large-scale Grid environments.&lt;br/&gt;&lt;br/&gt;The project will design, develop, and prototype a virtual software instrument as a vehicle for designing and prototyping scalable, steerable scientific simulations for the Grid. It will use a Monte Carlo simulation program, MCell, as a prototype application for development and testing of the virtual instrument. The virtual instrument itself will consist of a set of software modules, libraries, interfaces, and steering-sensitive scheduling algorithms. The project will have impact on both the computer science and disciplinary science communities. It will foster new research in computer science through the development of event models, performance models, data management strategies, and adaptive scheduling and steering algorithms. It will also enable domain scientists to obtain new results in neuroscience.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86092</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86092</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1040" target="n1041">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Data Centers - Managing Data with Profiles</data>
      <data key="e_abstract">This research addresses the problem of adding data management facilities to inherently autonomous, distributed information sources such as those that occur in the web. Here, by data management, is meant the allocation and structuring of resources to provide more responsive access to data for applications. In this kind of environment, data management must be superimposed through an independently controlled service that exists between the data sources and the applications. This is facilitated through the introduction of architecture based on data centers, a collection of machines that prestage and distribute data for its clients. Client applications submit profiles describing their overall data needs, and the data center gathers data and organizes it on behalf of their clients in order to provide efficient data access. This research explores systems issues and techniques for the design and operation of data centers. This includes the management of large numbers of profiles, heuristics for balancing the needs of large numbers of users against the available resources of the data center, and the efficient processing of future client data needs against the data that is managed by the data center.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86057</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86057</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1040" target="n1042">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Data Centers - Managing Data with Profiles</data>
      <data key="e_abstract">This research addresses the problem of adding data management facilities to inherently autonomous, distributed information sources such as those that occur in the web. Here, by data management, is meant the allocation and structuring of resources to provide more responsive access to data for applications. In this kind of environment, data management must be superimposed through an independently controlled service that exists between the data sources and the applications. This is facilitated through the introduction of architecture based on data centers, a collection of machines that prestage and distribute data for its clients. Client applications submit profiles describing their overall data needs, and the data center gathers data and organizes it on behalf of their clients in order to provide efficient data access. This research explores systems issues and techniques for the design and operation of data centers. This includes the management of large numbers of profiles, heuristics for balancing the needs of large numbers of users against the available resources of the data center, and the efficient processing of future client data needs against the data that is managed by the data center.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86057</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86057</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1040" target="n1043">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Data Centers - Managing Data with Profiles</data>
      <data key="e_abstract">This research addresses the problem of adding data management facilities to inherently autonomous, distributed information sources such as those that occur in the web. Here, by data management, is meant the allocation and structuring of resources to provide more responsive access to data for applications. In this kind of environment, data management must be superimposed through an independently controlled service that exists between the data sources and the applications. This is facilitated through the introduction of architecture based on data centers, a collection of machines that prestage and distribute data for its clients. Client applications submit profiles describing their overall data needs, and the data center gathers data and organizes it on behalf of their clients in order to provide efficient data access. This research explores systems issues and techniques for the design and operation of data centers. This includes the management of large numbers of profiles, heuristics for balancing the needs of large numbers of users against the available resources of the data center, and the efficient processing of future client data needs against the data that is managed by the data center.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86057</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86057</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1041" target="n1042">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Data Centers - Managing Data with Profiles</data>
      <data key="e_abstract">This research addresses the problem of adding data management facilities to inherently autonomous, distributed information sources such as those that occur in the web. Here, by data management, is meant the allocation and structuring of resources to provide more responsive access to data for applications. In this kind of environment, data management must be superimposed through an independently controlled service that exists between the data sources and the applications. This is facilitated through the introduction of architecture based on data centers, a collection of machines that prestage and distribute data for its clients. Client applications submit profiles describing their overall data needs, and the data center gathers data and organizes it on behalf of their clients in order to provide efficient data access. This research explores systems issues and techniques for the design and operation of data centers. This includes the management of large numbers of profiles, heuristics for balancing the needs of large numbers of users against the available resources of the data center, and the efficient processing of future client data needs against the data that is managed by the data center.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86057</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86057</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1041" target="n1043">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Data Centers - Managing Data with Profiles</data>
      <data key="e_abstract">This research addresses the problem of adding data management facilities to inherently autonomous, distributed information sources such as those that occur in the web. Here, by data management, is meant the allocation and structuring of resources to provide more responsive access to data for applications. In this kind of environment, data management must be superimposed through an independently controlled service that exists between the data sources and the applications. This is facilitated through the introduction of architecture based on data centers, a collection of machines that prestage and distribute data for its clients. Client applications submit profiles describing their overall data needs, and the data center gathers data and organizes it on behalf of their clients in order to provide efficient data access. This research explores systems issues and techniques for the design and operation of data centers. This includes the management of large numbers of profiles, heuristics for balancing the needs of large numbers of users against the available resources of the data center, and the efficient processing of future client data needs against the data that is managed by the data center.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86057</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86057</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1042" target="n1043">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Data Centers - Managing Data with Profiles</data>
      <data key="e_abstract">This research addresses the problem of adding data management facilities to inherently autonomous, distributed information sources such as those that occur in the web. Here, by data management, is meant the allocation and structuring of resources to provide more responsive access to data for applications. In this kind of environment, data management must be superimposed through an independently controlled service that exists between the data sources and the applications. This is facilitated through the introduction of architecture based on data centers, a collection of machines that prestage and distribute data for its clients. Client applications submit profiles describing their overall data needs, and the data center gathers data and organizes it on behalf of their clients in order to provide efficient data access. This research explores systems issues and techniques for the design and operation of data centers. This includes the management of large numbers of profiles, heuristics for balancing the needs of large numbers of users against the available resources of the data center, and the efficient processing of future client data needs against the data that is managed by the data center.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86057</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86057</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1044" target="n1045">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Computational Infrastructure for Microfluidic Systems with Applications to Biotechnology</data>
      <data key="e_abstract">The applications of microfluidic devices (which involve liquids moving in spaces measured in micrometers, i.e. millionths of a meter) are growing explosively. As a specific example, consider the development of microsystems for blood testing and screening. For consumers, one could envision devices available in drugstores that could perform genetic screening for conditions of concern to individuals. At a larger scale, use of such devices in blood banks could significantly reduce the time and blood lost in screening the 14 million pints of blood donated per year. Sample preparation is a critical bottleneck in the development of integrated miniature analytical systems, and it remains largely unaddressed. It is currently done outside the microsystem by mixing, shaking, and pipetting, because there are no effective integrated design method. Improved computational methods promise to allow integration and interconnection of microfluidics. This will have an effect analogous to automated methods for VLSI design on microelectronics; it will revolutionize the field.&lt;br/&gt;&lt;br/&gt;This project will develop a computational infrastructure for simulation and design of microfluidic systems involving non-Newtonian, micrometer/nanometer-scale flows dominated by surface-related phenomena. Computational tools and analytical tools will be developed and used to compare with theoretical and experimental results. The project emphasizes methods to deliver complex molecules to flow surfaces, to create surface reaction sites and to provide the components for molecular-scale mixing and dispensing. It will design, fabricate, and characterize both stationary and oscillating MEMS fluidic channels and surfaces to evaluate molecular-scale mixing, flow, delivery, and dispensing of complex biological fluids. The focus will be on surface dominated flow and reaction phenomena that can be scaled for delivery of single molecules to programmed reaction sites. Such surface-related phenomena should find broad application in making MEMS-based, &quot;chip-scale&quot; analytical instruments and &quot;biochips&quot;. The computational tools required to analyze and design such devices are currently nonexistent. This project brings together a team of computer scientists, numerical analysts, fluid dynamicists, experimentalists, and microscale process theoreticians who will collaborate closely on creating those tools and using them.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86061</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86061</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1044" target="n1046">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Computational Infrastructure for Microfluidic Systems with Applications to Biotechnology</data>
      <data key="e_abstract">The applications of microfluidic devices (which involve liquids moving in spaces measured in micrometers, i.e. millionths of a meter) are growing explosively. As a specific example, consider the development of microsystems for blood testing and screening. For consumers, one could envision devices available in drugstores that could perform genetic screening for conditions of concern to individuals. At a larger scale, use of such devices in blood banks could significantly reduce the time and blood lost in screening the 14 million pints of blood donated per year. Sample preparation is a critical bottleneck in the development of integrated miniature analytical systems, and it remains largely unaddressed. It is currently done outside the microsystem by mixing, shaking, and pipetting, because there are no effective integrated design method. Improved computational methods promise to allow integration and interconnection of microfluidics. This will have an effect analogous to automated methods for VLSI design on microelectronics; it will revolutionize the field.&lt;br/&gt;&lt;br/&gt;This project will develop a computational infrastructure for simulation and design of microfluidic systems involving non-Newtonian, micrometer/nanometer-scale flows dominated by surface-related phenomena. Computational tools and analytical tools will be developed and used to compare with theoretical and experimental results. The project emphasizes methods to deliver complex molecules to flow surfaces, to create surface reaction sites and to provide the components for molecular-scale mixing and dispensing. It will design, fabricate, and characterize both stationary and oscillating MEMS fluidic channels and surfaces to evaluate molecular-scale mixing, flow, delivery, and dispensing of complex biological fluids. The focus will be on surface dominated flow and reaction phenomena that can be scaled for delivery of single molecules to programmed reaction sites. Such surface-related phenomena should find broad application in making MEMS-based, &quot;chip-scale&quot; analytical instruments and &quot;biochips&quot;. The computational tools required to analyze and design such devices are currently nonexistent. This project brings together a team of computer scientists, numerical analysts, fluid dynamicists, experimentalists, and microscale process theoreticians who will collaborate closely on creating those tools and using them.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86061</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86061</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n861" target="n1044">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Computational Infrastructure for Microfluidic Systems with Applications to Biotechnology</data>
      <data key="e_abstract">The applications of microfluidic devices (which involve liquids moving in spaces measured in micrometers, i.e. millionths of a meter) are growing explosively. As a specific example, consider the development of microsystems for blood testing and screening. For consumers, one could envision devices available in drugstores that could perform genetic screening for conditions of concern to individuals. At a larger scale, use of such devices in blood banks could significantly reduce the time and blood lost in screening the 14 million pints of blood donated per year. Sample preparation is a critical bottleneck in the development of integrated miniature analytical systems, and it remains largely unaddressed. It is currently done outside the microsystem by mixing, shaking, and pipetting, because there are no effective integrated design method. Improved computational methods promise to allow integration and interconnection of microfluidics. This will have an effect analogous to automated methods for VLSI design on microelectronics; it will revolutionize the field.&lt;br/&gt;&lt;br/&gt;This project will develop a computational infrastructure for simulation and design of microfluidic systems involving non-Newtonian, micrometer/nanometer-scale flows dominated by surface-related phenomena. Computational tools and analytical tools will be developed and used to compare with theoretical and experimental results. The project emphasizes methods to deliver complex molecules to flow surfaces, to create surface reaction sites and to provide the components for molecular-scale mixing and dispensing. It will design, fabricate, and characterize both stationary and oscillating MEMS fluidic channels and surfaces to evaluate molecular-scale mixing, flow, delivery, and dispensing of complex biological fluids. The focus will be on surface dominated flow and reaction phenomena that can be scaled for delivery of single molecules to programmed reaction sites. Such surface-related phenomena should find broad application in making MEMS-based, &quot;chip-scale&quot; analytical instruments and &quot;biochips&quot;. The computational tools required to analyze and design such devices are currently nonexistent. This project brings together a team of computer scientists, numerical analysts, fluid dynamicists, experimentalists, and microscale process theoreticians who will collaborate closely on creating those tools and using them.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86061</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86061</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1044" target="n1048">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Computational Infrastructure for Microfluidic Systems with Applications to Biotechnology</data>
      <data key="e_abstract">The applications of microfluidic devices (which involve liquids moving in spaces measured in micrometers, i.e. millionths of a meter) are growing explosively. As a specific example, consider the development of microsystems for blood testing and screening. For consumers, one could envision devices available in drugstores that could perform genetic screening for conditions of concern to individuals. At a larger scale, use of such devices in blood banks could significantly reduce the time and blood lost in screening the 14 million pints of blood donated per year. Sample preparation is a critical bottleneck in the development of integrated miniature analytical systems, and it remains largely unaddressed. It is currently done outside the microsystem by mixing, shaking, and pipetting, because there are no effective integrated design method. Improved computational methods promise to allow integration and interconnection of microfluidics. This will have an effect analogous to automated methods for VLSI design on microelectronics; it will revolutionize the field.&lt;br/&gt;&lt;br/&gt;This project will develop a computational infrastructure for simulation and design of microfluidic systems involving non-Newtonian, micrometer/nanometer-scale flows dominated by surface-related phenomena. Computational tools and analytical tools will be developed and used to compare with theoretical and experimental results. The project emphasizes methods to deliver complex molecules to flow surfaces, to create surface reaction sites and to provide the components for molecular-scale mixing and dispensing. It will design, fabricate, and characterize both stationary and oscillating MEMS fluidic channels and surfaces to evaluate molecular-scale mixing, flow, delivery, and dispensing of complex biological fluids. The focus will be on surface dominated flow and reaction phenomena that can be scaled for delivery of single molecules to programmed reaction sites. Such surface-related phenomena should find broad application in making MEMS-based, &quot;chip-scale&quot; analytical instruments and &quot;biochips&quot;. The computational tools required to analyze and design such devices are currently nonexistent. This project brings together a team of computer scientists, numerical analysts, fluid dynamicists, experimentalists, and microscale process theoreticians who will collaborate closely on creating those tools and using them.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86061</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86061</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1045" target="n1046">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Computational Infrastructure for Microfluidic Systems with Applications to Biotechnology</data>
      <data key="e_abstract">The applications of microfluidic devices (which involve liquids moving in spaces measured in micrometers, i.e. millionths of a meter) are growing explosively. As a specific example, consider the development of microsystems for blood testing and screening. For consumers, one could envision devices available in drugstores that could perform genetic screening for conditions of concern to individuals. At a larger scale, use of such devices in blood banks could significantly reduce the time and blood lost in screening the 14 million pints of blood donated per year. Sample preparation is a critical bottleneck in the development of integrated miniature analytical systems, and it remains largely unaddressed. It is currently done outside the microsystem by mixing, shaking, and pipetting, because there are no effective integrated design method. Improved computational methods promise to allow integration and interconnection of microfluidics. This will have an effect analogous to automated methods for VLSI design on microelectronics; it will revolutionize the field.&lt;br/&gt;&lt;br/&gt;This project will develop a computational infrastructure for simulation and design of microfluidic systems involving non-Newtonian, micrometer/nanometer-scale flows dominated by surface-related phenomena. Computational tools and analytical tools will be developed and used to compare with theoretical and experimental results. The project emphasizes methods to deliver complex molecules to flow surfaces, to create surface reaction sites and to provide the components for molecular-scale mixing and dispensing. It will design, fabricate, and characterize both stationary and oscillating MEMS fluidic channels and surfaces to evaluate molecular-scale mixing, flow, delivery, and dispensing of complex biological fluids. The focus will be on surface dominated flow and reaction phenomena that can be scaled for delivery of single molecules to programmed reaction sites. Such surface-related phenomena should find broad application in making MEMS-based, &quot;chip-scale&quot; analytical instruments and &quot;biochips&quot;. The computational tools required to analyze and design such devices are currently nonexistent. This project brings together a team of computer scientists, numerical analysts, fluid dynamicists, experimentalists, and microscale process theoreticians who will collaborate closely on creating those tools and using them.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86061</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86061</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n861" target="n1045">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Computational Infrastructure for Microfluidic Systems with Applications to Biotechnology</data>
      <data key="e_abstract">The applications of microfluidic devices (which involve liquids moving in spaces measured in micrometers, i.e. millionths of a meter) are growing explosively. As a specific example, consider the development of microsystems for blood testing and screening. For consumers, one could envision devices available in drugstores that could perform genetic screening for conditions of concern to individuals. At a larger scale, use of such devices in blood banks could significantly reduce the time and blood lost in screening the 14 million pints of blood donated per year. Sample preparation is a critical bottleneck in the development of integrated miniature analytical systems, and it remains largely unaddressed. It is currently done outside the microsystem by mixing, shaking, and pipetting, because there are no effective integrated design method. Improved computational methods promise to allow integration and interconnection of microfluidics. This will have an effect analogous to automated methods for VLSI design on microelectronics; it will revolutionize the field.&lt;br/&gt;&lt;br/&gt;This project will develop a computational infrastructure for simulation and design of microfluidic systems involving non-Newtonian, micrometer/nanometer-scale flows dominated by surface-related phenomena. Computational tools and analytical tools will be developed and used to compare with theoretical and experimental results. The project emphasizes methods to deliver complex molecules to flow surfaces, to create surface reaction sites and to provide the components for molecular-scale mixing and dispensing. It will design, fabricate, and characterize both stationary and oscillating MEMS fluidic channels and surfaces to evaluate molecular-scale mixing, flow, delivery, and dispensing of complex biological fluids. The focus will be on surface dominated flow and reaction phenomena that can be scaled for delivery of single molecules to programmed reaction sites. Such surface-related phenomena should find broad application in making MEMS-based, &quot;chip-scale&quot; analytical instruments and &quot;biochips&quot;. The computational tools required to analyze and design such devices are currently nonexistent. This project brings together a team of computer scientists, numerical analysts, fluid dynamicists, experimentalists, and microscale process theoreticians who will collaborate closely on creating those tools and using them.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86061</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86061</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1045" target="n1048">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Computational Infrastructure for Microfluidic Systems with Applications to Biotechnology</data>
      <data key="e_abstract">The applications of microfluidic devices (which involve liquids moving in spaces measured in micrometers, i.e. millionths of a meter) are growing explosively. As a specific example, consider the development of microsystems for blood testing and screening. For consumers, one could envision devices available in drugstores that could perform genetic screening for conditions of concern to individuals. At a larger scale, use of such devices in blood banks could significantly reduce the time and blood lost in screening the 14 million pints of blood donated per year. Sample preparation is a critical bottleneck in the development of integrated miniature analytical systems, and it remains largely unaddressed. It is currently done outside the microsystem by mixing, shaking, and pipetting, because there are no effective integrated design method. Improved computational methods promise to allow integration and interconnection of microfluidics. This will have an effect analogous to automated methods for VLSI design on microelectronics; it will revolutionize the field.&lt;br/&gt;&lt;br/&gt;This project will develop a computational infrastructure for simulation and design of microfluidic systems involving non-Newtonian, micrometer/nanometer-scale flows dominated by surface-related phenomena. Computational tools and analytical tools will be developed and used to compare with theoretical and experimental results. The project emphasizes methods to deliver complex molecules to flow surfaces, to create surface reaction sites and to provide the components for molecular-scale mixing and dispensing. It will design, fabricate, and characterize both stationary and oscillating MEMS fluidic channels and surfaces to evaluate molecular-scale mixing, flow, delivery, and dispensing of complex biological fluids. The focus will be on surface dominated flow and reaction phenomena that can be scaled for delivery of single molecules to programmed reaction sites. Such surface-related phenomena should find broad application in making MEMS-based, &quot;chip-scale&quot; analytical instruments and &quot;biochips&quot;. The computational tools required to analyze and design such devices are currently nonexistent. This project brings together a team of computer scientists, numerical analysts, fluid dynamicists, experimentalists, and microscale process theoreticians who will collaborate closely on creating those tools and using them.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86061</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86061</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n861" target="n1046">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Computational Infrastructure for Microfluidic Systems with Applications to Biotechnology</data>
      <data key="e_abstract">The applications of microfluidic devices (which involve liquids moving in spaces measured in micrometers, i.e. millionths of a meter) are growing explosively. As a specific example, consider the development of microsystems for blood testing and screening. For consumers, one could envision devices available in drugstores that could perform genetic screening for conditions of concern to individuals. At a larger scale, use of such devices in blood banks could significantly reduce the time and blood lost in screening the 14 million pints of blood donated per year. Sample preparation is a critical bottleneck in the development of integrated miniature analytical systems, and it remains largely unaddressed. It is currently done outside the microsystem by mixing, shaking, and pipetting, because there are no effective integrated design method. Improved computational methods promise to allow integration and interconnection of microfluidics. This will have an effect analogous to automated methods for VLSI design on microelectronics; it will revolutionize the field.&lt;br/&gt;&lt;br/&gt;This project will develop a computational infrastructure for simulation and design of microfluidic systems involving non-Newtonian, micrometer/nanometer-scale flows dominated by surface-related phenomena. Computational tools and analytical tools will be developed and used to compare with theoretical and experimental results. The project emphasizes methods to deliver complex molecules to flow surfaces, to create surface reaction sites and to provide the components for molecular-scale mixing and dispensing. It will design, fabricate, and characterize both stationary and oscillating MEMS fluidic channels and surfaces to evaluate molecular-scale mixing, flow, delivery, and dispensing of complex biological fluids. The focus will be on surface dominated flow and reaction phenomena that can be scaled for delivery of single molecules to programmed reaction sites. Such surface-related phenomena should find broad application in making MEMS-based, &quot;chip-scale&quot; analytical instruments and &quot;biochips&quot;. The computational tools required to analyze and design such devices are currently nonexistent. This project brings together a team of computer scientists, numerical analysts, fluid dynamicists, experimentalists, and microscale process theoreticians who will collaborate closely on creating those tools and using them.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86061</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86061</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1046" target="n1048">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Computational Infrastructure for Microfluidic Systems with Applications to Biotechnology</data>
      <data key="e_abstract">The applications of microfluidic devices (which involve liquids moving in spaces measured in micrometers, i.e. millionths of a meter) are growing explosively. As a specific example, consider the development of microsystems for blood testing and screening. For consumers, one could envision devices available in drugstores that could perform genetic screening for conditions of concern to individuals. At a larger scale, use of such devices in blood banks could significantly reduce the time and blood lost in screening the 14 million pints of blood donated per year. Sample preparation is a critical bottleneck in the development of integrated miniature analytical systems, and it remains largely unaddressed. It is currently done outside the microsystem by mixing, shaking, and pipetting, because there are no effective integrated design method. Improved computational methods promise to allow integration and interconnection of microfluidics. This will have an effect analogous to automated methods for VLSI design on microelectronics; it will revolutionize the field.&lt;br/&gt;&lt;br/&gt;This project will develop a computational infrastructure for simulation and design of microfluidic systems involving non-Newtonian, micrometer/nanometer-scale flows dominated by surface-related phenomena. Computational tools and analytical tools will be developed and used to compare with theoretical and experimental results. The project emphasizes methods to deliver complex molecules to flow surfaces, to create surface reaction sites and to provide the components for molecular-scale mixing and dispensing. It will design, fabricate, and characterize both stationary and oscillating MEMS fluidic channels and surfaces to evaluate molecular-scale mixing, flow, delivery, and dispensing of complex biological fluids. The focus will be on surface dominated flow and reaction phenomena that can be scaled for delivery of single molecules to programmed reaction sites. Such surface-related phenomena should find broad application in making MEMS-based, &quot;chip-scale&quot; analytical instruments and &quot;biochips&quot;. The computational tools required to analyze and design such devices are currently nonexistent. This project brings together a team of computer scientists, numerical analysts, fluid dynamicists, experimentalists, and microscale process theoreticians who will collaborate closely on creating those tools and using them.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86061</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86061</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n861" target="n1048">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Computational Infrastructure for Microfluidic Systems with Applications to Biotechnology</data>
      <data key="e_abstract">The applications of microfluidic devices (which involve liquids moving in spaces measured in micrometers, i.e. millionths of a meter) are growing explosively. As a specific example, consider the development of microsystems for blood testing and screening. For consumers, one could envision devices available in drugstores that could perform genetic screening for conditions of concern to individuals. At a larger scale, use of such devices in blood banks could significantly reduce the time and blood lost in screening the 14 million pints of blood donated per year. Sample preparation is a critical bottleneck in the development of integrated miniature analytical systems, and it remains largely unaddressed. It is currently done outside the microsystem by mixing, shaking, and pipetting, because there are no effective integrated design method. Improved computational methods promise to allow integration and interconnection of microfluidics. This will have an effect analogous to automated methods for VLSI design on microelectronics; it will revolutionize the field.&lt;br/&gt;&lt;br/&gt;This project will develop a computational infrastructure for simulation and design of microfluidic systems involving non-Newtonian, micrometer/nanometer-scale flows dominated by surface-related phenomena. Computational tools and analytical tools will be developed and used to compare with theoretical and experimental results. The project emphasizes methods to deliver complex molecules to flow surfaces, to create surface reaction sites and to provide the components for molecular-scale mixing and dispensing. It will design, fabricate, and characterize both stationary and oscillating MEMS fluidic channels and surfaces to evaluate molecular-scale mixing, flow, delivery, and dispensing of complex biological fluids. The focus will be on surface dominated flow and reaction phenomena that can be scaled for delivery of single molecules to programmed reaction sites. Such surface-related phenomena should find broad application in making MEMS-based, &quot;chip-scale&quot; analytical instruments and &quot;biochips&quot;. The computational tools required to analyze and design such devices are currently nonexistent. This project brings together a team of computer scientists, numerical analysts, fluid dynamicists, experimentalists, and microscale process theoreticians who will collaborate closely on creating those tools and using them.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86061</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86061</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1049" target="n1050">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Visualization of Multi-valued Scientific Data: Applying Ideas from Art and Perceptual Psychology</data>
      <data key="e_abstract">Visualization of Multi-valued Scientific Data: Applying Ideas from Art and Perceptual Psychology&lt;br/&gt;&lt;br/&gt;This is a multi-disciplinary research project to discover new visualization tools for interacting with and understanding multi-valued volumes of scientific data and the physical phenomena they measure. The tools will be developed and evaluated in close collaboration with scientists in three disciplines: neurobiologists studying neural development and disease via biological imaging, computational flow researchers studying blood flow through arteries, and geographers using remote sensing for environmental monitoring and resource management. We will factor out common patterns from the problems in these multiple disciplines to develop interaction metaphors and visualization techniques that are generalizable and widely applicable.&lt;br/&gt;&lt;br/&gt;This project develops new visualization evaluation methodologies, an area that has only begun to be addressed. And it compares the effectiveness of visualization applications in several interactive and static computing and display environments including a 4-wall Cave, a 40&apos;x40&apos; virtual environment with a head mounted display, stereo head-tracked workbenches, desktop workstations, paper, and 3D rapid-prototyping output. Immersive environments will be studied&lt;br/&gt;because the value of these non-traditional working environments has not been established and because they present an opportunity to explore fundamentally different interaction metaphors. Comparisons will be performed for both interactive and static cases with appropriate technology determined for each application.&lt;br/&gt;&lt;br/&gt;This project brings together experience from art and perceptual psychology for inspiration. Through several centuries, artists have evolved a tradition of techniques to create visual representations for particular communication goals. Art history provides a language for understanding that knowledge. We will draw inspiration from painting, sculpture, drawing, and graphic design and apply these techniques to the scientific problems.&lt;br/&gt;&lt;br/&gt;Beyond inspiration, perceptual psychology also brings a second set of knowledge to bear on scientific visualization problems. Evaluating the effectiveness of visualization methods is difficult because, not only are the goals difficult to define and codify, tests that evaluate them meaningfully are difficult to design and execute. These evaluations are akin to evaluating how the human perceptual system works. Perceptual psychologists have been developing experiments for understanding perception for decades, and they will help develop methodology and expertise for evaluating visualization methods in close collaboration with biologists, fluids researchers, geographers, artists, and computer scientists.&lt;br/&gt;&lt;br/&gt;While many of the individual components of this project are important alone, the collaborative aspects are the most notable. Mining ideas from art and perception will suggest unusually innovative visualization ideas. The application of new visualization techniques and collaboration with researchers in other fields will provide us with a unique opportunity to validate the techniques and ensure that they are responsive to the needs of the scientific problems. Because the techniques will be developed with application to multiple disciplines, they are likely to find further application within these and other disciplines. The assembled team brings strengths in all of the disciplines and has already demonstrated a track record of collaborative work.&lt;br/&gt;&lt;br/&gt;The broader impact of the proposed research lies not only in the information technology arena, where new methods will help scientists in many disciplines to more effectively interact with and understand their data and gain insight about the physical phenomena they represent, but also in the specific scientific domains we will study. The study of blood flow could lead to improved understanding of and treatment for cardiovascular pathologies. An understanding of early neural development could enable new therapies for birth defects, genetic disorders, and other diseases. Remote sensing advances could provide more effective resource monitoring and permit widespread improvements in global quality of life.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86065</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86065</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n316" target="n1049">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Visualization of Multi-valued Scientific Data: Applying Ideas from Art and Perceptual Psychology</data>
      <data key="e_abstract">Visualization of Multi-valued Scientific Data: Applying Ideas from Art and Perceptual Psychology&lt;br/&gt;&lt;br/&gt;This is a multi-disciplinary research project to discover new visualization tools for interacting with and understanding multi-valued volumes of scientific data and the physical phenomena they measure. The tools will be developed and evaluated in close collaboration with scientists in three disciplines: neurobiologists studying neural development and disease via biological imaging, computational flow researchers studying blood flow through arteries, and geographers using remote sensing for environmental monitoring and resource management. We will factor out common patterns from the problems in these multiple disciplines to develop interaction metaphors and visualization techniques that are generalizable and widely applicable.&lt;br/&gt;&lt;br/&gt;This project develops new visualization evaluation methodologies, an area that has only begun to be addressed. And it compares the effectiveness of visualization applications in several interactive and static computing and display environments including a 4-wall Cave, a 40&apos;x40&apos; virtual environment with a head mounted display, stereo head-tracked workbenches, desktop workstations, paper, and 3D rapid-prototyping output. Immersive environments will be studied&lt;br/&gt;because the value of these non-traditional working environments has not been established and because they present an opportunity to explore fundamentally different interaction metaphors. Comparisons will be performed for both interactive and static cases with appropriate technology determined for each application.&lt;br/&gt;&lt;br/&gt;This project brings together experience from art and perceptual psychology for inspiration. Through several centuries, artists have evolved a tradition of techniques to create visual representations for particular communication goals. Art history provides a language for understanding that knowledge. We will draw inspiration from painting, sculpture, drawing, and graphic design and apply these techniques to the scientific problems.&lt;br/&gt;&lt;br/&gt;Beyond inspiration, perceptual psychology also brings a second set of knowledge to bear on scientific visualization problems. Evaluating the effectiveness of visualization methods is difficult because, not only are the goals difficult to define and codify, tests that evaluate them meaningfully are difficult to design and execute. These evaluations are akin to evaluating how the human perceptual system works. Perceptual psychologists have been developing experiments for understanding perception for decades, and they will help develop methodology and expertise for evaluating visualization methods in close collaboration with biologists, fluids researchers, geographers, artists, and computer scientists.&lt;br/&gt;&lt;br/&gt;While many of the individual components of this project are important alone, the collaborative aspects are the most notable. Mining ideas from art and perception will suggest unusually innovative visualization ideas. The application of new visualization techniques and collaboration with researchers in other fields will provide us with a unique opportunity to validate the techniques and ensure that they are responsive to the needs of the scientific problems. Because the techniques will be developed with application to multiple disciplines, they are likely to find further application within these and other disciplines. The assembled team brings strengths in all of the disciplines and has already demonstrated a track record of collaborative work.&lt;br/&gt;&lt;br/&gt;The broader impact of the proposed research lies not only in the information technology arena, where new methods will help scientists in many disciplines to more effectively interact with and understand their data and gain insight about the physical phenomena they represent, but also in the specific scientific domains we will study. The study of blood flow could lead to improved understanding of and treatment for cardiovascular pathologies. An understanding of early neural development could enable new therapies for birth defects, genetic disorders, and other diseases. Remote sensing advances could provide more effective resource monitoring and permit widespread improvements in global quality of life.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86065</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86065</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n316" target="n1050">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Visualization of Multi-valued Scientific Data: Applying Ideas from Art and Perceptual Psychology</data>
      <data key="e_abstract">Visualization of Multi-valued Scientific Data: Applying Ideas from Art and Perceptual Psychology&lt;br/&gt;&lt;br/&gt;This is a multi-disciplinary research project to discover new visualization tools for interacting with and understanding multi-valued volumes of scientific data and the physical phenomena they measure. The tools will be developed and evaluated in close collaboration with scientists in three disciplines: neurobiologists studying neural development and disease via biological imaging, computational flow researchers studying blood flow through arteries, and geographers using remote sensing for environmental monitoring and resource management. We will factor out common patterns from the problems in these multiple disciplines to develop interaction metaphors and visualization techniques that are generalizable and widely applicable.&lt;br/&gt;&lt;br/&gt;This project develops new visualization evaluation methodologies, an area that has only begun to be addressed. And it compares the effectiveness of visualization applications in several interactive and static computing and display environments including a 4-wall Cave, a 40&apos;x40&apos; virtual environment with a head mounted display, stereo head-tracked workbenches, desktop workstations, paper, and 3D rapid-prototyping output. Immersive environments will be studied&lt;br/&gt;because the value of these non-traditional working environments has not been established and because they present an opportunity to explore fundamentally different interaction metaphors. Comparisons will be performed for both interactive and static cases with appropriate technology determined for each application.&lt;br/&gt;&lt;br/&gt;This project brings together experience from art and perceptual psychology for inspiration. Through several centuries, artists have evolved a tradition of techniques to create visual representations for particular communication goals. Art history provides a language for understanding that knowledge. We will draw inspiration from painting, sculpture, drawing, and graphic design and apply these techniques to the scientific problems.&lt;br/&gt;&lt;br/&gt;Beyond inspiration, perceptual psychology also brings a second set of knowledge to bear on scientific visualization problems. Evaluating the effectiveness of visualization methods is difficult because, not only are the goals difficult to define and codify, tests that evaluate them meaningfully are difficult to design and execute. These evaluations are akin to evaluating how the human perceptual system works. Perceptual psychologists have been developing experiments for understanding perception for decades, and they will help develop methodology and expertise for evaluating visualization methods in close collaboration with biologists, fluids researchers, geographers, artists, and computer scientists.&lt;br/&gt;&lt;br/&gt;While many of the individual components of this project are important alone, the collaborative aspects are the most notable. Mining ideas from art and perception will suggest unusually innovative visualization ideas. The application of new visualization techniques and collaboration with researchers in other fields will provide us with a unique opportunity to validate the techniques and ensure that they are responsive to the needs of the scientific problems. Because the techniques will be developed with application to multiple disciplines, they are likely to find further application within these and other disciplines. The assembled team brings strengths in all of the disciplines and has already demonstrated a track record of collaborative work.&lt;br/&gt;&lt;br/&gt;The broader impact of the proposed research lies not only in the information technology arena, where new methods will help scientists in many disciplines to more effectively interact with and understand their data and gain insight about the physical phenomena they represent, but also in the specific scientific domains we will study. The study of blood flow could lead to improved understanding of and treatment for cardiovascular pathologies. An understanding of early neural development could enable new therapies for birth defects, genetic disorders, and other diseases. Remote sensing advances could provide more effective resource monitoring and permit widespread improvements in global quality of life.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86065</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86065</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n451" target="n1054">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: High-Speed Distributed Wireless Communication Networks</data>
      <data key="e_abstract">The proposed research studies the design of agile wireless networks that accommodate time&lt;br/&gt;variations in the communication channels, the information sources, and the network topology. The research will lead to design principles that, in addition to enabling more efficient use of the current cellular and PCS bands, will allow exploitation of frequency bands in the 10-100 GHz range to provide high-speed multimedia services for both indoor and outdoor applications. While the basic cellular paradigm of wireless access to a high-speed communication and computing backbone will be adhered to, nearly every other assumption in existing second-generation and projected third-generation cellular and PCS networks will be reexamined. Some of the significant differences from current designs are as follows. A dense network of base stations will provide connectivity despite the high path losses and sharp shadowing at higher frequency bands. Cells with well-defined boundaries may no longer exist, and mobile terminals will see a rapidly varying network topology. A variety of traffic classes, such as voice, data, and video, with diverse requirements regarding delay, loss, quality of reproduction, and number of potential receivers will be considered. Packetized transmission will be considered as a flexible means of supporting such multimedia applications so that current circuit-based cellular trunking is not applicable.&lt;br/&gt;A key element of the approach is to envision new applications and new overall system architec-&lt;br/&gt;tures. Modeling based on propagation studies and analysis of requirements for carried datastreams will be used both to continually revise the concept systems and to provide models for the design of algorithms for such things as joint implementation of source coding, channel estimation, interference suppression, routing and congestion control. The performance of the algorithms will be evaluated broadly, including aspects of implementation in VLSI as appropriate, and the results will lead to further revision and refinement of the overall system architecture. Many tradeoffs will be explored, such as the tradeoff_ between the performance of distributed soft detection and the network capacity needed to assemble the required information. The integrated research effort will be conducted by five overlapping research teams of University of Illinois faculty investigators and their students, organized around the following interdisciplinary&lt;br/&gt;projects:&lt;br/&gt;&lt;br/&gt;Concept Systems, Modeling, and Performance Limits&lt;br/&gt;&lt;br/&gt;Design Principles for Wireless Packet Networks&lt;br/&gt;&lt;br/&gt;Design for Time-Varying Channels&lt;br/&gt;&lt;br/&gt;Jointly Optimized Source Coding, Channel Coding, and Estimation&lt;br/&gt;&lt;br/&gt;VLSI Algorithms, Architectures, and Bounds&lt;br/&gt;The bulk of the requested funding will be used to support the students. Most of the faculty and&lt;br/&gt;students have adjacent offices within the Coordinated Science Laboratory, an environment carefully cultivated to promote intense interaction and collaboration. Regularly scheduled weekly meetings of the investigators are used to coordinate the research and identify opportunities for better integration of the design approach. An External Advisory Committee of key technical people from industry will help ensure that the research is focused on problems and issues likely to be important in the future. This proposal is to supplement NSF grant NSF CCR 79381 \An Integrated Exploration of Wireless Network Communication.&quot; That grant is funded at a level of $700K for three years beginning October 1, 1999, whereas the original funding request was for $2.5M over five years. Although the proposal was rated as highly competitive, the funding received covers only six students and no faculty time. Additional funding will greatly enhance our ability to achieve our research and educational goals. &lt;br/&gt;&lt;br/&gt;Keywords: Wireless networks, fading channels, multimedia communications</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85929</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85929</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n451" target="n648">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: High-Speed Distributed Wireless Communication Networks</data>
      <data key="e_abstract">The proposed research studies the design of agile wireless networks that accommodate time&lt;br/&gt;variations in the communication channels, the information sources, and the network topology. The research will lead to design principles that, in addition to enabling more efficient use of the current cellular and PCS bands, will allow exploitation of frequency bands in the 10-100 GHz range to provide high-speed multimedia services for both indoor and outdoor applications. While the basic cellular paradigm of wireless access to a high-speed communication and computing backbone will be adhered to, nearly every other assumption in existing second-generation and projected third-generation cellular and PCS networks will be reexamined. Some of the significant differences from current designs are as follows. A dense network of base stations will provide connectivity despite the high path losses and sharp shadowing at higher frequency bands. Cells with well-defined boundaries may no longer exist, and mobile terminals will see a rapidly varying network topology. A variety of traffic classes, such as voice, data, and video, with diverse requirements regarding delay, loss, quality of reproduction, and number of potential receivers will be considered. Packetized transmission will be considered as a flexible means of supporting such multimedia applications so that current circuit-based cellular trunking is not applicable.&lt;br/&gt;A key element of the approach is to envision new applications and new overall system architec-&lt;br/&gt;tures. Modeling based on propagation studies and analysis of requirements for carried datastreams will be used both to continually revise the concept systems and to provide models for the design of algorithms for such things as joint implementation of source coding, channel estimation, interference suppression, routing and congestion control. The performance of the algorithms will be evaluated broadly, including aspects of implementation in VLSI as appropriate, and the results will lead to further revision and refinement of the overall system architecture. Many tradeoffs will be explored, such as the tradeoff_ between the performance of distributed soft detection and the network capacity needed to assemble the required information. The integrated research effort will be conducted by five overlapping research teams of University of Illinois faculty investigators and their students, organized around the following interdisciplinary&lt;br/&gt;projects:&lt;br/&gt;&lt;br/&gt;Concept Systems, Modeling, and Performance Limits&lt;br/&gt;&lt;br/&gt;Design Principles for Wireless Packet Networks&lt;br/&gt;&lt;br/&gt;Design for Time-Varying Channels&lt;br/&gt;&lt;br/&gt;Jointly Optimized Source Coding, Channel Coding, and Estimation&lt;br/&gt;&lt;br/&gt;VLSI Algorithms, Architectures, and Bounds&lt;br/&gt;The bulk of the requested funding will be used to support the students. Most of the faculty and&lt;br/&gt;students have adjacent offices within the Coordinated Science Laboratory, an environment carefully cultivated to promote intense interaction and collaboration. Regularly scheduled weekly meetings of the investigators are used to coordinate the research and identify opportunities for better integration of the design approach. An External Advisory Committee of key technical people from industry will help ensure that the research is focused on problems and issues likely to be important in the future. This proposal is to supplement NSF grant NSF CCR 79381 \An Integrated Exploration of Wireless Network Communication.&quot; That grant is funded at a level of $700K for three years beginning October 1, 1999, whereas the original funding request was for $2.5M over five years. Although the proposal was rated as highly competitive, the funding received covers only six students and no faculty time. Additional funding will greatly enhance our ability to achieve our research and educational goals. &lt;br/&gt;&lt;br/&gt;Keywords: Wireless networks, fading channels, multimedia communications</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85929</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85929</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n451" target="n1056">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: High-Speed Distributed Wireless Communication Networks</data>
      <data key="e_abstract">The proposed research studies the design of agile wireless networks that accommodate time&lt;br/&gt;variations in the communication channels, the information sources, and the network topology. The research will lead to design principles that, in addition to enabling more efficient use of the current cellular and PCS bands, will allow exploitation of frequency bands in the 10-100 GHz range to provide high-speed multimedia services for both indoor and outdoor applications. While the basic cellular paradigm of wireless access to a high-speed communication and computing backbone will be adhered to, nearly every other assumption in existing second-generation and projected third-generation cellular and PCS networks will be reexamined. Some of the significant differences from current designs are as follows. A dense network of base stations will provide connectivity despite the high path losses and sharp shadowing at higher frequency bands. Cells with well-defined boundaries may no longer exist, and mobile terminals will see a rapidly varying network topology. A variety of traffic classes, such as voice, data, and video, with diverse requirements regarding delay, loss, quality of reproduction, and number of potential receivers will be considered. Packetized transmission will be considered as a flexible means of supporting such multimedia applications so that current circuit-based cellular trunking is not applicable.&lt;br/&gt;A key element of the approach is to envision new applications and new overall system architec-&lt;br/&gt;tures. Modeling based on propagation studies and analysis of requirements for carried datastreams will be used both to continually revise the concept systems and to provide models for the design of algorithms for such things as joint implementation of source coding, channel estimation, interference suppression, routing and congestion control. The performance of the algorithms will be evaluated broadly, including aspects of implementation in VLSI as appropriate, and the results will lead to further revision and refinement of the overall system architecture. Many tradeoffs will be explored, such as the tradeoff_ between the performance of distributed soft detection and the network capacity needed to assemble the required information. The integrated research effort will be conducted by five overlapping research teams of University of Illinois faculty investigators and their students, organized around the following interdisciplinary&lt;br/&gt;projects:&lt;br/&gt;&lt;br/&gt;Concept Systems, Modeling, and Performance Limits&lt;br/&gt;&lt;br/&gt;Design Principles for Wireless Packet Networks&lt;br/&gt;&lt;br/&gt;Design for Time-Varying Channels&lt;br/&gt;&lt;br/&gt;Jointly Optimized Source Coding, Channel Coding, and Estimation&lt;br/&gt;&lt;br/&gt;VLSI Algorithms, Architectures, and Bounds&lt;br/&gt;The bulk of the requested funding will be used to support the students. Most of the faculty and&lt;br/&gt;students have adjacent offices within the Coordinated Science Laboratory, an environment carefully cultivated to promote intense interaction and collaboration. Regularly scheduled weekly meetings of the investigators are used to coordinate the research and identify opportunities for better integration of the design approach. An External Advisory Committee of key technical people from industry will help ensure that the research is focused on problems and issues likely to be important in the future. This proposal is to supplement NSF grant NSF CCR 79381 \An Integrated Exploration of Wireless Network Communication.&quot; That grant is funded at a level of $700K for three years beginning October 1, 1999, whereas the original funding request was for $2.5M over five years. Although the proposal was rated as highly competitive, the funding received covers only six students and no faculty time. Additional funding will greatly enhance our ability to achieve our research and educational goals. &lt;br/&gt;&lt;br/&gt;Keywords: Wireless networks, fading channels, multimedia communications</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85929</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85929</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n451" target="n649">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: High-Speed Distributed Wireless Communication Networks</data>
      <data key="e_abstract">The proposed research studies the design of agile wireless networks that accommodate time&lt;br/&gt;variations in the communication channels, the information sources, and the network topology. The research will lead to design principles that, in addition to enabling more efficient use of the current cellular and PCS bands, will allow exploitation of frequency bands in the 10-100 GHz range to provide high-speed multimedia services for both indoor and outdoor applications. While the basic cellular paradigm of wireless access to a high-speed communication and computing backbone will be adhered to, nearly every other assumption in existing second-generation and projected third-generation cellular and PCS networks will be reexamined. Some of the significant differences from current designs are as follows. A dense network of base stations will provide connectivity despite the high path losses and sharp shadowing at higher frequency bands. Cells with well-defined boundaries may no longer exist, and mobile terminals will see a rapidly varying network topology. A variety of traffic classes, such as voice, data, and video, with diverse requirements regarding delay, loss, quality of reproduction, and number of potential receivers will be considered. Packetized transmission will be considered as a flexible means of supporting such multimedia applications so that current circuit-based cellular trunking is not applicable.&lt;br/&gt;A key element of the approach is to envision new applications and new overall system architec-&lt;br/&gt;tures. Modeling based on propagation studies and analysis of requirements for carried datastreams will be used both to continually revise the concept systems and to provide models for the design of algorithms for such things as joint implementation of source coding, channel estimation, interference suppression, routing and congestion control. The performance of the algorithms will be evaluated broadly, including aspects of implementation in VLSI as appropriate, and the results will lead to further revision and refinement of the overall system architecture. Many tradeoffs will be explored, such as the tradeoff_ between the performance of distributed soft detection and the network capacity needed to assemble the required information. The integrated research effort will be conducted by five overlapping research teams of University of Illinois faculty investigators and their students, organized around the following interdisciplinary&lt;br/&gt;projects:&lt;br/&gt;&lt;br/&gt;Concept Systems, Modeling, and Performance Limits&lt;br/&gt;&lt;br/&gt;Design Principles for Wireless Packet Networks&lt;br/&gt;&lt;br/&gt;Design for Time-Varying Channels&lt;br/&gt;&lt;br/&gt;Jointly Optimized Source Coding, Channel Coding, and Estimation&lt;br/&gt;&lt;br/&gt;VLSI Algorithms, Architectures, and Bounds&lt;br/&gt;The bulk of the requested funding will be used to support the students. Most of the faculty and&lt;br/&gt;students have adjacent offices within the Coordinated Science Laboratory, an environment carefully cultivated to promote intense interaction and collaboration. Regularly scheduled weekly meetings of the investigators are used to coordinate the research and identify opportunities for better integration of the design approach. An External Advisory Committee of key technical people from industry will help ensure that the research is focused on problems and issues likely to be important in the future. This proposal is to supplement NSF grant NSF CCR 79381 \An Integrated Exploration of Wireless Network Communication.&quot; That grant is funded at a level of $700K for three years beginning October 1, 1999, whereas the original funding request was for $2.5M over five years. Although the proposal was rated as highly competitive, the funding received covers only six students and no faculty time. Additional funding will greatly enhance our ability to achieve our research and educational goals. &lt;br/&gt;&lt;br/&gt;Keywords: Wireless networks, fading channels, multimedia communications</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85929</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85929</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n648" target="n1054">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: High-Speed Distributed Wireless Communication Networks</data>
      <data key="e_abstract">The proposed research studies the design of agile wireless networks that accommodate time&lt;br/&gt;variations in the communication channels, the information sources, and the network topology. The research will lead to design principles that, in addition to enabling more efficient use of the current cellular and PCS bands, will allow exploitation of frequency bands in the 10-100 GHz range to provide high-speed multimedia services for both indoor and outdoor applications. While the basic cellular paradigm of wireless access to a high-speed communication and computing backbone will be adhered to, nearly every other assumption in existing second-generation and projected third-generation cellular and PCS networks will be reexamined. Some of the significant differences from current designs are as follows. A dense network of base stations will provide connectivity despite the high path losses and sharp shadowing at higher frequency bands. Cells with well-defined boundaries may no longer exist, and mobile terminals will see a rapidly varying network topology. A variety of traffic classes, such as voice, data, and video, with diverse requirements regarding delay, loss, quality of reproduction, and number of potential receivers will be considered. Packetized transmission will be considered as a flexible means of supporting such multimedia applications so that current circuit-based cellular trunking is not applicable.&lt;br/&gt;A key element of the approach is to envision new applications and new overall system architec-&lt;br/&gt;tures. Modeling based on propagation studies and analysis of requirements for carried datastreams will be used both to continually revise the concept systems and to provide models for the design of algorithms for such things as joint implementation of source coding, channel estimation, interference suppression, routing and congestion control. The performance of the algorithms will be evaluated broadly, including aspects of implementation in VLSI as appropriate, and the results will lead to further revision and refinement of the overall system architecture. Many tradeoffs will be explored, such as the tradeoff_ between the performance of distributed soft detection and the network capacity needed to assemble the required information. The integrated research effort will be conducted by five overlapping research teams of University of Illinois faculty investigators and their students, organized around the following interdisciplinary&lt;br/&gt;projects:&lt;br/&gt;&lt;br/&gt;Concept Systems, Modeling, and Performance Limits&lt;br/&gt;&lt;br/&gt;Design Principles for Wireless Packet Networks&lt;br/&gt;&lt;br/&gt;Design for Time-Varying Channels&lt;br/&gt;&lt;br/&gt;Jointly Optimized Source Coding, Channel Coding, and Estimation&lt;br/&gt;&lt;br/&gt;VLSI Algorithms, Architectures, and Bounds&lt;br/&gt;The bulk of the requested funding will be used to support the students. Most of the faculty and&lt;br/&gt;students have adjacent offices within the Coordinated Science Laboratory, an environment carefully cultivated to promote intense interaction and collaboration. Regularly scheduled weekly meetings of the investigators are used to coordinate the research and identify opportunities for better integration of the design approach. An External Advisory Committee of key technical people from industry will help ensure that the research is focused on problems and issues likely to be important in the future. This proposal is to supplement NSF grant NSF CCR 79381 \An Integrated Exploration of Wireless Network Communication.&quot; That grant is funded at a level of $700K for three years beginning October 1, 1999, whereas the original funding request was for $2.5M over five years. Although the proposal was rated as highly competitive, the funding received covers only six students and no faculty time. Additional funding will greatly enhance our ability to achieve our research and educational goals. &lt;br/&gt;&lt;br/&gt;Keywords: Wireless networks, fading channels, multimedia communications</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85929</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85929</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1054" target="n1056">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: High-Speed Distributed Wireless Communication Networks</data>
      <data key="e_abstract">The proposed research studies the design of agile wireless networks that accommodate time&lt;br/&gt;variations in the communication channels, the information sources, and the network topology. The research will lead to design principles that, in addition to enabling more efficient use of the current cellular and PCS bands, will allow exploitation of frequency bands in the 10-100 GHz range to provide high-speed multimedia services for both indoor and outdoor applications. While the basic cellular paradigm of wireless access to a high-speed communication and computing backbone will be adhered to, nearly every other assumption in existing second-generation and projected third-generation cellular and PCS networks will be reexamined. Some of the significant differences from current designs are as follows. A dense network of base stations will provide connectivity despite the high path losses and sharp shadowing at higher frequency bands. Cells with well-defined boundaries may no longer exist, and mobile terminals will see a rapidly varying network topology. A variety of traffic classes, such as voice, data, and video, with diverse requirements regarding delay, loss, quality of reproduction, and number of potential receivers will be considered. Packetized transmission will be considered as a flexible means of supporting such multimedia applications so that current circuit-based cellular trunking is not applicable.&lt;br/&gt;A key element of the approach is to envision new applications and new overall system architec-&lt;br/&gt;tures. Modeling based on propagation studies and analysis of requirements for carried datastreams will be used both to continually revise the concept systems and to provide models for the design of algorithms for such things as joint implementation of source coding, channel estimation, interference suppression, routing and congestion control. The performance of the algorithms will be evaluated broadly, including aspects of implementation in VLSI as appropriate, and the results will lead to further revision and refinement of the overall system architecture. Many tradeoffs will be explored, such as the tradeoff_ between the performance of distributed soft detection and the network capacity needed to assemble the required information. The integrated research effort will be conducted by five overlapping research teams of University of Illinois faculty investigators and their students, organized around the following interdisciplinary&lt;br/&gt;projects:&lt;br/&gt;&lt;br/&gt;Concept Systems, Modeling, and Performance Limits&lt;br/&gt;&lt;br/&gt;Design Principles for Wireless Packet Networks&lt;br/&gt;&lt;br/&gt;Design for Time-Varying Channels&lt;br/&gt;&lt;br/&gt;Jointly Optimized Source Coding, Channel Coding, and Estimation&lt;br/&gt;&lt;br/&gt;VLSI Algorithms, Architectures, and Bounds&lt;br/&gt;The bulk of the requested funding will be used to support the students. Most of the faculty and&lt;br/&gt;students have adjacent offices within the Coordinated Science Laboratory, an environment carefully cultivated to promote intense interaction and collaboration. Regularly scheduled weekly meetings of the investigators are used to coordinate the research and identify opportunities for better integration of the design approach. An External Advisory Committee of key technical people from industry will help ensure that the research is focused on problems and issues likely to be important in the future. This proposal is to supplement NSF grant NSF CCR 79381 \An Integrated Exploration of Wireless Network Communication.&quot; That grant is funded at a level of $700K for three years beginning October 1, 1999, whereas the original funding request was for $2.5M over five years. Although the proposal was rated as highly competitive, the funding received covers only six students and no faculty time. Additional funding will greatly enhance our ability to achieve our research and educational goals. &lt;br/&gt;&lt;br/&gt;Keywords: Wireless networks, fading channels, multimedia communications</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85929</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85929</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n649" target="n1054">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: High-Speed Distributed Wireless Communication Networks</data>
      <data key="e_abstract">The proposed research studies the design of agile wireless networks that accommodate time&lt;br/&gt;variations in the communication channels, the information sources, and the network topology. The research will lead to design principles that, in addition to enabling more efficient use of the current cellular and PCS bands, will allow exploitation of frequency bands in the 10-100 GHz range to provide high-speed multimedia services for both indoor and outdoor applications. While the basic cellular paradigm of wireless access to a high-speed communication and computing backbone will be adhered to, nearly every other assumption in existing second-generation and projected third-generation cellular and PCS networks will be reexamined. Some of the significant differences from current designs are as follows. A dense network of base stations will provide connectivity despite the high path losses and sharp shadowing at higher frequency bands. Cells with well-defined boundaries may no longer exist, and mobile terminals will see a rapidly varying network topology. A variety of traffic classes, such as voice, data, and video, with diverse requirements regarding delay, loss, quality of reproduction, and number of potential receivers will be considered. Packetized transmission will be considered as a flexible means of supporting such multimedia applications so that current circuit-based cellular trunking is not applicable.&lt;br/&gt;A key element of the approach is to envision new applications and new overall system architec-&lt;br/&gt;tures. Modeling based on propagation studies and analysis of requirements for carried datastreams will be used both to continually revise the concept systems and to provide models for the design of algorithms for such things as joint implementation of source coding, channel estimation, interference suppression, routing and congestion control. The performance of the algorithms will be evaluated broadly, including aspects of implementation in VLSI as appropriate, and the results will lead to further revision and refinement of the overall system architecture. Many tradeoffs will be explored, such as the tradeoff_ between the performance of distributed soft detection and the network capacity needed to assemble the required information. The integrated research effort will be conducted by five overlapping research teams of University of Illinois faculty investigators and their students, organized around the following interdisciplinary&lt;br/&gt;projects:&lt;br/&gt;&lt;br/&gt;Concept Systems, Modeling, and Performance Limits&lt;br/&gt;&lt;br/&gt;Design Principles for Wireless Packet Networks&lt;br/&gt;&lt;br/&gt;Design for Time-Varying Channels&lt;br/&gt;&lt;br/&gt;Jointly Optimized Source Coding, Channel Coding, and Estimation&lt;br/&gt;&lt;br/&gt;VLSI Algorithms, Architectures, and Bounds&lt;br/&gt;The bulk of the requested funding will be used to support the students. Most of the faculty and&lt;br/&gt;students have adjacent offices within the Coordinated Science Laboratory, an environment carefully cultivated to promote intense interaction and collaboration. Regularly scheduled weekly meetings of the investigators are used to coordinate the research and identify opportunities for better integration of the design approach. An External Advisory Committee of key technical people from industry will help ensure that the research is focused on problems and issues likely to be important in the future. This proposal is to supplement NSF grant NSF CCR 79381 \An Integrated Exploration of Wireless Network Communication.&quot; That grant is funded at a level of $700K for three years beginning October 1, 1999, whereas the original funding request was for $2.5M over five years. Although the proposal was rated as highly competitive, the funding received covers only six students and no faculty time. Additional funding will greatly enhance our ability to achieve our research and educational goals. &lt;br/&gt;&lt;br/&gt;Keywords: Wireless networks, fading channels, multimedia communications</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85929</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85929</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n648" target="n1056">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: High-Speed Distributed Wireless Communication Networks</data>
      <data key="e_abstract">The proposed research studies the design of agile wireless networks that accommodate time&lt;br/&gt;variations in the communication channels, the information sources, and the network topology. The research will lead to design principles that, in addition to enabling more efficient use of the current cellular and PCS bands, will allow exploitation of frequency bands in the 10-100 GHz range to provide high-speed multimedia services for both indoor and outdoor applications. While the basic cellular paradigm of wireless access to a high-speed communication and computing backbone will be adhered to, nearly every other assumption in existing second-generation and projected third-generation cellular and PCS networks will be reexamined. Some of the significant differences from current designs are as follows. A dense network of base stations will provide connectivity despite the high path losses and sharp shadowing at higher frequency bands. Cells with well-defined boundaries may no longer exist, and mobile terminals will see a rapidly varying network topology. A variety of traffic classes, such as voice, data, and video, with diverse requirements regarding delay, loss, quality of reproduction, and number of potential receivers will be considered. Packetized transmission will be considered as a flexible means of supporting such multimedia applications so that current circuit-based cellular trunking is not applicable.&lt;br/&gt;A key element of the approach is to envision new applications and new overall system architec-&lt;br/&gt;tures. Modeling based on propagation studies and analysis of requirements for carried datastreams will be used both to continually revise the concept systems and to provide models for the design of algorithms for such things as joint implementation of source coding, channel estimation, interference suppression, routing and congestion control. The performance of the algorithms will be evaluated broadly, including aspects of implementation in VLSI as appropriate, and the results will lead to further revision and refinement of the overall system architecture. Many tradeoffs will be explored, such as the tradeoff_ between the performance of distributed soft detection and the network capacity needed to assemble the required information. The integrated research effort will be conducted by five overlapping research teams of University of Illinois faculty investigators and their students, organized around the following interdisciplinary&lt;br/&gt;projects:&lt;br/&gt;&lt;br/&gt;Concept Systems, Modeling, and Performance Limits&lt;br/&gt;&lt;br/&gt;Design Principles for Wireless Packet Networks&lt;br/&gt;&lt;br/&gt;Design for Time-Varying Channels&lt;br/&gt;&lt;br/&gt;Jointly Optimized Source Coding, Channel Coding, and Estimation&lt;br/&gt;&lt;br/&gt;VLSI Algorithms, Architectures, and Bounds&lt;br/&gt;The bulk of the requested funding will be used to support the students. Most of the faculty and&lt;br/&gt;students have adjacent offices within the Coordinated Science Laboratory, an environment carefully cultivated to promote intense interaction and collaboration. Regularly scheduled weekly meetings of the investigators are used to coordinate the research and identify opportunities for better integration of the design approach. An External Advisory Committee of key technical people from industry will help ensure that the research is focused on problems and issues likely to be important in the future. This proposal is to supplement NSF grant NSF CCR 79381 \An Integrated Exploration of Wireless Network Communication.&quot; That grant is funded at a level of $700K for three years beginning October 1, 1999, whereas the original funding request was for $2.5M over five years. Although the proposal was rated as highly competitive, the funding received covers only six students and no faculty time. Additional funding will greatly enhance our ability to achieve our research and educational goals. &lt;br/&gt;&lt;br/&gt;Keywords: Wireless networks, fading channels, multimedia communications</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85929</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85929</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n648" target="n649">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: High-Speed Distributed Wireless Communication Networks</data>
      <data key="e_abstract">The proposed research studies the design of agile wireless networks that accommodate time&lt;br/&gt;variations in the communication channels, the information sources, and the network topology. The research will lead to design principles that, in addition to enabling more efficient use of the current cellular and PCS bands, will allow exploitation of frequency bands in the 10-100 GHz range to provide high-speed multimedia services for both indoor and outdoor applications. While the basic cellular paradigm of wireless access to a high-speed communication and computing backbone will be adhered to, nearly every other assumption in existing second-generation and projected third-generation cellular and PCS networks will be reexamined. Some of the significant differences from current designs are as follows. A dense network of base stations will provide connectivity despite the high path losses and sharp shadowing at higher frequency bands. Cells with well-defined boundaries may no longer exist, and mobile terminals will see a rapidly varying network topology. A variety of traffic classes, such as voice, data, and video, with diverse requirements regarding delay, loss, quality of reproduction, and number of potential receivers will be considered. Packetized transmission will be considered as a flexible means of supporting such multimedia applications so that current circuit-based cellular trunking is not applicable.&lt;br/&gt;A key element of the approach is to envision new applications and new overall system architec-&lt;br/&gt;tures. Modeling based on propagation studies and analysis of requirements for carried datastreams will be used both to continually revise the concept systems and to provide models for the design of algorithms for such things as joint implementation of source coding, channel estimation, interference suppression, routing and congestion control. The performance of the algorithms will be evaluated broadly, including aspects of implementation in VLSI as appropriate, and the results will lead to further revision and refinement of the overall system architecture. Many tradeoffs will be explored, such as the tradeoff_ between the performance of distributed soft detection and the network capacity needed to assemble the required information. The integrated research effort will be conducted by five overlapping research teams of University of Illinois faculty investigators and their students, organized around the following interdisciplinary&lt;br/&gt;projects:&lt;br/&gt;&lt;br/&gt;Concept Systems, Modeling, and Performance Limits&lt;br/&gt;&lt;br/&gt;Design Principles for Wireless Packet Networks&lt;br/&gt;&lt;br/&gt;Design for Time-Varying Channels&lt;br/&gt;&lt;br/&gt;Jointly Optimized Source Coding, Channel Coding, and Estimation&lt;br/&gt;&lt;br/&gt;VLSI Algorithms, Architectures, and Bounds&lt;br/&gt;The bulk of the requested funding will be used to support the students. Most of the faculty and&lt;br/&gt;students have adjacent offices within the Coordinated Science Laboratory, an environment carefully cultivated to promote intense interaction and collaboration. Regularly scheduled weekly meetings of the investigators are used to coordinate the research and identify opportunities for better integration of the design approach. An External Advisory Committee of key technical people from industry will help ensure that the research is focused on problems and issues likely to be important in the future. This proposal is to supplement NSF grant NSF CCR 79381 \An Integrated Exploration of Wireless Network Communication.&quot; That grant is funded at a level of $700K for three years beginning October 1, 1999, whereas the original funding request was for $2.5M over five years. Although the proposal was rated as highly competitive, the funding received covers only six students and no faculty time. Additional funding will greatly enhance our ability to achieve our research and educational goals. &lt;br/&gt;&lt;br/&gt;Keywords: Wireless networks, fading channels, multimedia communications</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85929</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85929</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n649" target="n1056">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: High-Speed Distributed Wireless Communication Networks</data>
      <data key="e_abstract">The proposed research studies the design of agile wireless networks that accommodate time&lt;br/&gt;variations in the communication channels, the information sources, and the network topology. The research will lead to design principles that, in addition to enabling more efficient use of the current cellular and PCS bands, will allow exploitation of frequency bands in the 10-100 GHz range to provide high-speed multimedia services for both indoor and outdoor applications. While the basic cellular paradigm of wireless access to a high-speed communication and computing backbone will be adhered to, nearly every other assumption in existing second-generation and projected third-generation cellular and PCS networks will be reexamined. Some of the significant differences from current designs are as follows. A dense network of base stations will provide connectivity despite the high path losses and sharp shadowing at higher frequency bands. Cells with well-defined boundaries may no longer exist, and mobile terminals will see a rapidly varying network topology. A variety of traffic classes, such as voice, data, and video, with diverse requirements regarding delay, loss, quality of reproduction, and number of potential receivers will be considered. Packetized transmission will be considered as a flexible means of supporting such multimedia applications so that current circuit-based cellular trunking is not applicable.&lt;br/&gt;A key element of the approach is to envision new applications and new overall system architec-&lt;br/&gt;tures. Modeling based on propagation studies and analysis of requirements for carried datastreams will be used both to continually revise the concept systems and to provide models for the design of algorithms for such things as joint implementation of source coding, channel estimation, interference suppression, routing and congestion control. The performance of the algorithms will be evaluated broadly, including aspects of implementation in VLSI as appropriate, and the results will lead to further revision and refinement of the overall system architecture. Many tradeoffs will be explored, such as the tradeoff_ between the performance of distributed soft detection and the network capacity needed to assemble the required information. The integrated research effort will be conducted by five overlapping research teams of University of Illinois faculty investigators and their students, organized around the following interdisciplinary&lt;br/&gt;projects:&lt;br/&gt;&lt;br/&gt;Concept Systems, Modeling, and Performance Limits&lt;br/&gt;&lt;br/&gt;Design Principles for Wireless Packet Networks&lt;br/&gt;&lt;br/&gt;Design for Time-Varying Channels&lt;br/&gt;&lt;br/&gt;Jointly Optimized Source Coding, Channel Coding, and Estimation&lt;br/&gt;&lt;br/&gt;VLSI Algorithms, Architectures, and Bounds&lt;br/&gt;The bulk of the requested funding will be used to support the students. Most of the faculty and&lt;br/&gt;students have adjacent offices within the Coordinated Science Laboratory, an environment carefully cultivated to promote intense interaction and collaboration. Regularly scheduled weekly meetings of the investigators are used to coordinate the research and identify opportunities for better integration of the design approach. An External Advisory Committee of key technical people from industry will help ensure that the research is focused on problems and issues likely to be important in the future. This proposal is to supplement NSF grant NSF CCR 79381 \An Integrated Exploration of Wireless Network Communication.&quot; That grant is funded at a level of $700K for three years beginning October 1, 1999, whereas the original funding request was for $2.5M over five years. Although the proposal was rated as highly competitive, the funding received covers only six students and no faculty time. Additional funding will greatly enhance our ability to achieve our research and educational goals. &lt;br/&gt;&lt;br/&gt;Keywords: Wireless networks, fading channels, multimedia communications</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85929</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85929</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1058" target="n1059">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Quantum Computing Using Electrons on Helium Films</data>
      <data key="e_abstract">EIA-0085922&lt;br/&gt;Arnold J, Dahm&lt;br/&gt;Case Western Reserve University&lt;br/&gt;&lt;br/&gt;Title: ITR: QUANTUM COMPUTING USING ELECTRONS ON HELIUM FILMS&lt;br/&gt;&lt;br/&gt;This project is a combined experimental and theoretical research effort to manufacture and investigate a system of interacting quantum bits (qubits) based on electrons on a helium film which covers an array of micro electrodes, and to develop methods for controlling this system. In particular, the team is studying the lifetimes and coherence times of the excited state in configurations suitable for qubit operation, including effects of electron-electron interaction, in-plane confinement, and a magnetic field. Problems of broad physical interest, such as quantum localization and chaos in a controlled system with interacting excitations is also being investigated. The project is focused on 1) trapping the electrons over micro dots, 2) controlling the energy-level spacing of targeted electrons by the microdot potential, 3) selective writing of information on individual qubits by appropriately tailored pulses of resonant microwave radiation and microdot potential 4) logical operations on a system of two qubits and 5) reading out the quantum register by detecting single electrons released from the surface or determining the electron distribution over energy levels with underlying micro dots, with a hope of building a system with a large number of qubits to be used as a multi-qubit quantum computer. If successful, such a computer can be manufactured using mainly conventional technologies and can operate at a temperature accessible with commercial refrigerators.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85922</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85922</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1058" target="n1060">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Quantum Computing Using Electrons on Helium Films</data>
      <data key="e_abstract">EIA-0085922&lt;br/&gt;Arnold J, Dahm&lt;br/&gt;Case Western Reserve University&lt;br/&gt;&lt;br/&gt;Title: ITR: QUANTUM COMPUTING USING ELECTRONS ON HELIUM FILMS&lt;br/&gt;&lt;br/&gt;This project is a combined experimental and theoretical research effort to manufacture and investigate a system of interacting quantum bits (qubits) based on electrons on a helium film which covers an array of micro electrodes, and to develop methods for controlling this system. In particular, the team is studying the lifetimes and coherence times of the excited state in configurations suitable for qubit operation, including effects of electron-electron interaction, in-plane confinement, and a magnetic field. Problems of broad physical interest, such as quantum localization and chaos in a controlled system with interacting excitations is also being investigated. The project is focused on 1) trapping the electrons over micro dots, 2) controlling the energy-level spacing of targeted electrons by the microdot potential, 3) selective writing of information on individual qubits by appropriately tailored pulses of resonant microwave radiation and microdot potential 4) logical operations on a system of two qubits and 5) reading out the quantum register by detecting single electrons released from the surface or determining the electron distribution over energy levels with underlying micro dots, with a hope of building a system with a large number of qubits to be used as a multi-qubit quantum computer. If successful, such a computer can be manufactured using mainly conventional technologies and can operate at a temperature accessible with commercial refrigerators.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85922</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85922</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1059" target="n1060">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Quantum Computing Using Electrons on Helium Films</data>
      <data key="e_abstract">EIA-0085922&lt;br/&gt;Arnold J, Dahm&lt;br/&gt;Case Western Reserve University&lt;br/&gt;&lt;br/&gt;Title: ITR: QUANTUM COMPUTING USING ELECTRONS ON HELIUM FILMS&lt;br/&gt;&lt;br/&gt;This project is a combined experimental and theoretical research effort to manufacture and investigate a system of interacting quantum bits (qubits) based on electrons on a helium film which covers an array of micro electrodes, and to develop methods for controlling this system. In particular, the team is studying the lifetimes and coherence times of the excited state in configurations suitable for qubit operation, including effects of electron-electron interaction, in-plane confinement, and a magnetic field. Problems of broad physical interest, such as quantum localization and chaos in a controlled system with interacting excitations is also being investigated. The project is focused on 1) trapping the electrons over micro dots, 2) controlling the energy-level spacing of targeted electrons by the microdot potential, 3) selective writing of information on individual qubits by appropriately tailored pulses of resonant microwave radiation and microdot potential 4) logical operations on a system of two qubits and 5) reading out the quantum register by detecting single electrons released from the surface or determining the electron distribution over energy levels with underlying micro dots, with a hope of building a system with a large number of qubits to be used as a multi-qubit quantum computer. If successful, such a computer can be manufactured using mainly conventional technologies and can operate at a temperature accessible with commercial refrigerators.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85922</data>
      <data key="e_expirationDate">2009-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85922</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1061" target="n1062">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Investigation of a Model for Online Resource Creation and Sharing in Educational Settings</data>
      <data key="e_abstract">EIA-0085921&lt;br/&gt;Kortemeyer, Gerd&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;Information Technology Research: Investigation of a Model for Online Resource Creation and Sharing in Educational Settings&lt;br/&gt;&lt;br/&gt;The advent of widely accessible networked computer technology has opened&lt;br/&gt;new avenues for educators, yet the creation of appropriate resources for&lt;br/&gt;this medium is extremely time and effort intensive. To truly become more&lt;br/&gt;effective than a textbook, a resource needs to take advantage of its medium&lt;br/&gt;by being interactive and part of a learner-centered, adaptable and&lt;br/&gt;individualizing whole. This research project is designed to address&lt;br/&gt;questions of resource pooling and sharing across content areas. The&lt;br/&gt;investigators will incubate multi-institutional collaboration and bring&lt;br/&gt;together stakeholders to address content issues such as reuse,&lt;br/&gt;customization, online community building, quality, and effectiveness. An&lt;br/&gt;existing software system under development will be used as a model to&lt;br/&gt;support the formation and study of an online collaborative community,&lt;br/&gt;including workshops, conferences, support, evaluation, and dissemination.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">85921</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85921</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1061" target="n1063">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Investigation of a Model for Online Resource Creation and Sharing in Educational Settings</data>
      <data key="e_abstract">EIA-0085921&lt;br/&gt;Kortemeyer, Gerd&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;Information Technology Research: Investigation of a Model for Online Resource Creation and Sharing in Educational Settings&lt;br/&gt;&lt;br/&gt;The advent of widely accessible networked computer technology has opened&lt;br/&gt;new avenues for educators, yet the creation of appropriate resources for&lt;br/&gt;this medium is extremely time and effort intensive. To truly become more&lt;br/&gt;effective than a textbook, a resource needs to take advantage of its medium&lt;br/&gt;by being interactive and part of a learner-centered, adaptable and&lt;br/&gt;individualizing whole. This research project is designed to address&lt;br/&gt;questions of resource pooling and sharing across content areas. The&lt;br/&gt;investigators will incubate multi-institutional collaboration and bring&lt;br/&gt;together stakeholders to address content issues such as reuse,&lt;br/&gt;customization, online community building, quality, and effectiveness. An&lt;br/&gt;existing software system under development will be used as a model to&lt;br/&gt;support the formation and study of an online collaborative community,&lt;br/&gt;including workshops, conferences, support, evaluation, and dissemination.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">85921</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85921</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1061" target="n1064">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Investigation of a Model for Online Resource Creation and Sharing in Educational Settings</data>
      <data key="e_abstract">EIA-0085921&lt;br/&gt;Kortemeyer, Gerd&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;Information Technology Research: Investigation of a Model for Online Resource Creation and Sharing in Educational Settings&lt;br/&gt;&lt;br/&gt;The advent of widely accessible networked computer technology has opened&lt;br/&gt;new avenues for educators, yet the creation of appropriate resources for&lt;br/&gt;this medium is extremely time and effort intensive. To truly become more&lt;br/&gt;effective than a textbook, a resource needs to take advantage of its medium&lt;br/&gt;by being interactive and part of a learner-centered, adaptable and&lt;br/&gt;individualizing whole. This research project is designed to address&lt;br/&gt;questions of resource pooling and sharing across content areas. The&lt;br/&gt;investigators will incubate multi-institutional collaboration and bring&lt;br/&gt;together stakeholders to address content issues such as reuse,&lt;br/&gt;customization, online community building, quality, and effectiveness. An&lt;br/&gt;existing software system under development will be used as a model to&lt;br/&gt;support the formation and study of an online collaborative community,&lt;br/&gt;including workshops, conferences, support, evaluation, and dissemination.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">85921</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85921</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1061" target="n1065">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Investigation of a Model for Online Resource Creation and Sharing in Educational Settings</data>
      <data key="e_abstract">EIA-0085921&lt;br/&gt;Kortemeyer, Gerd&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;Information Technology Research: Investigation of a Model for Online Resource Creation and Sharing in Educational Settings&lt;br/&gt;&lt;br/&gt;The advent of widely accessible networked computer technology has opened&lt;br/&gt;new avenues for educators, yet the creation of appropriate resources for&lt;br/&gt;this medium is extremely time and effort intensive. To truly become more&lt;br/&gt;effective than a textbook, a resource needs to take advantage of its medium&lt;br/&gt;by being interactive and part of a learner-centered, adaptable and&lt;br/&gt;individualizing whole. This research project is designed to address&lt;br/&gt;questions of resource pooling and sharing across content areas. The&lt;br/&gt;investigators will incubate multi-institutional collaboration and bring&lt;br/&gt;together stakeholders to address content issues such as reuse,&lt;br/&gt;customization, online community building, quality, and effectiveness. An&lt;br/&gt;existing software system under development will be used as a model to&lt;br/&gt;support the formation and study of an online collaborative community,&lt;br/&gt;including workshops, conferences, support, evaluation, and dissemination.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">85921</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85921</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1062" target="n1063">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Investigation of a Model for Online Resource Creation and Sharing in Educational Settings</data>
      <data key="e_abstract">EIA-0085921&lt;br/&gt;Kortemeyer, Gerd&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;Information Technology Research: Investigation of a Model for Online Resource Creation and Sharing in Educational Settings&lt;br/&gt;&lt;br/&gt;The advent of widely accessible networked computer technology has opened&lt;br/&gt;new avenues for educators, yet the creation of appropriate resources for&lt;br/&gt;this medium is extremely time and effort intensive. To truly become more&lt;br/&gt;effective than a textbook, a resource needs to take advantage of its medium&lt;br/&gt;by being interactive and part of a learner-centered, adaptable and&lt;br/&gt;individualizing whole. This research project is designed to address&lt;br/&gt;questions of resource pooling and sharing across content areas. The&lt;br/&gt;investigators will incubate multi-institutional collaboration and bring&lt;br/&gt;together stakeholders to address content issues such as reuse,&lt;br/&gt;customization, online community building, quality, and effectiveness. An&lt;br/&gt;existing software system under development will be used as a model to&lt;br/&gt;support the formation and study of an online collaborative community,&lt;br/&gt;including workshops, conferences, support, evaluation, and dissemination.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">85921</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85921</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1062" target="n1064">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Investigation of a Model for Online Resource Creation and Sharing in Educational Settings</data>
      <data key="e_abstract">EIA-0085921&lt;br/&gt;Kortemeyer, Gerd&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;Information Technology Research: Investigation of a Model for Online Resource Creation and Sharing in Educational Settings&lt;br/&gt;&lt;br/&gt;The advent of widely accessible networked computer technology has opened&lt;br/&gt;new avenues for educators, yet the creation of appropriate resources for&lt;br/&gt;this medium is extremely time and effort intensive. To truly become more&lt;br/&gt;effective than a textbook, a resource needs to take advantage of its medium&lt;br/&gt;by being interactive and part of a learner-centered, adaptable and&lt;br/&gt;individualizing whole. This research project is designed to address&lt;br/&gt;questions of resource pooling and sharing across content areas. The&lt;br/&gt;investigators will incubate multi-institutional collaboration and bring&lt;br/&gt;together stakeholders to address content issues such as reuse,&lt;br/&gt;customization, online community building, quality, and effectiveness. An&lt;br/&gt;existing software system under development will be used as a model to&lt;br/&gt;support the formation and study of an online collaborative community,&lt;br/&gt;including workshops, conferences, support, evaluation, and dissemination.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">85921</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85921</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1062" target="n1065">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Investigation of a Model for Online Resource Creation and Sharing in Educational Settings</data>
      <data key="e_abstract">EIA-0085921&lt;br/&gt;Kortemeyer, Gerd&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;Information Technology Research: Investigation of a Model for Online Resource Creation and Sharing in Educational Settings&lt;br/&gt;&lt;br/&gt;The advent of widely accessible networked computer technology has opened&lt;br/&gt;new avenues for educators, yet the creation of appropriate resources for&lt;br/&gt;this medium is extremely time and effort intensive. To truly become more&lt;br/&gt;effective than a textbook, a resource needs to take advantage of its medium&lt;br/&gt;by being interactive and part of a learner-centered, adaptable and&lt;br/&gt;individualizing whole. This research project is designed to address&lt;br/&gt;questions of resource pooling and sharing across content areas. The&lt;br/&gt;investigators will incubate multi-institutional collaboration and bring&lt;br/&gt;together stakeholders to address content issues such as reuse,&lt;br/&gt;customization, online community building, quality, and effectiveness. An&lt;br/&gt;existing software system under development will be used as a model to&lt;br/&gt;support the formation and study of an online collaborative community,&lt;br/&gt;including workshops, conferences, support, evaluation, and dissemination.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">85921</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85921</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1063" target="n1064">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Investigation of a Model for Online Resource Creation and Sharing in Educational Settings</data>
      <data key="e_abstract">EIA-0085921&lt;br/&gt;Kortemeyer, Gerd&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;Information Technology Research: Investigation of a Model for Online Resource Creation and Sharing in Educational Settings&lt;br/&gt;&lt;br/&gt;The advent of widely accessible networked computer technology has opened&lt;br/&gt;new avenues for educators, yet the creation of appropriate resources for&lt;br/&gt;this medium is extremely time and effort intensive. To truly become more&lt;br/&gt;effective than a textbook, a resource needs to take advantage of its medium&lt;br/&gt;by being interactive and part of a learner-centered, adaptable and&lt;br/&gt;individualizing whole. This research project is designed to address&lt;br/&gt;questions of resource pooling and sharing across content areas. The&lt;br/&gt;investigators will incubate multi-institutional collaboration and bring&lt;br/&gt;together stakeholders to address content issues such as reuse,&lt;br/&gt;customization, online community building, quality, and effectiveness. An&lt;br/&gt;existing software system under development will be used as a model to&lt;br/&gt;support the formation and study of an online collaborative community,&lt;br/&gt;including workshops, conferences, support, evaluation, and dissemination.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">85921</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85921</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1063" target="n1065">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Investigation of a Model for Online Resource Creation and Sharing in Educational Settings</data>
      <data key="e_abstract">EIA-0085921&lt;br/&gt;Kortemeyer, Gerd&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;Information Technology Research: Investigation of a Model for Online Resource Creation and Sharing in Educational Settings&lt;br/&gt;&lt;br/&gt;The advent of widely accessible networked computer technology has opened&lt;br/&gt;new avenues for educators, yet the creation of appropriate resources for&lt;br/&gt;this medium is extremely time and effort intensive. To truly become more&lt;br/&gt;effective than a textbook, a resource needs to take advantage of its medium&lt;br/&gt;by being interactive and part of a learner-centered, adaptable and&lt;br/&gt;individualizing whole. This research project is designed to address&lt;br/&gt;questions of resource pooling and sharing across content areas. The&lt;br/&gt;investigators will incubate multi-institutional collaboration and bring&lt;br/&gt;together stakeholders to address content issues such as reuse,&lt;br/&gt;customization, online community building, quality, and effectiveness. An&lt;br/&gt;existing software system under development will be used as a model to&lt;br/&gt;support the formation and study of an online collaborative community,&lt;br/&gt;including workshops, conferences, support, evaluation, and dissemination.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">85921</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85921</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1064" target="n1065">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Investigation of a Model for Online Resource Creation and Sharing in Educational Settings</data>
      <data key="e_abstract">EIA-0085921&lt;br/&gt;Kortemeyer, Gerd&lt;br/&gt;Michigan State University&lt;br/&gt;&lt;br/&gt;Information Technology Research: Investigation of a Model for Online Resource Creation and Sharing in Educational Settings&lt;br/&gt;&lt;br/&gt;The advent of widely accessible networked computer technology has opened&lt;br/&gt;new avenues for educators, yet the creation of appropriate resources for&lt;br/&gt;this medium is extremely time and effort intensive. To truly become more&lt;br/&gt;effective than a textbook, a resource needs to take advantage of its medium&lt;br/&gt;by being interactive and part of a learner-centered, adaptable and&lt;br/&gt;individualizing whole. This research project is designed to address&lt;br/&gt;questions of resource pooling and sharing across content areas. The&lt;br/&gt;investigators will incubate multi-institutional collaboration and bring&lt;br/&gt;together stakeholders to address content issues such as reuse,&lt;br/&gt;customization, online community building, quality, and effectiveness. An&lt;br/&gt;existing software system under development will be used as a model to&lt;br/&gt;support the formation and study of an online collaborative community,&lt;br/&gt;including workshops, conferences, support, evaluation, and dissemination.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">85921</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85921</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1067" target="n1068">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Hierarchical and Reconfigurable Schemes for Distributed Control over Heterogeneous Networks</data>
      <data key="e_abstract">Research proposed here deals with the problem of reliable control of geographically distributed complex real-time systems over a heterogeneous communication network. It is aimed at developing thefoundations of network-based control, from theory to applications. The overall objectives are: &lt;br/&gt;&lt;br/&gt; the design, analysis, implementation, and performance characterization of distributed and decen-&lt;br/&gt;tralized control algorithms and middleware that are affected through hierarchical and heteroge-neous networks comprised of wired and wireless subnets, and&lt;br/&gt; the specification and implementation of network services and support required for the development and deployment of distributed control algorithms over hierarchical heterogeneous networks, and demonstration of efficient and fault tolerant remote control using such networks for a number of emerging commercial and scientific/engineering applications.&lt;br/&gt;Our research agenda will cover the following domains:&lt;br/&gt; Research toward a network based control theory that emphasizes and accounts for decentralized, distributed and delay aspects of information transmission dictated by speciffic network structures, and bandwidth limitations.&lt;br/&gt; Basic research to leverage the latest developments in distributed robust fault-tolerant control, and to build a new theory for multifaceted control of remote objects over heterogeneous networks, using also the framework of dynamic games.&lt;br/&gt; Development of dynamic and adaptive methods for representation of large systems and computation of associated control strategies, using of hierarchical, adaptive graphs and distributed agents.&lt;br/&gt; Design and implementation of algorithms and middleware that will interact with the host-node communication protocols and provide the necessary support for the implementation of coordinated distributed control applications. Furthermore, design and implementation of an embedded real-time operating system kernel which will support hard deadlines and stringent QoS guarantees.&lt;br/&gt; Development of a demonstration prototype to be deployed on a small-scale heterogeneous hierarchical network comprised of a wireless subnetwork and campus-wide nodes of local area and Internet hosts.&lt;br/&gt;In addition to the development of new analytical paradigms and approaches, a component of the research program is the development of reusable simulation and design software, so that the research output can be parlayed to other researchers, practitioners, and industry. We envision numerous future scenarios where the results from this research program will apply. Among these are satellite control, air traffic control, congestion control over highways, remote guidance of airplanes, power networks, electronic commerce, and remote surgery.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85917</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85917</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1067" target="n1069">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Hierarchical and Reconfigurable Schemes for Distributed Control over Heterogeneous Networks</data>
      <data key="e_abstract">Research proposed here deals with the problem of reliable control of geographically distributed complex real-time systems over a heterogeneous communication network. It is aimed at developing thefoundations of network-based control, from theory to applications. The overall objectives are: &lt;br/&gt;&lt;br/&gt; the design, analysis, implementation, and performance characterization of distributed and decen-&lt;br/&gt;tralized control algorithms and middleware that are affected through hierarchical and heteroge-neous networks comprised of wired and wireless subnets, and&lt;br/&gt; the specification and implementation of network services and support required for the development and deployment of distributed control algorithms over hierarchical heterogeneous networks, and demonstration of efficient and fault tolerant remote control using such networks for a number of emerging commercial and scientific/engineering applications.&lt;br/&gt;Our research agenda will cover the following domains:&lt;br/&gt; Research toward a network based control theory that emphasizes and accounts for decentralized, distributed and delay aspects of information transmission dictated by speciffic network structures, and bandwidth limitations.&lt;br/&gt; Basic research to leverage the latest developments in distributed robust fault-tolerant control, and to build a new theory for multifaceted control of remote objects over heterogeneous networks, using also the framework of dynamic games.&lt;br/&gt; Development of dynamic and adaptive methods for representation of large systems and computation of associated control strategies, using of hierarchical, adaptive graphs and distributed agents.&lt;br/&gt; Design and implementation of algorithms and middleware that will interact with the host-node communication protocols and provide the necessary support for the implementation of coordinated distributed control applications. Furthermore, design and implementation of an embedded real-time operating system kernel which will support hard deadlines and stringent QoS guarantees.&lt;br/&gt; Development of a demonstration prototype to be deployed on a small-scale heterogeneous hierarchical network comprised of a wireless subnetwork and campus-wide nodes of local area and Internet hosts.&lt;br/&gt;In addition to the development of new analytical paradigms and approaches, a component of the research program is the development of reusable simulation and design software, so that the research output can be parlayed to other researchers, practitioners, and industry. We envision numerous future scenarios where the results from this research program will apply. Among these are satellite control, air traffic control, congestion control over highways, remote guidance of airplanes, power networks, electronic commerce, and remote surgery.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85917</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85917</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1067" target="n1070">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Hierarchical and Reconfigurable Schemes for Distributed Control over Heterogeneous Networks</data>
      <data key="e_abstract">Research proposed here deals with the problem of reliable control of geographically distributed complex real-time systems over a heterogeneous communication network. It is aimed at developing thefoundations of network-based control, from theory to applications. The overall objectives are: &lt;br/&gt;&lt;br/&gt; the design, analysis, implementation, and performance characterization of distributed and decen-&lt;br/&gt;tralized control algorithms and middleware that are affected through hierarchical and heteroge-neous networks comprised of wired and wireless subnets, and&lt;br/&gt; the specification and implementation of network services and support required for the development and deployment of distributed control algorithms over hierarchical heterogeneous networks, and demonstration of efficient and fault tolerant remote control using such networks for a number of emerging commercial and scientific/engineering applications.&lt;br/&gt;Our research agenda will cover the following domains:&lt;br/&gt; Research toward a network based control theory that emphasizes and accounts for decentralized, distributed and delay aspects of information transmission dictated by speciffic network structures, and bandwidth limitations.&lt;br/&gt; Basic research to leverage the latest developments in distributed robust fault-tolerant control, and to build a new theory for multifaceted control of remote objects over heterogeneous networks, using also the framework of dynamic games.&lt;br/&gt; Development of dynamic and adaptive methods for representation of large systems and computation of associated control strategies, using of hierarchical, adaptive graphs and distributed agents.&lt;br/&gt; Design and implementation of algorithms and middleware that will interact with the host-node communication protocols and provide the necessary support for the implementation of coordinated distributed control applications. Furthermore, design and implementation of an embedded real-time operating system kernel which will support hard deadlines and stringent QoS guarantees.&lt;br/&gt; Development of a demonstration prototype to be deployed on a small-scale heterogeneous hierarchical network comprised of a wireless subnetwork and campus-wide nodes of local area and Internet hosts.&lt;br/&gt;In addition to the development of new analytical paradigms and approaches, a component of the research program is the development of reusable simulation and design software, so that the research output can be parlayed to other researchers, practitioners, and industry. We envision numerous future scenarios where the results from this research program will apply. Among these are satellite control, air traffic control, congestion control over highways, remote guidance of airplanes, power networks, electronic commerce, and remote surgery.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85917</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85917</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1068" target="n1069">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Hierarchical and Reconfigurable Schemes for Distributed Control over Heterogeneous Networks</data>
      <data key="e_abstract">Research proposed here deals with the problem of reliable control of geographically distributed complex real-time systems over a heterogeneous communication network. It is aimed at developing thefoundations of network-based control, from theory to applications. The overall objectives are: &lt;br/&gt;&lt;br/&gt; the design, analysis, implementation, and performance characterization of distributed and decen-&lt;br/&gt;tralized control algorithms and middleware that are affected through hierarchical and heteroge-neous networks comprised of wired and wireless subnets, and&lt;br/&gt; the specification and implementation of network services and support required for the development and deployment of distributed control algorithms over hierarchical heterogeneous networks, and demonstration of efficient and fault tolerant remote control using such networks for a number of emerging commercial and scientific/engineering applications.&lt;br/&gt;Our research agenda will cover the following domains:&lt;br/&gt; Research toward a network based control theory that emphasizes and accounts for decentralized, distributed and delay aspects of information transmission dictated by speciffic network structures, and bandwidth limitations.&lt;br/&gt; Basic research to leverage the latest developments in distributed robust fault-tolerant control, and to build a new theory for multifaceted control of remote objects over heterogeneous networks, using also the framework of dynamic games.&lt;br/&gt; Development of dynamic and adaptive methods for representation of large systems and computation of associated control strategies, using of hierarchical, adaptive graphs and distributed agents.&lt;br/&gt; Design and implementation of algorithms and middleware that will interact with the host-node communication protocols and provide the necessary support for the implementation of coordinated distributed control applications. Furthermore, design and implementation of an embedded real-time operating system kernel which will support hard deadlines and stringent QoS guarantees.&lt;br/&gt; Development of a demonstration prototype to be deployed on a small-scale heterogeneous hierarchical network comprised of a wireless subnetwork and campus-wide nodes of local area and Internet hosts.&lt;br/&gt;In addition to the development of new analytical paradigms and approaches, a component of the research program is the development of reusable simulation and design software, so that the research output can be parlayed to other researchers, practitioners, and industry. We envision numerous future scenarios where the results from this research program will apply. Among these are satellite control, air traffic control, congestion control over highways, remote guidance of airplanes, power networks, electronic commerce, and remote surgery.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85917</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85917</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1068" target="n1070">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Hierarchical and Reconfigurable Schemes for Distributed Control over Heterogeneous Networks</data>
      <data key="e_abstract">Research proposed here deals with the problem of reliable control of geographically distributed complex real-time systems over a heterogeneous communication network. It is aimed at developing thefoundations of network-based control, from theory to applications. The overall objectives are: &lt;br/&gt;&lt;br/&gt; the design, analysis, implementation, and performance characterization of distributed and decen-&lt;br/&gt;tralized control algorithms and middleware that are affected through hierarchical and heteroge-neous networks comprised of wired and wireless subnets, and&lt;br/&gt; the specification and implementation of network services and support required for the development and deployment of distributed control algorithms over hierarchical heterogeneous networks, and demonstration of efficient and fault tolerant remote control using such networks for a number of emerging commercial and scientific/engineering applications.&lt;br/&gt;Our research agenda will cover the following domains:&lt;br/&gt; Research toward a network based control theory that emphasizes and accounts for decentralized, distributed and delay aspects of information transmission dictated by speciffic network structures, and bandwidth limitations.&lt;br/&gt; Basic research to leverage the latest developments in distributed robust fault-tolerant control, and to build a new theory for multifaceted control of remote objects over heterogeneous networks, using also the framework of dynamic games.&lt;br/&gt; Development of dynamic and adaptive methods for representation of large systems and computation of associated control strategies, using of hierarchical, adaptive graphs and distributed agents.&lt;br/&gt; Design and implementation of algorithms and middleware that will interact with the host-node communication protocols and provide the necessary support for the implementation of coordinated distributed control applications. Furthermore, design and implementation of an embedded real-time operating system kernel which will support hard deadlines and stringent QoS guarantees.&lt;br/&gt; Development of a demonstration prototype to be deployed on a small-scale heterogeneous hierarchical network comprised of a wireless subnetwork and campus-wide nodes of local area and Internet hosts.&lt;br/&gt;In addition to the development of new analytical paradigms and approaches, a component of the research program is the development of reusable simulation and design software, so that the research output can be parlayed to other researchers, practitioners, and industry. We envision numerous future scenarios where the results from this research program will apply. Among these are satellite control, air traffic control, congestion control over highways, remote guidance of airplanes, power networks, electronic commerce, and remote surgery.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85917</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85917</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1069" target="n1070">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Hierarchical and Reconfigurable Schemes for Distributed Control over Heterogeneous Networks</data>
      <data key="e_abstract">Research proposed here deals with the problem of reliable control of geographically distributed complex real-time systems over a heterogeneous communication network. It is aimed at developing thefoundations of network-based control, from theory to applications. The overall objectives are: &lt;br/&gt;&lt;br/&gt; the design, analysis, implementation, and performance characterization of distributed and decen-&lt;br/&gt;tralized control algorithms and middleware that are affected through hierarchical and heteroge-neous networks comprised of wired and wireless subnets, and&lt;br/&gt; the specification and implementation of network services and support required for the development and deployment of distributed control algorithms over hierarchical heterogeneous networks, and demonstration of efficient and fault tolerant remote control using such networks for a number of emerging commercial and scientific/engineering applications.&lt;br/&gt;Our research agenda will cover the following domains:&lt;br/&gt; Research toward a network based control theory that emphasizes and accounts for decentralized, distributed and delay aspects of information transmission dictated by speciffic network structures, and bandwidth limitations.&lt;br/&gt; Basic research to leverage the latest developments in distributed robust fault-tolerant control, and to build a new theory for multifaceted control of remote objects over heterogeneous networks, using also the framework of dynamic games.&lt;br/&gt; Development of dynamic and adaptive methods for representation of large systems and computation of associated control strategies, using of hierarchical, adaptive graphs and distributed agents.&lt;br/&gt; Design and implementation of algorithms and middleware that will interact with the host-node communication protocols and provide the necessary support for the implementation of coordinated distributed control applications. Furthermore, design and implementation of an embedded real-time operating system kernel which will support hard deadlines and stringent QoS guarantees.&lt;br/&gt; Development of a demonstration prototype to be deployed on a small-scale heterogeneous hierarchical network comprised of a wireless subnetwork and campus-wide nodes of local area and Internet hosts.&lt;br/&gt;In addition to the development of new analytical paradigms and approaches, a component of the research program is the development of reusable simulation and design software, so that the research output can be parlayed to other researchers, practitioners, and industry. We envision numerous future scenarios where the results from this research program will apply. Among these are satellite control, air traffic control, congestion control over highways, remote guidance of airplanes, power networks, electronic commerce, and remote surgery.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85917</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85917</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n555" target="n1072">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Computational Techniques for Applied Bioinformatics</data>
      <data key="e_abstract">This award provides support for a collaborative project involving two computer scientists and a plant geneticist who will develop new methods, efficient algorithms, and software tools for several important problems in the field of bioinformatics. This supported work includes research into computational paradigms such as quartet methods, interactive systems, and approximation algorithms as applied to the evolutionary analysis of gene sequences, gene duplication, and horizontal transfer events in the genomes of chloroplasts, a DNA-containing organelle found in all plants. Additional studies will examine the information content of genomes by improving and testing a recently developed sequence entropy estimator and a distance metric for genomic sequences. Work in this area will include the application of the improved methods to sequence data from the genomes of mitochondria, viruses, chloroplasts and bacteria. Other efforts will address the important problem of simultaneous multiple sequence alignment and evolutionary tree reconstruction. The multiple sequence alignment approaches to be developed are based on the use of conserved blocks that have few or no gaps, and multiple alignments within a constant band. Work in a fourth area will develop efficient algorithms for computing short and long interspersed nuclear elements (SINES and LINES) in genomic sequences of lengths up to billions of nucleotides. Because of the large amounts of data that must be analyzed, this will require the development or adaptation of appropriate external memory algorithms. &lt;br/&gt;&lt;br/&gt;Biological, biomedical and pharmaceutical research is undergoing a major revolution as new analytical technologies produce unprecedented amounts of genetic data. The exploration of this information is critically dependent upon the development of advanced computational and software techniques for data analysis, storage and retrieval. From this dependency, a new interdisciplinary research field, bioinformatics (or computational molecular biology) has emerged in recent years. The work supported through this award is expected to make both fundamental and applied contributions to the field. The fundamental research will explore and explicate new ideas and methods for solving algorithmic problems in bioinformatics and the applied research will involve the development and evaluation of software tools in the practice of plant genomics. Although the efforts are aimed at improving the understanding of the evolution of chloroplast genomes, the approaches should be readily extensible to analysis of all other genomes.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85910</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85910</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1074" target="n1075">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Taming the Data Flood: Systems that Evolve, are Available, and Maintainable (SEAM)</data>
      <data key="e_abstract">The problem of attaining peak performance--which has dominated the research agenda for the past 20 years--will be secondary to concerns of availability, maintainability, and evolutionary growth (AME) in the PostPC era, where computers must cope with the flood of new data and yet be much more dependable and maintainable. This project develops fault insertion techniques to test for graceful recovery from hardware and software failures; self-scrubbing data structures that check and repair themselves to improve software reliability; and the ability to isolate subsets of live systems to test AME in the field. A large-scale prototype is being constructed, with the help of industrial partners, to demonstrate these ideas. This research is being carried out in collaboration with Matthew Merzbacher of Mills College.&quot;</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85899</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85899</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n379" target="n1074">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Taming the Data Flood: Systems that Evolve, are Available, and Maintainable (SEAM)</data>
      <data key="e_abstract">The problem of attaining peak performance--which has dominated the research agenda for the past 20 years--will be secondary to concerns of availability, maintainability, and evolutionary growth (AME) in the PostPC era, where computers must cope with the flood of new data and yet be much more dependable and maintainable. This project develops fault insertion techniques to test for graceful recovery from hardware and software failures; self-scrubbing data structures that check and repair themselves to improve software reliability; and the ability to isolate subsets of live systems to test AME in the field. A large-scale prototype is being constructed, with the help of industrial partners, to demonstrate these ideas. This research is being carried out in collaboration with Matthew Merzbacher of Mills College.&quot;</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85899</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85899</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n379" target="n1075">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Taming the Data Flood: Systems that Evolve, are Available, and Maintainable (SEAM)</data>
      <data key="e_abstract">The problem of attaining peak performance--which has dominated the research agenda for the past 20 years--will be secondary to concerns of availability, maintainability, and evolutionary growth (AME) in the PostPC era, where computers must cope with the flood of new data and yet be much more dependable and maintainable. This project develops fault insertion techniques to test for graceful recovery from hardware and software failures; self-scrubbing data structures that check and repair themselves to improve software reliability; and the ability to isolate subsets of live systems to test AME in the field. A large-scale prototype is being constructed, with the help of industrial partners, to demonstrate these ideas. This research is being carried out in collaboration with Matthew Merzbacher of Mills College.&quot;</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85899</data>
      <data key="e_expirationDate">2005-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85899</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n648" target="n1077">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CRCD: A Cryptography Center for Research and Education</data>
      <data key="e_abstract">0088063&lt;br/&gt;Boston, Nigel&lt;br/&gt;University of Illinois at Urbana&lt;br/&gt;&lt;br/&gt;CRCD: A Cryptography Center for Research and Education&lt;br/&gt;&lt;br/&gt;This project is concerned with the field of cryptography. This university is establishing the Illinois Center for Cryptography and Information Protection (ICCIP), a multidisciplinary center focused on research and education in fields that influence information protection and are influenced by information protection techniques. This CRCD project lays the groundwork for the educational and curricular aspects of the center. Multidisciplinary groups of upper-level undergraduate students and graduate students (from engineering, computer science and mathematics) are organized into teams attacking problems in modern cryptography. This project also addresses the development of cryptography related courses that are cross-listed in mathematics, engineering, and computer science. The project involves industrial partners who contribute practical problems and interact with teams. Industry, government, and academe need employees with a broad range of skills who can work in this field in inter/multidisciplinary teams. For example, attacks on current crypto-systems can be sophisticated mathematical ones or direct physical ones. To counter such attacks takes concerted efforts from team members having different expertise in mathematics, engineering, and computing. The same holds true for the creation of new information protection schemes. Computing and engineering practitioners are faced with physical limitations of circuits, software, and devices in implementing mathematical solutions to information protection. This curriculum produces individuals with broad backgrounds and experience in working in teams on realistic problems in cryptography and the creation of novel collaborations that will advance technological developments much quicker than has historically been the case.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88063</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88063</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1077" target="n1079">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CRCD: A Cryptography Center for Research and Education</data>
      <data key="e_abstract">0088063&lt;br/&gt;Boston, Nigel&lt;br/&gt;University of Illinois at Urbana&lt;br/&gt;&lt;br/&gt;CRCD: A Cryptography Center for Research and Education&lt;br/&gt;&lt;br/&gt;This project is concerned with the field of cryptography. This university is establishing the Illinois Center for Cryptography and Information Protection (ICCIP), a multidisciplinary center focused on research and education in fields that influence information protection and are influenced by information protection techniques. This CRCD project lays the groundwork for the educational and curricular aspects of the center. Multidisciplinary groups of upper-level undergraduate students and graduate students (from engineering, computer science and mathematics) are organized into teams attacking problems in modern cryptography. This project also addresses the development of cryptography related courses that are cross-listed in mathematics, engineering, and computer science. The project involves industrial partners who contribute practical problems and interact with teams. Industry, government, and academe need employees with a broad range of skills who can work in this field in inter/multidisciplinary teams. For example, attacks on current crypto-systems can be sophisticated mathematical ones or direct physical ones. To counter such attacks takes concerted efforts from team members having different expertise in mathematics, engineering, and computing. The same holds true for the creation of new information protection schemes. Computing and engineering practitioners are faced with physical limitations of circuits, software, and devices in implementing mathematical solutions to information protection. This curriculum produces individuals with broad backgrounds and experience in working in teams on realistic problems in cryptography and the creation of novel collaborations that will advance technological developments much quicker than has historically been the case.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88063</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88063</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n648" target="n1079">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CRCD: A Cryptography Center for Research and Education</data>
      <data key="e_abstract">0088063&lt;br/&gt;Boston, Nigel&lt;br/&gt;University of Illinois at Urbana&lt;br/&gt;&lt;br/&gt;CRCD: A Cryptography Center for Research and Education&lt;br/&gt;&lt;br/&gt;This project is concerned with the field of cryptography. This university is establishing the Illinois Center for Cryptography and Information Protection (ICCIP), a multidisciplinary center focused on research and education in fields that influence information protection and are influenced by information protection techniques. This CRCD project lays the groundwork for the educational and curricular aspects of the center. Multidisciplinary groups of upper-level undergraduate students and graduate students (from engineering, computer science and mathematics) are organized into teams attacking problems in modern cryptography. This project also addresses the development of cryptography related courses that are cross-listed in mathematics, engineering, and computer science. The project involves industrial partners who contribute practical problems and interact with teams. Industry, government, and academe need employees with a broad range of skills who can work in this field in inter/multidisciplinary teams. For example, attacks on current crypto-systems can be sophisticated mathematical ones or direct physical ones. To counter such attacks takes concerted efforts from team members having different expertise in mathematics, engineering, and computing. The same holds true for the creation of new information protection schemes. Computing and engineering practitioners are faced with physical limitations of circuits, software, and devices in implementing mathematical solutions to information protection. This curriculum produces individuals with broad backgrounds and experience in working in teams on realistic problems in cryptography and the creation of novel collaborations that will advance technological developments much quicker than has historically been the case.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88063</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88063</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1080" target="n1081">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: HomeNetToo: Motivational, affective and cognitive factors and Internet use: Explaining the digital divide and the Internet paradox</data>
      <data key="e_abstract">HomeNetToo is a research project designed to address two fundamental questions: What causes people to use or not use the Internet? What effect does Internet use have on people? A model of Internet use is proposed that addresses these questions by considering motivational, affective, and cognitive factors as antecedents and consequences of Internet use. Participants in HomeNetToo will be low-income African American and European American families who will be introduced to the Internet as a communication tool or an information tool. On-line surveys and computer-logged measures of Internet use will be used to test hypotheses about the antecedents and consequences of Internet use over an 18-month trial. Relationships between cognitive style and interface design will also be examined. Results will have implications for how to reduce the digital divide, how to design user interfaces to accommodate diverse cognitive styles, and for identifying factors that influence whether Internet use will have desirable personal, social and professional consequences.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85348</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85348</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1080" target="n1082">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: HomeNetToo: Motivational, affective and cognitive factors and Internet use: Explaining the digital divide and the Internet paradox</data>
      <data key="e_abstract">HomeNetToo is a research project designed to address two fundamental questions: What causes people to use or not use the Internet? What effect does Internet use have on people? A model of Internet use is proposed that addresses these questions by considering motivational, affective, and cognitive factors as antecedents and consequences of Internet use. Participants in HomeNetToo will be low-income African American and European American families who will be introduced to the Internet as a communication tool or an information tool. On-line surveys and computer-logged measures of Internet use will be used to test hypotheses about the antecedents and consequences of Internet use over an 18-month trial. Relationships between cognitive style and interface design will also be examined. Results will have implications for how to reduce the digital divide, how to design user interfaces to accommodate diverse cognitive styles, and for identifying factors that influence whether Internet use will have desirable personal, social and professional consequences.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85348</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85348</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n138" target="n1080">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: HomeNetToo: Motivational, affective and cognitive factors and Internet use: Explaining the digital divide and the Internet paradox</data>
      <data key="e_abstract">HomeNetToo is a research project designed to address two fundamental questions: What causes people to use or not use the Internet? What effect does Internet use have on people? A model of Internet use is proposed that addresses these questions by considering motivational, affective, and cognitive factors as antecedents and consequences of Internet use. Participants in HomeNetToo will be low-income African American and European American families who will be introduced to the Internet as a communication tool or an information tool. On-line surveys and computer-logged measures of Internet use will be used to test hypotheses about the antecedents and consequences of Internet use over an 18-month trial. Relationships between cognitive style and interface design will also be examined. Results will have implications for how to reduce the digital divide, how to design user interfaces to accommodate diverse cognitive styles, and for identifying factors that influence whether Internet use will have desirable personal, social and professional consequences.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85348</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85348</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1080" target="n1084">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: HomeNetToo: Motivational, affective and cognitive factors and Internet use: Explaining the digital divide and the Internet paradox</data>
      <data key="e_abstract">HomeNetToo is a research project designed to address two fundamental questions: What causes people to use or not use the Internet? What effect does Internet use have on people? A model of Internet use is proposed that addresses these questions by considering motivational, affective, and cognitive factors as antecedents and consequences of Internet use. Participants in HomeNetToo will be low-income African American and European American families who will be introduced to the Internet as a communication tool or an information tool. On-line surveys and computer-logged measures of Internet use will be used to test hypotheses about the antecedents and consequences of Internet use over an 18-month trial. Relationships between cognitive style and interface design will also be examined. Results will have implications for how to reduce the digital divide, how to design user interfaces to accommodate diverse cognitive styles, and for identifying factors that influence whether Internet use will have desirable personal, social and professional consequences.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85348</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85348</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1081" target="n1082">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: HomeNetToo: Motivational, affective and cognitive factors and Internet use: Explaining the digital divide and the Internet paradox</data>
      <data key="e_abstract">HomeNetToo is a research project designed to address two fundamental questions: What causes people to use or not use the Internet? What effect does Internet use have on people? A model of Internet use is proposed that addresses these questions by considering motivational, affective, and cognitive factors as antecedents and consequences of Internet use. Participants in HomeNetToo will be low-income African American and European American families who will be introduced to the Internet as a communication tool or an information tool. On-line surveys and computer-logged measures of Internet use will be used to test hypotheses about the antecedents and consequences of Internet use over an 18-month trial. Relationships between cognitive style and interface design will also be examined. Results will have implications for how to reduce the digital divide, how to design user interfaces to accommodate diverse cognitive styles, and for identifying factors that influence whether Internet use will have desirable personal, social and professional consequences.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85348</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85348</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n138" target="n1081">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: HomeNetToo: Motivational, affective and cognitive factors and Internet use: Explaining the digital divide and the Internet paradox</data>
      <data key="e_abstract">HomeNetToo is a research project designed to address two fundamental questions: What causes people to use or not use the Internet? What effect does Internet use have on people? A model of Internet use is proposed that addresses these questions by considering motivational, affective, and cognitive factors as antecedents and consequences of Internet use. Participants in HomeNetToo will be low-income African American and European American families who will be introduced to the Internet as a communication tool or an information tool. On-line surveys and computer-logged measures of Internet use will be used to test hypotheses about the antecedents and consequences of Internet use over an 18-month trial. Relationships between cognitive style and interface design will also be examined. Results will have implications for how to reduce the digital divide, how to design user interfaces to accommodate diverse cognitive styles, and for identifying factors that influence whether Internet use will have desirable personal, social and professional consequences.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85348</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85348</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1081" target="n1084">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: HomeNetToo: Motivational, affective and cognitive factors and Internet use: Explaining the digital divide and the Internet paradox</data>
      <data key="e_abstract">HomeNetToo is a research project designed to address two fundamental questions: What causes people to use or not use the Internet? What effect does Internet use have on people? A model of Internet use is proposed that addresses these questions by considering motivational, affective, and cognitive factors as antecedents and consequences of Internet use. Participants in HomeNetToo will be low-income African American and European American families who will be introduced to the Internet as a communication tool or an information tool. On-line surveys and computer-logged measures of Internet use will be used to test hypotheses about the antecedents and consequences of Internet use over an 18-month trial. Relationships between cognitive style and interface design will also be examined. Results will have implications for how to reduce the digital divide, how to design user interfaces to accommodate diverse cognitive styles, and for identifying factors that influence whether Internet use will have desirable personal, social and professional consequences.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85348</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85348</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n138" target="n1082">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: HomeNetToo: Motivational, affective and cognitive factors and Internet use: Explaining the digital divide and the Internet paradox</data>
      <data key="e_abstract">HomeNetToo is a research project designed to address two fundamental questions: What causes people to use or not use the Internet? What effect does Internet use have on people? A model of Internet use is proposed that addresses these questions by considering motivational, affective, and cognitive factors as antecedents and consequences of Internet use. Participants in HomeNetToo will be low-income African American and European American families who will be introduced to the Internet as a communication tool or an information tool. On-line surveys and computer-logged measures of Internet use will be used to test hypotheses about the antecedents and consequences of Internet use over an 18-month trial. Relationships between cognitive style and interface design will also be examined. Results will have implications for how to reduce the digital divide, how to design user interfaces to accommodate diverse cognitive styles, and for identifying factors that influence whether Internet use will have desirable personal, social and professional consequences.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85348</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85348</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1082" target="n1084">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: HomeNetToo: Motivational, affective and cognitive factors and Internet use: Explaining the digital divide and the Internet paradox</data>
      <data key="e_abstract">HomeNetToo is a research project designed to address two fundamental questions: What causes people to use or not use the Internet? What effect does Internet use have on people? A model of Internet use is proposed that addresses these questions by considering motivational, affective, and cognitive factors as antecedents and consequences of Internet use. Participants in HomeNetToo will be low-income African American and European American families who will be introduced to the Internet as a communication tool or an information tool. On-line surveys and computer-logged measures of Internet use will be used to test hypotheses about the antecedents and consequences of Internet use over an 18-month trial. Relationships between cognitive style and interface design will also be examined. Results will have implications for how to reduce the digital divide, how to design user interfaces to accommodate diverse cognitive styles, and for identifying factors that influence whether Internet use will have desirable personal, social and professional consequences.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85348</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85348</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n138" target="n1084">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: HomeNetToo: Motivational, affective and cognitive factors and Internet use: Explaining the digital divide and the Internet paradox</data>
      <data key="e_abstract">HomeNetToo is a research project designed to address two fundamental questions: What causes people to use or not use the Internet? What effect does Internet use have on people? A model of Internet use is proposed that addresses these questions by considering motivational, affective, and cognitive factors as antecedents and consequences of Internet use. Participants in HomeNetToo will be low-income African American and European American families who will be introduced to the Internet as a communication tool or an information tool. On-line surveys and computer-logged measures of Internet use will be used to test hypotheses about the antecedents and consequences of Internet use over an 18-month trial. Relationships between cognitive style and interface design will also be examined. Results will have implications for how to reduce the digital divide, how to design user interfaces to accommodate diverse cognitive styles, and for identifying factors that influence whether Internet use will have desirable personal, social and professional consequences.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85348</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85348</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1085" target="n1086">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Information Technology Pathways in Academe: Identifying Barriers for Women and Students of Color</data>
      <data key="e_abstract">Institution: University of Michigan&lt;br/&gt;Proposal Number: EIA 0090006&lt;br/&gt;PI: Sandra Gregerman&lt;br/&gt;Title: Information Technology Pathways in Academe: Identifying Barriers for Women and Minority Students&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to study the factors that contribute to the small numbers of women and underrepresented minorities who obtain degrees in computer science (CS) and computer engineering (CE) at a major research university. Both quantitative and qualitative protocols will be used to determine why diverse students do not elect to follow academic career paths in CS and CE fields. Specifically, the project will consist of five parts: the Academic/Career Pathways Study, the Student Perceptions and Perspective Study, the Student Beliefs and Attitudes Study, the Course-taking Pattern and Retention Study, and the Faculty Perceptions and Attitudes Study. The first four studies will also look at the influence of program interventions such as the Undergraduate Research Opportunity Program and the Women in Science and Engineering Residence Program. This project has the potential to provide valuable insights into the recruitment and retention of women and underrepresented minorities in IT majors.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">90006</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90006</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1085" target="n1087">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Information Technology Pathways in Academe: Identifying Barriers for Women and Students of Color</data>
      <data key="e_abstract">Institution: University of Michigan&lt;br/&gt;Proposal Number: EIA 0090006&lt;br/&gt;PI: Sandra Gregerman&lt;br/&gt;Title: Information Technology Pathways in Academe: Identifying Barriers for Women and Minority Students&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to study the factors that contribute to the small numbers of women and underrepresented minorities who obtain degrees in computer science (CS) and computer engineering (CE) at a major research university. Both quantitative and qualitative protocols will be used to determine why diverse students do not elect to follow academic career paths in CS and CE fields. Specifically, the project will consist of five parts: the Academic/Career Pathways Study, the Student Perceptions and Perspective Study, the Student Beliefs and Attitudes Study, the Course-taking Pattern and Retention Study, and the Faculty Perceptions and Attitudes Study. The first four studies will also look at the influence of program interventions such as the Undergraduate Research Opportunity Program and the Women in Science and Engineering Residence Program. This project has the potential to provide valuable insights into the recruitment and retention of women and underrepresented minorities in IT majors.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">90006</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90006</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1085" target="n1088">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Information Technology Pathways in Academe: Identifying Barriers for Women and Students of Color</data>
      <data key="e_abstract">Institution: University of Michigan&lt;br/&gt;Proposal Number: EIA 0090006&lt;br/&gt;PI: Sandra Gregerman&lt;br/&gt;Title: Information Technology Pathways in Academe: Identifying Barriers for Women and Minority Students&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to study the factors that contribute to the small numbers of women and underrepresented minorities who obtain degrees in computer science (CS) and computer engineering (CE) at a major research university. Both quantitative and qualitative protocols will be used to determine why diverse students do not elect to follow academic career paths in CS and CE fields. Specifically, the project will consist of five parts: the Academic/Career Pathways Study, the Student Perceptions and Perspective Study, the Student Beliefs and Attitudes Study, the Course-taking Pattern and Retention Study, and the Faculty Perceptions and Attitudes Study. The first four studies will also look at the influence of program interventions such as the Undergraduate Research Opportunity Program and the Women in Science and Engineering Residence Program. This project has the potential to provide valuable insights into the recruitment and retention of women and underrepresented minorities in IT majors.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">90006</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90006</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1086" target="n1087">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Information Technology Pathways in Academe: Identifying Barriers for Women and Students of Color</data>
      <data key="e_abstract">Institution: University of Michigan&lt;br/&gt;Proposal Number: EIA 0090006&lt;br/&gt;PI: Sandra Gregerman&lt;br/&gt;Title: Information Technology Pathways in Academe: Identifying Barriers for Women and Minority Students&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to study the factors that contribute to the small numbers of women and underrepresented minorities who obtain degrees in computer science (CS) and computer engineering (CE) at a major research university. Both quantitative and qualitative protocols will be used to determine why diverse students do not elect to follow academic career paths in CS and CE fields. Specifically, the project will consist of five parts: the Academic/Career Pathways Study, the Student Perceptions and Perspective Study, the Student Beliefs and Attitudes Study, the Course-taking Pattern and Retention Study, and the Faculty Perceptions and Attitudes Study. The first four studies will also look at the influence of program interventions such as the Undergraduate Research Opportunity Program and the Women in Science and Engineering Residence Program. This project has the potential to provide valuable insights into the recruitment and retention of women and underrepresented minorities in IT majors.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">90006</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90006</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1086" target="n1088">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Information Technology Pathways in Academe: Identifying Barriers for Women and Students of Color</data>
      <data key="e_abstract">Institution: University of Michigan&lt;br/&gt;Proposal Number: EIA 0090006&lt;br/&gt;PI: Sandra Gregerman&lt;br/&gt;Title: Information Technology Pathways in Academe: Identifying Barriers for Women and Minority Students&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to study the factors that contribute to the small numbers of women and underrepresented minorities who obtain degrees in computer science (CS) and computer engineering (CE) at a major research university. Both quantitative and qualitative protocols will be used to determine why diverse students do not elect to follow academic career paths in CS and CE fields. Specifically, the project will consist of five parts: the Academic/Career Pathways Study, the Student Perceptions and Perspective Study, the Student Beliefs and Attitudes Study, the Course-taking Pattern and Retention Study, and the Faculty Perceptions and Attitudes Study. The first four studies will also look at the influence of program interventions such as the Undergraduate Research Opportunity Program and the Women in Science and Engineering Residence Program. This project has the potential to provide valuable insights into the recruitment and retention of women and underrepresented minorities in IT majors.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">90006</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90006</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1087" target="n1088">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Information Technology Pathways in Academe: Identifying Barriers for Women and Students of Color</data>
      <data key="e_abstract">Institution: University of Michigan&lt;br/&gt;Proposal Number: EIA 0090006&lt;br/&gt;PI: Sandra Gregerman&lt;br/&gt;Title: Information Technology Pathways in Academe: Identifying Barriers for Women and Minority Students&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to study the factors that contribute to the small numbers of women and underrepresented minorities who obtain degrees in computer science (CS) and computer engineering (CE) at a major research university. Both quantitative and qualitative protocols will be used to determine why diverse students do not elect to follow academic career paths in CS and CE fields. Specifically, the project will consist of five parts: the Academic/Career Pathways Study, the Student Perceptions and Perspective Study, the Student Beliefs and Attitudes Study, the Course-taking Pattern and Retention Study, and the Faculty Perceptions and Attitudes Study. The first four studies will also look at the influence of program interventions such as the Undergraduate Research Opportunity Program and the Women in Science and Engineering Residence Program. This project has the potential to provide valuable insights into the recruitment and retention of women and underrepresented minorities in IT majors.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">90006</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90006</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1089" target="n1090">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">SGER: Application of Communication-Theoretic Principles to Nano Interconnect Research</data>
      <data key="e_abstract">Interconnect has been recognized one of ten hardest problems in nano technologies. A basic observation underlying this project is that nano-interconnect issues are much similar to that in real-world communication. Much research has been conducted to ensure the reliable, fast and secure communication over a noisy and stochastic environment. Therefore, the research is exploiting communication-theoretic principles and developing innovative signaling concepts in solving the stochastic nature of nano interconnect. The primary focus is on nano silicon technologies in CMOS with feature sizes below 100nm, and the goal is to explore ways to achieve reliable and fast signaling over the noisy and stochastically limited nano-interconnect environment. The specific objectives are&lt;br/&gt;1. to develop realistic-yet-simple communication models for various nano interconnect scenarios,&lt;br/&gt;2. To study fundamental signaling limits dictated by communication theory (estimates of achievable rates indicate up to Tbits/sec.),&lt;br/&gt;3. to demonstrate interconnect design techniques for nano-signaling that can potentially approach the theoretical signaling limits&lt;br/&gt;This is being made possible by a combination of several innovations that include (i) multi-wire (differential) full-duplex signaling, (ii) signal modulation, coding and equalization, and (iii) utilization, instead of avoiding, very-deep-submicron (VDSM) effects such as wave transmission for potential signaling.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">90012</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">90012</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n187" target="n1089">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">SGER: Application of Communication-Theoretic Principles to Nano Interconnect Research</data>
      <data key="e_abstract">Interconnect has been recognized one of ten hardest problems in nano technologies. A basic observation underlying this project is that nano-interconnect issues are much similar to that in real-world communication. Much research has been conducted to ensure the reliable, fast and secure communication over a noisy and stochastic environment. Therefore, the research is exploiting communication-theoretic principles and developing innovative signaling concepts in solving the stochastic nature of nano interconnect. The primary focus is on nano silicon technologies in CMOS with feature sizes below 100nm, and the goal is to explore ways to achieve reliable and fast signaling over the noisy and stochastically limited nano-interconnect environment. The specific objectives are&lt;br/&gt;1. to develop realistic-yet-simple communication models for various nano interconnect scenarios,&lt;br/&gt;2. To study fundamental signaling limits dictated by communication theory (estimates of achievable rates indicate up to Tbits/sec.),&lt;br/&gt;3. to demonstrate interconnect design techniques for nano-signaling that can potentially approach the theoretical signaling limits&lt;br/&gt;This is being made possible by a combination of several innovations that include (i) multi-wire (differential) full-duplex signaling, (ii) signal modulation, coding and equalization, and (iii) utilization, instead of avoiding, very-deep-submicron (VDSM) effects such as wave transmission for potential signaling.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">90012</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">90012</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n187" target="n1090">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">SGER: Application of Communication-Theoretic Principles to Nano Interconnect Research</data>
      <data key="e_abstract">Interconnect has been recognized one of ten hardest problems in nano technologies. A basic observation underlying this project is that nano-interconnect issues are much similar to that in real-world communication. Much research has been conducted to ensure the reliable, fast and secure communication over a noisy and stochastic environment. Therefore, the research is exploiting communication-theoretic principles and developing innovative signaling concepts in solving the stochastic nature of nano interconnect. The primary focus is on nano silicon technologies in CMOS with feature sizes below 100nm, and the goal is to explore ways to achieve reliable and fast signaling over the noisy and stochastically limited nano-interconnect environment. The specific objectives are&lt;br/&gt;1. to develop realistic-yet-simple communication models for various nano interconnect scenarios,&lt;br/&gt;2. To study fundamental signaling limits dictated by communication theory (estimates of achievable rates indicate up to Tbits/sec.),&lt;br/&gt;3. to demonstrate interconnect design techniques for nano-signaling that can potentially approach the theoretical signaling limits&lt;br/&gt;This is being made possible by a combination of several innovations that include (i) multi-wire (differential) full-duplex signaling, (ii) signal modulation, coding and equalization, and (iii) utilization, instead of avoiding, very-deep-submicron (VDSM) effects such as wave transmission for potential signaling.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">90012</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">90012</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n866" target="n1093">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Attracting and Retaining Women in Information Technology Programs: A Comparative Study of Three Programmatic Approaches</data>
      <data key="e_abstract">Institution: University of Colorado at Boulder&lt;br/&gt;Proposal Number: EIA 0090026&lt;br/&gt;PI: Robert B. Schnabel&lt;br/&gt;Title: Attracting and Retaining Women in Information Technology Programs: A Comparative Study of Three Programmatic Approaches&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to examine the factors that cause women to select and remain in IT educational programs, in the context of three programs at the University of Colorado at Boulder. One is the recently developed multidisciplinary Technology, Arts and Media (TAM) undergraduate certificate program that is attracting a very high percentage of women. Another is the traditional undergraduate computer science major that is sparsely populated by women but taking steps to amend this situation. The third is a Virtual Development Center supported by the Institute for Women and Technology that is specifically intended to involve women in formulating and conducting IT projects. The overarching goal of this research is to gain knowledge that will lead to greater participation in and retention of women in higher education IT programs by identifying program features that appeal to women. This project has the potential to provide valuable insights about the recruitment and retention of women in IT majors.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">90026</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90026</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1093" target="n1095">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Attracting and Retaining Women in Information Technology Programs: A Comparative Study of Three Programmatic Approaches</data>
      <data key="e_abstract">Institution: University of Colorado at Boulder&lt;br/&gt;Proposal Number: EIA 0090026&lt;br/&gt;PI: Robert B. Schnabel&lt;br/&gt;Title: Attracting and Retaining Women in Information Technology Programs: A Comparative Study of Three Programmatic Approaches&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to examine the factors that cause women to select and remain in IT educational programs, in the context of three programs at the University of Colorado at Boulder. One is the recently developed multidisciplinary Technology, Arts and Media (TAM) undergraduate certificate program that is attracting a very high percentage of women. Another is the traditional undergraduate computer science major that is sparsely populated by women but taking steps to amend this situation. The third is a Virtual Development Center supported by the Institute for Women and Technology that is specifically intended to involve women in formulating and conducting IT projects. The overarching goal of this research is to gain knowledge that will lead to greater participation in and retention of women in higher education IT programs by identifying program features that appeal to women. This project has the potential to provide valuable insights about the recruitment and retention of women in IT majors.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">90026</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90026</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1093" target="n1096">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Attracting and Retaining Women in Information Technology Programs: A Comparative Study of Three Programmatic Approaches</data>
      <data key="e_abstract">Institution: University of Colorado at Boulder&lt;br/&gt;Proposal Number: EIA 0090026&lt;br/&gt;PI: Robert B. Schnabel&lt;br/&gt;Title: Attracting and Retaining Women in Information Technology Programs: A Comparative Study of Three Programmatic Approaches&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to examine the factors that cause women to select and remain in IT educational programs, in the context of three programs at the University of Colorado at Boulder. One is the recently developed multidisciplinary Technology, Arts and Media (TAM) undergraduate certificate program that is attracting a very high percentage of women. Another is the traditional undergraduate computer science major that is sparsely populated by women but taking steps to amend this situation. The third is a Virtual Development Center supported by the Institute for Women and Technology that is specifically intended to involve women in formulating and conducting IT projects. The overarching goal of this research is to gain knowledge that will lead to greater participation in and retention of women in higher education IT programs by identifying program features that appeal to women. This project has the potential to provide valuable insights about the recruitment and retention of women in IT majors.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">90026</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90026</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1093" target="n1097">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Attracting and Retaining Women in Information Technology Programs: A Comparative Study of Three Programmatic Approaches</data>
      <data key="e_abstract">Institution: University of Colorado at Boulder&lt;br/&gt;Proposal Number: EIA 0090026&lt;br/&gt;PI: Robert B. Schnabel&lt;br/&gt;Title: Attracting and Retaining Women in Information Technology Programs: A Comparative Study of Three Programmatic Approaches&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to examine the factors that cause women to select and remain in IT educational programs, in the context of three programs at the University of Colorado at Boulder. One is the recently developed multidisciplinary Technology, Arts and Media (TAM) undergraduate certificate program that is attracting a very high percentage of women. Another is the traditional undergraduate computer science major that is sparsely populated by women but taking steps to amend this situation. The third is a Virtual Development Center supported by the Institute for Women and Technology that is specifically intended to involve women in formulating and conducting IT projects. The overarching goal of this research is to gain knowledge that will lead to greater participation in and retention of women in higher education IT programs by identifying program features that appeal to women. This project has the potential to provide valuable insights about the recruitment and retention of women in IT majors.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">90026</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90026</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n866" target="n1095">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Attracting and Retaining Women in Information Technology Programs: A Comparative Study of Three Programmatic Approaches</data>
      <data key="e_abstract">Institution: University of Colorado at Boulder&lt;br/&gt;Proposal Number: EIA 0090026&lt;br/&gt;PI: Robert B. Schnabel&lt;br/&gt;Title: Attracting and Retaining Women in Information Technology Programs: A Comparative Study of Three Programmatic Approaches&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to examine the factors that cause women to select and remain in IT educational programs, in the context of three programs at the University of Colorado at Boulder. One is the recently developed multidisciplinary Technology, Arts and Media (TAM) undergraduate certificate program that is attracting a very high percentage of women. Another is the traditional undergraduate computer science major that is sparsely populated by women but taking steps to amend this situation. The third is a Virtual Development Center supported by the Institute for Women and Technology that is specifically intended to involve women in formulating and conducting IT projects. The overarching goal of this research is to gain knowledge that will lead to greater participation in and retention of women in higher education IT programs by identifying program features that appeal to women. This project has the potential to provide valuable insights about the recruitment and retention of women in IT majors.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">90026</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90026</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n866" target="n1096">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Attracting and Retaining Women in Information Technology Programs: A Comparative Study of Three Programmatic Approaches</data>
      <data key="e_abstract">Institution: University of Colorado at Boulder&lt;br/&gt;Proposal Number: EIA 0090026&lt;br/&gt;PI: Robert B. Schnabel&lt;br/&gt;Title: Attracting and Retaining Women in Information Technology Programs: A Comparative Study of Three Programmatic Approaches&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to examine the factors that cause women to select and remain in IT educational programs, in the context of three programs at the University of Colorado at Boulder. One is the recently developed multidisciplinary Technology, Arts and Media (TAM) undergraduate certificate program that is attracting a very high percentage of women. Another is the traditional undergraduate computer science major that is sparsely populated by women but taking steps to amend this situation. The third is a Virtual Development Center supported by the Institute for Women and Technology that is specifically intended to involve women in formulating and conducting IT projects. The overarching goal of this research is to gain knowledge that will lead to greater participation in and retention of women in higher education IT programs by identifying program features that appeal to women. This project has the potential to provide valuable insights about the recruitment and retention of women in IT majors.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">90026</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90026</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n866" target="n1097">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Attracting and Retaining Women in Information Technology Programs: A Comparative Study of Three Programmatic Approaches</data>
      <data key="e_abstract">Institution: University of Colorado at Boulder&lt;br/&gt;Proposal Number: EIA 0090026&lt;br/&gt;PI: Robert B. Schnabel&lt;br/&gt;Title: Attracting and Retaining Women in Information Technology Programs: A Comparative Study of Three Programmatic Approaches&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to examine the factors that cause women to select and remain in IT educational programs, in the context of three programs at the University of Colorado at Boulder. One is the recently developed multidisciplinary Technology, Arts and Media (TAM) undergraduate certificate program that is attracting a very high percentage of women. Another is the traditional undergraduate computer science major that is sparsely populated by women but taking steps to amend this situation. The third is a Virtual Development Center supported by the Institute for Women and Technology that is specifically intended to involve women in formulating and conducting IT projects. The overarching goal of this research is to gain knowledge that will lead to greater participation in and retention of women in higher education IT programs by identifying program features that appeal to women. This project has the potential to provide valuable insights about the recruitment and retention of women in IT majors.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">90026</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90026</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1095" target="n1096">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Attracting and Retaining Women in Information Technology Programs: A Comparative Study of Three Programmatic Approaches</data>
      <data key="e_abstract">Institution: University of Colorado at Boulder&lt;br/&gt;Proposal Number: EIA 0090026&lt;br/&gt;PI: Robert B. Schnabel&lt;br/&gt;Title: Attracting and Retaining Women in Information Technology Programs: A Comparative Study of Three Programmatic Approaches&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to examine the factors that cause women to select and remain in IT educational programs, in the context of three programs at the University of Colorado at Boulder. One is the recently developed multidisciplinary Technology, Arts and Media (TAM) undergraduate certificate program that is attracting a very high percentage of women. Another is the traditional undergraduate computer science major that is sparsely populated by women but taking steps to amend this situation. The third is a Virtual Development Center supported by the Institute for Women and Technology that is specifically intended to involve women in formulating and conducting IT projects. The overarching goal of this research is to gain knowledge that will lead to greater participation in and retention of women in higher education IT programs by identifying program features that appeal to women. This project has the potential to provide valuable insights about the recruitment and retention of women in IT majors.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">90026</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90026</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1095" target="n1097">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Attracting and Retaining Women in Information Technology Programs: A Comparative Study of Three Programmatic Approaches</data>
      <data key="e_abstract">Institution: University of Colorado at Boulder&lt;br/&gt;Proposal Number: EIA 0090026&lt;br/&gt;PI: Robert B. Schnabel&lt;br/&gt;Title: Attracting and Retaining Women in Information Technology Programs: A Comparative Study of Three Programmatic Approaches&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to examine the factors that cause women to select and remain in IT educational programs, in the context of three programs at the University of Colorado at Boulder. One is the recently developed multidisciplinary Technology, Arts and Media (TAM) undergraduate certificate program that is attracting a very high percentage of women. Another is the traditional undergraduate computer science major that is sparsely populated by women but taking steps to amend this situation. The third is a Virtual Development Center supported by the Institute for Women and Technology that is specifically intended to involve women in formulating and conducting IT projects. The overarching goal of this research is to gain knowledge that will lead to greater participation in and retention of women in higher education IT programs by identifying program features that appeal to women. This project has the potential to provide valuable insights about the recruitment and retention of women in IT majors.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">90026</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90026</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1096" target="n1097">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Attracting and Retaining Women in Information Technology Programs: A Comparative Study of Three Programmatic Approaches</data>
      <data key="e_abstract">Institution: University of Colorado at Boulder&lt;br/&gt;Proposal Number: EIA 0090026&lt;br/&gt;PI: Robert B. Schnabel&lt;br/&gt;Title: Attracting and Retaining Women in Information Technology Programs: A Comparative Study of Three Programmatic Approaches&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to examine the factors that cause women to select and remain in IT educational programs, in the context of three programs at the University of Colorado at Boulder. One is the recently developed multidisciplinary Technology, Arts and Media (TAM) undergraduate certificate program that is attracting a very high percentage of women. Another is the traditional undergraduate computer science major that is sparsely populated by women but taking steps to amend this situation. The third is a Virtual Development Center supported by the Institute for Women and Technology that is specifically intended to involve women in formulating and conducting IT projects. The overarching goal of this research is to gain knowledge that will lead to greater participation in and retention of women in higher education IT programs by identifying program features that appeal to women. This project has the potential to provide valuable insights about the recruitment and retention of women in IT majors.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">90026</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90026</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n458" target="n1098">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR/IM-SII: A Distributed Information Management Framework (REGNET) for Environmental Laws and Regulations</data>
      <data key="e_abstract">This project will develop a formal, but practical information technology infrastructure to make government regulations more effectively available to the public. The topic area is hazardous waste management. Federal and California state environmental protection agencies will be engaged in the project. The infrastructure to be developed will include distributed data repositories, and also tools to locate, merge, compare, and analyze the information. Project phases are 1) textual storage, 2) semi-structured indexed storage, 3) means to resolve semantic ambiguities, 4) cross-referencing appropriate for automated access from relevant legal documents, and 5) on-line compliance checking of government regulations. This is an interdisciplinary project with the involvement of experts in law, computer science, and civil/environmental engineering.&lt;br/&gt;&lt;br/&gt;=================================================</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85998</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85998</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n459" target="n1098">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR/IM-SII: A Distributed Information Management Framework (REGNET) for Environmental Laws and Regulations</data>
      <data key="e_abstract">This project will develop a formal, but practical information technology infrastructure to make government regulations more effectively available to the public. The topic area is hazardous waste management. Federal and California state environmental protection agencies will be engaged in the project. The infrastructure to be developed will include distributed data repositories, and also tools to locate, merge, compare, and analyze the information. Project phases are 1) textual storage, 2) semi-structured indexed storage, 3) means to resolve semantic ambiguities, 4) cross-referencing appropriate for automated access from relevant legal documents, and 5) on-line compliance checking of government regulations. This is an interdisciplinary project with the involvement of experts in law, computer science, and civil/environmental engineering.&lt;br/&gt;&lt;br/&gt;=================================================</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85998</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85998</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1098" target="n1101">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR/IM-SII: A Distributed Information Management Framework (REGNET) for Environmental Laws and Regulations</data>
      <data key="e_abstract">This project will develop a formal, but practical information technology infrastructure to make government regulations more effectively available to the public. The topic area is hazardous waste management. Federal and California state environmental protection agencies will be engaged in the project. The infrastructure to be developed will include distributed data repositories, and also tools to locate, merge, compare, and analyze the information. Project phases are 1) textual storage, 2) semi-structured indexed storage, 3) means to resolve semantic ambiguities, 4) cross-referencing appropriate for automated access from relevant legal documents, and 5) on-line compliance checking of government regulations. This is an interdisciplinary project with the involvement of experts in law, computer science, and civil/environmental engineering.&lt;br/&gt;&lt;br/&gt;=================================================</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85998</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85998</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n458" target="n459">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR/IM-SII: A Distributed Information Management Framework (REGNET) for Environmental Laws and Regulations</data>
      <data key="e_abstract">This project will develop a formal, but practical information technology infrastructure to make government regulations more effectively available to the public. The topic area is hazardous waste management. Federal and California state environmental protection agencies will be engaged in the project. The infrastructure to be developed will include distributed data repositories, and also tools to locate, merge, compare, and analyze the information. Project phases are 1) textual storage, 2) semi-structured indexed storage, 3) means to resolve semantic ambiguities, 4) cross-referencing appropriate for automated access from relevant legal documents, and 5) on-line compliance checking of government regulations. This is an interdisciplinary project with the involvement of experts in law, computer science, and civil/environmental engineering.&lt;br/&gt;&lt;br/&gt;=================================================</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85998</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85998</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n458" target="n1101">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR/IM-SII: A Distributed Information Management Framework (REGNET) for Environmental Laws and Regulations</data>
      <data key="e_abstract">This project will develop a formal, but practical information technology infrastructure to make government regulations more effectively available to the public. The topic area is hazardous waste management. Federal and California state environmental protection agencies will be engaged in the project. The infrastructure to be developed will include distributed data repositories, and also tools to locate, merge, compare, and analyze the information. Project phases are 1) textual storage, 2) semi-structured indexed storage, 3) means to resolve semantic ambiguities, 4) cross-referencing appropriate for automated access from relevant legal documents, and 5) on-line compliance checking of government regulations. This is an interdisciplinary project with the involvement of experts in law, computer science, and civil/environmental engineering.&lt;br/&gt;&lt;br/&gt;=================================================</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85998</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85998</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n459" target="n1101">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR/IM-SII: A Distributed Information Management Framework (REGNET) for Environmental Laws and Regulations</data>
      <data key="e_abstract">This project will develop a formal, but practical information technology infrastructure to make government regulations more effectively available to the public. The topic area is hazardous waste management. Federal and California state environmental protection agencies will be engaged in the project. The infrastructure to be developed will include distributed data repositories, and also tools to locate, merge, compare, and analyze the information. Project phases are 1) textual storage, 2) semi-structured indexed storage, 3) means to resolve semantic ambiguities, 4) cross-referencing appropriate for automated access from relevant legal documents, and 5) on-line compliance checking of government regulations. This is an interdisciplinary project with the involvement of experts in law, computer science, and civil/environmental engineering.&lt;br/&gt;&lt;br/&gt;=================================================</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85998</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85998</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1102" target="n1103">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research: &apos;Free&apos; Bits: The Challenge of the Wireless Internet</data>
      <data key="e_abstract">Second and third generation wireless systems have been designed primarily for voice, so they are con-nection oriented, delay sensitive, and provide fixed bit rates. Additionally, since service is desired &quot;any-time/anywhere,&quot; these systems must provide ubiquitous coverage. This coverage is achieved through rela-tively uniform grids of cell sites, which are placed to control interference and minimize outage rather than to maximize throughout. As a result, such systems deliver low bit rates and are relatively expensive when used for large amounts of information. At the same time, wireline connection to the Internet has encouraged uses (and users) that depend upon bits being virtually &quot;free.&quot; We contend that this &quot;economic&quot; mismatch between wired and wireless access is the primary obstacle to the dramatic growth of a wireless Internet. The solution may lie in designing systems specifically for wireless data, recognizing that data services are often connectionless, delay insensitive and have no specific bit rate requirements. These differences suggest that ubiquitous (anytime/anywhere) coverage is not a strict requirement for wireless data networks and makes possible systems in which small, separated coverage areas facilitate transfers of megabytes of data in fractions of a second, and for a fraction of the cost associated with conventional ubiquitous coverage. Communication theory and simple link budget calculations tell us that it is possible to build such sys-tems, but the signal processing challenges are numerous and distinct from the historical challenges offered by connection-oriented wireless services. When a mobile user passes an Infostation, there will be a window of opportunity, perhaps as short as a fraction of a second, in which the user will have access to a high-rate communication channel. A key task is to identify that window and transmit at an appropriate rate. The mobile must make these decisions based on measurements of a wideband radio channel in which there is frequency selectivity and time variation in the fading as well as in the interference. In the specific context&lt;br/&gt;of an Infostations system, we plan to divide our research into four components: &lt;br/&gt;&lt;br/&gt;Radio Channel Modeling: The characterization of typical Infostation radio channels. &lt;br/&gt;&lt;br/&gt;Transceiver Design: The analysis and performance evaluation of transmitters and receivers for both single carrier and multicarrier systems.&lt;br/&gt;&lt;br/&gt;Radio Resource Management Transmitter power and rate adaptation policies derived from receiver measurements.&lt;br/&gt;&lt;br/&gt;Algorithm Development Testbed A platform employing DSP and FPGA technology for the practical evaluation of transmitter and receiver algorithms. &lt;br/&gt;&lt;br/&gt;The activities of this project will encompass three research institutions in New Jersey (New Jersey In-stitute of Technology, Princeton University, Rutgers University) under the auspices of the N.J. Center for Wireless Telecommunications (NJCWT). The NJCWT is an inter-institutional research and educational or-ganization sponsored and funded by the N.J. Commission on Science and Technology. The focus of the center is a multi-year effort in Digital Radio Technology for Computing, Communications and Information Systems. This effort is supportive of and will enhance the present proposed project in wireless networks.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85986</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85986</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1102" target="n1104">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research: &apos;Free&apos; Bits: The Challenge of the Wireless Internet</data>
      <data key="e_abstract">Second and third generation wireless systems have been designed primarily for voice, so they are con-nection oriented, delay sensitive, and provide fixed bit rates. Additionally, since service is desired &quot;any-time/anywhere,&quot; these systems must provide ubiquitous coverage. This coverage is achieved through rela-tively uniform grids of cell sites, which are placed to control interference and minimize outage rather than to maximize throughout. As a result, such systems deliver low bit rates and are relatively expensive when used for large amounts of information. At the same time, wireline connection to the Internet has encouraged uses (and users) that depend upon bits being virtually &quot;free.&quot; We contend that this &quot;economic&quot; mismatch between wired and wireless access is the primary obstacle to the dramatic growth of a wireless Internet. The solution may lie in designing systems specifically for wireless data, recognizing that data services are often connectionless, delay insensitive and have no specific bit rate requirements. These differences suggest that ubiquitous (anytime/anywhere) coverage is not a strict requirement for wireless data networks and makes possible systems in which small, separated coverage areas facilitate transfers of megabytes of data in fractions of a second, and for a fraction of the cost associated with conventional ubiquitous coverage. Communication theory and simple link budget calculations tell us that it is possible to build such sys-tems, but the signal processing challenges are numerous and distinct from the historical challenges offered by connection-oriented wireless services. When a mobile user passes an Infostation, there will be a window of opportunity, perhaps as short as a fraction of a second, in which the user will have access to a high-rate communication channel. A key task is to identify that window and transmit at an appropriate rate. The mobile must make these decisions based on measurements of a wideband radio channel in which there is frequency selectivity and time variation in the fading as well as in the interference. In the specific context&lt;br/&gt;of an Infostations system, we plan to divide our research into four components: &lt;br/&gt;&lt;br/&gt;Radio Channel Modeling: The characterization of typical Infostation radio channels. &lt;br/&gt;&lt;br/&gt;Transceiver Design: The analysis and performance evaluation of transmitters and receivers for both single carrier and multicarrier systems.&lt;br/&gt;&lt;br/&gt;Radio Resource Management Transmitter power and rate adaptation policies derived from receiver measurements.&lt;br/&gt;&lt;br/&gt;Algorithm Development Testbed A platform employing DSP and FPGA technology for the practical evaluation of transmitter and receiver algorithms. &lt;br/&gt;&lt;br/&gt;The activities of this project will encompass three research institutions in New Jersey (New Jersey In-stitute of Technology, Princeton University, Rutgers University) under the auspices of the N.J. Center for Wireless Telecommunications (NJCWT). The NJCWT is an inter-institutional research and educational or-ganization sponsored and funded by the N.J. Commission on Science and Technology. The focus of the center is a multi-year effort in Digital Radio Technology for Computing, Communications and Information Systems. This effort is supportive of and will enhance the present proposed project in wireless networks.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85986</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85986</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1103" target="n1104">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research: &apos;Free&apos; Bits: The Challenge of the Wireless Internet</data>
      <data key="e_abstract">Second and third generation wireless systems have been designed primarily for voice, so they are con-nection oriented, delay sensitive, and provide fixed bit rates. Additionally, since service is desired &quot;any-time/anywhere,&quot; these systems must provide ubiquitous coverage. This coverage is achieved through rela-tively uniform grids of cell sites, which are placed to control interference and minimize outage rather than to maximize throughout. As a result, such systems deliver low bit rates and are relatively expensive when used for large amounts of information. At the same time, wireline connection to the Internet has encouraged uses (and users) that depend upon bits being virtually &quot;free.&quot; We contend that this &quot;economic&quot; mismatch between wired and wireless access is the primary obstacle to the dramatic growth of a wireless Internet. The solution may lie in designing systems specifically for wireless data, recognizing that data services are often connectionless, delay insensitive and have no specific bit rate requirements. These differences suggest that ubiquitous (anytime/anywhere) coverage is not a strict requirement for wireless data networks and makes possible systems in which small, separated coverage areas facilitate transfers of megabytes of data in fractions of a second, and for a fraction of the cost associated with conventional ubiquitous coverage. Communication theory and simple link budget calculations tell us that it is possible to build such sys-tems, but the signal processing challenges are numerous and distinct from the historical challenges offered by connection-oriented wireless services. When a mobile user passes an Infostation, there will be a window of opportunity, perhaps as short as a fraction of a second, in which the user will have access to a high-rate communication channel. A key task is to identify that window and transmit at an appropriate rate. The mobile must make these decisions based on measurements of a wideband radio channel in which there is frequency selectivity and time variation in the fading as well as in the interference. In the specific context&lt;br/&gt;of an Infostations system, we plan to divide our research into four components: &lt;br/&gt;&lt;br/&gt;Radio Channel Modeling: The characterization of typical Infostation radio channels. &lt;br/&gt;&lt;br/&gt;Transceiver Design: The analysis and performance evaluation of transmitters and receivers for both single carrier and multicarrier systems.&lt;br/&gt;&lt;br/&gt;Radio Resource Management Transmitter power and rate adaptation policies derived from receiver measurements.&lt;br/&gt;&lt;br/&gt;Algorithm Development Testbed A platform employing DSP and FPGA technology for the practical evaluation of transmitter and receiver algorithms. &lt;br/&gt;&lt;br/&gt;The activities of this project will encompass three research institutions in New Jersey (New Jersey In-stitute of Technology, Princeton University, Rutgers University) under the auspices of the N.J. Center for Wireless Telecommunications (NJCWT). The NJCWT is an inter-institutional research and educational or-ganization sponsored and funded by the N.J. Commission on Science and Technology. The focus of the center is a multi-year effort in Digital Radio Technology for Computing, Communications and Information Systems. This effort is supportive of and will enhance the present proposed project in wireless networks.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85986</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85986</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1105" target="n1106">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Strategic Software Design: Value-Driven Software Definition, Development, Deployment and Evolution</data>
      <data key="e_abstract">Current software design concepts for products, processes, projects, programs, portfolios, and policies largely overlook a simple but fundamental idea: the goal of software design decision making is to create the maximum value added for any given investment of valuable resources. In a business context, profit and opportunities to profit are most valued. In other context, other measures of value apply, such as the solution of major social problems by philanthropic foundations. The investigation will focus on developing and enhancing scientific foundations for software design decision-making models, methods and tools explicitly tied to value-maximization objectives. Strategic approaches to value creation under conditions of uncertainty, incomplete knowledge, competition, and the need for cooperation among self-interested stakeholders will receive particular attention. The research is a joint effort of the University of Virginia, Carnegie Mellon University, the University of Washington, and the University of Southern California.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86003</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86003</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1105" target="n1107">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Strategic Software Design: Value-Driven Software Definition, Development, Deployment and Evolution</data>
      <data key="e_abstract">Current software design concepts for products, processes, projects, programs, portfolios, and policies largely overlook a simple but fundamental idea: the goal of software design decision making is to create the maximum value added for any given investment of valuable resources. In a business context, profit and opportunities to profit are most valued. In other context, other measures of value apply, such as the solution of major social problems by philanthropic foundations. The investigation will focus on developing and enhancing scientific foundations for software design decision-making models, methods and tools explicitly tied to value-maximization objectives. Strategic approaches to value creation under conditions of uncertainty, incomplete knowledge, competition, and the need for cooperation among self-interested stakeholders will receive particular attention. The research is a joint effort of the University of Virginia, Carnegie Mellon University, the University of Washington, and the University of Southern California.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86003</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86003</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1105" target="n1108">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Strategic Software Design: Value-Driven Software Definition, Development, Deployment and Evolution</data>
      <data key="e_abstract">Current software design concepts for products, processes, projects, programs, portfolios, and policies largely overlook a simple but fundamental idea: the goal of software design decision making is to create the maximum value added for any given investment of valuable resources. In a business context, profit and opportunities to profit are most valued. In other context, other measures of value apply, such as the solution of major social problems by philanthropic foundations. The investigation will focus on developing and enhancing scientific foundations for software design decision-making models, methods and tools explicitly tied to value-maximization objectives. Strategic approaches to value creation under conditions of uncertainty, incomplete knowledge, competition, and the need for cooperation among self-interested stakeholders will receive particular attention. The research is a joint effort of the University of Virginia, Carnegie Mellon University, the University of Washington, and the University of Southern California.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86003</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86003</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1106" target="n1107">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Strategic Software Design: Value-Driven Software Definition, Development, Deployment and Evolution</data>
      <data key="e_abstract">Current software design concepts for products, processes, projects, programs, portfolios, and policies largely overlook a simple but fundamental idea: the goal of software design decision making is to create the maximum value added for any given investment of valuable resources. In a business context, profit and opportunities to profit are most valued. In other context, other measures of value apply, such as the solution of major social problems by philanthropic foundations. The investigation will focus on developing and enhancing scientific foundations for software design decision-making models, methods and tools explicitly tied to value-maximization objectives. Strategic approaches to value creation under conditions of uncertainty, incomplete knowledge, competition, and the need for cooperation among self-interested stakeholders will receive particular attention. The research is a joint effort of the University of Virginia, Carnegie Mellon University, the University of Washington, and the University of Southern California.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86003</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86003</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1106" target="n1108">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Strategic Software Design: Value-Driven Software Definition, Development, Deployment and Evolution</data>
      <data key="e_abstract">Current software design concepts for products, processes, projects, programs, portfolios, and policies largely overlook a simple but fundamental idea: the goal of software design decision making is to create the maximum value added for any given investment of valuable resources. In a business context, profit and opportunities to profit are most valued. In other context, other measures of value apply, such as the solution of major social problems by philanthropic foundations. The investigation will focus on developing and enhancing scientific foundations for software design decision-making models, methods and tools explicitly tied to value-maximization objectives. Strategic approaches to value creation under conditions of uncertainty, incomplete knowledge, competition, and the need for cooperation among self-interested stakeholders will receive particular attention. The research is a joint effort of the University of Virginia, Carnegie Mellon University, the University of Washington, and the University of Southern California.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86003</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86003</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1107" target="n1108">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Strategic Software Design: Value-Driven Software Definition, Development, Deployment and Evolution</data>
      <data key="e_abstract">Current software design concepts for products, processes, projects, programs, portfolios, and policies largely overlook a simple but fundamental idea: the goal of software design decision making is to create the maximum value added for any given investment of valuable resources. In a business context, profit and opportunities to profit are most valued. In other context, other measures of value apply, such as the solution of major social problems by philanthropic foundations. The investigation will focus on developing and enhancing scientific foundations for software design decision-making models, methods and tools explicitly tied to value-maximization objectives. Strategic approaches to value creation under conditions of uncertainty, incomplete knowledge, competition, and the need for cooperation among self-interested stakeholders will receive particular attention. The research is a joint effort of the University of Virginia, Carnegie Mellon University, the University of Washington, and the University of Southern California.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86003</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86003</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1109" target="n1110">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: A Petabyte in Your Pocket</data>
      <data key="e_abstract">In 2015, for a few hundred dollars a year, everyone will have a personal petabyte database (PetDB) that can be accessed from any point of connection, with any device from a high-powered workstation to a PDA. Each individual&apos;s PetDB will be their evolving and customized view of all the on-line digital data that exists anywhere. It will store and organize any kind of digital data, without losing structure or information. All this data will be queryable and arranged by type, content, structure, association and multiple categorizations and groupings. The PetDB is an example of what could be done with a new generation of software infrastructure we term Net Data Managers (NDMs). The object of this research is not to build a PetDB per se; but to design and implement the NDM technology upon which PetDBs and other applications could be readily built. NDMs are a departure from current database management systems. They focus on data movement, rather than data storage and must handle data of arbitrary types, without necessarily having a matching database schema. They must execute queries over tens of thousands of information sites, while monitoring possibly millions of triggers over rapidly changing information sources. The first exploratory steps have been taken towards NDMs with initial work on XML querying, text in context indexing and searching, multi-trigger planning and data stream processing. This work has demonstrated some of the basic capabilities of NDMs. Continued research will address the challenging fundamental problems that must be solved to extend these capabilities to exploit the full capabilities of NDMs. At the same time, the scale at which NDMs operate will be extended, in terms of numbers of users, tasks and sites, as well as data volumes, through distributed and parallel implementations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86002</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86002</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n551" target="n1111">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Multimodal Human Computer Interaction: Toward a Proactive Computer</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This project is based on the belief that, to be more accessible to the general population, computers must be more proactive in their interactions with people. In human interaction, someone who waits for each command before making any communication attempt would be regarded as uncooperative and unhelpful. In order for a computer to be more proactive and, thus, to bear its part of the burden of initiation in interactions, it must have (1) much more real-time information about its user, and (2) algorithms that select actions based on this information rather than simply on user commands. The computer needs information about the user&apos;s current and past emotional, motivational and cognitive state as well as the state of the task at hand. A theory, is needed to guide the development of algorithms that select appropriate actions based on user and task state. This research constitutes the next steps in an attempt by the PI&apos;s multidisciplinary team to develop this capability.&lt;br/&gt;&lt;br/&gt;Proposed research includes: (1) further development of methods to sense user postures, movements, expressions and speech; (2) analysis and fusion of this information to identify and track user states; (3) task state tracking; (4) creating a corpus of emotion- and action-labeled videotapes for use with computer learning; (5) further development of affective communication; (6) development of the basis for human-centered state-based action decisions; and (7) evaluation of computer proaction on human behavior and response. The testbed is an environment for hands-on education in science and engineering, using the Lego Mindstorms construction and robotics environment, with children of middle school age. An emphasis will be on developing proactive computing methods for encouraging interest and conceptual development of minority children and females, who often show lower achievement in science. Although the work will be conducted within an educational environment, the methods developed and studied will be broadly applicable, and this project should serve as an exemplar of the type of work that is needed in other computer-aided situations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85980</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85980</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1111" target="n1113">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Multimodal Human Computer Interaction: Toward a Proactive Computer</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This project is based on the belief that, to be more accessible to the general population, computers must be more proactive in their interactions with people. In human interaction, someone who waits for each command before making any communication attempt would be regarded as uncooperative and unhelpful. In order for a computer to be more proactive and, thus, to bear its part of the burden of initiation in interactions, it must have (1) much more real-time information about its user, and (2) algorithms that select actions based on this information rather than simply on user commands. The computer needs information about the user&apos;s current and past emotional, motivational and cognitive state as well as the state of the task at hand. A theory, is needed to guide the development of algorithms that select appropriate actions based on user and task state. This research constitutes the next steps in an attempt by the PI&apos;s multidisciplinary team to develop this capability.&lt;br/&gt;&lt;br/&gt;Proposed research includes: (1) further development of methods to sense user postures, movements, expressions and speech; (2) analysis and fusion of this information to identify and track user states; (3) task state tracking; (4) creating a corpus of emotion- and action-labeled videotapes for use with computer learning; (5) further development of affective communication; (6) development of the basis for human-centered state-based action decisions; and (7) evaluation of computer proaction on human behavior and response. The testbed is an environment for hands-on education in science and engineering, using the Lego Mindstorms construction and robotics environment, with children of middle school age. An emphasis will be on developing proactive computing methods for encouraging interest and conceptual development of minority children and females, who often show lower achievement in science. Although the work will be conducted within an educational environment, the methods developed and studied will be broadly applicable, and this project should serve as an exemplar of the type of work that is needed in other computer-aided situations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85980</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85980</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1111" target="n1114">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Multimodal Human Computer Interaction: Toward a Proactive Computer</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This project is based on the belief that, to be more accessible to the general population, computers must be more proactive in their interactions with people. In human interaction, someone who waits for each command before making any communication attempt would be regarded as uncooperative and unhelpful. In order for a computer to be more proactive and, thus, to bear its part of the burden of initiation in interactions, it must have (1) much more real-time information about its user, and (2) algorithms that select actions based on this information rather than simply on user commands. The computer needs information about the user&apos;s current and past emotional, motivational and cognitive state as well as the state of the task at hand. A theory, is needed to guide the development of algorithms that select appropriate actions based on user and task state. This research constitutes the next steps in an attempt by the PI&apos;s multidisciplinary team to develop this capability.&lt;br/&gt;&lt;br/&gt;Proposed research includes: (1) further development of methods to sense user postures, movements, expressions and speech; (2) analysis and fusion of this information to identify and track user states; (3) task state tracking; (4) creating a corpus of emotion- and action-labeled videotapes for use with computer learning; (5) further development of affective communication; (6) development of the basis for human-centered state-based action decisions; and (7) evaluation of computer proaction on human behavior and response. The testbed is an environment for hands-on education in science and engineering, using the Lego Mindstorms construction and robotics environment, with children of middle school age. An emphasis will be on developing proactive computing methods for encouraging interest and conceptual development of minority children and females, who often show lower achievement in science. Although the work will be conducted within an educational environment, the methods developed and studied will be broadly applicable, and this project should serve as an exemplar of the type of work that is needed in other computer-aided situations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85980</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85980</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1111" target="n1115">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Multimodal Human Computer Interaction: Toward a Proactive Computer</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This project is based on the belief that, to be more accessible to the general population, computers must be more proactive in their interactions with people. In human interaction, someone who waits for each command before making any communication attempt would be regarded as uncooperative and unhelpful. In order for a computer to be more proactive and, thus, to bear its part of the burden of initiation in interactions, it must have (1) much more real-time information about its user, and (2) algorithms that select actions based on this information rather than simply on user commands. The computer needs information about the user&apos;s current and past emotional, motivational and cognitive state as well as the state of the task at hand. A theory, is needed to guide the development of algorithms that select appropriate actions based on user and task state. This research constitutes the next steps in an attempt by the PI&apos;s multidisciplinary team to develop this capability.&lt;br/&gt;&lt;br/&gt;Proposed research includes: (1) further development of methods to sense user postures, movements, expressions and speech; (2) analysis and fusion of this information to identify and track user states; (3) task state tracking; (4) creating a corpus of emotion- and action-labeled videotapes for use with computer learning; (5) further development of affective communication; (6) development of the basis for human-centered state-based action decisions; and (7) evaluation of computer proaction on human behavior and response. The testbed is an environment for hands-on education in science and engineering, using the Lego Mindstorms construction and robotics environment, with children of middle school age. An emphasis will be on developing proactive computing methods for encouraging interest and conceptual development of minority children and females, who often show lower achievement in science. Although the work will be conducted within an educational environment, the methods developed and studied will be broadly applicable, and this project should serve as an exemplar of the type of work that is needed in other computer-aided situations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85980</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85980</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n268" target="n1111">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Multimodal Human Computer Interaction: Toward a Proactive Computer</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This project is based on the belief that, to be more accessible to the general population, computers must be more proactive in their interactions with people. In human interaction, someone who waits for each command before making any communication attempt would be regarded as uncooperative and unhelpful. In order for a computer to be more proactive and, thus, to bear its part of the burden of initiation in interactions, it must have (1) much more real-time information about its user, and (2) algorithms that select actions based on this information rather than simply on user commands. The computer needs information about the user&apos;s current and past emotional, motivational and cognitive state as well as the state of the task at hand. A theory, is needed to guide the development of algorithms that select appropriate actions based on user and task state. This research constitutes the next steps in an attempt by the PI&apos;s multidisciplinary team to develop this capability.&lt;br/&gt;&lt;br/&gt;Proposed research includes: (1) further development of methods to sense user postures, movements, expressions and speech; (2) analysis and fusion of this information to identify and track user states; (3) task state tracking; (4) creating a corpus of emotion- and action-labeled videotapes for use with computer learning; (5) further development of affective communication; (6) development of the basis for human-centered state-based action decisions; and (7) evaluation of computer proaction on human behavior and response. The testbed is an environment for hands-on education in science and engineering, using the Lego Mindstorms construction and robotics environment, with children of middle school age. An emphasis will be on developing proactive computing methods for encouraging interest and conceptual development of minority children and females, who often show lower achievement in science. Although the work will be conducted within an educational environment, the methods developed and studied will be broadly applicable, and this project should serve as an exemplar of the type of work that is needed in other computer-aided situations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85980</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85980</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n551" target="n1113">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Multimodal Human Computer Interaction: Toward a Proactive Computer</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This project is based on the belief that, to be more accessible to the general population, computers must be more proactive in their interactions with people. In human interaction, someone who waits for each command before making any communication attempt would be regarded as uncooperative and unhelpful. In order for a computer to be more proactive and, thus, to bear its part of the burden of initiation in interactions, it must have (1) much more real-time information about its user, and (2) algorithms that select actions based on this information rather than simply on user commands. The computer needs information about the user&apos;s current and past emotional, motivational and cognitive state as well as the state of the task at hand. A theory, is needed to guide the development of algorithms that select appropriate actions based on user and task state. This research constitutes the next steps in an attempt by the PI&apos;s multidisciplinary team to develop this capability.&lt;br/&gt;&lt;br/&gt;Proposed research includes: (1) further development of methods to sense user postures, movements, expressions and speech; (2) analysis and fusion of this information to identify and track user states; (3) task state tracking; (4) creating a corpus of emotion- and action-labeled videotapes for use with computer learning; (5) further development of affective communication; (6) development of the basis for human-centered state-based action decisions; and (7) evaluation of computer proaction on human behavior and response. The testbed is an environment for hands-on education in science and engineering, using the Lego Mindstorms construction and robotics environment, with children of middle school age. An emphasis will be on developing proactive computing methods for encouraging interest and conceptual development of minority children and females, who often show lower achievement in science. Although the work will be conducted within an educational environment, the methods developed and studied will be broadly applicable, and this project should serve as an exemplar of the type of work that is needed in other computer-aided situations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85980</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85980</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n551" target="n1114">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Multimodal Human Computer Interaction: Toward a Proactive Computer</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This project is based on the belief that, to be more accessible to the general population, computers must be more proactive in their interactions with people. In human interaction, someone who waits for each command before making any communication attempt would be regarded as uncooperative and unhelpful. In order for a computer to be more proactive and, thus, to bear its part of the burden of initiation in interactions, it must have (1) much more real-time information about its user, and (2) algorithms that select actions based on this information rather than simply on user commands. The computer needs information about the user&apos;s current and past emotional, motivational and cognitive state as well as the state of the task at hand. A theory, is needed to guide the development of algorithms that select appropriate actions based on user and task state. This research constitutes the next steps in an attempt by the PI&apos;s multidisciplinary team to develop this capability.&lt;br/&gt;&lt;br/&gt;Proposed research includes: (1) further development of methods to sense user postures, movements, expressions and speech; (2) analysis and fusion of this information to identify and track user states; (3) task state tracking; (4) creating a corpus of emotion- and action-labeled videotapes for use with computer learning; (5) further development of affective communication; (6) development of the basis for human-centered state-based action decisions; and (7) evaluation of computer proaction on human behavior and response. The testbed is an environment for hands-on education in science and engineering, using the Lego Mindstorms construction and robotics environment, with children of middle school age. An emphasis will be on developing proactive computing methods for encouraging interest and conceptual development of minority children and females, who often show lower achievement in science. Although the work will be conducted within an educational environment, the methods developed and studied will be broadly applicable, and this project should serve as an exemplar of the type of work that is needed in other computer-aided situations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85980</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85980</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n551" target="n1115">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Multimodal Human Computer Interaction: Toward a Proactive Computer</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This project is based on the belief that, to be more accessible to the general population, computers must be more proactive in their interactions with people. In human interaction, someone who waits for each command before making any communication attempt would be regarded as uncooperative and unhelpful. In order for a computer to be more proactive and, thus, to bear its part of the burden of initiation in interactions, it must have (1) much more real-time information about its user, and (2) algorithms that select actions based on this information rather than simply on user commands. The computer needs information about the user&apos;s current and past emotional, motivational and cognitive state as well as the state of the task at hand. A theory, is needed to guide the development of algorithms that select appropriate actions based on user and task state. This research constitutes the next steps in an attempt by the PI&apos;s multidisciplinary team to develop this capability.&lt;br/&gt;&lt;br/&gt;Proposed research includes: (1) further development of methods to sense user postures, movements, expressions and speech; (2) analysis and fusion of this information to identify and track user states; (3) task state tracking; (4) creating a corpus of emotion- and action-labeled videotapes for use with computer learning; (5) further development of affective communication; (6) development of the basis for human-centered state-based action decisions; and (7) evaluation of computer proaction on human behavior and response. The testbed is an environment for hands-on education in science and engineering, using the Lego Mindstorms construction and robotics environment, with children of middle school age. An emphasis will be on developing proactive computing methods for encouraging interest and conceptual development of minority children and females, who often show lower achievement in science. Although the work will be conducted within an educational environment, the methods developed and studied will be broadly applicable, and this project should serve as an exemplar of the type of work that is needed in other computer-aided situations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85980</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85980</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n268" target="n551">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Multimodal Human Computer Interaction: Toward a Proactive Computer</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This project is based on the belief that, to be more accessible to the general population, computers must be more proactive in their interactions with people. In human interaction, someone who waits for each command before making any communication attempt would be regarded as uncooperative and unhelpful. In order for a computer to be more proactive and, thus, to bear its part of the burden of initiation in interactions, it must have (1) much more real-time information about its user, and (2) algorithms that select actions based on this information rather than simply on user commands. The computer needs information about the user&apos;s current and past emotional, motivational and cognitive state as well as the state of the task at hand. A theory, is needed to guide the development of algorithms that select appropriate actions based on user and task state. This research constitutes the next steps in an attempt by the PI&apos;s multidisciplinary team to develop this capability.&lt;br/&gt;&lt;br/&gt;Proposed research includes: (1) further development of methods to sense user postures, movements, expressions and speech; (2) analysis and fusion of this information to identify and track user states; (3) task state tracking; (4) creating a corpus of emotion- and action-labeled videotapes for use with computer learning; (5) further development of affective communication; (6) development of the basis for human-centered state-based action decisions; and (7) evaluation of computer proaction on human behavior and response. The testbed is an environment for hands-on education in science and engineering, using the Lego Mindstorms construction and robotics environment, with children of middle school age. An emphasis will be on developing proactive computing methods for encouraging interest and conceptual development of minority children and females, who often show lower achievement in science. Although the work will be conducted within an educational environment, the methods developed and studied will be broadly applicable, and this project should serve as an exemplar of the type of work that is needed in other computer-aided situations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85980</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85980</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1113" target="n1114">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Multimodal Human Computer Interaction: Toward a Proactive Computer</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This project is based on the belief that, to be more accessible to the general population, computers must be more proactive in their interactions with people. In human interaction, someone who waits for each command before making any communication attempt would be regarded as uncooperative and unhelpful. In order for a computer to be more proactive and, thus, to bear its part of the burden of initiation in interactions, it must have (1) much more real-time information about its user, and (2) algorithms that select actions based on this information rather than simply on user commands. The computer needs information about the user&apos;s current and past emotional, motivational and cognitive state as well as the state of the task at hand. A theory, is needed to guide the development of algorithms that select appropriate actions based on user and task state. This research constitutes the next steps in an attempt by the PI&apos;s multidisciplinary team to develop this capability.&lt;br/&gt;&lt;br/&gt;Proposed research includes: (1) further development of methods to sense user postures, movements, expressions and speech; (2) analysis and fusion of this information to identify and track user states; (3) task state tracking; (4) creating a corpus of emotion- and action-labeled videotapes for use with computer learning; (5) further development of affective communication; (6) development of the basis for human-centered state-based action decisions; and (7) evaluation of computer proaction on human behavior and response. The testbed is an environment for hands-on education in science and engineering, using the Lego Mindstorms construction and robotics environment, with children of middle school age. An emphasis will be on developing proactive computing methods for encouraging interest and conceptual development of minority children and females, who often show lower achievement in science. Although the work will be conducted within an educational environment, the methods developed and studied will be broadly applicable, and this project should serve as an exemplar of the type of work that is needed in other computer-aided situations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85980</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85980</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1113" target="n1115">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Multimodal Human Computer Interaction: Toward a Proactive Computer</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This project is based on the belief that, to be more accessible to the general population, computers must be more proactive in their interactions with people. In human interaction, someone who waits for each command before making any communication attempt would be regarded as uncooperative and unhelpful. In order for a computer to be more proactive and, thus, to bear its part of the burden of initiation in interactions, it must have (1) much more real-time information about its user, and (2) algorithms that select actions based on this information rather than simply on user commands. The computer needs information about the user&apos;s current and past emotional, motivational and cognitive state as well as the state of the task at hand. A theory, is needed to guide the development of algorithms that select appropriate actions based on user and task state. This research constitutes the next steps in an attempt by the PI&apos;s multidisciplinary team to develop this capability.&lt;br/&gt;&lt;br/&gt;Proposed research includes: (1) further development of methods to sense user postures, movements, expressions and speech; (2) analysis and fusion of this information to identify and track user states; (3) task state tracking; (4) creating a corpus of emotion- and action-labeled videotapes for use with computer learning; (5) further development of affective communication; (6) development of the basis for human-centered state-based action decisions; and (7) evaluation of computer proaction on human behavior and response. The testbed is an environment for hands-on education in science and engineering, using the Lego Mindstorms construction and robotics environment, with children of middle school age. An emphasis will be on developing proactive computing methods for encouraging interest and conceptual development of minority children and females, who often show lower achievement in science. Although the work will be conducted within an educational environment, the methods developed and studied will be broadly applicable, and this project should serve as an exemplar of the type of work that is needed in other computer-aided situations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85980</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85980</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n268" target="n1113">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Multimodal Human Computer Interaction: Toward a Proactive Computer</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This project is based on the belief that, to be more accessible to the general population, computers must be more proactive in their interactions with people. In human interaction, someone who waits for each command before making any communication attempt would be regarded as uncooperative and unhelpful. In order for a computer to be more proactive and, thus, to bear its part of the burden of initiation in interactions, it must have (1) much more real-time information about its user, and (2) algorithms that select actions based on this information rather than simply on user commands. The computer needs information about the user&apos;s current and past emotional, motivational and cognitive state as well as the state of the task at hand. A theory, is needed to guide the development of algorithms that select appropriate actions based on user and task state. This research constitutes the next steps in an attempt by the PI&apos;s multidisciplinary team to develop this capability.&lt;br/&gt;&lt;br/&gt;Proposed research includes: (1) further development of methods to sense user postures, movements, expressions and speech; (2) analysis and fusion of this information to identify and track user states; (3) task state tracking; (4) creating a corpus of emotion- and action-labeled videotapes for use with computer learning; (5) further development of affective communication; (6) development of the basis for human-centered state-based action decisions; and (7) evaluation of computer proaction on human behavior and response. The testbed is an environment for hands-on education in science and engineering, using the Lego Mindstorms construction and robotics environment, with children of middle school age. An emphasis will be on developing proactive computing methods for encouraging interest and conceptual development of minority children and females, who often show lower achievement in science. Although the work will be conducted within an educational environment, the methods developed and studied will be broadly applicable, and this project should serve as an exemplar of the type of work that is needed in other computer-aided situations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85980</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85980</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1114" target="n1115">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Multimodal Human Computer Interaction: Toward a Proactive Computer</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This project is based on the belief that, to be more accessible to the general population, computers must be more proactive in their interactions with people. In human interaction, someone who waits for each command before making any communication attempt would be regarded as uncooperative and unhelpful. In order for a computer to be more proactive and, thus, to bear its part of the burden of initiation in interactions, it must have (1) much more real-time information about its user, and (2) algorithms that select actions based on this information rather than simply on user commands. The computer needs information about the user&apos;s current and past emotional, motivational and cognitive state as well as the state of the task at hand. A theory, is needed to guide the development of algorithms that select appropriate actions based on user and task state. This research constitutes the next steps in an attempt by the PI&apos;s multidisciplinary team to develop this capability.&lt;br/&gt;&lt;br/&gt;Proposed research includes: (1) further development of methods to sense user postures, movements, expressions and speech; (2) analysis and fusion of this information to identify and track user states; (3) task state tracking; (4) creating a corpus of emotion- and action-labeled videotapes for use with computer learning; (5) further development of affective communication; (6) development of the basis for human-centered state-based action decisions; and (7) evaluation of computer proaction on human behavior and response. The testbed is an environment for hands-on education in science and engineering, using the Lego Mindstorms construction and robotics environment, with children of middle school age. An emphasis will be on developing proactive computing methods for encouraging interest and conceptual development of minority children and females, who often show lower achievement in science. Although the work will be conducted within an educational environment, the methods developed and studied will be broadly applicable, and this project should serve as an exemplar of the type of work that is needed in other computer-aided situations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85980</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85980</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n268" target="n1114">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Multimodal Human Computer Interaction: Toward a Proactive Computer</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This project is based on the belief that, to be more accessible to the general population, computers must be more proactive in their interactions with people. In human interaction, someone who waits for each command before making any communication attempt would be regarded as uncooperative and unhelpful. In order for a computer to be more proactive and, thus, to bear its part of the burden of initiation in interactions, it must have (1) much more real-time information about its user, and (2) algorithms that select actions based on this information rather than simply on user commands. The computer needs information about the user&apos;s current and past emotional, motivational and cognitive state as well as the state of the task at hand. A theory, is needed to guide the development of algorithms that select appropriate actions based on user and task state. This research constitutes the next steps in an attempt by the PI&apos;s multidisciplinary team to develop this capability.&lt;br/&gt;&lt;br/&gt;Proposed research includes: (1) further development of methods to sense user postures, movements, expressions and speech; (2) analysis and fusion of this information to identify and track user states; (3) task state tracking; (4) creating a corpus of emotion- and action-labeled videotapes for use with computer learning; (5) further development of affective communication; (6) development of the basis for human-centered state-based action decisions; and (7) evaluation of computer proaction on human behavior and response. The testbed is an environment for hands-on education in science and engineering, using the Lego Mindstorms construction and robotics environment, with children of middle school age. An emphasis will be on developing proactive computing methods for encouraging interest and conceptual development of minority children and females, who often show lower achievement in science. Although the work will be conducted within an educational environment, the methods developed and studied will be broadly applicable, and this project should serve as an exemplar of the type of work that is needed in other computer-aided situations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85980</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85980</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n268" target="n1115">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Multimodal Human Computer Interaction: Toward a Proactive Computer</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. This project is based on the belief that, to be more accessible to the general population, computers must be more proactive in their interactions with people. In human interaction, someone who waits for each command before making any communication attempt would be regarded as uncooperative and unhelpful. In order for a computer to be more proactive and, thus, to bear its part of the burden of initiation in interactions, it must have (1) much more real-time information about its user, and (2) algorithms that select actions based on this information rather than simply on user commands. The computer needs information about the user&apos;s current and past emotional, motivational and cognitive state as well as the state of the task at hand. A theory, is needed to guide the development of algorithms that select appropriate actions based on user and task state. This research constitutes the next steps in an attempt by the PI&apos;s multidisciplinary team to develop this capability.&lt;br/&gt;&lt;br/&gt;Proposed research includes: (1) further development of methods to sense user postures, movements, expressions and speech; (2) analysis and fusion of this information to identify and track user states; (3) task state tracking; (4) creating a corpus of emotion- and action-labeled videotapes for use with computer learning; (5) further development of affective communication; (6) development of the basis for human-centered state-based action decisions; and (7) evaluation of computer proaction on human behavior and response. The testbed is an environment for hands-on education in science and engineering, using the Lego Mindstorms construction and robotics environment, with children of middle school age. An emphasis will be on developing proactive computing methods for encouraging interest and conceptual development of minority children and females, who often show lower achievement in science. Although the work will be conducted within an educational environment, the methods developed and studied will be broadly applicable, and this project should serve as an exemplar of the type of work that is needed in other computer-aided situations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85980</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85980</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n501" target="n793">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: A Multiresolution Analysis for the Global Internet</data>
      <data key="e_abstract">An interdisciplinary team, bridging academia and industry, proposes a united effort to study the dynamics of the global Internet, moving beyond the traditional single-timescale, single-network, single-protocol paradigm to a description that compactly incorporates a wide range of time-scales, a broad spectrum of spatial network topology structures, andmultiple protocols interacting with one another and across the different networking layers. Achieving such a global, multi-scale, and multi-layer understanding of complex large-scale networks is imperative for the successful design and development of the next-generation Internet protocols and engineering tools, where issues related to robustness,scalability, and efficiency take center stage.&lt;br/&gt; In this united effort, there are three main ingredients. First, the researchers plan to fully exploit a new breed of datasets of network-wide measurements - unprecedented in volume and quality - that are the result of recent exciting networking research projects, such as the National Internet Measurement Infrastructure project (NIMI). Another source of such data will be various Internet Service Providers (ISPs), such as AT&amp;T, employer of one of the PIs, which will also provide supplementary funding if this proposal is accepted. Second, the researchers will rely on a new breed of network simulation tools, such as SSFNET, largely developed by another of our PIs, and capable of simulating internetworks unprecedented in scale and detail. All these new measurements, whether real or virtual, will constitute huge datasets with very high and networking-specific semantic content, creating completely novel challenges for data analysis. This is where the third ingredient comes in: multiscale and multiresolution/wavelet techniques, developed by several of the PIs, will take center stage when it comes to analyzing, visualizing, and uncovering the rich information that is contained in these network measurements. Although the flexibility and the speed of wavelet decompositions have been put to good use in the past in many applications, including the empirical observation of certain types of time-scaling behaviors in measured network traffic, the technology as it is known and used today cannot yet cope with the fascinating new challenges posed by the available and anticipated Internet data. A central objective of this project is to develop Internet-appropriate multiresolution techniques that match the multiscale nature of the underlying Internet structure and can be validated step-by-step against measurements. The researchers expect that tools and theories that have been developed in the context of computer graphics, irregular sampling, and scattered data approximation will be utilized to this end.&lt;br/&gt; The ultimate goal of the proposed research effort is to identify interesting patterns that can be extracted from the measured data via fast algorithms, that are linked to physical concepts and are meaningful in the networking context, that characterize different states or behaviors of the network, and that can aid the development of novel network measurement analysis and visualization techniques in support of a new generation of monitoring and engineering tools for future Internet architectures. Given that so much of this area is still uncharted, it may be not realistic to hope to attain this goal in a few years&apos; time. Nevertheless, we are convinced that only an interdisciplinary effort like ours can hope to achieve anything in this direction; we expect that our collaboration will lead to deeper insights and understanding; a first identification of models, patterns, the influences of various external factors and protocols; and an initial glimpse at the underlying &quot;physics of the Internet&quot; - a solid understanding of how basic networking mechanisms and user behaviors contribute to the fascinating dynamic observed in today&apos;s Internet.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85984</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85984</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n501" target="n1119">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: A Multiresolution Analysis for the Global Internet</data>
      <data key="e_abstract">An interdisciplinary team, bridging academia and industry, proposes a united effort to study the dynamics of the global Internet, moving beyond the traditional single-timescale, single-network, single-protocol paradigm to a description that compactly incorporates a wide range of time-scales, a broad spectrum of spatial network topology structures, andmultiple protocols interacting with one another and across the different networking layers. Achieving such a global, multi-scale, and multi-layer understanding of complex large-scale networks is imperative for the successful design and development of the next-generation Internet protocols and engineering tools, where issues related to robustness,scalability, and efficiency take center stage.&lt;br/&gt; In this united effort, there are three main ingredients. First, the researchers plan to fully exploit a new breed of datasets of network-wide measurements - unprecedented in volume and quality - that are the result of recent exciting networking research projects, such as the National Internet Measurement Infrastructure project (NIMI). Another source of such data will be various Internet Service Providers (ISPs), such as AT&amp;T, employer of one of the PIs, which will also provide supplementary funding if this proposal is accepted. Second, the researchers will rely on a new breed of network simulation tools, such as SSFNET, largely developed by another of our PIs, and capable of simulating internetworks unprecedented in scale and detail. All these new measurements, whether real or virtual, will constitute huge datasets with very high and networking-specific semantic content, creating completely novel challenges for data analysis. This is where the third ingredient comes in: multiscale and multiresolution/wavelet techniques, developed by several of the PIs, will take center stage when it comes to analyzing, visualizing, and uncovering the rich information that is contained in these network measurements. Although the flexibility and the speed of wavelet decompositions have been put to good use in the past in many applications, including the empirical observation of certain types of time-scaling behaviors in measured network traffic, the technology as it is known and used today cannot yet cope with the fascinating new challenges posed by the available and anticipated Internet data. A central objective of this project is to develop Internet-appropriate multiresolution techniques that match the multiscale nature of the underlying Internet structure and can be validated step-by-step against measurements. The researchers expect that tools and theories that have been developed in the context of computer graphics, irregular sampling, and scattered data approximation will be utilized to this end.&lt;br/&gt; The ultimate goal of the proposed research effort is to identify interesting patterns that can be extracted from the measured data via fast algorithms, that are linked to physical concepts and are meaningful in the networking context, that characterize different states or behaviors of the network, and that can aid the development of novel network measurement analysis and visualization techniques in support of a new generation of monitoring and engineering tools for future Internet architectures. Given that so much of this area is still uncharted, it may be not realistic to hope to attain this goal in a few years&apos; time. Nevertheless, we are convinced that only an interdisciplinary effort like ours can hope to achieve anything in this direction; we expect that our collaboration will lead to deeper insights and understanding; a first identification of models, patterns, the influences of various external factors and protocols; and an initial glimpse at the underlying &quot;physics of the Internet&quot; - a solid understanding of how basic networking mechanisms and user behaviors contribute to the fascinating dynamic observed in today&apos;s Internet.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85984</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85984</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n501" target="n1120">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: A Multiresolution Analysis for the Global Internet</data>
      <data key="e_abstract">An interdisciplinary team, bridging academia and industry, proposes a united effort to study the dynamics of the global Internet, moving beyond the traditional single-timescale, single-network, single-protocol paradigm to a description that compactly incorporates a wide range of time-scales, a broad spectrum of spatial network topology structures, andmultiple protocols interacting with one another and across the different networking layers. Achieving such a global, multi-scale, and multi-layer understanding of complex large-scale networks is imperative for the successful design and development of the next-generation Internet protocols and engineering tools, where issues related to robustness,scalability, and efficiency take center stage.&lt;br/&gt; In this united effort, there are three main ingredients. First, the researchers plan to fully exploit a new breed of datasets of network-wide measurements - unprecedented in volume and quality - that are the result of recent exciting networking research projects, such as the National Internet Measurement Infrastructure project (NIMI). Another source of such data will be various Internet Service Providers (ISPs), such as AT&amp;T, employer of one of the PIs, which will also provide supplementary funding if this proposal is accepted. Second, the researchers will rely on a new breed of network simulation tools, such as SSFNET, largely developed by another of our PIs, and capable of simulating internetworks unprecedented in scale and detail. All these new measurements, whether real or virtual, will constitute huge datasets with very high and networking-specific semantic content, creating completely novel challenges for data analysis. This is where the third ingredient comes in: multiscale and multiresolution/wavelet techniques, developed by several of the PIs, will take center stage when it comes to analyzing, visualizing, and uncovering the rich information that is contained in these network measurements. Although the flexibility and the speed of wavelet decompositions have been put to good use in the past in many applications, including the empirical observation of certain types of time-scaling behaviors in measured network traffic, the technology as it is known and used today cannot yet cope with the fascinating new challenges posed by the available and anticipated Internet data. A central objective of this project is to develop Internet-appropriate multiresolution techniques that match the multiscale nature of the underlying Internet structure and can be validated step-by-step against measurements. The researchers expect that tools and theories that have been developed in the context of computer graphics, irregular sampling, and scattered data approximation will be utilized to this end.&lt;br/&gt; The ultimate goal of the proposed research effort is to identify interesting patterns that can be extracted from the measured data via fast algorithms, that are linked to physical concepts and are meaningful in the networking context, that characterize different states or behaviors of the network, and that can aid the development of novel network measurement analysis and visualization techniques in support of a new generation of monitoring and engineering tools for future Internet architectures. Given that so much of this area is still uncharted, it may be not realistic to hope to attain this goal in a few years&apos; time. Nevertheless, we are convinced that only an interdisciplinary effort like ours can hope to achieve anything in this direction; we expect that our collaboration will lead to deeper insights and understanding; a first identification of models, patterns, the influences of various external factors and protocols; and an initial glimpse at the underlying &quot;physics of the Internet&quot; - a solid understanding of how basic networking mechanisms and user behaviors contribute to the fascinating dynamic observed in today&apos;s Internet.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85984</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85984</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n501" target="n1121">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: A Multiresolution Analysis for the Global Internet</data>
      <data key="e_abstract">An interdisciplinary team, bridging academia and industry, proposes a united effort to study the dynamics of the global Internet, moving beyond the traditional single-timescale, single-network, single-protocol paradigm to a description that compactly incorporates a wide range of time-scales, a broad spectrum of spatial network topology structures, andmultiple protocols interacting with one another and across the different networking layers. Achieving such a global, multi-scale, and multi-layer understanding of complex large-scale networks is imperative for the successful design and development of the next-generation Internet protocols and engineering tools, where issues related to robustness,scalability, and efficiency take center stage.&lt;br/&gt; In this united effort, there are three main ingredients. First, the researchers plan to fully exploit a new breed of datasets of network-wide measurements - unprecedented in volume and quality - that are the result of recent exciting networking research projects, such as the National Internet Measurement Infrastructure project (NIMI). Another source of such data will be various Internet Service Providers (ISPs), such as AT&amp;T, employer of one of the PIs, which will also provide supplementary funding if this proposal is accepted. Second, the researchers will rely on a new breed of network simulation tools, such as SSFNET, largely developed by another of our PIs, and capable of simulating internetworks unprecedented in scale and detail. All these new measurements, whether real or virtual, will constitute huge datasets with very high and networking-specific semantic content, creating completely novel challenges for data analysis. This is where the third ingredient comes in: multiscale and multiresolution/wavelet techniques, developed by several of the PIs, will take center stage when it comes to analyzing, visualizing, and uncovering the rich information that is contained in these network measurements. Although the flexibility and the speed of wavelet decompositions have been put to good use in the past in many applications, including the empirical observation of certain types of time-scaling behaviors in measured network traffic, the technology as it is known and used today cannot yet cope with the fascinating new challenges posed by the available and anticipated Internet data. A central objective of this project is to develop Internet-appropriate multiresolution techniques that match the multiscale nature of the underlying Internet structure and can be validated step-by-step against measurements. The researchers expect that tools and theories that have been developed in the context of computer graphics, irregular sampling, and scattered data approximation will be utilized to this end.&lt;br/&gt; The ultimate goal of the proposed research effort is to identify interesting patterns that can be extracted from the measured data via fast algorithms, that are linked to physical concepts and are meaningful in the networking context, that characterize different states or behaviors of the network, and that can aid the development of novel network measurement analysis and visualization techniques in support of a new generation of monitoring and engineering tools for future Internet architectures. Given that so much of this area is still uncharted, it may be not realistic to hope to attain this goal in a few years&apos; time. Nevertheless, we are convinced that only an interdisciplinary effort like ours can hope to achieve anything in this direction; we expect that our collaboration will lead to deeper insights and understanding; a first identification of models, patterns, the influences of various external factors and protocols; and an initial glimpse at the underlying &quot;physics of the Internet&quot; - a solid understanding of how basic networking mechanisms and user behaviors contribute to the fascinating dynamic observed in today&apos;s Internet.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85984</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85984</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n793" target="n1119">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: A Multiresolution Analysis for the Global Internet</data>
      <data key="e_abstract">An interdisciplinary team, bridging academia and industry, proposes a united effort to study the dynamics of the global Internet, moving beyond the traditional single-timescale, single-network, single-protocol paradigm to a description that compactly incorporates a wide range of time-scales, a broad spectrum of spatial network topology structures, andmultiple protocols interacting with one another and across the different networking layers. Achieving such a global, multi-scale, and multi-layer understanding of complex large-scale networks is imperative for the successful design and development of the next-generation Internet protocols and engineering tools, where issues related to robustness,scalability, and efficiency take center stage.&lt;br/&gt; In this united effort, there are three main ingredients. First, the researchers plan to fully exploit a new breed of datasets of network-wide measurements - unprecedented in volume and quality - that are the result of recent exciting networking research projects, such as the National Internet Measurement Infrastructure project (NIMI). Another source of such data will be various Internet Service Providers (ISPs), such as AT&amp;T, employer of one of the PIs, which will also provide supplementary funding if this proposal is accepted. Second, the researchers will rely on a new breed of network simulation tools, such as SSFNET, largely developed by another of our PIs, and capable of simulating internetworks unprecedented in scale and detail. All these new measurements, whether real or virtual, will constitute huge datasets with very high and networking-specific semantic content, creating completely novel challenges for data analysis. This is where the third ingredient comes in: multiscale and multiresolution/wavelet techniques, developed by several of the PIs, will take center stage when it comes to analyzing, visualizing, and uncovering the rich information that is contained in these network measurements. Although the flexibility and the speed of wavelet decompositions have been put to good use in the past in many applications, including the empirical observation of certain types of time-scaling behaviors in measured network traffic, the technology as it is known and used today cannot yet cope with the fascinating new challenges posed by the available and anticipated Internet data. A central objective of this project is to develop Internet-appropriate multiresolution techniques that match the multiscale nature of the underlying Internet structure and can be validated step-by-step against measurements. The researchers expect that tools and theories that have been developed in the context of computer graphics, irregular sampling, and scattered data approximation will be utilized to this end.&lt;br/&gt; The ultimate goal of the proposed research effort is to identify interesting patterns that can be extracted from the measured data via fast algorithms, that are linked to physical concepts and are meaningful in the networking context, that characterize different states or behaviors of the network, and that can aid the development of novel network measurement analysis and visualization techniques in support of a new generation of monitoring and engineering tools for future Internet architectures. Given that so much of this area is still uncharted, it may be not realistic to hope to attain this goal in a few years&apos; time. Nevertheless, we are convinced that only an interdisciplinary effort like ours can hope to achieve anything in this direction; we expect that our collaboration will lead to deeper insights and understanding; a first identification of models, patterns, the influences of various external factors and protocols; and an initial glimpse at the underlying &quot;physics of the Internet&quot; - a solid understanding of how basic networking mechanisms and user behaviors contribute to the fascinating dynamic observed in today&apos;s Internet.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85984</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85984</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n793" target="n1120">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: A Multiresolution Analysis for the Global Internet</data>
      <data key="e_abstract">An interdisciplinary team, bridging academia and industry, proposes a united effort to study the dynamics of the global Internet, moving beyond the traditional single-timescale, single-network, single-protocol paradigm to a description that compactly incorporates a wide range of time-scales, a broad spectrum of spatial network topology structures, andmultiple protocols interacting with one another and across the different networking layers. Achieving such a global, multi-scale, and multi-layer understanding of complex large-scale networks is imperative for the successful design and development of the next-generation Internet protocols and engineering tools, where issues related to robustness,scalability, and efficiency take center stage.&lt;br/&gt; In this united effort, there are three main ingredients. First, the researchers plan to fully exploit a new breed of datasets of network-wide measurements - unprecedented in volume and quality - that are the result of recent exciting networking research projects, such as the National Internet Measurement Infrastructure project (NIMI). Another source of such data will be various Internet Service Providers (ISPs), such as AT&amp;T, employer of one of the PIs, which will also provide supplementary funding if this proposal is accepted. Second, the researchers will rely on a new breed of network simulation tools, such as SSFNET, largely developed by another of our PIs, and capable of simulating internetworks unprecedented in scale and detail. All these new measurements, whether real or virtual, will constitute huge datasets with very high and networking-specific semantic content, creating completely novel challenges for data analysis. This is where the third ingredient comes in: multiscale and multiresolution/wavelet techniques, developed by several of the PIs, will take center stage when it comes to analyzing, visualizing, and uncovering the rich information that is contained in these network measurements. Although the flexibility and the speed of wavelet decompositions have been put to good use in the past in many applications, including the empirical observation of certain types of time-scaling behaviors in measured network traffic, the technology as it is known and used today cannot yet cope with the fascinating new challenges posed by the available and anticipated Internet data. A central objective of this project is to develop Internet-appropriate multiresolution techniques that match the multiscale nature of the underlying Internet structure and can be validated step-by-step against measurements. The researchers expect that tools and theories that have been developed in the context of computer graphics, irregular sampling, and scattered data approximation will be utilized to this end.&lt;br/&gt; The ultimate goal of the proposed research effort is to identify interesting patterns that can be extracted from the measured data via fast algorithms, that are linked to physical concepts and are meaningful in the networking context, that characterize different states or behaviors of the network, and that can aid the development of novel network measurement analysis and visualization techniques in support of a new generation of monitoring and engineering tools for future Internet architectures. Given that so much of this area is still uncharted, it may be not realistic to hope to attain this goal in a few years&apos; time. Nevertheless, we are convinced that only an interdisciplinary effort like ours can hope to achieve anything in this direction; we expect that our collaboration will lead to deeper insights and understanding; a first identification of models, patterns, the influences of various external factors and protocols; and an initial glimpse at the underlying &quot;physics of the Internet&quot; - a solid understanding of how basic networking mechanisms and user behaviors contribute to the fascinating dynamic observed in today&apos;s Internet.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85984</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85984</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n793" target="n1121">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: A Multiresolution Analysis for the Global Internet</data>
      <data key="e_abstract">An interdisciplinary team, bridging academia and industry, proposes a united effort to study the dynamics of the global Internet, moving beyond the traditional single-timescale, single-network, single-protocol paradigm to a description that compactly incorporates a wide range of time-scales, a broad spectrum of spatial network topology structures, andmultiple protocols interacting with one another and across the different networking layers. Achieving such a global, multi-scale, and multi-layer understanding of complex large-scale networks is imperative for the successful design and development of the next-generation Internet protocols and engineering tools, where issues related to robustness,scalability, and efficiency take center stage.&lt;br/&gt; In this united effort, there are three main ingredients. First, the researchers plan to fully exploit a new breed of datasets of network-wide measurements - unprecedented in volume and quality - that are the result of recent exciting networking research projects, such as the National Internet Measurement Infrastructure project (NIMI). Another source of such data will be various Internet Service Providers (ISPs), such as AT&amp;T, employer of one of the PIs, which will also provide supplementary funding if this proposal is accepted. Second, the researchers will rely on a new breed of network simulation tools, such as SSFNET, largely developed by another of our PIs, and capable of simulating internetworks unprecedented in scale and detail. All these new measurements, whether real or virtual, will constitute huge datasets with very high and networking-specific semantic content, creating completely novel challenges for data analysis. This is where the third ingredient comes in: multiscale and multiresolution/wavelet techniques, developed by several of the PIs, will take center stage when it comes to analyzing, visualizing, and uncovering the rich information that is contained in these network measurements. Although the flexibility and the speed of wavelet decompositions have been put to good use in the past in many applications, including the empirical observation of certain types of time-scaling behaviors in measured network traffic, the technology as it is known and used today cannot yet cope with the fascinating new challenges posed by the available and anticipated Internet data. A central objective of this project is to develop Internet-appropriate multiresolution techniques that match the multiscale nature of the underlying Internet structure and can be validated step-by-step against measurements. The researchers expect that tools and theories that have been developed in the context of computer graphics, irregular sampling, and scattered data approximation will be utilized to this end.&lt;br/&gt; The ultimate goal of the proposed research effort is to identify interesting patterns that can be extracted from the measured data via fast algorithms, that are linked to physical concepts and are meaningful in the networking context, that characterize different states or behaviors of the network, and that can aid the development of novel network measurement analysis and visualization techniques in support of a new generation of monitoring and engineering tools for future Internet architectures. Given that so much of this area is still uncharted, it may be not realistic to hope to attain this goal in a few years&apos; time. Nevertheless, we are convinced that only an interdisciplinary effort like ours can hope to achieve anything in this direction; we expect that our collaboration will lead to deeper insights and understanding; a first identification of models, patterns, the influences of various external factors and protocols; and an initial glimpse at the underlying &quot;physics of the Internet&quot; - a solid understanding of how basic networking mechanisms and user behaviors contribute to the fascinating dynamic observed in today&apos;s Internet.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85984</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85984</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1119" target="n1120">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: A Multiresolution Analysis for the Global Internet</data>
      <data key="e_abstract">An interdisciplinary team, bridging academia and industry, proposes a united effort to study the dynamics of the global Internet, moving beyond the traditional single-timescale, single-network, single-protocol paradigm to a description that compactly incorporates a wide range of time-scales, a broad spectrum of spatial network topology structures, andmultiple protocols interacting with one another and across the different networking layers. Achieving such a global, multi-scale, and multi-layer understanding of complex large-scale networks is imperative for the successful design and development of the next-generation Internet protocols and engineering tools, where issues related to robustness,scalability, and efficiency take center stage.&lt;br/&gt; In this united effort, there are three main ingredients. First, the researchers plan to fully exploit a new breed of datasets of network-wide measurements - unprecedented in volume and quality - that are the result of recent exciting networking research projects, such as the National Internet Measurement Infrastructure project (NIMI). Another source of such data will be various Internet Service Providers (ISPs), such as AT&amp;T, employer of one of the PIs, which will also provide supplementary funding if this proposal is accepted. Second, the researchers will rely on a new breed of network simulation tools, such as SSFNET, largely developed by another of our PIs, and capable of simulating internetworks unprecedented in scale and detail. All these new measurements, whether real or virtual, will constitute huge datasets with very high and networking-specific semantic content, creating completely novel challenges for data analysis. This is where the third ingredient comes in: multiscale and multiresolution/wavelet techniques, developed by several of the PIs, will take center stage when it comes to analyzing, visualizing, and uncovering the rich information that is contained in these network measurements. Although the flexibility and the speed of wavelet decompositions have been put to good use in the past in many applications, including the empirical observation of certain types of time-scaling behaviors in measured network traffic, the technology as it is known and used today cannot yet cope with the fascinating new challenges posed by the available and anticipated Internet data. A central objective of this project is to develop Internet-appropriate multiresolution techniques that match the multiscale nature of the underlying Internet structure and can be validated step-by-step against measurements. The researchers expect that tools and theories that have been developed in the context of computer graphics, irregular sampling, and scattered data approximation will be utilized to this end.&lt;br/&gt; The ultimate goal of the proposed research effort is to identify interesting patterns that can be extracted from the measured data via fast algorithms, that are linked to physical concepts and are meaningful in the networking context, that characterize different states or behaviors of the network, and that can aid the development of novel network measurement analysis and visualization techniques in support of a new generation of monitoring and engineering tools for future Internet architectures. Given that so much of this area is still uncharted, it may be not realistic to hope to attain this goal in a few years&apos; time. Nevertheless, we are convinced that only an interdisciplinary effort like ours can hope to achieve anything in this direction; we expect that our collaboration will lead to deeper insights and understanding; a first identification of models, patterns, the influences of various external factors and protocols; and an initial glimpse at the underlying &quot;physics of the Internet&quot; - a solid understanding of how basic networking mechanisms and user behaviors contribute to the fascinating dynamic observed in today&apos;s Internet.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85984</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85984</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1119" target="n1121">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: A Multiresolution Analysis for the Global Internet</data>
      <data key="e_abstract">An interdisciplinary team, bridging academia and industry, proposes a united effort to study the dynamics of the global Internet, moving beyond the traditional single-timescale, single-network, single-protocol paradigm to a description that compactly incorporates a wide range of time-scales, a broad spectrum of spatial network topology structures, andmultiple protocols interacting with one another and across the different networking layers. Achieving such a global, multi-scale, and multi-layer understanding of complex large-scale networks is imperative for the successful design and development of the next-generation Internet protocols and engineering tools, where issues related to robustness,scalability, and efficiency take center stage.&lt;br/&gt; In this united effort, there are three main ingredients. First, the researchers plan to fully exploit a new breed of datasets of network-wide measurements - unprecedented in volume and quality - that are the result of recent exciting networking research projects, such as the National Internet Measurement Infrastructure project (NIMI). Another source of such data will be various Internet Service Providers (ISPs), such as AT&amp;T, employer of one of the PIs, which will also provide supplementary funding if this proposal is accepted. Second, the researchers will rely on a new breed of network simulation tools, such as SSFNET, largely developed by another of our PIs, and capable of simulating internetworks unprecedented in scale and detail. All these new measurements, whether real or virtual, will constitute huge datasets with very high and networking-specific semantic content, creating completely novel challenges for data analysis. This is where the third ingredient comes in: multiscale and multiresolution/wavelet techniques, developed by several of the PIs, will take center stage when it comes to analyzing, visualizing, and uncovering the rich information that is contained in these network measurements. Although the flexibility and the speed of wavelet decompositions have been put to good use in the past in many applications, including the empirical observation of certain types of time-scaling behaviors in measured network traffic, the technology as it is known and used today cannot yet cope with the fascinating new challenges posed by the available and anticipated Internet data. A central objective of this project is to develop Internet-appropriate multiresolution techniques that match the multiscale nature of the underlying Internet structure and can be validated step-by-step against measurements. The researchers expect that tools and theories that have been developed in the context of computer graphics, irregular sampling, and scattered data approximation will be utilized to this end.&lt;br/&gt; The ultimate goal of the proposed research effort is to identify interesting patterns that can be extracted from the measured data via fast algorithms, that are linked to physical concepts and are meaningful in the networking context, that characterize different states or behaviors of the network, and that can aid the development of novel network measurement analysis and visualization techniques in support of a new generation of monitoring and engineering tools for future Internet architectures. Given that so much of this area is still uncharted, it may be not realistic to hope to attain this goal in a few years&apos; time. Nevertheless, we are convinced that only an interdisciplinary effort like ours can hope to achieve anything in this direction; we expect that our collaboration will lead to deeper insights and understanding; a first identification of models, patterns, the influences of various external factors and protocols; and an initial glimpse at the underlying &quot;physics of the Internet&quot; - a solid understanding of how basic networking mechanisms and user behaviors contribute to the fascinating dynamic observed in today&apos;s Internet.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85984</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85984</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1120" target="n1121">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: A Multiresolution Analysis for the Global Internet</data>
      <data key="e_abstract">An interdisciplinary team, bridging academia and industry, proposes a united effort to study the dynamics of the global Internet, moving beyond the traditional single-timescale, single-network, single-protocol paradigm to a description that compactly incorporates a wide range of time-scales, a broad spectrum of spatial network topology structures, andmultiple protocols interacting with one another and across the different networking layers. Achieving such a global, multi-scale, and multi-layer understanding of complex large-scale networks is imperative for the successful design and development of the next-generation Internet protocols and engineering tools, where issues related to robustness,scalability, and efficiency take center stage.&lt;br/&gt; In this united effort, there are three main ingredients. First, the researchers plan to fully exploit a new breed of datasets of network-wide measurements - unprecedented in volume and quality - that are the result of recent exciting networking research projects, such as the National Internet Measurement Infrastructure project (NIMI). Another source of such data will be various Internet Service Providers (ISPs), such as AT&amp;T, employer of one of the PIs, which will also provide supplementary funding if this proposal is accepted. Second, the researchers will rely on a new breed of network simulation tools, such as SSFNET, largely developed by another of our PIs, and capable of simulating internetworks unprecedented in scale and detail. All these new measurements, whether real or virtual, will constitute huge datasets with very high and networking-specific semantic content, creating completely novel challenges for data analysis. This is where the third ingredient comes in: multiscale and multiresolution/wavelet techniques, developed by several of the PIs, will take center stage when it comes to analyzing, visualizing, and uncovering the rich information that is contained in these network measurements. Although the flexibility and the speed of wavelet decompositions have been put to good use in the past in many applications, including the empirical observation of certain types of time-scaling behaviors in measured network traffic, the technology as it is known and used today cannot yet cope with the fascinating new challenges posed by the available and anticipated Internet data. A central objective of this project is to develop Internet-appropriate multiresolution techniques that match the multiscale nature of the underlying Internet structure and can be validated step-by-step against measurements. The researchers expect that tools and theories that have been developed in the context of computer graphics, irregular sampling, and scattered data approximation will be utilized to this end.&lt;br/&gt; The ultimate goal of the proposed research effort is to identify interesting patterns that can be extracted from the measured data via fast algorithms, that are linked to physical concepts and are meaningful in the networking context, that characterize different states or behaviors of the network, and that can aid the development of novel network measurement analysis and visualization techniques in support of a new generation of monitoring and engineering tools for future Internet architectures. Given that so much of this area is still uncharted, it may be not realistic to hope to attain this goal in a few years&apos; time. Nevertheless, we are convinced that only an interdisciplinary effort like ours can hope to achieve anything in this direction; we expect that our collaboration will lead to deeper insights and understanding; a first identification of models, patterns, the influences of various external factors and protocols; and an initial glimpse at the underlying &quot;physics of the Internet&quot; - a solid understanding of how basic networking mechanisms and user behaviors contribute to the fascinating dynamic observed in today&apos;s Internet.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85984</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85984</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1122" target="n1123">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Algorithms: From Theory to Application</data>
      <data key="e_abstract">With the explosion in connectivity of computers and in the size of data sets available for analysis, mathematically sophisticated algorithms are becoming increasingly crucial for a wide range of real-world applications. Unfortunately, it often takes many years for an algorithm to make it from theory into applications. In fact, the trend has been for different areas to develop their own algorithms independently, with the result that similar techniques reinvented many times in different contexts, and radically new approaches that require an algorithmic level of abstraction take a long time to make it into applications. &lt;br/&gt;The intellectual core of this proposal is to create a coordinated effort in &quot;Algorithms from Theory to Practice&quot; that connects the basic development of fundamental algorithms and data structures to their many disparate uses. This work will address critical needs by connecting relevant algorithms to application areas, by exposing and tackling important issues that are common to multiple applications, and by developing fundamentally new approaches to solving key problems via the connections made. &lt;br/&gt; This proposal aims to provide impact at a number of different levels. At the lowest level are specific research projects that target key application domains. These include algorithms for mesh generation with applications to scientific simulations and graphics, algorithms for indexing and searching needed for a number of data analysis tasks, and protocols that connect machine learning with cryptography to produce a fundamentally new way for people to securely authenticate to their computers. At a higher level, this proposal will create a center to which researchers in application areas can come to build connections and integrate algorithmic techniques and principles into their own projects. At the highest level, this proposal will create tools to improve the process of moving algorithms from theory to applications more broadly. As one example, the course &quot;Algorithms in the Real World&quot; run by PI Blelloch has already developed a set of web pages detailing how algorithms are used in various applications and what turn out to be the crucial issues involved. A new, extensible version of this database would provide support for theoreticians, practitioners, and educators. We hope the end result to be both a faster pipeline from algorithm design to application, and improved sharing of algorithm techniques across application areas. In addition, we expect the students supported by this effort to fulfill the highest-level goals of this project becoming the next generation vertically-integrated algorithm researchers.&lt;br/&gt; The PIs each have a strong track-record in algorithms, both theoretical and applied. Guy Blelloch is developer of the NESL parallel programming language, as well as fast parallel algorithms for a number of core problems. Arvin Blum is known for his work in machine learning and approximation algorithms, and is developer of the Graphphan planning algorithm, used as the basis of many AI planning systems. Manuel Blum is winner of the ACM Turning Award for his work in the foundation of computational complexity theory and its applications to cryptography and program checking. John lafferty is known for his work in language modeling and information retrieval, and is co-developer (along with PI Sleator)of the Link Grammar natural-language parser. Daniel Sleator is winner of this year&apos;s ACM Kanellakis &quot;Theory and Practice&quot; award for the development of the Splay Tree data structure, and more recently been developing algorithms for natural language applications.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85982</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85982</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1122" target="n1124">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Algorithms: From Theory to Application</data>
      <data key="e_abstract">With the explosion in connectivity of computers and in the size of data sets available for analysis, mathematically sophisticated algorithms are becoming increasingly crucial for a wide range of real-world applications. Unfortunately, it often takes many years for an algorithm to make it from theory into applications. In fact, the trend has been for different areas to develop their own algorithms independently, with the result that similar techniques reinvented many times in different contexts, and radically new approaches that require an algorithmic level of abstraction take a long time to make it into applications. &lt;br/&gt;The intellectual core of this proposal is to create a coordinated effort in &quot;Algorithms from Theory to Practice&quot; that connects the basic development of fundamental algorithms and data structures to their many disparate uses. This work will address critical needs by connecting relevant algorithms to application areas, by exposing and tackling important issues that are common to multiple applications, and by developing fundamentally new approaches to solving key problems via the connections made. &lt;br/&gt; This proposal aims to provide impact at a number of different levels. At the lowest level are specific research projects that target key application domains. These include algorithms for mesh generation with applications to scientific simulations and graphics, algorithms for indexing and searching needed for a number of data analysis tasks, and protocols that connect machine learning with cryptography to produce a fundamentally new way for people to securely authenticate to their computers. At a higher level, this proposal will create a center to which researchers in application areas can come to build connections and integrate algorithmic techniques and principles into their own projects. At the highest level, this proposal will create tools to improve the process of moving algorithms from theory to applications more broadly. As one example, the course &quot;Algorithms in the Real World&quot; run by PI Blelloch has already developed a set of web pages detailing how algorithms are used in various applications and what turn out to be the crucial issues involved. A new, extensible version of this database would provide support for theoreticians, practitioners, and educators. We hope the end result to be both a faster pipeline from algorithm design to application, and improved sharing of algorithm techniques across application areas. In addition, we expect the students supported by this effort to fulfill the highest-level goals of this project becoming the next generation vertically-integrated algorithm researchers.&lt;br/&gt; The PIs each have a strong track-record in algorithms, both theoretical and applied. Guy Blelloch is developer of the NESL parallel programming language, as well as fast parallel algorithms for a number of core problems. Arvin Blum is known for his work in machine learning and approximation algorithms, and is developer of the Graphphan planning algorithm, used as the basis of many AI planning systems. Manuel Blum is winner of the ACM Turning Award for his work in the foundation of computational complexity theory and its applications to cryptography and program checking. John lafferty is known for his work in language modeling and information retrieval, and is co-developer (along with PI Sleator)of the Link Grammar natural-language parser. Daniel Sleator is winner of this year&apos;s ACM Kanellakis &quot;Theory and Practice&quot; award for the development of the Splay Tree data structure, and more recently been developing algorithms for natural language applications.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85982</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85982</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1122" target="n1125">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Algorithms: From Theory to Application</data>
      <data key="e_abstract">With the explosion in connectivity of computers and in the size of data sets available for analysis, mathematically sophisticated algorithms are becoming increasingly crucial for a wide range of real-world applications. Unfortunately, it often takes many years for an algorithm to make it from theory into applications. In fact, the trend has been for different areas to develop their own algorithms independently, with the result that similar techniques reinvented many times in different contexts, and radically new approaches that require an algorithmic level of abstraction take a long time to make it into applications. &lt;br/&gt;The intellectual core of this proposal is to create a coordinated effort in &quot;Algorithms from Theory to Practice&quot; that connects the basic development of fundamental algorithms and data structures to their many disparate uses. This work will address critical needs by connecting relevant algorithms to application areas, by exposing and tackling important issues that are common to multiple applications, and by developing fundamentally new approaches to solving key problems via the connections made. &lt;br/&gt; This proposal aims to provide impact at a number of different levels. At the lowest level are specific research projects that target key application domains. These include algorithms for mesh generation with applications to scientific simulations and graphics, algorithms for indexing and searching needed for a number of data analysis tasks, and protocols that connect machine learning with cryptography to produce a fundamentally new way for people to securely authenticate to their computers. At a higher level, this proposal will create a center to which researchers in application areas can come to build connections and integrate algorithmic techniques and principles into their own projects. At the highest level, this proposal will create tools to improve the process of moving algorithms from theory to applications more broadly. As one example, the course &quot;Algorithms in the Real World&quot; run by PI Blelloch has already developed a set of web pages detailing how algorithms are used in various applications and what turn out to be the crucial issues involved. A new, extensible version of this database would provide support for theoreticians, practitioners, and educators. We hope the end result to be both a faster pipeline from algorithm design to application, and improved sharing of algorithm techniques across application areas. In addition, we expect the students supported by this effort to fulfill the highest-level goals of this project becoming the next generation vertically-integrated algorithm researchers.&lt;br/&gt; The PIs each have a strong track-record in algorithms, both theoretical and applied. Guy Blelloch is developer of the NESL parallel programming language, as well as fast parallel algorithms for a number of core problems. Arvin Blum is known for his work in machine learning and approximation algorithms, and is developer of the Graphphan planning algorithm, used as the basis of many AI planning systems. Manuel Blum is winner of the ACM Turning Award for his work in the foundation of computational complexity theory and its applications to cryptography and program checking. John lafferty is known for his work in language modeling and information retrieval, and is co-developer (along with PI Sleator)of the Link Grammar natural-language parser. Daniel Sleator is winner of this year&apos;s ACM Kanellakis &quot;Theory and Practice&quot; award for the development of the Splay Tree data structure, and more recently been developing algorithms for natural language applications.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85982</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85982</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1122" target="n1126">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Algorithms: From Theory to Application</data>
      <data key="e_abstract">With the explosion in connectivity of computers and in the size of data sets available for analysis, mathematically sophisticated algorithms are becoming increasingly crucial for a wide range of real-world applications. Unfortunately, it often takes many years for an algorithm to make it from theory into applications. In fact, the trend has been for different areas to develop their own algorithms independently, with the result that similar techniques reinvented many times in different contexts, and radically new approaches that require an algorithmic level of abstraction take a long time to make it into applications. &lt;br/&gt;The intellectual core of this proposal is to create a coordinated effort in &quot;Algorithms from Theory to Practice&quot; that connects the basic development of fundamental algorithms and data structures to their many disparate uses. This work will address critical needs by connecting relevant algorithms to application areas, by exposing and tackling important issues that are common to multiple applications, and by developing fundamentally new approaches to solving key problems via the connections made. &lt;br/&gt; This proposal aims to provide impact at a number of different levels. At the lowest level are specific research projects that target key application domains. These include algorithms for mesh generation with applications to scientific simulations and graphics, algorithms for indexing and searching needed for a number of data analysis tasks, and protocols that connect machine learning with cryptography to produce a fundamentally new way for people to securely authenticate to their computers. At a higher level, this proposal will create a center to which researchers in application areas can come to build connections and integrate algorithmic techniques and principles into their own projects. At the highest level, this proposal will create tools to improve the process of moving algorithms from theory to applications more broadly. As one example, the course &quot;Algorithms in the Real World&quot; run by PI Blelloch has already developed a set of web pages detailing how algorithms are used in various applications and what turn out to be the crucial issues involved. A new, extensible version of this database would provide support for theoreticians, practitioners, and educators. We hope the end result to be both a faster pipeline from algorithm design to application, and improved sharing of algorithm techniques across application areas. In addition, we expect the students supported by this effort to fulfill the highest-level goals of this project becoming the next generation vertically-integrated algorithm researchers.&lt;br/&gt; The PIs each have a strong track-record in algorithms, both theoretical and applied. Guy Blelloch is developer of the NESL parallel programming language, as well as fast parallel algorithms for a number of core problems. Arvin Blum is known for his work in machine learning and approximation algorithms, and is developer of the Graphphan planning algorithm, used as the basis of many AI planning systems. Manuel Blum is winner of the ACM Turning Award for his work in the foundation of computational complexity theory and its applications to cryptography and program checking. John lafferty is known for his work in language modeling and information retrieval, and is co-developer (along with PI Sleator)of the Link Grammar natural-language parser. Daniel Sleator is winner of this year&apos;s ACM Kanellakis &quot;Theory and Practice&quot; award for the development of the Splay Tree data structure, and more recently been developing algorithms for natural language applications.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85982</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85982</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1123" target="n1124">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Algorithms: From Theory to Application</data>
      <data key="e_abstract">With the explosion in connectivity of computers and in the size of data sets available for analysis, mathematically sophisticated algorithms are becoming increasingly crucial for a wide range of real-world applications. Unfortunately, it often takes many years for an algorithm to make it from theory into applications. In fact, the trend has been for different areas to develop their own algorithms independently, with the result that similar techniques reinvented many times in different contexts, and radically new approaches that require an algorithmic level of abstraction take a long time to make it into applications. &lt;br/&gt;The intellectual core of this proposal is to create a coordinated effort in &quot;Algorithms from Theory to Practice&quot; that connects the basic development of fundamental algorithms and data structures to their many disparate uses. This work will address critical needs by connecting relevant algorithms to application areas, by exposing and tackling important issues that are common to multiple applications, and by developing fundamentally new approaches to solving key problems via the connections made. &lt;br/&gt; This proposal aims to provide impact at a number of different levels. At the lowest level are specific research projects that target key application domains. These include algorithms for mesh generation with applications to scientific simulations and graphics, algorithms for indexing and searching needed for a number of data analysis tasks, and protocols that connect machine learning with cryptography to produce a fundamentally new way for people to securely authenticate to their computers. At a higher level, this proposal will create a center to which researchers in application areas can come to build connections and integrate algorithmic techniques and principles into their own projects. At the highest level, this proposal will create tools to improve the process of moving algorithms from theory to applications more broadly. As one example, the course &quot;Algorithms in the Real World&quot; run by PI Blelloch has already developed a set of web pages detailing how algorithms are used in various applications and what turn out to be the crucial issues involved. A new, extensible version of this database would provide support for theoreticians, practitioners, and educators. We hope the end result to be both a faster pipeline from algorithm design to application, and improved sharing of algorithm techniques across application areas. In addition, we expect the students supported by this effort to fulfill the highest-level goals of this project becoming the next generation vertically-integrated algorithm researchers.&lt;br/&gt; The PIs each have a strong track-record in algorithms, both theoretical and applied. Guy Blelloch is developer of the NESL parallel programming language, as well as fast parallel algorithms for a number of core problems. Arvin Blum is known for his work in machine learning and approximation algorithms, and is developer of the Graphphan planning algorithm, used as the basis of many AI planning systems. Manuel Blum is winner of the ACM Turning Award for his work in the foundation of computational complexity theory and its applications to cryptography and program checking. John lafferty is known for his work in language modeling and information retrieval, and is co-developer (along with PI Sleator)of the Link Grammar natural-language parser. Daniel Sleator is winner of this year&apos;s ACM Kanellakis &quot;Theory and Practice&quot; award for the development of the Splay Tree data structure, and more recently been developing algorithms for natural language applications.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85982</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85982</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1123" target="n1125">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Algorithms: From Theory to Application</data>
      <data key="e_abstract">With the explosion in connectivity of computers and in the size of data sets available for analysis, mathematically sophisticated algorithms are becoming increasingly crucial for a wide range of real-world applications. Unfortunately, it often takes many years for an algorithm to make it from theory into applications. In fact, the trend has been for different areas to develop their own algorithms independently, with the result that similar techniques reinvented many times in different contexts, and radically new approaches that require an algorithmic level of abstraction take a long time to make it into applications. &lt;br/&gt;The intellectual core of this proposal is to create a coordinated effort in &quot;Algorithms from Theory to Practice&quot; that connects the basic development of fundamental algorithms and data structures to their many disparate uses. This work will address critical needs by connecting relevant algorithms to application areas, by exposing and tackling important issues that are common to multiple applications, and by developing fundamentally new approaches to solving key problems via the connections made. &lt;br/&gt; This proposal aims to provide impact at a number of different levels. At the lowest level are specific research projects that target key application domains. These include algorithms for mesh generation with applications to scientific simulations and graphics, algorithms for indexing and searching needed for a number of data analysis tasks, and protocols that connect machine learning with cryptography to produce a fundamentally new way for people to securely authenticate to their computers. At a higher level, this proposal will create a center to which researchers in application areas can come to build connections and integrate algorithmic techniques and principles into their own projects. At the highest level, this proposal will create tools to improve the process of moving algorithms from theory to applications more broadly. As one example, the course &quot;Algorithms in the Real World&quot; run by PI Blelloch has already developed a set of web pages detailing how algorithms are used in various applications and what turn out to be the crucial issues involved. A new, extensible version of this database would provide support for theoreticians, practitioners, and educators. We hope the end result to be both a faster pipeline from algorithm design to application, and improved sharing of algorithm techniques across application areas. In addition, we expect the students supported by this effort to fulfill the highest-level goals of this project becoming the next generation vertically-integrated algorithm researchers.&lt;br/&gt; The PIs each have a strong track-record in algorithms, both theoretical and applied. Guy Blelloch is developer of the NESL parallel programming language, as well as fast parallel algorithms for a number of core problems. Arvin Blum is known for his work in machine learning and approximation algorithms, and is developer of the Graphphan planning algorithm, used as the basis of many AI planning systems. Manuel Blum is winner of the ACM Turning Award for his work in the foundation of computational complexity theory and its applications to cryptography and program checking. John lafferty is known for his work in language modeling and information retrieval, and is co-developer (along with PI Sleator)of the Link Grammar natural-language parser. Daniel Sleator is winner of this year&apos;s ACM Kanellakis &quot;Theory and Practice&quot; award for the development of the Splay Tree data structure, and more recently been developing algorithms for natural language applications.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85982</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85982</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1123" target="n1126">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Algorithms: From Theory to Application</data>
      <data key="e_abstract">With the explosion in connectivity of computers and in the size of data sets available for analysis, mathematically sophisticated algorithms are becoming increasingly crucial for a wide range of real-world applications. Unfortunately, it often takes many years for an algorithm to make it from theory into applications. In fact, the trend has been for different areas to develop their own algorithms independently, with the result that similar techniques reinvented many times in different contexts, and radically new approaches that require an algorithmic level of abstraction take a long time to make it into applications. &lt;br/&gt;The intellectual core of this proposal is to create a coordinated effort in &quot;Algorithms from Theory to Practice&quot; that connects the basic development of fundamental algorithms and data structures to their many disparate uses. This work will address critical needs by connecting relevant algorithms to application areas, by exposing and tackling important issues that are common to multiple applications, and by developing fundamentally new approaches to solving key problems via the connections made. &lt;br/&gt; This proposal aims to provide impact at a number of different levels. At the lowest level are specific research projects that target key application domains. These include algorithms for mesh generation with applications to scientific simulations and graphics, algorithms for indexing and searching needed for a number of data analysis tasks, and protocols that connect machine learning with cryptography to produce a fundamentally new way for people to securely authenticate to their computers. At a higher level, this proposal will create a center to which researchers in application areas can come to build connections and integrate algorithmic techniques and principles into their own projects. At the highest level, this proposal will create tools to improve the process of moving algorithms from theory to applications more broadly. As one example, the course &quot;Algorithms in the Real World&quot; run by PI Blelloch has already developed a set of web pages detailing how algorithms are used in various applications and what turn out to be the crucial issues involved. A new, extensible version of this database would provide support for theoreticians, practitioners, and educators. We hope the end result to be both a faster pipeline from algorithm design to application, and improved sharing of algorithm techniques across application areas. In addition, we expect the students supported by this effort to fulfill the highest-level goals of this project becoming the next generation vertically-integrated algorithm researchers.&lt;br/&gt; The PIs each have a strong track-record in algorithms, both theoretical and applied. Guy Blelloch is developer of the NESL parallel programming language, as well as fast parallel algorithms for a number of core problems. Arvin Blum is known for his work in machine learning and approximation algorithms, and is developer of the Graphphan planning algorithm, used as the basis of many AI planning systems. Manuel Blum is winner of the ACM Turning Award for his work in the foundation of computational complexity theory and its applications to cryptography and program checking. John lafferty is known for his work in language modeling and information retrieval, and is co-developer (along with PI Sleator)of the Link Grammar natural-language parser. Daniel Sleator is winner of this year&apos;s ACM Kanellakis &quot;Theory and Practice&quot; award for the development of the Splay Tree data structure, and more recently been developing algorithms for natural language applications.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85982</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85982</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1124" target="n1125">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Algorithms: From Theory to Application</data>
      <data key="e_abstract">With the explosion in connectivity of computers and in the size of data sets available for analysis, mathematically sophisticated algorithms are becoming increasingly crucial for a wide range of real-world applications. Unfortunately, it often takes many years for an algorithm to make it from theory into applications. In fact, the trend has been for different areas to develop their own algorithms independently, with the result that similar techniques reinvented many times in different contexts, and radically new approaches that require an algorithmic level of abstraction take a long time to make it into applications. &lt;br/&gt;The intellectual core of this proposal is to create a coordinated effort in &quot;Algorithms from Theory to Practice&quot; that connects the basic development of fundamental algorithms and data structures to their many disparate uses. This work will address critical needs by connecting relevant algorithms to application areas, by exposing and tackling important issues that are common to multiple applications, and by developing fundamentally new approaches to solving key problems via the connections made. &lt;br/&gt; This proposal aims to provide impact at a number of different levels. At the lowest level are specific research projects that target key application domains. These include algorithms for mesh generation with applications to scientific simulations and graphics, algorithms for indexing and searching needed for a number of data analysis tasks, and protocols that connect machine learning with cryptography to produce a fundamentally new way for people to securely authenticate to their computers. At a higher level, this proposal will create a center to which researchers in application areas can come to build connections and integrate algorithmic techniques and principles into their own projects. At the highest level, this proposal will create tools to improve the process of moving algorithms from theory to applications more broadly. As one example, the course &quot;Algorithms in the Real World&quot; run by PI Blelloch has already developed a set of web pages detailing how algorithms are used in various applications and what turn out to be the crucial issues involved. A new, extensible version of this database would provide support for theoreticians, practitioners, and educators. We hope the end result to be both a faster pipeline from algorithm design to application, and improved sharing of algorithm techniques across application areas. In addition, we expect the students supported by this effort to fulfill the highest-level goals of this project becoming the next generation vertically-integrated algorithm researchers.&lt;br/&gt; The PIs each have a strong track-record in algorithms, both theoretical and applied. Guy Blelloch is developer of the NESL parallel programming language, as well as fast parallel algorithms for a number of core problems. Arvin Blum is known for his work in machine learning and approximation algorithms, and is developer of the Graphphan planning algorithm, used as the basis of many AI planning systems. Manuel Blum is winner of the ACM Turning Award for his work in the foundation of computational complexity theory and its applications to cryptography and program checking. John lafferty is known for his work in language modeling and information retrieval, and is co-developer (along with PI Sleator)of the Link Grammar natural-language parser. Daniel Sleator is winner of this year&apos;s ACM Kanellakis &quot;Theory and Practice&quot; award for the development of the Splay Tree data structure, and more recently been developing algorithms for natural language applications.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85982</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85982</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1124" target="n1126">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Algorithms: From Theory to Application</data>
      <data key="e_abstract">With the explosion in connectivity of computers and in the size of data sets available for analysis, mathematically sophisticated algorithms are becoming increasingly crucial for a wide range of real-world applications. Unfortunately, it often takes many years for an algorithm to make it from theory into applications. In fact, the trend has been for different areas to develop their own algorithms independently, with the result that similar techniques reinvented many times in different contexts, and radically new approaches that require an algorithmic level of abstraction take a long time to make it into applications. &lt;br/&gt;The intellectual core of this proposal is to create a coordinated effort in &quot;Algorithms from Theory to Practice&quot; that connects the basic development of fundamental algorithms and data structures to their many disparate uses. This work will address critical needs by connecting relevant algorithms to application areas, by exposing and tackling important issues that are common to multiple applications, and by developing fundamentally new approaches to solving key problems via the connections made. &lt;br/&gt; This proposal aims to provide impact at a number of different levels. At the lowest level are specific research projects that target key application domains. These include algorithms for mesh generation with applications to scientific simulations and graphics, algorithms for indexing and searching needed for a number of data analysis tasks, and protocols that connect machine learning with cryptography to produce a fundamentally new way for people to securely authenticate to their computers. At a higher level, this proposal will create a center to which researchers in application areas can come to build connections and integrate algorithmic techniques and principles into their own projects. At the highest level, this proposal will create tools to improve the process of moving algorithms from theory to applications more broadly. As one example, the course &quot;Algorithms in the Real World&quot; run by PI Blelloch has already developed a set of web pages detailing how algorithms are used in various applications and what turn out to be the crucial issues involved. A new, extensible version of this database would provide support for theoreticians, practitioners, and educators. We hope the end result to be both a faster pipeline from algorithm design to application, and improved sharing of algorithm techniques across application areas. In addition, we expect the students supported by this effort to fulfill the highest-level goals of this project becoming the next generation vertically-integrated algorithm researchers.&lt;br/&gt; The PIs each have a strong track-record in algorithms, both theoretical and applied. Guy Blelloch is developer of the NESL parallel programming language, as well as fast parallel algorithms for a number of core problems. Arvin Blum is known for his work in machine learning and approximation algorithms, and is developer of the Graphphan planning algorithm, used as the basis of many AI planning systems. Manuel Blum is winner of the ACM Turning Award for his work in the foundation of computational complexity theory and its applications to cryptography and program checking. John lafferty is known for his work in language modeling and information retrieval, and is co-developer (along with PI Sleator)of the Link Grammar natural-language parser. Daniel Sleator is winner of this year&apos;s ACM Kanellakis &quot;Theory and Practice&quot; award for the development of the Splay Tree data structure, and more recently been developing algorithms for natural language applications.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85982</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85982</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1125" target="n1126">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Algorithms: From Theory to Application</data>
      <data key="e_abstract">With the explosion in connectivity of computers and in the size of data sets available for analysis, mathematically sophisticated algorithms are becoming increasingly crucial for a wide range of real-world applications. Unfortunately, it often takes many years for an algorithm to make it from theory into applications. In fact, the trend has been for different areas to develop their own algorithms independently, with the result that similar techniques reinvented many times in different contexts, and radically new approaches that require an algorithmic level of abstraction take a long time to make it into applications. &lt;br/&gt;The intellectual core of this proposal is to create a coordinated effort in &quot;Algorithms from Theory to Practice&quot; that connects the basic development of fundamental algorithms and data structures to their many disparate uses. This work will address critical needs by connecting relevant algorithms to application areas, by exposing and tackling important issues that are common to multiple applications, and by developing fundamentally new approaches to solving key problems via the connections made. &lt;br/&gt; This proposal aims to provide impact at a number of different levels. At the lowest level are specific research projects that target key application domains. These include algorithms for mesh generation with applications to scientific simulations and graphics, algorithms for indexing and searching needed for a number of data analysis tasks, and protocols that connect machine learning with cryptography to produce a fundamentally new way for people to securely authenticate to their computers. At a higher level, this proposal will create a center to which researchers in application areas can come to build connections and integrate algorithmic techniques and principles into their own projects. At the highest level, this proposal will create tools to improve the process of moving algorithms from theory to applications more broadly. As one example, the course &quot;Algorithms in the Real World&quot; run by PI Blelloch has already developed a set of web pages detailing how algorithms are used in various applications and what turn out to be the crucial issues involved. A new, extensible version of this database would provide support for theoreticians, practitioners, and educators. We hope the end result to be both a faster pipeline from algorithm design to application, and improved sharing of algorithm techniques across application areas. In addition, we expect the students supported by this effort to fulfill the highest-level goals of this project becoming the next generation vertically-integrated algorithm researchers.&lt;br/&gt; The PIs each have a strong track-record in algorithms, both theoretical and applied. Guy Blelloch is developer of the NESL parallel programming language, as well as fast parallel algorithms for a number of core problems. Arvin Blum is known for his work in machine learning and approximation algorithms, and is developer of the Graphphan planning algorithm, used as the basis of many AI planning systems. Manuel Blum is winner of the ACM Turning Award for his work in the foundation of computational complexity theory and its applications to cryptography and program checking. John lafferty is known for his work in language modeling and information retrieval, and is co-developer (along with PI Sleator)of the Link Grammar natural-language parser. Daniel Sleator is winner of this year&apos;s ACM Kanellakis &quot;Theory and Practice&quot; award for the development of the Splay Tree data structure, and more recently been developing algorithms for natural language applications.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85982</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85982</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1128" target="n1129">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Asynchronous digital signal processing for the software radio</data>
      <data key="e_abstract">The 1990&apos;s have witnessed explosive, if not revolutionary, growth in wireless telecommunications, fueled in large measure by the technology scaling of digital processing power. Third-generation (3G) wire-less systems promise even more with high-bit-rate data services, such as video and Internet access, with wide-band spread-spectrum modulation. Yet, despite the continued benefit of technology scaling below&lt;br/&gt;0.1am in the next decade, these mobile 3G systems will demand digital processing power, along with programmability and energy efficiency, which are not achievable with conventional digital design techniques. Reconfigurable and flexible software radios, which implement digital IF as well as baseband function, will be essential for 3G systems because of the need to support multiple modulation waveforms and multiple air interface standards. These systems can be expected to demand 1000 MIPS of digital IF-processing&lt;br/&gt;power and up to 2000 MIPS of baseband DSP power. To achieve power levels necessary for mobile sys-tem, energy efficiency of better than 0.15 mW/MIPS while deliever 3000 MIPS of processing power will be necessary, an order of magnitude better than what can be achieve today with conventional design practices. In this extensive three-year research effort, we will consider asynchronous design techniques for achieving&lt;br/&gt;energy-efficient, high-performance digital signal processing for 3G wireless applications. We will fully exploit the inherent high-performance and low-power properties of asynchronous design, to consume power only where needed and to &quot;adapt&quot; to the actual data inputs. Specifically, we will combine fine-grained asynchronous micropipelines with adaptive (or programmable) power-supply control and a dataflow-driven microarchitecture to provide a funcadmental leap in complex programmable digital &lt;br/&gt;signal processing power and energy efficiency, unachievable using existing synchronous techniques. We in-tend to develop an asynchronous programmable DSP that is capable of supporting a significant fraction of the IS-95 and W-CDMA standards in software, including the performance- and power-intensive I and Q modulation and demodulation at less than 400 mW of peak power dissipation.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86007</data>
      <data key="e_expirationDate">2005-05-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86007</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1130" target="n1131">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Web Host Access Tools</data>
      <data key="e_abstract">EIA-0079770&lt;br/&gt;Cassel, Lillian N.&lt;br/&gt;Villanova University&lt;br/&gt;&lt;br/&gt;MRI: Web Host Access Tools&lt;br/&gt;&lt;br/&gt;This is a cooperative effort among computing faculty at Villanova University and the College of New Jersey and the objective is the acquisition of web host access tools. The combined research activities address important questions in artificial intelligence, information gathering, human-computer interface and networking all in the context of a common problem. The problem that joins these topics is assisting a user retrieving and using information obtained from the World Wide Web. The actives involve an integration of research and education with an explicit goal to introduce students to a significant research project while advancing the state of the art in enhanced web resource access.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79770</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79770</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1130" target="n1132">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Web Host Access Tools</data>
      <data key="e_abstract">EIA-0079770&lt;br/&gt;Cassel, Lillian N.&lt;br/&gt;Villanova University&lt;br/&gt;&lt;br/&gt;MRI: Web Host Access Tools&lt;br/&gt;&lt;br/&gt;This is a cooperative effort among computing faculty at Villanova University and the College of New Jersey and the objective is the acquisition of web host access tools. The combined research activities address important questions in artificial intelligence, information gathering, human-computer interface and networking all in the context of a common problem. The problem that joins these topics is assisting a user retrieving and using information obtained from the World Wide Web. The actives involve an integration of research and education with an explicit goal to introduce students to a significant research project while advancing the state of the art in enhanced web resource access.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79770</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79770</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1130" target="n1133">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Web Host Access Tools</data>
      <data key="e_abstract">EIA-0079770&lt;br/&gt;Cassel, Lillian N.&lt;br/&gt;Villanova University&lt;br/&gt;&lt;br/&gt;MRI: Web Host Access Tools&lt;br/&gt;&lt;br/&gt;This is a cooperative effort among computing faculty at Villanova University and the College of New Jersey and the objective is the acquisition of web host access tools. The combined research activities address important questions in artificial intelligence, information gathering, human-computer interface and networking all in the context of a common problem. The problem that joins these topics is assisting a user retrieving and using information obtained from the World Wide Web. The actives involve an integration of research and education with an explicit goal to introduce students to a significant research project while advancing the state of the art in enhanced web resource access.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79770</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79770</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1130" target="n1134">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Web Host Access Tools</data>
      <data key="e_abstract">EIA-0079770&lt;br/&gt;Cassel, Lillian N.&lt;br/&gt;Villanova University&lt;br/&gt;&lt;br/&gt;MRI: Web Host Access Tools&lt;br/&gt;&lt;br/&gt;This is a cooperative effort among computing faculty at Villanova University and the College of New Jersey and the objective is the acquisition of web host access tools. The combined research activities address important questions in artificial intelligence, information gathering, human-computer interface and networking all in the context of a common problem. The problem that joins these topics is assisting a user retrieving and using information obtained from the World Wide Web. The actives involve an integration of research and education with an explicit goal to introduce students to a significant research project while advancing the state of the art in enhanced web resource access.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79770</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79770</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1131" target="n1132">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Web Host Access Tools</data>
      <data key="e_abstract">EIA-0079770&lt;br/&gt;Cassel, Lillian N.&lt;br/&gt;Villanova University&lt;br/&gt;&lt;br/&gt;MRI: Web Host Access Tools&lt;br/&gt;&lt;br/&gt;This is a cooperative effort among computing faculty at Villanova University and the College of New Jersey and the objective is the acquisition of web host access tools. The combined research activities address important questions in artificial intelligence, information gathering, human-computer interface and networking all in the context of a common problem. The problem that joins these topics is assisting a user retrieving and using information obtained from the World Wide Web. The actives involve an integration of research and education with an explicit goal to introduce students to a significant research project while advancing the state of the art in enhanced web resource access.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79770</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79770</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1131" target="n1133">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Web Host Access Tools</data>
      <data key="e_abstract">EIA-0079770&lt;br/&gt;Cassel, Lillian N.&lt;br/&gt;Villanova University&lt;br/&gt;&lt;br/&gt;MRI: Web Host Access Tools&lt;br/&gt;&lt;br/&gt;This is a cooperative effort among computing faculty at Villanova University and the College of New Jersey and the objective is the acquisition of web host access tools. The combined research activities address important questions in artificial intelligence, information gathering, human-computer interface and networking all in the context of a common problem. The problem that joins these topics is assisting a user retrieving and using information obtained from the World Wide Web. The actives involve an integration of research and education with an explicit goal to introduce students to a significant research project while advancing the state of the art in enhanced web resource access.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79770</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79770</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1131" target="n1134">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Web Host Access Tools</data>
      <data key="e_abstract">EIA-0079770&lt;br/&gt;Cassel, Lillian N.&lt;br/&gt;Villanova University&lt;br/&gt;&lt;br/&gt;MRI: Web Host Access Tools&lt;br/&gt;&lt;br/&gt;This is a cooperative effort among computing faculty at Villanova University and the College of New Jersey and the objective is the acquisition of web host access tools. The combined research activities address important questions in artificial intelligence, information gathering, human-computer interface and networking all in the context of a common problem. The problem that joins these topics is assisting a user retrieving and using information obtained from the World Wide Web. The actives involve an integration of research and education with an explicit goal to introduce students to a significant research project while advancing the state of the art in enhanced web resource access.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79770</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79770</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1132" target="n1133">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Web Host Access Tools</data>
      <data key="e_abstract">EIA-0079770&lt;br/&gt;Cassel, Lillian N.&lt;br/&gt;Villanova University&lt;br/&gt;&lt;br/&gt;MRI: Web Host Access Tools&lt;br/&gt;&lt;br/&gt;This is a cooperative effort among computing faculty at Villanova University and the College of New Jersey and the objective is the acquisition of web host access tools. The combined research activities address important questions in artificial intelligence, information gathering, human-computer interface and networking all in the context of a common problem. The problem that joins these topics is assisting a user retrieving and using information obtained from the World Wide Web. The actives involve an integration of research and education with an explicit goal to introduce students to a significant research project while advancing the state of the art in enhanced web resource access.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79770</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79770</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1132" target="n1134">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Web Host Access Tools</data>
      <data key="e_abstract">EIA-0079770&lt;br/&gt;Cassel, Lillian N.&lt;br/&gt;Villanova University&lt;br/&gt;&lt;br/&gt;MRI: Web Host Access Tools&lt;br/&gt;&lt;br/&gt;This is a cooperative effort among computing faculty at Villanova University and the College of New Jersey and the objective is the acquisition of web host access tools. The combined research activities address important questions in artificial intelligence, information gathering, human-computer interface and networking all in the context of a common problem. The problem that joins these topics is assisting a user retrieving and using information obtained from the World Wide Web. The actives involve an integration of research and education with an explicit goal to introduce students to a significant research project while advancing the state of the art in enhanced web resource access.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79770</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79770</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1133" target="n1134">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Web Host Access Tools</data>
      <data key="e_abstract">EIA-0079770&lt;br/&gt;Cassel, Lillian N.&lt;br/&gt;Villanova University&lt;br/&gt;&lt;br/&gt;MRI: Web Host Access Tools&lt;br/&gt;&lt;br/&gt;This is a cooperative effort among computing faculty at Villanova University and the College of New Jersey and the objective is the acquisition of web host access tools. The combined research activities address important questions in artificial intelligence, information gathering, human-computer interface and networking all in the context of a common problem. The problem that joins these topics is assisting a user retrieving and using information obtained from the World Wide Web. The actives involve an integration of research and education with an explicit goal to introduce students to a significant research project while advancing the state of the art in enhanced web resource access.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79770</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79770</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1135" target="n1136">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Integrated Design of High-Performance Clustered Computing Systems and Algorithms for Large Scale Finite Element Simulation</data>
      <data key="e_abstract">For large-scale simulation problems, experience shows that, despite the rapid evolution of microprocessor and parallel computing technology, only a small fraction of peak performance from high-performance computing systems is generally realized. This is so because there are typically critical mismatches between the architecture of high-performance computing systems and the fundamental structure of the target simulations. This is clearly a problem of critical importance to information technology. It represents both an opportunity and a challenge to the disciplines of system-level design, targeting special-purpose architectures for high-performance computing, and algorithm and data structure design for large-scale engineering and software applications. Accordingly, this project will investigate systematic methodologies for designing the systems and algorithms for high performance applications. On the system design side, the project will focus on specialized billion-transistor chip multiprocessor architectures and a hierarchically distributed organization of resources tailored to the needs of the application. On the algorithm and application side, it will focus on finite element simulation applications, including representative problems from semiconductor device design and coupled flow and heat transfer processes.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81791</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81791</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1137" target="n1138">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Stochastic Summation of High-Order Feynman Graph Expansions</data>
      <data key="e_abstract">The study of interacting fermions is fundamentally important to a wide range of physics research, including fields as diverse as electronic structure theory of solids, strongly correlated electron physics, quantum chemistry, and the theory of nuclear matter. Among other applications, the understanding of high-temperature superconductors depends on these interactions. This project will develop a new computational method for the controlled approximate solution of interacting fermion models. The method combines Monte Carlo (MC) summation techniques with self-consistent high-order Feynman diagram expansions. The implementation of the MC diagram summation method poses major algorithmic and computational challenges in several distinct areas of computational science and, by its very nature, requires a multi-disciplinary approach.&lt;br/&gt;&lt;br/&gt;Technically, the project will develop novel computational graph theory algorithms and employ them to achieve a computationally efficient representation, generation and classification of Feynman graph topologies. New MC updating, scoring and variance minimization approaches will be implemented to carry out the simultaneous stochastic summation over diagram topologies and over internal momentum-energy variables. For the two-particle calculation, a novel combination of Lanczos matrix inversion and MC techniques is used to achieve efficient solutions of the Bethe-Salpeter equations with the full high-order irreducible interaction vertex. The efficient parallel implementation of the MC code is achieved by software pipelining and ring message passing approaches. These parallel applications are supported by novel parallel run-time systems that provide dynamic performance optimization.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81789</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81789</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n96" target="n1137">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Stochastic Summation of High-Order Feynman Graph Expansions</data>
      <data key="e_abstract">The study of interacting fermions is fundamentally important to a wide range of physics research, including fields as diverse as electronic structure theory of solids, strongly correlated electron physics, quantum chemistry, and the theory of nuclear matter. Among other applications, the understanding of high-temperature superconductors depends on these interactions. This project will develop a new computational method for the controlled approximate solution of interacting fermion models. The method combines Monte Carlo (MC) summation techniques with self-consistent high-order Feynman diagram expansions. The implementation of the MC diagram summation method poses major algorithmic and computational challenges in several distinct areas of computational science and, by its very nature, requires a multi-disciplinary approach.&lt;br/&gt;&lt;br/&gt;Technically, the project will develop novel computational graph theory algorithms and employ them to achieve a computationally efficient representation, generation and classification of Feynman graph topologies. New MC updating, scoring and variance minimization approaches will be implemented to carry out the simultaneous stochastic summation over diagram topologies and over internal momentum-energy variables. For the two-particle calculation, a novel combination of Lanczos matrix inversion and MC techniques is used to achieve efficient solutions of the Bethe-Salpeter equations with the full high-order irreducible interaction vertex. The efficient parallel implementation of the MC code is achieved by software pipelining and ring message passing approaches. These parallel applications are supported by novel parallel run-time systems that provide dynamic performance optimization.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81789</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81789</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1137" target="n1140">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Stochastic Summation of High-Order Feynman Graph Expansions</data>
      <data key="e_abstract">The study of interacting fermions is fundamentally important to a wide range of physics research, including fields as diverse as electronic structure theory of solids, strongly correlated electron physics, quantum chemistry, and the theory of nuclear matter. Among other applications, the understanding of high-temperature superconductors depends on these interactions. This project will develop a new computational method for the controlled approximate solution of interacting fermion models. The method combines Monte Carlo (MC) summation techniques with self-consistent high-order Feynman diagram expansions. The implementation of the MC diagram summation method poses major algorithmic and computational challenges in several distinct areas of computational science and, by its very nature, requires a multi-disciplinary approach.&lt;br/&gt;&lt;br/&gt;Technically, the project will develop novel computational graph theory algorithms and employ them to achieve a computationally efficient representation, generation and classification of Feynman graph topologies. New MC updating, scoring and variance minimization approaches will be implemented to carry out the simultaneous stochastic summation over diagram topologies and over internal momentum-energy variables. For the two-particle calculation, a novel combination of Lanczos matrix inversion and MC techniques is used to achieve efficient solutions of the Bethe-Salpeter equations with the full high-order irreducible interaction vertex. The efficient parallel implementation of the MC code is achieved by software pipelining and ring message passing approaches. These parallel applications are supported by novel parallel run-time systems that provide dynamic performance optimization.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81789</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81789</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n96" target="n1138">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Stochastic Summation of High-Order Feynman Graph Expansions</data>
      <data key="e_abstract">The study of interacting fermions is fundamentally important to a wide range of physics research, including fields as diverse as electronic structure theory of solids, strongly correlated electron physics, quantum chemistry, and the theory of nuclear matter. Among other applications, the understanding of high-temperature superconductors depends on these interactions. This project will develop a new computational method for the controlled approximate solution of interacting fermion models. The method combines Monte Carlo (MC) summation techniques with self-consistent high-order Feynman diagram expansions. The implementation of the MC diagram summation method poses major algorithmic and computational challenges in several distinct areas of computational science and, by its very nature, requires a multi-disciplinary approach.&lt;br/&gt;&lt;br/&gt;Technically, the project will develop novel computational graph theory algorithms and employ them to achieve a computationally efficient representation, generation and classification of Feynman graph topologies. New MC updating, scoring and variance minimization approaches will be implemented to carry out the simultaneous stochastic summation over diagram topologies and over internal momentum-energy variables. For the two-particle calculation, a novel combination of Lanczos matrix inversion and MC techniques is used to achieve efficient solutions of the Bethe-Salpeter equations with the full high-order irreducible interaction vertex. The efficient parallel implementation of the MC code is achieved by software pipelining and ring message passing approaches. These parallel applications are supported by novel parallel run-time systems that provide dynamic performance optimization.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81789</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81789</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1138" target="n1140">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Stochastic Summation of High-Order Feynman Graph Expansions</data>
      <data key="e_abstract">The study of interacting fermions is fundamentally important to a wide range of physics research, including fields as diverse as electronic structure theory of solids, strongly correlated electron physics, quantum chemistry, and the theory of nuclear matter. Among other applications, the understanding of high-temperature superconductors depends on these interactions. This project will develop a new computational method for the controlled approximate solution of interacting fermion models. The method combines Monte Carlo (MC) summation techniques with self-consistent high-order Feynman diagram expansions. The implementation of the MC diagram summation method poses major algorithmic and computational challenges in several distinct areas of computational science and, by its very nature, requires a multi-disciplinary approach.&lt;br/&gt;&lt;br/&gt;Technically, the project will develop novel computational graph theory algorithms and employ them to achieve a computationally efficient representation, generation and classification of Feynman graph topologies. New MC updating, scoring and variance minimization approaches will be implemented to carry out the simultaneous stochastic summation over diagram topologies and over internal momentum-energy variables. For the two-particle calculation, a novel combination of Lanczos matrix inversion and MC techniques is used to achieve efficient solutions of the Bethe-Salpeter equations with the full high-order irreducible interaction vertex. The efficient parallel implementation of the MC code is achieved by software pipelining and ring message passing approaches. These parallel applications are supported by novel parallel run-time systems that provide dynamic performance optimization.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81789</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81789</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n96" target="n1140">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Stochastic Summation of High-Order Feynman Graph Expansions</data>
      <data key="e_abstract">The study of interacting fermions is fundamentally important to a wide range of physics research, including fields as diverse as electronic structure theory of solids, strongly correlated electron physics, quantum chemistry, and the theory of nuclear matter. Among other applications, the understanding of high-temperature superconductors depends on these interactions. This project will develop a new computational method for the controlled approximate solution of interacting fermion models. The method combines Monte Carlo (MC) summation techniques with self-consistent high-order Feynman diagram expansions. The implementation of the MC diagram summation method poses major algorithmic and computational challenges in several distinct areas of computational science and, by its very nature, requires a multi-disciplinary approach.&lt;br/&gt;&lt;br/&gt;Technically, the project will develop novel computational graph theory algorithms and employ them to achieve a computationally efficient representation, generation and classification of Feynman graph topologies. New MC updating, scoring and variance minimization approaches will be implemented to carry out the simultaneous stochastic summation over diagram topologies and over internal momentum-energy variables. For the two-particle calculation, a novel combination of Lanczos matrix inversion and MC techniques is used to achieve efficient solutions of the Bethe-Salpeter equations with the full high-order irreducible interaction vertex. The efficient parallel implementation of the MC code is achieved by software pipelining and ring message passing approaches. These parallel applications are supported by novel parallel run-time systems that provide dynamic performance optimization.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81789</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81789</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1142" target="n1143">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Interactive Exploration of Complex Datasets Via the Effective Generation of Text and Graphics</data>
      <data key="e_abstract">This is accomplished through an interlocking collection of textual and graphical discourse engines, a plan recognition system, and an interaction manager that harnesses the power of these tools. Users begin by asking questions about the data stored in their dataset. The system answers using a combination of text and graphics. Text responses are built by a discourse engine, while graphical images are constructed using a perceptual visualization assistant. The collection of all possible responses is evaluated to select the most effective answer, be it text, graphics, or a combination of the two. A plan recognition system is used to analyze the users&apos; queries and their reactions to the responses they receive. This allows the system to anticipate future queries, cache relevant statistics, and guide the discourse and visualization systems during the evaluation and selection of appropriate answers to each user query. Results from this project will include: (1) plan recognition and interaction plan construction performed by the system to identify and model current and future analyses conducted by the users; (2) presentations that are sensitive to both the current and anticipated future state of users&apos; investigations; (3) assisted navigation techniques; (4) methods for evaluating the effectiveness of the use of text and/or graphics; and (5) perceptual visualization techniques. Results will be disseminated through journal and conference publications, online datasets with results and analysis, and online software demos of relevant research components. Although this project will study applications from the oceanography and public school domains, these results are relevant in any situation where interactive exploration of large, complex datasets is required.&lt;br/&gt;http://www.csc.ncsu.edu/faculty/healey/NSF-IDM-00</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">9.98851e+06</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98851e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1142" target="n1144">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Interactive Exploration of Complex Datasets Via the Effective Generation of Text and Graphics</data>
      <data key="e_abstract">This is accomplished through an interlocking collection of textual and graphical discourse engines, a plan recognition system, and an interaction manager that harnesses the power of these tools. Users begin by asking questions about the data stored in their dataset. The system answers using a combination of text and graphics. Text responses are built by a discourse engine, while graphical images are constructed using a perceptual visualization assistant. The collection of all possible responses is evaluated to select the most effective answer, be it text, graphics, or a combination of the two. A plan recognition system is used to analyze the users&apos; queries and their reactions to the responses they receive. This allows the system to anticipate future queries, cache relevant statistics, and guide the discourse and visualization systems during the evaluation and selection of appropriate answers to each user query. Results from this project will include: (1) plan recognition and interaction plan construction performed by the system to identify and model current and future analyses conducted by the users; (2) presentations that are sensitive to both the current and anticipated future state of users&apos; investigations; (3) assisted navigation techniques; (4) methods for evaluating the effectiveness of the use of text and/or graphics; and (5) perceptual visualization techniques. Results will be disseminated through journal and conference publications, online datasets with results and analysis, and online software demos of relevant research components. Although this project will study applications from the oceanography and public school domains, these results are relevant in any situation where interactive exploration of large, complex datasets is required.&lt;br/&gt;http://www.csc.ncsu.edu/faculty/healey/NSF-IDM-00</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">9.98851e+06</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98851e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1143" target="n1144">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Interactive Exploration of Complex Datasets Via the Effective Generation of Text and Graphics</data>
      <data key="e_abstract">This is accomplished through an interlocking collection of textual and graphical discourse engines, a plan recognition system, and an interaction manager that harnesses the power of these tools. Users begin by asking questions about the data stored in their dataset. The system answers using a combination of text and graphics. Text responses are built by a discourse engine, while graphical images are constructed using a perceptual visualization assistant. The collection of all possible responses is evaluated to select the most effective answer, be it text, graphics, or a combination of the two. A plan recognition system is used to analyze the users&apos; queries and their reactions to the responses they receive. This allows the system to anticipate future queries, cache relevant statistics, and guide the discourse and visualization systems during the evaluation and selection of appropriate answers to each user query. Results from this project will include: (1) plan recognition and interaction plan construction performed by the system to identify and model current and future analyses conducted by the users; (2) presentations that are sensitive to both the current and anticipated future state of users&apos; investigations; (3) assisted navigation techniques; (4) methods for evaluating the effectiveness of the use of text and/or graphics; and (5) perceptual visualization techniques. Results will be disseminated through journal and conference publications, online datasets with results and analysis, and online software demos of relevant research components. Although this project will study applications from the oceanography and public school domains, these results are relevant in any situation where interactive exploration of large, complex datasets is required.&lt;br/&gt;http://www.csc.ncsu.edu/faculty/healey/NSF-IDM-00</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">9.98851e+06</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98851e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1145" target="n1146">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Discreet Proofs for Electronic Commerce Applications</data>
      <data key="e_abstract">The goal of this project is to develop new tools based on the notion of &quot;discreet proof&quot; for ensuring security of electronic-commerce transactions. This type of proof is &quot;discreet&quot; in the sense that it reveals no more than is strictly necessary for the purposes of a given transaction. Discreet proofs are useful in providing authenticity, confidentiality, anonymity, accountability, and other properties often needed to ensure security of a transaction. For example, discreet proofs can be used to decouple the information in a medical database&lt;br/&gt;regarding a patient&apos;s medical condition and treatment from the patient&apos;s identity, thereby protecting the patient&apos;s privacy while simultaneously providing accountability in the dispensing of drugs and facilitating the collection of aggregate data for socially desirable goals. Constructing discreet proofs for a given application can be technically challenging. Expected research results include i) software tools to automate the construction of discreet proofs; ii) specific proofs for frequently used cryptographic primitives in&lt;br/&gt;E-commerce; iii) practical protocols based on discreet proofs for E-commerce applications. Through its educational component, the project is expected to increase the pool of adequately trained personnel in the growing area of E-commerce.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81823</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">81823</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1155" target="n1156">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Formal Methods Visualization</data>
      <data key="e_abstract">CCR-9988357&lt;br/&gt;J Strother Moore &amp; Chandrajit Bajaj&lt;br/&gt;U Texas Austin&lt;br/&gt;&lt;br/&gt;The proposed work will enable real-time interrogative visualization and steering of&lt;br/&gt;the data-intensive computations of applied symbolic logic, in support of hardware&lt;br/&gt;and software verification. &lt;br/&gt;&lt;br/&gt;The microprocessor industry is finding increasing use for mechanized formal methods:&lt;br/&gt;the mathematical specification and mechanized symbolic analysis of hardware and&lt;br/&gt;software systems. For example, the ACL2 theorem prover has been used by Advanced&lt;br/&gt;Micro Devices, Inc., to find bugs in the AMD-K7 TM floating-point hardware, bugs&lt;br/&gt;that had escaped over 80 million test vectors. Theorem provers also enable symbolic&lt;br/&gt;simulation of new designs from formal specifications and security analysis of&lt;br/&gt;devices for use in e-commerce (see below). Such applications represent the practical&lt;br/&gt;emergence of applied symbolic logic.&lt;br/&gt;&lt;br/&gt;Formal logic is the mathematical tool of choice for modeling computing&lt;br/&gt;systems, in exactly the same sense that differential equations are the tool of&lt;br/&gt;choice for modeling physical systems. While study of logic is well established, the&lt;br/&gt;application of logic is new, because until the invention of the digital computer,&lt;br/&gt;engineered artifacts were essentially mechanical. The semiconductor industry is now&lt;br/&gt;turning to logic-based mechanized tools because models of modern processors are too&lt;br/&gt;complex to analyze any other way; and that complexity cannot be eliminated because&lt;br/&gt;it is used to buy speed and speed provides the competitive edge. &lt;br/&gt;&lt;br/&gt;The semi-automatic theorem provers that make such analysis possible are industrial-&lt;br/&gt;strength symbol manipulation engines. These engines process symbolic data&lt;br/&gt;representing logical formulas. Intermediate formulas may consume megabytes of&lt;br/&gt;storage. The engines explore vast search spaces determined by thousands of&lt;br/&gt;definitions and theorems in data bases created by various design team members.&lt;br/&gt;Proofs may contain millions of primitive inference steps. The engines use decision&lt;br/&gt;procedures where feasible and are otherwise guided both by heuristics and the human&lt;br/&gt;user.&lt;br/&gt;&lt;br/&gt;Due to the computational intensity of the problem, today&apos;s semi-automatic deduction&lt;br/&gt;engines are designed for use in an iterated guess style, where the guess&lt;br/&gt;determines the search space. When a proof attempt fails, the rea-son is often&lt;br/&gt;mathematical rather than search-strategic. The user&apos;s role is creative: diagnose the&lt;br/&gt;problem and invent the mathematical abstractions necessary to facilitate proof. This&lt;br/&gt;may require defining new formal concepts to state generalizations or decompositions.&lt;br/&gt;Such input from the user fundamentally alters the search space.&lt;br/&gt;&lt;br/&gt;The research proposed here will produce a new paradigm for semi-automatic theorem&lt;br/&gt;provers in which the user steers the system in real time. Visualization paradigms&lt;br/&gt;for symbolic data and symbolic manipulation will be developed, including what we&lt;br/&gt;call symbolic spreadsheets which allow inter-related symbolic data to be displayed&lt;br/&gt;coherently. The search space will be explicit, visible and dynamically computed.&lt;br/&gt;Structure will be imposed via the definitions and lemmas in the data base. Four&lt;br/&gt;fundamental research topics will be addressed: (i) selection of the abstract&lt;br/&gt;entities to be visualized, the visual reification of these abstractions, and the&lt;br/&gt;requirements for effective steering; (ii) visualization algorithms for efficient&lt;br/&gt;display and interrogation of symbolic logical data and processes; (iii) theorem&lt;br/&gt;proving algorithms producing human surveyable proofs while allowing real-time&lt;br/&gt;interaction; and (iv) a client/server model of semi-automatic theorem proving&lt;br/&gt;allowing the visualization and deduction engines to be decoupled but interoperable.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This is an ideal setting in which to study the visualization of symbolic data&lt;br/&gt;processing, with a clear path to immediate and critical applications in the&lt;br/&gt;semiconductor industry and, due to the pervasiveness of formal systems in computing&lt;br/&gt;(e.g., programming languages) promising connections to much wider applications. The&lt;br/&gt;research will deeply affect both applied logic and visualization.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">9.98836e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98836e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1157" target="n1158">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Parallel Algorithms and Software for Steepest Descent Fast Multipole Method</data>
      <data key="e_abstract">Parallel Algorithms and Software for Steepest Descent Fast Multipole Method&lt;br/&gt;&lt;br/&gt;Srinivas Aluru &amp; Shanker Balasubramaniam &lt;br/&gt;Iowa State&lt;br/&gt;&lt;br/&gt;The goal of this project is to develop theoretically rigorous and practically efficient parallel algorithms for the steepest descent fast multipole method (SDFMM) and to build a parallel software library for the solution of a wide variety of complex problems related to electromagnetic scattering and radiation by quasi-planar surfaces. Several important applications that can be studied with the use of this software include soil and ocean modeling, remote sensing, long range communications, applications in radio astronomy, full-wave analysis of electrical circuits and the design and analysis of antennas, diffractive optical gratings, solar cells and quantum well infrared detectors. Classical iterative integral techniques for solving such applications require quadratic work per iteration. SDFMM drastically reduces this computational complexity, to linear time, in the case of uniform distributions, by exploiting the redundancy in the classical representation of fields and the geometric structure of quasi-planar surfaces. Efficient sequential and parallel algorithms will be developed for solving non-uniform problems and for solving problems involving multiple, mutually interacting quasi-planar regions. Particular emphasis will be placed on developing distribution-independent algorithms, i.e., provably efficient algorithms for which the run-time is independent of the distribution without making any assumptions on either the range of distributions or the limited precision of computer arithmetic. Research will also be carried out in developing numerical methods that have a demonstrable order of accuracy. The software library will be written using the Message Passing Interface (MPI). Validation of the software will be carried out via comparisons against experimental data as well as numerical results obtained from slower, established solvers. This is an interdisciplinary research project involving the knowledge of algorithm development, computational electromagnetics, numerical methods and parallel computing. The research will be carried out by a group of three PI&apos;s (two from Iowa State University and one from the University of Illinois, Urbana Champaign), representing a mix of expertise and research interests that is vital to the success of this project. All the PIs have access to high-performance parallel computers and clusters, required for the software development and experimental validation of its practical efficiency.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">9.98835e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98835e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1128" target="n1163">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: A CAD Framework for the Design and Optimization of Large-Scale Asynchronous Digital Systems</data>
      <data key="e_abstract">Asynchronous (or clockless) circuits have become the focus of renewed interest because of their potential to alleviate a number of challenging problems in future-generation chip design: clock distribution, power management, and design reuse. To overcome the limitations of current asynchronous design methodologies, this project is developing an automated CAD framework for the synthesis and optimization of large-scale asynchronous systems. In addition to basic high-level scheduling, binding and allocation, and Hardware Description Language support (Verilog HDL), the project is exploring the open and challenging problems of: (a) high-performance pipeline synthesis and optimization; (b) architectural exploration (targeted to the frequent common-case operations); (c) distributed controller synthesis and optimization; (d) system-level performance and power analysis; and (e) the synthesis of mixed asynchronous/synchronous systems. The tool framework is being applied to a number of commercial examples and validated through chip design, fabrication, and test.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86036</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86036</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1164" target="n1165">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Institute for Quantum Information</data>
      <data key="e_abstract">EIA -0086038&lt;br/&gt;Kimble, H.J.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Institute for Quantum Information&lt;br/&gt;&lt;br/&gt;An interdisciplinary team of researchers in Physics, Applied Physics, Electrical Engineering and Computer Science are establishing an Institute for Quantum Information (IQI) to facilitate the investigation of quantum information science to provide new capabilities in the revolutionary field of quantum computing. To this end, efforts are being made to develop new algorithms for the manipulation, processing, and distribution of quantum information (including information capacities of communication channels, reliable schemes for distributed computation, efficient quantum error correcting codes). Investigations of physical systems for the implementation of quantum computation and communication, as well as coherent nanotechnology, principally by the way of theoretical models and analysis, are being performed. The team is also pursuing techniques to develop active control of quantum effects in nanoscale integrated circuits involving systematic approaches to the suppression of unwanted quantum effects via on-chip feedback networks and methods for stabilizing and exploiting emergent quantum behaviors in the context of analog/hybrid VLSI.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86038</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86038</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1164" target="n1166">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Institute for Quantum Information</data>
      <data key="e_abstract">EIA -0086038&lt;br/&gt;Kimble, H.J.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Institute for Quantum Information&lt;br/&gt;&lt;br/&gt;An interdisciplinary team of researchers in Physics, Applied Physics, Electrical Engineering and Computer Science are establishing an Institute for Quantum Information (IQI) to facilitate the investigation of quantum information science to provide new capabilities in the revolutionary field of quantum computing. To this end, efforts are being made to develop new algorithms for the manipulation, processing, and distribution of quantum information (including information capacities of communication channels, reliable schemes for distributed computation, efficient quantum error correcting codes). Investigations of physical systems for the implementation of quantum computation and communication, as well as coherent nanotechnology, principally by the way of theoretical models and analysis, are being performed. The team is also pursuing techniques to develop active control of quantum effects in nanoscale integrated circuits involving systematic approaches to the suppression of unwanted quantum effects via on-chip feedback networks and methods for stabilizing and exploiting emergent quantum behaviors in the context of analog/hybrid VLSI.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86038</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86038</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n295" target="n1164">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Institute for Quantum Information</data>
      <data key="e_abstract">EIA -0086038&lt;br/&gt;Kimble, H.J.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Institute for Quantum Information&lt;br/&gt;&lt;br/&gt;An interdisciplinary team of researchers in Physics, Applied Physics, Electrical Engineering and Computer Science are establishing an Institute for Quantum Information (IQI) to facilitate the investigation of quantum information science to provide new capabilities in the revolutionary field of quantum computing. To this end, efforts are being made to develop new algorithms for the manipulation, processing, and distribution of quantum information (including information capacities of communication channels, reliable schemes for distributed computation, efficient quantum error correcting codes). Investigations of physical systems for the implementation of quantum computation and communication, as well as coherent nanotechnology, principally by the way of theoretical models and analysis, are being performed. The team is also pursuing techniques to develop active control of quantum effects in nanoscale integrated circuits involving systematic approaches to the suppression of unwanted quantum effects via on-chip feedback networks and methods for stabilizing and exploiting emergent quantum behaviors in the context of analog/hybrid VLSI.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86038</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86038</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1164" target="n1168">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Institute for Quantum Information</data>
      <data key="e_abstract">EIA -0086038&lt;br/&gt;Kimble, H.J.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Institute for Quantum Information&lt;br/&gt;&lt;br/&gt;An interdisciplinary team of researchers in Physics, Applied Physics, Electrical Engineering and Computer Science are establishing an Institute for Quantum Information (IQI) to facilitate the investigation of quantum information science to provide new capabilities in the revolutionary field of quantum computing. To this end, efforts are being made to develop new algorithms for the manipulation, processing, and distribution of quantum information (including information capacities of communication channels, reliable schemes for distributed computation, efficient quantum error correcting codes). Investigations of physical systems for the implementation of quantum computation and communication, as well as coherent nanotechnology, principally by the way of theoretical models and analysis, are being performed. The team is also pursuing techniques to develop active control of quantum effects in nanoscale integrated circuits involving systematic approaches to the suppression of unwanted quantum effects via on-chip feedback networks and methods for stabilizing and exploiting emergent quantum behaviors in the context of analog/hybrid VLSI.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86038</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86038</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1164" target="n1169">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Institute for Quantum Information</data>
      <data key="e_abstract">EIA -0086038&lt;br/&gt;Kimble, H.J.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Institute for Quantum Information&lt;br/&gt;&lt;br/&gt;An interdisciplinary team of researchers in Physics, Applied Physics, Electrical Engineering and Computer Science are establishing an Institute for Quantum Information (IQI) to facilitate the investigation of quantum information science to provide new capabilities in the revolutionary field of quantum computing. To this end, efforts are being made to develop new algorithms for the manipulation, processing, and distribution of quantum information (including information capacities of communication channels, reliable schemes for distributed computation, efficient quantum error correcting codes). Investigations of physical systems for the implementation of quantum computation and communication, as well as coherent nanotechnology, principally by the way of theoretical models and analysis, are being performed. The team is also pursuing techniques to develop active control of quantum effects in nanoscale integrated circuits involving systematic approaches to the suppression of unwanted quantum effects via on-chip feedback networks and methods for stabilizing and exploiting emergent quantum behaviors in the context of analog/hybrid VLSI.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86038</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86038</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1165" target="n1166">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Institute for Quantum Information</data>
      <data key="e_abstract">EIA -0086038&lt;br/&gt;Kimble, H.J.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Institute for Quantum Information&lt;br/&gt;&lt;br/&gt;An interdisciplinary team of researchers in Physics, Applied Physics, Electrical Engineering and Computer Science are establishing an Institute for Quantum Information (IQI) to facilitate the investigation of quantum information science to provide new capabilities in the revolutionary field of quantum computing. To this end, efforts are being made to develop new algorithms for the manipulation, processing, and distribution of quantum information (including information capacities of communication channels, reliable schemes for distributed computation, efficient quantum error correcting codes). Investigations of physical systems for the implementation of quantum computation and communication, as well as coherent nanotechnology, principally by the way of theoretical models and analysis, are being performed. The team is also pursuing techniques to develop active control of quantum effects in nanoscale integrated circuits involving systematic approaches to the suppression of unwanted quantum effects via on-chip feedback networks and methods for stabilizing and exploiting emergent quantum behaviors in the context of analog/hybrid VLSI.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86038</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86038</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n295" target="n1165">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Institute for Quantum Information</data>
      <data key="e_abstract">EIA -0086038&lt;br/&gt;Kimble, H.J.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Institute for Quantum Information&lt;br/&gt;&lt;br/&gt;An interdisciplinary team of researchers in Physics, Applied Physics, Electrical Engineering and Computer Science are establishing an Institute for Quantum Information (IQI) to facilitate the investigation of quantum information science to provide new capabilities in the revolutionary field of quantum computing. To this end, efforts are being made to develop new algorithms for the manipulation, processing, and distribution of quantum information (including information capacities of communication channels, reliable schemes for distributed computation, efficient quantum error correcting codes). Investigations of physical systems for the implementation of quantum computation and communication, as well as coherent nanotechnology, principally by the way of theoretical models and analysis, are being performed. The team is also pursuing techniques to develop active control of quantum effects in nanoscale integrated circuits involving systematic approaches to the suppression of unwanted quantum effects via on-chip feedback networks and methods for stabilizing and exploiting emergent quantum behaviors in the context of analog/hybrid VLSI.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86038</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86038</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1165" target="n1168">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Institute for Quantum Information</data>
      <data key="e_abstract">EIA -0086038&lt;br/&gt;Kimble, H.J.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Institute for Quantum Information&lt;br/&gt;&lt;br/&gt;An interdisciplinary team of researchers in Physics, Applied Physics, Electrical Engineering and Computer Science are establishing an Institute for Quantum Information (IQI) to facilitate the investigation of quantum information science to provide new capabilities in the revolutionary field of quantum computing. To this end, efforts are being made to develop new algorithms for the manipulation, processing, and distribution of quantum information (including information capacities of communication channels, reliable schemes for distributed computation, efficient quantum error correcting codes). Investigations of physical systems for the implementation of quantum computation and communication, as well as coherent nanotechnology, principally by the way of theoretical models and analysis, are being performed. The team is also pursuing techniques to develop active control of quantum effects in nanoscale integrated circuits involving systematic approaches to the suppression of unwanted quantum effects via on-chip feedback networks and methods for stabilizing and exploiting emergent quantum behaviors in the context of analog/hybrid VLSI.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86038</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86038</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1165" target="n1169">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Institute for Quantum Information</data>
      <data key="e_abstract">EIA -0086038&lt;br/&gt;Kimble, H.J.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Institute for Quantum Information&lt;br/&gt;&lt;br/&gt;An interdisciplinary team of researchers in Physics, Applied Physics, Electrical Engineering and Computer Science are establishing an Institute for Quantum Information (IQI) to facilitate the investigation of quantum information science to provide new capabilities in the revolutionary field of quantum computing. To this end, efforts are being made to develop new algorithms for the manipulation, processing, and distribution of quantum information (including information capacities of communication channels, reliable schemes for distributed computation, efficient quantum error correcting codes). Investigations of physical systems for the implementation of quantum computation and communication, as well as coherent nanotechnology, principally by the way of theoretical models and analysis, are being performed. The team is also pursuing techniques to develop active control of quantum effects in nanoscale integrated circuits involving systematic approaches to the suppression of unwanted quantum effects via on-chip feedback networks and methods for stabilizing and exploiting emergent quantum behaviors in the context of analog/hybrid VLSI.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86038</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86038</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n295" target="n1166">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Institute for Quantum Information</data>
      <data key="e_abstract">EIA -0086038&lt;br/&gt;Kimble, H.J.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Institute for Quantum Information&lt;br/&gt;&lt;br/&gt;An interdisciplinary team of researchers in Physics, Applied Physics, Electrical Engineering and Computer Science are establishing an Institute for Quantum Information (IQI) to facilitate the investigation of quantum information science to provide new capabilities in the revolutionary field of quantum computing. To this end, efforts are being made to develop new algorithms for the manipulation, processing, and distribution of quantum information (including information capacities of communication channels, reliable schemes for distributed computation, efficient quantum error correcting codes). Investigations of physical systems for the implementation of quantum computation and communication, as well as coherent nanotechnology, principally by the way of theoretical models and analysis, are being performed. The team is also pursuing techniques to develop active control of quantum effects in nanoscale integrated circuits involving systematic approaches to the suppression of unwanted quantum effects via on-chip feedback networks and methods for stabilizing and exploiting emergent quantum behaviors in the context of analog/hybrid VLSI.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86038</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86038</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1166" target="n1168">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Institute for Quantum Information</data>
      <data key="e_abstract">EIA -0086038&lt;br/&gt;Kimble, H.J.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Institute for Quantum Information&lt;br/&gt;&lt;br/&gt;An interdisciplinary team of researchers in Physics, Applied Physics, Electrical Engineering and Computer Science are establishing an Institute for Quantum Information (IQI) to facilitate the investigation of quantum information science to provide new capabilities in the revolutionary field of quantum computing. To this end, efforts are being made to develop new algorithms for the manipulation, processing, and distribution of quantum information (including information capacities of communication channels, reliable schemes for distributed computation, efficient quantum error correcting codes). Investigations of physical systems for the implementation of quantum computation and communication, as well as coherent nanotechnology, principally by the way of theoretical models and analysis, are being performed. The team is also pursuing techniques to develop active control of quantum effects in nanoscale integrated circuits involving systematic approaches to the suppression of unwanted quantum effects via on-chip feedback networks and methods for stabilizing and exploiting emergent quantum behaviors in the context of analog/hybrid VLSI.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86038</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86038</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1166" target="n1169">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Institute for Quantum Information</data>
      <data key="e_abstract">EIA -0086038&lt;br/&gt;Kimble, H.J.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Institute for Quantum Information&lt;br/&gt;&lt;br/&gt;An interdisciplinary team of researchers in Physics, Applied Physics, Electrical Engineering and Computer Science are establishing an Institute for Quantum Information (IQI) to facilitate the investigation of quantum information science to provide new capabilities in the revolutionary field of quantum computing. To this end, efforts are being made to develop new algorithms for the manipulation, processing, and distribution of quantum information (including information capacities of communication channels, reliable schemes for distributed computation, efficient quantum error correcting codes). Investigations of physical systems for the implementation of quantum computation and communication, as well as coherent nanotechnology, principally by the way of theoretical models and analysis, are being performed. The team is also pursuing techniques to develop active control of quantum effects in nanoscale integrated circuits involving systematic approaches to the suppression of unwanted quantum effects via on-chip feedback networks and methods for stabilizing and exploiting emergent quantum behaviors in the context of analog/hybrid VLSI.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86038</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86038</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n295" target="n1168">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Institute for Quantum Information</data>
      <data key="e_abstract">EIA -0086038&lt;br/&gt;Kimble, H.J.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Institute for Quantum Information&lt;br/&gt;&lt;br/&gt;An interdisciplinary team of researchers in Physics, Applied Physics, Electrical Engineering and Computer Science are establishing an Institute for Quantum Information (IQI) to facilitate the investigation of quantum information science to provide new capabilities in the revolutionary field of quantum computing. To this end, efforts are being made to develop new algorithms for the manipulation, processing, and distribution of quantum information (including information capacities of communication channels, reliable schemes for distributed computation, efficient quantum error correcting codes). Investigations of physical systems for the implementation of quantum computation and communication, as well as coherent nanotechnology, principally by the way of theoretical models and analysis, are being performed. The team is also pursuing techniques to develop active control of quantum effects in nanoscale integrated circuits involving systematic approaches to the suppression of unwanted quantum effects via on-chip feedback networks and methods for stabilizing and exploiting emergent quantum behaviors in the context of analog/hybrid VLSI.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86038</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86038</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n295" target="n1169">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Institute for Quantum Information</data>
      <data key="e_abstract">EIA -0086038&lt;br/&gt;Kimble, H.J.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Institute for Quantum Information&lt;br/&gt;&lt;br/&gt;An interdisciplinary team of researchers in Physics, Applied Physics, Electrical Engineering and Computer Science are establishing an Institute for Quantum Information (IQI) to facilitate the investigation of quantum information science to provide new capabilities in the revolutionary field of quantum computing. To this end, efforts are being made to develop new algorithms for the manipulation, processing, and distribution of quantum information (including information capacities of communication channels, reliable schemes for distributed computation, efficient quantum error correcting codes). Investigations of physical systems for the implementation of quantum computation and communication, as well as coherent nanotechnology, principally by the way of theoretical models and analysis, are being performed. The team is also pursuing techniques to develop active control of quantum effects in nanoscale integrated circuits involving systematic approaches to the suppression of unwanted quantum effects via on-chip feedback networks and methods for stabilizing and exploiting emergent quantum behaviors in the context of analog/hybrid VLSI.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86038</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86038</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1168" target="n1169">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Institute for Quantum Information</data>
      <data key="e_abstract">EIA -0086038&lt;br/&gt;Kimble, H.J.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Institute for Quantum Information&lt;br/&gt;&lt;br/&gt;An interdisciplinary team of researchers in Physics, Applied Physics, Electrical Engineering and Computer Science are establishing an Institute for Quantum Information (IQI) to facilitate the investigation of quantum information science to provide new capabilities in the revolutionary field of quantum computing. To this end, efforts are being made to develop new algorithms for the manipulation, processing, and distribution of quantum information (including information capacities of communication channels, reliable schemes for distributed computation, efficient quantum error correcting codes). Investigations of physical systems for the implementation of quantum computation and communication, as well as coherent nanotechnology, principally by the way of theoretical models and analysis, are being performed. The team is also pursuing techniques to develop active control of quantum effects in nanoscale integrated circuits involving systematic approaches to the suppression of unwanted quantum effects via on-chip feedback networks and methods for stabilizing and exploiting emergent quantum behaviors in the context of analog/hybrid VLSI.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86038</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86038</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1170" target="n1171">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">SGER: Initial User Experiments Towards a Spoken Language Interface for Programming</data>
      <data key="e_abstract">The PIs are exploring issues relating to the development of viable systems that would allow programmers to create Java programs via spoken English input. They have to this end designed and implemented a proof-of-concept prototype called NaturalJava which supports user input of simple Java programs via written English sentences. Their ability to improve the prototype is limited at present both by software architecture shortcomings and by a lack of understanding of how real users would prefer to interact with a spoken programming interface. What kinds of written and spoken English sentences would they use? What type of navigation/editing model would be effective in such an environment? To answer questions such as these, the PIs plan to conduct a Wizard-of-Oz user study in which they will invite Java programmers to write programs under controlled conditions. Subjects will write programs using a system that behaves like an improved version of NaturalJava; half of them will use a written English interface, and the other half will use a spoken English interface. A hidden expert Java programmer will play the role of NaturalJava, reading (or listening to) the commands and creating the source code. The PIs expect to learn a great deal by recording and transcribing the sessions, interviewing the programmers, and examining the resulting Java programs. At the end of the year the PIs expect to have acquired a solid understanding of the issues, so that they will then be in a position to detail and pursue their long-term agenda which, if successful, would prove especially valuable to people with certain kinds of disabilities. The research team will include a graduate student who is blind, and for whom most of the NSF funds are allocated; the PIs are not charging for their time.</data>
      <data key="e_pgm">6846</data>
      <data key="e_label">90100</data>
      <data key="e_expirationDate">2003-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">90100</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n475" target="n1173">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Computational Geometry for Structural Biology and Bioinformatics</data>
      <data key="e_abstract">Life at its most detailed level depends on the geometric shape of molecules. Nevertheless, geometric methods are relatively uncommon in computational biology, primarily because of difficult and unsolved issues in applying geometric computing to biology. This project will address these causes by investigating geometric representations and developing novel geometric methods. It will incorporate these into software that helps structural biologists with their work and integrates with their current tools. The key research issues include: 1)representation and classification of geometric shape; 2)synthesis of geometric, physical, and statistical information; 3)computation and representation of motion; 4)organization of shapes for rapid searches; and 5) hierarchies for everything.&lt;br/&gt;&lt;br/&gt;This research is expected to shed light on some of the most important unsolved biological puzzles: prediction of protein structure, simulation of protein folding, and analysis of ligand to protein docking. These processes link form to function. Understanding them will pave the way to a post-genomic era in biological research, in which the wealth of DNA sequence information is complemented by corresponding knowledge of geometric shape. Together, sequence and shape will provide a description of the biological function so critical for all life.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86013</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86013</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1173" target="n1175">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Computational Geometry for Structural Biology and Bioinformatics</data>
      <data key="e_abstract">Life at its most detailed level depends on the geometric shape of molecules. Nevertheless, geometric methods are relatively uncommon in computational biology, primarily because of difficult and unsolved issues in applying geometric computing to biology. This project will address these causes by investigating geometric representations and developing novel geometric methods. It will incorporate these into software that helps structural biologists with their work and integrates with their current tools. The key research issues include: 1)representation and classification of geometric shape; 2)synthesis of geometric, physical, and statistical information; 3)computation and representation of motion; 4)organization of shapes for rapid searches; and 5) hierarchies for everything.&lt;br/&gt;&lt;br/&gt;This research is expected to shed light on some of the most important unsolved biological puzzles: prediction of protein structure, simulation of protein folding, and analysis of ligand to protein docking. These processes link form to function. Understanding them will pave the way to a post-genomic era in biological research, in which the wealth of DNA sequence information is complemented by corresponding knowledge of geometric shape. Together, sequence and shape will provide a description of the biological function so critical for all life.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86013</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86013</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1173" target="n1176">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Computational Geometry for Structural Biology and Bioinformatics</data>
      <data key="e_abstract">Life at its most detailed level depends on the geometric shape of molecules. Nevertheless, geometric methods are relatively uncommon in computational biology, primarily because of difficult and unsolved issues in applying geometric computing to biology. This project will address these causes by investigating geometric representations and developing novel geometric methods. It will incorporate these into software that helps structural biologists with their work and integrates with their current tools. The key research issues include: 1)representation and classification of geometric shape; 2)synthesis of geometric, physical, and statistical information; 3)computation and representation of motion; 4)organization of shapes for rapid searches; and 5) hierarchies for everything.&lt;br/&gt;&lt;br/&gt;This research is expected to shed light on some of the most important unsolved biological puzzles: prediction of protein structure, simulation of protein folding, and analysis of ligand to protein docking. These processes link form to function. Understanding them will pave the way to a post-genomic era in biological research, in which the wealth of DNA sequence information is complemented by corresponding knowledge of geometric shape. Together, sequence and shape will provide a description of the biological function so critical for all life.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86013</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86013</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n498" target="n1173">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Computational Geometry for Structural Biology and Bioinformatics</data>
      <data key="e_abstract">Life at its most detailed level depends on the geometric shape of molecules. Nevertheless, geometric methods are relatively uncommon in computational biology, primarily because of difficult and unsolved issues in applying geometric computing to biology. This project will address these causes by investigating geometric representations and developing novel geometric methods. It will incorporate these into software that helps structural biologists with their work and integrates with their current tools. The key research issues include: 1)representation and classification of geometric shape; 2)synthesis of geometric, physical, and statistical information; 3)computation and representation of motion; 4)organization of shapes for rapid searches; and 5) hierarchies for everything.&lt;br/&gt;&lt;br/&gt;This research is expected to shed light on some of the most important unsolved biological puzzles: prediction of protein structure, simulation of protein folding, and analysis of ligand to protein docking. These processes link form to function. Understanding them will pave the way to a post-genomic era in biological research, in which the wealth of DNA sequence information is complemented by corresponding knowledge of geometric shape. Together, sequence and shape will provide a description of the biological function so critical for all life.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86013</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86013</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n475" target="n1175">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Computational Geometry for Structural Biology and Bioinformatics</data>
      <data key="e_abstract">Life at its most detailed level depends on the geometric shape of molecules. Nevertheless, geometric methods are relatively uncommon in computational biology, primarily because of difficult and unsolved issues in applying geometric computing to biology. This project will address these causes by investigating geometric representations and developing novel geometric methods. It will incorporate these into software that helps structural biologists with their work and integrates with their current tools. The key research issues include: 1)representation and classification of geometric shape; 2)synthesis of geometric, physical, and statistical information; 3)computation and representation of motion; 4)organization of shapes for rapid searches; and 5) hierarchies for everything.&lt;br/&gt;&lt;br/&gt;This research is expected to shed light on some of the most important unsolved biological puzzles: prediction of protein structure, simulation of protein folding, and analysis of ligand to protein docking. These processes link form to function. Understanding them will pave the way to a post-genomic era in biological research, in which the wealth of DNA sequence information is complemented by corresponding knowledge of geometric shape. Together, sequence and shape will provide a description of the biological function so critical for all life.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86013</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86013</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n475" target="n1176">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Computational Geometry for Structural Biology and Bioinformatics</data>
      <data key="e_abstract">Life at its most detailed level depends on the geometric shape of molecules. Nevertheless, geometric methods are relatively uncommon in computational biology, primarily because of difficult and unsolved issues in applying geometric computing to biology. This project will address these causes by investigating geometric representations and developing novel geometric methods. It will incorporate these into software that helps structural biologists with their work and integrates with their current tools. The key research issues include: 1)representation and classification of geometric shape; 2)synthesis of geometric, physical, and statistical information; 3)computation and representation of motion; 4)organization of shapes for rapid searches; and 5) hierarchies for everything.&lt;br/&gt;&lt;br/&gt;This research is expected to shed light on some of the most important unsolved biological puzzles: prediction of protein structure, simulation of protein folding, and analysis of ligand to protein docking. These processes link form to function. Understanding them will pave the way to a post-genomic era in biological research, in which the wealth of DNA sequence information is complemented by corresponding knowledge of geometric shape. Together, sequence and shape will provide a description of the biological function so critical for all life.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86013</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86013</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n475" target="n498">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Computational Geometry for Structural Biology and Bioinformatics</data>
      <data key="e_abstract">Life at its most detailed level depends on the geometric shape of molecules. Nevertheless, geometric methods are relatively uncommon in computational biology, primarily because of difficult and unsolved issues in applying geometric computing to biology. This project will address these causes by investigating geometric representations and developing novel geometric methods. It will incorporate these into software that helps structural biologists with their work and integrates with their current tools. The key research issues include: 1)representation and classification of geometric shape; 2)synthesis of geometric, physical, and statistical information; 3)computation and representation of motion; 4)organization of shapes for rapid searches; and 5) hierarchies for everything.&lt;br/&gt;&lt;br/&gt;This research is expected to shed light on some of the most important unsolved biological puzzles: prediction of protein structure, simulation of protein folding, and analysis of ligand to protein docking. These processes link form to function. Understanding them will pave the way to a post-genomic era in biological research, in which the wealth of DNA sequence information is complemented by corresponding knowledge of geometric shape. Together, sequence and shape will provide a description of the biological function so critical for all life.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86013</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86013</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1175" target="n1176">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Computational Geometry for Structural Biology and Bioinformatics</data>
      <data key="e_abstract">Life at its most detailed level depends on the geometric shape of molecules. Nevertheless, geometric methods are relatively uncommon in computational biology, primarily because of difficult and unsolved issues in applying geometric computing to biology. This project will address these causes by investigating geometric representations and developing novel geometric methods. It will incorporate these into software that helps structural biologists with their work and integrates with their current tools. The key research issues include: 1)representation and classification of geometric shape; 2)synthesis of geometric, physical, and statistical information; 3)computation and representation of motion; 4)organization of shapes for rapid searches; and 5) hierarchies for everything.&lt;br/&gt;&lt;br/&gt;This research is expected to shed light on some of the most important unsolved biological puzzles: prediction of protein structure, simulation of protein folding, and analysis of ligand to protein docking. These processes link form to function. Understanding them will pave the way to a post-genomic era in biological research, in which the wealth of DNA sequence information is complemented by corresponding knowledge of geometric shape. Together, sequence and shape will provide a description of the biological function so critical for all life.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86013</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86013</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n498" target="n1175">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Computational Geometry for Structural Biology and Bioinformatics</data>
      <data key="e_abstract">Life at its most detailed level depends on the geometric shape of molecules. Nevertheless, geometric methods are relatively uncommon in computational biology, primarily because of difficult and unsolved issues in applying geometric computing to biology. This project will address these causes by investigating geometric representations and developing novel geometric methods. It will incorporate these into software that helps structural biologists with their work and integrates with their current tools. The key research issues include: 1)representation and classification of geometric shape; 2)synthesis of geometric, physical, and statistical information; 3)computation and representation of motion; 4)organization of shapes for rapid searches; and 5) hierarchies for everything.&lt;br/&gt;&lt;br/&gt;This research is expected to shed light on some of the most important unsolved biological puzzles: prediction of protein structure, simulation of protein folding, and analysis of ligand to protein docking. These processes link form to function. Understanding them will pave the way to a post-genomic era in biological research, in which the wealth of DNA sequence information is complemented by corresponding knowledge of geometric shape. Together, sequence and shape will provide a description of the biological function so critical for all life.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86013</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86013</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n498" target="n1176">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Computational Geometry for Structural Biology and Bioinformatics</data>
      <data key="e_abstract">Life at its most detailed level depends on the geometric shape of molecules. Nevertheless, geometric methods are relatively uncommon in computational biology, primarily because of difficult and unsolved issues in applying geometric computing to biology. This project will address these causes by investigating geometric representations and developing novel geometric methods. It will incorporate these into software that helps structural biologists with their work and integrates with their current tools. The key research issues include: 1)representation and classification of geometric shape; 2)synthesis of geometric, physical, and statistical information; 3)computation and representation of motion; 4)organization of shapes for rapid searches; and 5) hierarchies for everything.&lt;br/&gt;&lt;br/&gt;This research is expected to shed light on some of the most important unsolved biological puzzles: prediction of protein structure, simulation of protein folding, and analysis of ligand to protein docking. These processes link form to function. Understanding them will pave the way to a post-genomic era in biological research, in which the wealth of DNA sequence information is complemented by corresponding knowledge of geometric shape. Together, sequence and shape will provide a description of the biological function so critical for all life.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86013</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86013</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1178" target="n1179">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Development of a Next-Generation Ultrasound Research Platform</data>
      <data key="e_abstract">EIA-0079639&lt;br/&gt;Walker, William F.&lt;br/&gt;University of Virginia&lt;br/&gt;&lt;br/&gt;MRI: Development of a Next-Generation Ultrasound Research Platform&lt;br/&gt;&lt;br/&gt;This application proposes the development of an advanced experimental system to support ultrasonic imaging research. The proposed system will be capable of continuing acquisition over a period of 1.6 seconds, the equivalent of roughly 50 image frames. The system will also incorporate a data interface to allow future connection to custom processing units, ultimately enabling real-time processing of aperture domain data. The system will be constructed around AgilentTechnologies SONOS 5500 ultrasonic imaging system to enable real-time imaging and preserve broad signal bandwitdth, high signal to noise ration, and wide dynamic range.</data>
      <data key="e_pgm">4720</data>
      <data key="e_label">79639</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0504</data>
      <data key="e_awardID">79639</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1178" target="n1180">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Development of a Next-Generation Ultrasound Research Platform</data>
      <data key="e_abstract">EIA-0079639&lt;br/&gt;Walker, William F.&lt;br/&gt;University of Virginia&lt;br/&gt;&lt;br/&gt;MRI: Development of a Next-Generation Ultrasound Research Platform&lt;br/&gt;&lt;br/&gt;This application proposes the development of an advanced experimental system to support ultrasonic imaging research. The proposed system will be capable of continuing acquisition over a period of 1.6 seconds, the equivalent of roughly 50 image frames. The system will also incorporate a data interface to allow future connection to custom processing units, ultimately enabling real-time processing of aperture domain data. The system will be constructed around AgilentTechnologies SONOS 5500 ultrasonic imaging system to enable real-time imaging and preserve broad signal bandwitdth, high signal to noise ration, and wide dynamic range.</data>
      <data key="e_pgm">4720</data>
      <data key="e_label">79639</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0504</data>
      <data key="e_awardID">79639</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1179" target="n1180">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Development of a Next-Generation Ultrasound Research Platform</data>
      <data key="e_abstract">EIA-0079639&lt;br/&gt;Walker, William F.&lt;br/&gt;University of Virginia&lt;br/&gt;&lt;br/&gt;MRI: Development of a Next-Generation Ultrasound Research Platform&lt;br/&gt;&lt;br/&gt;This application proposes the development of an advanced experimental system to support ultrasonic imaging research. The proposed system will be capable of continuing acquisition over a period of 1.6 seconds, the equivalent of roughly 50 image frames. The system will also incorporate a data interface to allow future connection to custom processing units, ultimately enabling real-time processing of aperture domain data. The system will be constructed around AgilentTechnologies SONOS 5500 ultrasonic imaging system to enable real-time imaging and preserve broad signal bandwitdth, high signal to noise ration, and wide dynamic range.</data>
      <data key="e_pgm">4720</data>
      <data key="e_label">79639</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0504</data>
      <data key="e_awardID">79639</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1181" target="n1182">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Static and Dynamic Tools for Software Design</data>
      <data key="e_abstract">Proposal Number: 0086160&lt;br/&gt;Title: Static and Dynamic Tools for Software Design&lt;br/&gt;PIs: Monica Lam and Dawson Engler&lt;br/&gt;&lt;br/&gt;This project aims to develop a new programming methodology whereby the&lt;br/&gt;programmer, the compiler and the runtime system all cooperate in&lt;br/&gt;maintaining the integrity of a software program. The proposed system&lt;br/&gt;allows programmers to capture application-level semantics and&lt;br/&gt;invariants of interest at a high abstract level. Whereas specific&lt;br/&gt;tools have been developed by compiler writers to detect specific&lt;br/&gt;common programming errors, this system will allow programmers to&lt;br/&gt;formulate the correctness property or safety criterion that they wish&lt;br/&gt;to check in their programs. It places the full power of sophisticated&lt;br/&gt;static and dynamic analyses in programmers&apos; hands, allowing them to&lt;br/&gt;analyze and manipulate the program at ease. Success of this reserach&lt;br/&gt;will have a significant impact on improving software reliability.&lt;br/&gt;&lt;br/&gt;Expected results of this research include (1) a high-level interface&lt;br/&gt;with which the programmers communicate information to the system, (2)&lt;br/&gt;technology for creating new application-specific program analysis, (3)&lt;br/&gt;deep program analysis techniques such as pointer alias analysis and&lt;br/&gt;path-sensitive analysis to improve the precision of the static&lt;br/&gt;checker, and (4) new ways to combine static and dynamic analysis to&lt;br/&gt;locate violations of the stated properties in the code. A prototype&lt;br/&gt;system will be developed and tested on general-purpose codes such as&lt;br/&gt;open-source operating systems, compilers and browsers as well as&lt;br/&gt;embedded systems such as routers and telephone switches.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86160</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86160</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1186" target="n1187">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Predicate-Sensitive Software and Hardware Analysis to Enable Optimization and Speculation</data>
      <data key="e_abstract">Predicated execution is a feature used in the Explicitly Parallel&lt;br/&gt;Instruction Computing (EPIC) architecture for achieving the&lt;br/&gt;instruction level parallelism (ILP) needed to keep increasing future&lt;br/&gt;processor performance. The IA-64 processor being developed at Intel&lt;br/&gt;with Hewlett Packard is an example of an EPIC architecture. An&lt;br/&gt;advantage of predicated execution is the elimination of&lt;br/&gt;hard-to-predict branches by combining both paths of a branch into a&lt;br/&gt;single path, thereby obtaining additional opportunities for ILP.&lt;br/&gt;However, this merging of several paths into one has disadvantages, as&lt;br/&gt;it complicates optimizations and scheduling in both software and&lt;br/&gt;hardware.&lt;br/&gt;&lt;br/&gt;This research develops a comprehensive framework for new compiler and&lt;br/&gt;hardware analysis whose projected impact is to realize the performance&lt;br/&gt;of predicated execution. Underlying our framework is the efficient&lt;br/&gt;maintenance and use of predicate relationships and precise information&lt;br/&gt;about predicated regions. This proposal builds on our prior work by&lt;br/&gt;(1) incorporating critical path and resource constraints into a&lt;br/&gt;compiler intermediate form for predicated compilation, (2) developing&lt;br/&gt;hardware structures to allow predicate speculation and out-of-order&lt;br/&gt;execution, (3) developing software and hardware dynamic predication&lt;br/&gt;and (4) developing predicate-sensitive compiler optimizations,&lt;br/&gt;especially those based on value prediction or profiling.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">73551</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">73551</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1190" target="n1191">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR:- Virtual surfaces for human/robot mutual labor</data>
      <data key="e_abstract">This is the first year funding of a three-year continuing award. This project addresses the need to assist humans in heavy materials handling, since such tasks expose the worker to known risk factors for work-related musculoskeletal disorders, such as lifting, bending, twisting, and maintenance of awkward postures. This project will study the use of cobots to implement ergonomic guiding surfaces to assist a human in the manipulation of a heavy load. With such a mechanism, the load can be constrained to move along a frictionless guide, and the human is allowed to apply forces in directions which are comfortable while the guide directs the motion to the goal. Cobots use rolling contacts to directly implement passive guiding constraints, and as a result they are safer to interact with and use less power than a conventional robot. To design assistive guide constraints, the project will study how humans naturally interact with constraints assuming that the essential nature of this interaction can be modeled by the human&apos;s desire to minimize some notion of effort. With this model, The PIs will design guides that minimize the necessary human effort and will experimentally verify the correctness of the model. The PIs will develop software for automatically planning near-optimal guides in cluttered workspaces and will test the guides using cobot hardware on realistic materials handling tasks. The resulted ergonomic virtual surfaces in materials handling will reduce the occurrence of work-related musculoskeletal disorders. It will also increase productivity, providing an intuitive and safe interface between human and computer. Finally, this work will expose new principles in human motor control, as the design of assistive guiding surfaces requires a better understanding of how humans interact with constraints.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82957</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82957</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n308" target="n1190">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR:- Virtual surfaces for human/robot mutual labor</data>
      <data key="e_abstract">This is the first year funding of a three-year continuing award. This project addresses the need to assist humans in heavy materials handling, since such tasks expose the worker to known risk factors for work-related musculoskeletal disorders, such as lifting, bending, twisting, and maintenance of awkward postures. This project will study the use of cobots to implement ergonomic guiding surfaces to assist a human in the manipulation of a heavy load. With such a mechanism, the load can be constrained to move along a frictionless guide, and the human is allowed to apply forces in directions which are comfortable while the guide directs the motion to the goal. Cobots use rolling contacts to directly implement passive guiding constraints, and as a result they are safer to interact with and use less power than a conventional robot. To design assistive guide constraints, the project will study how humans naturally interact with constraints assuming that the essential nature of this interaction can be modeled by the human&apos;s desire to minimize some notion of effort. With this model, The PIs will design guides that minimize the necessary human effort and will experimentally verify the correctness of the model. The PIs will develop software for automatically planning near-optimal guides in cluttered workspaces and will test the guides using cobot hardware on realistic materials handling tasks. The resulted ergonomic virtual surfaces in materials handling will reduce the occurrence of work-related musculoskeletal disorders. It will also increase productivity, providing an intuitive and safe interface between human and computer. Finally, this work will expose new principles in human motor control, as the design of assistive guiding surfaces requires a better understanding of how humans interact with constraints.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82957</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82957</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1190" target="n1193">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR:- Virtual surfaces for human/robot mutual labor</data>
      <data key="e_abstract">This is the first year funding of a three-year continuing award. This project addresses the need to assist humans in heavy materials handling, since such tasks expose the worker to known risk factors for work-related musculoskeletal disorders, such as lifting, bending, twisting, and maintenance of awkward postures. This project will study the use of cobots to implement ergonomic guiding surfaces to assist a human in the manipulation of a heavy load. With such a mechanism, the load can be constrained to move along a frictionless guide, and the human is allowed to apply forces in directions which are comfortable while the guide directs the motion to the goal. Cobots use rolling contacts to directly implement passive guiding constraints, and as a result they are safer to interact with and use less power than a conventional robot. To design assistive guide constraints, the project will study how humans naturally interact with constraints assuming that the essential nature of this interaction can be modeled by the human&apos;s desire to minimize some notion of effort. With this model, The PIs will design guides that minimize the necessary human effort and will experimentally verify the correctness of the model. The PIs will develop software for automatically planning near-optimal guides in cluttered workspaces and will test the guides using cobot hardware on realistic materials handling tasks. The resulted ergonomic virtual surfaces in materials handling will reduce the occurrence of work-related musculoskeletal disorders. It will also increase productivity, providing an intuitive and safe interface between human and computer. Finally, this work will expose new principles in human motor control, as the design of assistive guiding surfaces requires a better understanding of how humans interact with constraints.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82957</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82957</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n308" target="n1191">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR:- Virtual surfaces for human/robot mutual labor</data>
      <data key="e_abstract">This is the first year funding of a three-year continuing award. This project addresses the need to assist humans in heavy materials handling, since such tasks expose the worker to known risk factors for work-related musculoskeletal disorders, such as lifting, bending, twisting, and maintenance of awkward postures. This project will study the use of cobots to implement ergonomic guiding surfaces to assist a human in the manipulation of a heavy load. With such a mechanism, the load can be constrained to move along a frictionless guide, and the human is allowed to apply forces in directions which are comfortable while the guide directs the motion to the goal. Cobots use rolling contacts to directly implement passive guiding constraints, and as a result they are safer to interact with and use less power than a conventional robot. To design assistive guide constraints, the project will study how humans naturally interact with constraints assuming that the essential nature of this interaction can be modeled by the human&apos;s desire to minimize some notion of effort. With this model, The PIs will design guides that minimize the necessary human effort and will experimentally verify the correctness of the model. The PIs will develop software for automatically planning near-optimal guides in cluttered workspaces and will test the guides using cobot hardware on realistic materials handling tasks. The resulted ergonomic virtual surfaces in materials handling will reduce the occurrence of work-related musculoskeletal disorders. It will also increase productivity, providing an intuitive and safe interface between human and computer. Finally, this work will expose new principles in human motor control, as the design of assistive guiding surfaces requires a better understanding of how humans interact with constraints.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82957</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82957</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1191" target="n1193">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR:- Virtual surfaces for human/robot mutual labor</data>
      <data key="e_abstract">This is the first year funding of a three-year continuing award. This project addresses the need to assist humans in heavy materials handling, since such tasks expose the worker to known risk factors for work-related musculoskeletal disorders, such as lifting, bending, twisting, and maintenance of awkward postures. This project will study the use of cobots to implement ergonomic guiding surfaces to assist a human in the manipulation of a heavy load. With such a mechanism, the load can be constrained to move along a frictionless guide, and the human is allowed to apply forces in directions which are comfortable while the guide directs the motion to the goal. Cobots use rolling contacts to directly implement passive guiding constraints, and as a result they are safer to interact with and use less power than a conventional robot. To design assistive guide constraints, the project will study how humans naturally interact with constraints assuming that the essential nature of this interaction can be modeled by the human&apos;s desire to minimize some notion of effort. With this model, The PIs will design guides that minimize the necessary human effort and will experimentally verify the correctness of the model. The PIs will develop software for automatically planning near-optimal guides in cluttered workspaces and will test the guides using cobot hardware on realistic materials handling tasks. The resulted ergonomic virtual surfaces in materials handling will reduce the occurrence of work-related musculoskeletal disorders. It will also increase productivity, providing an intuitive and safe interface between human and computer. Finally, this work will expose new principles in human motor control, as the design of assistive guiding surfaces requires a better understanding of how humans interact with constraints.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82957</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82957</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n308" target="n1193">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR:- Virtual surfaces for human/robot mutual labor</data>
      <data key="e_abstract">This is the first year funding of a three-year continuing award. This project addresses the need to assist humans in heavy materials handling, since such tasks expose the worker to known risk factors for work-related musculoskeletal disorders, such as lifting, bending, twisting, and maintenance of awkward postures. This project will study the use of cobots to implement ergonomic guiding surfaces to assist a human in the manipulation of a heavy load. With such a mechanism, the load can be constrained to move along a frictionless guide, and the human is allowed to apply forces in directions which are comfortable while the guide directs the motion to the goal. Cobots use rolling contacts to directly implement passive guiding constraints, and as a result they are safer to interact with and use less power than a conventional robot. To design assistive guide constraints, the project will study how humans naturally interact with constraints assuming that the essential nature of this interaction can be modeled by the human&apos;s desire to minimize some notion of effort. With this model, The PIs will design guides that minimize the necessary human effort and will experimentally verify the correctness of the model. The PIs will develop software for automatically planning near-optimal guides in cluttered workspaces and will test the guides using cobot hardware on realistic materials handling tasks. The resulted ergonomic virtual surfaces in materials handling will reduce the occurrence of work-related musculoskeletal disorders. It will also increase productivity, providing an intuitive and safe interface between human and computer. Finally, this work will expose new principles in human motor control, as the design of assistive guiding surfaces requires a better understanding of how humans interact with constraints.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82957</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82957</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1195" target="n1196">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: A Novel Graph Database Architecture for Mining Discoveries from the Human Genome</data>
      <data key="e_abstract">The goal of the Sutter graph database architecture is to help biologists make sense of the human genome, by enabling them to search genome sequence alignments for patterns of functional and structural relationships between genes. Sequence alignments are the key for discovering meaningful connections between diverse biological data, to make sense of the completed Human Genome. Yet no current database is designed to query detailed sequence alignment relationships as is needed. The Sutter architecture is designed to provide a fast, flexible, and intuitive query system for genomic alignment data, based on storing the entire graph database in a set of indexes, enabling direct lookup for any item to find its relationships. By focusing on indexing, Sutter can move away from the fixed, inflexible schema (table structure) of relational systems, while retaining some of the basis of their speed. A major design goal of Sutter is to implement genomic data objects efficiently, enabling it to store a full genome database in RAM, and achieve dramatically faster query performance. Sutter&apos;s first application is to serve the genome research community as an online resource for mining single-nucleotide polymorphisms, their effects on protein function, and mapping disease genes.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82964</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82964</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1200" target="n1201">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Global Active IP Networks (GAIN): Support for U.S. Participation in International FAIN Consortium</data>
      <data key="e_abstract">The fundamental project hypothesis is that management and control of the Internet is the ideal&lt;br/&gt;application of active networks; the researchers propose a system architecture to test this.&lt;br/&gt;Active Networks are constructed from elements, such as packet routers, allowing programmability&lt;br/&gt;on a per-user or even per-packet basis. With the new software capabilities available from systems such as Caml and Java, active networks offer the promise of more rapid adaptation to changes in technology or requirements, and more rapid introduction of new services. These potential advantages come with the disadvantages of increased complexity, and its consequences for performance and security.&lt;br/&gt; Early prototype systems (ANTS, CANES, Smart Packets, SwitchWare and others) illustrated&lt;br/&gt;various points in the design space, trading off among usability, performance, and security. The&lt;br/&gt;prototypes demonstrated first, that such systems could be built, that applications did indeed exist,&lt;br/&gt;(e.g., Active Bridging and Active Reliable Multicast), and second, that they performed well enough&lt;br/&gt;(10-100 Mbps) to handle the throughputs of almost all current Internet access points. Thus much&lt;br/&gt;of the &quot;edge&quot; of the Internet can add active network capabilities with minimal performance impact.&lt;br/&gt; A more interesting possibility exists, that of using active networking technology to incrementally&lt;br/&gt;activate the IP Internet. The researchers believe this can be achieved, as described within the proposal, by&lt;br/&gt;co-locating programmable elements with IP routers capable of fast packet forwarding. The researchers have&lt;br/&gt;experimented with this idea on a small scale and it offers considerable promise for increasing the&lt;br/&gt;manageability of the Internet with its exponential increases in scale.&lt;br/&gt; The Global Active IP Network (GAIN) project represents the University of Pennsylvania&apos;s&lt;br/&gt;research program as part of a larger 10M Euro research effort (FAIN). FAIN was considered and&lt;br/&gt;top-ranked within the E.U. IST Programme competition. European members were funded, with the&lt;br/&gt;expectation that Penn would seek funding from U.S. sources. The consortium includes University&lt;br/&gt;College London (UK), the Jozef Stefan Institute (Slovenia), the National Technical University of&lt;br/&gt;Athens (Greece), the Universitat Politecnica de Catalunya (Spain), Deutsche Telekom Berkom&lt;br/&gt;(Germany), France Telecom/CNET (France), KPN (Netherlands), Hitachi Europe Ltd. (UK),&lt;br/&gt;Hitachi Ltd. (Japan), SAG ICN (Germany), ETH Zurich (Switzerland), GMD Forschungszentrum&lt;br/&gt;Informationstechnik (Germany), IKV++ (Germany), INTEGRASys (Spain), and U. Penn in the&lt;br/&gt;United States. This proposal to NSF is a request for funds to support Penn in this international&lt;br/&gt;consortium.&lt;br/&gt; Penn&apos;s focus with GAIN is applications of Active Networks to IP network resource management&lt;br/&gt;and security. The researchers will investigate the prevention and mitigation of sophisticated &quot;denial of service&quot; attacks on security. The researchers are playing a strong role in experiment definition and evaluation for FAIN. This proposal to NSF provides background on Active Networking, outlines the research goals for an active IP network, sets this work with the context of FAIN, and argues the importance of providing U.S. participation in a truly global consortium with European and Japanese collaborators.&lt;br/&gt;( The FAIN proposal has been provided to NSF.)</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">82386</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82386</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1203" target="n1204">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: /Groupware-Mediated Cooperative Programming: Teaching Web Technology to Non-Scientists</data>
      <data key="e_abstract">EIA-0082393&lt;br/&gt;Hickey, Timothy&lt;br/&gt;Brandeis University&lt;br/&gt;&lt;br/&gt;ITR: Groupware-Mediated Cooperative Programming: Teaching Web&lt;br/&gt;Technology to Non-Scientists&lt;br/&gt;&lt;br/&gt;One consequence of the rapid growth of the Internet is the corresponding&lt;br/&gt;rapid increase in the demand for teams of software and Web site developers&lt;br/&gt;to support the creation of Web content and services. Complicating matters&lt;br/&gt;is the need to train large numbers of workers with non-technical&lt;br/&gt;backgrounds for the growing electronic workplace. This research explores&lt;br/&gt;groupware-mediated cooperative tools to teach IT skills to novices. A same&lt;br/&gt;time/different place groupware system will be built, deployed, and&lt;br/&gt;experimentally tested that supports collaborative learning of Web&lt;br/&gt;development and applet programming for a computer science general service&lt;br/&gt;course serving social science, humanities, and fine arts students.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82393</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82393</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1206" target="n1207">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Algorithms and Experimentation in Computational Geometry</data>
      <data key="e_abstract">Algorithms and Experimentation in Computational Geometry&lt;br/&gt;&lt;br/&gt;The project mixes central theoretical investigations in &lt;br/&gt;geometric computing together with experimentation with geometric codes.&lt;br/&gt;A major part of the effort will be devoted&lt;br/&gt;to problems of multi-scale representation and simplification&lt;br/&gt;of shapes in 3D, with applications to computer graphics&lt;br/&gt;and virtual reality, sampling and optimization, algorithm animation and &lt;br/&gt;visualization. On the theoretical side, it is anticipated that the work&lt;br/&gt;will draw mostly from complexity theory, discrepancy theory, and &lt;br/&gt;algorithm design, while the experimental aspect will emphasize&lt;br/&gt;software building using available geometric codes and&lt;br/&gt;animation tools</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">9.98817e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98817e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1208" target="n1209">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Development of Brain Tissue Scanner</data>
      <data key="e_abstract">EIA-0079874&lt;br/&gt;McCormick, Bruce H.&lt;br/&gt;The Texas A&amp;M University System-HSC Research Foundation&lt;br/&gt;&lt;br/&gt;MRI: Development of Brain Tissue Scanner&lt;br/&gt;&lt;br/&gt;The proposed study is a pilot study for imaging the microstructure of brain tissue to visualize brain development and connectivity. The brain tissue scanner, a new instrument for mapping brain microstructure, will be developed for the rapid and massive data acquisition necessary to investigate cytoarchitectural developmental patterns of mammalian brain. Volume scanning rates of 1 teravoxel/day are anticipated.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79874</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">79874</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1208" target="n1210">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Development of Brain Tissue Scanner</data>
      <data key="e_abstract">EIA-0079874&lt;br/&gt;McCormick, Bruce H.&lt;br/&gt;The Texas A&amp;M University System-HSC Research Foundation&lt;br/&gt;&lt;br/&gt;MRI: Development of Brain Tissue Scanner&lt;br/&gt;&lt;br/&gt;The proposed study is a pilot study for imaging the microstructure of brain tissue to visualize brain development and connectivity. The brain tissue scanner, a new instrument for mapping brain microstructure, will be developed for the rapid and massive data acquisition necessary to investigate cytoarchitectural developmental patterns of mammalian brain. Volume scanning rates of 1 teravoxel/day are anticipated.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79874</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">79874</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1209" target="n1210">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Development of Brain Tissue Scanner</data>
      <data key="e_abstract">EIA-0079874&lt;br/&gt;McCormick, Bruce H.&lt;br/&gt;The Texas A&amp;M University System-HSC Research Foundation&lt;br/&gt;&lt;br/&gt;MRI: Development of Brain Tissue Scanner&lt;br/&gt;&lt;br/&gt;The proposed study is a pilot study for imaging the microstructure of brain tissue to visualize brain development and connectivity. The brain tissue scanner, a new instrument for mapping brain microstructure, will be developed for the rapid and massive data acquisition necessary to investigate cytoarchitectural developmental patterns of mammalian brain. Volume scanning rates of 1 teravoxel/day are anticipated.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79874</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">79874</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1211" target="n1212">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Adaptive Interfaces for Collecting Survey Data from Users</data>
      <data key="e_abstract">The objective of this research is to determine how best to design computer systems for collecting &lt;br/&gt;data from (rather than providing data to) users. Government agencies might use such systems to gather the factual data used to calculate the unemployment rate or the Consumer Price Index. Three sets of laboratory experiments focus on actual and simulated desktop (i.e., keyboard and mouse entry) and speech survey interviewing systems. The first set of studies examines response accuracy and user satisfaction with systems that monitor users&apos; speed of responding and speech patterns in order to diagnose when users misinterpret concepts in the survey questions and could use additional clarification. The second set of studies examines user response accuracy and satisfaction with interfaces that do (or do not) tailor this clarification through dialogue. The third set of studies contrasts interfaces that require users to educate themselves about how the questions should be interpreted with interfaces that engage users in dialogue to figure out the correct answer. The project uses the methods of experimental psychology to provide guidelines for future development of interfaces that collect information from users. This research could significantly improve the accuracy of data collected online by government agencies and others.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81550</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81550</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1213" target="n1214">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research in Internet Topology Models - A Foundation for Large-Scale Simulations</data>
      <data key="e_abstract">Network simulation is an indispensable tool for researchers seeking to understand the principles of network architecture and protocol design. A key parameter in any moderate to large-scale simulation is the topology, i.e., the way the nodes of the network are organized and connected to each other. &quot;Good&quot; models for topology are essential for good simulations.&lt;br/&gt; The PIs have developed graph modeling software that currently is widely used as a tool for generating topologies, particularly models of large internetworks. The Georgia Tech Internet Topology Models (GT-ITM) package allows researchers to construct model topologies whose structure arguably resembles the node-level structure of the Internet: routers or switches, connected by (bidirectional) links, and grouped into domains. The GT-ITM software is included with &quot;ns2&quot; [2], the defacto open-source standard for network simulation.&lt;br/&gt; Despite the wide-spread use of GT-ITM, in general, and its transit-stub model, in particular, a number of critical and fundamental questions remain unanswered about network topology modeling. For example,&lt;br/&gt; 1)Topology models. Recent data indicates that the current Internet topology has some properties that are not well reflected in the transit-stub model of GT-ITM [17]. For example, features such as the exchanges where many transit domains come together are lacking. Are there &quot;better&quot; techniques to generate topologies intended to model the Internet? More fundamentally, how should a topology generation technique be evaluated (i.e., how is &quot;better&quot; measured)?&lt;br/&gt; 2)Topology scaling. Although strides are being made in supporting large-scale simulations [33], most researchers will continue to simulate their protocols on topologies that are smaller than the target operational large-scale networks. How should smaller topologies be configured so that they reasonable reflect their larger counterparts? Is there a theory of topology scaling that can provide the fundamental grounding for configuring topologies of various sizes?&lt;br/&gt; 3)Topology use. The PIs primary interest in topology modeling is to provide a foundation for large-scale simulations. Facilitating the use of topologies in simulations must go beyond providing theoretically sound models, however, and include a set of complementary tools for graph visualization, routing table construction, etc. What visualization tools are useful to researchers and assist in accurate intuitive understanding of underlying topology? How can different routing policies be effectively reflected in routing table construction?&lt;br/&gt; The researchers propose (1) to address these and other fundamental questions in the area of topology modeling and (2) to reflect their understanding in a set of topology tools and benchmarks made available to the research community at large. This work will build on the PIs prior experience in modeling internetworks. &lt;br/&gt; The proposed work will contribute to fundamental understanding in the area of topology modeling. The work will include a set of evaluation criteria to assess the quality of a topology generation method and improvements in topology models. The work will also produce an evolutionary theory of topology scaling, with implications for efficient simulation using topologies that are smaller than the target. In addition to contributions to fundamental understanding, a central component of the proposed work is a set of tools and benchmarks to be made available to the research community at large, following in the tradition of the GT-ITM&lt;br/&gt;suite. These tools will allow other researchers to generate topologies, assess the quality of candidate topology modeling methods, utilize benchmarks based on current and future technologies, and interact with a visualization of topology.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81557</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">81557</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1215" target="n1216">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Volume Illustration: Non-Photorealistic Rendering of Volume Models</data>
      <data key="e_abstract">This project studies the process of visualizing information at every point in space through volume rendering. Traditionally, volume rendering has employed one of two approaches. The first attempts a physically accurate simulation of a process such as X-rays passing through tissue or light passing through a fog, producing the most realistic views of volume data (at least for data with an appropriate physical meaning). The second approach is only loosely based on the physical behavior of light, using instead an arbitrary appearance of each value in space and an accumulation process through space to create a wider range of appearances for the volume in the visualization. This project proposes a new approach to volume rendering: the augmentation of a physics-based rendering process with non-photorealistic rendering (NPR) techniques to enhance the expressiveness of the visualization. NPR draws inspiration from such fields as art and technical illustration to develop automatic methods to synthesize images with an illustrated look from geometric surface models. The new approach, called volume illustration, combines the familiarity of a physics-based illumination model with the ability to enhance important features using non-photorealistic rendering techniques.&lt;br/&gt;&lt;br/&gt;Technically, the project faces several challenges. In surface-based NPR, the surfaces (features) are well defined, whereas with volumes, volumetric feature areas are often amorphous regions that must be determined through analysis of local volumetric properties. Once these volumetric feature volumes are identified, user selected parametric properties can be used to enhance and illustrate them. Volume illustration provides a flexible unified framework for enhancing structural perception of volume models through the amplification of features, the addition of illumination effects, and the application of procedural textures. Volume illustration will work on both presampled and procedurally defined volume models, enabling a range of image styles from practical technical illustrations to more abstract painterly effects. The project will develop a collection of volume illustration techniques, including novel volume illustration techniques and techniques that adapt and extend NPR techniques to volume objects.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81581</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81581</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1209" target="n1218">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: An Adaptive Wavefront Construction Algorithm for Optimal Seismic Ray Tracing</data>
      <data key="e_abstract">Ray tracing methods are an important tool in computational geophysics, and are used regularly both for simulating seismic wave propagation and for generating images of subsurface geologic structures in inversion and imaging methods. For example, these are some of the basic tools in oil exploration. However, even recent high-efficiency ray tracing implementations have some potentially important limitations. On the other hand, wavefront construction algorithms are another class of solutions that do compute all of the relevant physical information. This project will develop these methods. They will find use in a wide range of activities ranging from fundamental studies of the nature of the Earth&apos;s deep interior to analysis and characterization of oil reservoirs in the petroleum industry.&lt;br/&gt;&lt;br/&gt;Technically, this project will develop new wavefront construction methods for seismic ray tracing in 3-D, anisotropic models of the Earth. This allows more general and realistic Earth models than are currently possible. At the same time, this is an important problem in advanced computational science, because the 3-D mesh constructed in the course of the simulation will have geometries and features not usually considered in other common mesh construction algorithms. Another important aspect of the research will be the development of self-tuning components in the software to allow the algorithm to automatically adapt itself for maximum computational speed on hardware configurations ranging from small desktop workstations to massively parallel supercomputers.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81510</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81510</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n704" target="n1221">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Design and Analysis of End-to-End Feedback Congestion Control Schemes</data>
      <data key="e_abstract">The need to improve congestion control techniques for the Internet has grown recently. Non real-time data traffic, which is currently transported on a &quot;best effort&quot; basis, will increasingly have more stringent delay and throughput requirements to meet critical scientific, corporate and e-commerce applications. In order to meet this enhanced quality of service requirement, the existing feedback congestion control scheme incorporated in the Transmission Control Protocol (TCP) needs to be understood. For this purpose, the PIs intend to:&lt;br/&gt;&lt;br/&gt;1) Build an analytical model that can incorporate key features of TCP Reno and TCP enhancements under consideration by the IETF. The model the PIs present here is interesting in that it can accommodate multiple TCP flows and possibly multiple network nodes.&lt;br/&gt;&lt;br/&gt;2) Place TCP in a control-theoretic framework so that its stability and transient behavior is well understood. The use of non-linear control techniques proposed here is novel, and should make available a new set of mathematical tools to study this problem.&lt;br/&gt;&lt;br/&gt;3) The synthesis of these two activities will in turn suggest methods that ensure that any proposed TCP successor is stable, as well as improving throughput and fairness. A more limited goal but with perhaps more immediate impact is a better understanding of the stability and performance of TCP Reno.&lt;br/&gt;&lt;br/&gt;All of this work will be performed within the complementary congestion control efforts underway such as traffic engineering using the Multiprotocol Label Switching (MPLS) protocol, traffic shaping and policing, service scheduling and buffer management.&lt;br/&gt;&lt;br/&gt;In addition to a survey of the state of the art, the PIs present some preliminary results and ideas for future research. The PIs demonstrate an analytical model of TCP Reno for the single node case, and some of the insights that even such a simple model provides. The PIs also present a way of extending recent control theoretic work in flow control to a more realistic and general context, where boundary effects and unknown, time-varying propagation delays appear in networks. Lyapunov theory, the theory of functional (retarded) differential equations and constructive design methods in modern nonlinear control will play a key role in the synthesis of new feedback congestion control schemes for the Internet.&lt;br/&gt;&lt;br/&gt;The broader impacts of this research include the development of closer interaction between the control theory and networks research communities, curriculum enhancement at the graduate and undergraduate levels, applying some of the practical implications of this work through ongoing industry interactions, and inputs to IETF groups.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81527</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">81527</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1224" target="n1225">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Efficient Algorithms for Large Scale Dynamical Systems</data>
      <data key="e_abstract">This grant will develop novel high performance numerical algorithms and codes for critical problems in large dynamical systems. This includes reducing models to fewer degrees of freedom, better eigenvalue solvers, and parallel implementations. This project has applications ranging from control systems to electric power systems, circuit simulations, earth sciences (meterology, hydrology), and filtering.&lt;br/&gt;&lt;br/&gt;This is a joint project together with CCR-9912388 by Ahmed Sameh of Purdue University.</data>
      <data key="e_pgm">2865</data>
      <data key="e_label">9.91242e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.91242e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1227" target="n1228">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: From Bits to Information: Statistical Learning Technologies for Digital Information Management and Search</data>
      <data key="e_abstract">Modern statistical learning approaches are expected to play a key role in providing more powerful tools to harvest information from bits, a crucial and growing problem for the Internet. The goal of this project is thus to develop a new technology for the management, organization, and search of multimedia digital information by exploiting and extending new statistical learning theories and algorithms. In the process we expect to prototype key system components and to develop scientific insights. Anticipated outcomes of the research are (1) new learning algorithms and associated representations that can be applied to categorize text, images, and video, (2) new theoretical analyses of these learning algorithms and query-answering methods and (3) demonstrations and evaluations of prototype systems for classifying and routing email messages and searching, categorizing, and extracting information on the Web.&lt;br/&gt;&lt;br/&gt;Smarter classification software for multimedia data is a prerequisite to enable a second, more intelligent wave of Internet technologies. Automatic techniques to route, organize and search information are needed to help individuals and organizations exploit the sea of data that the computer networks are creating. The success of projects like this will make such a step possible and accelerate the evolution of the Internet.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85836</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85836</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1232" target="n1233">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Open E-Market Technology for Algorithmic Intellectual Property</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;This project seeks to develop an Internet accessible software broker to match scientists and engineers (or companies of such) offering algorithms, and/or their use, for a fee to algorithm users. To this end this project includes a theoretical investigation into scheduling, accounting and pricing related to electronic brokers for open electronic markets for algorithmic intellectual property. The technology for developing this broker would make use of the capabilities of the Internet, interoperable software such as JAVA and CORBA as well as new scheduler and accounting advances to be made in this research program.</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">9.91233e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.91233e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n301" target="n1235">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: New Algorithms for Scalable Modeling in Materials Science</data>
      <data key="e_abstract">)One of the most significant achievements of the last century has been the development of accurate methods to predict the electronic and structural properties of matter. These methods, based on density functional theory and pseudopotentials, allow us to explore the properties of materials without resort to experiments. We can now predict new materials and their properties based on numerical calculations. The only inherent limitations of these methods are computational constraints; current electronic structures methods have a very high computational cost. While the use of modern high-performance computers has enabled tremendous progress in raw computational power for these problems, gains on the algorithms side are also necessary to accommodate more complex materials.&lt;br/&gt;&lt;br/&gt;This project will introduce new methods, based on efficient algorithms, for bypassing the computational limitations mentioned above. It will seek novel solution methodologies that improve efficiency without sacrificing accuracy and functionality. In particular, one goal will be to avoid the use of eigenvectors, the primary cost for both computation and memory. The project will do this by examining the fundamental physics of the problem, which reveals that a different basis for the subspace spanned by the same eigenvectors can be computed and used instead. The project will find efficient and robust methods for computing these bases, find efficient solutions to the time-dependent and self-consistent Kohn-Sham equations, develop effective out-of-core parallel methods for solving these very large systems, and use these methods to perform pioneering calculations of real materials.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82094</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82094</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n301" target="n1236">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: New Algorithms for Scalable Modeling in Materials Science</data>
      <data key="e_abstract">)One of the most significant achievements of the last century has been the development of accurate methods to predict the electronic and structural properties of matter. These methods, based on density functional theory and pseudopotentials, allow us to explore the properties of materials without resort to experiments. We can now predict new materials and their properties based on numerical calculations. The only inherent limitations of these methods are computational constraints; current electronic structures methods have a very high computational cost. While the use of modern high-performance computers has enabled tremendous progress in raw computational power for these problems, gains on the algorithms side are also necessary to accommodate more complex materials.&lt;br/&gt;&lt;br/&gt;This project will introduce new methods, based on efficient algorithms, for bypassing the computational limitations mentioned above. It will seek novel solution methodologies that improve efficiency without sacrificing accuracy and functionality. In particular, one goal will be to avoid the use of eigenvectors, the primary cost for both computation and memory. The project will do this by examining the fundamental physics of the problem, which reveals that a different basis for the subspace spanned by the same eigenvectors can be computed and used instead. The project will find efficient and robust methods for computing these bases, find efficient solutions to the time-dependent and self-consistent Kohn-Sham equations, develop effective out-of-core parallel methods for solving these very large systems, and use these methods to perform pioneering calculations of real materials.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82094</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82094</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1235" target="n1236">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: New Algorithms for Scalable Modeling in Materials Science</data>
      <data key="e_abstract">)One of the most significant achievements of the last century has been the development of accurate methods to predict the electronic and structural properties of matter. These methods, based on density functional theory and pseudopotentials, allow us to explore the properties of materials without resort to experiments. We can now predict new materials and their properties based on numerical calculations. The only inherent limitations of these methods are computational constraints; current electronic structures methods have a very high computational cost. While the use of modern high-performance computers has enabled tremendous progress in raw computational power for these problems, gains on the algorithms side are also necessary to accommodate more complex materials.&lt;br/&gt;&lt;br/&gt;This project will introduce new methods, based on efficient algorithms, for bypassing the computational limitations mentioned above. It will seek novel solution methodologies that improve efficiency without sacrificing accuracy and functionality. In particular, one goal will be to avoid the use of eigenvectors, the primary cost for both computation and memory. The project will do this by examining the fundamental physics of the problem, which reveals that a different basis for the subspace spanned by the same eigenvectors can be computed and used instead. The project will find efficient and robust methods for computing these bases, find efficient solutions to the time-dependent and self-consistent Kohn-Sham equations, develop effective out-of-core parallel methods for solving these very large systems, and use these methods to perform pioneering calculations of real materials.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82094</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82094</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1237" target="n1238">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Learning Behaviors and Background Characteristics that Promote Retention of Women and Minorities in Undergraduate Computer Science Programs</data>
      <data key="e_abstract">Institution: University of Pittsburg&lt;br/&gt;Proposal Number: EIA 0089963&lt;br/&gt;PI: Sandra Katz&lt;br/&gt;Title: Learning Behaviors and Background Characteristics that Promote Retention of Women and Minorities in Undergraduate Computer Science Programs&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to study how students&apos; learning strategies and behaviors affect their performance in undergraduate computer science programs. Since programming is one of the first skills that computer science students learn, and a stumbling block for many, the study will focus on students&apos; programming learning strategies. The study will consist of three main activities. The first will be the identification of learning and programming behaviors that distinguish successful from unsuccessful CS students; and the determination of which behaviors, if any, are more characteristic of males than females (and the reverse) and Caucasian students than African American students (and the reverse). The second activity will be the development and evaluation of an intervention to train effective learning and programming behaviors. The third activity will be a survey of students and successful computer scientists to determine whether certain pre-college experiences predict effective learning behaviors and success in computer science. This project has the potential to provide valuable insights into the recruitment and retention of women and underrepresented minorities in computer science majors.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">89963</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">89963</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1239" target="n1240">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITW: Women and Information Systems: Modeling the Impact of Work Values, Attitudes, and Attributes on Career Choices</data>
      <data key="e_abstract">Institution: Washington State University&lt;br/&gt;Proposal Number: EIA 0089986&lt;br/&gt;PI: Kristine M. Kuhn&lt;br/&gt;Title: Women and Information Systems: Modeling the Impact of Work Values, Attitudes, and Attributes on Career Choices&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to assess whether women are more attracted to careers in Information Systems (IS) because of the perception that it is more people-oriented than other computer-related fields. Both men and women undergraduate students will be surveyed as to their work values, beliefs, and attitudes related to IS, and job preferences in order to test a model of the underlying factors driving gender differences with respect to career choices. This project has the potential to provide significant insights about factors affecting the recruitment and retention of women in IT majors</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">89986</data>
      <data key="e_expirationDate">2002-05-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">89986</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1241" target="n1242">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Development of State-of-the-Art Instrumentation for Advanced Distributed Computing Systems for Research and Research Training</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">1189</data>
      <data key="e_label">196111</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">196111</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1243" target="n1244">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITW: Retention of Women and Minorities in the IT Workforce</data>
      <data key="e_abstract">Institution: Georgia State University Research Foundation, Inc.&lt;br/&gt;Proposal Number: EIA 0089995&lt;br/&gt;PI: Paula E. Stephan&lt;br/&gt;Title: Retention of Women and Minorities in the IT Workforce&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to study whether women and underrepresented minorities either trained in IT or employed in IT careers have different rates of retention and advancement. The study also examines whether the rate by which individuals not trained in IT are absorbed into IT jobs differs by gender and minority status. The study will use the NSF&apos;s SESTAT database, which is a comprehensive, integrated database containing information over time on the employment, educational, and demographic characteristics of scientists and engineers who possess at least a bachelor&apos;s degree. This project has the potential to provide valuable insights about the retention and advancement of women and underrepresented minorities in IT careers.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">89995</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">89995</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1247" target="n1248">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Universal Access to Programming--A Cross-Generation Learning Community</data>
      <data key="e_abstract">EIA-0081102&lt;br/&gt;Rosson, Mary&lt;br/&gt;Virginia Polytechnic Institute and State University&lt;br/&gt;&lt;br/&gt;ITR: Universal Access to Programming-- A Cross-Generational Learning&lt;br/&gt;Community&lt;br/&gt;&lt;br/&gt;Issues of lifelong learning can be addressed by informal education--&lt;br/&gt;voluntary and self-directed learning activities taking place in diverse&lt;br/&gt;settings (often outside traditional classrooms) that incorporate a variety&lt;br/&gt;of learning methods and are motivated by intrinsic interests such as&lt;br/&gt;curiosity, completion of a task itself, or social interaction. The&lt;br/&gt;voluntary and self-directed nature of these informal activities make them&lt;br/&gt;ideal for reaching populations outside traditional education settings.&lt;br/&gt;With respect to informal education on programming, modern visual simulation&lt;br/&gt;environments have many features that make them appealing. This research&lt;br/&gt;will investigate the effectiveness of a state-of-the-art simulation&lt;br/&gt;programming environment as support for informal education of a diverse&lt;br/&gt;population of end-user programmers within the context of an ongoing&lt;br/&gt;research project on community network infrastructure. Key research&lt;br/&gt;objectives include:&lt;br/&gt;&lt;br/&gt;* Characterization of the programming literacy gained through visual&lt;br/&gt;simulation programming and a determination of how this knowledge and its&lt;br/&gt;accusation is mediated by the maturity and background of end users.&lt;br/&gt;&lt;br/&gt;* Analysis of the role of the existing local community as a learning&lt;br/&gt;community in building and maintaining the programming skills of end users&lt;br/&gt;of varying ages and roles.&lt;br/&gt;&lt;br/&gt;* Prototyping and evaluating a framework for cumulating and sharing the&lt;br/&gt;artifacts and practices of end-user programming within the learning&lt;br/&gt;community.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">81102</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">81102</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1249" target="n1250">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Educational Innovation: Introducing Fundamental Concepts and Evaluation Methods for Distributed Systems and Applications in the Computer Science Undergraduate Curriculum</data>
      <data key="e_abstract">EIA-0086251 &lt;br/&gt; Guha University of Central Florida&lt;br/&gt;Guha, Ratan K.&lt;br/&gt;&lt;br/&gt;CISE Educational Program: Introducing Fundamental Concepts and Evaluation Methods for Distributed Systems and Applications in the Computer Science Undergraduate Curriculum&lt;br/&gt;&lt;br/&gt;This CISE Educational Innovation award supports the development of innovative curricula for teaching contemporary concepts of distributed computer systems, computer network technologies, and principles of distributed applications to undergraduate students at the University of Central Florida and three collaborating institutions. The focus of the project is on the development of modules, course materials, courses, a delivery infrastructure, faculty enhancement workshops, and web-based data collection. Module topics include: networks and the Internet, mobile and wireless computing, network management, concepts of distributed systems, network security, performance evaluation, distributed applications, and parallel and distributed simulation. This project provides a web and CD-ROM based mechanism for distribution of modules, courses, support-software and evaluation instruments. A one-week workshop for faculty, government, and industry covering these topics will be conducted in 2002 and 2003. In addition to transferring current research in distributed systems into the undergraduate curriculum at the University of Central Florida, the project also enables three partner institutions with underrepresented student populations (Grambling State University, Florida A&amp;M University, and the University of Houston) to actively participate in the project. The collaborating institutions are involved in development and evaluation of the instructional modules and use them in their programs either as new courses or thread the modules through existing undergraduate courses.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">86251</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86251</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1249" target="n1251">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Educational Innovation: Introducing Fundamental Concepts and Evaluation Methods for Distributed Systems and Applications in the Computer Science Undergraduate Curriculum</data>
      <data key="e_abstract">EIA-0086251 &lt;br/&gt; Guha University of Central Florida&lt;br/&gt;Guha, Ratan K.&lt;br/&gt;&lt;br/&gt;CISE Educational Program: Introducing Fundamental Concepts and Evaluation Methods for Distributed Systems and Applications in the Computer Science Undergraduate Curriculum&lt;br/&gt;&lt;br/&gt;This CISE Educational Innovation award supports the development of innovative curricula for teaching contemporary concepts of distributed computer systems, computer network technologies, and principles of distributed applications to undergraduate students at the University of Central Florida and three collaborating institutions. The focus of the project is on the development of modules, course materials, courses, a delivery infrastructure, faculty enhancement workshops, and web-based data collection. Module topics include: networks and the Internet, mobile and wireless computing, network management, concepts of distributed systems, network security, performance evaluation, distributed applications, and parallel and distributed simulation. This project provides a web and CD-ROM based mechanism for distribution of modules, courses, support-software and evaluation instruments. A one-week workshop for faculty, government, and industry covering these topics will be conducted in 2002 and 2003. In addition to transferring current research in distributed systems into the undergraduate curriculum at the University of Central Florida, the project also enables three partner institutions with underrepresented student populations (Grambling State University, Florida A&amp;M University, and the University of Houston) to actively participate in the project. The collaborating institutions are involved in development and evaluation of the instructional modules and use them in their programs either as new courses or thread the modules through existing undergraduate courses.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">86251</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86251</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1250" target="n1251">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Educational Innovation: Introducing Fundamental Concepts and Evaluation Methods for Distributed Systems and Applications in the Computer Science Undergraduate Curriculum</data>
      <data key="e_abstract">EIA-0086251 &lt;br/&gt; Guha University of Central Florida&lt;br/&gt;Guha, Ratan K.&lt;br/&gt;&lt;br/&gt;CISE Educational Program: Introducing Fundamental Concepts and Evaluation Methods for Distributed Systems and Applications in the Computer Science Undergraduate Curriculum&lt;br/&gt;&lt;br/&gt;This CISE Educational Innovation award supports the development of innovative curricula for teaching contemporary concepts of distributed computer systems, computer network technologies, and principles of distributed applications to undergraduate students at the University of Central Florida and three collaborating institutions. The focus of the project is on the development of modules, course materials, courses, a delivery infrastructure, faculty enhancement workshops, and web-based data collection. Module topics include: networks and the Internet, mobile and wireless computing, network management, concepts of distributed systems, network security, performance evaluation, distributed applications, and parallel and distributed simulation. This project provides a web and CD-ROM based mechanism for distribution of modules, courses, support-software and evaluation instruments. A one-week workshop for faculty, government, and industry covering these topics will be conducted in 2002 and 2003. In addition to transferring current research in distributed systems into the undergraduate curriculum at the University of Central Florida, the project also enables three partner institutions with underrepresented student populations (Grambling State University, Florida A&amp;M University, and the University of Houston) to actively participate in the project. The collaborating institutions are involved in development and evaluation of the instructional modules and use them in their programs either as new courses or thread the modules through existing undergraduate courses.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">86251</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86251</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1254" target="n1255">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Computationally Aggressive Approaches to Adaptive Design</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;This project involves interdisciplinary research in which algorithmic&lt;br/&gt;approaches are developed to design and analyze adaptive experiments. An&lt;br/&gt;adaptive (sequential) design is one whose characteristics change in&lt;br/&gt;accordance with information arising from the ongoing experiment, as &lt;br/&gt;opposed to classical statistical designs where such characteristics are&lt;br/&gt;set in advance and remain fixed throughout. Adaptive designs have a wide&lt;br/&gt;range of application in clinical trials, destructive testing, behavioral&lt;br/&gt;ecology, computer performance prediction, adaptive control, etc., where&lt;br/&gt;they have the potential to reduce the expenditure of experimental &lt;br/&gt;``resources&apos;&apos; such as time, money, or quality of life. Unfortunately,&lt;br/&gt;adaptive designs are difficult to analyze and optimize. Exact analytic&lt;br/&gt;solutions are rarely available, and thus, historically, such designs have&lt;br/&gt;been predominantly approached via asymptotic methods and ad hoc&lt;br/&gt;approximations. Computationally, adaptive designs require significant&lt;br/&gt;time and space that has often made exact calculations infeasible.&lt;br/&gt;&lt;br/&gt;This project will expand the size and scope of solvable problems by&lt;br/&gt;developing new computational approaches for creating and evaluating&lt;br/&gt;designs and utilizing state of the art computational facilities.&lt;br/&gt;Attention is directed to problems that are important in applications,&lt;br/&gt;with a major emphasis on supplying researchers greater flexibility in&lt;br/&gt;modeling their statistical and cost objectives. For many of these&lt;br/&gt;problems, exact optimality will be unattainable, and thus techniques for&lt;br/&gt;producing near-optimal designs will also be pursued. Several of these&lt;br/&gt;techniques are based on optimizing smaller or simpler problems and&lt;br/&gt;extrapolating their solution structure to larger or more complex&lt;br/&gt;problems. This compliments analytical, asymptotic work and provides new&lt;br/&gt;insights into the structure of solutions. In other cases, a shift from&lt;br/&gt;serial algorithms to parallel ones will be used to address the&lt;br/&gt;additional complexity.</data>
      <data key="e_pgm">1269</data>
      <data key="e_label">72910</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0304</data>
      <data key="e_awardID">72910</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1259" target="n1260">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Experimental Partnership-Experimental Research in Speculative Multithreading</data>
      <data key="e_abstract">EIA-0071924&lt;br/&gt;Gurindar S. Sohi&lt;br/&gt;University of Wisconsin-Madison&lt;br/&gt;&lt;br/&gt;Experimental Partnership-Experimental Research in Speculative Multithreading&lt;br/&gt;&lt;br/&gt;Speculative threads do not depend on conservative guarantees of safe data communications among threads. Rather, threads are dispatched speculatively, and data can be communicated speculatively. i.e. by assuming that it is correct. Incorrect speculations are detected later, and whenever they occur, recovery is under taken to assure correctness. This less conservative approach to defining and dispatching threads find parallelism in ways that conservative methods cannot.&lt;br/&gt;&lt;br/&gt;In this research, the principal investigators will build a comprehensive, integrated experimental infrastructure and use it to carry out an investigation of issues related to the design of speculative multithreaded processors. Along with their graduate students the principal investigators will conduct experimental research in speculative multithreaded processors. Modern parallel processing systems decompose a program into multiple threads that execute in parallel to provide high performance. The convention method is to specify parallel threads where all communication of data is carefully synchronized to guarantee correctness a priori. This approach often means that a conservative approach must be used to provide the necessary guarantees, there by constraining parallelism. &lt;br/&gt;&lt;br/&gt;Using this infrastructure, the investigators will conduct experimental research in three primary areas.&lt;br/&gt;&lt;br/&gt;(1) Speculative Thread Identification and Usage. This will include conventional &quot;control-driven&quot; threads where the focus will be on new opportunities provided by object-oriented programs and commercial workloads. It will also include &quot;data-driven&quot; threads, a new form of speculative thread, which promises to open new opportunities for extracting parallelism from conventional programs.&lt;br/&gt;&lt;br/&gt;(2) Software/Hardware Interaction. Dynamic program characteristics of threads are likely to be critical&lt;br/&gt;for managing their identification, scheduling, and data communication. Dynamic linking will be done as well, in many large network-based applications and will very much limit the static compiler&apos;s view. The investigators will research new methods by which hardware and software can interact to compile and execute speculative multithreaded programs. This will include architecture features to permit efficient communication and the use of dynamic profiling and re-compilation techniques.&lt;br/&gt;&lt;br/&gt;(3) Hybrid &quot;Mixed Thread&quot; Processing. In future processors and systems, it is likely that several thread types will co-exist. This includes the speculative threads that are the central focus of the proposed research. It also includes the traditional non-speculative threads, which may be either explicitly programmed or implicitly extracted b software compilation tools and/or hardware. Consequently, processors and systems that integrate the complementary thread types into a cohesive &quot;mixed thread&quot; processing model will be developed and studied.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">71924</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">71924</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1262" target="n1263">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Regulation, Scheduling, and Flow Control for Quality of Service Support</data>
      <data key="e_abstract">The proposed project will investigate a number of problems in the efficient support of different qualities of service (QoS). Due to the scarcity of network resources and the emergence of new applications, it is considered crucial that the current internet evolve from a best effort service to one that supports different QoS. This problem was explored in the IETF first in the context of integrated services and more recently in the context of differentiated services. One of the key problems with the work in integrated services was the lack of scalability of per-microflow scheduling in the core of the network, and the basic tenet of differentiated services has been to move per-microflow operations to the edge of the network and deal with only macroflows in the interior of the network. Nevertheless, some of the key traffic elements such as regulators (to shape, police, and mark microflows at the edge as well as macroflows at the boundary of different subnetworks) and schedulers (across different macroflows and service classes) are still needed to provide the different QoS. Higher multiplexing gains are also sought by looking at flows statistically rather than in the worst case. Moreover, schemes that use a combination of end-to-end flow control and policing/marking at the microflow level and dropping (RIO)/scheduling at the macroflow level have also been proposed to provide a combination of assured QoS along with the ability to use additional available bandwidth. The objective of this proposal is to engineer these key traffic control elements - regulators, schedulers, and flow control mechanisms - so as to not only deliver the different QoS desired, but to do so in a manner that maximizes the network utility.&lt;br/&gt; In order to accomplish this objective, the project will rely on a modeling and analysis framework that has recently been developed by the researcher and his collaborators. This framework provides a unified mathematical model (based on the notion of traffic envelopes/ service curves / service processes) for regulators and schedulers as well as an easy way to analyze fork-join networks of such elements with (or without window/rate flow control). In particular, the researcher&apos;s prior work has shown how the impact of the entire network may be reduced to that of an equivalent single element with an (end-to-end) service curve given in terms of the service curves of the individual elements using an easy composition rule. Furthermore, the worst-case and probabilistic end-to-end performance for such networks is then easily obtained in terms of this end-to-end service curve. Since the framework is analyzable, one can &quot;invert&quot; the results of the analysis to derive rules for the design of efficient traffic control elements. More interestingly, the framework allows for the representation of a sophisticated service curve for a single traffic control element, as a network of simple service curves. This representation can then be exploited to synthesize complex but highly efficient regulators and schedulers. This modeling framework therefore provides a systematic way to analyze, design, and synthesize traffic control elements for differentiated services networks.&lt;br/&gt; In particular, the proposed project will lead to the following: It will result in the synthesis of more efficient regulators and schedulers. It will develop other guaranteed services (besides leased line emulation) in the context of differentiated services networks. It shall produce better admission control schemes for probabilistic service. For both guaranteed and probabilistic service, it will also lead to efficient allocation of network resources into service curves for different flows via pricing. In the case of adaptive service with certain minimum requirements, it will lead to a better understanding of the interplay between the in-profile and out-of-profile traffic, and how this interplay may be exploited to enhance existing adaptive mechanisms such as TCP.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">9.98053e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98053e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1262" target="n1264">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Regulation, Scheduling, and Flow Control for Quality of Service Support</data>
      <data key="e_abstract">The proposed project will investigate a number of problems in the efficient support of different qualities of service (QoS). Due to the scarcity of network resources and the emergence of new applications, it is considered crucial that the current internet evolve from a best effort service to one that supports different QoS. This problem was explored in the IETF first in the context of integrated services and more recently in the context of differentiated services. One of the key problems with the work in integrated services was the lack of scalability of per-microflow scheduling in the core of the network, and the basic tenet of differentiated services has been to move per-microflow operations to the edge of the network and deal with only macroflows in the interior of the network. Nevertheless, some of the key traffic elements such as regulators (to shape, police, and mark microflows at the edge as well as macroflows at the boundary of different subnetworks) and schedulers (across different macroflows and service classes) are still needed to provide the different QoS. Higher multiplexing gains are also sought by looking at flows statistically rather than in the worst case. Moreover, schemes that use a combination of end-to-end flow control and policing/marking at the microflow level and dropping (RIO)/scheduling at the macroflow level have also been proposed to provide a combination of assured QoS along with the ability to use additional available bandwidth. The objective of this proposal is to engineer these key traffic control elements - regulators, schedulers, and flow control mechanisms - so as to not only deliver the different QoS desired, but to do so in a manner that maximizes the network utility.&lt;br/&gt; In order to accomplish this objective, the project will rely on a modeling and analysis framework that has recently been developed by the researcher and his collaborators. This framework provides a unified mathematical model (based on the notion of traffic envelopes/ service curves / service processes) for regulators and schedulers as well as an easy way to analyze fork-join networks of such elements with (or without window/rate flow control). In particular, the researcher&apos;s prior work has shown how the impact of the entire network may be reduced to that of an equivalent single element with an (end-to-end) service curve given in terms of the service curves of the individual elements using an easy composition rule. Furthermore, the worst-case and probabilistic end-to-end performance for such networks is then easily obtained in terms of this end-to-end service curve. Since the framework is analyzable, one can &quot;invert&quot; the results of the analysis to derive rules for the design of efficient traffic control elements. More interestingly, the framework allows for the representation of a sophisticated service curve for a single traffic control element, as a network of simple service curves. This representation can then be exploited to synthesize complex but highly efficient regulators and schedulers. This modeling framework therefore provides a systematic way to analyze, design, and synthesize traffic control elements for differentiated services networks.&lt;br/&gt; In particular, the proposed project will lead to the following: It will result in the synthesis of more efficient regulators and schedulers. It will develop other guaranteed services (besides leased line emulation) in the context of differentiated services networks. It shall produce better admission control schemes for probabilistic service. For both guaranteed and probabilistic service, it will also lead to efficient allocation of network resources into service curves for different flows via pricing. In the case of adaptive service with certain minimum requirements, it will lead to a better understanding of the interplay between the in-profile and out-of-profile traffic, and how this interplay may be exploited to enhance existing adaptive mechanisms such as TCP.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">9.98053e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98053e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1263" target="n1264">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Regulation, Scheduling, and Flow Control for Quality of Service Support</data>
      <data key="e_abstract">The proposed project will investigate a number of problems in the efficient support of different qualities of service (QoS). Due to the scarcity of network resources and the emergence of new applications, it is considered crucial that the current internet evolve from a best effort service to one that supports different QoS. This problem was explored in the IETF first in the context of integrated services and more recently in the context of differentiated services. One of the key problems with the work in integrated services was the lack of scalability of per-microflow scheduling in the core of the network, and the basic tenet of differentiated services has been to move per-microflow operations to the edge of the network and deal with only macroflows in the interior of the network. Nevertheless, some of the key traffic elements such as regulators (to shape, police, and mark microflows at the edge as well as macroflows at the boundary of different subnetworks) and schedulers (across different macroflows and service classes) are still needed to provide the different QoS. Higher multiplexing gains are also sought by looking at flows statistically rather than in the worst case. Moreover, schemes that use a combination of end-to-end flow control and policing/marking at the microflow level and dropping (RIO)/scheduling at the macroflow level have also been proposed to provide a combination of assured QoS along with the ability to use additional available bandwidth. The objective of this proposal is to engineer these key traffic control elements - regulators, schedulers, and flow control mechanisms - so as to not only deliver the different QoS desired, but to do so in a manner that maximizes the network utility.&lt;br/&gt; In order to accomplish this objective, the project will rely on a modeling and analysis framework that has recently been developed by the researcher and his collaborators. This framework provides a unified mathematical model (based on the notion of traffic envelopes/ service curves / service processes) for regulators and schedulers as well as an easy way to analyze fork-join networks of such elements with (or without window/rate flow control). In particular, the researcher&apos;s prior work has shown how the impact of the entire network may be reduced to that of an equivalent single element with an (end-to-end) service curve given in terms of the service curves of the individual elements using an easy composition rule. Furthermore, the worst-case and probabilistic end-to-end performance for such networks is then easily obtained in terms of this end-to-end service curve. Since the framework is analyzable, one can &quot;invert&quot; the results of the analysis to derive rules for the design of efficient traffic control elements. More interestingly, the framework allows for the representation of a sophisticated service curve for a single traffic control element, as a network of simple service curves. This representation can then be exploited to synthesize complex but highly efficient regulators and schedulers. This modeling framework therefore provides a systematic way to analyze, design, and synthesize traffic control elements for differentiated services networks.&lt;br/&gt; In particular, the proposed project will lead to the following: It will result in the synthesis of more efficient regulators and schedulers. It will develop other guaranteed services (besides leased line emulation) in the context of differentiated services networks. It shall produce better admission control schemes for probabilistic service. For both guaranteed and probabilistic service, it will also lead to efficient allocation of network resources into service curves for different flows via pricing. In the case of adaptive service with certain minimum requirements, it will lead to a better understanding of the interplay between the in-profile and out-of-profile traffic, and how this interplay may be exploited to enhance existing adaptive mechanisms such as TCP.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">9.98053e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98053e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1266" target="n1267">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Collaborative Research - New Directions in Turbo Coding</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;This research involves the general area of error control coding for digital communication and storage systems. In particular, it describes a number of fundamental research topics related to a powerful new method of error control coding called turbo coding. The research has two major goals: (1) to propose new turbo coding schemes with performance and/or complexity advantages compared to the current state-of-the-art, and (2) to advance the fundamental state of knowledge regarding this exciting new approach to error control coding. Although still very new, turbo coding is beginning to be applied in numerous areas that require error control techniques, including deep space communication, satellite communication, and digital cellular telephony, to name just a few. Because of its ability to perform close to theoretical limits with reasonable implementation complexity, it is anticipated that turbo coding and related techniques will have an enormous impact on virtually all applications of error control coding over the next 10 years or so.&lt;br/&gt;&lt;br/&gt;Turbo coding can achieve moderate bit error rates (in the range of 10-4 to 10-6) at signal-to-noise ratios very close to channel capacity. However, there is still room for improvement in turbo coding performance, particularly in applications that require bit error rates below 10-6. Further, there is considerable theoretical interest in achieving a more complete fundamental understanding of the key properties of turbo codes that result in such excellent performance. The investigators study several new basic research problems in turbo coding. Among the topics to be investigated are (1) several new turbo code designs capable of achieving even better performance than existing schemes, (2) the introduction of a more general class of turbo codes that has the potential to yield better codes and/or reduced decoding complexity compared to standard turbo coding methods, and (3) the development of a new sub-optimum soft-in, soft-out decoding approach that can be used with more codes, thus offering the promise of near capacity performance at very low bit error rates, say below 10-10.</data>
      <data key="e_pgm">4096</data>
      <data key="e_label">75514</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">75514</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1266" target="n1268">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Collaborative Research - New Directions in Turbo Coding</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;This research involves the general area of error control coding for digital communication and storage systems. In particular, it describes a number of fundamental research topics related to a powerful new method of error control coding called turbo coding. The research has two major goals: (1) to propose new turbo coding schemes with performance and/or complexity advantages compared to the current state-of-the-art, and (2) to advance the fundamental state of knowledge regarding this exciting new approach to error control coding. Although still very new, turbo coding is beginning to be applied in numerous areas that require error control techniques, including deep space communication, satellite communication, and digital cellular telephony, to name just a few. Because of its ability to perform close to theoretical limits with reasonable implementation complexity, it is anticipated that turbo coding and related techniques will have an enormous impact on virtually all applications of error control coding over the next 10 years or so.&lt;br/&gt;&lt;br/&gt;Turbo coding can achieve moderate bit error rates (in the range of 10-4 to 10-6) at signal-to-noise ratios very close to channel capacity. However, there is still room for improvement in turbo coding performance, particularly in applications that require bit error rates below 10-6. Further, there is considerable theoretical interest in achieving a more complete fundamental understanding of the key properties of turbo codes that result in such excellent performance. The investigators study several new basic research problems in turbo coding. Among the topics to be investigated are (1) several new turbo code designs capable of achieving even better performance than existing schemes, (2) the introduction of a more general class of turbo codes that has the potential to yield better codes and/or reduced decoding complexity compared to standard turbo coding methods, and (3) the development of a new sub-optimum soft-in, soft-out decoding approach that can be used with more codes, thus offering the promise of near capacity performance at very low bit error rates, say below 10-10.</data>
      <data key="e_pgm">4096</data>
      <data key="e_label">75514</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">75514</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1267" target="n1268">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Collaborative Research - New Directions in Turbo Coding</data>
      <data key="e_abstract">ABSTRACT&lt;br/&gt;&lt;br/&gt;This research involves the general area of error control coding for digital communication and storage systems. In particular, it describes a number of fundamental research topics related to a powerful new method of error control coding called turbo coding. The research has two major goals: (1) to propose new turbo coding schemes with performance and/or complexity advantages compared to the current state-of-the-art, and (2) to advance the fundamental state of knowledge regarding this exciting new approach to error control coding. Although still very new, turbo coding is beginning to be applied in numerous areas that require error control techniques, including deep space communication, satellite communication, and digital cellular telephony, to name just a few. Because of its ability to perform close to theoretical limits with reasonable implementation complexity, it is anticipated that turbo coding and related techniques will have an enormous impact on virtually all applications of error control coding over the next 10 years or so.&lt;br/&gt;&lt;br/&gt;Turbo coding can achieve moderate bit error rates (in the range of 10-4 to 10-6) at signal-to-noise ratios very close to channel capacity. However, there is still room for improvement in turbo coding performance, particularly in applications that require bit error rates below 10-6. Further, there is considerable theoretical interest in achieving a more complete fundamental understanding of the key properties of turbo codes that result in such excellent performance. The investigators study several new basic research problems in turbo coding. Among the topics to be investigated are (1) several new turbo code designs capable of achieving even better performance than existing schemes, (2) the introduction of a more general class of turbo codes that has the potential to yield better codes and/or reduced decoding complexity compared to standard turbo coding methods, and (3) the development of a new sub-optimum soft-in, soft-out decoding approach that can be used with more codes, thus offering the promise of near capacity performance at very low bit error rates, say below 10-10.</data>
      <data key="e_pgm">4096</data>
      <data key="e_label">75514</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">75514</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1269" target="n1270">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Information Access to Spoken Documents</data>
      <data key="e_abstract">This is the first year funding of a four-year continuing award. This project addresses issues relating to the construction of a system for answering questions about information contained in a collection of spoken documents. It focuses on the key scientific questions that arise in the integration of prosodic information, speech recognition and parsing in the retrieval of spoken documents, but will not involve implementation of a complete system. There are four key themes in the research: utilizing parsing in information retrieval; integrating prosodic information in parsing spoken language; incorporating uncertainty in parsing to handle speech recognition errors; and improvements to speech recognition of spontaneous speech. All components will share a probabilistic formulation, thereby affording a systematic framework for integrating the information they provide. A primary project goal is to better understand how information provided by one of these components might be effectively utilized to improve he performance of other components in the information retrieval task. Absent a corpus tailored to the information retrieval topics the PI and his team plan to study, progress will be evaluated using existing annotated text collections such as Switchboard and LDC&apos;s Broadcast News collections. The work will lead to advances in information extraction from telephone messages, conversations, university lectures, or from any text (such as encyclopedias), and should potentially serve as the basis for a sorely needed sophisticated web browser technology and data mining applications, which in turn would enable people who currently under-utilize computers to become full participants in the information revolution.</data>
      <data key="e_pgm">5977</data>
      <data key="e_label">85940</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">85940</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n1269" target="n1271">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Information Access to Spoken Documents</data>
      <data key="e_abstract">This is the first year funding of a four-year continuing award. This project addresses issues relating to the construction of a system for answering questions about information contained in a collection of spoken documents. It focuses on the key scientific questions that arise in the integration of prosodic information, speech recognition and parsing in the retrieval of spoken documents, but will not involve implementation of a complete system. There are four key themes in the research: utilizing parsing in information retrieval; integrating prosodic information in parsing spoken language; incorporating uncertainty in parsing to handle speech recognition errors; and improvements to speech recognition of spontaneous speech. All components will share a probabilistic formulation, thereby affording a systematic framework for integrating the information they provide. A primary project goal is to better understand how information provided by one of these components might be effectively utilized to improve he performance of other components in the information retrieval task. Absent a corpus tailored to the information retrieval topics the PI and his team plan to study, progress will be evaluated using existing annotated text collections such as Switchboard and LDC&apos;s Broadcast News collections. The work will lead to advances in information extraction from telephone messages, conversations, university lectures, or from any text (such as encyclopedias), and should potentially serve as the basis for a sorely needed sophisticated web browser technology and data mining applications, which in turn would enable people who currently under-utilize computers to become full participants in the information revolution.</data>
      <data key="e_pgm">5977</data>
      <data key="e_label">85940</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">85940</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n380" target="n1269">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Information Access to Spoken Documents</data>
      <data key="e_abstract">This is the first year funding of a four-year continuing award. This project addresses issues relating to the construction of a system for answering questions about information contained in a collection of spoken documents. It focuses on the key scientific questions that arise in the integration of prosodic information, speech recognition and parsing in the retrieval of spoken documents, but will not involve implementation of a complete system. There are four key themes in the research: utilizing parsing in information retrieval; integrating prosodic information in parsing spoken language; incorporating uncertainty in parsing to handle speech recognition errors; and improvements to speech recognition of spontaneous speech. All components will share a probabilistic formulation, thereby affording a systematic framework for integrating the information they provide. A primary project goal is to better understand how information provided by one of these components might be effectively utilized to improve he performance of other components in the information retrieval task. Absent a corpus tailored to the information retrieval topics the PI and his team plan to study, progress will be evaluated using existing annotated text collections such as Switchboard and LDC&apos;s Broadcast News collections. The work will lead to advances in information extraction from telephone messages, conversations, university lectures, or from any text (such as encyclopedias), and should potentially serve as the basis for a sorely needed sophisticated web browser technology and data mining applications, which in turn would enable people who currently under-utilize computers to become full participants in the information revolution.</data>
      <data key="e_pgm">5977</data>
      <data key="e_label">85940</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">85940</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n1269" target="n1273">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Information Access to Spoken Documents</data>
      <data key="e_abstract">This is the first year funding of a four-year continuing award. This project addresses issues relating to the construction of a system for answering questions about information contained in a collection of spoken documents. It focuses on the key scientific questions that arise in the integration of prosodic information, speech recognition and parsing in the retrieval of spoken documents, but will not involve implementation of a complete system. There are four key themes in the research: utilizing parsing in information retrieval; integrating prosodic information in parsing spoken language; incorporating uncertainty in parsing to handle speech recognition errors; and improvements to speech recognition of spontaneous speech. All components will share a probabilistic formulation, thereby affording a systematic framework for integrating the information they provide. A primary project goal is to better understand how information provided by one of these components might be effectively utilized to improve he performance of other components in the information retrieval task. Absent a corpus tailored to the information retrieval topics the PI and his team plan to study, progress will be evaluated using existing annotated text collections such as Switchboard and LDC&apos;s Broadcast News collections. The work will lead to advances in information extraction from telephone messages, conversations, university lectures, or from any text (such as encyclopedias), and should potentially serve as the basis for a sorely needed sophisticated web browser technology and data mining applications, which in turn would enable people who currently under-utilize computers to become full participants in the information revolution.</data>
      <data key="e_pgm">5977</data>
      <data key="e_label">85940</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">85940</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n1270" target="n1271">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Information Access to Spoken Documents</data>
      <data key="e_abstract">This is the first year funding of a four-year continuing award. This project addresses issues relating to the construction of a system for answering questions about information contained in a collection of spoken documents. It focuses on the key scientific questions that arise in the integration of prosodic information, speech recognition and parsing in the retrieval of spoken documents, but will not involve implementation of a complete system. There are four key themes in the research: utilizing parsing in information retrieval; integrating prosodic information in parsing spoken language; incorporating uncertainty in parsing to handle speech recognition errors; and improvements to speech recognition of spontaneous speech. All components will share a probabilistic formulation, thereby affording a systematic framework for integrating the information they provide. A primary project goal is to better understand how information provided by one of these components might be effectively utilized to improve he performance of other components in the information retrieval task. Absent a corpus tailored to the information retrieval topics the PI and his team plan to study, progress will be evaluated using existing annotated text collections such as Switchboard and LDC&apos;s Broadcast News collections. The work will lead to advances in information extraction from telephone messages, conversations, university lectures, or from any text (such as encyclopedias), and should potentially serve as the basis for a sorely needed sophisticated web browser technology and data mining applications, which in turn would enable people who currently under-utilize computers to become full participants in the information revolution.</data>
      <data key="e_pgm">5977</data>
      <data key="e_label">85940</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">85940</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n380" target="n1270">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Information Access to Spoken Documents</data>
      <data key="e_abstract">This is the first year funding of a four-year continuing award. This project addresses issues relating to the construction of a system for answering questions about information contained in a collection of spoken documents. It focuses on the key scientific questions that arise in the integration of prosodic information, speech recognition and parsing in the retrieval of spoken documents, but will not involve implementation of a complete system. There are four key themes in the research: utilizing parsing in information retrieval; integrating prosodic information in parsing spoken language; incorporating uncertainty in parsing to handle speech recognition errors; and improvements to speech recognition of spontaneous speech. All components will share a probabilistic formulation, thereby affording a systematic framework for integrating the information they provide. A primary project goal is to better understand how information provided by one of these components might be effectively utilized to improve he performance of other components in the information retrieval task. Absent a corpus tailored to the information retrieval topics the PI and his team plan to study, progress will be evaluated using existing annotated text collections such as Switchboard and LDC&apos;s Broadcast News collections. The work will lead to advances in information extraction from telephone messages, conversations, university lectures, or from any text (such as encyclopedias), and should potentially serve as the basis for a sorely needed sophisticated web browser technology and data mining applications, which in turn would enable people who currently under-utilize computers to become full participants in the information revolution.</data>
      <data key="e_pgm">5977</data>
      <data key="e_label">85940</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">85940</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n1270" target="n1273">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Information Access to Spoken Documents</data>
      <data key="e_abstract">This is the first year funding of a four-year continuing award. This project addresses issues relating to the construction of a system for answering questions about information contained in a collection of spoken documents. It focuses on the key scientific questions that arise in the integration of prosodic information, speech recognition and parsing in the retrieval of spoken documents, but will not involve implementation of a complete system. There are four key themes in the research: utilizing parsing in information retrieval; integrating prosodic information in parsing spoken language; incorporating uncertainty in parsing to handle speech recognition errors; and improvements to speech recognition of spontaneous speech. All components will share a probabilistic formulation, thereby affording a systematic framework for integrating the information they provide. A primary project goal is to better understand how information provided by one of these components might be effectively utilized to improve he performance of other components in the information retrieval task. Absent a corpus tailored to the information retrieval topics the PI and his team plan to study, progress will be evaluated using existing annotated text collections such as Switchboard and LDC&apos;s Broadcast News collections. The work will lead to advances in information extraction from telephone messages, conversations, university lectures, or from any text (such as encyclopedias), and should potentially serve as the basis for a sorely needed sophisticated web browser technology and data mining applications, which in turn would enable people who currently under-utilize computers to become full participants in the information revolution.</data>
      <data key="e_pgm">5977</data>
      <data key="e_label">85940</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">85940</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n380" target="n1271">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Information Access to Spoken Documents</data>
      <data key="e_abstract">This is the first year funding of a four-year continuing award. This project addresses issues relating to the construction of a system for answering questions about information contained in a collection of spoken documents. It focuses on the key scientific questions that arise in the integration of prosodic information, speech recognition and parsing in the retrieval of spoken documents, but will not involve implementation of a complete system. There are four key themes in the research: utilizing parsing in information retrieval; integrating prosodic information in parsing spoken language; incorporating uncertainty in parsing to handle speech recognition errors; and improvements to speech recognition of spontaneous speech. All components will share a probabilistic formulation, thereby affording a systematic framework for integrating the information they provide. A primary project goal is to better understand how information provided by one of these components might be effectively utilized to improve he performance of other components in the information retrieval task. Absent a corpus tailored to the information retrieval topics the PI and his team plan to study, progress will be evaluated using existing annotated text collections such as Switchboard and LDC&apos;s Broadcast News collections. The work will lead to advances in information extraction from telephone messages, conversations, university lectures, or from any text (such as encyclopedias), and should potentially serve as the basis for a sorely needed sophisticated web browser technology and data mining applications, which in turn would enable people who currently under-utilize computers to become full participants in the information revolution.</data>
      <data key="e_pgm">5977</data>
      <data key="e_label">85940</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">85940</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n1271" target="n1273">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Information Access to Spoken Documents</data>
      <data key="e_abstract">This is the first year funding of a four-year continuing award. This project addresses issues relating to the construction of a system for answering questions about information contained in a collection of spoken documents. It focuses on the key scientific questions that arise in the integration of prosodic information, speech recognition and parsing in the retrieval of spoken documents, but will not involve implementation of a complete system. There are four key themes in the research: utilizing parsing in information retrieval; integrating prosodic information in parsing spoken language; incorporating uncertainty in parsing to handle speech recognition errors; and improvements to speech recognition of spontaneous speech. All components will share a probabilistic formulation, thereby affording a systematic framework for integrating the information they provide. A primary project goal is to better understand how information provided by one of these components might be effectively utilized to improve he performance of other components in the information retrieval task. Absent a corpus tailored to the information retrieval topics the PI and his team plan to study, progress will be evaluated using existing annotated text collections such as Switchboard and LDC&apos;s Broadcast News collections. The work will lead to advances in information extraction from telephone messages, conversations, university lectures, or from any text (such as encyclopedias), and should potentially serve as the basis for a sorely needed sophisticated web browser technology and data mining applications, which in turn would enable people who currently under-utilize computers to become full participants in the information revolution.</data>
      <data key="e_pgm">5977</data>
      <data key="e_label">85940</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">85940</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n380" target="n1273">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Information Access to Spoken Documents</data>
      <data key="e_abstract">This is the first year funding of a four-year continuing award. This project addresses issues relating to the construction of a system for answering questions about information contained in a collection of spoken documents. It focuses on the key scientific questions that arise in the integration of prosodic information, speech recognition and parsing in the retrieval of spoken documents, but will not involve implementation of a complete system. There are four key themes in the research: utilizing parsing in information retrieval; integrating prosodic information in parsing spoken language; incorporating uncertainty in parsing to handle speech recognition errors; and improvements to speech recognition of spontaneous speech. All components will share a probabilistic formulation, thereby affording a systematic framework for integrating the information they provide. A primary project goal is to better understand how information provided by one of these components might be effectively utilized to improve he performance of other components in the information retrieval task. Absent a corpus tailored to the information retrieval topics the PI and his team plan to study, progress will be evaluated using existing annotated text collections such as Switchboard and LDC&apos;s Broadcast News collections. The work will lead to advances in information extraction from telephone messages, conversations, university lectures, or from any text (such as encyclopedias), and should potentially serve as the basis for a sorely needed sophisticated web browser technology and data mining applications, which in turn would enable people who currently under-utilize computers to become full participants in the information revolution.</data>
      <data key="e_pgm">5977</data>
      <data key="e_label">85940</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">85940</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n1279" target="n1280">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Dynamic Negotiation Agents in Mobile Computing</data>
      <data key="e_abstract">With the advent of third and fourth generation wireless infrastructure, and the simultaneous emergency of pervasive connectivity for all devices based on bluetooth like systems and ad-hoc networks, a new vista is open for research in the area. We propose ideas for a research program aimed at realizing ubiquitous computing systems based on the cooperation of autonomous, dynamic and adaptive components (hardware as well as software) which are located in vicinity of one another. These systems will be composed of a collection of independently designed components that automatically become aware of each other, establish basic (wireless) communication, exchange information about their capabilities and requirements, discover and exchange APIs, and learn to cooperate effectively to accomplish their individual and collective goals. The proposed work will enable a new class of applications that effectively use mobility and pervasive computing. We address several research problems that span the fields of distributed computing, data management, and dynamic collaboration between components. The team of researchers is located at UMBC and UI-Chicago, and plans to interact closely with collaborators at industrial labs (IBM, Hughes, Sun).</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">70802</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">70802</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1279" target="n1281">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Dynamic Negotiation Agents in Mobile Computing</data>
      <data key="e_abstract">With the advent of third and fourth generation wireless infrastructure, and the simultaneous emergency of pervasive connectivity for all devices based on bluetooth like systems and ad-hoc networks, a new vista is open for research in the area. We propose ideas for a research program aimed at realizing ubiquitous computing systems based on the cooperation of autonomous, dynamic and adaptive components (hardware as well as software) which are located in vicinity of one another. These systems will be composed of a collection of independently designed components that automatically become aware of each other, establish basic (wireless) communication, exchange information about their capabilities and requirements, discover and exchange APIs, and learn to cooperate effectively to accomplish their individual and collective goals. The proposed work will enable a new class of applications that effectively use mobility and pervasive computing. We address several research problems that span the fields of distributed computing, data management, and dynamic collaboration between components. The team of researchers is located at UMBC and UI-Chicago, and plans to interact closely with collaborators at industrial labs (IBM, Hughes, Sun).</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">70802</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">70802</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1280" target="n1281">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Dynamic Negotiation Agents in Mobile Computing</data>
      <data key="e_abstract">With the advent of third and fourth generation wireless infrastructure, and the simultaneous emergency of pervasive connectivity for all devices based on bluetooth like systems and ad-hoc networks, a new vista is open for research in the area. We propose ideas for a research program aimed at realizing ubiquitous computing systems based on the cooperation of autonomous, dynamic and adaptive components (hardware as well as software) which are located in vicinity of one another. These systems will be composed of a collection of independently designed components that automatically become aware of each other, establish basic (wireless) communication, exchange information about their capabilities and requirements, discover and exchange APIs, and learn to cooperate effectively to accomplish their individual and collective goals. The proposed work will enable a new class of applications that effectively use mobility and pervasive computing. We address several research problems that span the fields of distributed computing, data management, and dynamic collaboration between components. The team of researchers is located at UMBC and UI-Chicago, and plans to interact closely with collaborators at industrial labs (IBM, Hughes, Sun).</data>
      <data key="e_pgm">2876</data>
      <data key="e_label">70802</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">70802</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1092" target="n1283">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Adaptive Reduced-Rank Interference Suppression: Algorithms, Performance, and Low Power VLSI</data>
      <data key="e_abstract">This research is concerned with algorithmic, performance, and hardware issues related to reduced-rank adaptive filtering. Reduced-rank filters project the incoming received signal onto a lower dimensional subspace, which reduces the amount of training data needed relative to a conventional full-rank algorithm.&lt;br/&gt; Algorithmic techniques will be studied initially within the context of interference suppression for Direct Sequence (DS)-Code-Division Multiple Access (CDMA), although they can be applied to any adaptive linear filter. The focus of the research is on a recently developed class of reduced-rank adaptive algorithms based on the multi-stage Wiener filter of Goldstein and Reed. This technique can achieve full-rank performance&lt;br/&gt;with a very low filter rank, which enables rapid convergence and tracking. Furthermore, these algorithms&lt;br/&gt;do not rely on an explicit estimate of the signal subspace. The project is multi-disciplinary in that it combines the expertise of the two co-PIs in the areas of adaptive signal processing and low power VLSI design.&lt;br/&gt;The main objective of the research is to build a low-power special purpose hardware prototype, which can serve as the computational engine for reduced-rank filtering in a variety of applications. Algorithmic issues to be studied include selection of filter rank, performance in different adaptive filtering applications, such as equalization, and numerical stability and dynamic range problems.</data>
      <data key="e_pgm">4720</data>
      <data key="e_label">73686</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">73686</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1284" target="n1285">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Associative Mining of Large Datasets</data>
      <data key="e_abstract">The goal of this project is to develop a methodology and set of prototype tools to enable &quot;associative&quot; mining of very large scientific data sets. The researchers will use content, such as solution features, patterns, and shapes, to examine the data and retrieve required information. Unlike other approaches that use index-based coordinates, this lets scientists answer the kinds of questions that they typically ask - such as &quot;Have I seen this evolution before?&quot; and &quot;Is it similar to any experimental observations?&quot; The tools developed by this project will operate on distributed time-varying data and will act as templates for other methods. Specific technical objectives include developing distributed multi-resolution techniques for cataloging interesting phenomena and searching both run-time and archived data for interesting phenomena. The research will specifically target two domains that are representative of other scientific areas and have a pressing need for scientific mining tools: fluid dynamics (large-scale, high-accuracy Direct Numerical Simulation of compressible turbulence) and oceanography (comparison of simulation data with acoustic observations of hydrothermal plumes).</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82634</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82634</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1284" target="n1286">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Associative Mining of Large Datasets</data>
      <data key="e_abstract">The goal of this project is to develop a methodology and set of prototype tools to enable &quot;associative&quot; mining of very large scientific data sets. The researchers will use content, such as solution features, patterns, and shapes, to examine the data and retrieve required information. Unlike other approaches that use index-based coordinates, this lets scientists answer the kinds of questions that they typically ask - such as &quot;Have I seen this evolution before?&quot; and &quot;Is it similar to any experimental observations?&quot; The tools developed by this project will operate on distributed time-varying data and will act as templates for other methods. Specific technical objectives include developing distributed multi-resolution techniques for cataloging interesting phenomena and searching both run-time and archived data for interesting phenomena. The research will specifically target two domains that are representative of other scientific areas and have a pressing need for scientific mining tools: fluid dynamics (large-scale, high-accuracy Direct Numerical Simulation of compressible turbulence) and oceanography (comparison of simulation data with acoustic observations of hydrothermal plumes).</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82634</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82634</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n148" target="n1284">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Associative Mining of Large Datasets</data>
      <data key="e_abstract">The goal of this project is to develop a methodology and set of prototype tools to enable &quot;associative&quot; mining of very large scientific data sets. The researchers will use content, such as solution features, patterns, and shapes, to examine the data and retrieve required information. Unlike other approaches that use index-based coordinates, this lets scientists answer the kinds of questions that they typically ask - such as &quot;Have I seen this evolution before?&quot; and &quot;Is it similar to any experimental observations?&quot; The tools developed by this project will operate on distributed time-varying data and will act as templates for other methods. Specific technical objectives include developing distributed multi-resolution techniques for cataloging interesting phenomena and searching both run-time and archived data for interesting phenomena. The research will specifically target two domains that are representative of other scientific areas and have a pressing need for scientific mining tools: fluid dynamics (large-scale, high-accuracy Direct Numerical Simulation of compressible turbulence) and oceanography (comparison of simulation data with acoustic observations of hydrothermal plumes).</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82634</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82634</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1285" target="n1286">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Associative Mining of Large Datasets</data>
      <data key="e_abstract">The goal of this project is to develop a methodology and set of prototype tools to enable &quot;associative&quot; mining of very large scientific data sets. The researchers will use content, such as solution features, patterns, and shapes, to examine the data and retrieve required information. Unlike other approaches that use index-based coordinates, this lets scientists answer the kinds of questions that they typically ask - such as &quot;Have I seen this evolution before?&quot; and &quot;Is it similar to any experimental observations?&quot; The tools developed by this project will operate on distributed time-varying data and will act as templates for other methods. Specific technical objectives include developing distributed multi-resolution techniques for cataloging interesting phenomena and searching both run-time and archived data for interesting phenomena. The research will specifically target two domains that are representative of other scientific areas and have a pressing need for scientific mining tools: fluid dynamics (large-scale, high-accuracy Direct Numerical Simulation of compressible turbulence) and oceanography (comparison of simulation data with acoustic observations of hydrothermal plumes).</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82634</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82634</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n148" target="n1285">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Associative Mining of Large Datasets</data>
      <data key="e_abstract">The goal of this project is to develop a methodology and set of prototype tools to enable &quot;associative&quot; mining of very large scientific data sets. The researchers will use content, such as solution features, patterns, and shapes, to examine the data and retrieve required information. Unlike other approaches that use index-based coordinates, this lets scientists answer the kinds of questions that they typically ask - such as &quot;Have I seen this evolution before?&quot; and &quot;Is it similar to any experimental observations?&quot; The tools developed by this project will operate on distributed time-varying data and will act as templates for other methods. Specific technical objectives include developing distributed multi-resolution techniques for cataloging interesting phenomena and searching both run-time and archived data for interesting phenomena. The research will specifically target two domains that are representative of other scientific areas and have a pressing need for scientific mining tools: fluid dynamics (large-scale, high-accuracy Direct Numerical Simulation of compressible turbulence) and oceanography (comparison of simulation data with acoustic observations of hydrothermal plumes).</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82634</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82634</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n148" target="n1286">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Associative Mining of Large Datasets</data>
      <data key="e_abstract">The goal of this project is to develop a methodology and set of prototype tools to enable &quot;associative&quot; mining of very large scientific data sets. The researchers will use content, such as solution features, patterns, and shapes, to examine the data and retrieve required information. Unlike other approaches that use index-based coordinates, this lets scientists answer the kinds of questions that they typically ask - such as &quot;Have I seen this evolution before?&quot; and &quot;Is it similar to any experimental observations?&quot; The tools developed by this project will operate on distributed time-varying data and will act as templates for other methods. Specific technical objectives include developing distributed multi-resolution techniques for cataloging interesting phenomena and searching both run-time and archived data for interesting phenomena. The research will specifically target two domains that are representative of other scientific areas and have a pressing need for scientific mining tools: fluid dynamics (large-scale, high-accuracy Direct Numerical Simulation of compressible turbulence) and oceanography (comparison of simulation data with acoustic observations of hydrothermal plumes).</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82634</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82634</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1288" target="n1289">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Collaborative Research--Ascertaining Runtime Branch Characteristics through Algebraic Analysis of Programs</data>
      <data key="e_abstract">The goal of this research is to reduce the cost and improve the&lt;br/&gt;performance of observation-based branch characterization mechanisms in&lt;br/&gt;the compiler and hardware. Often, correlation discovered at great cost&lt;br/&gt;or missed entirely through execution can be determined in a simple&lt;br/&gt;algebraic fashion at compile time. Relationships between program&lt;br/&gt;structures can be inferred from these algebraic expressions and&lt;br/&gt;subsequently conveyed to compiler optimizations and to the hardware&lt;br/&gt;through appropriate mechanisms to be developed by this research. Once&lt;br/&gt;employed, these relationships can refocus the efforts expended by&lt;br/&gt;observation-based mechanisms, or can eliminate the need for them&lt;br/&gt;altogether.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82630</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82630</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n269" target="n1291">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Reduced Basis Methodologies for Computation, Analysis and Visualization of Bio-Molecular Simulations</data>
      <data key="e_abstract">`The PIs will investigate and develop software and computational methods to address one of the most challenging current problems in molecular dynamics (MD) simulations - large proteins, consisting of tens of thousands of atoms in solution over the physiological range of at least a microsecond of folding time. This will be accomplished using a reduced basis approach based on singular value decomposition (SVD) of the MD trajectory.&lt;br/&gt;&lt;br/&gt;The work will entail:&lt;br/&gt;&lt;br/&gt;- developing a block SVD updating scheme that will enable new trajectory information to be adjoined to a current truncated SVD approximation to a prior trajectory and avoid having to store the entire trajectory &lt;br/&gt;- developing, analyzing, and implementing a reduced basis integrator that will work in concert with the SVD updating scheme to compute the reduced basis simulation more rapidly&lt;br/&gt;- adapting the fast marching algorithms developed for latent semantic indexing to develop the rapid graphical query tools to locate sites that potentially match local structures of interest&lt;br/&gt;- developing the I/O support and visualization capabilities to handle the extremely large data manipulation and representation problems that will be generated&lt;br/&gt;&lt;br/&gt;The result of the research will be an efficient time integration scheme that can drastically limit storage and yet resolve detail on multiple scales. It will be demonstrated on a fully solvated protein molecule over a time scale of a microsecond, and the high order, low frequency motions will be visualized.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82645</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82645</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1292" target="n1293">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Security Education in Embedded Computing</data>
      <data key="e_abstract">EIA-0082635&lt;br/&gt;Williams, Ronald&lt;br/&gt;University of Virginia&lt;br/&gt;&lt;br/&gt;ITR: Security Education in Embedded Computing&lt;br/&gt;&lt;br/&gt;Education and research will be tightly coupled as this effort focuses upon&lt;br/&gt;security in networked embedded computer systems. The vulnerability of&lt;br/&gt;networked computers to malicious or mischievous attack has become apparent&lt;br/&gt;with several recent incidents. With the present trend to connect more&lt;br/&gt;embedded systems to global networks, security is a growing concern. The&lt;br/&gt;investigators on this project seek to identify aspects of well-studied&lt;br/&gt;embedded computing characteristics that can be extended and applied to&lt;br/&gt;issues of security. &lt;br/&gt;&lt;br/&gt;The primary objectives of this work are to develop a theoretical foundation&lt;br/&gt;for embedded system security and to expand the base of security education&lt;br/&gt;for computer engineers. The theoretical foundation will build upon the&lt;br/&gt;body of knowledge developed over past decades in availability, reliability,&lt;br/&gt;and safety research. Educational modules will be made available and can be&lt;br/&gt;used in the future to form the basis for a more extensive educational&lt;br/&gt;program in this area. The potential impact of this effort will include a&lt;br/&gt;significant increase in the sensitivity to embedded system security issues&lt;br/&gt;among those computer engineers who participate in the project or are&lt;br/&gt;exposed to the educational modules. Further, new applications of existing&lt;br/&gt;techniques will improve the evaluation and design of embedded systems to&lt;br/&gt;include security considerations. Both of these impacts are significant to&lt;br/&gt;increase the integrity of networked embedded computer systems.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82635</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82635</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1296" target="n1297">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: DHARMA: Domain-Specific Metaware for Hydrologic Applications</data>
      <data key="e_abstract">The DHARMA domain-specific middleware system being developed in this project is intended to allow hydrologic field engineers to address water-management problems, on a scale previously impossible, with sophisticated domain-specific computational management systems. DHARMA is used to provide automatic data acquisition via the Internet; data fusion from online, local, and cached resources; smart caching of intermediate results to allow for reuse in future simulation cycles; and smart scheduling to optimize execution times on metacomputing systems. Our target watershed model, WEPP, is a continuous simulation processed-based model which represents new soil erosion prediction capabilities. However, it is limited to very small watersheds with current computer technology. A revolutionary change in hydrologic modeling on the watershed scale will be brought about by applying the WEPP model to the 925 square miles Lake Decatur watershed. Currently, all watershed scale hydrologic models are based on empirical relationships. WEPP is the only model which scientifically accounts for soils, sediment transport, runoff, channel flow, plant growth, decomposition, snow melt, freeze-thaw effects, and climatic conditions. Through DHARMA, WEPP is the only model which will provide accurate predictions and evaluate the effects of alternative watershed management practices on watershed water quality.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82667</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82667</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1296" target="n1298">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: DHARMA: Domain-Specific Metaware for Hydrologic Applications</data>
      <data key="e_abstract">The DHARMA domain-specific middleware system being developed in this project is intended to allow hydrologic field engineers to address water-management problems, on a scale previously impossible, with sophisticated domain-specific computational management systems. DHARMA is used to provide automatic data acquisition via the Internet; data fusion from online, local, and cached resources; smart caching of intermediate results to allow for reuse in future simulation cycles; and smart scheduling to optimize execution times on metacomputing systems. Our target watershed model, WEPP, is a continuous simulation processed-based model which represents new soil erosion prediction capabilities. However, it is limited to very small watersheds with current computer technology. A revolutionary change in hydrologic modeling on the watershed scale will be brought about by applying the WEPP model to the 925 square miles Lake Decatur watershed. Currently, all watershed scale hydrologic models are based on empirical relationships. WEPP is the only model which scientifically accounts for soils, sediment transport, runoff, channel flow, plant growth, decomposition, snow melt, freeze-thaw effects, and climatic conditions. Through DHARMA, WEPP is the only model which will provide accurate predictions and evaluate the effects of alternative watershed management practices on watershed water quality.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82667</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82667</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1296" target="n1299">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: DHARMA: Domain-Specific Metaware for Hydrologic Applications</data>
      <data key="e_abstract">The DHARMA domain-specific middleware system being developed in this project is intended to allow hydrologic field engineers to address water-management problems, on a scale previously impossible, with sophisticated domain-specific computational management systems. DHARMA is used to provide automatic data acquisition via the Internet; data fusion from online, local, and cached resources; smart caching of intermediate results to allow for reuse in future simulation cycles; and smart scheduling to optimize execution times on metacomputing systems. Our target watershed model, WEPP, is a continuous simulation processed-based model which represents new soil erosion prediction capabilities. However, it is limited to very small watersheds with current computer technology. A revolutionary change in hydrologic modeling on the watershed scale will be brought about by applying the WEPP model to the 925 square miles Lake Decatur watershed. Currently, all watershed scale hydrologic models are based on empirical relationships. WEPP is the only model which scientifically accounts for soils, sediment transport, runoff, channel flow, plant growth, decomposition, snow melt, freeze-thaw effects, and climatic conditions. Through DHARMA, WEPP is the only model which will provide accurate predictions and evaluate the effects of alternative watershed management practices on watershed water quality.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82667</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82667</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1296" target="n1300">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: DHARMA: Domain-Specific Metaware for Hydrologic Applications</data>
      <data key="e_abstract">The DHARMA domain-specific middleware system being developed in this project is intended to allow hydrologic field engineers to address water-management problems, on a scale previously impossible, with sophisticated domain-specific computational management systems. DHARMA is used to provide automatic data acquisition via the Internet; data fusion from online, local, and cached resources; smart caching of intermediate results to allow for reuse in future simulation cycles; and smart scheduling to optimize execution times on metacomputing systems. Our target watershed model, WEPP, is a continuous simulation processed-based model which represents new soil erosion prediction capabilities. However, it is limited to very small watersheds with current computer technology. A revolutionary change in hydrologic modeling on the watershed scale will be brought about by applying the WEPP model to the 925 square miles Lake Decatur watershed. Currently, all watershed scale hydrologic models are based on empirical relationships. WEPP is the only model which scientifically accounts for soils, sediment transport, runoff, channel flow, plant growth, decomposition, snow melt, freeze-thaw effects, and climatic conditions. Through DHARMA, WEPP is the only model which will provide accurate predictions and evaluate the effects of alternative watershed management practices on watershed water quality.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82667</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82667</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1297" target="n1298">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: DHARMA: Domain-Specific Metaware for Hydrologic Applications</data>
      <data key="e_abstract">The DHARMA domain-specific middleware system being developed in this project is intended to allow hydrologic field engineers to address water-management problems, on a scale previously impossible, with sophisticated domain-specific computational management systems. DHARMA is used to provide automatic data acquisition via the Internet; data fusion from online, local, and cached resources; smart caching of intermediate results to allow for reuse in future simulation cycles; and smart scheduling to optimize execution times on metacomputing systems. Our target watershed model, WEPP, is a continuous simulation processed-based model which represents new soil erosion prediction capabilities. However, it is limited to very small watersheds with current computer technology. A revolutionary change in hydrologic modeling on the watershed scale will be brought about by applying the WEPP model to the 925 square miles Lake Decatur watershed. Currently, all watershed scale hydrologic models are based on empirical relationships. WEPP is the only model which scientifically accounts for soils, sediment transport, runoff, channel flow, plant growth, decomposition, snow melt, freeze-thaw effects, and climatic conditions. Through DHARMA, WEPP is the only model which will provide accurate predictions and evaluate the effects of alternative watershed management practices on watershed water quality.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82667</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82667</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1297" target="n1299">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: DHARMA: Domain-Specific Metaware for Hydrologic Applications</data>
      <data key="e_abstract">The DHARMA domain-specific middleware system being developed in this project is intended to allow hydrologic field engineers to address water-management problems, on a scale previously impossible, with sophisticated domain-specific computational management systems. DHARMA is used to provide automatic data acquisition via the Internet; data fusion from online, local, and cached resources; smart caching of intermediate results to allow for reuse in future simulation cycles; and smart scheduling to optimize execution times on metacomputing systems. Our target watershed model, WEPP, is a continuous simulation processed-based model which represents new soil erosion prediction capabilities. However, it is limited to very small watersheds with current computer technology. A revolutionary change in hydrologic modeling on the watershed scale will be brought about by applying the WEPP model to the 925 square miles Lake Decatur watershed. Currently, all watershed scale hydrologic models are based on empirical relationships. WEPP is the only model which scientifically accounts for soils, sediment transport, runoff, channel flow, plant growth, decomposition, snow melt, freeze-thaw effects, and climatic conditions. Through DHARMA, WEPP is the only model which will provide accurate predictions and evaluate the effects of alternative watershed management practices on watershed water quality.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82667</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82667</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1297" target="n1300">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: DHARMA: Domain-Specific Metaware for Hydrologic Applications</data>
      <data key="e_abstract">The DHARMA domain-specific middleware system being developed in this project is intended to allow hydrologic field engineers to address water-management problems, on a scale previously impossible, with sophisticated domain-specific computational management systems. DHARMA is used to provide automatic data acquisition via the Internet; data fusion from online, local, and cached resources; smart caching of intermediate results to allow for reuse in future simulation cycles; and smart scheduling to optimize execution times on metacomputing systems. Our target watershed model, WEPP, is a continuous simulation processed-based model which represents new soil erosion prediction capabilities. However, it is limited to very small watersheds with current computer technology. A revolutionary change in hydrologic modeling on the watershed scale will be brought about by applying the WEPP model to the 925 square miles Lake Decatur watershed. Currently, all watershed scale hydrologic models are based on empirical relationships. WEPP is the only model which scientifically accounts for soils, sediment transport, runoff, channel flow, plant growth, decomposition, snow melt, freeze-thaw effects, and climatic conditions. Through DHARMA, WEPP is the only model which will provide accurate predictions and evaluate the effects of alternative watershed management practices on watershed water quality.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82667</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82667</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1298" target="n1299">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: DHARMA: Domain-Specific Metaware for Hydrologic Applications</data>
      <data key="e_abstract">The DHARMA domain-specific middleware system being developed in this project is intended to allow hydrologic field engineers to address water-management problems, on a scale previously impossible, with sophisticated domain-specific computational management systems. DHARMA is used to provide automatic data acquisition via the Internet; data fusion from online, local, and cached resources; smart caching of intermediate results to allow for reuse in future simulation cycles; and smart scheduling to optimize execution times on metacomputing systems. Our target watershed model, WEPP, is a continuous simulation processed-based model which represents new soil erosion prediction capabilities. However, it is limited to very small watersheds with current computer technology. A revolutionary change in hydrologic modeling on the watershed scale will be brought about by applying the WEPP model to the 925 square miles Lake Decatur watershed. Currently, all watershed scale hydrologic models are based on empirical relationships. WEPP is the only model which scientifically accounts for soils, sediment transport, runoff, channel flow, plant growth, decomposition, snow melt, freeze-thaw effects, and climatic conditions. Through DHARMA, WEPP is the only model which will provide accurate predictions and evaluate the effects of alternative watershed management practices on watershed water quality.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82667</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82667</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1298" target="n1300">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: DHARMA: Domain-Specific Metaware for Hydrologic Applications</data>
      <data key="e_abstract">The DHARMA domain-specific middleware system being developed in this project is intended to allow hydrologic field engineers to address water-management problems, on a scale previously impossible, with sophisticated domain-specific computational management systems. DHARMA is used to provide automatic data acquisition via the Internet; data fusion from online, local, and cached resources; smart caching of intermediate results to allow for reuse in future simulation cycles; and smart scheduling to optimize execution times on metacomputing systems. Our target watershed model, WEPP, is a continuous simulation processed-based model which represents new soil erosion prediction capabilities. However, it is limited to very small watersheds with current computer technology. A revolutionary change in hydrologic modeling on the watershed scale will be brought about by applying the WEPP model to the 925 square miles Lake Decatur watershed. Currently, all watershed scale hydrologic models are based on empirical relationships. WEPP is the only model which scientifically accounts for soils, sediment transport, runoff, channel flow, plant growth, decomposition, snow melt, freeze-thaw effects, and climatic conditions. Through DHARMA, WEPP is the only model which will provide accurate predictions and evaluate the effects of alternative watershed management practices on watershed water quality.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82667</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82667</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1299" target="n1300">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: DHARMA: Domain-Specific Metaware for Hydrologic Applications</data>
      <data key="e_abstract">The DHARMA domain-specific middleware system being developed in this project is intended to allow hydrologic field engineers to address water-management problems, on a scale previously impossible, with sophisticated domain-specific computational management systems. DHARMA is used to provide automatic data acquisition via the Internet; data fusion from online, local, and cached resources; smart caching of intermediate results to allow for reuse in future simulation cycles; and smart scheduling to optimize execution times on metacomputing systems. Our target watershed model, WEPP, is a continuous simulation processed-based model which represents new soil erosion prediction capabilities. However, it is limited to very small watersheds with current computer technology. A revolutionary change in hydrologic modeling on the watershed scale will be brought about by applying the WEPP model to the 925 square miles Lake Decatur watershed. Currently, all watershed scale hydrologic models are based on empirical relationships. WEPP is the only model which scientifically accounts for soils, sediment transport, runoff, channel flow, plant growth, decomposition, snow melt, freeze-thaw effects, and climatic conditions. Through DHARMA, WEPP is the only model which will provide accurate predictions and evaluate the effects of alternative watershed management practices on watershed water quality.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82667</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82667</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1302" target="n1303">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Quality Software by Design</data>
      <data key="e_abstract">PROPOSAL NUMBER: CCR-0083099&lt;br/&gt;INSTITUTION: Univ of Ca - Irvine&lt;br/&gt;PI: Debra Richardson and David Redmiles&lt;br/&gt;TITLE: ITR: Quality Software by Design&lt;br/&gt;&lt;br/&gt;ABSTRACT&lt;br/&gt;Quality has always been a concern with respect to software. Yet now, with such great reliance on software in every aspect of our lives, there is even greater need to address quality in software development. High quality software means software whose specifications meet customers&apos; requirements and whose implementations meet specifications. The focus of this proposal is helping software developers design quality into their systems, which is far more cost-effective than relying solely on post-implementation quality evaluation and corrective maintenance. In particular, the proposed research encompasses a plan for combining for the first time (1) formal architecture and component design models, (2) analysis and testing techniques based on these formalisms, together with (3) cognitive-based, design environments for critiquing software design. The proposed research explores innovative user interface approaches to delivering critical design-related quality assessment information to software developers as they interactively develop designs. The information to be delivered is based on design heuristics, formal analysis and testing, and usage data and feedback. Information is to be delivered in a manner consistent with research in human cognition. Finally, to ensure that this research has the potential to impact real work, the formal architecture and component design models leverage and extend industry standards.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">83099</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">83099</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1304" target="n1305">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: System Support for Enterprise Application Servers</data>
      <data key="e_abstract">EIA-0080206&lt;br/&gt;Keleher, Peter&lt;br/&gt;University of Maryland&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: System Support for Enterprise Application Servers&lt;br/&gt;&lt;br/&gt;This proposal is for the acquisition and maintenance of a single large-scale shared-memory multiprocessor (SMP), and an active disk array. The equipment will be used to support a broad program of research into system support for enterprise applications. These applications include database servers, file servers, multimedia servers, and &quot;enterprise application&quot; servers. Though the group&apos;s projects include a broad range of specific activities, the overriding vision for the group will be a unified investigation of how to structure and support large-scale server applications. The researchers will take a vertical approach to this problem, investigating issues at every level from the application down to the network protocol level.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80206</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80206</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n149" target="n1304">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: System Support for Enterprise Application Servers</data>
      <data key="e_abstract">EIA-0080206&lt;br/&gt;Keleher, Peter&lt;br/&gt;University of Maryland&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: System Support for Enterprise Application Servers&lt;br/&gt;&lt;br/&gt;This proposal is for the acquisition and maintenance of a single large-scale shared-memory multiprocessor (SMP), and an active disk array. The equipment will be used to support a broad program of research into system support for enterprise applications. These applications include database servers, file servers, multimedia servers, and &quot;enterprise application&quot; servers. Though the group&apos;s projects include a broad range of specific activities, the overriding vision for the group will be a unified investigation of how to structure and support large-scale server applications. The researchers will take a vertical approach to this problem, investigating issues at every level from the application down to the network protocol level.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80206</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80206</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1304" target="n1307">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: System Support for Enterprise Application Servers</data>
      <data key="e_abstract">EIA-0080206&lt;br/&gt;Keleher, Peter&lt;br/&gt;University of Maryland&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: System Support for Enterprise Application Servers&lt;br/&gt;&lt;br/&gt;This proposal is for the acquisition and maintenance of a single large-scale shared-memory multiprocessor (SMP), and an active disk array. The equipment will be used to support a broad program of research into system support for enterprise applications. These applications include database servers, file servers, multimedia servers, and &quot;enterprise application&quot; servers. Though the group&apos;s projects include a broad range of specific activities, the overriding vision for the group will be a unified investigation of how to structure and support large-scale server applications. The researchers will take a vertical approach to this problem, investigating issues at every level from the application down to the network protocol level.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80206</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80206</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n737" target="n1304">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: System Support for Enterprise Application Servers</data>
      <data key="e_abstract">EIA-0080206&lt;br/&gt;Keleher, Peter&lt;br/&gt;University of Maryland&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: System Support for Enterprise Application Servers&lt;br/&gt;&lt;br/&gt;This proposal is for the acquisition and maintenance of a single large-scale shared-memory multiprocessor (SMP), and an active disk array. The equipment will be used to support a broad program of research into system support for enterprise applications. These applications include database servers, file servers, multimedia servers, and &quot;enterprise application&quot; servers. Though the group&apos;s projects include a broad range of specific activities, the overriding vision for the group will be a unified investigation of how to structure and support large-scale server applications. The researchers will take a vertical approach to this problem, investigating issues at every level from the application down to the network protocol level.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80206</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80206</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n149" target="n1305">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: System Support for Enterprise Application Servers</data>
      <data key="e_abstract">EIA-0080206&lt;br/&gt;Keleher, Peter&lt;br/&gt;University of Maryland&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: System Support for Enterprise Application Servers&lt;br/&gt;&lt;br/&gt;This proposal is for the acquisition and maintenance of a single large-scale shared-memory multiprocessor (SMP), and an active disk array. The equipment will be used to support a broad program of research into system support for enterprise applications. These applications include database servers, file servers, multimedia servers, and &quot;enterprise application&quot; servers. Though the group&apos;s projects include a broad range of specific activities, the overriding vision for the group will be a unified investigation of how to structure and support large-scale server applications. The researchers will take a vertical approach to this problem, investigating issues at every level from the application down to the network protocol level.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80206</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80206</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1305" target="n1307">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: System Support for Enterprise Application Servers</data>
      <data key="e_abstract">EIA-0080206&lt;br/&gt;Keleher, Peter&lt;br/&gt;University of Maryland&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: System Support for Enterprise Application Servers&lt;br/&gt;&lt;br/&gt;This proposal is for the acquisition and maintenance of a single large-scale shared-memory multiprocessor (SMP), and an active disk array. The equipment will be used to support a broad program of research into system support for enterprise applications. These applications include database servers, file servers, multimedia servers, and &quot;enterprise application&quot; servers. Though the group&apos;s projects include a broad range of specific activities, the overriding vision for the group will be a unified investigation of how to structure and support large-scale server applications. The researchers will take a vertical approach to this problem, investigating issues at every level from the application down to the network protocol level.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80206</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80206</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n737" target="n1305">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: System Support for Enterprise Application Servers</data>
      <data key="e_abstract">EIA-0080206&lt;br/&gt;Keleher, Peter&lt;br/&gt;University of Maryland&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: System Support for Enterprise Application Servers&lt;br/&gt;&lt;br/&gt;This proposal is for the acquisition and maintenance of a single large-scale shared-memory multiprocessor (SMP), and an active disk array. The equipment will be used to support a broad program of research into system support for enterprise applications. These applications include database servers, file servers, multimedia servers, and &quot;enterprise application&quot; servers. Though the group&apos;s projects include a broad range of specific activities, the overriding vision for the group will be a unified investigation of how to structure and support large-scale server applications. The researchers will take a vertical approach to this problem, investigating issues at every level from the application down to the network protocol level.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80206</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80206</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n149" target="n1307">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: System Support for Enterprise Application Servers</data>
      <data key="e_abstract">EIA-0080206&lt;br/&gt;Keleher, Peter&lt;br/&gt;University of Maryland&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: System Support for Enterprise Application Servers&lt;br/&gt;&lt;br/&gt;This proposal is for the acquisition and maintenance of a single large-scale shared-memory multiprocessor (SMP), and an active disk array. The equipment will be used to support a broad program of research into system support for enterprise applications. These applications include database servers, file servers, multimedia servers, and &quot;enterprise application&quot; servers. Though the group&apos;s projects include a broad range of specific activities, the overriding vision for the group will be a unified investigation of how to structure and support large-scale server applications. The researchers will take a vertical approach to this problem, investigating issues at every level from the application down to the network protocol level.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80206</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80206</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n149" target="n737">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: System Support for Enterprise Application Servers</data>
      <data key="e_abstract">EIA-0080206&lt;br/&gt;Keleher, Peter&lt;br/&gt;University of Maryland&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: System Support for Enterprise Application Servers&lt;br/&gt;&lt;br/&gt;This proposal is for the acquisition and maintenance of a single large-scale shared-memory multiprocessor (SMP), and an active disk array. The equipment will be used to support a broad program of research into system support for enterprise applications. These applications include database servers, file servers, multimedia servers, and &quot;enterprise application&quot; servers. Though the group&apos;s projects include a broad range of specific activities, the overriding vision for the group will be a unified investigation of how to structure and support large-scale server applications. The researchers will take a vertical approach to this problem, investigating issues at every level from the application down to the network protocol level.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80206</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80206</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n737" target="n1307">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: System Support for Enterprise Application Servers</data>
      <data key="e_abstract">EIA-0080206&lt;br/&gt;Keleher, Peter&lt;br/&gt;University of Maryland&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: System Support for Enterprise Application Servers&lt;br/&gt;&lt;br/&gt;This proposal is for the acquisition and maintenance of a single large-scale shared-memory multiprocessor (SMP), and an active disk array. The equipment will be used to support a broad program of research into system support for enterprise applications. These applications include database servers, file servers, multimedia servers, and &quot;enterprise application&quot; servers. Though the group&apos;s projects include a broad range of specific activities, the overriding vision for the group will be a unified investigation of how to structure and support large-scale server applications. The researchers will take a vertical approach to this problem, investigating issues at every level from the application down to the network protocol level.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80206</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80206</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1309" target="n1310">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Graduate Education for Minority Students in Computer Science and Engineering: Extending the Pipeline</data>
      <data key="e_abstract">EIA-0080940&lt;br/&gt;Novick, David G.&lt;br/&gt;University of Texas at El Paso&lt;br/&gt;&lt;br/&gt;CISE Minority Institutions Infrastructure: Graduate Education for Minority Students in Computer Science and Engineering: Extending the Pipeline&lt;br/&gt;&lt;br/&gt;The proposed project concentrates on: (i) building community among relatively isolated majority-Hispanic institutions, so that students early in the pipeline have access to role models, (ii) building a strong graduate program in a majority-Hispanic community, so that students and the program can take advantage of existing community and university support structures, and (iii) building communities of interest among the four-year colleges, an M.S. and Ph.D. granting university with the experience and capabilities to support Hispanic students, and Ph.D. granting research universities so that the students late in the pipeline have a high level of engagement with excellent research.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80940</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80940</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1309" target="n1311">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Graduate Education for Minority Students in Computer Science and Engineering: Extending the Pipeline</data>
      <data key="e_abstract">EIA-0080940&lt;br/&gt;Novick, David G.&lt;br/&gt;University of Texas at El Paso&lt;br/&gt;&lt;br/&gt;CISE Minority Institutions Infrastructure: Graduate Education for Minority Students in Computer Science and Engineering: Extending the Pipeline&lt;br/&gt;&lt;br/&gt;The proposed project concentrates on: (i) building community among relatively isolated majority-Hispanic institutions, so that students early in the pipeline have access to role models, (ii) building a strong graduate program in a majority-Hispanic community, so that students and the program can take advantage of existing community and university support structures, and (iii) building communities of interest among the four-year colleges, an M.S. and Ph.D. granting university with the experience and capabilities to support Hispanic students, and Ph.D. granting research universities so that the students late in the pipeline have a high level of engagement with excellent research.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80940</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80940</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1309" target="n1312">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Graduate Education for Minority Students in Computer Science and Engineering: Extending the Pipeline</data>
      <data key="e_abstract">EIA-0080940&lt;br/&gt;Novick, David G.&lt;br/&gt;University of Texas at El Paso&lt;br/&gt;&lt;br/&gt;CISE Minority Institutions Infrastructure: Graduate Education for Minority Students in Computer Science and Engineering: Extending the Pipeline&lt;br/&gt;&lt;br/&gt;The proposed project concentrates on: (i) building community among relatively isolated majority-Hispanic institutions, so that students early in the pipeline have access to role models, (ii) building a strong graduate program in a majority-Hispanic community, so that students and the program can take advantage of existing community and university support structures, and (iii) building communities of interest among the four-year colleges, an M.S. and Ph.D. granting university with the experience and capabilities to support Hispanic students, and Ph.D. granting research universities so that the students late in the pipeline have a high level of engagement with excellent research.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80940</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80940</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1310" target="n1311">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Graduate Education for Minority Students in Computer Science and Engineering: Extending the Pipeline</data>
      <data key="e_abstract">EIA-0080940&lt;br/&gt;Novick, David G.&lt;br/&gt;University of Texas at El Paso&lt;br/&gt;&lt;br/&gt;CISE Minority Institutions Infrastructure: Graduate Education for Minority Students in Computer Science and Engineering: Extending the Pipeline&lt;br/&gt;&lt;br/&gt;The proposed project concentrates on: (i) building community among relatively isolated majority-Hispanic institutions, so that students early in the pipeline have access to role models, (ii) building a strong graduate program in a majority-Hispanic community, so that students and the program can take advantage of existing community and university support structures, and (iii) building communities of interest among the four-year colleges, an M.S. and Ph.D. granting university with the experience and capabilities to support Hispanic students, and Ph.D. granting research universities so that the students late in the pipeline have a high level of engagement with excellent research.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80940</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80940</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1310" target="n1312">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Graduate Education for Minority Students in Computer Science and Engineering: Extending the Pipeline</data>
      <data key="e_abstract">EIA-0080940&lt;br/&gt;Novick, David G.&lt;br/&gt;University of Texas at El Paso&lt;br/&gt;&lt;br/&gt;CISE Minority Institutions Infrastructure: Graduate Education for Minority Students in Computer Science and Engineering: Extending the Pipeline&lt;br/&gt;&lt;br/&gt;The proposed project concentrates on: (i) building community among relatively isolated majority-Hispanic institutions, so that students early in the pipeline have access to role models, (ii) building a strong graduate program in a majority-Hispanic community, so that students and the program can take advantage of existing community and university support structures, and (iii) building communities of interest among the four-year colleges, an M.S. and Ph.D. granting university with the experience and capabilities to support Hispanic students, and Ph.D. granting research universities so that the students late in the pipeline have a high level of engagement with excellent research.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80940</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80940</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1311" target="n1312">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MII: Graduate Education for Minority Students in Computer Science and Engineering: Extending the Pipeline</data>
      <data key="e_abstract">EIA-0080940&lt;br/&gt;Novick, David G.&lt;br/&gt;University of Texas at El Paso&lt;br/&gt;&lt;br/&gt;CISE Minority Institutions Infrastructure: Graduate Education for Minority Students in Computer Science and Engineering: Extending the Pipeline&lt;br/&gt;&lt;br/&gt;The proposed project concentrates on: (i) building community among relatively isolated majority-Hispanic institutions, so that students early in the pipeline have access to role models, (ii) building a strong graduate program in a majority-Hispanic community, so that students and the program can take advantage of existing community and university support structures, and (iii) building communities of interest among the four-year colleges, an M.S. and Ph.D. granting university with the experience and capabilities to support Hispanic students, and Ph.D. granting research universities so that the students late in the pipeline have a high level of engagement with excellent research.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80940</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80940</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1317" target="n1318">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Social and Economic Implications of Information Technology: What Is Really Happening?</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. The world of business and organizations is entering a period of dramatic and rapid technology-based transformations that many people believe will be as significant as those that characterized the Industrial Revolution. This project will investigate the profound socioeconomic changes likely to be associated with such transformations through conducting empirical research that is broad-based, in-depth, and longitudinal. Using multiple theoretical and methodological approaches, a panel of strategically-selected firms in established as well as entrepreneurial and emergent businesses will be tracked over time. A comprehensive set of systematic and grounded empirical data in these organizations will be collected and analyzed over five years, and will generate deep insights and general theories about what is really happening as organizations use information technology to transform how they work and interact with the market over time.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85725</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85725</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1317" target="n1319">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Social and Economic Implications of Information Technology: What Is Really Happening?</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. The world of business and organizations is entering a period of dramatic and rapid technology-based transformations that many people believe will be as significant as those that characterized the Industrial Revolution. This project will investigate the profound socioeconomic changes likely to be associated with such transformations through conducting empirical research that is broad-based, in-depth, and longitudinal. Using multiple theoretical and methodological approaches, a panel of strategically-selected firms in established as well as entrepreneurial and emergent businesses will be tracked over time. A comprehensive set of systematic and grounded empirical data in these organizations will be collected and analyzed over five years, and will generate deep insights and general theories about what is really happening as organizations use information technology to transform how they work and interact with the market over time.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85725</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85725</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1317" target="n1320">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Social and Economic Implications of Information Technology: What Is Really Happening?</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. The world of business and organizations is entering a period of dramatic and rapid technology-based transformations that many people believe will be as significant as those that characterized the Industrial Revolution. This project will investigate the profound socioeconomic changes likely to be associated with such transformations through conducting empirical research that is broad-based, in-depth, and longitudinal. Using multiple theoretical and methodological approaches, a panel of strategically-selected firms in established as well as entrepreneurial and emergent businesses will be tracked over time. A comprehensive set of systematic and grounded empirical data in these organizations will be collected and analyzed over five years, and will generate deep insights and general theories about what is really happening as organizations use information technology to transform how they work and interact with the market over time.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85725</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85725</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1317" target="n1321">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Social and Economic Implications of Information Technology: What Is Really Happening?</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. The world of business and organizations is entering a period of dramatic and rapid technology-based transformations that many people believe will be as significant as those that characterized the Industrial Revolution. This project will investigate the profound socioeconomic changes likely to be associated with such transformations through conducting empirical research that is broad-based, in-depth, and longitudinal. Using multiple theoretical and methodological approaches, a panel of strategically-selected firms in established as well as entrepreneurial and emergent businesses will be tracked over time. A comprehensive set of systematic and grounded empirical data in these organizations will be collected and analyzed over five years, and will generate deep insights and general theories about what is really happening as organizations use information technology to transform how they work and interact with the market over time.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85725</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85725</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1318" target="n1319">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Social and Economic Implications of Information Technology: What Is Really Happening?</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. The world of business and organizations is entering a period of dramatic and rapid technology-based transformations that many people believe will be as significant as those that characterized the Industrial Revolution. This project will investigate the profound socioeconomic changes likely to be associated with such transformations through conducting empirical research that is broad-based, in-depth, and longitudinal. Using multiple theoretical and methodological approaches, a panel of strategically-selected firms in established as well as entrepreneurial and emergent businesses will be tracked over time. A comprehensive set of systematic and grounded empirical data in these organizations will be collected and analyzed over five years, and will generate deep insights and general theories about what is really happening as organizations use information technology to transform how they work and interact with the market over time.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85725</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85725</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1318" target="n1320">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Social and Economic Implications of Information Technology: What Is Really Happening?</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. The world of business and organizations is entering a period of dramatic and rapid technology-based transformations that many people believe will be as significant as those that characterized the Industrial Revolution. This project will investigate the profound socioeconomic changes likely to be associated with such transformations through conducting empirical research that is broad-based, in-depth, and longitudinal. Using multiple theoretical and methodological approaches, a panel of strategically-selected firms in established as well as entrepreneurial and emergent businesses will be tracked over time. A comprehensive set of systematic and grounded empirical data in these organizations will be collected and analyzed over five years, and will generate deep insights and general theories about what is really happening as organizations use information technology to transform how they work and interact with the market over time.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85725</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85725</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1318" target="n1321">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Social and Economic Implications of Information Technology: What Is Really Happening?</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. The world of business and organizations is entering a period of dramatic and rapid technology-based transformations that many people believe will be as significant as those that characterized the Industrial Revolution. This project will investigate the profound socioeconomic changes likely to be associated with such transformations through conducting empirical research that is broad-based, in-depth, and longitudinal. Using multiple theoretical and methodological approaches, a panel of strategically-selected firms in established as well as entrepreneurial and emergent businesses will be tracked over time. A comprehensive set of systematic and grounded empirical data in these organizations will be collected and analyzed over five years, and will generate deep insights and general theories about what is really happening as organizations use information technology to transform how they work and interact with the market over time.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85725</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85725</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1319" target="n1320">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Social and Economic Implications of Information Technology: What Is Really Happening?</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. The world of business and organizations is entering a period of dramatic and rapid technology-based transformations that many people believe will be as significant as those that characterized the Industrial Revolution. This project will investigate the profound socioeconomic changes likely to be associated with such transformations through conducting empirical research that is broad-based, in-depth, and longitudinal. Using multiple theoretical and methodological approaches, a panel of strategically-selected firms in established as well as entrepreneurial and emergent businesses will be tracked over time. A comprehensive set of systematic and grounded empirical data in these organizations will be collected and analyzed over five years, and will generate deep insights and general theories about what is really happening as organizations use information technology to transform how they work and interact with the market over time.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85725</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85725</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1319" target="n1321">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Social and Economic Implications of Information Technology: What Is Really Happening?</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. The world of business and organizations is entering a period of dramatic and rapid technology-based transformations that many people believe will be as significant as those that characterized the Industrial Revolution. This project will investigate the profound socioeconomic changes likely to be associated with such transformations through conducting empirical research that is broad-based, in-depth, and longitudinal. Using multiple theoretical and methodological approaches, a panel of strategically-selected firms in established as well as entrepreneurial and emergent businesses will be tracked over time. A comprehensive set of systematic and grounded empirical data in these organizations will be collected and analyzed over five years, and will generate deep insights and general theories about what is really happening as organizations use information technology to transform how they work and interact with the market over time.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85725</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85725</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1320" target="n1321">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Social and Economic Implications of Information Technology: What Is Really Happening?</data>
      <data key="e_abstract">This is the first year funding of a five-year continuing award. The world of business and organizations is entering a period of dramatic and rapid technology-based transformations that many people believe will be as significant as those that characterized the Industrial Revolution. This project will investigate the profound socioeconomic changes likely to be associated with such transformations through conducting empirical research that is broad-based, in-depth, and longitudinal. Using multiple theoretical and methodological approaches, a panel of strategically-selected firms in established as well as entrepreneurial and emergent businesses will be tracked over time. A comprehensive set of systematic and grounded empirical data in these organizations will be collected and analyzed over five years, and will generate deep insights and general theories about what is really happening as organizations use information technology to transform how they work and interact with the market over time.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85725</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85725</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n905" target="n1259">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Techniques for Dynamic Verification of Parallel Distributed Systems</data>
      <data key="e_abstract">Recently, a novel approach for runtime verification of complex superscalar processors has been proposed. In this approach, every computation is dynamically verified by hardware using a form of complete induction. This project focuses on extending the notion of inductive dynamic verification to the general case of parallel distributed systems. The principles of dynamic inductive checking for parallel semantics are investigated. This includes an understanding of the characteristics and properties of systems, where the approach can be used effectively. The initial vehicles used for developing such techniques are the problems of cache coherence and support for sequential consistency in shared-memory multiprocessors. As a natural extension of this research, the project formulates a method for state machine abstraction, which can be applied in the general case. This method rests on the separation of architected state from additional implementation state, and the identification of that subset of state transitions that correspond to changes in the architected state only.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">83126</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">83126</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1326" target="n1327">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE MII: Infrastructure for a New Program in Computer Science at the University of Puerto Rico - Río Piedras Campus</data>
      <data key="e_abstract">EIA-0080926&lt;br/&gt;Moreno, Oscar&lt;br/&gt;University of Puerto Rico-Rio Piedras&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for a New Program in Computer Science at the University of Puerto Rico - Rio Piedras Campus&lt;br/&gt;&lt;br/&gt;This project focuses on establishing a Division of Computer Science at the University of Puerto Rico-Rio Piedras (UPR-RP). Specifically, the project will involve: (i) cooperation with University of Puerto Rico-Mayaguez to offer a joint Ph.D. program in computing and information sciences and engineering, (ii) enhancement of cutting-edge computer science research at UPR-RP, (iii) increase in the number of underrepresented students completing a Ph.D. in computer science and (iv) increase in the production of cutting edge research projects in computer science. The new equipment will be used to enhance existing research activities at UPR-PR and to attract four new faculty who have a proven ability to engage in cutting edge research. The grant will support directly nine graduate students.&lt;br/&gt;&lt;br/&gt;.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80926</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80926</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1326" target="n1328">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE MII: Infrastructure for a New Program in Computer Science at the University of Puerto Rico - Río Piedras Campus</data>
      <data key="e_abstract">EIA-0080926&lt;br/&gt;Moreno, Oscar&lt;br/&gt;University of Puerto Rico-Rio Piedras&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for a New Program in Computer Science at the University of Puerto Rico - Rio Piedras Campus&lt;br/&gt;&lt;br/&gt;This project focuses on establishing a Division of Computer Science at the University of Puerto Rico-Rio Piedras (UPR-RP). Specifically, the project will involve: (i) cooperation with University of Puerto Rico-Mayaguez to offer a joint Ph.D. program in computing and information sciences and engineering, (ii) enhancement of cutting-edge computer science research at UPR-RP, (iii) increase in the number of underrepresented students completing a Ph.D. in computer science and (iv) increase in the production of cutting edge research projects in computer science. The new equipment will be used to enhance existing research activities at UPR-PR and to attract four new faculty who have a proven ability to engage in cutting edge research. The grant will support directly nine graduate students.&lt;br/&gt;&lt;br/&gt;.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80926</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80926</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1327" target="n1328">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE MII: Infrastructure for a New Program in Computer Science at the University of Puerto Rico - Río Piedras Campus</data>
      <data key="e_abstract">EIA-0080926&lt;br/&gt;Moreno, Oscar&lt;br/&gt;University of Puerto Rico-Rio Piedras&lt;br/&gt;&lt;br/&gt;MII: Infrastructure for a New Program in Computer Science at the University of Puerto Rico - Rio Piedras Campus&lt;br/&gt;&lt;br/&gt;This project focuses on establishing a Division of Computer Science at the University of Puerto Rico-Rio Piedras (UPR-RP). Specifically, the project will involve: (i) cooperation with University of Puerto Rico-Mayaguez to offer a joint Ph.D. program in computing and information sciences and engineering, (ii) enhancement of cutting-edge computer science research at UPR-RP, (iii) increase in the number of underrepresented students completing a Ph.D. in computer science and (iv) increase in the production of cutting edge research projects in computer science. The new equipment will be used to enhance existing research activities at UPR-PR and to attract four new faculty who have a proven ability to engage in cutting edge research. The grant will support directly nine graduate students.&lt;br/&gt;&lt;br/&gt;.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80926</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80926</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n435" target="n1330">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Instrumentation: Support for System-on-a-Chip and Embedded System Research</data>
      <data key="e_abstract">EIA-9911078&lt;br/&gt;Margaret Martonosi&lt;br/&gt;Princeton University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Instrumentation Support for System-On-A-Chip and Embedded System Research&lt;br/&gt;&lt;br/&gt;The Department of Electrical Engineering at Princeton University will purchase a high-end server, workstations, networking hardware, and CAD tools, which will be dedicated to support research in computer engineering. The equipment will be used for several research projects, all generally in the areas of Computer Architecture, Computer-Aided Design, and particularly focused on advancing design and architecture techniques for embedded systems and systems-on-a chip.&lt;br/&gt;&lt;br/&gt;In a fundamental paradigm shift system design in the semiconductor industry, entire systems are being built on a single chip, using multiple embedded functional blocks called cores. This has been made possible by the ever-increasing density of chips. The current 0.25-micron technology has made it possible to integrate tens of millions of transistors on one chip, and considerable interest is focused on discussing what the contents of billion-transistor systems-on- a-chip (SOCs) ought to be.&lt;br/&gt;&lt;br/&gt;We propose to develop algorithms and tools to provide key technologies with breakthrough potential to semiconductor companies,and to develop efficient software environments and tools to deal with all aspects of the SOC design problem.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.91108e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.91108e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n435" target="n1331">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Instrumentation: Support for System-on-a-Chip and Embedded System Research</data>
      <data key="e_abstract">EIA-9911078&lt;br/&gt;Margaret Martonosi&lt;br/&gt;Princeton University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Instrumentation Support for System-On-A-Chip and Embedded System Research&lt;br/&gt;&lt;br/&gt;The Department of Electrical Engineering at Princeton University will purchase a high-end server, workstations, networking hardware, and CAD tools, which will be dedicated to support research in computer engineering. The equipment will be used for several research projects, all generally in the areas of Computer Architecture, Computer-Aided Design, and particularly focused on advancing design and architecture techniques for embedded systems and systems-on-a chip.&lt;br/&gt;&lt;br/&gt;In a fundamental paradigm shift system design in the semiconductor industry, entire systems are being built on a single chip, using multiple embedded functional blocks called cores. This has been made possible by the ever-increasing density of chips. The current 0.25-micron technology has made it possible to integrate tens of millions of transistors on one chip, and considerable interest is focused on discussing what the contents of billion-transistor systems-on- a-chip (SOCs) ought to be.&lt;br/&gt;&lt;br/&gt;We propose to develop algorithms and tools to provide key technologies with breakthrough potential to semiconductor companies,and to develop efficient software environments and tools to deal with all aspects of the SOC design problem.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.91108e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.91108e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n435" target="n1332">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Instrumentation: Support for System-on-a-Chip and Embedded System Research</data>
      <data key="e_abstract">EIA-9911078&lt;br/&gt;Margaret Martonosi&lt;br/&gt;Princeton University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Instrumentation Support for System-On-A-Chip and Embedded System Research&lt;br/&gt;&lt;br/&gt;The Department of Electrical Engineering at Princeton University will purchase a high-end server, workstations, networking hardware, and CAD tools, which will be dedicated to support research in computer engineering. The equipment will be used for several research projects, all generally in the areas of Computer Architecture, Computer-Aided Design, and particularly focused on advancing design and architecture techniques for embedded systems and systems-on-a chip.&lt;br/&gt;&lt;br/&gt;In a fundamental paradigm shift system design in the semiconductor industry, entire systems are being built on a single chip, using multiple embedded functional blocks called cores. This has been made possible by the ever-increasing density of chips. The current 0.25-micron technology has made it possible to integrate tens of millions of transistors on one chip, and considerable interest is focused on discussing what the contents of billion-transistor systems-on- a-chip (SOCs) ought to be.&lt;br/&gt;&lt;br/&gt;We propose to develop algorithms and tools to provide key technologies with breakthrough potential to semiconductor companies,and to develop efficient software environments and tools to deal with all aspects of the SOC design problem.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.91108e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.91108e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1330" target="n1331">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Instrumentation: Support for System-on-a-Chip and Embedded System Research</data>
      <data key="e_abstract">EIA-9911078&lt;br/&gt;Margaret Martonosi&lt;br/&gt;Princeton University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Instrumentation Support for System-On-A-Chip and Embedded System Research&lt;br/&gt;&lt;br/&gt;The Department of Electrical Engineering at Princeton University will purchase a high-end server, workstations, networking hardware, and CAD tools, which will be dedicated to support research in computer engineering. The equipment will be used for several research projects, all generally in the areas of Computer Architecture, Computer-Aided Design, and particularly focused on advancing design and architecture techniques for embedded systems and systems-on-a chip.&lt;br/&gt;&lt;br/&gt;In a fundamental paradigm shift system design in the semiconductor industry, entire systems are being built on a single chip, using multiple embedded functional blocks called cores. This has been made possible by the ever-increasing density of chips. The current 0.25-micron technology has made it possible to integrate tens of millions of transistors on one chip, and considerable interest is focused on discussing what the contents of billion-transistor systems-on- a-chip (SOCs) ought to be.&lt;br/&gt;&lt;br/&gt;We propose to develop algorithms and tools to provide key technologies with breakthrough potential to semiconductor companies,and to develop efficient software environments and tools to deal with all aspects of the SOC design problem.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.91108e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.91108e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1330" target="n1332">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Instrumentation: Support for System-on-a-Chip and Embedded System Research</data>
      <data key="e_abstract">EIA-9911078&lt;br/&gt;Margaret Martonosi&lt;br/&gt;Princeton University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Instrumentation Support for System-On-A-Chip and Embedded System Research&lt;br/&gt;&lt;br/&gt;The Department of Electrical Engineering at Princeton University will purchase a high-end server, workstations, networking hardware, and CAD tools, which will be dedicated to support research in computer engineering. The equipment will be used for several research projects, all generally in the areas of Computer Architecture, Computer-Aided Design, and particularly focused on advancing design and architecture techniques for embedded systems and systems-on-a chip.&lt;br/&gt;&lt;br/&gt;In a fundamental paradigm shift system design in the semiconductor industry, entire systems are being built on a single chip, using multiple embedded functional blocks called cores. This has been made possible by the ever-increasing density of chips. The current 0.25-micron technology has made it possible to integrate tens of millions of transistors on one chip, and considerable interest is focused on discussing what the contents of billion-transistor systems-on- a-chip (SOCs) ought to be.&lt;br/&gt;&lt;br/&gt;We propose to develop algorithms and tools to provide key technologies with breakthrough potential to semiconductor companies,and to develop efficient software environments and tools to deal with all aspects of the SOC design problem.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.91108e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.91108e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1331" target="n1332">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Instrumentation: Support for System-on-a-Chip and Embedded System Research</data>
      <data key="e_abstract">EIA-9911078&lt;br/&gt;Margaret Martonosi&lt;br/&gt;Princeton University&lt;br/&gt;&lt;br/&gt;CISE Research Instrumentation: Instrumentation Support for System-On-A-Chip and Embedded System Research&lt;br/&gt;&lt;br/&gt;The Department of Electrical Engineering at Princeton University will purchase a high-end server, workstations, networking hardware, and CAD tools, which will be dedicated to support research in computer engineering. The equipment will be used for several research projects, all generally in the areas of Computer Architecture, Computer-Aided Design, and particularly focused on advancing design and architecture techniques for embedded systems and systems-on-a chip.&lt;br/&gt;&lt;br/&gt;In a fundamental paradigm shift system design in the semiconductor industry, entire systems are being built on a single chip, using multiple embedded functional blocks called cores. This has been made possible by the ever-increasing density of chips. The current 0.25-micron technology has made it possible to integrate tens of millions of transistors on one chip, and considerable interest is focused on discussing what the contents of billion-transistor systems-on- a-chip (SOCs) ought to be.&lt;br/&gt;&lt;br/&gt;We propose to develop algorithms and tools to provide key technologies with breakthrough potential to semiconductor companies,and to develop efficient software environments and tools to deal with all aspects of the SOC design problem.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.91108e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.91108e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n887" target="n1334">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Personal Robotic Assistants for the Elderly</data>
      <data key="e_abstract">This is the first year funding of a four-year continuing award. The proportion of elderly in the United States is growing at a phenomenal rate, yet little of today&apos;s information technology addresses the critical problems that arise as a result of this demographic shift. This project will develop advances in IT to address one such challenge, that of enabling the elderly to remain living in their homes for as long as possible. More specifically, the PI and her team will design and build a personal mobile robotic assistant for the monitoring and guidance of the daily activities of an elderly person. To achieve that vision research will be conducted to advance the state-of-the-art in several areas of IT, including: the development of advanced techniques for flexibly reasoning about plans and monitoring their execution; the development of statistical algorithms for learning models of people&apos;s daily activities; the design and evaluation of self-tailoring multi-modal interfaces that enable elderly people to interact easily with the robotic assistant; and the development of new sensor modalities to meet the needs of a mobile robot in an elder&apos;s home. The work will be evaluated along three dimensions: through theoretical and simulation-based evaluations of components, through targeted user studies, and through a series of field tests conducted at regular intervals throughout the project in the homes of elderly people. The research team is multi-university and interdisciplinary, comprising experts in the fields of computer science, robotics, human-computer interaction, and health care. The project also includes a strong education focus in that it seeks to provide a unique forum for training undergraduate and graduate students in issues related to IT for the elderly.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85796</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85796</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n887" target="n1335">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Personal Robotic Assistants for the Elderly</data>
      <data key="e_abstract">This is the first year funding of a four-year continuing award. The proportion of elderly in the United States is growing at a phenomenal rate, yet little of today&apos;s information technology addresses the critical problems that arise as a result of this demographic shift. This project will develop advances in IT to address one such challenge, that of enabling the elderly to remain living in their homes for as long as possible. More specifically, the PI and her team will design and build a personal mobile robotic assistant for the monitoring and guidance of the daily activities of an elderly person. To achieve that vision research will be conducted to advance the state-of-the-art in several areas of IT, including: the development of advanced techniques for flexibly reasoning about plans and monitoring their execution; the development of statistical algorithms for learning models of people&apos;s daily activities; the design and evaluation of self-tailoring multi-modal interfaces that enable elderly people to interact easily with the robotic assistant; and the development of new sensor modalities to meet the needs of a mobile robot in an elder&apos;s home. The work will be evaluated along three dimensions: through theoretical and simulation-based evaluations of components, through targeted user studies, and through a series of field tests conducted at regular intervals throughout the project in the homes of elderly people. The research team is multi-university and interdisciplinary, comprising experts in the fields of computer science, robotics, human-computer interaction, and health care. The project also includes a strong education focus in that it seeks to provide a unique forum for training undergraduate and graduate students in issues related to IT for the elderly.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85796</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85796</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1334" target="n1335">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Personal Robotic Assistants for the Elderly</data>
      <data key="e_abstract">This is the first year funding of a four-year continuing award. The proportion of elderly in the United States is growing at a phenomenal rate, yet little of today&apos;s information technology addresses the critical problems that arise as a result of this demographic shift. This project will develop advances in IT to address one such challenge, that of enabling the elderly to remain living in their homes for as long as possible. More specifically, the PI and her team will design and build a personal mobile robotic assistant for the monitoring and guidance of the daily activities of an elderly person. To achieve that vision research will be conducted to advance the state-of-the-art in several areas of IT, including: the development of advanced techniques for flexibly reasoning about plans and monitoring their execution; the development of statistical algorithms for learning models of people&apos;s daily activities; the design and evaluation of self-tailoring multi-modal interfaces that enable elderly people to interact easily with the robotic assistant; and the development of new sensor modalities to meet the needs of a mobile robot in an elder&apos;s home. The work will be evaluated along three dimensions: through theoretical and simulation-based evaluations of components, through targeted user studies, and through a series of field tests conducted at regular intervals throughout the project in the homes of elderly people. The research team is multi-university and interdisciplinary, comprising experts in the fields of computer science, robotics, human-computer interaction, and health care. The project also includes a strong education focus in that it seeks to provide a unique forum for training undergraduate and graduate students in issues related to IT for the elderly.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85796</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85796</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1337" target="n1338">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Scalable and Secure Information Republication</data>
      <data key="e_abstract">Our society increasingly relies on prompt, accurate delivery of information over the Internet. Users need assurance that the information they get in this way is authentic, and they need to get this assurance in a cheap and reliable way.&lt;br/&gt;&lt;br/&gt;The research centers around a new approach for engendering this confidence. The starting point is to separate the roles of the &quot;owner&quot; of a database and its &quot;publisher&quot; (or publishers). With this approach the user need not trust the publisher. Instead, the owner of the database provides the user with a small amount of &quot;summary information&quot;. After that, the publisher not only answers the user&apos;s questions, but also provides, along with each answer, a short &quot;digital certificate&quot; of accuracy. Using the summary information, the certificate lets the user check that the information received is correct and complete. Developing and evaluating good schemes to construct these certificates is the key technical challenge.&lt;br/&gt;&lt;br/&gt;The publisher need not maintain a trusted system, lowering his cost of doing business. The publisher can also more easily provide information from multiple owners. Overall, the approach should make it cheaper to obtain reliable data over the Internet, and will expand the settings where the data is used.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85961</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85961</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1337" target="n1339">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Scalable and Secure Information Republication</data>
      <data key="e_abstract">Our society increasingly relies on prompt, accurate delivery of information over the Internet. Users need assurance that the information they get in this way is authentic, and they need to get this assurance in a cheap and reliable way.&lt;br/&gt;&lt;br/&gt;The research centers around a new approach for engendering this confidence. The starting point is to separate the roles of the &quot;owner&quot; of a database and its &quot;publisher&quot; (or publishers). With this approach the user need not trust the publisher. Instead, the owner of the database provides the user with a small amount of &quot;summary information&quot;. After that, the publisher not only answers the user&apos;s questions, but also provides, along with each answer, a short &quot;digital certificate&quot; of accuracy. Using the summary information, the certificate lets the user check that the information received is correct and complete. Developing and evaluating good schemes to construct these certificates is the key technical challenge.&lt;br/&gt;&lt;br/&gt;The publisher need not maintain a trusted system, lowering his cost of doing business. The publisher can also more easily provide information from multiple owners. Overall, the approach should make it cheaper to obtain reliable data over the Internet, and will expand the settings where the data is used.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85961</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85961</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1337" target="n1340">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Scalable and Secure Information Republication</data>
      <data key="e_abstract">Our society increasingly relies on prompt, accurate delivery of information over the Internet. Users need assurance that the information they get in this way is authentic, and they need to get this assurance in a cheap and reliable way.&lt;br/&gt;&lt;br/&gt;The research centers around a new approach for engendering this confidence. The starting point is to separate the roles of the &quot;owner&quot; of a database and its &quot;publisher&quot; (or publishers). With this approach the user need not trust the publisher. Instead, the owner of the database provides the user with a small amount of &quot;summary information&quot;. After that, the publisher not only answers the user&apos;s questions, but also provides, along with each answer, a short &quot;digital certificate&quot; of accuracy. Using the summary information, the certificate lets the user check that the information received is correct and complete. Developing and evaluating good schemes to construct these certificates is the key technical challenge.&lt;br/&gt;&lt;br/&gt;The publisher need not maintain a trusted system, lowering his cost of doing business. The publisher can also more easily provide information from multiple owners. Overall, the approach should make it cheaper to obtain reliable data over the Internet, and will expand the settings where the data is used.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85961</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85961</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1338" target="n1339">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Scalable and Secure Information Republication</data>
      <data key="e_abstract">Our society increasingly relies on prompt, accurate delivery of information over the Internet. Users need assurance that the information they get in this way is authentic, and they need to get this assurance in a cheap and reliable way.&lt;br/&gt;&lt;br/&gt;The research centers around a new approach for engendering this confidence. The starting point is to separate the roles of the &quot;owner&quot; of a database and its &quot;publisher&quot; (or publishers). With this approach the user need not trust the publisher. Instead, the owner of the database provides the user with a small amount of &quot;summary information&quot;. After that, the publisher not only answers the user&apos;s questions, but also provides, along with each answer, a short &quot;digital certificate&quot; of accuracy. Using the summary information, the certificate lets the user check that the information received is correct and complete. Developing and evaluating good schemes to construct these certificates is the key technical challenge.&lt;br/&gt;&lt;br/&gt;The publisher need not maintain a trusted system, lowering his cost of doing business. The publisher can also more easily provide information from multiple owners. Overall, the approach should make it cheaper to obtain reliable data over the Internet, and will expand the settings where the data is used.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85961</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85961</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1338" target="n1340">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Scalable and Secure Information Republication</data>
      <data key="e_abstract">Our society increasingly relies on prompt, accurate delivery of information over the Internet. Users need assurance that the information they get in this way is authentic, and they need to get this assurance in a cheap and reliable way.&lt;br/&gt;&lt;br/&gt;The research centers around a new approach for engendering this confidence. The starting point is to separate the roles of the &quot;owner&quot; of a database and its &quot;publisher&quot; (or publishers). With this approach the user need not trust the publisher. Instead, the owner of the database provides the user with a small amount of &quot;summary information&quot;. After that, the publisher not only answers the user&apos;s questions, but also provides, along with each answer, a short &quot;digital certificate&quot; of accuracy. Using the summary information, the certificate lets the user check that the information received is correct and complete. Developing and evaluating good schemes to construct these certificates is the key technical challenge.&lt;br/&gt;&lt;br/&gt;The publisher need not maintain a trusted system, lowering his cost of doing business. The publisher can also more easily provide information from multiple owners. Overall, the approach should make it cheaper to obtain reliable data over the Internet, and will expand the settings where the data is used.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85961</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85961</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1339" target="n1340">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Scalable and Secure Information Republication</data>
      <data key="e_abstract">Our society increasingly relies on prompt, accurate delivery of information over the Internet. Users need assurance that the information they get in this way is authentic, and they need to get this assurance in a cheap and reliable way.&lt;br/&gt;&lt;br/&gt;The research centers around a new approach for engendering this confidence. The starting point is to separate the roles of the &quot;owner&quot; of a database and its &quot;publisher&quot; (or publishers). With this approach the user need not trust the publisher. Instead, the owner of the database provides the user with a small amount of &quot;summary information&quot;. After that, the publisher not only answers the user&apos;s questions, but also provides, along with each answer, a short &quot;digital certificate&quot; of accuracy. Using the summary information, the certificate lets the user check that the information received is correct and complete. Developing and evaluating good schemes to construct these certificates is the key technical challenge.&lt;br/&gt;&lt;br/&gt;The publisher need not maintain a trusted system, lowering his cost of doing business. The publisher can also more easily provide information from multiple owners. Overall, the approach should make it cheaper to obtain reliable data over the Internet, and will expand the settings where the data is used.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85961</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85961</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1341" target="n1342">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Adaptive Software for Field-driven Simulations</data>
      <data key="e_abstract">Successful life forms in nature adapt to changes in the environment in ways that improve the efficiency of their activities or enhance their chances for survival. In the context of computer software, opportunities for adaptation arise from changes in the computational needs of an application as it executes, changes in availability of resources, failure of hardware, etc. However, most software today does not adapt to such changes, so its efficiency and survivability may be far from optimal. This project will develop general principles for building software systems that can adapt to such changes, and will demonstrate the validity of those principles by building prototype applications.&lt;br/&gt;&lt;br/&gt;Progress in this area can only by made by focusing on a particular application domain that requires adaptive software, and assembling a team of applications researchers and computer scientists to tackle the problems in that domain at all levels. This project brings together a strong interdisciplinary team to focus on Computational Field Simulation (CFS). In particular, it will focus on computational fracture mechanics and reactive, multiphase fluid flows, both of which have an enormous number of opportunities for adaptive methods. These adaptations can be classified into three distinct categories.&lt;br/&gt;Application-level adaptivity. A variety of mathematical models may be available to describe the science of a problem, and it may be advantageous to switch adaptively between them to trade accuracy for time or other resources.&lt;br/&gt;Algorithm-level adaptivity. There are often multiple algorithms for solving a given model (e.g. direct or iterative methods for solving linear equations), and it may be advantageous to switch adaptively between them to manage resource availability or properties of the desired output.&lt;br/&gt;System-level adaptivity. The computational environment may change (e.g. more processors may become available or some communications links may fail), and the computation must adapt to these changes or risk crashing or taking significantly more time than necessary.&lt;br/&gt;This project argues that a general architecture designed to exploit these opportunities must have a set of interoperable components on a software bus, hardware/software sensors for monitoring stimuli, and control modules for orchestrating the components in response to the stimuli. It will undertake algorithmic research in each of the component areas, and will synthesize complete adaptive codes from the components using ideas similar to those in hardware synthesis. Adaptivity will be exploited at all levels in the resulting software. The project finally hopes to abstract a general theory of adaptive software systems from its experience in building such systems for field-driven simulations.</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">85969</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">85969</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n1341" target="n1343">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Adaptive Software for Field-driven Simulations</data>
      <data key="e_abstract">Successful life forms in nature adapt to changes in the environment in ways that improve the efficiency of their activities or enhance their chances for survival. In the context of computer software, opportunities for adaptation arise from changes in the computational needs of an application as it executes, changes in availability of resources, failure of hardware, etc. However, most software today does not adapt to such changes, so its efficiency and survivability may be far from optimal. This project will develop general principles for building software systems that can adapt to such changes, and will demonstrate the validity of those principles by building prototype applications.&lt;br/&gt;&lt;br/&gt;Progress in this area can only by made by focusing on a particular application domain that requires adaptive software, and assembling a team of applications researchers and computer scientists to tackle the problems in that domain at all levels. This project brings together a strong interdisciplinary team to focus on Computational Field Simulation (CFS). In particular, it will focus on computational fracture mechanics and reactive, multiphase fluid flows, both of which have an enormous number of opportunities for adaptive methods. These adaptations can be classified into three distinct categories.&lt;br/&gt;Application-level adaptivity. A variety of mathematical models may be available to describe the science of a problem, and it may be advantageous to switch adaptively between them to trade accuracy for time or other resources.&lt;br/&gt;Algorithm-level adaptivity. There are often multiple algorithms for solving a given model (e.g. direct or iterative methods for solving linear equations), and it may be advantageous to switch adaptively between them to manage resource availability or properties of the desired output.&lt;br/&gt;System-level adaptivity. The computational environment may change (e.g. more processors may become available or some communications links may fail), and the computation must adapt to these changes or risk crashing or taking significantly more time than necessary.&lt;br/&gt;This project argues that a general architecture designed to exploit these opportunities must have a set of interoperable components on a software bus, hardware/software sensors for monitoring stimuli, and control modules for orchestrating the components in response to the stimuli. It will undertake algorithmic research in each of the component areas, and will synthesize complete adaptive codes from the components using ideas similar to those in hardware synthesis. Adaptivity will be exploited at all levels in the resulting software. The project finally hopes to abstract a general theory of adaptive software systems from its experience in building such systems for field-driven simulations.</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">85969</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">85969</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n5" target="n1341">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Adaptive Software for Field-driven Simulations</data>
      <data key="e_abstract">Successful life forms in nature adapt to changes in the environment in ways that improve the efficiency of their activities or enhance their chances for survival. In the context of computer software, opportunities for adaptation arise from changes in the computational needs of an application as it executes, changes in availability of resources, failure of hardware, etc. However, most software today does not adapt to such changes, so its efficiency and survivability may be far from optimal. This project will develop general principles for building software systems that can adapt to such changes, and will demonstrate the validity of those principles by building prototype applications.&lt;br/&gt;&lt;br/&gt;Progress in this area can only by made by focusing on a particular application domain that requires adaptive software, and assembling a team of applications researchers and computer scientists to tackle the problems in that domain at all levels. This project brings together a strong interdisciplinary team to focus on Computational Field Simulation (CFS). In particular, it will focus on computational fracture mechanics and reactive, multiphase fluid flows, both of which have an enormous number of opportunities for adaptive methods. These adaptations can be classified into three distinct categories.&lt;br/&gt;Application-level adaptivity. A variety of mathematical models may be available to describe the science of a problem, and it may be advantageous to switch adaptively between them to trade accuracy for time or other resources.&lt;br/&gt;Algorithm-level adaptivity. There are often multiple algorithms for solving a given model (e.g. direct or iterative methods for solving linear equations), and it may be advantageous to switch adaptively between them to manage resource availability or properties of the desired output.&lt;br/&gt;System-level adaptivity. The computational environment may change (e.g. more processors may become available or some communications links may fail), and the computation must adapt to these changes or risk crashing or taking significantly more time than necessary.&lt;br/&gt;This project argues that a general architecture designed to exploit these opportunities must have a set of interoperable components on a software bus, hardware/software sensors for monitoring stimuli, and control modules for orchestrating the components in response to the stimuli. It will undertake algorithmic research in each of the component areas, and will synthesize complete adaptive codes from the components using ideas similar to those in hardware synthesis. Adaptivity will be exploited at all levels in the resulting software. The project finally hopes to abstract a general theory of adaptive software systems from its experience in building such systems for field-driven simulations.</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">85969</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">85969</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n1341" target="n1345">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Adaptive Software for Field-driven Simulations</data>
      <data key="e_abstract">Successful life forms in nature adapt to changes in the environment in ways that improve the efficiency of their activities or enhance their chances for survival. In the context of computer software, opportunities for adaptation arise from changes in the computational needs of an application as it executes, changes in availability of resources, failure of hardware, etc. However, most software today does not adapt to such changes, so its efficiency and survivability may be far from optimal. This project will develop general principles for building software systems that can adapt to such changes, and will demonstrate the validity of those principles by building prototype applications.&lt;br/&gt;&lt;br/&gt;Progress in this area can only by made by focusing on a particular application domain that requires adaptive software, and assembling a team of applications researchers and computer scientists to tackle the problems in that domain at all levels. This project brings together a strong interdisciplinary team to focus on Computational Field Simulation (CFS). In particular, it will focus on computational fracture mechanics and reactive, multiphase fluid flows, both of which have an enormous number of opportunities for adaptive methods. These adaptations can be classified into three distinct categories.&lt;br/&gt;Application-level adaptivity. A variety of mathematical models may be available to describe the science of a problem, and it may be advantageous to switch adaptively between them to trade accuracy for time or other resources.&lt;br/&gt;Algorithm-level adaptivity. There are often multiple algorithms for solving a given model (e.g. direct or iterative methods for solving linear equations), and it may be advantageous to switch adaptively between them to manage resource availability or properties of the desired output.&lt;br/&gt;System-level adaptivity. The computational environment may change (e.g. more processors may become available or some communications links may fail), and the computation must adapt to these changes or risk crashing or taking significantly more time than necessary.&lt;br/&gt;This project argues that a general architecture designed to exploit these opportunities must have a set of interoperable components on a software bus, hardware/software sensors for monitoring stimuli, and control modules for orchestrating the components in response to the stimuli. It will undertake algorithmic research in each of the component areas, and will synthesize complete adaptive codes from the components using ideas similar to those in hardware synthesis. Adaptivity will be exploited at all levels in the resulting software. The project finally hopes to abstract a general theory of adaptive software systems from its experience in building such systems for field-driven simulations.</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">85969</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">85969</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n1342" target="n1343">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Adaptive Software for Field-driven Simulations</data>
      <data key="e_abstract">Successful life forms in nature adapt to changes in the environment in ways that improve the efficiency of their activities or enhance their chances for survival. In the context of computer software, opportunities for adaptation arise from changes in the computational needs of an application as it executes, changes in availability of resources, failure of hardware, etc. However, most software today does not adapt to such changes, so its efficiency and survivability may be far from optimal. This project will develop general principles for building software systems that can adapt to such changes, and will demonstrate the validity of those principles by building prototype applications.&lt;br/&gt;&lt;br/&gt;Progress in this area can only by made by focusing on a particular application domain that requires adaptive software, and assembling a team of applications researchers and computer scientists to tackle the problems in that domain at all levels. This project brings together a strong interdisciplinary team to focus on Computational Field Simulation (CFS). In particular, it will focus on computational fracture mechanics and reactive, multiphase fluid flows, both of which have an enormous number of opportunities for adaptive methods. These adaptations can be classified into three distinct categories.&lt;br/&gt;Application-level adaptivity. A variety of mathematical models may be available to describe the science of a problem, and it may be advantageous to switch adaptively between them to trade accuracy for time or other resources.&lt;br/&gt;Algorithm-level adaptivity. There are often multiple algorithms for solving a given model (e.g. direct or iterative methods for solving linear equations), and it may be advantageous to switch adaptively between them to manage resource availability or properties of the desired output.&lt;br/&gt;System-level adaptivity. The computational environment may change (e.g. more processors may become available or some communications links may fail), and the computation must adapt to these changes or risk crashing or taking significantly more time than necessary.&lt;br/&gt;This project argues that a general architecture designed to exploit these opportunities must have a set of interoperable components on a software bus, hardware/software sensors for monitoring stimuli, and control modules for orchestrating the components in response to the stimuli. It will undertake algorithmic research in each of the component areas, and will synthesize complete adaptive codes from the components using ideas similar to those in hardware synthesis. Adaptivity will be exploited at all levels in the resulting software. The project finally hopes to abstract a general theory of adaptive software systems from its experience in building such systems for field-driven simulations.</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">85969</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">85969</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n5" target="n1342">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Adaptive Software for Field-driven Simulations</data>
      <data key="e_abstract">Successful life forms in nature adapt to changes in the environment in ways that improve the efficiency of their activities or enhance their chances for survival. In the context of computer software, opportunities for adaptation arise from changes in the computational needs of an application as it executes, changes in availability of resources, failure of hardware, etc. However, most software today does not adapt to such changes, so its efficiency and survivability may be far from optimal. This project will develop general principles for building software systems that can adapt to such changes, and will demonstrate the validity of those principles by building prototype applications.&lt;br/&gt;&lt;br/&gt;Progress in this area can only by made by focusing on a particular application domain that requires adaptive software, and assembling a team of applications researchers and computer scientists to tackle the problems in that domain at all levels. This project brings together a strong interdisciplinary team to focus on Computational Field Simulation (CFS). In particular, it will focus on computational fracture mechanics and reactive, multiphase fluid flows, both of which have an enormous number of opportunities for adaptive methods. These adaptations can be classified into three distinct categories.&lt;br/&gt;Application-level adaptivity. A variety of mathematical models may be available to describe the science of a problem, and it may be advantageous to switch adaptively between them to trade accuracy for time or other resources.&lt;br/&gt;Algorithm-level adaptivity. There are often multiple algorithms for solving a given model (e.g. direct or iterative methods for solving linear equations), and it may be advantageous to switch adaptively between them to manage resource availability or properties of the desired output.&lt;br/&gt;System-level adaptivity. The computational environment may change (e.g. more processors may become available or some communications links may fail), and the computation must adapt to these changes or risk crashing or taking significantly more time than necessary.&lt;br/&gt;This project argues that a general architecture designed to exploit these opportunities must have a set of interoperable components on a software bus, hardware/software sensors for monitoring stimuli, and control modules for orchestrating the components in response to the stimuli. It will undertake algorithmic research in each of the component areas, and will synthesize complete adaptive codes from the components using ideas similar to those in hardware synthesis. Adaptivity will be exploited at all levels in the resulting software. The project finally hopes to abstract a general theory of adaptive software systems from its experience in building such systems for field-driven simulations.</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">85969</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">85969</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n1342" target="n1345">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Adaptive Software for Field-driven Simulations</data>
      <data key="e_abstract">Successful life forms in nature adapt to changes in the environment in ways that improve the efficiency of their activities or enhance their chances for survival. In the context of computer software, opportunities for adaptation arise from changes in the computational needs of an application as it executes, changes in availability of resources, failure of hardware, etc. However, most software today does not adapt to such changes, so its efficiency and survivability may be far from optimal. This project will develop general principles for building software systems that can adapt to such changes, and will demonstrate the validity of those principles by building prototype applications.&lt;br/&gt;&lt;br/&gt;Progress in this area can only by made by focusing on a particular application domain that requires adaptive software, and assembling a team of applications researchers and computer scientists to tackle the problems in that domain at all levels. This project brings together a strong interdisciplinary team to focus on Computational Field Simulation (CFS). In particular, it will focus on computational fracture mechanics and reactive, multiphase fluid flows, both of which have an enormous number of opportunities for adaptive methods. These adaptations can be classified into three distinct categories.&lt;br/&gt;Application-level adaptivity. A variety of mathematical models may be available to describe the science of a problem, and it may be advantageous to switch adaptively between them to trade accuracy for time or other resources.&lt;br/&gt;Algorithm-level adaptivity. There are often multiple algorithms for solving a given model (e.g. direct or iterative methods for solving linear equations), and it may be advantageous to switch adaptively between them to manage resource availability or properties of the desired output.&lt;br/&gt;System-level adaptivity. The computational environment may change (e.g. more processors may become available or some communications links may fail), and the computation must adapt to these changes or risk crashing or taking significantly more time than necessary.&lt;br/&gt;This project argues that a general architecture designed to exploit these opportunities must have a set of interoperable components on a software bus, hardware/software sensors for monitoring stimuli, and control modules for orchestrating the components in response to the stimuli. It will undertake algorithmic research in each of the component areas, and will synthesize complete adaptive codes from the components using ideas similar to those in hardware synthesis. Adaptivity will be exploited at all levels in the resulting software. The project finally hopes to abstract a general theory of adaptive software systems from its experience in building such systems for field-driven simulations.</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">85969</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">85969</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n5" target="n1343">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Adaptive Software for Field-driven Simulations</data>
      <data key="e_abstract">Successful life forms in nature adapt to changes in the environment in ways that improve the efficiency of their activities or enhance their chances for survival. In the context of computer software, opportunities for adaptation arise from changes in the computational needs of an application as it executes, changes in availability of resources, failure of hardware, etc. However, most software today does not adapt to such changes, so its efficiency and survivability may be far from optimal. This project will develop general principles for building software systems that can adapt to such changes, and will demonstrate the validity of those principles by building prototype applications.&lt;br/&gt;&lt;br/&gt;Progress in this area can only by made by focusing on a particular application domain that requires adaptive software, and assembling a team of applications researchers and computer scientists to tackle the problems in that domain at all levels. This project brings together a strong interdisciplinary team to focus on Computational Field Simulation (CFS). In particular, it will focus on computational fracture mechanics and reactive, multiphase fluid flows, both of which have an enormous number of opportunities for adaptive methods. These adaptations can be classified into three distinct categories.&lt;br/&gt;Application-level adaptivity. A variety of mathematical models may be available to describe the science of a problem, and it may be advantageous to switch adaptively between them to trade accuracy for time or other resources.&lt;br/&gt;Algorithm-level adaptivity. There are often multiple algorithms for solving a given model (e.g. direct or iterative methods for solving linear equations), and it may be advantageous to switch adaptively between them to manage resource availability or properties of the desired output.&lt;br/&gt;System-level adaptivity. The computational environment may change (e.g. more processors may become available or some communications links may fail), and the computation must adapt to these changes or risk crashing or taking significantly more time than necessary.&lt;br/&gt;This project argues that a general architecture designed to exploit these opportunities must have a set of interoperable components on a software bus, hardware/software sensors for monitoring stimuli, and control modules for orchestrating the components in response to the stimuli. It will undertake algorithmic research in each of the component areas, and will synthesize complete adaptive codes from the components using ideas similar to those in hardware synthesis. Adaptivity will be exploited at all levels in the resulting software. The project finally hopes to abstract a general theory of adaptive software systems from its experience in building such systems for field-driven simulations.</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">85969</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">85969</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n1343" target="n1345">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Adaptive Software for Field-driven Simulations</data>
      <data key="e_abstract">Successful life forms in nature adapt to changes in the environment in ways that improve the efficiency of their activities or enhance their chances for survival. In the context of computer software, opportunities for adaptation arise from changes in the computational needs of an application as it executes, changes in availability of resources, failure of hardware, etc. However, most software today does not adapt to such changes, so its efficiency and survivability may be far from optimal. This project will develop general principles for building software systems that can adapt to such changes, and will demonstrate the validity of those principles by building prototype applications.&lt;br/&gt;&lt;br/&gt;Progress in this area can only by made by focusing on a particular application domain that requires adaptive software, and assembling a team of applications researchers and computer scientists to tackle the problems in that domain at all levels. This project brings together a strong interdisciplinary team to focus on Computational Field Simulation (CFS). In particular, it will focus on computational fracture mechanics and reactive, multiphase fluid flows, both of which have an enormous number of opportunities for adaptive methods. These adaptations can be classified into three distinct categories.&lt;br/&gt;Application-level adaptivity. A variety of mathematical models may be available to describe the science of a problem, and it may be advantageous to switch adaptively between them to trade accuracy for time or other resources.&lt;br/&gt;Algorithm-level adaptivity. There are often multiple algorithms for solving a given model (e.g. direct or iterative methods for solving linear equations), and it may be advantageous to switch adaptively between them to manage resource availability or properties of the desired output.&lt;br/&gt;System-level adaptivity. The computational environment may change (e.g. more processors may become available or some communications links may fail), and the computation must adapt to these changes or risk crashing or taking significantly more time than necessary.&lt;br/&gt;This project argues that a general architecture designed to exploit these opportunities must have a set of interoperable components on a software bus, hardware/software sensors for monitoring stimuli, and control modules for orchestrating the components in response to the stimuli. It will undertake algorithmic research in each of the component areas, and will synthesize complete adaptive codes from the components using ideas similar to those in hardware synthesis. Adaptivity will be exploited at all levels in the resulting software. The project finally hopes to abstract a general theory of adaptive software systems from its experience in building such systems for field-driven simulations.</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">85969</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">85969</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n5" target="n1345">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Adaptive Software for Field-driven Simulations</data>
      <data key="e_abstract">Successful life forms in nature adapt to changes in the environment in ways that improve the efficiency of their activities or enhance their chances for survival. In the context of computer software, opportunities for adaptation arise from changes in the computational needs of an application as it executes, changes in availability of resources, failure of hardware, etc. However, most software today does not adapt to such changes, so its efficiency and survivability may be far from optimal. This project will develop general principles for building software systems that can adapt to such changes, and will demonstrate the validity of those principles by building prototype applications.&lt;br/&gt;&lt;br/&gt;Progress in this area can only by made by focusing on a particular application domain that requires adaptive software, and assembling a team of applications researchers and computer scientists to tackle the problems in that domain at all levels. This project brings together a strong interdisciplinary team to focus on Computational Field Simulation (CFS). In particular, it will focus on computational fracture mechanics and reactive, multiphase fluid flows, both of which have an enormous number of opportunities for adaptive methods. These adaptations can be classified into three distinct categories.&lt;br/&gt;Application-level adaptivity. A variety of mathematical models may be available to describe the science of a problem, and it may be advantageous to switch adaptively between them to trade accuracy for time or other resources.&lt;br/&gt;Algorithm-level adaptivity. There are often multiple algorithms for solving a given model (e.g. direct or iterative methods for solving linear equations), and it may be advantageous to switch adaptively between them to manage resource availability or properties of the desired output.&lt;br/&gt;System-level adaptivity. The computational environment may change (e.g. more processors may become available or some communications links may fail), and the computation must adapt to these changes or risk crashing or taking significantly more time than necessary.&lt;br/&gt;This project argues that a general architecture designed to exploit these opportunities must have a set of interoperable components on a software bus, hardware/software sensors for monitoring stimuli, and control modules for orchestrating the components in response to the stimuli. It will undertake algorithmic research in each of the component areas, and will synthesize complete adaptive codes from the components using ideas similar to those in hardware synthesis. Adaptivity will be exploited at all levels in the resulting software. The project finally hopes to abstract a general theory of adaptive software systems from its experience in building such systems for field-driven simulations.</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">85969</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">85969</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n863" target="n1347">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Educational Innovation: A Tool-Supported Programming Languages Curriculum</data>
      <data key="e_abstract">EIA-0086255&lt;br/&gt;Waite, William M.&lt;br/&gt;University of Colorado at Boulder &lt;br/&gt;&lt;br/&gt;CISE Educational Innovation: A Tool-Supported Programming Languages Curriculum&lt;br/&gt;&lt;br/&gt;This project integrates research and development in software tools development into the undergraduate computer science program at this institution with special emphasis on computer-supported cooperative work and decision support. The project strategy involves &quot;scaffolding&quot; content-related problem solving skills and process-related group interaction skills by integrating a suite of tools, developed by the research community, into a three-course sequence required of all computer science majors. The key idea is that students are involved in realistic projects, beginning with their introductory courses, but an infrastructure selectively hides complexity from students. As students progress to more advanced courses, more of the complexity is revealed, until finally, students are involved in realistic projects involving significant collaborative work. The project has two components: (i) a software infrastructure that manages the complexity that students are exposed to, and (ii) a collection of tools that allow students to undertake larger projects than would normally be possible in a classroom setting. Students also use a web-based journal tool to keep track of significant milestones, problems, and other issues related to tasks assigned to them. The backbone of the collaborative system used in this project consists of a repository that contains code and implementation monitors for student projects, tools that students use when interacting with the materials in the repository, and a set of policies that control how students can interact with the repository contents. This project addresses the national need of educating software practitioners who can work on large, complex software systems in teams.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">86255</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86255</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n863" target="n1348">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Educational Innovation: A Tool-Supported Programming Languages Curriculum</data>
      <data key="e_abstract">EIA-0086255&lt;br/&gt;Waite, William M.&lt;br/&gt;University of Colorado at Boulder &lt;br/&gt;&lt;br/&gt;CISE Educational Innovation: A Tool-Supported Programming Languages Curriculum&lt;br/&gt;&lt;br/&gt;This project integrates research and development in software tools development into the undergraduate computer science program at this institution with special emphasis on computer-supported cooperative work and decision support. The project strategy involves &quot;scaffolding&quot; content-related problem solving skills and process-related group interaction skills by integrating a suite of tools, developed by the research community, into a three-course sequence required of all computer science majors. The key idea is that students are involved in realistic projects, beginning with their introductory courses, but an infrastructure selectively hides complexity from students. As students progress to more advanced courses, more of the complexity is revealed, until finally, students are involved in realistic projects involving significant collaborative work. The project has two components: (i) a software infrastructure that manages the complexity that students are exposed to, and (ii) a collection of tools that allow students to undertake larger projects than would normally be possible in a classroom setting. Students also use a web-based journal tool to keep track of significant milestones, problems, and other issues related to tasks assigned to them. The backbone of the collaborative system used in this project consists of a repository that contains code and implementation monitors for student projects, tools that students use when interacting with the materials in the repository, and a set of policies that control how students can interact with the repository contents. This project addresses the national need of educating software practitioners who can work on large, complex software systems in teams.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">86255</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86255</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1347" target="n1348">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Educational Innovation: A Tool-Supported Programming Languages Curriculum</data>
      <data key="e_abstract">EIA-0086255&lt;br/&gt;Waite, William M.&lt;br/&gt;University of Colorado at Boulder &lt;br/&gt;&lt;br/&gt;CISE Educational Innovation: A Tool-Supported Programming Languages Curriculum&lt;br/&gt;&lt;br/&gt;This project integrates research and development in software tools development into the undergraduate computer science program at this institution with special emphasis on computer-supported cooperative work and decision support. The project strategy involves &quot;scaffolding&quot; content-related problem solving skills and process-related group interaction skills by integrating a suite of tools, developed by the research community, into a three-course sequence required of all computer science majors. The key idea is that students are involved in realistic projects, beginning with their introductory courses, but an infrastructure selectively hides complexity from students. As students progress to more advanced courses, more of the complexity is revealed, until finally, students are involved in realistic projects involving significant collaborative work. The project has two components: (i) a software infrastructure that manages the complexity that students are exposed to, and (ii) a collection of tools that allow students to undertake larger projects than would normally be possible in a classroom setting. Students also use a web-based journal tool to keep track of significant milestones, problems, and other issues related to tasks assigned to them. The backbone of the collaborative system used in this project consists of a repository that contains code and implementation monitors for student projects, tools that students use when interacting with the materials in the repository, and a set of policies that control how students can interact with the repository contents. This project addresses the national need of educating software practitioners who can work on large, complex software systems in teams.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">86255</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86255</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n564" target="n1349">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Educational Innovation: &quot;Collaborative Research:&quot; Integrating Logic in the Computer Science Curriculum</data>
      <data key="e_abstract">Institution: William Marsh Rice University&lt;br/&gt;Proposal Number: EIA 0086264&lt;br/&gt;PI: Moshe Y. Vardi&lt;br/&gt;Title: Collaborative Research: Integrating Logic in the Computer Science Curriculum&lt;br/&gt;&lt;br/&gt;This CISE Educational Innovation (EI) proposal requests funds to develop a series of modules that seamlessly integrate logic and logic-based software tools into existing, widely taught computer science (CS) courses. Few undergraduate computer science curricula prepare students adequately in logic. The typical student sees a few weeks of truth tables and propositional logic in discrete practical work. These modules would allow CS departments to easily modify their curricula to rectify this situation. By supplying modules, complete with lecture notes, presentations, problem sets, and tools, the investigators hope to facilitate curricular treatment of applied logic at all levels of college education, particularly in CS departments with scarce resources. This project has the potential to have a major impact on the way that CS is taught.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">86264</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86264</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1349" target="n1351">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Educational Innovation: &quot;Collaborative Research:&quot; Integrating Logic in the Computer Science Curriculum</data>
      <data key="e_abstract">Institution: William Marsh Rice University&lt;br/&gt;Proposal Number: EIA 0086264&lt;br/&gt;PI: Moshe Y. Vardi&lt;br/&gt;Title: Collaborative Research: Integrating Logic in the Computer Science Curriculum&lt;br/&gt;&lt;br/&gt;This CISE Educational Innovation (EI) proposal requests funds to develop a series of modules that seamlessly integrate logic and logic-based software tools into existing, widely taught computer science (CS) courses. Few undergraduate computer science curricula prepare students adequately in logic. The typical student sees a few weeks of truth tables and propositional logic in discrete practical work. These modules would allow CS departments to easily modify their curricula to rectify this situation. By supplying modules, complete with lecture notes, presentations, problem sets, and tools, the investigators hope to facilitate curricular treatment of applied logic at all levels of college education, particularly in CS departments with scarce resources. This project has the potential to have a major impact on the way that CS is taught.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">86264</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86264</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n564" target="n1351">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Educational Innovation: &quot;Collaborative Research:&quot; Integrating Logic in the Computer Science Curriculum</data>
      <data key="e_abstract">Institution: William Marsh Rice University&lt;br/&gt;Proposal Number: EIA 0086264&lt;br/&gt;PI: Moshe Y. Vardi&lt;br/&gt;Title: Collaborative Research: Integrating Logic in the Computer Science Curriculum&lt;br/&gt;&lt;br/&gt;This CISE Educational Innovation (EI) proposal requests funds to develop a series of modules that seamlessly integrate logic and logic-based software tools into existing, widely taught computer science (CS) courses. Few undergraduate computer science curricula prepare students adequately in logic. The typical student sees a few weeks of truth tables and propositional logic in discrete practical work. These modules would allow CS departments to easily modify their curricula to rectify this situation. By supplying modules, complete with lecture notes, presentations, problem sets, and tools, the investigators hope to facilitate curricular treatment of applied logic at all levels of college education, particularly in CS departments with scarce resources. This project has the potential to have a major impact on the way that CS is taught.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">86264</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86264</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1354" target="n1355">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Exploiting Style as Retrieval and Classification Mechanism</data>
      <data key="e_abstract">Vast musical databases are currently accessible over computer networks (e.g., the Web), creating a need for sophisticated methods to search and organize these databases. Because music is a multifaceted, multi-dimensional medium, it demands specialized representations, abstractions and processing techniques for effective search that are fundamentally different from those used for other retrieval tasks. By exploiting reductionist theories of musical structure and performance (i.e., musical style), this project will develop hierarchical, stochastic music representations and concomitant storage and retrieval mechanisms that are well-suited to music&apos;s unique characteristics, and are both musically and psycho-acoustically plausible. A software system exploiting these representations and retrieval mechanisms will be developed that accepts sonic input, compares abstractions of this input to those in a database of digital recordings, returns sonic samples of the database that best match the query, and allows the user to refine the query using music/acoustic-based interfaces of varying degrees of complexity. This research will yield a working music-search engine will application to e-commerce and will provide new scientific knowledge in terms of algorithms and mathematic models in the fields of databases, information retrieval, artificial intelligence, signal processing and perception, for music search and retrieval, which will generalize to other multimedia processing tasks.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85945</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85945</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1354" target="n1356">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Exploiting Style as Retrieval and Classification Mechanism</data>
      <data key="e_abstract">Vast musical databases are currently accessible over computer networks (e.g., the Web), creating a need for sophisticated methods to search and organize these databases. Because music is a multifaceted, multi-dimensional medium, it demands specialized representations, abstractions and processing techniques for effective search that are fundamentally different from those used for other retrieval tasks. By exploiting reductionist theories of musical structure and performance (i.e., musical style), this project will develop hierarchical, stochastic music representations and concomitant storage and retrieval mechanisms that are well-suited to music&apos;s unique characteristics, and are both musically and psycho-acoustically plausible. A software system exploiting these representations and retrieval mechanisms will be developed that accepts sonic input, compares abstractions of this input to those in a database of digital recordings, returns sonic samples of the database that best match the query, and allows the user to refine the query using music/acoustic-based interfaces of varying degrees of complexity. This research will yield a working music-search engine will application to e-commerce and will provide new scientific knowledge in terms of algorithms and mathematic models in the fields of databases, information retrieval, artificial intelligence, signal processing and perception, for music search and retrieval, which will generalize to other multimedia processing tasks.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85945</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85945</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1354" target="n1357">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Exploiting Style as Retrieval and Classification Mechanism</data>
      <data key="e_abstract">Vast musical databases are currently accessible over computer networks (e.g., the Web), creating a need for sophisticated methods to search and organize these databases. Because music is a multifaceted, multi-dimensional medium, it demands specialized representations, abstractions and processing techniques for effective search that are fundamentally different from those used for other retrieval tasks. By exploiting reductionist theories of musical structure and performance (i.e., musical style), this project will develop hierarchical, stochastic music representations and concomitant storage and retrieval mechanisms that are well-suited to music&apos;s unique characteristics, and are both musically and psycho-acoustically plausible. A software system exploiting these representations and retrieval mechanisms will be developed that accepts sonic input, compares abstractions of this input to those in a database of digital recordings, returns sonic samples of the database that best match the query, and allows the user to refine the query using music/acoustic-based interfaces of varying degrees of complexity. This research will yield a working music-search engine will application to e-commerce and will provide new scientific knowledge in terms of algorithms and mathematic models in the fields of databases, information retrieval, artificial intelligence, signal processing and perception, for music search and retrieval, which will generalize to other multimedia processing tasks.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85945</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85945</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1355" target="n1356">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Exploiting Style as Retrieval and Classification Mechanism</data>
      <data key="e_abstract">Vast musical databases are currently accessible over computer networks (e.g., the Web), creating a need for sophisticated methods to search and organize these databases. Because music is a multifaceted, multi-dimensional medium, it demands specialized representations, abstractions and processing techniques for effective search that are fundamentally different from those used for other retrieval tasks. By exploiting reductionist theories of musical structure and performance (i.e., musical style), this project will develop hierarchical, stochastic music representations and concomitant storage and retrieval mechanisms that are well-suited to music&apos;s unique characteristics, and are both musically and psycho-acoustically plausible. A software system exploiting these representations and retrieval mechanisms will be developed that accepts sonic input, compares abstractions of this input to those in a database of digital recordings, returns sonic samples of the database that best match the query, and allows the user to refine the query using music/acoustic-based interfaces of varying degrees of complexity. This research will yield a working music-search engine will application to e-commerce and will provide new scientific knowledge in terms of algorithms and mathematic models in the fields of databases, information retrieval, artificial intelligence, signal processing and perception, for music search and retrieval, which will generalize to other multimedia processing tasks.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85945</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85945</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1355" target="n1357">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Exploiting Style as Retrieval and Classification Mechanism</data>
      <data key="e_abstract">Vast musical databases are currently accessible over computer networks (e.g., the Web), creating a need for sophisticated methods to search and organize these databases. Because music is a multifaceted, multi-dimensional medium, it demands specialized representations, abstractions and processing techniques for effective search that are fundamentally different from those used for other retrieval tasks. By exploiting reductionist theories of musical structure and performance (i.e., musical style), this project will develop hierarchical, stochastic music representations and concomitant storage and retrieval mechanisms that are well-suited to music&apos;s unique characteristics, and are both musically and psycho-acoustically plausible. A software system exploiting these representations and retrieval mechanisms will be developed that accepts sonic input, compares abstractions of this input to those in a database of digital recordings, returns sonic samples of the database that best match the query, and allows the user to refine the query using music/acoustic-based interfaces of varying degrees of complexity. This research will yield a working music-search engine will application to e-commerce and will provide new scientific knowledge in terms of algorithms and mathematic models in the fields of databases, information retrieval, artificial intelligence, signal processing and perception, for music search and retrieval, which will generalize to other multimedia processing tasks.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85945</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85945</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1356" target="n1357">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Exploiting Style as Retrieval and Classification Mechanism</data>
      <data key="e_abstract">Vast musical databases are currently accessible over computer networks (e.g., the Web), creating a need for sophisticated methods to search and organize these databases. Because music is a multifaceted, multi-dimensional medium, it demands specialized representations, abstractions and processing techniques for effective search that are fundamentally different from those used for other retrieval tasks. By exploiting reductionist theories of musical structure and performance (i.e., musical style), this project will develop hierarchical, stochastic music representations and concomitant storage and retrieval mechanisms that are well-suited to music&apos;s unique characteristics, and are both musically and psycho-acoustically plausible. A software system exploiting these representations and retrieval mechanisms will be developed that accepts sonic input, compares abstractions of this input to those in a database of digital recordings, returns sonic samples of the database that best match the query, and allows the user to refine the query using music/acoustic-based interfaces of varying degrees of complexity. This research will yield a working music-search engine will application to e-commerce and will provide new scientific knowledge in terms of algorithms and mathematic models in the fields of databases, information retrieval, artificial intelligence, signal processing and perception, for music search and retrieval, which will generalize to other multimedia processing tasks.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85945</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85945</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1358" target="n1359">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Learning-Centered Design Methodology: Meeting the Nation&apos;s Need for Computational Tools for K-12 Science Education (Engineering Scaffolded Work Environments)</data>
      <data key="e_abstract">EIA-0085946&lt;br/&gt;Soloway, Elliot&lt;br/&gt;University of Michigan&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Learning-Centered Design Methodolodgy: Meeting the Nation&apos;s Need for Computational Tools for K-12 Science Education (Engineering Scaffolded Work Environments)&lt;br/&gt;&lt;br/&gt;Learning while working in the knowledge-work professions can be supported&lt;br/&gt;by appropriately-designed &quot;scaffolded work environments (SWEets).&quot; Several&lt;br/&gt;researchers have assembled a unique team of 10 senior researchers from four&lt;br/&gt;major universities to develop a principled engineering process for&lt;br/&gt;constructing SWEets. The work will be based in the knowledge work context&lt;br/&gt;of &quot;science inquiry&quot; using an engineering process to construct SWEets for&lt;br/&gt;fourth through eleventh graders learning science via scientific&lt;br/&gt;investigations in Detroit and Chicago classrooms. The results of the&lt;br/&gt;proposed research will be explicit guidelines on how to build effective,&lt;br/&gt;computationally-based work environments that scaffold and support&lt;br/&gt;individuals engaged in knowledge work. The aim of this effort is to&lt;br/&gt;transform what presently appears to be the art into a principled, software&lt;br/&gt;engineering approach.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">85946</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85946</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1358" target="n1360">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Learning-Centered Design Methodology: Meeting the Nation&apos;s Need for Computational Tools for K-12 Science Education (Engineering Scaffolded Work Environments)</data>
      <data key="e_abstract">EIA-0085946&lt;br/&gt;Soloway, Elliot&lt;br/&gt;University of Michigan&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Learning-Centered Design Methodolodgy: Meeting the Nation&apos;s Need for Computational Tools for K-12 Science Education (Engineering Scaffolded Work Environments)&lt;br/&gt;&lt;br/&gt;Learning while working in the knowledge-work professions can be supported&lt;br/&gt;by appropriately-designed &quot;scaffolded work environments (SWEets).&quot; Several&lt;br/&gt;researchers have assembled a unique team of 10 senior researchers from four&lt;br/&gt;major universities to develop a principled engineering process for&lt;br/&gt;constructing SWEets. The work will be based in the knowledge work context&lt;br/&gt;of &quot;science inquiry&quot; using an engineering process to construct SWEets for&lt;br/&gt;fourth through eleventh graders learning science via scientific&lt;br/&gt;investigations in Detroit and Chicago classrooms. The results of the&lt;br/&gt;proposed research will be explicit guidelines on how to build effective,&lt;br/&gt;computationally-based work environments that scaffold and support&lt;br/&gt;individuals engaged in knowledge work. The aim of this effort is to&lt;br/&gt;transform what presently appears to be the art into a principled, software&lt;br/&gt;engineering approach.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">85946</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85946</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n822" target="n1358">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Learning-Centered Design Methodology: Meeting the Nation&apos;s Need for Computational Tools for K-12 Science Education (Engineering Scaffolded Work Environments)</data>
      <data key="e_abstract">EIA-0085946&lt;br/&gt;Soloway, Elliot&lt;br/&gt;University of Michigan&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Learning-Centered Design Methodolodgy: Meeting the Nation&apos;s Need for Computational Tools for K-12 Science Education (Engineering Scaffolded Work Environments)&lt;br/&gt;&lt;br/&gt;Learning while working in the knowledge-work professions can be supported&lt;br/&gt;by appropriately-designed &quot;scaffolded work environments (SWEets).&quot; Several&lt;br/&gt;researchers have assembled a unique team of 10 senior researchers from four&lt;br/&gt;major universities to develop a principled engineering process for&lt;br/&gt;constructing SWEets. The work will be based in the knowledge work context&lt;br/&gt;of &quot;science inquiry&quot; using an engineering process to construct SWEets for&lt;br/&gt;fourth through eleventh graders learning science via scientific&lt;br/&gt;investigations in Detroit and Chicago classrooms. The results of the&lt;br/&gt;proposed research will be explicit guidelines on how to build effective,&lt;br/&gt;computationally-based work environments that scaffold and support&lt;br/&gt;individuals engaged in knowledge work. The aim of this effort is to&lt;br/&gt;transform what presently appears to be the art into a principled, software&lt;br/&gt;engineering approach.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">85946</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85946</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1358" target="n1362">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Learning-Centered Design Methodology: Meeting the Nation&apos;s Need for Computational Tools for K-12 Science Education (Engineering Scaffolded Work Environments)</data>
      <data key="e_abstract">EIA-0085946&lt;br/&gt;Soloway, Elliot&lt;br/&gt;University of Michigan&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Learning-Centered Design Methodolodgy: Meeting the Nation&apos;s Need for Computational Tools for K-12 Science Education (Engineering Scaffolded Work Environments)&lt;br/&gt;&lt;br/&gt;Learning while working in the knowledge-work professions can be supported&lt;br/&gt;by appropriately-designed &quot;scaffolded work environments (SWEets).&quot; Several&lt;br/&gt;researchers have assembled a unique team of 10 senior researchers from four&lt;br/&gt;major universities to develop a principled engineering process for&lt;br/&gt;constructing SWEets. The work will be based in the knowledge work context&lt;br/&gt;of &quot;science inquiry&quot; using an engineering process to construct SWEets for&lt;br/&gt;fourth through eleventh graders learning science via scientific&lt;br/&gt;investigations in Detroit and Chicago classrooms. The results of the&lt;br/&gt;proposed research will be explicit guidelines on how to build effective,&lt;br/&gt;computationally-based work environments that scaffold and support&lt;br/&gt;individuals engaged in knowledge work. The aim of this effort is to&lt;br/&gt;transform what presently appears to be the art into a principled, software&lt;br/&gt;engineering approach.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">85946</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85946</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1359" target="n1360">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Learning-Centered Design Methodology: Meeting the Nation&apos;s Need for Computational Tools for K-12 Science Education (Engineering Scaffolded Work Environments)</data>
      <data key="e_abstract">EIA-0085946&lt;br/&gt;Soloway, Elliot&lt;br/&gt;University of Michigan&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Learning-Centered Design Methodolodgy: Meeting the Nation&apos;s Need for Computational Tools for K-12 Science Education (Engineering Scaffolded Work Environments)&lt;br/&gt;&lt;br/&gt;Learning while working in the knowledge-work professions can be supported&lt;br/&gt;by appropriately-designed &quot;scaffolded work environments (SWEets).&quot; Several&lt;br/&gt;researchers have assembled a unique team of 10 senior researchers from four&lt;br/&gt;major universities to develop a principled engineering process for&lt;br/&gt;constructing SWEets. The work will be based in the knowledge work context&lt;br/&gt;of &quot;science inquiry&quot; using an engineering process to construct SWEets for&lt;br/&gt;fourth through eleventh graders learning science via scientific&lt;br/&gt;investigations in Detroit and Chicago classrooms. The results of the&lt;br/&gt;proposed research will be explicit guidelines on how to build effective,&lt;br/&gt;computationally-based work environments that scaffold and support&lt;br/&gt;individuals engaged in knowledge work. The aim of this effort is to&lt;br/&gt;transform what presently appears to be the art into a principled, software&lt;br/&gt;engineering approach.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">85946</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85946</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n822" target="n1359">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Learning-Centered Design Methodology: Meeting the Nation&apos;s Need for Computational Tools for K-12 Science Education (Engineering Scaffolded Work Environments)</data>
      <data key="e_abstract">EIA-0085946&lt;br/&gt;Soloway, Elliot&lt;br/&gt;University of Michigan&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Learning-Centered Design Methodolodgy: Meeting the Nation&apos;s Need for Computational Tools for K-12 Science Education (Engineering Scaffolded Work Environments)&lt;br/&gt;&lt;br/&gt;Learning while working in the knowledge-work professions can be supported&lt;br/&gt;by appropriately-designed &quot;scaffolded work environments (SWEets).&quot; Several&lt;br/&gt;researchers have assembled a unique team of 10 senior researchers from four&lt;br/&gt;major universities to develop a principled engineering process for&lt;br/&gt;constructing SWEets. The work will be based in the knowledge work context&lt;br/&gt;of &quot;science inquiry&quot; using an engineering process to construct SWEets for&lt;br/&gt;fourth through eleventh graders learning science via scientific&lt;br/&gt;investigations in Detroit and Chicago classrooms. The results of the&lt;br/&gt;proposed research will be explicit guidelines on how to build effective,&lt;br/&gt;computationally-based work environments that scaffold and support&lt;br/&gt;individuals engaged in knowledge work. The aim of this effort is to&lt;br/&gt;transform what presently appears to be the art into a principled, software&lt;br/&gt;engineering approach.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">85946</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85946</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1359" target="n1362">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Learning-Centered Design Methodology: Meeting the Nation&apos;s Need for Computational Tools for K-12 Science Education (Engineering Scaffolded Work Environments)</data>
      <data key="e_abstract">EIA-0085946&lt;br/&gt;Soloway, Elliot&lt;br/&gt;University of Michigan&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Learning-Centered Design Methodolodgy: Meeting the Nation&apos;s Need for Computational Tools for K-12 Science Education (Engineering Scaffolded Work Environments)&lt;br/&gt;&lt;br/&gt;Learning while working in the knowledge-work professions can be supported&lt;br/&gt;by appropriately-designed &quot;scaffolded work environments (SWEets).&quot; Several&lt;br/&gt;researchers have assembled a unique team of 10 senior researchers from four&lt;br/&gt;major universities to develop a principled engineering process for&lt;br/&gt;constructing SWEets. The work will be based in the knowledge work context&lt;br/&gt;of &quot;science inquiry&quot; using an engineering process to construct SWEets for&lt;br/&gt;fourth through eleventh graders learning science via scientific&lt;br/&gt;investigations in Detroit and Chicago classrooms. The results of the&lt;br/&gt;proposed research will be explicit guidelines on how to build effective,&lt;br/&gt;computationally-based work environments that scaffold and support&lt;br/&gt;individuals engaged in knowledge work. The aim of this effort is to&lt;br/&gt;transform what presently appears to be the art into a principled, software&lt;br/&gt;engineering approach.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">85946</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85946</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n822" target="n1360">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Learning-Centered Design Methodology: Meeting the Nation&apos;s Need for Computational Tools for K-12 Science Education (Engineering Scaffolded Work Environments)</data>
      <data key="e_abstract">EIA-0085946&lt;br/&gt;Soloway, Elliot&lt;br/&gt;University of Michigan&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Learning-Centered Design Methodolodgy: Meeting the Nation&apos;s Need for Computational Tools for K-12 Science Education (Engineering Scaffolded Work Environments)&lt;br/&gt;&lt;br/&gt;Learning while working in the knowledge-work professions can be supported&lt;br/&gt;by appropriately-designed &quot;scaffolded work environments (SWEets).&quot; Several&lt;br/&gt;researchers have assembled a unique team of 10 senior researchers from four&lt;br/&gt;major universities to develop a principled engineering process for&lt;br/&gt;constructing SWEets. The work will be based in the knowledge work context&lt;br/&gt;of &quot;science inquiry&quot; using an engineering process to construct SWEets for&lt;br/&gt;fourth through eleventh graders learning science via scientific&lt;br/&gt;investigations in Detroit and Chicago classrooms. The results of the&lt;br/&gt;proposed research will be explicit guidelines on how to build effective,&lt;br/&gt;computationally-based work environments that scaffold and support&lt;br/&gt;individuals engaged in knowledge work. The aim of this effort is to&lt;br/&gt;transform what presently appears to be the art into a principled, software&lt;br/&gt;engineering approach.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">85946</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85946</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1360" target="n1362">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Learning-Centered Design Methodology: Meeting the Nation&apos;s Need for Computational Tools for K-12 Science Education (Engineering Scaffolded Work Environments)</data>
      <data key="e_abstract">EIA-0085946&lt;br/&gt;Soloway, Elliot&lt;br/&gt;University of Michigan&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Learning-Centered Design Methodolodgy: Meeting the Nation&apos;s Need for Computational Tools for K-12 Science Education (Engineering Scaffolded Work Environments)&lt;br/&gt;&lt;br/&gt;Learning while working in the knowledge-work professions can be supported&lt;br/&gt;by appropriately-designed &quot;scaffolded work environments (SWEets).&quot; Several&lt;br/&gt;researchers have assembled a unique team of 10 senior researchers from four&lt;br/&gt;major universities to develop a principled engineering process for&lt;br/&gt;constructing SWEets. The work will be based in the knowledge work context&lt;br/&gt;of &quot;science inquiry&quot; using an engineering process to construct SWEets for&lt;br/&gt;fourth through eleventh graders learning science via scientific&lt;br/&gt;investigations in Detroit and Chicago classrooms. The results of the&lt;br/&gt;proposed research will be explicit guidelines on how to build effective,&lt;br/&gt;computationally-based work environments that scaffold and support&lt;br/&gt;individuals engaged in knowledge work. The aim of this effort is to&lt;br/&gt;transform what presently appears to be the art into a principled, software&lt;br/&gt;engineering approach.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">85946</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85946</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n822" target="n1362">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Learning-Centered Design Methodology: Meeting the Nation&apos;s Need for Computational Tools for K-12 Science Education (Engineering Scaffolded Work Environments)</data>
      <data key="e_abstract">EIA-0085946&lt;br/&gt;Soloway, Elliot&lt;br/&gt;University of Michigan&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Learning-Centered Design Methodolodgy: Meeting the Nation&apos;s Need for Computational Tools for K-12 Science Education (Engineering Scaffolded Work Environments)&lt;br/&gt;&lt;br/&gt;Learning while working in the knowledge-work professions can be supported&lt;br/&gt;by appropriately-designed &quot;scaffolded work environments (SWEets).&quot; Several&lt;br/&gt;researchers have assembled a unique team of 10 senior researchers from four&lt;br/&gt;major universities to develop a principled engineering process for&lt;br/&gt;constructing SWEets. The work will be based in the knowledge work context&lt;br/&gt;of &quot;science inquiry&quot; using an engineering process to construct SWEets for&lt;br/&gt;fourth through eleventh graders learning science via scientific&lt;br/&gt;investigations in Detroit and Chicago classrooms. The results of the&lt;br/&gt;proposed research will be explicit guidelines on how to build effective,&lt;br/&gt;computationally-based work environments that scaffold and support&lt;br/&gt;individuals engaged in knowledge work. The aim of this effort is to&lt;br/&gt;transform what presently appears to be the art into a principled, software&lt;br/&gt;engineering approach.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">85946</data>
      <data key="e_expirationDate">2005-06-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85946</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1205" target="n1363">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: The Open Source Quality Project</data>
      <data key="e_abstract">This project investigates techniques and tools for assuring software&lt;br/&gt;quality: finding and removing defects in software systems, as well as&lt;br/&gt;improving current methodology for designing high-quality software&lt;br/&gt;systems at the outset. The project consists of both experimental&lt;br/&gt;and theoretical components.&lt;br/&gt;&lt;br/&gt;The experimental effort is focused on designing and building tools to&lt;br/&gt;improve the quality of Open Source software. Open Source is&lt;br/&gt;attractive as a research vehicle in software quality because of the&lt;br/&gt;critical role it plays in the nation&apos;s economy and precisely because&lt;br/&gt;it has the unique feature that it is a real-world system that is&lt;br/&gt;completely open and available for study. Because of the Open Source&lt;br/&gt;tradition of incorporating useful new techniques and tools into the&lt;br/&gt;Open Source environment, there is also an opportunity for direct and&lt;br/&gt;widespread impact.&lt;br/&gt;&lt;br/&gt;The foundational work in this project combines expertise in the three&lt;br/&gt;branches of the discipline of the analysis of software: formal&lt;br/&gt;verification and theorem proving, model checking, and large-scale&lt;br/&gt;software analysis. These three areas have developed rapidly in recent years, seeing both significant theoretical and practical advances. A central thesis of this project is that significant further advances are possible by bringing together these areas to work on a common set of problems.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85949</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85949</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1188" target="n1363">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: The Open Source Quality Project</data>
      <data key="e_abstract">This project investigates techniques and tools for assuring software&lt;br/&gt;quality: finding and removing defects in software systems, as well as&lt;br/&gt;improving current methodology for designing high-quality software&lt;br/&gt;systems at the outset. The project consists of both experimental&lt;br/&gt;and theoretical components.&lt;br/&gt;&lt;br/&gt;The experimental effort is focused on designing and building tools to&lt;br/&gt;improve the quality of Open Source software. Open Source is&lt;br/&gt;attractive as a research vehicle in software quality because of the&lt;br/&gt;critical role it plays in the nation&apos;s economy and precisely because&lt;br/&gt;it has the unique feature that it is a real-world system that is&lt;br/&gt;completely open and available for study. Because of the Open Source&lt;br/&gt;tradition of incorporating useful new techniques and tools into the&lt;br/&gt;Open Source environment, there is also an opportunity for direct and&lt;br/&gt;widespread impact.&lt;br/&gt;&lt;br/&gt;The foundational work in this project combines expertise in the three&lt;br/&gt;branches of the discipline of the analysis of software: formal&lt;br/&gt;verification and theorem proving, model checking, and large-scale&lt;br/&gt;software analysis. These three areas have developed rapidly in recent years, seeing both significant theoretical and practical advances. A central thesis of this project is that significant further advances are possible by bringing together these areas to work on a common set of problems.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85949</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85949</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1188" target="n1205">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: The Open Source Quality Project</data>
      <data key="e_abstract">This project investigates techniques and tools for assuring software&lt;br/&gt;quality: finding and removing defects in software systems, as well as&lt;br/&gt;improving current methodology for designing high-quality software&lt;br/&gt;systems at the outset. The project consists of both experimental&lt;br/&gt;and theoretical components.&lt;br/&gt;&lt;br/&gt;The experimental effort is focused on designing and building tools to&lt;br/&gt;improve the quality of Open Source software. Open Source is&lt;br/&gt;attractive as a research vehicle in software quality because of the&lt;br/&gt;critical role it plays in the nation&apos;s economy and precisely because&lt;br/&gt;it has the unique feature that it is a real-world system that is&lt;br/&gt;completely open and available for study. Because of the Open Source&lt;br/&gt;tradition of incorporating useful new techniques and tools into the&lt;br/&gt;Open Source environment, there is also an opportunity for direct and&lt;br/&gt;widespread impact.&lt;br/&gt;&lt;br/&gt;The foundational work in this project combines expertise in the three&lt;br/&gt;branches of the discipline of the analysis of software: formal&lt;br/&gt;verification and theorem proving, model checking, and large-scale&lt;br/&gt;software analysis. These three areas have developed rapidly in recent years, seeing both significant theoretical and practical advances. A central thesis of this project is that significant further advances are possible by bringing together these areas to work on a common set of problems.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85949</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85949</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1366" target="n1367">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Sustainable and Generalizable Technologies to Support Collaboration in Science</data>
      <data key="e_abstract">Collaboratories are new organizational forms to help scientists who are geographically dispersed to work closely together. A number of collaboratories have been built, and their successes and failures have been due to a combination of technical and social factors not yet fully synthesized. This project will define, abstract, and codify the underlying technical and social mechanisms that lead to successful collaboratories. It will provide the vocabulary, associated principles, and design methods for propagating and sustaining collaboratories across a wide range of circumstances. This project will enhance both the practice of science and the training of new scientists. These goals will be achieved through three coordinated activities: (1) The qualitative and quantitative study of collaboratory design and usage, examining both technical and social aspects of performance; (2) creation and maintenance of a Collaboratory Knowledge Base, a Web-accessible archive of primary source material, summaries and abstracts, and relevant generalizations and principles, a database of collaboratory resources, and other related material; and (3) the abstraction and codification of principles, heuristics, and frameworks to guide the rapid creation and deployment of successful collaboratories, including principles of design or customization.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85951</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85951</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1366" target="n1368">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Sustainable and Generalizable Technologies to Support Collaboration in Science</data>
      <data key="e_abstract">Collaboratories are new organizational forms to help scientists who are geographically dispersed to work closely together. A number of collaboratories have been built, and their successes and failures have been due to a combination of technical and social factors not yet fully synthesized. This project will define, abstract, and codify the underlying technical and social mechanisms that lead to successful collaboratories. It will provide the vocabulary, associated principles, and design methods for propagating and sustaining collaboratories across a wide range of circumstances. This project will enhance both the practice of science and the training of new scientists. These goals will be achieved through three coordinated activities: (1) The qualitative and quantitative study of collaboratory design and usage, examining both technical and social aspects of performance; (2) creation and maintenance of a Collaboratory Knowledge Base, a Web-accessible archive of primary source material, summaries and abstracts, and relevant generalizations and principles, a database of collaboratory resources, and other related material; and (3) the abstraction and codification of principles, heuristics, and frameworks to guide the rapid creation and deployment of successful collaboratories, including principles of design or customization.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85951</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85951</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1366" target="n1369">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Sustainable and Generalizable Technologies to Support Collaboration in Science</data>
      <data key="e_abstract">Collaboratories are new organizational forms to help scientists who are geographically dispersed to work closely together. A number of collaboratories have been built, and their successes and failures have been due to a combination of technical and social factors not yet fully synthesized. This project will define, abstract, and codify the underlying technical and social mechanisms that lead to successful collaboratories. It will provide the vocabulary, associated principles, and design methods for propagating and sustaining collaboratories across a wide range of circumstances. This project will enhance both the practice of science and the training of new scientists. These goals will be achieved through three coordinated activities: (1) The qualitative and quantitative study of collaboratory design and usage, examining both technical and social aspects of performance; (2) creation and maintenance of a Collaboratory Knowledge Base, a Web-accessible archive of primary source material, summaries and abstracts, and relevant generalizations and principles, a database of collaboratory resources, and other related material; and (3) the abstraction and codification of principles, heuristics, and frameworks to guide the rapid creation and deployment of successful collaboratories, including principles of design or customization.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85951</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85951</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1366" target="n1370">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Sustainable and Generalizable Technologies to Support Collaboration in Science</data>
      <data key="e_abstract">Collaboratories are new organizational forms to help scientists who are geographically dispersed to work closely together. A number of collaboratories have been built, and their successes and failures have been due to a combination of technical and social factors not yet fully synthesized. This project will define, abstract, and codify the underlying technical and social mechanisms that lead to successful collaboratories. It will provide the vocabulary, associated principles, and design methods for propagating and sustaining collaboratories across a wide range of circumstances. This project will enhance both the practice of science and the training of new scientists. These goals will be achieved through three coordinated activities: (1) The qualitative and quantitative study of collaboratory design and usage, examining both technical and social aspects of performance; (2) creation and maintenance of a Collaboratory Knowledge Base, a Web-accessible archive of primary source material, summaries and abstracts, and relevant generalizations and principles, a database of collaboratory resources, and other related material; and (3) the abstraction and codification of principles, heuristics, and frameworks to guide the rapid creation and deployment of successful collaboratories, including principles of design or customization.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85951</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85951</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1367" target="n1368">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Sustainable and Generalizable Technologies to Support Collaboration in Science</data>
      <data key="e_abstract">Collaboratories are new organizational forms to help scientists who are geographically dispersed to work closely together. A number of collaboratories have been built, and their successes and failures have been due to a combination of technical and social factors not yet fully synthesized. This project will define, abstract, and codify the underlying technical and social mechanisms that lead to successful collaboratories. It will provide the vocabulary, associated principles, and design methods for propagating and sustaining collaboratories across a wide range of circumstances. This project will enhance both the practice of science and the training of new scientists. These goals will be achieved through three coordinated activities: (1) The qualitative and quantitative study of collaboratory design and usage, examining both technical and social aspects of performance; (2) creation and maintenance of a Collaboratory Knowledge Base, a Web-accessible archive of primary source material, summaries and abstracts, and relevant generalizations and principles, a database of collaboratory resources, and other related material; and (3) the abstraction and codification of principles, heuristics, and frameworks to guide the rapid creation and deployment of successful collaboratories, including principles of design or customization.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85951</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85951</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1367" target="n1369">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Sustainable and Generalizable Technologies to Support Collaboration in Science</data>
      <data key="e_abstract">Collaboratories are new organizational forms to help scientists who are geographically dispersed to work closely together. A number of collaboratories have been built, and their successes and failures have been due to a combination of technical and social factors not yet fully synthesized. This project will define, abstract, and codify the underlying technical and social mechanisms that lead to successful collaboratories. It will provide the vocabulary, associated principles, and design methods for propagating and sustaining collaboratories across a wide range of circumstances. This project will enhance both the practice of science and the training of new scientists. These goals will be achieved through three coordinated activities: (1) The qualitative and quantitative study of collaboratory design and usage, examining both technical and social aspects of performance; (2) creation and maintenance of a Collaboratory Knowledge Base, a Web-accessible archive of primary source material, summaries and abstracts, and relevant generalizations and principles, a database of collaboratory resources, and other related material; and (3) the abstraction and codification of principles, heuristics, and frameworks to guide the rapid creation and deployment of successful collaboratories, including principles of design or customization.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85951</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85951</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1367" target="n1370">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Sustainable and Generalizable Technologies to Support Collaboration in Science</data>
      <data key="e_abstract">Collaboratories are new organizational forms to help scientists who are geographically dispersed to work closely together. A number of collaboratories have been built, and their successes and failures have been due to a combination of technical and social factors not yet fully synthesized. This project will define, abstract, and codify the underlying technical and social mechanisms that lead to successful collaboratories. It will provide the vocabulary, associated principles, and design methods for propagating and sustaining collaboratories across a wide range of circumstances. This project will enhance both the practice of science and the training of new scientists. These goals will be achieved through three coordinated activities: (1) The qualitative and quantitative study of collaboratory design and usage, examining both technical and social aspects of performance; (2) creation and maintenance of a Collaboratory Knowledge Base, a Web-accessible archive of primary source material, summaries and abstracts, and relevant generalizations and principles, a database of collaboratory resources, and other related material; and (3) the abstraction and codification of principles, heuristics, and frameworks to guide the rapid creation and deployment of successful collaboratories, including principles of design or customization.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85951</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85951</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1368" target="n1369">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Sustainable and Generalizable Technologies to Support Collaboration in Science</data>
      <data key="e_abstract">Collaboratories are new organizational forms to help scientists who are geographically dispersed to work closely together. A number of collaboratories have been built, and their successes and failures have been due to a combination of technical and social factors not yet fully synthesized. This project will define, abstract, and codify the underlying technical and social mechanisms that lead to successful collaboratories. It will provide the vocabulary, associated principles, and design methods for propagating and sustaining collaboratories across a wide range of circumstances. This project will enhance both the practice of science and the training of new scientists. These goals will be achieved through three coordinated activities: (1) The qualitative and quantitative study of collaboratory design and usage, examining both technical and social aspects of performance; (2) creation and maintenance of a Collaboratory Knowledge Base, a Web-accessible archive of primary source material, summaries and abstracts, and relevant generalizations and principles, a database of collaboratory resources, and other related material; and (3) the abstraction and codification of principles, heuristics, and frameworks to guide the rapid creation and deployment of successful collaboratories, including principles of design or customization.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85951</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85951</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1368" target="n1370">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Sustainable and Generalizable Technologies to Support Collaboration in Science</data>
      <data key="e_abstract">Collaboratories are new organizational forms to help scientists who are geographically dispersed to work closely together. A number of collaboratories have been built, and their successes and failures have been due to a combination of technical and social factors not yet fully synthesized. This project will define, abstract, and codify the underlying technical and social mechanisms that lead to successful collaboratories. It will provide the vocabulary, associated principles, and design methods for propagating and sustaining collaboratories across a wide range of circumstances. This project will enhance both the practice of science and the training of new scientists. These goals will be achieved through three coordinated activities: (1) The qualitative and quantitative study of collaboratory design and usage, examining both technical and social aspects of performance; (2) creation and maintenance of a Collaboratory Knowledge Base, a Web-accessible archive of primary source material, summaries and abstracts, and relevant generalizations and principles, a database of collaboratory resources, and other related material; and (3) the abstraction and codification of principles, heuristics, and frameworks to guide the rapid creation and deployment of successful collaboratories, including principles of design or customization.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85951</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85951</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1369" target="n1370">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Sustainable and Generalizable Technologies to Support Collaboration in Science</data>
      <data key="e_abstract">Collaboratories are new organizational forms to help scientists who are geographically dispersed to work closely together. A number of collaboratories have been built, and their successes and failures have been due to a combination of technical and social factors not yet fully synthesized. This project will define, abstract, and codify the underlying technical and social mechanisms that lead to successful collaboratories. It will provide the vocabulary, associated principles, and design methods for propagating and sustaining collaboratories across a wide range of circumstances. This project will enhance both the practice of science and the training of new scientists. These goals will be achieved through three coordinated activities: (1) The qualitative and quantitative study of collaboratory design and usage, examining both technical and social aspects of performance; (2) creation and maintenance of a Collaboratory Knowledge Base, a Web-accessible archive of primary source material, summaries and abstracts, and relevant generalizations and principles, a database of collaboratory resources, and other related material; and (3) the abstraction and codification of principles, heuristics, and frameworks to guide the rapid creation and deployment of successful collaboratories, including principles of design or customization.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85951</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85951</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1371" target="n1372">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research: Scalable Services for the Global Network</data>
      <data key="e_abstract">The emergence of new applications has fostered a number of attempts to add functionalities to the minimalist Internet core. However, the adoption of enhancements to the Internet has either been slow, failed entirely, or limited to special-purpose private networks. The key reasons for this failure are extensibility and scalability: First, IP networks were not designed to be extensible at the internetworking level. Second, proposals for new network layer services often require that vast amounts of state information be managed in the core network infrastructure, thus, introducing scalability bottlenecks which exacerbate the existing scalability problem of the growing Internet.&lt;br/&gt; Today, the development and deployment of advanced services on the Internet has reached a crossroads: efforts to add new services have quickly encountered scalability problems, yet new services are in critical demand and must be rapidly and widely deployed.&lt;br/&gt; The research goal is to develop truly scalable services for each of the three fundamental components of the Internet&apos;s infrastructure: information communication, replication, and storage. Taking a new and unified approach to the seemingly conflicting requirements for scalability and sophisticated network services, the researchers propose to develop:&lt;br/&gt; 1. Scalable Performance-Predictable Communication: a new foundation for quality-of-service communication via a scalable edge-based architecture.&lt;br/&gt; 2. Scalable Multicast for Efficient Data Dissemination: a self-organizing multicast infrastructure scalable to many spontaneously-formed groups.&lt;br/&gt; 3. Scalable Storage for Next Generation Information Services: an infrastructure which brings information&lt;br/&gt;closer to users and enables scalable third-party information storage services.&lt;br/&gt; 4. Design Principles of Scalable Services: a multi-faceted approach for the development and deployment&lt;br/&gt;of scalable services in the global Internet, under consideration of economic models, industrial structure,&lt;br/&gt;theories and algorithms, engineering, and deployment.&lt;br/&gt; Thus, this project proposes to develop architectures and methodologies for deploying scalable services in the global Internet. The impact of this project will be to provide the theoretical underpinnings, basic architecture, and a prototype implementation for the information communication of the global Internet of the 21st century. An integral part of this project is the dissemination of results and the infiltration of standard organizations with the concepts developed within this project, and innovative approaches to educate the next generation of engineers for the future Internet.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85955</data>
      <data key="e_expirationDate">2007-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85955</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1374" target="n1375">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Opportunistic Parallel Computation</data>
      <data key="e_abstract">This project proposes runtime and compiler support that will enable programs to harvest idle SMPs and/or single processor workstations to perform parallel computations. The unique feature of this system is the ability to adapt parallel programs to the dynamic availability of processors while exploiting the locality within an SMP. The project integrates several goals, namely:&lt;br/&gt;&lt;br/&gt;1) Extend the Distributed Shared Memory, Strings, to support thread migration, incorporate adaptation to the changing number of available processors at runtime and propose techniques to balance data locality and the parallelism used when the number of processors changes at runtime. &lt;br/&gt; &lt;br/&gt;2) Study the impact of eviction time on remapping strategies and constraints. &lt;br/&gt;&lt;br/&gt;3) Develop compile-time support for parallel programs which can be executed in an environment where the number and the availability of the processors can change.&lt;br/&gt;&lt;br/&gt;4) Develop analytic models and extensively evaluate the above compiler and runtime techniques using several &quot;real programs&quot;.&lt;br/&gt;&lt;br/&gt;5) Integrate the utilization of idle cycles for parallel computing on cluster of SMP workstations into the existing parallelization environment.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81696</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">81696</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1374" target="n1376">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Opportunistic Parallel Computation</data>
      <data key="e_abstract">This project proposes runtime and compiler support that will enable programs to harvest idle SMPs and/or single processor workstations to perform parallel computations. The unique feature of this system is the ability to adapt parallel programs to the dynamic availability of processors while exploiting the locality within an SMP. The project integrates several goals, namely:&lt;br/&gt;&lt;br/&gt;1) Extend the Distributed Shared Memory, Strings, to support thread migration, incorporate adaptation to the changing number of available processors at runtime and propose techniques to balance data locality and the parallelism used when the number of processors changes at runtime. &lt;br/&gt; &lt;br/&gt;2) Study the impact of eviction time on remapping strategies and constraints. &lt;br/&gt;&lt;br/&gt;3) Develop compile-time support for parallel programs which can be executed in an environment where the number and the availability of the processors can change.&lt;br/&gt;&lt;br/&gt;4) Develop analytic models and extensively evaluate the above compiler and runtime techniques using several &quot;real programs&quot;.&lt;br/&gt;&lt;br/&gt;5) Integrate the utilization of idle cycles for parallel computing on cluster of SMP workstations into the existing parallelization environment.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81696</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">81696</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1375" target="n1376">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Opportunistic Parallel Computation</data>
      <data key="e_abstract">This project proposes runtime and compiler support that will enable programs to harvest idle SMPs and/or single processor workstations to perform parallel computations. The unique feature of this system is the ability to adapt parallel programs to the dynamic availability of processors while exploiting the locality within an SMP. The project integrates several goals, namely:&lt;br/&gt;&lt;br/&gt;1) Extend the Distributed Shared Memory, Strings, to support thread migration, incorporate adaptation to the changing number of available processors at runtime and propose techniques to balance data locality and the parallelism used when the number of processors changes at runtime. &lt;br/&gt; &lt;br/&gt;2) Study the impact of eviction time on remapping strategies and constraints. &lt;br/&gt;&lt;br/&gt;3) Develop compile-time support for parallel programs which can be executed in an environment where the number and the availability of the processors can change.&lt;br/&gt;&lt;br/&gt;4) Develop analytic models and extensively evaluate the above compiler and runtime techniques using several &quot;real programs&quot;.&lt;br/&gt;&lt;br/&gt;5) Integrate the utilization of idle cycles for parallel computing on cluster of SMP workstations into the existing parallelization environment.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81696</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">81696</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1377" target="n1378">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Analysis of Internet Algorithms: Optimization, Game Theory and Competitive Analysis</data>
      <data key="e_abstract">As the complexity of the Internet, the nature of its applications, and its socioeconomic framework evolve, new algorithmic and architectural ideas will be proposed, tested, and adopted. While the original Internet design principles will likely remain valid, the researchers believe that it is important to have in place a mathematical framework within which these design principles can be expressed and applied to the next generation of Internet algorithms and architectures. Building such a framework is the ultimate goal. The mathematical tools will come from optimization, game theory and competitive analysis. The researchers shall work on the following topics.&lt;br/&gt;&lt;br/&gt;Multicast. The researchers shall seek to determine the relative efficiency, in terms of link usage, of multicast versus unicast, devise and analyze efficient methods of multicast error recovery, and determine how efficiently multicast can be simulated in the application layer by a coordinated set of unicasts.&lt;br/&gt;&lt;br/&gt;Congestion Probing. The TCP congestion control protocol controls its window size with an additive-increase and multiplicative-decrease (AIMD) algorithm. One can think of this as a probing algorithm in which the flow attempts to discover the maximum rate of traffic that&lt;br/&gt;can be send under current conditions; if a packet drop is recorded it is assumed the bandwidth rate was too high and so the window size is reduced. The researchers shall develop efficient probing algorithms and theoretical limits on the efficiency of probing under different models of Internet congestion.&lt;br/&gt;&lt;br/&gt;Cost Sharing. How are the recipients of a multicast transmission to share the network costs? The researchers assume that the information to be multicast is of a certain value to each possible recipient, but this value is private to that individual. The researchers shall investigate strategyproof cost sharing methods where each user is assured that their outcome is maximized if they truthfully reveal their value to the network. The researchers&apos; goal is to characterize the set of protocols that are acceptable on both game-theoretic and complexity grounds.&lt;br/&gt;&lt;br/&gt;Information Dissemination. While traditional databases require transactional consistency, many repositories of information require only the much weaker notion of eventual consistency. That is, in such cases we care only whether, and how quickly, the information is disseminated, but do not require global consistency during the dissemination. The researchers shall identify message-efficient strategies for selectively propagating information so that the network will eventually converge to a fully updated state.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81698</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">81698</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1377" target="n1379">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Analysis of Internet Algorithms: Optimization, Game Theory and Competitive Analysis</data>
      <data key="e_abstract">As the complexity of the Internet, the nature of its applications, and its socioeconomic framework evolve, new algorithmic and architectural ideas will be proposed, tested, and adopted. While the original Internet design principles will likely remain valid, the researchers believe that it is important to have in place a mathematical framework within which these design principles can be expressed and applied to the next generation of Internet algorithms and architectures. Building such a framework is the ultimate goal. The mathematical tools will come from optimization, game theory and competitive analysis. The researchers shall work on the following topics.&lt;br/&gt;&lt;br/&gt;Multicast. The researchers shall seek to determine the relative efficiency, in terms of link usage, of multicast versus unicast, devise and analyze efficient methods of multicast error recovery, and determine how efficiently multicast can be simulated in the application layer by a coordinated set of unicasts.&lt;br/&gt;&lt;br/&gt;Congestion Probing. The TCP congestion control protocol controls its window size with an additive-increase and multiplicative-decrease (AIMD) algorithm. One can think of this as a probing algorithm in which the flow attempts to discover the maximum rate of traffic that&lt;br/&gt;can be send under current conditions; if a packet drop is recorded it is assumed the bandwidth rate was too high and so the window size is reduced. The researchers shall develop efficient probing algorithms and theoretical limits on the efficiency of probing under different models of Internet congestion.&lt;br/&gt;&lt;br/&gt;Cost Sharing. How are the recipients of a multicast transmission to share the network costs? The researchers assume that the information to be multicast is of a certain value to each possible recipient, but this value is private to that individual. The researchers shall investigate strategyproof cost sharing methods where each user is assured that their outcome is maximized if they truthfully reveal their value to the network. The researchers&apos; goal is to characterize the set of protocols that are acceptable on both game-theoretic and complexity grounds.&lt;br/&gt;&lt;br/&gt;Information Dissemination. While traditional databases require transactional consistency, many repositories of information require only the much weaker notion of eventual consistency. That is, in such cases we care only whether, and how quickly, the information is disseminated, but do not require global consistency during the dissemination. The researchers shall identify message-efficient strategies for selectively propagating information so that the network will eventually converge to a fully updated state.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81698</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">81698</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1378" target="n1379">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Analysis of Internet Algorithms: Optimization, Game Theory and Competitive Analysis</data>
      <data key="e_abstract">As the complexity of the Internet, the nature of its applications, and its socioeconomic framework evolve, new algorithmic and architectural ideas will be proposed, tested, and adopted. While the original Internet design principles will likely remain valid, the researchers believe that it is important to have in place a mathematical framework within which these design principles can be expressed and applied to the next generation of Internet algorithms and architectures. Building such a framework is the ultimate goal. The mathematical tools will come from optimization, game theory and competitive analysis. The researchers shall work on the following topics.&lt;br/&gt;&lt;br/&gt;Multicast. The researchers shall seek to determine the relative efficiency, in terms of link usage, of multicast versus unicast, devise and analyze efficient methods of multicast error recovery, and determine how efficiently multicast can be simulated in the application layer by a coordinated set of unicasts.&lt;br/&gt;&lt;br/&gt;Congestion Probing. The TCP congestion control protocol controls its window size with an additive-increase and multiplicative-decrease (AIMD) algorithm. One can think of this as a probing algorithm in which the flow attempts to discover the maximum rate of traffic that&lt;br/&gt;can be send under current conditions; if a packet drop is recorded it is assumed the bandwidth rate was too high and so the window size is reduced. The researchers shall develop efficient probing algorithms and theoretical limits on the efficiency of probing under different models of Internet congestion.&lt;br/&gt;&lt;br/&gt;Cost Sharing. How are the recipients of a multicast transmission to share the network costs? The researchers assume that the information to be multicast is of a certain value to each possible recipient, but this value is private to that individual. The researchers shall investigate strategyproof cost sharing methods where each user is assured that their outcome is maximized if they truthfully reveal their value to the network. The researchers&apos; goal is to characterize the set of protocols that are acceptable on both game-theoretic and complexity grounds.&lt;br/&gt;&lt;br/&gt;Information Dissemination. While traditional databases require transactional consistency, many repositories of information require only the much weaker notion of eventual consistency. That is, in such cases we care only whether, and how quickly, the information is disseminated, but do not require global consistency during the dissemination. The researchers shall identify message-efficient strategies for selectively propagating information so that the network will eventually converge to a fully updated state.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81698</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">81698</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1380" target="n1381">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Principles of Distributed Component-Based Software</data>
      <data key="e_abstract">Reasoning about the behavior of large component-based software systems demands a &quot;modular&quot; or &quot;compositional&quot; reasoning system, in which summary properties of a system&apos;s pieces are composable to deduce properties of the entire system without delving into the internal details of those pieces. This research focuses on contributing principles for how to design component-based software that supports modular reasoning, and to help bring this new knowledge into practical application with commercial distributed component technologies. Specifically, the project investigates: (1) developing and describing detailed principles for designing the interfaces of software components so that they support both modular reasoning about system behavior and effective and efficient distribution and execution; (2) showing how to write human-understandable behavioral specifications for the interfaces of components designed using the above principles; (3) demonstrating additional practical benefits from having formal specifications available to software engineering tools. The generality and efficacy of the results will be evaluated through construction of prototype tools that support distributed component-based software design and development in a programming-language-neutral environment, and by observing the effects of using such tools in the classroom to see how much students benefit from the tools&apos; new specification-enabled capabilities as they&lt;br/&gt;design and develop distributed component-based software systems.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81596</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81596</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1380" target="n1382">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Principles of Distributed Component-Based Software</data>
      <data key="e_abstract">Reasoning about the behavior of large component-based software systems demands a &quot;modular&quot; or &quot;compositional&quot; reasoning system, in which summary properties of a system&apos;s pieces are composable to deduce properties of the entire system without delving into the internal details of those pieces. This research focuses on contributing principles for how to design component-based software that supports modular reasoning, and to help bring this new knowledge into practical application with commercial distributed component technologies. Specifically, the project investigates: (1) developing and describing detailed principles for designing the interfaces of software components so that they support both modular reasoning about system behavior and effective and efficient distribution and execution; (2) showing how to write human-understandable behavioral specifications for the interfaces of components designed using the above principles; (3) demonstrating additional practical benefits from having formal specifications available to software engineering tools. The generality and efficacy of the results will be evaluated through construction of prototype tools that support distributed component-based software design and development in a programming-language-neutral environment, and by observing the effects of using such tools in the classroom to see how much students benefit from the tools&apos; new specification-enabled capabilities as they&lt;br/&gt;design and develop distributed component-based software systems.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81596</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81596</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1380" target="n1383">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Principles of Distributed Component-Based Software</data>
      <data key="e_abstract">Reasoning about the behavior of large component-based software systems demands a &quot;modular&quot; or &quot;compositional&quot; reasoning system, in which summary properties of a system&apos;s pieces are composable to deduce properties of the entire system without delving into the internal details of those pieces. This research focuses on contributing principles for how to design component-based software that supports modular reasoning, and to help bring this new knowledge into practical application with commercial distributed component technologies. Specifically, the project investigates: (1) developing and describing detailed principles for designing the interfaces of software components so that they support both modular reasoning about system behavior and effective and efficient distribution and execution; (2) showing how to write human-understandable behavioral specifications for the interfaces of components designed using the above principles; (3) demonstrating additional practical benefits from having formal specifications available to software engineering tools. The generality and efficacy of the results will be evaluated through construction of prototype tools that support distributed component-based software design and development in a programming-language-neutral environment, and by observing the effects of using such tools in the classroom to see how much students benefit from the tools&apos; new specification-enabled capabilities as they&lt;br/&gt;design and develop distributed component-based software systems.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81596</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81596</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1380" target="n1384">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Principles of Distributed Component-Based Software</data>
      <data key="e_abstract">Reasoning about the behavior of large component-based software systems demands a &quot;modular&quot; or &quot;compositional&quot; reasoning system, in which summary properties of a system&apos;s pieces are composable to deduce properties of the entire system without delving into the internal details of those pieces. This research focuses on contributing principles for how to design component-based software that supports modular reasoning, and to help bring this new knowledge into practical application with commercial distributed component technologies. Specifically, the project investigates: (1) developing and describing detailed principles for designing the interfaces of software components so that they support both modular reasoning about system behavior and effective and efficient distribution and execution; (2) showing how to write human-understandable behavioral specifications for the interfaces of components designed using the above principles; (3) demonstrating additional practical benefits from having formal specifications available to software engineering tools. The generality and efficacy of the results will be evaluated through construction of prototype tools that support distributed component-based software design and development in a programming-language-neutral environment, and by observing the effects of using such tools in the classroom to see how much students benefit from the tools&apos; new specification-enabled capabilities as they&lt;br/&gt;design and develop distributed component-based software systems.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81596</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81596</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1381" target="n1382">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Principles of Distributed Component-Based Software</data>
      <data key="e_abstract">Reasoning about the behavior of large component-based software systems demands a &quot;modular&quot; or &quot;compositional&quot; reasoning system, in which summary properties of a system&apos;s pieces are composable to deduce properties of the entire system without delving into the internal details of those pieces. This research focuses on contributing principles for how to design component-based software that supports modular reasoning, and to help bring this new knowledge into practical application with commercial distributed component technologies. Specifically, the project investigates: (1) developing and describing detailed principles for designing the interfaces of software components so that they support both modular reasoning about system behavior and effective and efficient distribution and execution; (2) showing how to write human-understandable behavioral specifications for the interfaces of components designed using the above principles; (3) demonstrating additional practical benefits from having formal specifications available to software engineering tools. The generality and efficacy of the results will be evaluated through construction of prototype tools that support distributed component-based software design and development in a programming-language-neutral environment, and by observing the effects of using such tools in the classroom to see how much students benefit from the tools&apos; new specification-enabled capabilities as they&lt;br/&gt;design and develop distributed component-based software systems.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81596</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81596</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1381" target="n1383">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Principles of Distributed Component-Based Software</data>
      <data key="e_abstract">Reasoning about the behavior of large component-based software systems demands a &quot;modular&quot; or &quot;compositional&quot; reasoning system, in which summary properties of a system&apos;s pieces are composable to deduce properties of the entire system without delving into the internal details of those pieces. This research focuses on contributing principles for how to design component-based software that supports modular reasoning, and to help bring this new knowledge into practical application with commercial distributed component technologies. Specifically, the project investigates: (1) developing and describing detailed principles for designing the interfaces of software components so that they support both modular reasoning about system behavior and effective and efficient distribution and execution; (2) showing how to write human-understandable behavioral specifications for the interfaces of components designed using the above principles; (3) demonstrating additional practical benefits from having formal specifications available to software engineering tools. The generality and efficacy of the results will be evaluated through construction of prototype tools that support distributed component-based software design and development in a programming-language-neutral environment, and by observing the effects of using such tools in the classroom to see how much students benefit from the tools&apos; new specification-enabled capabilities as they&lt;br/&gt;design and develop distributed component-based software systems.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81596</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81596</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1381" target="n1384">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Principles of Distributed Component-Based Software</data>
      <data key="e_abstract">Reasoning about the behavior of large component-based software systems demands a &quot;modular&quot; or &quot;compositional&quot; reasoning system, in which summary properties of a system&apos;s pieces are composable to deduce properties of the entire system without delving into the internal details of those pieces. This research focuses on contributing principles for how to design component-based software that supports modular reasoning, and to help bring this new knowledge into practical application with commercial distributed component technologies. Specifically, the project investigates: (1) developing and describing detailed principles for designing the interfaces of software components so that they support both modular reasoning about system behavior and effective and efficient distribution and execution; (2) showing how to write human-understandable behavioral specifications for the interfaces of components designed using the above principles; (3) demonstrating additional practical benefits from having formal specifications available to software engineering tools. The generality and efficacy of the results will be evaluated through construction of prototype tools that support distributed component-based software design and development in a programming-language-neutral environment, and by observing the effects of using such tools in the classroom to see how much students benefit from the tools&apos; new specification-enabled capabilities as they&lt;br/&gt;design and develop distributed component-based software systems.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81596</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81596</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1382" target="n1383">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Principles of Distributed Component-Based Software</data>
      <data key="e_abstract">Reasoning about the behavior of large component-based software systems demands a &quot;modular&quot; or &quot;compositional&quot; reasoning system, in which summary properties of a system&apos;s pieces are composable to deduce properties of the entire system without delving into the internal details of those pieces. This research focuses on contributing principles for how to design component-based software that supports modular reasoning, and to help bring this new knowledge into practical application with commercial distributed component technologies. Specifically, the project investigates: (1) developing and describing detailed principles for designing the interfaces of software components so that they support both modular reasoning about system behavior and effective and efficient distribution and execution; (2) showing how to write human-understandable behavioral specifications for the interfaces of components designed using the above principles; (3) demonstrating additional practical benefits from having formal specifications available to software engineering tools. The generality and efficacy of the results will be evaluated through construction of prototype tools that support distributed component-based software design and development in a programming-language-neutral environment, and by observing the effects of using such tools in the classroom to see how much students benefit from the tools&apos; new specification-enabled capabilities as they&lt;br/&gt;design and develop distributed component-based software systems.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81596</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81596</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1382" target="n1384">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Principles of Distributed Component-Based Software</data>
      <data key="e_abstract">Reasoning about the behavior of large component-based software systems demands a &quot;modular&quot; or &quot;compositional&quot; reasoning system, in which summary properties of a system&apos;s pieces are composable to deduce properties of the entire system without delving into the internal details of those pieces. This research focuses on contributing principles for how to design component-based software that supports modular reasoning, and to help bring this new knowledge into practical application with commercial distributed component technologies. Specifically, the project investigates: (1) developing and describing detailed principles for designing the interfaces of software components so that they support both modular reasoning about system behavior and effective and efficient distribution and execution; (2) showing how to write human-understandable behavioral specifications for the interfaces of components designed using the above principles; (3) demonstrating additional practical benefits from having formal specifications available to software engineering tools. The generality and efficacy of the results will be evaluated through construction of prototype tools that support distributed component-based software design and development in a programming-language-neutral environment, and by observing the effects of using such tools in the classroom to see how much students benefit from the tools&apos; new specification-enabled capabilities as they&lt;br/&gt;design and develop distributed component-based software systems.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81596</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81596</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1383" target="n1384">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Principles of Distributed Component-Based Software</data>
      <data key="e_abstract">Reasoning about the behavior of large component-based software systems demands a &quot;modular&quot; or &quot;compositional&quot; reasoning system, in which summary properties of a system&apos;s pieces are composable to deduce properties of the entire system without delving into the internal details of those pieces. This research focuses on contributing principles for how to design component-based software that supports modular reasoning, and to help bring this new knowledge into practical application with commercial distributed component technologies. Specifically, the project investigates: (1) developing and describing detailed principles for designing the interfaces of software components so that they support both modular reasoning about system behavior and effective and efficient distribution and execution; (2) showing how to write human-understandable behavioral specifications for the interfaces of components designed using the above principles; (3) demonstrating additional practical benefits from having formal specifications available to software engineering tools. The generality and efficacy of the results will be evaluated through construction of prototype tools that support distributed component-based software design and development in a programming-language-neutral environment, and by observing the effects of using such tools in the classroom to see how much students benefit from the tools&apos; new specification-enabled capabilities as they&lt;br/&gt;design and develop distributed component-based software systems.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81596</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81596</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1387" target="n1388">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Combined Research-Curriculum Development in Computer Vision</data>
      <data key="e_abstract">0088086&lt;br/&gt;Bebis, George&lt;br/&gt;University of Nevada Reno&lt;br/&gt;&lt;br/&gt;Combined Research-Curriculum Development in Computer Vision&lt;br/&gt;&lt;br/&gt;This project integrates recent and ongoing research in computer vision into this institution&apos;s computer science and engineering curriculum. There are several objectives for this project. One is the design, implementation, and testing of innovative approaches for integrating teaching with research, leading to a comprehensive instructional program offering systematic and constant research experiences to as many students as possible. To this end, students are included in summer research programs involving computer vision. Another objective of the project is the integration of computer vision principles into several core courses and the introduction of a new course in mathematical methods for computer vision to support the integration of vision research into core courses. The final objective is the development of a new course in object recognition that includes current research advances and evolving principles of object recognition. The evaluation of the success of educational techniques and the dissemination of the results of this activity are critical elements of the project.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88086</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88086</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1387" target="n1389">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Combined Research-Curriculum Development in Computer Vision</data>
      <data key="e_abstract">0088086&lt;br/&gt;Bebis, George&lt;br/&gt;University of Nevada Reno&lt;br/&gt;&lt;br/&gt;Combined Research-Curriculum Development in Computer Vision&lt;br/&gt;&lt;br/&gt;This project integrates recent and ongoing research in computer vision into this institution&apos;s computer science and engineering curriculum. There are several objectives for this project. One is the design, implementation, and testing of innovative approaches for integrating teaching with research, leading to a comprehensive instructional program offering systematic and constant research experiences to as many students as possible. To this end, students are included in summer research programs involving computer vision. Another objective of the project is the integration of computer vision principles into several core courses and the introduction of a new course in mathematical methods for computer vision to support the integration of vision research into core courses. The final objective is the development of a new course in object recognition that includes current research advances and evolving principles of object recognition. The evaluation of the success of educational techniques and the dissemination of the results of this activity are critical elements of the project.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88086</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88086</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1387" target="n1390">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Combined Research-Curriculum Development in Computer Vision</data>
      <data key="e_abstract">0088086&lt;br/&gt;Bebis, George&lt;br/&gt;University of Nevada Reno&lt;br/&gt;&lt;br/&gt;Combined Research-Curriculum Development in Computer Vision&lt;br/&gt;&lt;br/&gt;This project integrates recent and ongoing research in computer vision into this institution&apos;s computer science and engineering curriculum. There are several objectives for this project. One is the design, implementation, and testing of innovative approaches for integrating teaching with research, leading to a comprehensive instructional program offering systematic and constant research experiences to as many students as possible. To this end, students are included in summer research programs involving computer vision. Another objective of the project is the integration of computer vision principles into several core courses and the introduction of a new course in mathematical methods for computer vision to support the integration of vision research into core courses. The final objective is the development of a new course in object recognition that includes current research advances and evolving principles of object recognition. The evaluation of the success of educational techniques and the dissemination of the results of this activity are critical elements of the project.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88086</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88086</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1388" target="n1389">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Combined Research-Curriculum Development in Computer Vision</data>
      <data key="e_abstract">0088086&lt;br/&gt;Bebis, George&lt;br/&gt;University of Nevada Reno&lt;br/&gt;&lt;br/&gt;Combined Research-Curriculum Development in Computer Vision&lt;br/&gt;&lt;br/&gt;This project integrates recent and ongoing research in computer vision into this institution&apos;s computer science and engineering curriculum. There are several objectives for this project. One is the design, implementation, and testing of innovative approaches for integrating teaching with research, leading to a comprehensive instructional program offering systematic and constant research experiences to as many students as possible. To this end, students are included in summer research programs involving computer vision. Another objective of the project is the integration of computer vision principles into several core courses and the introduction of a new course in mathematical methods for computer vision to support the integration of vision research into core courses. The final objective is the development of a new course in object recognition that includes current research advances and evolving principles of object recognition. The evaluation of the success of educational techniques and the dissemination of the results of this activity are critical elements of the project.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88086</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88086</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1388" target="n1390">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Combined Research-Curriculum Development in Computer Vision</data>
      <data key="e_abstract">0088086&lt;br/&gt;Bebis, George&lt;br/&gt;University of Nevada Reno&lt;br/&gt;&lt;br/&gt;Combined Research-Curriculum Development in Computer Vision&lt;br/&gt;&lt;br/&gt;This project integrates recent and ongoing research in computer vision into this institution&apos;s computer science and engineering curriculum. There are several objectives for this project. One is the design, implementation, and testing of innovative approaches for integrating teaching with research, leading to a comprehensive instructional program offering systematic and constant research experiences to as many students as possible. To this end, students are included in summer research programs involving computer vision. Another objective of the project is the integration of computer vision principles into several core courses and the introduction of a new course in mathematical methods for computer vision to support the integration of vision research into core courses. The final objective is the development of a new course in object recognition that includes current research advances and evolving principles of object recognition. The evaluation of the success of educational techniques and the dissemination of the results of this activity are critical elements of the project.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88086</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88086</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1389" target="n1390">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Combined Research-Curriculum Development in Computer Vision</data>
      <data key="e_abstract">0088086&lt;br/&gt;Bebis, George&lt;br/&gt;University of Nevada Reno&lt;br/&gt;&lt;br/&gt;Combined Research-Curriculum Development in Computer Vision&lt;br/&gt;&lt;br/&gt;This project integrates recent and ongoing research in computer vision into this institution&apos;s computer science and engineering curriculum. There are several objectives for this project. One is the design, implementation, and testing of innovative approaches for integrating teaching with research, leading to a comprehensive instructional program offering systematic and constant research experiences to as many students as possible. To this end, students are included in summer research programs involving computer vision. Another objective of the project is the integration of computer vision principles into several core courses and the introduction of a new course in mathematical methods for computer vision to support the integration of vision research into core courses. The final objective is the development of a new course in object recognition that includes current research advances and evolving principles of object recognition. The evaluation of the success of educational techniques and the dissemination of the results of this activity are critical elements of the project.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88086</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88086</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1391" target="n1392">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MRI: Acquisition of Equipment for Quantum Information Processing</data>
      <data key="e_abstract">EIA-0079842&lt;br/&gt;Yablonovitch, Eli&lt;br/&gt;University of California Los Angeles&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of Equipment for Quantum Information Processing&lt;br/&gt;&lt;br/&gt;The focus of this proposal is acquisition of a Hitachi S-4700-II field emission scanning electron microscope (FESEM) with resolution (secondary emission mode) of 1.5nm at 15kV. The proposed equipment will help make an optical communications receiver and transmitter technology that can transfer quantum coherence and entanglement from photons, to spins in a semiconductor and back to photons again. Since the photo-electron spin is reasonable long-lived, the quantum information can be stored and act as quantum memory. If this technology is successful, it can become the front end for more complex semiconductor based quantum processors in the future, perhaps even a full-fledged quantum computer.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79842</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79842</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1391" target="n1393">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MRI: Acquisition of Equipment for Quantum Information Processing</data>
      <data key="e_abstract">EIA-0079842&lt;br/&gt;Yablonovitch, Eli&lt;br/&gt;University of California Los Angeles&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of Equipment for Quantum Information Processing&lt;br/&gt;&lt;br/&gt;The focus of this proposal is acquisition of a Hitachi S-4700-II field emission scanning electron microscope (FESEM) with resolution (secondary emission mode) of 1.5nm at 15kV. The proposed equipment will help make an optical communications receiver and transmitter technology that can transfer quantum coherence and entanglement from photons, to spins in a semiconductor and back to photons again. Since the photo-electron spin is reasonable long-lived, the quantum information can be stored and act as quantum memory. If this technology is successful, it can become the front end for more complex semiconductor based quantum processors in the future, perhaps even a full-fledged quantum computer.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79842</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79842</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1391" target="n1394">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MRI: Acquisition of Equipment for Quantum Information Processing</data>
      <data key="e_abstract">EIA-0079842&lt;br/&gt;Yablonovitch, Eli&lt;br/&gt;University of California Los Angeles&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of Equipment for Quantum Information Processing&lt;br/&gt;&lt;br/&gt;The focus of this proposal is acquisition of a Hitachi S-4700-II field emission scanning electron microscope (FESEM) with resolution (secondary emission mode) of 1.5nm at 15kV. The proposed equipment will help make an optical communications receiver and transmitter technology that can transfer quantum coherence and entanglement from photons, to spins in a semiconductor and back to photons again. Since the photo-electron spin is reasonable long-lived, the quantum information can be stored and act as quantum memory. If this technology is successful, it can become the front end for more complex semiconductor based quantum processors in the future, perhaps even a full-fledged quantum computer.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79842</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79842</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1392" target="n1393">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MRI: Acquisition of Equipment for Quantum Information Processing</data>
      <data key="e_abstract">EIA-0079842&lt;br/&gt;Yablonovitch, Eli&lt;br/&gt;University of California Los Angeles&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of Equipment for Quantum Information Processing&lt;br/&gt;&lt;br/&gt;The focus of this proposal is acquisition of a Hitachi S-4700-II field emission scanning electron microscope (FESEM) with resolution (secondary emission mode) of 1.5nm at 15kV. The proposed equipment will help make an optical communications receiver and transmitter technology that can transfer quantum coherence and entanglement from photons, to spins in a semiconductor and back to photons again. Since the photo-electron spin is reasonable long-lived, the quantum information can be stored and act as quantum memory. If this technology is successful, it can become the front end for more complex semiconductor based quantum processors in the future, perhaps even a full-fledged quantum computer.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79842</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79842</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1392" target="n1394">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MRI: Acquisition of Equipment for Quantum Information Processing</data>
      <data key="e_abstract">EIA-0079842&lt;br/&gt;Yablonovitch, Eli&lt;br/&gt;University of California Los Angeles&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of Equipment for Quantum Information Processing&lt;br/&gt;&lt;br/&gt;The focus of this proposal is acquisition of a Hitachi S-4700-II field emission scanning electron microscope (FESEM) with resolution (secondary emission mode) of 1.5nm at 15kV. The proposed equipment will help make an optical communications receiver and transmitter technology that can transfer quantum coherence and entanglement from photons, to spins in a semiconductor and back to photons again. Since the photo-electron spin is reasonable long-lived, the quantum information can be stored and act as quantum memory. If this technology is successful, it can become the front end for more complex semiconductor based quantum processors in the future, perhaps even a full-fledged quantum computer.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79842</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79842</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1393" target="n1394">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">MRI: Acquisition of Equipment for Quantum Information Processing</data>
      <data key="e_abstract">EIA-0079842&lt;br/&gt;Yablonovitch, Eli&lt;br/&gt;University of California Los Angeles&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of Equipment for Quantum Information Processing&lt;br/&gt;&lt;br/&gt;The focus of this proposal is acquisition of a Hitachi S-4700-II field emission scanning electron microscope (FESEM) with resolution (secondary emission mode) of 1.5nm at 15kV. The proposed equipment will help make an optical communications receiver and transmitter technology that can transfer quantum coherence and entanglement from photons, to spins in a semiconductor and back to photons again. Since the photo-electron spin is reasonable long-lived, the quantum information can be stored and act as quantum memory. If this technology is successful, it can become the front end for more complex semiconductor based quantum processors in the future, perhaps even a full-fledged quantum computer.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79842</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79842</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1289" target="n1331">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Designing &quot;Real-Power&quot; Systems: Static and Dynamic Techniques for Managing Power/Performance Tradeoffs</data>
      <data key="e_abstract">Power consumption, thermal issues, and battery lifetimes are primary design issues in many computer systems. To address the needs for effective and efficient power management in current computer systems, this project studies unified methodologies for creating power-aware hardware and software. A key underlying philosophy in this work is the notion of &quot;real-power&quot; systems. Drawing an analogy to real-time computer systems, real-power systems seek to maintain predictable and manageable levels of power consumption with the best possible performance, rather than simply reducing power consumption regardless of performance. A multi-level approach spanning operating systems, compilers, and hardware brings leverage to a problem that is difficult to address at the hardware level alone.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86031</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86031</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1331" target="n1332">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Designing &quot;Real-Power&quot; Systems: Static and Dynamic Techniques for Managing Power/Performance Tradeoffs</data>
      <data key="e_abstract">Power consumption, thermal issues, and battery lifetimes are primary design issues in many computer systems. To address the needs for effective and efficient power management in current computer systems, this project studies unified methodologies for creating power-aware hardware and software. A key underlying philosophy in this work is the notion of &quot;real-power&quot; systems. Drawing an analogy to real-time computer systems, real-power systems seek to maintain predictable and manageable levels of power consumption with the best possible performance, rather than simply reducing power consumption regardless of performance. A multi-level approach spanning operating systems, compilers, and hardware brings leverage to a problem that is difficult to address at the hardware level alone.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86031</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86031</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1289" target="n1332">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Designing &quot;Real-Power&quot; Systems: Static and Dynamic Techniques for Managing Power/Performance Tradeoffs</data>
      <data key="e_abstract">Power consumption, thermal issues, and battery lifetimes are primary design issues in many computer systems. To address the needs for effective and efficient power management in current computer systems, this project studies unified methodologies for creating power-aware hardware and software. A key underlying philosophy in this work is the notion of &quot;real-power&quot; systems. Drawing an analogy to real-time computer systems, real-power systems seek to maintain predictable and manageable levels of power consumption with the best possible performance, rather than simply reducing power consumption regardless of performance. A multi-level approach spanning operating systems, compilers, and hardware brings leverage to a problem that is difficult to address at the hardware level alone.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86031</data>
      <data key="e_expirationDate">2006-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86031</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1399" target="n1400">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Heterogeneous System Integration in System-on-a-Chip Designs</data>
      <data key="e_abstract">This project studies the integration of heterogeneous resources into a system-on-chip (SOC) solution. Heterogeneous SOC integration supports the fabrication of RF, analog, high performance digital, and re-configurable subsystems within a single piece of silicon, and includes issues of simulation, design, integration, test, and education. An example SOC is a human/machine transducer chip that provides a speech recognition interface to a ubiquitous wireless network. Such a system represents a standard interface modality. Multiple topics are being researched including low-power speaker identification, speech processing algorithms, and hardware implementations. Low power, high performance wireless protocols are also being developed to support the asymmetric communication loads, sending low bandwidth control messages produced from the recognized speech and receiving high-bandwidth information return for visual, audio, and other feedback to the user.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86032</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86032</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1399" target="n1401">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Heterogeneous System Integration in System-on-a-Chip Designs</data>
      <data key="e_abstract">This project studies the integration of heterogeneous resources into a system-on-chip (SOC) solution. Heterogeneous SOC integration supports the fabrication of RF, analog, high performance digital, and re-configurable subsystems within a single piece of silicon, and includes issues of simulation, design, integration, test, and education. An example SOC is a human/machine transducer chip that provides a speech recognition interface to a ubiquitous wireless network. Such a system represents a standard interface modality. Multiple topics are being researched including low-power speaker identification, speech processing algorithms, and hardware implementations. Low power, high performance wireless protocols are also being developed to support the asymmetric communication loads, sending low bandwidth control messages produced from the recognized speech and receiving high-bandwidth information return for visual, audio, and other feedback to the user.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86032</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86032</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1090" target="n1399">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Heterogeneous System Integration in System-on-a-Chip Designs</data>
      <data key="e_abstract">This project studies the integration of heterogeneous resources into a system-on-chip (SOC) solution. Heterogeneous SOC integration supports the fabrication of RF, analog, high performance digital, and re-configurable subsystems within a single piece of silicon, and includes issues of simulation, design, integration, test, and education. An example SOC is a human/machine transducer chip that provides a speech recognition interface to a ubiquitous wireless network. Such a system represents a standard interface modality. Multiple topics are being researched including low-power speaker identification, speech processing algorithms, and hardware implementations. Low power, high performance wireless protocols are also being developed to support the asymmetric communication loads, sending low bandwidth control messages produced from the recognized speech and receiving high-bandwidth information return for visual, audio, and other feedback to the user.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86032</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86032</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1399" target="n1403">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Heterogeneous System Integration in System-on-a-Chip Designs</data>
      <data key="e_abstract">This project studies the integration of heterogeneous resources into a system-on-chip (SOC) solution. Heterogeneous SOC integration supports the fabrication of RF, analog, high performance digital, and re-configurable subsystems within a single piece of silicon, and includes issues of simulation, design, integration, test, and education. An example SOC is a human/machine transducer chip that provides a speech recognition interface to a ubiquitous wireless network. Such a system represents a standard interface modality. Multiple topics are being researched including low-power speaker identification, speech processing algorithms, and hardware implementations. Low power, high performance wireless protocols are also being developed to support the asymmetric communication loads, sending low bandwidth control messages produced from the recognized speech and receiving high-bandwidth information return for visual, audio, and other feedback to the user.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86032</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86032</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1400" target="n1401">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Heterogeneous System Integration in System-on-a-Chip Designs</data>
      <data key="e_abstract">This project studies the integration of heterogeneous resources into a system-on-chip (SOC) solution. Heterogeneous SOC integration supports the fabrication of RF, analog, high performance digital, and re-configurable subsystems within a single piece of silicon, and includes issues of simulation, design, integration, test, and education. An example SOC is a human/machine transducer chip that provides a speech recognition interface to a ubiquitous wireless network. Such a system represents a standard interface modality. Multiple topics are being researched including low-power speaker identification, speech processing algorithms, and hardware implementations. Low power, high performance wireless protocols are also being developed to support the asymmetric communication loads, sending low bandwidth control messages produced from the recognized speech and receiving high-bandwidth information return for visual, audio, and other feedback to the user.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86032</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86032</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1090" target="n1400">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Heterogeneous System Integration in System-on-a-Chip Designs</data>
      <data key="e_abstract">This project studies the integration of heterogeneous resources into a system-on-chip (SOC) solution. Heterogeneous SOC integration supports the fabrication of RF, analog, high performance digital, and re-configurable subsystems within a single piece of silicon, and includes issues of simulation, design, integration, test, and education. An example SOC is a human/machine transducer chip that provides a speech recognition interface to a ubiquitous wireless network. Such a system represents a standard interface modality. Multiple topics are being researched including low-power speaker identification, speech processing algorithms, and hardware implementations. Low power, high performance wireless protocols are also being developed to support the asymmetric communication loads, sending low bandwidth control messages produced from the recognized speech and receiving high-bandwidth information return for visual, audio, and other feedback to the user.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86032</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86032</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1400" target="n1403">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Heterogeneous System Integration in System-on-a-Chip Designs</data>
      <data key="e_abstract">This project studies the integration of heterogeneous resources into a system-on-chip (SOC) solution. Heterogeneous SOC integration supports the fabrication of RF, analog, high performance digital, and re-configurable subsystems within a single piece of silicon, and includes issues of simulation, design, integration, test, and education. An example SOC is a human/machine transducer chip that provides a speech recognition interface to a ubiquitous wireless network. Such a system represents a standard interface modality. Multiple topics are being researched including low-power speaker identification, speech processing algorithms, and hardware implementations. Low power, high performance wireless protocols are also being developed to support the asymmetric communication loads, sending low bandwidth control messages produced from the recognized speech and receiving high-bandwidth information return for visual, audio, and other feedback to the user.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86032</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86032</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1090" target="n1401">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Heterogeneous System Integration in System-on-a-Chip Designs</data>
      <data key="e_abstract">This project studies the integration of heterogeneous resources into a system-on-chip (SOC) solution. Heterogeneous SOC integration supports the fabrication of RF, analog, high performance digital, and re-configurable subsystems within a single piece of silicon, and includes issues of simulation, design, integration, test, and education. An example SOC is a human/machine transducer chip that provides a speech recognition interface to a ubiquitous wireless network. Such a system represents a standard interface modality. Multiple topics are being researched including low-power speaker identification, speech processing algorithms, and hardware implementations. Low power, high performance wireless protocols are also being developed to support the asymmetric communication loads, sending low bandwidth control messages produced from the recognized speech and receiving high-bandwidth information return for visual, audio, and other feedback to the user.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86032</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86032</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1401" target="n1403">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Heterogeneous System Integration in System-on-a-Chip Designs</data>
      <data key="e_abstract">This project studies the integration of heterogeneous resources into a system-on-chip (SOC) solution. Heterogeneous SOC integration supports the fabrication of RF, analog, high performance digital, and re-configurable subsystems within a single piece of silicon, and includes issues of simulation, design, integration, test, and education. An example SOC is a human/machine transducer chip that provides a speech recognition interface to a ubiquitous wireless network. Such a system represents a standard interface modality. Multiple topics are being researched including low-power speaker identification, speech processing algorithms, and hardware implementations. Low power, high performance wireless protocols are also being developed to support the asymmetric communication loads, sending low bandwidth control messages produced from the recognized speech and receiving high-bandwidth information return for visual, audio, and other feedback to the user.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86032</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86032</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1090" target="n1403">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Heterogeneous System Integration in System-on-a-Chip Designs</data>
      <data key="e_abstract">This project studies the integration of heterogeneous resources into a system-on-chip (SOC) solution. Heterogeneous SOC integration supports the fabrication of RF, analog, high performance digital, and re-configurable subsystems within a single piece of silicon, and includes issues of simulation, design, integration, test, and education. An example SOC is a human/machine transducer chip that provides a speech recognition interface to a ubiquitous wireless network. Such a system represents a standard interface modality. Multiple topics are being researched including low-power speaker identification, speech processing algorithms, and hardware implementations. Low power, high performance wireless protocols are also being developed to support the asymmetric communication loads, sending low bandwidth control messages produced from the recognized speech and receiving high-bandwidth information return for visual, audio, and other feedback to the user.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86032</data>
      <data key="e_expirationDate">2007-02-28</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86032</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n186" target="n1405">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research:&apos;Free&apos; Bits: The Challenge of the Wireless Internet</data>
      <data key="e_abstract">Second and third generation wireless systems have been designed primarily for voice, so they are connection oriented, delay sensitive, and provide fixed bit rates. Additionally, since service is desired &quot;any-time/anywhere,&quot; these systems must provide ubiquitous coverage. This coverage is achieved through relatively uniform grids of cell sites, which are placed to control interference and minimize outage rather than to maximize throughout. As a result, such systems deliver low bit rates and are relatively expensive when used for large amounts of information. At the same time, wireline connection to the Internet has encouraged uses (and users) that depend upon bits being virtually &quot;free.&quot; We contend that this &quot;economic&quot; mismatch between wired and wireless access is the primary obstacle to the dramatic growth of a wireless Internet. &lt;br/&gt; The solution may lie in designing systems specifically for wireless data, recognizing that data services are often connectionless, delay insensitive and have no specific bit rate requirements. These differences suggest that ubiquitous (anytime/anywhere) coverage is not a strict requirement for wireless data networks and makes possible systems in which small, separated coverage areas facilitate transfers of megabytes of data in fractions of a second, and for a fraction of the cost associated with conventional ubiquitous coverage.&lt;br/&gt; Communication theory and simple link budget calculations tell us that it is possible to build such systems, but the signal processing challenges are numerous and distinct from the historical challenges offered by connection-oriented wireless services. When a mobile user passes an Infostation, there will be a window of opportunity, perhaps as short as a fraction of a second, in which the user will have access to a high-rate communication channel. A key task is to identify that window and transmit at an appropriate rate. The mobile must make these decisions based on measurements of a wideband radio channel in which there is frequency selectivity and time variation in the fading as well as in the interference. In the specific context of an Infostations system, we plan to divide our research into four components:&lt;br/&gt;&lt;br/&gt;Radio Channel Modeling: The characterization of typical Infostation radio channels.&lt;br/&gt;&lt;br/&gt;Transceiver Design: The analysis and performance evaluation of transmitters and receivers for both&lt;br/&gt;single carrier and multicarrier systems.&lt;br/&gt;&lt;br/&gt;Radio Resource Management Transmitter power and rate adaptation policies derived from receiver&lt;br/&gt;measurements.&lt;br/&gt;&lt;br/&gt;Algorithm Development Testbed A platform employing DSP and FPGA technology for the practical&lt;br/&gt;evaluation of transmitter and receiver algorithms.&lt;br/&gt;&lt;br/&gt; The activities of this project will encompass three research institutions in New Jersey (New Jersey Institute of Technology, Princeton University, Rutgers University) under the auspices of the N.J. Center for Wireless Telecommunications (NJCWT). The NJCWT is an inter-institutional research and educational organization sponsored and funded by the N.J. Commission on Science and Technology. The focus of the center is a multi-year effort in Digital Radio Technology for Computing, Communications and Information Systems. This effort is supportive of and will enhance the present proposed project in wireless networks.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86017</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86017</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1406" target="n1407">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Wireless Networking Solutions for Smart Sensor Biomedical Applications</data>
      <data key="e_abstract">Implanted biomedical devices have the potential to revolutionize medicine. The types of procedures that are being proposed could greatly improve the health and vitality of persons in ways previously not possible. Information technology is a critical component of this endeavor, requiring both novel hardware and software design. The limited power and computational capabilities of these biological implants present challenging research issues. As progress is made on these topics, there is great promise of long-term benefits. Multidisciplinary research, drawing on the expertise of researchers in a wide array of areas is required. This proposal assembles a multi-institutional team of researchers in computer science and engineering, solid state devices, and medicine. The combined talents of this team will be required to realize the goal of this proposal - small biomedical devices composed of smart sensors that are implanted for long-term use. These devices require the ability to communicate with an external diagnostic computer system via a wireless interface.&lt;br/&gt; A large-scale research program on smart sensors is on-going at Wayne State University, covering all aspects from materials characterization through integrated circuit design and simulation to hybrid device fabrication. This major research initiative requires a multidisciplinary team involving faculty, researchers, and students from the Colleges of Engineering, Science, and Medicine. All are members of the Smart Sensors and Integrated Microsystems (SSIM) research group. The research in this proposal adds a new dimension to the currently funded research of the SSIM program by providing wireless communication capabilities to the implanted microsensors. This additional capability is possible because of the close collaboration among researchers at Wayne State University and Colorado State University.&lt;br/&gt; The proposed work will take an integrated hardware and software approach to developing solutions for wireless networking of human-embedded microsensors. These solutions will be bio-compatible, energy-efficient, fault-tolerant, and scalable. In addition, they would support continuous operation and provide diagnostic capabilities. The proposed work will address several fundamental questions for the wireless networking of embedded microsensors, including those arising due to the need for low-powered, low-maintenance, highly-reliable, and scalable solutions. As a demonstration of our proposed techniques, an artificial retina prosthesis and related visual cortical implant will be developed. The goal is to design wireless network protocols for energy-efficient communication between multiple retinal sensor array/cortical implants and an external base station. The research in this proposal provides the building blocks for this wireless network. The severe limits on the computational and memory capabilities of the smart sensor implants place tight constraints on the communication protocol. For this reason, an external communication device, contained in a pair of eyeglasses, for example, will provide the additional resources necessary for protocol-compliant communication, and increased range and bandwidth. Software to display the message contents will be developed in order to validate the network protocols and the sensor communication. The software to perform image analysis and recognition will be also be developed by our research team. The developed solution will be evaluated, through both simulation and pro-totyping, for various performance and functionality criteria including bio-compatibility, energy-efficiency, reliability, and scalability&lt;br/&gt; Upon completion, the proposed work will have several benefits in the area of wireless networking of low-powered microsensors, which are suitable for biomedical applications. Other biomedical applications where this technology are useful is limited only by our imagination. For example, patients with Parkinson&apos;s disease and epilepsy could benefit from the ability to implant sensors in the neural pathways of the brain to alter the undesired signals and restore proper functioning. Existing technology is very crude and not suitable for chronic implanted devices or complex signal stimulation and detection. Another example is acoustic and optical biosensor arrays for blood analysis currently under development at Wayne State University. Similar sensors are being developed to detect cancer cells by implanting a smart sensor in the body of a recovering cancer patient. One of the main contributions of this project would be a framework for developing scalable wireless networking and powering solutions for biomedical applications.&lt;br/&gt; The integration of advances in wireless networking and smart sensor technology have great potential in several other applications such as the monitoring of distributed environmental sensors. It is envisioned that networked smart sensors will revolutionize our world in ways beyond our current imagination.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86020</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86020</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1406" target="n1408">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Wireless Networking Solutions for Smart Sensor Biomedical Applications</data>
      <data key="e_abstract">Implanted biomedical devices have the potential to revolutionize medicine. The types of procedures that are being proposed could greatly improve the health and vitality of persons in ways previously not possible. Information technology is a critical component of this endeavor, requiring both novel hardware and software design. The limited power and computational capabilities of these biological implants present challenging research issues. As progress is made on these topics, there is great promise of long-term benefits. Multidisciplinary research, drawing on the expertise of researchers in a wide array of areas is required. This proposal assembles a multi-institutional team of researchers in computer science and engineering, solid state devices, and medicine. The combined talents of this team will be required to realize the goal of this proposal - small biomedical devices composed of smart sensors that are implanted for long-term use. These devices require the ability to communicate with an external diagnostic computer system via a wireless interface.&lt;br/&gt; A large-scale research program on smart sensors is on-going at Wayne State University, covering all aspects from materials characterization through integrated circuit design and simulation to hybrid device fabrication. This major research initiative requires a multidisciplinary team involving faculty, researchers, and students from the Colleges of Engineering, Science, and Medicine. All are members of the Smart Sensors and Integrated Microsystems (SSIM) research group. The research in this proposal adds a new dimension to the currently funded research of the SSIM program by providing wireless communication capabilities to the implanted microsensors. This additional capability is possible because of the close collaboration among researchers at Wayne State University and Colorado State University.&lt;br/&gt; The proposed work will take an integrated hardware and software approach to developing solutions for wireless networking of human-embedded microsensors. These solutions will be bio-compatible, energy-efficient, fault-tolerant, and scalable. In addition, they would support continuous operation and provide diagnostic capabilities. The proposed work will address several fundamental questions for the wireless networking of embedded microsensors, including those arising due to the need for low-powered, low-maintenance, highly-reliable, and scalable solutions. As a demonstration of our proposed techniques, an artificial retina prosthesis and related visual cortical implant will be developed. The goal is to design wireless network protocols for energy-efficient communication between multiple retinal sensor array/cortical implants and an external base station. The research in this proposal provides the building blocks for this wireless network. The severe limits on the computational and memory capabilities of the smart sensor implants place tight constraints on the communication protocol. For this reason, an external communication device, contained in a pair of eyeglasses, for example, will provide the additional resources necessary for protocol-compliant communication, and increased range and bandwidth. Software to display the message contents will be developed in order to validate the network protocols and the sensor communication. The software to perform image analysis and recognition will be also be developed by our research team. The developed solution will be evaluated, through both simulation and pro-totyping, for various performance and functionality criteria including bio-compatibility, energy-efficiency, reliability, and scalability&lt;br/&gt; Upon completion, the proposed work will have several benefits in the area of wireless networking of low-powered microsensors, which are suitable for biomedical applications. Other biomedical applications where this technology are useful is limited only by our imagination. For example, patients with Parkinson&apos;s disease and epilepsy could benefit from the ability to implant sensors in the neural pathways of the brain to alter the undesired signals and restore proper functioning. Existing technology is very crude and not suitable for chronic implanted devices or complex signal stimulation and detection. Another example is acoustic and optical biosensor arrays for blood analysis currently under development at Wayne State University. Similar sensors are being developed to detect cancer cells by implanting a smart sensor in the body of a recovering cancer patient. One of the main contributions of this project would be a framework for developing scalable wireless networking and powering solutions for biomedical applications.&lt;br/&gt; The integration of advances in wireless networking and smart sensor technology have great potential in several other applications such as the monitoring of distributed environmental sensors. It is envisioned that networked smart sensors will revolutionize our world in ways beyond our current imagination.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86020</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86020</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1406" target="n1409">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Wireless Networking Solutions for Smart Sensor Biomedical Applications</data>
      <data key="e_abstract">Implanted biomedical devices have the potential to revolutionize medicine. The types of procedures that are being proposed could greatly improve the health and vitality of persons in ways previously not possible. Information technology is a critical component of this endeavor, requiring both novel hardware and software design. The limited power and computational capabilities of these biological implants present challenging research issues. As progress is made on these topics, there is great promise of long-term benefits. Multidisciplinary research, drawing on the expertise of researchers in a wide array of areas is required. This proposal assembles a multi-institutional team of researchers in computer science and engineering, solid state devices, and medicine. The combined talents of this team will be required to realize the goal of this proposal - small biomedical devices composed of smart sensors that are implanted for long-term use. These devices require the ability to communicate with an external diagnostic computer system via a wireless interface.&lt;br/&gt; A large-scale research program on smart sensors is on-going at Wayne State University, covering all aspects from materials characterization through integrated circuit design and simulation to hybrid device fabrication. This major research initiative requires a multidisciplinary team involving faculty, researchers, and students from the Colleges of Engineering, Science, and Medicine. All are members of the Smart Sensors and Integrated Microsystems (SSIM) research group. The research in this proposal adds a new dimension to the currently funded research of the SSIM program by providing wireless communication capabilities to the implanted microsensors. This additional capability is possible because of the close collaboration among researchers at Wayne State University and Colorado State University.&lt;br/&gt; The proposed work will take an integrated hardware and software approach to developing solutions for wireless networking of human-embedded microsensors. These solutions will be bio-compatible, energy-efficient, fault-tolerant, and scalable. In addition, they would support continuous operation and provide diagnostic capabilities. The proposed work will address several fundamental questions for the wireless networking of embedded microsensors, including those arising due to the need for low-powered, low-maintenance, highly-reliable, and scalable solutions. As a demonstration of our proposed techniques, an artificial retina prosthesis and related visual cortical implant will be developed. The goal is to design wireless network protocols for energy-efficient communication between multiple retinal sensor array/cortical implants and an external base station. The research in this proposal provides the building blocks for this wireless network. The severe limits on the computational and memory capabilities of the smart sensor implants place tight constraints on the communication protocol. For this reason, an external communication device, contained in a pair of eyeglasses, for example, will provide the additional resources necessary for protocol-compliant communication, and increased range and bandwidth. Software to display the message contents will be developed in order to validate the network protocols and the sensor communication. The software to perform image analysis and recognition will be also be developed by our research team. The developed solution will be evaluated, through both simulation and pro-totyping, for various performance and functionality criteria including bio-compatibility, energy-efficiency, reliability, and scalability&lt;br/&gt; Upon completion, the proposed work will have several benefits in the area of wireless networking of low-powered microsensors, which are suitable for biomedical applications. Other biomedical applications where this technology are useful is limited only by our imagination. For example, patients with Parkinson&apos;s disease and epilepsy could benefit from the ability to implant sensors in the neural pathways of the brain to alter the undesired signals and restore proper functioning. Existing technology is very crude and not suitable for chronic implanted devices or complex signal stimulation and detection. Another example is acoustic and optical biosensor arrays for blood analysis currently under development at Wayne State University. Similar sensors are being developed to detect cancer cells by implanting a smart sensor in the body of a recovering cancer patient. One of the main contributions of this project would be a framework for developing scalable wireless networking and powering solutions for biomedical applications.&lt;br/&gt; The integration of advances in wireless networking and smart sensor technology have great potential in several other applications such as the monitoring of distributed environmental sensors. It is envisioned that networked smart sensors will revolutionize our world in ways beyond our current imagination.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86020</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86020</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1406" target="n1410">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Wireless Networking Solutions for Smart Sensor Biomedical Applications</data>
      <data key="e_abstract">Implanted biomedical devices have the potential to revolutionize medicine. The types of procedures that are being proposed could greatly improve the health and vitality of persons in ways previously not possible. Information technology is a critical component of this endeavor, requiring both novel hardware and software design. The limited power and computational capabilities of these biological implants present challenging research issues. As progress is made on these topics, there is great promise of long-term benefits. Multidisciplinary research, drawing on the expertise of researchers in a wide array of areas is required. This proposal assembles a multi-institutional team of researchers in computer science and engineering, solid state devices, and medicine. The combined talents of this team will be required to realize the goal of this proposal - small biomedical devices composed of smart sensors that are implanted for long-term use. These devices require the ability to communicate with an external diagnostic computer system via a wireless interface.&lt;br/&gt; A large-scale research program on smart sensors is on-going at Wayne State University, covering all aspects from materials characterization through integrated circuit design and simulation to hybrid device fabrication. This major research initiative requires a multidisciplinary team involving faculty, researchers, and students from the Colleges of Engineering, Science, and Medicine. All are members of the Smart Sensors and Integrated Microsystems (SSIM) research group. The research in this proposal adds a new dimension to the currently funded research of the SSIM program by providing wireless communication capabilities to the implanted microsensors. This additional capability is possible because of the close collaboration among researchers at Wayne State University and Colorado State University.&lt;br/&gt; The proposed work will take an integrated hardware and software approach to developing solutions for wireless networking of human-embedded microsensors. These solutions will be bio-compatible, energy-efficient, fault-tolerant, and scalable. In addition, they would support continuous operation and provide diagnostic capabilities. The proposed work will address several fundamental questions for the wireless networking of embedded microsensors, including those arising due to the need for low-powered, low-maintenance, highly-reliable, and scalable solutions. As a demonstration of our proposed techniques, an artificial retina prosthesis and related visual cortical implant will be developed. The goal is to design wireless network protocols for energy-efficient communication between multiple retinal sensor array/cortical implants and an external base station. The research in this proposal provides the building blocks for this wireless network. The severe limits on the computational and memory capabilities of the smart sensor implants place tight constraints on the communication protocol. For this reason, an external communication device, contained in a pair of eyeglasses, for example, will provide the additional resources necessary for protocol-compliant communication, and increased range and bandwidth. Software to display the message contents will be developed in order to validate the network protocols and the sensor communication. The software to perform image analysis and recognition will be also be developed by our research team. The developed solution will be evaluated, through both simulation and pro-totyping, for various performance and functionality criteria including bio-compatibility, energy-efficiency, reliability, and scalability&lt;br/&gt; Upon completion, the proposed work will have several benefits in the area of wireless networking of low-powered microsensors, which are suitable for biomedical applications. Other biomedical applications where this technology are useful is limited only by our imagination. For example, patients with Parkinson&apos;s disease and epilepsy could benefit from the ability to implant sensors in the neural pathways of the brain to alter the undesired signals and restore proper functioning. Existing technology is very crude and not suitable for chronic implanted devices or complex signal stimulation and detection. Another example is acoustic and optical biosensor arrays for blood analysis currently under development at Wayne State University. Similar sensors are being developed to detect cancer cells by implanting a smart sensor in the body of a recovering cancer patient. One of the main contributions of this project would be a framework for developing scalable wireless networking and powering solutions for biomedical applications.&lt;br/&gt; The integration of advances in wireless networking and smart sensor technology have great potential in several other applications such as the monitoring of distributed environmental sensors. It is envisioned that networked smart sensors will revolutionize our world in ways beyond our current imagination.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86020</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86020</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1407" target="n1408">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Wireless Networking Solutions for Smart Sensor Biomedical Applications</data>
      <data key="e_abstract">Implanted biomedical devices have the potential to revolutionize medicine. The types of procedures that are being proposed could greatly improve the health and vitality of persons in ways previously not possible. Information technology is a critical component of this endeavor, requiring both novel hardware and software design. The limited power and computational capabilities of these biological implants present challenging research issues. As progress is made on these topics, there is great promise of long-term benefits. Multidisciplinary research, drawing on the expertise of researchers in a wide array of areas is required. This proposal assembles a multi-institutional team of researchers in computer science and engineering, solid state devices, and medicine. The combined talents of this team will be required to realize the goal of this proposal - small biomedical devices composed of smart sensors that are implanted for long-term use. These devices require the ability to communicate with an external diagnostic computer system via a wireless interface.&lt;br/&gt; A large-scale research program on smart sensors is on-going at Wayne State University, covering all aspects from materials characterization through integrated circuit design and simulation to hybrid device fabrication. This major research initiative requires a multidisciplinary team involving faculty, researchers, and students from the Colleges of Engineering, Science, and Medicine. All are members of the Smart Sensors and Integrated Microsystems (SSIM) research group. The research in this proposal adds a new dimension to the currently funded research of the SSIM program by providing wireless communication capabilities to the implanted microsensors. This additional capability is possible because of the close collaboration among researchers at Wayne State University and Colorado State University.&lt;br/&gt; The proposed work will take an integrated hardware and software approach to developing solutions for wireless networking of human-embedded microsensors. These solutions will be bio-compatible, energy-efficient, fault-tolerant, and scalable. In addition, they would support continuous operation and provide diagnostic capabilities. The proposed work will address several fundamental questions for the wireless networking of embedded microsensors, including those arising due to the need for low-powered, low-maintenance, highly-reliable, and scalable solutions. As a demonstration of our proposed techniques, an artificial retina prosthesis and related visual cortical implant will be developed. The goal is to design wireless network protocols for energy-efficient communication between multiple retinal sensor array/cortical implants and an external base station. The research in this proposal provides the building blocks for this wireless network. The severe limits on the computational and memory capabilities of the smart sensor implants place tight constraints on the communication protocol. For this reason, an external communication device, contained in a pair of eyeglasses, for example, will provide the additional resources necessary for protocol-compliant communication, and increased range and bandwidth. Software to display the message contents will be developed in order to validate the network protocols and the sensor communication. The software to perform image analysis and recognition will be also be developed by our research team. The developed solution will be evaluated, through both simulation and pro-totyping, for various performance and functionality criteria including bio-compatibility, energy-efficiency, reliability, and scalability&lt;br/&gt; Upon completion, the proposed work will have several benefits in the area of wireless networking of low-powered microsensors, which are suitable for biomedical applications. Other biomedical applications where this technology are useful is limited only by our imagination. For example, patients with Parkinson&apos;s disease and epilepsy could benefit from the ability to implant sensors in the neural pathways of the brain to alter the undesired signals and restore proper functioning. Existing technology is very crude and not suitable for chronic implanted devices or complex signal stimulation and detection. Another example is acoustic and optical biosensor arrays for blood analysis currently under development at Wayne State University. Similar sensors are being developed to detect cancer cells by implanting a smart sensor in the body of a recovering cancer patient. One of the main contributions of this project would be a framework for developing scalable wireless networking and powering solutions for biomedical applications.&lt;br/&gt; The integration of advances in wireless networking and smart sensor technology have great potential in several other applications such as the monitoring of distributed environmental sensors. It is envisioned that networked smart sensors will revolutionize our world in ways beyond our current imagination.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86020</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86020</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1407" target="n1409">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Wireless Networking Solutions for Smart Sensor Biomedical Applications</data>
      <data key="e_abstract">Implanted biomedical devices have the potential to revolutionize medicine. The types of procedures that are being proposed could greatly improve the health and vitality of persons in ways previously not possible. Information technology is a critical component of this endeavor, requiring both novel hardware and software design. The limited power and computational capabilities of these biological implants present challenging research issues. As progress is made on these topics, there is great promise of long-term benefits. Multidisciplinary research, drawing on the expertise of researchers in a wide array of areas is required. This proposal assembles a multi-institutional team of researchers in computer science and engineering, solid state devices, and medicine. The combined talents of this team will be required to realize the goal of this proposal - small biomedical devices composed of smart sensors that are implanted for long-term use. These devices require the ability to communicate with an external diagnostic computer system via a wireless interface.&lt;br/&gt; A large-scale research program on smart sensors is on-going at Wayne State University, covering all aspects from materials characterization through integrated circuit design and simulation to hybrid device fabrication. This major research initiative requires a multidisciplinary team involving faculty, researchers, and students from the Colleges of Engineering, Science, and Medicine. All are members of the Smart Sensors and Integrated Microsystems (SSIM) research group. The research in this proposal adds a new dimension to the currently funded research of the SSIM program by providing wireless communication capabilities to the implanted microsensors. This additional capability is possible because of the close collaboration among researchers at Wayne State University and Colorado State University.&lt;br/&gt; The proposed work will take an integrated hardware and software approach to developing solutions for wireless networking of human-embedded microsensors. These solutions will be bio-compatible, energy-efficient, fault-tolerant, and scalable. In addition, they would support continuous operation and provide diagnostic capabilities. The proposed work will address several fundamental questions for the wireless networking of embedded microsensors, including those arising due to the need for low-powered, low-maintenance, highly-reliable, and scalable solutions. As a demonstration of our proposed techniques, an artificial retina prosthesis and related visual cortical implant will be developed. The goal is to design wireless network protocols for energy-efficient communication between multiple retinal sensor array/cortical implants and an external base station. The research in this proposal provides the building blocks for this wireless network. The severe limits on the computational and memory capabilities of the smart sensor implants place tight constraints on the communication protocol. For this reason, an external communication device, contained in a pair of eyeglasses, for example, will provide the additional resources necessary for protocol-compliant communication, and increased range and bandwidth. Software to display the message contents will be developed in order to validate the network protocols and the sensor communication. The software to perform image analysis and recognition will be also be developed by our research team. The developed solution will be evaluated, through both simulation and pro-totyping, for various performance and functionality criteria including bio-compatibility, energy-efficiency, reliability, and scalability&lt;br/&gt; Upon completion, the proposed work will have several benefits in the area of wireless networking of low-powered microsensors, which are suitable for biomedical applications. Other biomedical applications where this technology are useful is limited only by our imagination. For example, patients with Parkinson&apos;s disease and epilepsy could benefit from the ability to implant sensors in the neural pathways of the brain to alter the undesired signals and restore proper functioning. Existing technology is very crude and not suitable for chronic implanted devices or complex signal stimulation and detection. Another example is acoustic and optical biosensor arrays for blood analysis currently under development at Wayne State University. Similar sensors are being developed to detect cancer cells by implanting a smart sensor in the body of a recovering cancer patient. One of the main contributions of this project would be a framework for developing scalable wireless networking and powering solutions for biomedical applications.&lt;br/&gt; The integration of advances in wireless networking and smart sensor technology have great potential in several other applications such as the monitoring of distributed environmental sensors. It is envisioned that networked smart sensors will revolutionize our world in ways beyond our current imagination.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86020</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86020</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1407" target="n1410">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Wireless Networking Solutions for Smart Sensor Biomedical Applications</data>
      <data key="e_abstract">Implanted biomedical devices have the potential to revolutionize medicine. The types of procedures that are being proposed could greatly improve the health and vitality of persons in ways previously not possible. Information technology is a critical component of this endeavor, requiring both novel hardware and software design. The limited power and computational capabilities of these biological implants present challenging research issues. As progress is made on these topics, there is great promise of long-term benefits. Multidisciplinary research, drawing on the expertise of researchers in a wide array of areas is required. This proposal assembles a multi-institutional team of researchers in computer science and engineering, solid state devices, and medicine. The combined talents of this team will be required to realize the goal of this proposal - small biomedical devices composed of smart sensors that are implanted for long-term use. These devices require the ability to communicate with an external diagnostic computer system via a wireless interface.&lt;br/&gt; A large-scale research program on smart sensors is on-going at Wayne State University, covering all aspects from materials characterization through integrated circuit design and simulation to hybrid device fabrication. This major research initiative requires a multidisciplinary team involving faculty, researchers, and students from the Colleges of Engineering, Science, and Medicine. All are members of the Smart Sensors and Integrated Microsystems (SSIM) research group. The research in this proposal adds a new dimension to the currently funded research of the SSIM program by providing wireless communication capabilities to the implanted microsensors. This additional capability is possible because of the close collaboration among researchers at Wayne State University and Colorado State University.&lt;br/&gt; The proposed work will take an integrated hardware and software approach to developing solutions for wireless networking of human-embedded microsensors. These solutions will be bio-compatible, energy-efficient, fault-tolerant, and scalable. In addition, they would support continuous operation and provide diagnostic capabilities. The proposed work will address several fundamental questions for the wireless networking of embedded microsensors, including those arising due to the need for low-powered, low-maintenance, highly-reliable, and scalable solutions. As a demonstration of our proposed techniques, an artificial retina prosthesis and related visual cortical implant will be developed. The goal is to design wireless network protocols for energy-efficient communication between multiple retinal sensor array/cortical implants and an external base station. The research in this proposal provides the building blocks for this wireless network. The severe limits on the computational and memory capabilities of the smart sensor implants place tight constraints on the communication protocol. For this reason, an external communication device, contained in a pair of eyeglasses, for example, will provide the additional resources necessary for protocol-compliant communication, and increased range and bandwidth. Software to display the message contents will be developed in order to validate the network protocols and the sensor communication. The software to perform image analysis and recognition will be also be developed by our research team. The developed solution will be evaluated, through both simulation and pro-totyping, for various performance and functionality criteria including bio-compatibility, energy-efficiency, reliability, and scalability&lt;br/&gt; Upon completion, the proposed work will have several benefits in the area of wireless networking of low-powered microsensors, which are suitable for biomedical applications. Other biomedical applications where this technology are useful is limited only by our imagination. For example, patients with Parkinson&apos;s disease and epilepsy could benefit from the ability to implant sensors in the neural pathways of the brain to alter the undesired signals and restore proper functioning. Existing technology is very crude and not suitable for chronic implanted devices or complex signal stimulation and detection. Another example is acoustic and optical biosensor arrays for blood analysis currently under development at Wayne State University. Similar sensors are being developed to detect cancer cells by implanting a smart sensor in the body of a recovering cancer patient. One of the main contributions of this project would be a framework for developing scalable wireless networking and powering solutions for biomedical applications.&lt;br/&gt; The integration of advances in wireless networking and smart sensor technology have great potential in several other applications such as the monitoring of distributed environmental sensors. It is envisioned that networked smart sensors will revolutionize our world in ways beyond our current imagination.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86020</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86020</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1408" target="n1409">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Wireless Networking Solutions for Smart Sensor Biomedical Applications</data>
      <data key="e_abstract">Implanted biomedical devices have the potential to revolutionize medicine. The types of procedures that are being proposed could greatly improve the health and vitality of persons in ways previously not possible. Information technology is a critical component of this endeavor, requiring both novel hardware and software design. The limited power and computational capabilities of these biological implants present challenging research issues. As progress is made on these topics, there is great promise of long-term benefits. Multidisciplinary research, drawing on the expertise of researchers in a wide array of areas is required. This proposal assembles a multi-institutional team of researchers in computer science and engineering, solid state devices, and medicine. The combined talents of this team will be required to realize the goal of this proposal - small biomedical devices composed of smart sensors that are implanted for long-term use. These devices require the ability to communicate with an external diagnostic computer system via a wireless interface.&lt;br/&gt; A large-scale research program on smart sensors is on-going at Wayne State University, covering all aspects from materials characterization through integrated circuit design and simulation to hybrid device fabrication. This major research initiative requires a multidisciplinary team involving faculty, researchers, and students from the Colleges of Engineering, Science, and Medicine. All are members of the Smart Sensors and Integrated Microsystems (SSIM) research group. The research in this proposal adds a new dimension to the currently funded research of the SSIM program by providing wireless communication capabilities to the implanted microsensors. This additional capability is possible because of the close collaboration among researchers at Wayne State University and Colorado State University.&lt;br/&gt; The proposed work will take an integrated hardware and software approach to developing solutions for wireless networking of human-embedded microsensors. These solutions will be bio-compatible, energy-efficient, fault-tolerant, and scalable. In addition, they would support continuous operation and provide diagnostic capabilities. The proposed work will address several fundamental questions for the wireless networking of embedded microsensors, including those arising due to the need for low-powered, low-maintenance, highly-reliable, and scalable solutions. As a demonstration of our proposed techniques, an artificial retina prosthesis and related visual cortical implant will be developed. The goal is to design wireless network protocols for energy-efficient communication between multiple retinal sensor array/cortical implants and an external base station. The research in this proposal provides the building blocks for this wireless network. The severe limits on the computational and memory capabilities of the smart sensor implants place tight constraints on the communication protocol. For this reason, an external communication device, contained in a pair of eyeglasses, for example, will provide the additional resources necessary for protocol-compliant communication, and increased range and bandwidth. Software to display the message contents will be developed in order to validate the network protocols and the sensor communication. The software to perform image analysis and recognition will be also be developed by our research team. The developed solution will be evaluated, through both simulation and pro-totyping, for various performance and functionality criteria including bio-compatibility, energy-efficiency, reliability, and scalability&lt;br/&gt; Upon completion, the proposed work will have several benefits in the area of wireless networking of low-powered microsensors, which are suitable for biomedical applications. Other biomedical applications where this technology are useful is limited only by our imagination. For example, patients with Parkinson&apos;s disease and epilepsy could benefit from the ability to implant sensors in the neural pathways of the brain to alter the undesired signals and restore proper functioning. Existing technology is very crude and not suitable for chronic implanted devices or complex signal stimulation and detection. Another example is acoustic and optical biosensor arrays for blood analysis currently under development at Wayne State University. Similar sensors are being developed to detect cancer cells by implanting a smart sensor in the body of a recovering cancer patient. One of the main contributions of this project would be a framework for developing scalable wireless networking and powering solutions for biomedical applications.&lt;br/&gt; The integration of advances in wireless networking and smart sensor technology have great potential in several other applications such as the monitoring of distributed environmental sensors. It is envisioned that networked smart sensors will revolutionize our world in ways beyond our current imagination.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86020</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86020</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1408" target="n1410">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Wireless Networking Solutions for Smart Sensor Biomedical Applications</data>
      <data key="e_abstract">Implanted biomedical devices have the potential to revolutionize medicine. The types of procedures that are being proposed could greatly improve the health and vitality of persons in ways previously not possible. Information technology is a critical component of this endeavor, requiring both novel hardware and software design. The limited power and computational capabilities of these biological implants present challenging research issues. As progress is made on these topics, there is great promise of long-term benefits. Multidisciplinary research, drawing on the expertise of researchers in a wide array of areas is required. This proposal assembles a multi-institutional team of researchers in computer science and engineering, solid state devices, and medicine. The combined talents of this team will be required to realize the goal of this proposal - small biomedical devices composed of smart sensors that are implanted for long-term use. These devices require the ability to communicate with an external diagnostic computer system via a wireless interface.&lt;br/&gt; A large-scale research program on smart sensors is on-going at Wayne State University, covering all aspects from materials characterization through integrated circuit design and simulation to hybrid device fabrication. This major research initiative requires a multidisciplinary team involving faculty, researchers, and students from the Colleges of Engineering, Science, and Medicine. All are members of the Smart Sensors and Integrated Microsystems (SSIM) research group. The research in this proposal adds a new dimension to the currently funded research of the SSIM program by providing wireless communication capabilities to the implanted microsensors. This additional capability is possible because of the close collaboration among researchers at Wayne State University and Colorado State University.&lt;br/&gt; The proposed work will take an integrated hardware and software approach to developing solutions for wireless networking of human-embedded microsensors. These solutions will be bio-compatible, energy-efficient, fault-tolerant, and scalable. In addition, they would support continuous operation and provide diagnostic capabilities. The proposed work will address several fundamental questions for the wireless networking of embedded microsensors, including those arising due to the need for low-powered, low-maintenance, highly-reliable, and scalable solutions. As a demonstration of our proposed techniques, an artificial retina prosthesis and related visual cortical implant will be developed. The goal is to design wireless network protocols for energy-efficient communication between multiple retinal sensor array/cortical implants and an external base station. The research in this proposal provides the building blocks for this wireless network. The severe limits on the computational and memory capabilities of the smart sensor implants place tight constraints on the communication protocol. For this reason, an external communication device, contained in a pair of eyeglasses, for example, will provide the additional resources necessary for protocol-compliant communication, and increased range and bandwidth. Software to display the message contents will be developed in order to validate the network protocols and the sensor communication. The software to perform image analysis and recognition will be also be developed by our research team. The developed solution will be evaluated, through both simulation and pro-totyping, for various performance and functionality criteria including bio-compatibility, energy-efficiency, reliability, and scalability&lt;br/&gt; Upon completion, the proposed work will have several benefits in the area of wireless networking of low-powered microsensors, which are suitable for biomedical applications. Other biomedical applications where this technology are useful is limited only by our imagination. For example, patients with Parkinson&apos;s disease and epilepsy could benefit from the ability to implant sensors in the neural pathways of the brain to alter the undesired signals and restore proper functioning. Existing technology is very crude and not suitable for chronic implanted devices or complex signal stimulation and detection. Another example is acoustic and optical biosensor arrays for blood analysis currently under development at Wayne State University. Similar sensors are being developed to detect cancer cells by implanting a smart sensor in the body of a recovering cancer patient. One of the main contributions of this project would be a framework for developing scalable wireless networking and powering solutions for biomedical applications.&lt;br/&gt; The integration of advances in wireless networking and smart sensor technology have great potential in several other applications such as the monitoring of distributed environmental sensors. It is envisioned that networked smart sensors will revolutionize our world in ways beyond our current imagination.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86020</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86020</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1409" target="n1410">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Wireless Networking Solutions for Smart Sensor Biomedical Applications</data>
      <data key="e_abstract">Implanted biomedical devices have the potential to revolutionize medicine. The types of procedures that are being proposed could greatly improve the health and vitality of persons in ways previously not possible. Information technology is a critical component of this endeavor, requiring both novel hardware and software design. The limited power and computational capabilities of these biological implants present challenging research issues. As progress is made on these topics, there is great promise of long-term benefits. Multidisciplinary research, drawing on the expertise of researchers in a wide array of areas is required. This proposal assembles a multi-institutional team of researchers in computer science and engineering, solid state devices, and medicine. The combined talents of this team will be required to realize the goal of this proposal - small biomedical devices composed of smart sensors that are implanted for long-term use. These devices require the ability to communicate with an external diagnostic computer system via a wireless interface.&lt;br/&gt; A large-scale research program on smart sensors is on-going at Wayne State University, covering all aspects from materials characterization through integrated circuit design and simulation to hybrid device fabrication. This major research initiative requires a multidisciplinary team involving faculty, researchers, and students from the Colleges of Engineering, Science, and Medicine. All are members of the Smart Sensors and Integrated Microsystems (SSIM) research group. The research in this proposal adds a new dimension to the currently funded research of the SSIM program by providing wireless communication capabilities to the implanted microsensors. This additional capability is possible because of the close collaboration among researchers at Wayne State University and Colorado State University.&lt;br/&gt; The proposed work will take an integrated hardware and software approach to developing solutions for wireless networking of human-embedded microsensors. These solutions will be bio-compatible, energy-efficient, fault-tolerant, and scalable. In addition, they would support continuous operation and provide diagnostic capabilities. The proposed work will address several fundamental questions for the wireless networking of embedded microsensors, including those arising due to the need for low-powered, low-maintenance, highly-reliable, and scalable solutions. As a demonstration of our proposed techniques, an artificial retina prosthesis and related visual cortical implant will be developed. The goal is to design wireless network protocols for energy-efficient communication between multiple retinal sensor array/cortical implants and an external base station. The research in this proposal provides the building blocks for this wireless network. The severe limits on the computational and memory capabilities of the smart sensor implants place tight constraints on the communication protocol. For this reason, an external communication device, contained in a pair of eyeglasses, for example, will provide the additional resources necessary for protocol-compliant communication, and increased range and bandwidth. Software to display the message contents will be developed in order to validate the network protocols and the sensor communication. The software to perform image analysis and recognition will be also be developed by our research team. The developed solution will be evaluated, through both simulation and pro-totyping, for various performance and functionality criteria including bio-compatibility, energy-efficiency, reliability, and scalability&lt;br/&gt; Upon completion, the proposed work will have several benefits in the area of wireless networking of low-powered microsensors, which are suitable for biomedical applications. Other biomedical applications where this technology are useful is limited only by our imagination. For example, patients with Parkinson&apos;s disease and epilepsy could benefit from the ability to implant sensors in the neural pathways of the brain to alter the undesired signals and restore proper functioning. Existing technology is very crude and not suitable for chronic implanted devices or complex signal stimulation and detection. Another example is acoustic and optical biosensor arrays for blood analysis currently under development at Wayne State University. Similar sensors are being developed to detect cancer cells by implanting a smart sensor in the body of a recovering cancer patient. One of the main contributions of this project would be a framework for developing scalable wireless networking and powering solutions for biomedical applications.&lt;br/&gt; The integration of advances in wireless networking and smart sensor technology have great potential in several other applications such as the monitoring of distributed environmental sensors. It is envisioned that networked smart sensors will revolutionize our world in ways beyond our current imagination.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86020</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86020</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1411" target="n1412">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Acquisition of a Complete Whole Arm Manipulator (WAM) Robot System</data>
      <data key="e_abstract">EIA-0079830&lt;br/&gt;Desai, Jaydev P.&lt;br/&gt;Drexel University&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Complete Whole Arm Manipulator (WAM) Robot System&lt;br/&gt;&lt;br/&gt;The goal of this work is to acquire a whole arm manipulator (WAM) robot which will greatly enhance the development of the medical robotics program, combining haptic and visual information for object exploration in unstructured environment. The robot will also be an invaluable tool for developing and testing novel nonlinear control models at Drexel University. The equipment will also have significant impact on the undergraduate and graduate education at Drexel in the mentioned research areas.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79830</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79830</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1411" target="n1413">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Acquisition of a Complete Whole Arm Manipulator (WAM) Robot System</data>
      <data key="e_abstract">EIA-0079830&lt;br/&gt;Desai, Jaydev P.&lt;br/&gt;Drexel University&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Complete Whole Arm Manipulator (WAM) Robot System&lt;br/&gt;&lt;br/&gt;The goal of this work is to acquire a whole arm manipulator (WAM) robot which will greatly enhance the development of the medical robotics program, combining haptic and visual information for object exploration in unstructured environment. The robot will also be an invaluable tool for developing and testing novel nonlinear control models at Drexel University. The equipment will also have significant impact on the undergraduate and graduate education at Drexel in the mentioned research areas.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79830</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79830</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1411" target="n1414">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Acquisition of a Complete Whole Arm Manipulator (WAM) Robot System</data>
      <data key="e_abstract">EIA-0079830&lt;br/&gt;Desai, Jaydev P.&lt;br/&gt;Drexel University&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Complete Whole Arm Manipulator (WAM) Robot System&lt;br/&gt;&lt;br/&gt;The goal of this work is to acquire a whole arm manipulator (WAM) robot which will greatly enhance the development of the medical robotics program, combining haptic and visual information for object exploration in unstructured environment. The robot will also be an invaluable tool for developing and testing novel nonlinear control models at Drexel University. The equipment will also have significant impact on the undergraduate and graduate education at Drexel in the mentioned research areas.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79830</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79830</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1411" target="n1415">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Acquisition of a Complete Whole Arm Manipulator (WAM) Robot System</data>
      <data key="e_abstract">EIA-0079830&lt;br/&gt;Desai, Jaydev P.&lt;br/&gt;Drexel University&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Complete Whole Arm Manipulator (WAM) Robot System&lt;br/&gt;&lt;br/&gt;The goal of this work is to acquire a whole arm manipulator (WAM) robot which will greatly enhance the development of the medical robotics program, combining haptic and visual information for object exploration in unstructured environment. The robot will also be an invaluable tool for developing and testing novel nonlinear control models at Drexel University. The equipment will also have significant impact on the undergraduate and graduate education at Drexel in the mentioned research areas.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79830</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79830</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1412" target="n1413">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Acquisition of a Complete Whole Arm Manipulator (WAM) Robot System</data>
      <data key="e_abstract">EIA-0079830&lt;br/&gt;Desai, Jaydev P.&lt;br/&gt;Drexel University&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Complete Whole Arm Manipulator (WAM) Robot System&lt;br/&gt;&lt;br/&gt;The goal of this work is to acquire a whole arm manipulator (WAM) robot which will greatly enhance the development of the medical robotics program, combining haptic and visual information for object exploration in unstructured environment. The robot will also be an invaluable tool for developing and testing novel nonlinear control models at Drexel University. The equipment will also have significant impact on the undergraduate and graduate education at Drexel in the mentioned research areas.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79830</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79830</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1412" target="n1414">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Acquisition of a Complete Whole Arm Manipulator (WAM) Robot System</data>
      <data key="e_abstract">EIA-0079830&lt;br/&gt;Desai, Jaydev P.&lt;br/&gt;Drexel University&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Complete Whole Arm Manipulator (WAM) Robot System&lt;br/&gt;&lt;br/&gt;The goal of this work is to acquire a whole arm manipulator (WAM) robot which will greatly enhance the development of the medical robotics program, combining haptic and visual information for object exploration in unstructured environment. The robot will also be an invaluable tool for developing and testing novel nonlinear control models at Drexel University. The equipment will also have significant impact on the undergraduate and graduate education at Drexel in the mentioned research areas.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79830</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79830</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1412" target="n1415">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Acquisition of a Complete Whole Arm Manipulator (WAM) Robot System</data>
      <data key="e_abstract">EIA-0079830&lt;br/&gt;Desai, Jaydev P.&lt;br/&gt;Drexel University&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Complete Whole Arm Manipulator (WAM) Robot System&lt;br/&gt;&lt;br/&gt;The goal of this work is to acquire a whole arm manipulator (WAM) robot which will greatly enhance the development of the medical robotics program, combining haptic and visual information for object exploration in unstructured environment. The robot will also be an invaluable tool for developing and testing novel nonlinear control models at Drexel University. The equipment will also have significant impact on the undergraduate and graduate education at Drexel in the mentioned research areas.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79830</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79830</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1413" target="n1414">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Acquisition of a Complete Whole Arm Manipulator (WAM) Robot System</data>
      <data key="e_abstract">EIA-0079830&lt;br/&gt;Desai, Jaydev P.&lt;br/&gt;Drexel University&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Complete Whole Arm Manipulator (WAM) Robot System&lt;br/&gt;&lt;br/&gt;The goal of this work is to acquire a whole arm manipulator (WAM) robot which will greatly enhance the development of the medical robotics program, combining haptic and visual information for object exploration in unstructured environment. The robot will also be an invaluable tool for developing and testing novel nonlinear control models at Drexel University. The equipment will also have significant impact on the undergraduate and graduate education at Drexel in the mentioned research areas.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79830</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79830</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1413" target="n1415">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Acquisition of a Complete Whole Arm Manipulator (WAM) Robot System</data>
      <data key="e_abstract">EIA-0079830&lt;br/&gt;Desai, Jaydev P.&lt;br/&gt;Drexel University&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Complete Whole Arm Manipulator (WAM) Robot System&lt;br/&gt;&lt;br/&gt;The goal of this work is to acquire a whole arm manipulator (WAM) robot which will greatly enhance the development of the medical robotics program, combining haptic and visual information for object exploration in unstructured environment. The robot will also be an invaluable tool for developing and testing novel nonlinear control models at Drexel University. The equipment will also have significant impact on the undergraduate and graduate education at Drexel in the mentioned research areas.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79830</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79830</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1414" target="n1415">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Acquisition of a Complete Whole Arm Manipulator (WAM) Robot System</data>
      <data key="e_abstract">EIA-0079830&lt;br/&gt;Desai, Jaydev P.&lt;br/&gt;Drexel University&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of a Complete Whole Arm Manipulator (WAM) Robot System&lt;br/&gt;&lt;br/&gt;The goal of this work is to acquire a whole arm manipulator (WAM) robot which will greatly enhance the development of the medical robotics program, combining haptic and visual information for object exploration in unstructured environment. The robot will also be an invaluable tool for developing and testing novel nonlinear control models at Drexel University. The equipment will also have significant impact on the undergraduate and graduate education at Drexel in the mentioned research areas.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79830</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79830</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1416" target="n1417">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Self-Assembly of DNA Nano-Scale Structures for Computation</data>
      <data key="e_abstract">EIA-0086015&lt;br/&gt;Reif, John H. &lt;br/&gt;Duke University&lt;br/&gt;&lt;br/&gt;ITR: Self-Assembly of DNA Nano-Scale Structures for Computation&lt;br/&gt;&lt;br/&gt;This project is a multi-institutional effort to investigate the use of DNA self-assembly to do massively parallel computations at the molecular scale. The main goal is to develop a proof-of-concept demonstration of the application of DNA self-assembly to various basic computational tasks, such as sequences of arithmetic and logical computations executed in massively parallel fashion, and the application of this method to computational problems such as integer factorization. The research involves testing of various input/output methods, development of novel DNA tiles with properties that facilitate the self-assembly and their visualization by imaging devices such as an atomic force microscope, and methods to minimize errors in self-assembly. Specifically, the experiments are being conducted to evaluate the speed and error rates of the various types of self-assembly reactions. In addition, experimental testing of massively parallel DNA self-assembly on particular problems, such as arithmetic and Boolean vector operations, is being performed. Some of the technological areas that could be impacted are the application of DNA Lattices as a substrate for surface chemistry and as a substrate for layout of nano-scale circuit components, the construction of 3D DNA lattices, and DNA motors and their application to DNA computations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86015</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86015</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1416" target="n1418">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Self-Assembly of DNA Nano-Scale Structures for Computation</data>
      <data key="e_abstract">EIA-0086015&lt;br/&gt;Reif, John H. &lt;br/&gt;Duke University&lt;br/&gt;&lt;br/&gt;ITR: Self-Assembly of DNA Nano-Scale Structures for Computation&lt;br/&gt;&lt;br/&gt;This project is a multi-institutional effort to investigate the use of DNA self-assembly to do massively parallel computations at the molecular scale. The main goal is to develop a proof-of-concept demonstration of the application of DNA self-assembly to various basic computational tasks, such as sequences of arithmetic and logical computations executed in massively parallel fashion, and the application of this method to computational problems such as integer factorization. The research involves testing of various input/output methods, development of novel DNA tiles with properties that facilitate the self-assembly and their visualization by imaging devices such as an atomic force microscope, and methods to minimize errors in self-assembly. Specifically, the experiments are being conducted to evaluate the speed and error rates of the various types of self-assembly reactions. In addition, experimental testing of massively parallel DNA self-assembly on particular problems, such as arithmetic and Boolean vector operations, is being performed. Some of the technological areas that could be impacted are the application of DNA Lattices as a substrate for surface chemistry and as a substrate for layout of nano-scale circuit components, the construction of 3D DNA lattices, and DNA motors and their application to DNA computations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86015</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86015</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n520" target="n1416">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Self-Assembly of DNA Nano-Scale Structures for Computation</data>
      <data key="e_abstract">EIA-0086015&lt;br/&gt;Reif, John H. &lt;br/&gt;Duke University&lt;br/&gt;&lt;br/&gt;ITR: Self-Assembly of DNA Nano-Scale Structures for Computation&lt;br/&gt;&lt;br/&gt;This project is a multi-institutional effort to investigate the use of DNA self-assembly to do massively parallel computations at the molecular scale. The main goal is to develop a proof-of-concept demonstration of the application of DNA self-assembly to various basic computational tasks, such as sequences of arithmetic and logical computations executed in massively parallel fashion, and the application of this method to computational problems such as integer factorization. The research involves testing of various input/output methods, development of novel DNA tiles with properties that facilitate the self-assembly and their visualization by imaging devices such as an atomic force microscope, and methods to minimize errors in self-assembly. Specifically, the experiments are being conducted to evaluate the speed and error rates of the various types of self-assembly reactions. In addition, experimental testing of massively parallel DNA self-assembly on particular problems, such as arithmetic and Boolean vector operations, is being performed. Some of the technological areas that could be impacted are the application of DNA Lattices as a substrate for surface chemistry and as a substrate for layout of nano-scale circuit components, the construction of 3D DNA lattices, and DNA motors and their application to DNA computations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86015</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86015</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1417" target="n1418">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Self-Assembly of DNA Nano-Scale Structures for Computation</data>
      <data key="e_abstract">EIA-0086015&lt;br/&gt;Reif, John H. &lt;br/&gt;Duke University&lt;br/&gt;&lt;br/&gt;ITR: Self-Assembly of DNA Nano-Scale Structures for Computation&lt;br/&gt;&lt;br/&gt;This project is a multi-institutional effort to investigate the use of DNA self-assembly to do massively parallel computations at the molecular scale. The main goal is to develop a proof-of-concept demonstration of the application of DNA self-assembly to various basic computational tasks, such as sequences of arithmetic and logical computations executed in massively parallel fashion, and the application of this method to computational problems such as integer factorization. The research involves testing of various input/output methods, development of novel DNA tiles with properties that facilitate the self-assembly and their visualization by imaging devices such as an atomic force microscope, and methods to minimize errors in self-assembly. Specifically, the experiments are being conducted to evaluate the speed and error rates of the various types of self-assembly reactions. In addition, experimental testing of massively parallel DNA self-assembly on particular problems, such as arithmetic and Boolean vector operations, is being performed. Some of the technological areas that could be impacted are the application of DNA Lattices as a substrate for surface chemistry and as a substrate for layout of nano-scale circuit components, the construction of 3D DNA lattices, and DNA motors and their application to DNA computations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86015</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86015</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n520" target="n1417">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Self-Assembly of DNA Nano-Scale Structures for Computation</data>
      <data key="e_abstract">EIA-0086015&lt;br/&gt;Reif, John H. &lt;br/&gt;Duke University&lt;br/&gt;&lt;br/&gt;ITR: Self-Assembly of DNA Nano-Scale Structures for Computation&lt;br/&gt;&lt;br/&gt;This project is a multi-institutional effort to investigate the use of DNA self-assembly to do massively parallel computations at the molecular scale. The main goal is to develop a proof-of-concept demonstration of the application of DNA self-assembly to various basic computational tasks, such as sequences of arithmetic and logical computations executed in massively parallel fashion, and the application of this method to computational problems such as integer factorization. The research involves testing of various input/output methods, development of novel DNA tiles with properties that facilitate the self-assembly and their visualization by imaging devices such as an atomic force microscope, and methods to minimize errors in self-assembly. Specifically, the experiments are being conducted to evaluate the speed and error rates of the various types of self-assembly reactions. In addition, experimental testing of massively parallel DNA self-assembly on particular problems, such as arithmetic and Boolean vector operations, is being performed. Some of the technological areas that could be impacted are the application of DNA Lattices as a substrate for surface chemistry and as a substrate for layout of nano-scale circuit components, the construction of 3D DNA lattices, and DNA motors and their application to DNA computations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86015</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86015</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n520" target="n1418">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Self-Assembly of DNA Nano-Scale Structures for Computation</data>
      <data key="e_abstract">EIA-0086015&lt;br/&gt;Reif, John H. &lt;br/&gt;Duke University&lt;br/&gt;&lt;br/&gt;ITR: Self-Assembly of DNA Nano-Scale Structures for Computation&lt;br/&gt;&lt;br/&gt;This project is a multi-institutional effort to investigate the use of DNA self-assembly to do massively parallel computations at the molecular scale. The main goal is to develop a proof-of-concept demonstration of the application of DNA self-assembly to various basic computational tasks, such as sequences of arithmetic and logical computations executed in massively parallel fashion, and the application of this method to computational problems such as integer factorization. The research involves testing of various input/output methods, development of novel DNA tiles with properties that facilitate the self-assembly and their visualization by imaging devices such as an atomic force microscope, and methods to minimize errors in self-assembly. Specifically, the experiments are being conducted to evaluate the speed and error rates of the various types of self-assembly reactions. In addition, experimental testing of massively parallel DNA self-assembly on particular problems, such as arithmetic and Boolean vector operations, is being performed. Some of the technological areas that could be impacted are the application of DNA Lattices as a substrate for surface chemistry and as a substrate for layout of nano-scale circuit components, the construction of 3D DNA lattices, and DNA motors and their application to DNA computations.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86015</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86015</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1420" target="n1421">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Development of the Distributed Teravoxel Data System: Acquisition, Networking, Archiving, Analysis, and Visualization</data>
      <data key="e_abstract">EIA-0079871&lt;br/&gt;Dimotakis, Paul E.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;MRI: Development of the Distributed Teravoxel Data System: Acquisition, Networking, Archiving, Analysis, and Visualization&lt;br/&gt;&lt;br/&gt;This project addresses the problem of handling very large datasets through the development of generic acquisition and processing capabilities to be hosted at Caltech and made available to both Caltech and outside collaborators. Driven by investigations of flow turbulence, the project is designed to handle both laboratory and numerical-simulation data. The infrastructure will support analysis and visualization of terascale experiment or simulation datasets and their comparison to validate theories and simulation results.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79871</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79871</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1420" target="n1422">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Development of the Distributed Teravoxel Data System: Acquisition, Networking, Archiving, Analysis, and Visualization</data>
      <data key="e_abstract">EIA-0079871&lt;br/&gt;Dimotakis, Paul E.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;MRI: Development of the Distributed Teravoxel Data System: Acquisition, Networking, Archiving, Analysis, and Visualization&lt;br/&gt;&lt;br/&gt;This project addresses the problem of handling very large datasets through the development of generic acquisition and processing capabilities to be hosted at Caltech and made available to both Caltech and outside collaborators. Driven by investigations of flow turbulence, the project is designed to handle both laboratory and numerical-simulation data. The infrastructure will support analysis and visualization of terascale experiment or simulation datasets and their comparison to validate theories and simulation results.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79871</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79871</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1420" target="n1423">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Development of the Distributed Teravoxel Data System: Acquisition, Networking, Archiving, Analysis, and Visualization</data>
      <data key="e_abstract">EIA-0079871&lt;br/&gt;Dimotakis, Paul E.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;MRI: Development of the Distributed Teravoxel Data System: Acquisition, Networking, Archiving, Analysis, and Visualization&lt;br/&gt;&lt;br/&gt;This project addresses the problem of handling very large datasets through the development of generic acquisition and processing capabilities to be hosted at Caltech and made available to both Caltech and outside collaborators. Driven by investigations of flow turbulence, the project is designed to handle both laboratory and numerical-simulation data. The infrastructure will support analysis and visualization of terascale experiment or simulation datasets and their comparison to validate theories and simulation results.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79871</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79871</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1420" target="n1424">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Development of the Distributed Teravoxel Data System: Acquisition, Networking, Archiving, Analysis, and Visualization</data>
      <data key="e_abstract">EIA-0079871&lt;br/&gt;Dimotakis, Paul E.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;MRI: Development of the Distributed Teravoxel Data System: Acquisition, Networking, Archiving, Analysis, and Visualization&lt;br/&gt;&lt;br/&gt;This project addresses the problem of handling very large datasets through the development of generic acquisition and processing capabilities to be hosted at Caltech and made available to both Caltech and outside collaborators. Driven by investigations of flow turbulence, the project is designed to handle both laboratory and numerical-simulation data. The infrastructure will support analysis and visualization of terascale experiment or simulation datasets and their comparison to validate theories and simulation results.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79871</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79871</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1421" target="n1422">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Development of the Distributed Teravoxel Data System: Acquisition, Networking, Archiving, Analysis, and Visualization</data>
      <data key="e_abstract">EIA-0079871&lt;br/&gt;Dimotakis, Paul E.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;MRI: Development of the Distributed Teravoxel Data System: Acquisition, Networking, Archiving, Analysis, and Visualization&lt;br/&gt;&lt;br/&gt;This project addresses the problem of handling very large datasets through the development of generic acquisition and processing capabilities to be hosted at Caltech and made available to both Caltech and outside collaborators. Driven by investigations of flow turbulence, the project is designed to handle both laboratory and numerical-simulation data. The infrastructure will support analysis and visualization of terascale experiment or simulation datasets and their comparison to validate theories and simulation results.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79871</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79871</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1421" target="n1423">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Development of the Distributed Teravoxel Data System: Acquisition, Networking, Archiving, Analysis, and Visualization</data>
      <data key="e_abstract">EIA-0079871&lt;br/&gt;Dimotakis, Paul E.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;MRI: Development of the Distributed Teravoxel Data System: Acquisition, Networking, Archiving, Analysis, and Visualization&lt;br/&gt;&lt;br/&gt;This project addresses the problem of handling very large datasets through the development of generic acquisition and processing capabilities to be hosted at Caltech and made available to both Caltech and outside collaborators. Driven by investigations of flow turbulence, the project is designed to handle both laboratory and numerical-simulation data. The infrastructure will support analysis and visualization of terascale experiment or simulation datasets and their comparison to validate theories and simulation results.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79871</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79871</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1421" target="n1424">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Development of the Distributed Teravoxel Data System: Acquisition, Networking, Archiving, Analysis, and Visualization</data>
      <data key="e_abstract">EIA-0079871&lt;br/&gt;Dimotakis, Paul E.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;MRI: Development of the Distributed Teravoxel Data System: Acquisition, Networking, Archiving, Analysis, and Visualization&lt;br/&gt;&lt;br/&gt;This project addresses the problem of handling very large datasets through the development of generic acquisition and processing capabilities to be hosted at Caltech and made available to both Caltech and outside collaborators. Driven by investigations of flow turbulence, the project is designed to handle both laboratory and numerical-simulation data. The infrastructure will support analysis and visualization of terascale experiment or simulation datasets and their comparison to validate theories and simulation results.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79871</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79871</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1422" target="n1423">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Development of the Distributed Teravoxel Data System: Acquisition, Networking, Archiving, Analysis, and Visualization</data>
      <data key="e_abstract">EIA-0079871&lt;br/&gt;Dimotakis, Paul E.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;MRI: Development of the Distributed Teravoxel Data System: Acquisition, Networking, Archiving, Analysis, and Visualization&lt;br/&gt;&lt;br/&gt;This project addresses the problem of handling very large datasets through the development of generic acquisition and processing capabilities to be hosted at Caltech and made available to both Caltech and outside collaborators. Driven by investigations of flow turbulence, the project is designed to handle both laboratory and numerical-simulation data. The infrastructure will support analysis and visualization of terascale experiment or simulation datasets and their comparison to validate theories and simulation results.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79871</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79871</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1422" target="n1424">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Development of the Distributed Teravoxel Data System: Acquisition, Networking, Archiving, Analysis, and Visualization</data>
      <data key="e_abstract">EIA-0079871&lt;br/&gt;Dimotakis, Paul E.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;MRI: Development of the Distributed Teravoxel Data System: Acquisition, Networking, Archiving, Analysis, and Visualization&lt;br/&gt;&lt;br/&gt;This project addresses the problem of handling very large datasets through the development of generic acquisition and processing capabilities to be hosted at Caltech and made available to both Caltech and outside collaborators. Driven by investigations of flow turbulence, the project is designed to handle both laboratory and numerical-simulation data. The infrastructure will support analysis and visualization of terascale experiment or simulation datasets and their comparison to validate theories and simulation results.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79871</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79871</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1423" target="n1424">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Development of the Distributed Teravoxel Data System: Acquisition, Networking, Archiving, Analysis, and Visualization</data>
      <data key="e_abstract">EIA-0079871&lt;br/&gt;Dimotakis, Paul E.&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;MRI: Development of the Distributed Teravoxel Data System: Acquisition, Networking, Archiving, Analysis, and Visualization&lt;br/&gt;&lt;br/&gt;This project addresses the problem of handling very large datasets through the development of generic acquisition and processing capabilities to be hosted at Caltech and made available to both Caltech and outside collaborators. Driven by investigations of flow turbulence, the project is designed to handle both laboratory and numerical-simulation data. The infrastructure will support analysis and visualization of terascale experiment or simulation datasets and their comparison to validate theories and simulation results.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79871</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">79871</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1425" target="n1427">
      <data key="e_effectiveDate">2000-09-16</data>
      <data key="e_title">Stretchable Architecture and Application-Driven Spectrum Allocation for 4G Wireless Networks</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">4097</data>
      <data key="e_label">196042</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">196042</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1433" target="n1434">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Expertise Coordination and Information Technology in High Velocity Work Environments</data>
      <data key="e_abstract">High velocity work environments such as medical trauma centers depend on robust and efficient coordination by team members to bring together appropriate knowledge and skills, or expertise. This project investigates the existing and potential role of information technology (IT) in expertise coordination in a type of high velocity, high outcome work environment: level-I trauma centers in this country. A combination of qualitative and quantitative methods are used to collect observational and survey data in a range of trauma centers regarding the expertise needs of trauma medical teams, the points of expertise needs, the modes of acquiring needed expertise, the sources of information related to work and expertise coordination, and uses of IT for the coordination of needed expertise, as well as broader organizational variables. The project will contribute to the development of guiding principles in the design of next generation IT for high velocity work environments in the areas of user interfaces, coordination modes, and the evaluation of IT on coordination. It will contribute to the understanding of expertise coordination and how IT affects these processes. Finally, the project will enhance an existing multi-disciplinary research program on information and coordination, particularly as it relates to issues in emergency trauma care.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81868</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81868</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1433" target="n1435">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Expertise Coordination and Information Technology in High Velocity Work Environments</data>
      <data key="e_abstract">High velocity work environments such as medical trauma centers depend on robust and efficient coordination by team members to bring together appropriate knowledge and skills, or expertise. This project investigates the existing and potential role of information technology (IT) in expertise coordination in a type of high velocity, high outcome work environment: level-I trauma centers in this country. A combination of qualitative and quantitative methods are used to collect observational and survey data in a range of trauma centers regarding the expertise needs of trauma medical teams, the points of expertise needs, the modes of acquiring needed expertise, the sources of information related to work and expertise coordination, and uses of IT for the coordination of needed expertise, as well as broader organizational variables. The project will contribute to the development of guiding principles in the design of next generation IT for high velocity work environments in the areas of user interfaces, coordination modes, and the evaluation of IT on coordination. It will contribute to the understanding of expertise coordination and how IT affects these processes. Finally, the project will enhance an existing multi-disciplinary research program on information and coordination, particularly as it relates to issues in emergency trauma care.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81868</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81868</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1433" target="n1436">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Expertise Coordination and Information Technology in High Velocity Work Environments</data>
      <data key="e_abstract">High velocity work environments such as medical trauma centers depend on robust and efficient coordination by team members to bring together appropriate knowledge and skills, or expertise. This project investigates the existing and potential role of information technology (IT) in expertise coordination in a type of high velocity, high outcome work environment: level-I trauma centers in this country. A combination of qualitative and quantitative methods are used to collect observational and survey data in a range of trauma centers regarding the expertise needs of trauma medical teams, the points of expertise needs, the modes of acquiring needed expertise, the sources of information related to work and expertise coordination, and uses of IT for the coordination of needed expertise, as well as broader organizational variables. The project will contribute to the development of guiding principles in the design of next generation IT for high velocity work environments in the areas of user interfaces, coordination modes, and the evaluation of IT on coordination. It will contribute to the understanding of expertise coordination and how IT affects these processes. Finally, the project will enhance an existing multi-disciplinary research program on information and coordination, particularly as it relates to issues in emergency trauma care.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81868</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81868</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1433" target="n1437">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Expertise Coordination and Information Technology in High Velocity Work Environments</data>
      <data key="e_abstract">High velocity work environments such as medical trauma centers depend on robust and efficient coordination by team members to bring together appropriate knowledge and skills, or expertise. This project investigates the existing and potential role of information technology (IT) in expertise coordination in a type of high velocity, high outcome work environment: level-I trauma centers in this country. A combination of qualitative and quantitative methods are used to collect observational and survey data in a range of trauma centers regarding the expertise needs of trauma medical teams, the points of expertise needs, the modes of acquiring needed expertise, the sources of information related to work and expertise coordination, and uses of IT for the coordination of needed expertise, as well as broader organizational variables. The project will contribute to the development of guiding principles in the design of next generation IT for high velocity work environments in the areas of user interfaces, coordination modes, and the evaluation of IT on coordination. It will contribute to the understanding of expertise coordination and how IT affects these processes. Finally, the project will enhance an existing multi-disciplinary research program on information and coordination, particularly as it relates to issues in emergency trauma care.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81868</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81868</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1434" target="n1435">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Expertise Coordination and Information Technology in High Velocity Work Environments</data>
      <data key="e_abstract">High velocity work environments such as medical trauma centers depend on robust and efficient coordination by team members to bring together appropriate knowledge and skills, or expertise. This project investigates the existing and potential role of information technology (IT) in expertise coordination in a type of high velocity, high outcome work environment: level-I trauma centers in this country. A combination of qualitative and quantitative methods are used to collect observational and survey data in a range of trauma centers regarding the expertise needs of trauma medical teams, the points of expertise needs, the modes of acquiring needed expertise, the sources of information related to work and expertise coordination, and uses of IT for the coordination of needed expertise, as well as broader organizational variables. The project will contribute to the development of guiding principles in the design of next generation IT for high velocity work environments in the areas of user interfaces, coordination modes, and the evaluation of IT on coordination. It will contribute to the understanding of expertise coordination and how IT affects these processes. Finally, the project will enhance an existing multi-disciplinary research program on information and coordination, particularly as it relates to issues in emergency trauma care.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81868</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81868</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1434" target="n1436">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Expertise Coordination and Information Technology in High Velocity Work Environments</data>
      <data key="e_abstract">High velocity work environments such as medical trauma centers depend on robust and efficient coordination by team members to bring together appropriate knowledge and skills, or expertise. This project investigates the existing and potential role of information technology (IT) in expertise coordination in a type of high velocity, high outcome work environment: level-I trauma centers in this country. A combination of qualitative and quantitative methods are used to collect observational and survey data in a range of trauma centers regarding the expertise needs of trauma medical teams, the points of expertise needs, the modes of acquiring needed expertise, the sources of information related to work and expertise coordination, and uses of IT for the coordination of needed expertise, as well as broader organizational variables. The project will contribute to the development of guiding principles in the design of next generation IT for high velocity work environments in the areas of user interfaces, coordination modes, and the evaluation of IT on coordination. It will contribute to the understanding of expertise coordination and how IT affects these processes. Finally, the project will enhance an existing multi-disciplinary research program on information and coordination, particularly as it relates to issues in emergency trauma care.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81868</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81868</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1434" target="n1437">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Expertise Coordination and Information Technology in High Velocity Work Environments</data>
      <data key="e_abstract">High velocity work environments such as medical trauma centers depend on robust and efficient coordination by team members to bring together appropriate knowledge and skills, or expertise. This project investigates the existing and potential role of information technology (IT) in expertise coordination in a type of high velocity, high outcome work environment: level-I trauma centers in this country. A combination of qualitative and quantitative methods are used to collect observational and survey data in a range of trauma centers regarding the expertise needs of trauma medical teams, the points of expertise needs, the modes of acquiring needed expertise, the sources of information related to work and expertise coordination, and uses of IT for the coordination of needed expertise, as well as broader organizational variables. The project will contribute to the development of guiding principles in the design of next generation IT for high velocity work environments in the areas of user interfaces, coordination modes, and the evaluation of IT on coordination. It will contribute to the understanding of expertise coordination and how IT affects these processes. Finally, the project will enhance an existing multi-disciplinary research program on information and coordination, particularly as it relates to issues in emergency trauma care.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81868</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81868</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1435" target="n1436">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Expertise Coordination and Information Technology in High Velocity Work Environments</data>
      <data key="e_abstract">High velocity work environments such as medical trauma centers depend on robust and efficient coordination by team members to bring together appropriate knowledge and skills, or expertise. This project investigates the existing and potential role of information technology (IT) in expertise coordination in a type of high velocity, high outcome work environment: level-I trauma centers in this country. A combination of qualitative and quantitative methods are used to collect observational and survey data in a range of trauma centers regarding the expertise needs of trauma medical teams, the points of expertise needs, the modes of acquiring needed expertise, the sources of information related to work and expertise coordination, and uses of IT for the coordination of needed expertise, as well as broader organizational variables. The project will contribute to the development of guiding principles in the design of next generation IT for high velocity work environments in the areas of user interfaces, coordination modes, and the evaluation of IT on coordination. It will contribute to the understanding of expertise coordination and how IT affects these processes. Finally, the project will enhance an existing multi-disciplinary research program on information and coordination, particularly as it relates to issues in emergency trauma care.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81868</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81868</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1435" target="n1437">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Expertise Coordination and Information Technology in High Velocity Work Environments</data>
      <data key="e_abstract">High velocity work environments such as medical trauma centers depend on robust and efficient coordination by team members to bring together appropriate knowledge and skills, or expertise. This project investigates the existing and potential role of information technology (IT) in expertise coordination in a type of high velocity, high outcome work environment: level-I trauma centers in this country. A combination of qualitative and quantitative methods are used to collect observational and survey data in a range of trauma centers regarding the expertise needs of trauma medical teams, the points of expertise needs, the modes of acquiring needed expertise, the sources of information related to work and expertise coordination, and uses of IT for the coordination of needed expertise, as well as broader organizational variables. The project will contribute to the development of guiding principles in the design of next generation IT for high velocity work environments in the areas of user interfaces, coordination modes, and the evaluation of IT on coordination. It will contribute to the understanding of expertise coordination and how IT affects these processes. Finally, the project will enhance an existing multi-disciplinary research program on information and coordination, particularly as it relates to issues in emergency trauma care.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81868</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81868</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1436" target="n1437">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Expertise Coordination and Information Technology in High Velocity Work Environments</data>
      <data key="e_abstract">High velocity work environments such as medical trauma centers depend on robust and efficient coordination by team members to bring together appropriate knowledge and skills, or expertise. This project investigates the existing and potential role of information technology (IT) in expertise coordination in a type of high velocity, high outcome work environment: level-I trauma centers in this country. A combination of qualitative and quantitative methods are used to collect observational and survey data in a range of trauma centers regarding the expertise needs of trauma medical teams, the points of expertise needs, the modes of acquiring needed expertise, the sources of information related to work and expertise coordination, and uses of IT for the coordination of needed expertise, as well as broader organizational variables. The project will contribute to the development of guiding principles in the design of next generation IT for high velocity work environments in the areas of user interfaces, coordination modes, and the evaluation of IT on coordination. It will contribute to the understanding of expertise coordination and how IT affects these processes. Finally, the project will enhance an existing multi-disciplinary research program on information and coordination, particularly as it relates to issues in emergency trauma care.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81868</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81868</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1439" target="n1440">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Static and Dynamic Techniques for Latency Hiding in Data-Intensive Applications</data>
      <data key="e_abstract">Many of today&apos;s performance-critical applications involve manipulating data&lt;br/&gt;sets that are either too large or too rarely used to be reliably found in&lt;br/&gt;local memory caches or local cache servers. Due to the enormous disparity&lt;br/&gt;between processor cycle times and disk and network access latencies, these&lt;br/&gt;applications waste a large fraction of their time waiting for data; as we&lt;br/&gt;look to the future, this problem is expected to become even worse. To&lt;br/&gt;overcome this problem, this research will combine aggressive storage&lt;br/&gt;prefetching with intelligent cache management to fully hide the data access&lt;br/&gt;latency while using memory resources intelligently. Program transformation&lt;br/&gt;tools and runtime support systems will be developed that collaborate to&lt;br/&gt;customize memory hierarchy and distributed cache resource management for&lt;br/&gt;data-intensive applications. With application-specific guidance of memory,&lt;br/&gt;network and disk resources, it is possible to decrease execution times by&lt;br/&gt;orders of magnitude. The ultimate goal of the research is for programs to&lt;br/&gt;never waste time waiting for data, and for programmers to never waste their&lt;br/&gt;valuable programming time thinking about this performance problem.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85938</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">85938</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n183" target="n342">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Personalized Spatial Audio via Scientific Computing and Computer Vision</data>
      <data key="e_abstract">This is the first 4 years funding of a five-year continuing award. Humans are very good at discerning the spatial origin of sound using a mixture of frequency-dependent interaural time difference (ITD), interaural level difference (ILD), and pinna spectral cues in disparate environments ranging from open spaces to small crowded rooms. This ability helps us to interact with others and the environment by sorting out individual sounds from a mixture, and helps us to survive by warning us of danger over a wider region of space compared to vision. These advantages of spatial sound are important for human-computer interaction. &lt;br/&gt;&lt;br/&gt;While the frequency-independent ITD cues (delays) associated with the two ears are relatively easy to render over headphones, the ILD (level difference) and pinna elevation cues are not. For a given source location and frequency content, the sound scattered by the person&apos;s torso, head and pinnae, and is received differently at the two ears, leading to differences in the intensity and spectral features of the received sound. These effects are encoded in an extremely individual &quot;Head Related Transfer Function&quot; (HRTF) that depends on the person&apos;s anatomical features (structure of the torso, head and pinnae). This individuality has made it difficult to use the HRTF in the proposed applications. Recent research, including that of members of this team, has focused on measuring the HRTFs for individuals in specific environments, on constructing models of the HRTF, on understanding how the geometry of the body is related to the characteristics of HRTF, and how the brain processes the cues to derive spatial information. However, this research has also indicated that the brain is extraordinarily perceptive to errors in cues that result when sound is rendered with an incorrect HRTF.&lt;br/&gt;&lt;br/&gt;In this project the PI and his team will use numerical methods to compute individualized HRTFs from accurate 3-D surface models of the body. They will use multiview, multiframe computational vision techniques to extract the surface models from imagery. They will then use boundary element methods employing fast multipole/ transform techniques and parallel processing to compute the HRTFs from the surface models. The resulting HRTFs will be evaluated both by objective comparisons with acoustically measured HRTFs and by psychoacoustic testing, and will be used in demonstrations of virtual reality, augmented reality, and teleconferencing. A major advantage of this vision-based approach is that it will allow the PI and his team to investigate and model the way that HRTFs change with body posture, providing the potential of tracking dynamic environments. Thus, the project will include fundamental research to extend the static HRTF measurements to dynamic situations in different environments, using a combination of visual tracking to locate the person in real space, and construction of in-room HRTFs from free-field HRTFs using fast iterative techniques. This will provide a scientific foundation for HCI applications of audio rendering. The research will in addition yield algorithms and understanding that will have an impact on varied fields, including computer vision based model creation; scientific computing; computational acoustics for noise control and land mine detection; neurophysiological understanding of human audition; etc.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86075</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86075</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n183" target="n1443">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Personalized Spatial Audio via Scientific Computing and Computer Vision</data>
      <data key="e_abstract">This is the first 4 years funding of a five-year continuing award. Humans are very good at discerning the spatial origin of sound using a mixture of frequency-dependent interaural time difference (ITD), interaural level difference (ILD), and pinna spectral cues in disparate environments ranging from open spaces to small crowded rooms. This ability helps us to interact with others and the environment by sorting out individual sounds from a mixture, and helps us to survive by warning us of danger over a wider region of space compared to vision. These advantages of spatial sound are important for human-computer interaction. &lt;br/&gt;&lt;br/&gt;While the frequency-independent ITD cues (delays) associated with the two ears are relatively easy to render over headphones, the ILD (level difference) and pinna elevation cues are not. For a given source location and frequency content, the sound scattered by the person&apos;s torso, head and pinnae, and is received differently at the two ears, leading to differences in the intensity and spectral features of the received sound. These effects are encoded in an extremely individual &quot;Head Related Transfer Function&quot; (HRTF) that depends on the person&apos;s anatomical features (structure of the torso, head and pinnae). This individuality has made it difficult to use the HRTF in the proposed applications. Recent research, including that of members of this team, has focused on measuring the HRTFs for individuals in specific environments, on constructing models of the HRTF, on understanding how the geometry of the body is related to the characteristics of HRTF, and how the brain processes the cues to derive spatial information. However, this research has also indicated that the brain is extraordinarily perceptive to errors in cues that result when sound is rendered with an incorrect HRTF.&lt;br/&gt;&lt;br/&gt;In this project the PI and his team will use numerical methods to compute individualized HRTFs from accurate 3-D surface models of the body. They will use multiview, multiframe computational vision techniques to extract the surface models from imagery. They will then use boundary element methods employing fast multipole/ transform techniques and parallel processing to compute the HRTFs from the surface models. The resulting HRTFs will be evaluated both by objective comparisons with acoustically measured HRTFs and by psychoacoustic testing, and will be used in demonstrations of virtual reality, augmented reality, and teleconferencing. A major advantage of this vision-based approach is that it will allow the PI and his team to investigate and model the way that HRTFs change with body posture, providing the potential of tracking dynamic environments. Thus, the project will include fundamental research to extend the static HRTF measurements to dynamic situations in different environments, using a combination of visual tracking to locate the person in real space, and construction of in-room HRTFs from free-field HRTFs using fast iterative techniques. This will provide a scientific foundation for HCI applications of audio rendering. The research will in addition yield algorithms and understanding that will have an impact on varied fields, including computer vision based model creation; scientific computing; computational acoustics for noise control and land mine detection; neurophysiological understanding of human audition; etc.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86075</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86075</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n183" target="n184">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Personalized Spatial Audio via Scientific Computing and Computer Vision</data>
      <data key="e_abstract">This is the first 4 years funding of a five-year continuing award. Humans are very good at discerning the spatial origin of sound using a mixture of frequency-dependent interaural time difference (ITD), interaural level difference (ILD), and pinna spectral cues in disparate environments ranging from open spaces to small crowded rooms. This ability helps us to interact with others and the environment by sorting out individual sounds from a mixture, and helps us to survive by warning us of danger over a wider region of space compared to vision. These advantages of spatial sound are important for human-computer interaction. &lt;br/&gt;&lt;br/&gt;While the frequency-independent ITD cues (delays) associated with the two ears are relatively easy to render over headphones, the ILD (level difference) and pinna elevation cues are not. For a given source location and frequency content, the sound scattered by the person&apos;s torso, head and pinnae, and is received differently at the two ears, leading to differences in the intensity and spectral features of the received sound. These effects are encoded in an extremely individual &quot;Head Related Transfer Function&quot; (HRTF) that depends on the person&apos;s anatomical features (structure of the torso, head and pinnae). This individuality has made it difficult to use the HRTF in the proposed applications. Recent research, including that of members of this team, has focused on measuring the HRTFs for individuals in specific environments, on constructing models of the HRTF, on understanding how the geometry of the body is related to the characteristics of HRTF, and how the brain processes the cues to derive spatial information. However, this research has also indicated that the brain is extraordinarily perceptive to errors in cues that result when sound is rendered with an incorrect HRTF.&lt;br/&gt;&lt;br/&gt;In this project the PI and his team will use numerical methods to compute individualized HRTFs from accurate 3-D surface models of the body. They will use multiview, multiframe computational vision techniques to extract the surface models from imagery. They will then use boundary element methods employing fast multipole/ transform techniques and parallel processing to compute the HRTFs from the surface models. The resulting HRTFs will be evaluated both by objective comparisons with acoustically measured HRTFs and by psychoacoustic testing, and will be used in demonstrations of virtual reality, augmented reality, and teleconferencing. A major advantage of this vision-based approach is that it will allow the PI and his team to investigate and model the way that HRTFs change with body posture, providing the potential of tracking dynamic environments. Thus, the project will include fundamental research to extend the static HRTF measurements to dynamic situations in different environments, using a combination of visual tracking to locate the person in real space, and construction of in-room HRTFs from free-field HRTFs using fast iterative techniques. This will provide a scientific foundation for HCI applications of audio rendering. The research will in addition yield algorithms and understanding that will have an impact on varied fields, including computer vision based model creation; scientific computing; computational acoustics for noise control and land mine detection; neurophysiological understanding of human audition; etc.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86075</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86075</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n183" target="n1445">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Personalized Spatial Audio via Scientific Computing and Computer Vision</data>
      <data key="e_abstract">This is the first 4 years funding of a five-year continuing award. Humans are very good at discerning the spatial origin of sound using a mixture of frequency-dependent interaural time difference (ITD), interaural level difference (ILD), and pinna spectral cues in disparate environments ranging from open spaces to small crowded rooms. This ability helps us to interact with others and the environment by sorting out individual sounds from a mixture, and helps us to survive by warning us of danger over a wider region of space compared to vision. These advantages of spatial sound are important for human-computer interaction. &lt;br/&gt;&lt;br/&gt;While the frequency-independent ITD cues (delays) associated with the two ears are relatively easy to render over headphones, the ILD (level difference) and pinna elevation cues are not. For a given source location and frequency content, the sound scattered by the person&apos;s torso, head and pinnae, and is received differently at the two ears, leading to differences in the intensity and spectral features of the received sound. These effects are encoded in an extremely individual &quot;Head Related Transfer Function&quot; (HRTF) that depends on the person&apos;s anatomical features (structure of the torso, head and pinnae). This individuality has made it difficult to use the HRTF in the proposed applications. Recent research, including that of members of this team, has focused on measuring the HRTFs for individuals in specific environments, on constructing models of the HRTF, on understanding how the geometry of the body is related to the characteristics of HRTF, and how the brain processes the cues to derive spatial information. However, this research has also indicated that the brain is extraordinarily perceptive to errors in cues that result when sound is rendered with an incorrect HRTF.&lt;br/&gt;&lt;br/&gt;In this project the PI and his team will use numerical methods to compute individualized HRTFs from accurate 3-D surface models of the body. They will use multiview, multiframe computational vision techniques to extract the surface models from imagery. They will then use boundary element methods employing fast multipole/ transform techniques and parallel processing to compute the HRTFs from the surface models. The resulting HRTFs will be evaluated both by objective comparisons with acoustically measured HRTFs and by psychoacoustic testing, and will be used in demonstrations of virtual reality, augmented reality, and teleconferencing. A major advantage of this vision-based approach is that it will allow the PI and his team to investigate and model the way that HRTFs change with body posture, providing the potential of tracking dynamic environments. Thus, the project will include fundamental research to extend the static HRTF measurements to dynamic situations in different environments, using a combination of visual tracking to locate the person in real space, and construction of in-room HRTFs from free-field HRTFs using fast iterative techniques. This will provide a scientific foundation for HCI applications of audio rendering. The research will in addition yield algorithms and understanding that will have an impact on varied fields, including computer vision based model creation; scientific computing; computational acoustics for noise control and land mine detection; neurophysiological understanding of human audition; etc.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86075</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86075</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n342" target="n1443">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Personalized Spatial Audio via Scientific Computing and Computer Vision</data>
      <data key="e_abstract">This is the first 4 years funding of a five-year continuing award. Humans are very good at discerning the spatial origin of sound using a mixture of frequency-dependent interaural time difference (ITD), interaural level difference (ILD), and pinna spectral cues in disparate environments ranging from open spaces to small crowded rooms. This ability helps us to interact with others and the environment by sorting out individual sounds from a mixture, and helps us to survive by warning us of danger over a wider region of space compared to vision. These advantages of spatial sound are important for human-computer interaction. &lt;br/&gt;&lt;br/&gt;While the frequency-independent ITD cues (delays) associated with the two ears are relatively easy to render over headphones, the ILD (level difference) and pinna elevation cues are not. For a given source location and frequency content, the sound scattered by the person&apos;s torso, head and pinnae, and is received differently at the two ears, leading to differences in the intensity and spectral features of the received sound. These effects are encoded in an extremely individual &quot;Head Related Transfer Function&quot; (HRTF) that depends on the person&apos;s anatomical features (structure of the torso, head and pinnae). This individuality has made it difficult to use the HRTF in the proposed applications. Recent research, including that of members of this team, has focused on measuring the HRTFs for individuals in specific environments, on constructing models of the HRTF, on understanding how the geometry of the body is related to the characteristics of HRTF, and how the brain processes the cues to derive spatial information. However, this research has also indicated that the brain is extraordinarily perceptive to errors in cues that result when sound is rendered with an incorrect HRTF.&lt;br/&gt;&lt;br/&gt;In this project the PI and his team will use numerical methods to compute individualized HRTFs from accurate 3-D surface models of the body. They will use multiview, multiframe computational vision techniques to extract the surface models from imagery. They will then use boundary element methods employing fast multipole/ transform techniques and parallel processing to compute the HRTFs from the surface models. The resulting HRTFs will be evaluated both by objective comparisons with acoustically measured HRTFs and by psychoacoustic testing, and will be used in demonstrations of virtual reality, augmented reality, and teleconferencing. A major advantage of this vision-based approach is that it will allow the PI and his team to investigate and model the way that HRTFs change with body posture, providing the potential of tracking dynamic environments. Thus, the project will include fundamental research to extend the static HRTF measurements to dynamic situations in different environments, using a combination of visual tracking to locate the person in real space, and construction of in-room HRTFs from free-field HRTFs using fast iterative techniques. This will provide a scientific foundation for HCI applications of audio rendering. The research will in addition yield algorithms and understanding that will have an impact on varied fields, including computer vision based model creation; scientific computing; computational acoustics for noise control and land mine detection; neurophysiological understanding of human audition; etc.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86075</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86075</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n184" target="n342">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Personalized Spatial Audio via Scientific Computing and Computer Vision</data>
      <data key="e_abstract">This is the first 4 years funding of a five-year continuing award. Humans are very good at discerning the spatial origin of sound using a mixture of frequency-dependent interaural time difference (ITD), interaural level difference (ILD), and pinna spectral cues in disparate environments ranging from open spaces to small crowded rooms. This ability helps us to interact with others and the environment by sorting out individual sounds from a mixture, and helps us to survive by warning us of danger over a wider region of space compared to vision. These advantages of spatial sound are important for human-computer interaction. &lt;br/&gt;&lt;br/&gt;While the frequency-independent ITD cues (delays) associated with the two ears are relatively easy to render over headphones, the ILD (level difference) and pinna elevation cues are not. For a given source location and frequency content, the sound scattered by the person&apos;s torso, head and pinnae, and is received differently at the two ears, leading to differences in the intensity and spectral features of the received sound. These effects are encoded in an extremely individual &quot;Head Related Transfer Function&quot; (HRTF) that depends on the person&apos;s anatomical features (structure of the torso, head and pinnae). This individuality has made it difficult to use the HRTF in the proposed applications. Recent research, including that of members of this team, has focused on measuring the HRTFs for individuals in specific environments, on constructing models of the HRTF, on understanding how the geometry of the body is related to the characteristics of HRTF, and how the brain processes the cues to derive spatial information. However, this research has also indicated that the brain is extraordinarily perceptive to errors in cues that result when sound is rendered with an incorrect HRTF.&lt;br/&gt;&lt;br/&gt;In this project the PI and his team will use numerical methods to compute individualized HRTFs from accurate 3-D surface models of the body. They will use multiview, multiframe computational vision techniques to extract the surface models from imagery. They will then use boundary element methods employing fast multipole/ transform techniques and parallel processing to compute the HRTFs from the surface models. The resulting HRTFs will be evaluated both by objective comparisons with acoustically measured HRTFs and by psychoacoustic testing, and will be used in demonstrations of virtual reality, augmented reality, and teleconferencing. A major advantage of this vision-based approach is that it will allow the PI and his team to investigate and model the way that HRTFs change with body posture, providing the potential of tracking dynamic environments. Thus, the project will include fundamental research to extend the static HRTF measurements to dynamic situations in different environments, using a combination of visual tracking to locate the person in real space, and construction of in-room HRTFs from free-field HRTFs using fast iterative techniques. This will provide a scientific foundation for HCI applications of audio rendering. The research will in addition yield algorithms and understanding that will have an impact on varied fields, including computer vision based model creation; scientific computing; computational acoustics for noise control and land mine detection; neurophysiological understanding of human audition; etc.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86075</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86075</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n342" target="n1445">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Personalized Spatial Audio via Scientific Computing and Computer Vision</data>
      <data key="e_abstract">This is the first 4 years funding of a five-year continuing award. Humans are very good at discerning the spatial origin of sound using a mixture of frequency-dependent interaural time difference (ITD), interaural level difference (ILD), and pinna spectral cues in disparate environments ranging from open spaces to small crowded rooms. This ability helps us to interact with others and the environment by sorting out individual sounds from a mixture, and helps us to survive by warning us of danger over a wider region of space compared to vision. These advantages of spatial sound are important for human-computer interaction. &lt;br/&gt;&lt;br/&gt;While the frequency-independent ITD cues (delays) associated with the two ears are relatively easy to render over headphones, the ILD (level difference) and pinna elevation cues are not. For a given source location and frequency content, the sound scattered by the person&apos;s torso, head and pinnae, and is received differently at the two ears, leading to differences in the intensity and spectral features of the received sound. These effects are encoded in an extremely individual &quot;Head Related Transfer Function&quot; (HRTF) that depends on the person&apos;s anatomical features (structure of the torso, head and pinnae). This individuality has made it difficult to use the HRTF in the proposed applications. Recent research, including that of members of this team, has focused on measuring the HRTFs for individuals in specific environments, on constructing models of the HRTF, on understanding how the geometry of the body is related to the characteristics of HRTF, and how the brain processes the cues to derive spatial information. However, this research has also indicated that the brain is extraordinarily perceptive to errors in cues that result when sound is rendered with an incorrect HRTF.&lt;br/&gt;&lt;br/&gt;In this project the PI and his team will use numerical methods to compute individualized HRTFs from accurate 3-D surface models of the body. They will use multiview, multiframe computational vision techniques to extract the surface models from imagery. They will then use boundary element methods employing fast multipole/ transform techniques and parallel processing to compute the HRTFs from the surface models. The resulting HRTFs will be evaluated both by objective comparisons with acoustically measured HRTFs and by psychoacoustic testing, and will be used in demonstrations of virtual reality, augmented reality, and teleconferencing. A major advantage of this vision-based approach is that it will allow the PI and his team to investigate and model the way that HRTFs change with body posture, providing the potential of tracking dynamic environments. Thus, the project will include fundamental research to extend the static HRTF measurements to dynamic situations in different environments, using a combination of visual tracking to locate the person in real space, and construction of in-room HRTFs from free-field HRTFs using fast iterative techniques. This will provide a scientific foundation for HCI applications of audio rendering. The research will in addition yield algorithms and understanding that will have an impact on varied fields, including computer vision based model creation; scientific computing; computational acoustics for noise control and land mine detection; neurophysiological understanding of human audition; etc.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86075</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86075</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n184" target="n1443">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Personalized Spatial Audio via Scientific Computing and Computer Vision</data>
      <data key="e_abstract">This is the first 4 years funding of a five-year continuing award. Humans are very good at discerning the spatial origin of sound using a mixture of frequency-dependent interaural time difference (ITD), interaural level difference (ILD), and pinna spectral cues in disparate environments ranging from open spaces to small crowded rooms. This ability helps us to interact with others and the environment by sorting out individual sounds from a mixture, and helps us to survive by warning us of danger over a wider region of space compared to vision. These advantages of spatial sound are important for human-computer interaction. &lt;br/&gt;&lt;br/&gt;While the frequency-independent ITD cues (delays) associated with the two ears are relatively easy to render over headphones, the ILD (level difference) and pinna elevation cues are not. For a given source location and frequency content, the sound scattered by the person&apos;s torso, head and pinnae, and is received differently at the two ears, leading to differences in the intensity and spectral features of the received sound. These effects are encoded in an extremely individual &quot;Head Related Transfer Function&quot; (HRTF) that depends on the person&apos;s anatomical features (structure of the torso, head and pinnae). This individuality has made it difficult to use the HRTF in the proposed applications. Recent research, including that of members of this team, has focused on measuring the HRTFs for individuals in specific environments, on constructing models of the HRTF, on understanding how the geometry of the body is related to the characteristics of HRTF, and how the brain processes the cues to derive spatial information. However, this research has also indicated that the brain is extraordinarily perceptive to errors in cues that result when sound is rendered with an incorrect HRTF.&lt;br/&gt;&lt;br/&gt;In this project the PI and his team will use numerical methods to compute individualized HRTFs from accurate 3-D surface models of the body. They will use multiview, multiframe computational vision techniques to extract the surface models from imagery. They will then use boundary element methods employing fast multipole/ transform techniques and parallel processing to compute the HRTFs from the surface models. The resulting HRTFs will be evaluated both by objective comparisons with acoustically measured HRTFs and by psychoacoustic testing, and will be used in demonstrations of virtual reality, augmented reality, and teleconferencing. A major advantage of this vision-based approach is that it will allow the PI and his team to investigate and model the way that HRTFs change with body posture, providing the potential of tracking dynamic environments. Thus, the project will include fundamental research to extend the static HRTF measurements to dynamic situations in different environments, using a combination of visual tracking to locate the person in real space, and construction of in-room HRTFs from free-field HRTFs using fast iterative techniques. This will provide a scientific foundation for HCI applications of audio rendering. The research will in addition yield algorithms and understanding that will have an impact on varied fields, including computer vision based model creation; scientific computing; computational acoustics for noise control and land mine detection; neurophysiological understanding of human audition; etc.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86075</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86075</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1443" target="n1445">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Personalized Spatial Audio via Scientific Computing and Computer Vision</data>
      <data key="e_abstract">This is the first 4 years funding of a five-year continuing award. Humans are very good at discerning the spatial origin of sound using a mixture of frequency-dependent interaural time difference (ITD), interaural level difference (ILD), and pinna spectral cues in disparate environments ranging from open spaces to small crowded rooms. This ability helps us to interact with others and the environment by sorting out individual sounds from a mixture, and helps us to survive by warning us of danger over a wider region of space compared to vision. These advantages of spatial sound are important for human-computer interaction. &lt;br/&gt;&lt;br/&gt;While the frequency-independent ITD cues (delays) associated with the two ears are relatively easy to render over headphones, the ILD (level difference) and pinna elevation cues are not. For a given source location and frequency content, the sound scattered by the person&apos;s torso, head and pinnae, and is received differently at the two ears, leading to differences in the intensity and spectral features of the received sound. These effects are encoded in an extremely individual &quot;Head Related Transfer Function&quot; (HRTF) that depends on the person&apos;s anatomical features (structure of the torso, head and pinnae). This individuality has made it difficult to use the HRTF in the proposed applications. Recent research, including that of members of this team, has focused on measuring the HRTFs for individuals in specific environments, on constructing models of the HRTF, on understanding how the geometry of the body is related to the characteristics of HRTF, and how the brain processes the cues to derive spatial information. However, this research has also indicated that the brain is extraordinarily perceptive to errors in cues that result when sound is rendered with an incorrect HRTF.&lt;br/&gt;&lt;br/&gt;In this project the PI and his team will use numerical methods to compute individualized HRTFs from accurate 3-D surface models of the body. They will use multiview, multiframe computational vision techniques to extract the surface models from imagery. They will then use boundary element methods employing fast multipole/ transform techniques and parallel processing to compute the HRTFs from the surface models. The resulting HRTFs will be evaluated both by objective comparisons with acoustically measured HRTFs and by psychoacoustic testing, and will be used in demonstrations of virtual reality, augmented reality, and teleconferencing. A major advantage of this vision-based approach is that it will allow the PI and his team to investigate and model the way that HRTFs change with body posture, providing the potential of tracking dynamic environments. Thus, the project will include fundamental research to extend the static HRTF measurements to dynamic situations in different environments, using a combination of visual tracking to locate the person in real space, and construction of in-room HRTFs from free-field HRTFs using fast iterative techniques. This will provide a scientific foundation for HCI applications of audio rendering. The research will in addition yield algorithms and understanding that will have an impact on varied fields, including computer vision based model creation; scientific computing; computational acoustics for noise control and land mine detection; neurophysiological understanding of human audition; etc.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86075</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86075</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n184" target="n1445">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Personalized Spatial Audio via Scientific Computing and Computer Vision</data>
      <data key="e_abstract">This is the first 4 years funding of a five-year continuing award. Humans are very good at discerning the spatial origin of sound using a mixture of frequency-dependent interaural time difference (ITD), interaural level difference (ILD), and pinna spectral cues in disparate environments ranging from open spaces to small crowded rooms. This ability helps us to interact with others and the environment by sorting out individual sounds from a mixture, and helps us to survive by warning us of danger over a wider region of space compared to vision. These advantages of spatial sound are important for human-computer interaction. &lt;br/&gt;&lt;br/&gt;While the frequency-independent ITD cues (delays) associated with the two ears are relatively easy to render over headphones, the ILD (level difference) and pinna elevation cues are not. For a given source location and frequency content, the sound scattered by the person&apos;s torso, head and pinnae, and is received differently at the two ears, leading to differences in the intensity and spectral features of the received sound. These effects are encoded in an extremely individual &quot;Head Related Transfer Function&quot; (HRTF) that depends on the person&apos;s anatomical features (structure of the torso, head and pinnae). This individuality has made it difficult to use the HRTF in the proposed applications. Recent research, including that of members of this team, has focused on measuring the HRTFs for individuals in specific environments, on constructing models of the HRTF, on understanding how the geometry of the body is related to the characteristics of HRTF, and how the brain processes the cues to derive spatial information. However, this research has also indicated that the brain is extraordinarily perceptive to errors in cues that result when sound is rendered with an incorrect HRTF.&lt;br/&gt;&lt;br/&gt;In this project the PI and his team will use numerical methods to compute individualized HRTFs from accurate 3-D surface models of the body. They will use multiview, multiframe computational vision techniques to extract the surface models from imagery. They will then use boundary element methods employing fast multipole/ transform techniques and parallel processing to compute the HRTFs from the surface models. The resulting HRTFs will be evaluated both by objective comparisons with acoustically measured HRTFs and by psychoacoustic testing, and will be used in demonstrations of virtual reality, augmented reality, and teleconferencing. A major advantage of this vision-based approach is that it will allow the PI and his team to investigate and model the way that HRTFs change with body posture, providing the potential of tracking dynamic environments. Thus, the project will include fundamental research to extend the static HRTF measurements to dynamic situations in different environments, using a combination of visual tracking to locate the person in real space, and construction of in-room HRTFs from free-field HRTFs using fast iterative techniques. This will provide a scientific foundation for HCI applications of audio rendering. The research will in addition yield algorithms and understanding that will have an impact on varied fields, including computer vision based model creation; scientific computing; computational acoustics for noise control and land mine detection; neurophysiological understanding of human audition; etc.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86075</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86075</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1446" target="n1447">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Workshop on Digital Government: An Urban Research Agenda</data>
      <data key="e_abstract">EIA-0089869&lt;br/&gt;Buy, Ugo&lt;br/&gt;University of Illinois Chicago&lt;br/&gt;&lt;br/&gt;Workshop on Digital Government: An Urban Research Agenda&lt;br/&gt;&lt;br/&gt;This grant will support a workshop to define the information and computer science research agenda related to urban government issues. It will involve a consortium of 17 urban universities (listed on page 4 of the proposal) called the Great Cities Universities, of which the University of Illinois Chicago is one.&lt;br/&gt;&lt;br/&gt;Information and networking technologies now have the potential to affect a variety of domains of interest in an urban environment including land use, criminal justice, digital governance and democracy, GIS-supported issues such as housing and business locations, social welfare services, education, and many more. This is a rich area of applications which should be of great interest to the computer and information science community.&lt;br/&gt;&lt;br/&gt;This will be a joint effort of UIC&apos;s Department of Electrical Engineering and Computer Science and UIC&apos;s College of Urban Planning and Public Affairs; both disciplines will be essential to the an effective workshop.&lt;br/&gt;.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">89869</data>
      <data key="e_expirationDate">2002-12-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">89869</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1448" target="n1449">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Simulation of Flows with Dynamic Interfaces on Multi-Teraflop Computers</data>
      <data key="e_abstract">This Project will develop advanced parallel algorithms and software for simulating complex flows with dynamic interfaces. The development of scalable, parallel high-accuracy algorithms for simulating such flows poses enormous challenges in computational science. The project will use these algorithms for microstructural simulation of blood flow. This application provides an excellent testbed for the methods: it is extremely computationally challenging and of critical medical importance.&lt;br/&gt;&lt;br/&gt;Blood flow belongs to a class of flow problems with dynamic interfaces. Blood is a mixture of interacting gel-filled solid sells and fluid plasma. Current blood flow models are macroscopic, treating the mixture as a homogeneous continuum. Microstructural models resolve individual cells deformations and their interaction with the surrounding plasma. Because of the computational difficulties of resolving tens of thousands of dynamically deforming cells, no one to date has simulated realistic blood flows at this level. Yet such simulations are necessary in order to gain a better understanding of blood damage - which is central to improved artificial organ design - and for the development of more rational macroscopic blood models.&lt;br/&gt;&lt;br/&gt;Simulating flows with dynamic interfaces is much more difficult than flows in well-understood fixed domains. The central challenges are to develop numerical algorithms that stably and accurately couple the moving fluid and solids, and geometric algorithms for computing the resulting dynamic meshes. This project takes the approach of treating both fluid and solid domains as collections of grid points, with associated meshes, that evolve over time and devising numerical algorithms that couple the domains seamlessly. It will attack the difficulty of creating and managing the evolving mesh by developing scalable parallel algorithms for the convex hull, Delaunay triangulation, and mesh partitioning components. With careful attention to fundamental algorithmic issues, these cheap geometric computations will enable these dynamic flow simulations to scale to thousands of processors as on mult-teraflop systems.&lt;br/&gt;&lt;br/&gt;This research will benefit a wide community of scientists and engineers. The computational algorithms will be widely applicable to a variety of fluid-solid and fluid-fluid interaction problems. More generally, the core parallel computational geometry kernels will provide generic support for the geometric computations underlying many dynamic irregular problems. The project will distribute a portable library of efficient implementations of these algorithms. Also, the project will undertake a broad-based, interdisciplinary program integrating research and education. It will be part of a new program in Computational Science and Engineering, serving as the archetype of how applications, computational, computer, and mathematical scientists can work together to tackle societal problems that cannot be solved solely by any one discipline.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86093</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86093</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1126" target="n1448">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Simulation of Flows with Dynamic Interfaces on Multi-Teraflop Computers</data>
      <data key="e_abstract">This Project will develop advanced parallel algorithms and software for simulating complex flows with dynamic interfaces. The development of scalable, parallel high-accuracy algorithms for simulating such flows poses enormous challenges in computational science. The project will use these algorithms for microstructural simulation of blood flow. This application provides an excellent testbed for the methods: it is extremely computationally challenging and of critical medical importance.&lt;br/&gt;&lt;br/&gt;Blood flow belongs to a class of flow problems with dynamic interfaces. Blood is a mixture of interacting gel-filled solid sells and fluid plasma. Current blood flow models are macroscopic, treating the mixture as a homogeneous continuum. Microstructural models resolve individual cells deformations and their interaction with the surrounding plasma. Because of the computational difficulties of resolving tens of thousands of dynamically deforming cells, no one to date has simulated realistic blood flows at this level. Yet such simulations are necessary in order to gain a better understanding of blood damage - which is central to improved artificial organ design - and for the development of more rational macroscopic blood models.&lt;br/&gt;&lt;br/&gt;Simulating flows with dynamic interfaces is much more difficult than flows in well-understood fixed domains. The central challenges are to develop numerical algorithms that stably and accurately couple the moving fluid and solids, and geometric algorithms for computing the resulting dynamic meshes. This project takes the approach of treating both fluid and solid domains as collections of grid points, with associated meshes, that evolve over time and devising numerical algorithms that couple the domains seamlessly. It will attack the difficulty of creating and managing the evolving mesh by developing scalable parallel algorithms for the convex hull, Delaunay triangulation, and mesh partitioning components. With careful attention to fundamental algorithmic issues, these cheap geometric computations will enable these dynamic flow simulations to scale to thousands of processors as on mult-teraflop systems.&lt;br/&gt;&lt;br/&gt;This research will benefit a wide community of scientists and engineers. The computational algorithms will be widely applicable to a variety of fluid-solid and fluid-fluid interaction problems. More generally, the core parallel computational geometry kernels will provide generic support for the geometric computations underlying many dynamic irregular problems. The project will distribute a portable library of efficient implementations of these algorithms. Also, the project will undertake a broad-based, interdisciplinary program integrating research and education. It will be part of a new program in Computational Science and Engineering, serving as the archetype of how applications, computational, computer, and mathematical scientists can work together to tackle societal problems that cannot be solved solely by any one discipline.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86093</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86093</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1448" target="n1451">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Simulation of Flows with Dynamic Interfaces on Multi-Teraflop Computers</data>
      <data key="e_abstract">This Project will develop advanced parallel algorithms and software for simulating complex flows with dynamic interfaces. The development of scalable, parallel high-accuracy algorithms for simulating such flows poses enormous challenges in computational science. The project will use these algorithms for microstructural simulation of blood flow. This application provides an excellent testbed for the methods: it is extremely computationally challenging and of critical medical importance.&lt;br/&gt;&lt;br/&gt;Blood flow belongs to a class of flow problems with dynamic interfaces. Blood is a mixture of interacting gel-filled solid sells and fluid plasma. Current blood flow models are macroscopic, treating the mixture as a homogeneous continuum. Microstructural models resolve individual cells deformations and their interaction with the surrounding plasma. Because of the computational difficulties of resolving tens of thousands of dynamically deforming cells, no one to date has simulated realistic blood flows at this level. Yet such simulations are necessary in order to gain a better understanding of blood damage - which is central to improved artificial organ design - and for the development of more rational macroscopic blood models.&lt;br/&gt;&lt;br/&gt;Simulating flows with dynamic interfaces is much more difficult than flows in well-understood fixed domains. The central challenges are to develop numerical algorithms that stably and accurately couple the moving fluid and solids, and geometric algorithms for computing the resulting dynamic meshes. This project takes the approach of treating both fluid and solid domains as collections of grid points, with associated meshes, that evolve over time and devising numerical algorithms that couple the domains seamlessly. It will attack the difficulty of creating and managing the evolving mesh by developing scalable parallel algorithms for the convex hull, Delaunay triangulation, and mesh partitioning components. With careful attention to fundamental algorithmic issues, these cheap geometric computations will enable these dynamic flow simulations to scale to thousands of processors as on mult-teraflop systems.&lt;br/&gt;&lt;br/&gt;This research will benefit a wide community of scientists and engineers. The computational algorithms will be widely applicable to a variety of fluid-solid and fluid-fluid interaction problems. More generally, the core parallel computational geometry kernels will provide generic support for the geometric computations underlying many dynamic irregular problems. The project will distribute a portable library of efficient implementations of these algorithms. Also, the project will undertake a broad-based, interdisciplinary program integrating research and education. It will be part of a new program in Computational Science and Engineering, serving as the archetype of how applications, computational, computer, and mathematical scientists can work together to tackle societal problems that cannot be solved solely by any one discipline.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86093</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86093</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1126" target="n1449">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Simulation of Flows with Dynamic Interfaces on Multi-Teraflop Computers</data>
      <data key="e_abstract">This Project will develop advanced parallel algorithms and software for simulating complex flows with dynamic interfaces. The development of scalable, parallel high-accuracy algorithms for simulating such flows poses enormous challenges in computational science. The project will use these algorithms for microstructural simulation of blood flow. This application provides an excellent testbed for the methods: it is extremely computationally challenging and of critical medical importance.&lt;br/&gt;&lt;br/&gt;Blood flow belongs to a class of flow problems with dynamic interfaces. Blood is a mixture of interacting gel-filled solid sells and fluid plasma. Current blood flow models are macroscopic, treating the mixture as a homogeneous continuum. Microstructural models resolve individual cells deformations and their interaction with the surrounding plasma. Because of the computational difficulties of resolving tens of thousands of dynamically deforming cells, no one to date has simulated realistic blood flows at this level. Yet such simulations are necessary in order to gain a better understanding of blood damage - which is central to improved artificial organ design - and for the development of more rational macroscopic blood models.&lt;br/&gt;&lt;br/&gt;Simulating flows with dynamic interfaces is much more difficult than flows in well-understood fixed domains. The central challenges are to develop numerical algorithms that stably and accurately couple the moving fluid and solids, and geometric algorithms for computing the resulting dynamic meshes. This project takes the approach of treating both fluid and solid domains as collections of grid points, with associated meshes, that evolve over time and devising numerical algorithms that couple the domains seamlessly. It will attack the difficulty of creating and managing the evolving mesh by developing scalable parallel algorithms for the convex hull, Delaunay triangulation, and mesh partitioning components. With careful attention to fundamental algorithmic issues, these cheap geometric computations will enable these dynamic flow simulations to scale to thousands of processors as on mult-teraflop systems.&lt;br/&gt;&lt;br/&gt;This research will benefit a wide community of scientists and engineers. The computational algorithms will be widely applicable to a variety of fluid-solid and fluid-fluid interaction problems. More generally, the core parallel computational geometry kernels will provide generic support for the geometric computations underlying many dynamic irregular problems. The project will distribute a portable library of efficient implementations of these algorithms. Also, the project will undertake a broad-based, interdisciplinary program integrating research and education. It will be part of a new program in Computational Science and Engineering, serving as the archetype of how applications, computational, computer, and mathematical scientists can work together to tackle societal problems that cannot be solved solely by any one discipline.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86093</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86093</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1449" target="n1451">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Simulation of Flows with Dynamic Interfaces on Multi-Teraflop Computers</data>
      <data key="e_abstract">This Project will develop advanced parallel algorithms and software for simulating complex flows with dynamic interfaces. The development of scalable, parallel high-accuracy algorithms for simulating such flows poses enormous challenges in computational science. The project will use these algorithms for microstructural simulation of blood flow. This application provides an excellent testbed for the methods: it is extremely computationally challenging and of critical medical importance.&lt;br/&gt;&lt;br/&gt;Blood flow belongs to a class of flow problems with dynamic interfaces. Blood is a mixture of interacting gel-filled solid sells and fluid plasma. Current blood flow models are macroscopic, treating the mixture as a homogeneous continuum. Microstructural models resolve individual cells deformations and their interaction with the surrounding plasma. Because of the computational difficulties of resolving tens of thousands of dynamically deforming cells, no one to date has simulated realistic blood flows at this level. Yet such simulations are necessary in order to gain a better understanding of blood damage - which is central to improved artificial organ design - and for the development of more rational macroscopic blood models.&lt;br/&gt;&lt;br/&gt;Simulating flows with dynamic interfaces is much more difficult than flows in well-understood fixed domains. The central challenges are to develop numerical algorithms that stably and accurately couple the moving fluid and solids, and geometric algorithms for computing the resulting dynamic meshes. This project takes the approach of treating both fluid and solid domains as collections of grid points, with associated meshes, that evolve over time and devising numerical algorithms that couple the domains seamlessly. It will attack the difficulty of creating and managing the evolving mesh by developing scalable parallel algorithms for the convex hull, Delaunay triangulation, and mesh partitioning components. With careful attention to fundamental algorithmic issues, these cheap geometric computations will enable these dynamic flow simulations to scale to thousands of processors as on mult-teraflop systems.&lt;br/&gt;&lt;br/&gt;This research will benefit a wide community of scientists and engineers. The computational algorithms will be widely applicable to a variety of fluid-solid and fluid-fluid interaction problems. More generally, the core parallel computational geometry kernels will provide generic support for the geometric computations underlying many dynamic irregular problems. The project will distribute a portable library of efficient implementations of these algorithms. Also, the project will undertake a broad-based, interdisciplinary program integrating research and education. It will be part of a new program in Computational Science and Engineering, serving as the archetype of how applications, computational, computer, and mathematical scientists can work together to tackle societal problems that cannot be solved solely by any one discipline.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86093</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86093</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1126" target="n1451">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Simulation of Flows with Dynamic Interfaces on Multi-Teraflop Computers</data>
      <data key="e_abstract">This Project will develop advanced parallel algorithms and software for simulating complex flows with dynamic interfaces. The development of scalable, parallel high-accuracy algorithms for simulating such flows poses enormous challenges in computational science. The project will use these algorithms for microstructural simulation of blood flow. This application provides an excellent testbed for the methods: it is extremely computationally challenging and of critical medical importance.&lt;br/&gt;&lt;br/&gt;Blood flow belongs to a class of flow problems with dynamic interfaces. Blood is a mixture of interacting gel-filled solid sells and fluid plasma. Current blood flow models are macroscopic, treating the mixture as a homogeneous continuum. Microstructural models resolve individual cells deformations and their interaction with the surrounding plasma. Because of the computational difficulties of resolving tens of thousands of dynamically deforming cells, no one to date has simulated realistic blood flows at this level. Yet such simulations are necessary in order to gain a better understanding of blood damage - which is central to improved artificial organ design - and for the development of more rational macroscopic blood models.&lt;br/&gt;&lt;br/&gt;Simulating flows with dynamic interfaces is much more difficult than flows in well-understood fixed domains. The central challenges are to develop numerical algorithms that stably and accurately couple the moving fluid and solids, and geometric algorithms for computing the resulting dynamic meshes. This project takes the approach of treating both fluid and solid domains as collections of grid points, with associated meshes, that evolve over time and devising numerical algorithms that couple the domains seamlessly. It will attack the difficulty of creating and managing the evolving mesh by developing scalable parallel algorithms for the convex hull, Delaunay triangulation, and mesh partitioning components. With careful attention to fundamental algorithmic issues, these cheap geometric computations will enable these dynamic flow simulations to scale to thousands of processors as on mult-teraflop systems.&lt;br/&gt;&lt;br/&gt;This research will benefit a wide community of scientists and engineers. The computational algorithms will be widely applicable to a variety of fluid-solid and fluid-fluid interaction problems. More generally, the core parallel computational geometry kernels will provide generic support for the geometric computations underlying many dynamic irregular problems. The project will distribute a portable library of efficient implementations of these algorithms. Also, the project will undertake a broad-based, interdisciplinary program integrating research and education. It will be part of a new program in Computational Science and Engineering, serving as the archetype of how applications, computational, computer, and mathematical scientists can work together to tackle societal problems that cannot be solved solely by any one discipline.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86093</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86093</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1114" target="n1452">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Active Information Spaces based on Ubiquitous Computing</data>
      <data key="e_abstract">The project researches a new form of operating system to manage a model of&lt;br/&gt;computing called an Active Space. It integrates physical spaces that contain&lt;br/&gt;ubiquitous computers into a computational environment that supports human&lt;br/&gt;activity and applications. With anytime/anywhere ubiquitous devices, the&lt;br/&gt;users&apos; view of the computational environment is extended beyond the physical&lt;br/&gt;limits of a computer and is placed into the surrounding physical space,&lt;br/&gt;augmented with computers that sense and affect that space around the user.&lt;br/&gt;Applications become mapped not just to views associated with specific&lt;br/&gt;windows in a monitor but instead to the physical environment. Therefore,&lt;br/&gt;the physical space, augmented with communicating computer devices, becomes a&lt;br/&gt;distributed computing system.&lt;br/&gt;Active Spaces have the potential for creating multi-billion dollar&lt;br/&gt;industries. Automated surgery, collaboration and engaged learning are a few&lt;br/&gt;of the compelling examples. Gaia, an operating system for Active Spaces,&lt;br/&gt;will accommodate diversity by exploiting standards for interoperation and&lt;br/&gt;cooperation. System services track, authenticate and support mobile users&lt;br/&gt;with reconfigurable graphics, multimedia and Active Space applications. A&lt;br/&gt;unifying object bus, component model, and adaptive stream model extends plug&lt;br/&gt;and play to distributed mobile ubiquitous computers cooperating to support a&lt;br/&gt;computational environment within physical spaces like cities, buildings and&lt;br/&gt;rooms.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86094</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86094</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n404" target="n1452">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Active Information Spaces based on Ubiquitous Computing</data>
      <data key="e_abstract">The project researches a new form of operating system to manage a model of&lt;br/&gt;computing called an Active Space. It integrates physical spaces that contain&lt;br/&gt;ubiquitous computers into a computational environment that supports human&lt;br/&gt;activity and applications. With anytime/anywhere ubiquitous devices, the&lt;br/&gt;users&apos; view of the computational environment is extended beyond the physical&lt;br/&gt;limits of a computer and is placed into the surrounding physical space,&lt;br/&gt;augmented with computers that sense and affect that space around the user.&lt;br/&gt;Applications become mapped not just to views associated with specific&lt;br/&gt;windows in a monitor but instead to the physical environment. Therefore,&lt;br/&gt;the physical space, augmented with communicating computer devices, becomes a&lt;br/&gt;distributed computing system.&lt;br/&gt;Active Spaces have the potential for creating multi-billion dollar&lt;br/&gt;industries. Automated surgery, collaboration and engaged learning are a few&lt;br/&gt;of the compelling examples. Gaia, an operating system for Active Spaces,&lt;br/&gt;will accommodate diversity by exploiting standards for interoperation and&lt;br/&gt;cooperation. System services track, authenticate and support mobile users&lt;br/&gt;with reconfigurable graphics, multimedia and Active Space applications. A&lt;br/&gt;unifying object bus, component model, and adaptive stream model extends plug&lt;br/&gt;and play to distributed mobile ubiquitous computers cooperating to support a&lt;br/&gt;computational environment within physical spaces like cities, buildings and&lt;br/&gt;rooms.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86094</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86094</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n922" target="n1452">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Active Information Spaces based on Ubiquitous Computing</data>
      <data key="e_abstract">The project researches a new form of operating system to manage a model of&lt;br/&gt;computing called an Active Space. It integrates physical spaces that contain&lt;br/&gt;ubiquitous computers into a computational environment that supports human&lt;br/&gt;activity and applications. With anytime/anywhere ubiquitous devices, the&lt;br/&gt;users&apos; view of the computational environment is extended beyond the physical&lt;br/&gt;limits of a computer and is placed into the surrounding physical space,&lt;br/&gt;augmented with computers that sense and affect that space around the user.&lt;br/&gt;Applications become mapped not just to views associated with specific&lt;br/&gt;windows in a monitor but instead to the physical environment. Therefore,&lt;br/&gt;the physical space, augmented with communicating computer devices, becomes a&lt;br/&gt;distributed computing system.&lt;br/&gt;Active Spaces have the potential for creating multi-billion dollar&lt;br/&gt;industries. Automated surgery, collaboration and engaged learning are a few&lt;br/&gt;of the compelling examples. Gaia, an operating system for Active Spaces,&lt;br/&gt;will accommodate diversity by exploiting standards for interoperation and&lt;br/&gt;cooperation. System services track, authenticate and support mobile users&lt;br/&gt;with reconfigurable graphics, multimedia and Active Space applications. A&lt;br/&gt;unifying object bus, component model, and adaptive stream model extends plug&lt;br/&gt;and play to distributed mobile ubiquitous computers cooperating to support a&lt;br/&gt;computational environment within physical spaces like cities, buildings and&lt;br/&gt;rooms.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86094</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86094</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1452" target="n1456">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Active Information Spaces based on Ubiquitous Computing</data>
      <data key="e_abstract">The project researches a new form of operating system to manage a model of&lt;br/&gt;computing called an Active Space. It integrates physical spaces that contain&lt;br/&gt;ubiquitous computers into a computational environment that supports human&lt;br/&gt;activity and applications. With anytime/anywhere ubiquitous devices, the&lt;br/&gt;users&apos; view of the computational environment is extended beyond the physical&lt;br/&gt;limits of a computer and is placed into the surrounding physical space,&lt;br/&gt;augmented with computers that sense and affect that space around the user.&lt;br/&gt;Applications become mapped not just to views associated with specific&lt;br/&gt;windows in a monitor but instead to the physical environment. Therefore,&lt;br/&gt;the physical space, augmented with communicating computer devices, becomes a&lt;br/&gt;distributed computing system.&lt;br/&gt;Active Spaces have the potential for creating multi-billion dollar&lt;br/&gt;industries. Automated surgery, collaboration and engaged learning are a few&lt;br/&gt;of the compelling examples. Gaia, an operating system for Active Spaces,&lt;br/&gt;will accommodate diversity by exploiting standards for interoperation and&lt;br/&gt;cooperation. System services track, authenticate and support mobile users&lt;br/&gt;with reconfigurable graphics, multimedia and Active Space applications. A&lt;br/&gt;unifying object bus, component model, and adaptive stream model extends plug&lt;br/&gt;and play to distributed mobile ubiquitous computers cooperating to support a&lt;br/&gt;computational environment within physical spaces like cities, buildings and&lt;br/&gt;rooms.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86094</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86094</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n404" target="n1114">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Active Information Spaces based on Ubiquitous Computing</data>
      <data key="e_abstract">The project researches a new form of operating system to manage a model of&lt;br/&gt;computing called an Active Space. It integrates physical spaces that contain&lt;br/&gt;ubiquitous computers into a computational environment that supports human&lt;br/&gt;activity and applications. With anytime/anywhere ubiquitous devices, the&lt;br/&gt;users&apos; view of the computational environment is extended beyond the physical&lt;br/&gt;limits of a computer and is placed into the surrounding physical space,&lt;br/&gt;augmented with computers that sense and affect that space around the user.&lt;br/&gt;Applications become mapped not just to views associated with specific&lt;br/&gt;windows in a monitor but instead to the physical environment. Therefore,&lt;br/&gt;the physical space, augmented with communicating computer devices, becomes a&lt;br/&gt;distributed computing system.&lt;br/&gt;Active Spaces have the potential for creating multi-billion dollar&lt;br/&gt;industries. Automated surgery, collaboration and engaged learning are a few&lt;br/&gt;of the compelling examples. Gaia, an operating system for Active Spaces,&lt;br/&gt;will accommodate diversity by exploiting standards for interoperation and&lt;br/&gt;cooperation. System services track, authenticate and support mobile users&lt;br/&gt;with reconfigurable graphics, multimedia and Active Space applications. A&lt;br/&gt;unifying object bus, component model, and adaptive stream model extends plug&lt;br/&gt;and play to distributed mobile ubiquitous computers cooperating to support a&lt;br/&gt;computational environment within physical spaces like cities, buildings and&lt;br/&gt;rooms.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86094</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86094</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n922" target="n1114">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Active Information Spaces based on Ubiquitous Computing</data>
      <data key="e_abstract">The project researches a new form of operating system to manage a model of&lt;br/&gt;computing called an Active Space. It integrates physical spaces that contain&lt;br/&gt;ubiquitous computers into a computational environment that supports human&lt;br/&gt;activity and applications. With anytime/anywhere ubiquitous devices, the&lt;br/&gt;users&apos; view of the computational environment is extended beyond the physical&lt;br/&gt;limits of a computer and is placed into the surrounding physical space,&lt;br/&gt;augmented with computers that sense and affect that space around the user.&lt;br/&gt;Applications become mapped not just to views associated with specific&lt;br/&gt;windows in a monitor but instead to the physical environment. Therefore,&lt;br/&gt;the physical space, augmented with communicating computer devices, becomes a&lt;br/&gt;distributed computing system.&lt;br/&gt;Active Spaces have the potential for creating multi-billion dollar&lt;br/&gt;industries. Automated surgery, collaboration and engaged learning are a few&lt;br/&gt;of the compelling examples. Gaia, an operating system for Active Spaces,&lt;br/&gt;will accommodate diversity by exploiting standards for interoperation and&lt;br/&gt;cooperation. System services track, authenticate and support mobile users&lt;br/&gt;with reconfigurable graphics, multimedia and Active Space applications. A&lt;br/&gt;unifying object bus, component model, and adaptive stream model extends plug&lt;br/&gt;and play to distributed mobile ubiquitous computers cooperating to support a&lt;br/&gt;computational environment within physical spaces like cities, buildings and&lt;br/&gt;rooms.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86094</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86094</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1114" target="n1456">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Active Information Spaces based on Ubiquitous Computing</data>
      <data key="e_abstract">The project researches a new form of operating system to manage a model of&lt;br/&gt;computing called an Active Space. It integrates physical spaces that contain&lt;br/&gt;ubiquitous computers into a computational environment that supports human&lt;br/&gt;activity and applications. With anytime/anywhere ubiquitous devices, the&lt;br/&gt;users&apos; view of the computational environment is extended beyond the physical&lt;br/&gt;limits of a computer and is placed into the surrounding physical space,&lt;br/&gt;augmented with computers that sense and affect that space around the user.&lt;br/&gt;Applications become mapped not just to views associated with specific&lt;br/&gt;windows in a monitor but instead to the physical environment. Therefore,&lt;br/&gt;the physical space, augmented with communicating computer devices, becomes a&lt;br/&gt;distributed computing system.&lt;br/&gt;Active Spaces have the potential for creating multi-billion dollar&lt;br/&gt;industries. Automated surgery, collaboration and engaged learning are a few&lt;br/&gt;of the compelling examples. Gaia, an operating system for Active Spaces,&lt;br/&gt;will accommodate diversity by exploiting standards for interoperation and&lt;br/&gt;cooperation. System services track, authenticate and support mobile users&lt;br/&gt;with reconfigurable graphics, multimedia and Active Space applications. A&lt;br/&gt;unifying object bus, component model, and adaptive stream model extends plug&lt;br/&gt;and play to distributed mobile ubiquitous computers cooperating to support a&lt;br/&gt;computational environment within physical spaces like cities, buildings and&lt;br/&gt;rooms.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86094</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86094</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n404" target="n922">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Active Information Spaces based on Ubiquitous Computing</data>
      <data key="e_abstract">The project researches a new form of operating system to manage a model of&lt;br/&gt;computing called an Active Space. It integrates physical spaces that contain&lt;br/&gt;ubiquitous computers into a computational environment that supports human&lt;br/&gt;activity and applications. With anytime/anywhere ubiquitous devices, the&lt;br/&gt;users&apos; view of the computational environment is extended beyond the physical&lt;br/&gt;limits of a computer and is placed into the surrounding physical space,&lt;br/&gt;augmented with computers that sense and affect that space around the user.&lt;br/&gt;Applications become mapped not just to views associated with specific&lt;br/&gt;windows in a monitor but instead to the physical environment. Therefore,&lt;br/&gt;the physical space, augmented with communicating computer devices, becomes a&lt;br/&gt;distributed computing system.&lt;br/&gt;Active Spaces have the potential for creating multi-billion dollar&lt;br/&gt;industries. Automated surgery, collaboration and engaged learning are a few&lt;br/&gt;of the compelling examples. Gaia, an operating system for Active Spaces,&lt;br/&gt;will accommodate diversity by exploiting standards for interoperation and&lt;br/&gt;cooperation. System services track, authenticate and support mobile users&lt;br/&gt;with reconfigurable graphics, multimedia and Active Space applications. A&lt;br/&gt;unifying object bus, component model, and adaptive stream model extends plug&lt;br/&gt;and play to distributed mobile ubiquitous computers cooperating to support a&lt;br/&gt;computational environment within physical spaces like cities, buildings and&lt;br/&gt;rooms.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86094</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86094</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n404" target="n1456">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Active Information Spaces based on Ubiquitous Computing</data>
      <data key="e_abstract">The project researches a new form of operating system to manage a model of&lt;br/&gt;computing called an Active Space. It integrates physical spaces that contain&lt;br/&gt;ubiquitous computers into a computational environment that supports human&lt;br/&gt;activity and applications. With anytime/anywhere ubiquitous devices, the&lt;br/&gt;users&apos; view of the computational environment is extended beyond the physical&lt;br/&gt;limits of a computer and is placed into the surrounding physical space,&lt;br/&gt;augmented with computers that sense and affect that space around the user.&lt;br/&gt;Applications become mapped not just to views associated with specific&lt;br/&gt;windows in a monitor but instead to the physical environment. Therefore,&lt;br/&gt;the physical space, augmented with communicating computer devices, becomes a&lt;br/&gt;distributed computing system.&lt;br/&gt;Active Spaces have the potential for creating multi-billion dollar&lt;br/&gt;industries. Automated surgery, collaboration and engaged learning are a few&lt;br/&gt;of the compelling examples. Gaia, an operating system for Active Spaces,&lt;br/&gt;will accommodate diversity by exploiting standards for interoperation and&lt;br/&gt;cooperation. System services track, authenticate and support mobile users&lt;br/&gt;with reconfigurable graphics, multimedia and Active Space applications. A&lt;br/&gt;unifying object bus, component model, and adaptive stream model extends plug&lt;br/&gt;and play to distributed mobile ubiquitous computers cooperating to support a&lt;br/&gt;computational environment within physical spaces like cities, buildings and&lt;br/&gt;rooms.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86094</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86094</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n922" target="n1456">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Active Information Spaces based on Ubiquitous Computing</data>
      <data key="e_abstract">The project researches a new form of operating system to manage a model of&lt;br/&gt;computing called an Active Space. It integrates physical spaces that contain&lt;br/&gt;ubiquitous computers into a computational environment that supports human&lt;br/&gt;activity and applications. With anytime/anywhere ubiquitous devices, the&lt;br/&gt;users&apos; view of the computational environment is extended beyond the physical&lt;br/&gt;limits of a computer and is placed into the surrounding physical space,&lt;br/&gt;augmented with computers that sense and affect that space around the user.&lt;br/&gt;Applications become mapped not just to views associated with specific&lt;br/&gt;windows in a monitor but instead to the physical environment. Therefore,&lt;br/&gt;the physical space, augmented with communicating computer devices, becomes a&lt;br/&gt;distributed computing system.&lt;br/&gt;Active Spaces have the potential for creating multi-billion dollar&lt;br/&gt;industries. Automated surgery, collaboration and engaged learning are a few&lt;br/&gt;of the compelling examples. Gaia, an operating system for Active Spaces,&lt;br/&gt;will accommodate diversity by exploiting standards for interoperation and&lt;br/&gt;cooperation. System services track, authenticate and support mobile users&lt;br/&gt;with reconfigurable graphics, multimedia and Active Space applications. A&lt;br/&gt;unifying object bus, component model, and adaptive stream model extends plug&lt;br/&gt;and play to distributed mobile ubiquitous computers cooperating to support a&lt;br/&gt;computational environment within physical spaces like cities, buildings and&lt;br/&gt;rooms.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86094</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86094</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1458" target="n1459">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Intelligent Data Analysis for Identifying Protein Disorder</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">6856</data>
      <data key="e_label">196237</data>
      <data key="e_expirationDate">2002-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">196237</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1461" target="n1462">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Distributed Bandwidth Management and Pricing in High-Speed Networks</data>
      <data key="e_abstract">Bandwidth management and pricing of elastic services is key to the efficient and profitable running of emerging high-speed networks. Elastic traffic or traffic which can alter its rate characteristics to adapt to the congestion state of the network, such as TCP/IP traffic, already forms a major part of the traffic carried by today&apos;s networks, the Internet being the current paradigm. In networks where the basic bandwidth availability and user characteristics are not efficiently priced as well as managed, profitability or efficiency is reduced since pricing and consequent user satisfaction are going to determine demand and useful throughput (\goodput&quot;). While the ideas of dynamic optimal resource allocation and pricing based on user resource requests and budgets are very attractive they cannot be implemented in a large network for scalability reasons. This is due to both communication as well as computational overheads. Hence, there is a need for creating a hierarchical structure and to perform aggregation in order to make the concept implementable and useful. However, the researchers believe that there is a need for a well-defined framework so that they can understand and quantify the trade-offs that have been made in terms of efficiency, overhead, fairness, complexity, responsiveness (especially if the propagation delays are large) and robustness. Other issues very much related to implementation are linked to measurement of congestion and to convergence and stability of efficient and computationally feasible distributed algorithms.&lt;br/&gt; The proposed research will:&lt;br/&gt; (1) Further develop and refine a game theoretic framework proposed by the PIs for bandwidth allocation for elastic traffic. The allocation maximizes the efficiency (in terms of network revenue) of network utilization and incorporates the crucial notion of fairness. The main thrust will be the development of appropriate solution concepts in non-static environments.&lt;br/&gt; (2) Use the above framework to develop pricing structures which will lead to network efficiency while providing user level satisfaction.&lt;br/&gt; (3) Develop distributed algorithms which enable the proper allocation of bandwidth as defined through game theory above based on users&apos; willingness to pay and bandwidth demands. In particular, the issues of&lt;br/&gt;network measurements and local information will be addressed. This can be seen as either a solution for a&lt;br/&gt;small network or as a benchmark to which the researchers proposed solutions incorporating hierarchy and aggregation will be compared.&lt;br/&gt; (4) Design of scalable solutions incorporating aggregation and an appropriate hierarchical structure of the network, i.e., user level, groups of users, etc. Particular attention will be paid to trade-offs in terms of&lt;br/&gt;performance and complexity. Both the pure bandwidth allocation based on performance as well as pricing&lt;br/&gt;based cases will be considered.&lt;br/&gt; (5) Study in the implementation of these solutions in the context of network protocols such as TCP/IP. This will involve issues related to stability, convergence, and adaptivity to changing conditions.&lt;br/&gt; (6) Develop notions of network bandwidth derivatives or option pricing for booking or provisioning of bandwidth resources based on a Black-Scholes paradigm. This is another example of the techniques that the researchers plan to use. This one will be used at an intermediate level of the researchers&apos; network hierarchy, namely between a service provider and a network operator.&lt;br/&gt; The researchers expect that the proposed work will involve: advances in applications of game theory; contributions to the notions of aggregation and fairness; development of an approach to bandwidth options; optimal pricing structures; development of feasible real-time algorithms for bandwidth allocation; and contributions to implementation issues based on available information. The techniques will be drawn from game theory, Lyapunov theory, convergence of stochastic algorithms, nonlinear constrained optimization, mathematics of finance, and stochastic analysis.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">73359</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">73359</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n33" target="n34">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Authentication Technologies and Their Privacy Implications</data>
      <data key="e_abstract">In response to a request to CSTB from the United States Government&apos;s Chief Counselor for Privacy, CSTB has framed an assessment of emerging approaches to authentication in computing and communications systems that focuses on the implications of authentication technologies for privacy. This assessment of authentication technologies would examine the state of the art and relevant trends. It would put the technology into a larger context: it would consider technical and nontechnical trends, addressing both the nature of capabilities and their implementation, and the nature of procedural and other nontechnical protections and their enforceability. It would examine differences in concerns associated with public sector versus private sector uses of authentication technologies. Attention would be paid to the likelihood of different mixes of options (e.g., diverse techniques to accommodate, a diverse population and set of needs versus narrower standardization for economic reasons.&lt;br/&gt; A report will be produced by the committee and subject to NRC approval. It will describe authentication technologies, the interplay of technical and nontechnical aspects of authentication, and the implications of alternative approaches for privacy; it will make recommendations about fostering relevant research and shaping appropriate policy. It will be distributred to the computer science and privacy communities, associated organizations, consumer organizations, and government agencies and the congress. Web distribution, briefings, discussions at key conferences and in relevant National Academies venues, and articles and announcements in journals and newsletters would be important to getting the word out and furthering discussion. The dissemination process would be launched by a public briefing about the report.</data>
      <data key="e_pgm">4090</data>
      <data key="e_label">90219</data>
      <data key="e_expirationDate">2004-02-29</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90219</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n154" target="n1470">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Digital Government: I2T: An Information Integration Testbed for Digital Government</data>
      <data key="e_abstract">EIA-9983510&lt;br/&gt;Baru, Chaitanya K&lt;br/&gt;University of California&lt;br/&gt;&lt;br/&gt;Digital Government: 12T An Information Testbed for Digital Government&lt;br/&gt;&lt;br/&gt;This project will address one of the major problems in government information systems, the inability to integrated information from various heterogeneous data sources. Usually these data are collected and managed by different agencies at different levels of government, providing more impediments to integration. Partners from the Bureau of the Census, National Archives and Records Administration, US Geological Survey, the State of Pennsylvania, and the San Diego Association of Governments will work with researchers from the San Diego Supercomputer Center, the University of California at San Diego, the University of Michigan, and the University of Pennsylvania.&lt;br/&gt;&lt;br/&gt;Building upon the initial work of the Mediation of Information using XML (MIX) project, this grant has four major technical thrusts:&lt;br/&gt;&lt;br/&gt;1. allow for an extension of MIX&apos;s wrapper technology to the domain of geospatial information,&lt;br/&gt;2. develop data transfer protocols for lightweight network-based agents,&lt;br/&gt;3. investigate new interfaces to the data, and&lt;br/&gt;4. build wrapper toolkits for geospatial and statistical survey metadata.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98351e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98351e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n154" target="n155">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Digital Government: I2T: An Information Integration Testbed for Digital Government</data>
      <data key="e_abstract">EIA-9983510&lt;br/&gt;Baru, Chaitanya K&lt;br/&gt;University of California&lt;br/&gt;&lt;br/&gt;Digital Government: 12T An Information Testbed for Digital Government&lt;br/&gt;&lt;br/&gt;This project will address one of the major problems in government information systems, the inability to integrated information from various heterogeneous data sources. Usually these data are collected and managed by different agencies at different levels of government, providing more impediments to integration. Partners from the Bureau of the Census, National Archives and Records Administration, US Geological Survey, the State of Pennsylvania, and the San Diego Association of Governments will work with researchers from the San Diego Supercomputer Center, the University of California at San Diego, the University of Michigan, and the University of Pennsylvania.&lt;br/&gt;&lt;br/&gt;Building upon the initial work of the Mediation of Information using XML (MIX) project, this grant has four major technical thrusts:&lt;br/&gt;&lt;br/&gt;1. allow for an extension of MIX&apos;s wrapper technology to the domain of geospatial information,&lt;br/&gt;2. develop data transfer protocols for lightweight network-based agents,&lt;br/&gt;3. investigate new interfaces to the data, and&lt;br/&gt;4. build wrapper toolkits for geospatial and statistical survey metadata.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98351e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98351e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n155" target="n1470">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Digital Government: I2T: An Information Integration Testbed for Digital Government</data>
      <data key="e_abstract">EIA-9983510&lt;br/&gt;Baru, Chaitanya K&lt;br/&gt;University of California&lt;br/&gt;&lt;br/&gt;Digital Government: 12T An Information Testbed for Digital Government&lt;br/&gt;&lt;br/&gt;This project will address one of the major problems in government information systems, the inability to integrated information from various heterogeneous data sources. Usually these data are collected and managed by different agencies at different levels of government, providing more impediments to integration. Partners from the Bureau of the Census, National Archives and Records Administration, US Geological Survey, the State of Pennsylvania, and the San Diego Association of Governments will work with researchers from the San Diego Supercomputer Center, the University of California at San Diego, the University of Michigan, and the University of Pennsylvania.&lt;br/&gt;&lt;br/&gt;Building upon the initial work of the Mediation of Information using XML (MIX) project, this grant has four major technical thrusts:&lt;br/&gt;&lt;br/&gt;1. allow for an extension of MIX&apos;s wrapper technology to the domain of geospatial information,&lt;br/&gt;2. develop data transfer protocols for lightweight network-based agents,&lt;br/&gt;3. investigate new interfaces to the data, and&lt;br/&gt;4. build wrapper toolkits for geospatial and statistical survey metadata.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98351e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98351e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1472" target="n1473">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Women-friendly Environments for Learning Information Technology</data>
      <data key="e_abstract">EIA-0082771&lt;br/&gt;Walters, Deborah&lt;br/&gt;SUNY at Buffalo&lt;br/&gt;&lt;br/&gt;ITR: Women-Friendly Environments for Learning Information Technology&lt;br/&gt;&lt;br/&gt;There is currently an unsatisfied national demand for people trained in&lt;br/&gt;information technology (IT) with a minority of the students enrolled in IT&lt;br/&gt;programs being female. The proposed project is to develop an educational&lt;br/&gt;approach that will appeal to women and provide them with an appropriate&lt;br/&gt;learning environment for IT courses. There are four components to the&lt;br/&gt;project. First, will be efforts to build upon results of investigations on&lt;br/&gt;how to use technology to increase learning and decrease costs as taught&lt;br/&gt;within an IT fluency course. Second, will be research on learning styles&lt;br/&gt;to provide quantitative evidence to explore the validity of learning styles&lt;br/&gt;in general, and the consistency of women&apos;s learning styles, particularly,&lt;br/&gt;correlations between learning styles, learning environments and&lt;br/&gt;performance. Third, will be research on how to improve Web-based&lt;br/&gt;collaborative experiences. Fourth, will be the testing of findings through&lt;br/&gt;comparisons of learning achieved in a traditional environment with learning&lt;br/&gt;in an environment restructured to take into account the project&apos;s findings&lt;br/&gt;and those of other researchers in the field. The specific application of&lt;br/&gt;the project will be the development an a women-friendly information design&lt;br/&gt;and technology undergraduate certificate program that is accessible to&lt;br/&gt;students of any major. The introductory courses of the certificate program&lt;br/&gt;will be designed so that students can move easily into either a traditional&lt;br/&gt;computer science or media studies program.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82771</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82771</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1472" target="n1474">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Women-friendly Environments for Learning Information Technology</data>
      <data key="e_abstract">EIA-0082771&lt;br/&gt;Walters, Deborah&lt;br/&gt;SUNY at Buffalo&lt;br/&gt;&lt;br/&gt;ITR: Women-Friendly Environments for Learning Information Technology&lt;br/&gt;&lt;br/&gt;There is currently an unsatisfied national demand for people trained in&lt;br/&gt;information technology (IT) with a minority of the students enrolled in IT&lt;br/&gt;programs being female. The proposed project is to develop an educational&lt;br/&gt;approach that will appeal to women and provide them with an appropriate&lt;br/&gt;learning environment for IT courses. There are four components to the&lt;br/&gt;project. First, will be efforts to build upon results of investigations on&lt;br/&gt;how to use technology to increase learning and decrease costs as taught&lt;br/&gt;within an IT fluency course. Second, will be research on learning styles&lt;br/&gt;to provide quantitative evidence to explore the validity of learning styles&lt;br/&gt;in general, and the consistency of women&apos;s learning styles, particularly,&lt;br/&gt;correlations between learning styles, learning environments and&lt;br/&gt;performance. Third, will be research on how to improve Web-based&lt;br/&gt;collaborative experiences. Fourth, will be the testing of findings through&lt;br/&gt;comparisons of learning achieved in a traditional environment with learning&lt;br/&gt;in an environment restructured to take into account the project&apos;s findings&lt;br/&gt;and those of other researchers in the field. The specific application of&lt;br/&gt;the project will be the development an a women-friendly information design&lt;br/&gt;and technology undergraduate certificate program that is accessible to&lt;br/&gt;students of any major. The introductory courses of the certificate program&lt;br/&gt;will be designed so that students can move easily into either a traditional&lt;br/&gt;computer science or media studies program.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82771</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82771</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1473" target="n1474">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Women-friendly Environments for Learning Information Technology</data>
      <data key="e_abstract">EIA-0082771&lt;br/&gt;Walters, Deborah&lt;br/&gt;SUNY at Buffalo&lt;br/&gt;&lt;br/&gt;ITR: Women-Friendly Environments for Learning Information Technology&lt;br/&gt;&lt;br/&gt;There is currently an unsatisfied national demand for people trained in&lt;br/&gt;information technology (IT) with a minority of the students enrolled in IT&lt;br/&gt;programs being female. The proposed project is to develop an educational&lt;br/&gt;approach that will appeal to women and provide them with an appropriate&lt;br/&gt;learning environment for IT courses. There are four components to the&lt;br/&gt;project. First, will be efforts to build upon results of investigations on&lt;br/&gt;how to use technology to increase learning and decrease costs as taught&lt;br/&gt;within an IT fluency course. Second, will be research on learning styles&lt;br/&gt;to provide quantitative evidence to explore the validity of learning styles&lt;br/&gt;in general, and the consistency of women&apos;s learning styles, particularly,&lt;br/&gt;correlations between learning styles, learning environments and&lt;br/&gt;performance. Third, will be research on how to improve Web-based&lt;br/&gt;collaborative experiences. Fourth, will be the testing of findings through&lt;br/&gt;comparisons of learning achieved in a traditional environment with learning&lt;br/&gt;in an environment restructured to take into account the project&apos;s findings&lt;br/&gt;and those of other researchers in the field. The specific application of&lt;br/&gt;the project will be the development an a women-friendly information design&lt;br/&gt;and technology undergraduate certificate program that is accessible to&lt;br/&gt;students of any major. The introductory courses of the certificate program&lt;br/&gt;will be designed so that students can move easily into either a traditional&lt;br/&gt;computer science or media studies program.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82771</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82771</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1475" target="n1476">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Survey2001: Information Technology&apos;s Impact on Community, Culture and Conservation</data>
      <data key="e_abstract">In 1998 academic researchers combined with the National Geographic Society (NGS) to design and implement &quot;Survey2000: Charting Communities and Change,&quot; an effort that explores the potential and limits of web-based survey research. A second survey effort to be hosted by the NGS website in the fall of the year 2001 is the cornerstone of this project. The substantive focus of Survey2001 centers on the impact of information technology on changing perceptions of global and local spheres in contemporary society. In particular, the survey examines respondents&apos; perceptions and understanding of &quot;global&quot; and &quot;local&quot; in three areas: community, culture and conservation. The survey explores the extent to which new information technology has redefined the distinction between global and local. A second aim is to consider methodological issues related to web survey research, in particular the non-random nature of a web survey sample. Toward this end, the project calls for a parallel telephone survey effort using standard random digit dialing techniques to replicate the web survey. Beyond questions of sampling and validity, Survey2001will pursue issues regarding web survey instrument development and implementation, as well as instrument and design effects in web surveys. Collaboration with the NGS assures that the results will receive broad public dissemination.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82750</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82750</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1477" target="n1478">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Design of Supervisory Control Software for Dynamic Systems with Decentralized Information</data>
      <data key="e_abstract">Rapid advances in computation and communication technologies are giving rise to an ever wider use of automation in the monitoring and control of dynamic engineering and computing systems in&lt;br/&gt;manufacturing, communication networks, transportation, and electric power systems, to mention but a few key areas. The software programs that monitor and control these systems are responsible for various functions, including start-up and shut-down procedures, monitoring and control, detection and isolation of significant events, system reconfiguration, etc. We refer to all of these software programs as the &quot;supervisory control software&quot;. This proposal is focused on systems that are distributed over several sites and where the supervisory control software is distributed as well. The goal is to conduct fundamental research on novel formal generic methodologies for designing distributed software systems that monitor and control systems with decentralized information. We propose to bring the concepts and techniques of Systems, Control, and Decision Theory to bear on the problem of designing distributed software systems that are provably correct and efficient. As part of this effort, we will educate computer science and engineering students at the University of Michigan and Wayne State University on the fundamental issues associated with distributed software design.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82784</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82784</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1477" target="n1479">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Design of Supervisory Control Software for Dynamic Systems with Decentralized Information</data>
      <data key="e_abstract">Rapid advances in computation and communication technologies are giving rise to an ever wider use of automation in the monitoring and control of dynamic engineering and computing systems in&lt;br/&gt;manufacturing, communication networks, transportation, and electric power systems, to mention but a few key areas. The software programs that monitor and control these systems are responsible for various functions, including start-up and shut-down procedures, monitoring and control, detection and isolation of significant events, system reconfiguration, etc. We refer to all of these software programs as the &quot;supervisory control software&quot;. This proposal is focused on systems that are distributed over several sites and where the supervisory control software is distributed as well. The goal is to conduct fundamental research on novel formal generic methodologies for designing distributed software systems that monitor and control systems with decentralized information. We propose to bring the concepts and techniques of Systems, Control, and Decision Theory to bear on the problem of designing distributed software systems that are provably correct and efficient. As part of this effort, we will educate computer science and engineering students at the University of Michigan and Wayne State University on the fundamental issues associated with distributed software design.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82784</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82784</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1478" target="n1479">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Design of Supervisory Control Software for Dynamic Systems with Decentralized Information</data>
      <data key="e_abstract">Rapid advances in computation and communication technologies are giving rise to an ever wider use of automation in the monitoring and control of dynamic engineering and computing systems in&lt;br/&gt;manufacturing, communication networks, transportation, and electric power systems, to mention but a few key areas. The software programs that monitor and control these systems are responsible for various functions, including start-up and shut-down procedures, monitoring and control, detection and isolation of significant events, system reconfiguration, etc. We refer to all of these software programs as the &quot;supervisory control software&quot;. This proposal is focused on systems that are distributed over several sites and where the supervisory control software is distributed as well. The goal is to conduct fundamental research on novel formal generic methodologies for designing distributed software systems that monitor and control systems with decentralized information. We propose to bring the concepts and techniques of Systems, Control, and Decision Theory to bear on the problem of designing distributed software systems that are provably correct and efficient. As part of this effort, we will educate computer science and engineering students at the University of Michigan and Wayne State University on the fundamental issues associated with distributed software design.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82784</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82784</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1480" target="n1481">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: A Distributed Simulation Infrastructure for a K-12 Inquiry Environment</data>
      <data key="e_abstract">EIA-0082791&lt;br/&gt;Gottschalk, Thomas&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;ITR: A Distributed Simulation Infrastructure for a K-12 Inquiry Environment&lt;br/&gt;&lt;br/&gt;This research involves planning the construction of a scaleable,&lt;br/&gt;persistent, interactive event simulation framework for supporting&lt;br/&gt;knowledge-building curricula within K-12 education. The proposed system&lt;br/&gt;uses advanced simulation and information technology methods to create an&lt;br/&gt;inquiry environment, fostering cooperative interactions and knowledge&lt;br/&gt;co-creation while students work together to solve curriculum-based problems&lt;br/&gt;and challenges. The technical basis of such an inquiry environment is a&lt;br/&gt;metacomputing system combining distributed discrete event simulations,&lt;br/&gt;information and operational databases, graphical control interfaces, and&lt;br/&gt;visualization. While much of the system can be built from familiar pieces,&lt;br/&gt;the focus on education provides a number of interesting opportunities for&lt;br/&gt;new research in computing and metacomputing strategies.&lt;br/&gt;&lt;br/&gt;The general area of user-directed simulations is viewed as a particular&lt;br/&gt;area in which High Performance Computing (HPC) can become a true enabling&lt;br/&gt;technology for substantive educational reform. The proposed work will plan&lt;br/&gt;the exploration, development, and extension of HPC technologies for K-12&lt;br/&gt;education. This work relies on strong partnerships in cognitive science&lt;br/&gt;and education to ensure that the technology features and capabilities are&lt;br/&gt;matched to objectives within progressive models of knowledge and learning.&lt;br/&gt;The K-12 simulation framework that could result from this work may be&lt;br/&gt;regarded as a first step in the larger task of matching HPC capabilities to&lt;br/&gt;contemporary cognitive science and educational research, enabling&lt;br/&gt;substantive advances in learning.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82791</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82791</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1480" target="n1482">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: A Distributed Simulation Infrastructure for a K-12 Inquiry Environment</data>
      <data key="e_abstract">EIA-0082791&lt;br/&gt;Gottschalk, Thomas&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;ITR: A Distributed Simulation Infrastructure for a K-12 Inquiry Environment&lt;br/&gt;&lt;br/&gt;This research involves planning the construction of a scaleable,&lt;br/&gt;persistent, interactive event simulation framework for supporting&lt;br/&gt;knowledge-building curricula within K-12 education. The proposed system&lt;br/&gt;uses advanced simulation and information technology methods to create an&lt;br/&gt;inquiry environment, fostering cooperative interactions and knowledge&lt;br/&gt;co-creation while students work together to solve curriculum-based problems&lt;br/&gt;and challenges. The technical basis of such an inquiry environment is a&lt;br/&gt;metacomputing system combining distributed discrete event simulations,&lt;br/&gt;information and operational databases, graphical control interfaces, and&lt;br/&gt;visualization. While much of the system can be built from familiar pieces,&lt;br/&gt;the focus on education provides a number of interesting opportunities for&lt;br/&gt;new research in computing and metacomputing strategies.&lt;br/&gt;&lt;br/&gt;The general area of user-directed simulations is viewed as a particular&lt;br/&gt;area in which High Performance Computing (HPC) can become a true enabling&lt;br/&gt;technology for substantive educational reform. The proposed work will plan&lt;br/&gt;the exploration, development, and extension of HPC technologies for K-12&lt;br/&gt;education. This work relies on strong partnerships in cognitive science&lt;br/&gt;and education to ensure that the technology features and capabilities are&lt;br/&gt;matched to objectives within progressive models of knowledge and learning.&lt;br/&gt;The K-12 simulation framework that could result from this work may be&lt;br/&gt;regarded as a first step in the larger task of matching HPC capabilities to&lt;br/&gt;contemporary cognitive science and educational research, enabling&lt;br/&gt;substantive advances in learning.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82791</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82791</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1481" target="n1482">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: A Distributed Simulation Infrastructure for a K-12 Inquiry Environment</data>
      <data key="e_abstract">EIA-0082791&lt;br/&gt;Gottschalk, Thomas&lt;br/&gt;California Institute of Technology&lt;br/&gt;&lt;br/&gt;ITR: A Distributed Simulation Infrastructure for a K-12 Inquiry Environment&lt;br/&gt;&lt;br/&gt;This research involves planning the construction of a scaleable,&lt;br/&gt;persistent, interactive event simulation framework for supporting&lt;br/&gt;knowledge-building curricula within K-12 education. The proposed system&lt;br/&gt;uses advanced simulation and information technology methods to create an&lt;br/&gt;inquiry environment, fostering cooperative interactions and knowledge&lt;br/&gt;co-creation while students work together to solve curriculum-based problems&lt;br/&gt;and challenges. The technical basis of such an inquiry environment is a&lt;br/&gt;metacomputing system combining distributed discrete event simulations,&lt;br/&gt;information and operational databases, graphical control interfaces, and&lt;br/&gt;visualization. While much of the system can be built from familiar pieces,&lt;br/&gt;the focus on education provides a number of interesting opportunities for&lt;br/&gt;new research in computing and metacomputing strategies.&lt;br/&gt;&lt;br/&gt;The general area of user-directed simulations is viewed as a particular&lt;br/&gt;area in which High Performance Computing (HPC) can become a true enabling&lt;br/&gt;technology for substantive educational reform. The proposed work will plan&lt;br/&gt;the exploration, development, and extension of HPC technologies for K-12&lt;br/&gt;education. This work relies on strong partnerships in cognitive science&lt;br/&gt;and education to ensure that the technology features and capabilities are&lt;br/&gt;matched to objectives within progressive models of knowledge and learning.&lt;br/&gt;The K-12 simulation framework that could result from this work may be&lt;br/&gt;regarded as a first step in the larger task of matching HPC capabilities to&lt;br/&gt;contemporary cognitive science and educational research, enabling&lt;br/&gt;substantive advances in learning.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82791</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82791</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1483" target="n1484">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Design Conformant Software</data>
      <data key="e_abstract">The proposed research will investigate new software engineering&lt;br/&gt;techniques and tools for infrastructural software that will improve&lt;br/&gt;its reliability, safety, and predictability. The key idea is to use&lt;br/&gt;abstract design models to drive new static analyses that check that&lt;br/&gt;the software correctly implements its design (and if not, identify the&lt;br/&gt;source of the problem). To ensure that our research addresses the&lt;br/&gt;important issues that developers face in the field, we will conduct&lt;br/&gt;our research in the context of the development of an air traffic&lt;br/&gt;control system component. Specifically, the research will investigate&lt;br/&gt;the use of object models to express important design properties and&lt;br/&gt;new pointer analysis algorithms to verify that the code correctly&lt;br/&gt;implements the object models. Object models describe essential object&lt;br/&gt;in the heap and the relationships between them; pointer analysis&lt;br/&gt;automatically analyzes code to extract information about how objects&lt;br/&gt;refer to each other. The research will investigate techniques that&lt;br/&gt;improve the precision of the pointer analysis by using the object&lt;br/&gt;model to focus the analysis on the properties of interest.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86154</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86154</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n916" target="n1483">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Design Conformant Software</data>
      <data key="e_abstract">The proposed research will investigate new software engineering&lt;br/&gt;techniques and tools for infrastructural software that will improve&lt;br/&gt;its reliability, safety, and predictability. The key idea is to use&lt;br/&gt;abstract design models to drive new static analyses that check that&lt;br/&gt;the software correctly implements its design (and if not, identify the&lt;br/&gt;source of the problem). To ensure that our research addresses the&lt;br/&gt;important issues that developers face in the field, we will conduct&lt;br/&gt;our research in the context of the development of an air traffic&lt;br/&gt;control system component. Specifically, the research will investigate&lt;br/&gt;the use of object models to express important design properties and&lt;br/&gt;new pointer analysis algorithms to verify that the code correctly&lt;br/&gt;implements the object models. Object models describe essential object&lt;br/&gt;in the heap and the relationships between them; pointer analysis&lt;br/&gt;automatically analyzes code to extract information about how objects&lt;br/&gt;refer to each other. The research will investigate techniques that&lt;br/&gt;improve the precision of the pointer analysis by using the object&lt;br/&gt;model to focus the analysis on the properties of interest.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86154</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86154</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n916" target="n1484">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITR: Design Conformant Software</data>
      <data key="e_abstract">The proposed research will investigate new software engineering&lt;br/&gt;techniques and tools for infrastructural software that will improve&lt;br/&gt;its reliability, safety, and predictability. The key idea is to use&lt;br/&gt;abstract design models to drive new static analyses that check that&lt;br/&gt;the software correctly implements its design (and if not, identify the&lt;br/&gt;source of the problem). To ensure that our research addresses the&lt;br/&gt;important issues that developers face in the field, we will conduct&lt;br/&gt;our research in the context of the development of an air traffic&lt;br/&gt;control system component. Specifically, the research will investigate&lt;br/&gt;the use of object models to express important design properties and&lt;br/&gt;new pointer analysis algorithms to verify that the code correctly&lt;br/&gt;implements the object models. Object models describe essential object&lt;br/&gt;in the heap and the relationships between them; pointer analysis&lt;br/&gt;automatically analyzes code to extract information about how objects&lt;br/&gt;refer to each other. The research will investigate techniques that&lt;br/&gt;improve the precision of the pointer analysis by using the object&lt;br/&gt;model to focus the analysis on the properties of interest.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86154</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">86154</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n739" target="n1488">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research: Real-time Capture, Management and Reconstruction of Spatio-Temporal Events</data>
      <data key="e_abstract">With the advances in embedded processors, low cost sensor technologies, and wireless communication, unprecedented amounts of diverse types of information about the real world and its activities are being generated. Much of the information is spatio-temporal in nature; concerning objects dispersed in space and time, and interacting and communicating with each other and their surroundings. An infrastructure that facilitates real-time capture, storage, processing, display, and analysis of the information generated will truly revolutionize a wide variety of application domains. Examples of domains that will benefit from this technology include avionics, ground traffic, commercial applications such as ship-ping and transportation, emergency response and disaster relief operations, physical phenomenon such as weather and storm tracking, forest fire tracking, migration patterns of animals/birds, command and control, smart environments, etc. Applications in the above domains require real-time monitoring, tracking and analysis of objects/events/phenomena in space and time. &lt;br/&gt;&lt;br/&gt;An integral component of such sensor enriched communication and information infrastructure is a database management technology that allows seamless access to information dispersed across a hierarchy of storage, communication and processing units - from sensor devices, where data originates, to large data banks where the information generated is stored for analysis and mining. This research will explore next generation database management system technology that provides effective support for information processing in highly distributed and dynamic sensor-enriched environments. The approach taken will be end-to-end - that is, research will be conducted on all aspects of the system ranging from representation, data modeling, query languages, data structures, query optimization, query processing, distribution, and concurrent accesses. A prototype database management infrastructure that supports highly dynamic geographically dispersed spatio-temporal data, multi-resolution representation of data, and provides effective support for visualization and analysis will be developed.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86144</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">86144</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1489" target="n1490">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Systems for Learning Science and Assessing Student Learning</data>
      <data key="e_abstract">EIA- 0086142&lt;br/&gt;Slator, Brian&lt;br/&gt;North Dakota State University, Fargo&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Systems for Learning Science and Assessing Student Learning&lt;br/&gt;&lt;br/&gt;This five-year project to study science learning in authentic, immersive,&lt;br/&gt;virtual environments involving 1) simulated environments for teaching&lt;br/&gt;science topics, each framed according to a theoretical approach, role-based&lt;br/&gt;learning, 2) an innovative, integrated, distributed software platform for&lt;br/&gt;developing and hosting virtual environments, 3) empirical studies using an&lt;br/&gt;innovative protocol, scenario-based assessment, for measuring student&lt;br/&gt;learning in virtual worlds, and 4) a graduate-level summer school course&lt;br/&gt;for in-service teachers who will be trained, beginning in year three, to&lt;br/&gt;use virtual environments in their classrooms. This interdisciplinary&lt;br/&gt;project, in part, depends on fundamental computer science research in the&lt;br/&gt;areas of distributed systems, software agents and intelligent tutoring, and&lt;br/&gt;virtual environments. The intent of this research is to produce a large,&lt;br/&gt;controlled study demonstrating the statistical significance of the impact&lt;br/&gt;of the above methods on student learning.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86142</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86142</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1489" target="n1491">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Systems for Learning Science and Assessing Student Learning</data>
      <data key="e_abstract">EIA- 0086142&lt;br/&gt;Slator, Brian&lt;br/&gt;North Dakota State University, Fargo&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Systems for Learning Science and Assessing Student Learning&lt;br/&gt;&lt;br/&gt;This five-year project to study science learning in authentic, immersive,&lt;br/&gt;virtual environments involving 1) simulated environments for teaching&lt;br/&gt;science topics, each framed according to a theoretical approach, role-based&lt;br/&gt;learning, 2) an innovative, integrated, distributed software platform for&lt;br/&gt;developing and hosting virtual environments, 3) empirical studies using an&lt;br/&gt;innovative protocol, scenario-based assessment, for measuring student&lt;br/&gt;learning in virtual worlds, and 4) a graduate-level summer school course&lt;br/&gt;for in-service teachers who will be trained, beginning in year three, to&lt;br/&gt;use virtual environments in their classrooms. This interdisciplinary&lt;br/&gt;project, in part, depends on fundamental computer science research in the&lt;br/&gt;areas of distributed systems, software agents and intelligent tutoring, and&lt;br/&gt;virtual environments. The intent of this research is to produce a large,&lt;br/&gt;controlled study demonstrating the statistical significance of the impact&lt;br/&gt;of the above methods on student learning.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86142</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86142</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1489" target="n1492">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Systems for Learning Science and Assessing Student Learning</data>
      <data key="e_abstract">EIA- 0086142&lt;br/&gt;Slator, Brian&lt;br/&gt;North Dakota State University, Fargo&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Systems for Learning Science and Assessing Student Learning&lt;br/&gt;&lt;br/&gt;This five-year project to study science learning in authentic, immersive,&lt;br/&gt;virtual environments involving 1) simulated environments for teaching&lt;br/&gt;science topics, each framed according to a theoretical approach, role-based&lt;br/&gt;learning, 2) an innovative, integrated, distributed software platform for&lt;br/&gt;developing and hosting virtual environments, 3) empirical studies using an&lt;br/&gt;innovative protocol, scenario-based assessment, for measuring student&lt;br/&gt;learning in virtual worlds, and 4) a graduate-level summer school course&lt;br/&gt;for in-service teachers who will be trained, beginning in year three, to&lt;br/&gt;use virtual environments in their classrooms. This interdisciplinary&lt;br/&gt;project, in part, depends on fundamental computer science research in the&lt;br/&gt;areas of distributed systems, software agents and intelligent tutoring, and&lt;br/&gt;virtual environments. The intent of this research is to produce a large,&lt;br/&gt;controlled study demonstrating the statistical significance of the impact&lt;br/&gt;of the above methods on student learning.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86142</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86142</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1489" target="n1493">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Systems for Learning Science and Assessing Student Learning</data>
      <data key="e_abstract">EIA- 0086142&lt;br/&gt;Slator, Brian&lt;br/&gt;North Dakota State University, Fargo&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Systems for Learning Science and Assessing Student Learning&lt;br/&gt;&lt;br/&gt;This five-year project to study science learning in authentic, immersive,&lt;br/&gt;virtual environments involving 1) simulated environments for teaching&lt;br/&gt;science topics, each framed according to a theoretical approach, role-based&lt;br/&gt;learning, 2) an innovative, integrated, distributed software platform for&lt;br/&gt;developing and hosting virtual environments, 3) empirical studies using an&lt;br/&gt;innovative protocol, scenario-based assessment, for measuring student&lt;br/&gt;learning in virtual worlds, and 4) a graduate-level summer school course&lt;br/&gt;for in-service teachers who will be trained, beginning in year three, to&lt;br/&gt;use virtual environments in their classrooms. This interdisciplinary&lt;br/&gt;project, in part, depends on fundamental computer science research in the&lt;br/&gt;areas of distributed systems, software agents and intelligent tutoring, and&lt;br/&gt;virtual environments. The intent of this research is to produce a large,&lt;br/&gt;controlled study demonstrating the statistical significance of the impact&lt;br/&gt;of the above methods on student learning.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86142</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86142</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1490" target="n1491">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Systems for Learning Science and Assessing Student Learning</data>
      <data key="e_abstract">EIA- 0086142&lt;br/&gt;Slator, Brian&lt;br/&gt;North Dakota State University, Fargo&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Systems for Learning Science and Assessing Student Learning&lt;br/&gt;&lt;br/&gt;This five-year project to study science learning in authentic, immersive,&lt;br/&gt;virtual environments involving 1) simulated environments for teaching&lt;br/&gt;science topics, each framed according to a theoretical approach, role-based&lt;br/&gt;learning, 2) an innovative, integrated, distributed software platform for&lt;br/&gt;developing and hosting virtual environments, 3) empirical studies using an&lt;br/&gt;innovative protocol, scenario-based assessment, for measuring student&lt;br/&gt;learning in virtual worlds, and 4) a graduate-level summer school course&lt;br/&gt;for in-service teachers who will be trained, beginning in year three, to&lt;br/&gt;use virtual environments in their classrooms. This interdisciplinary&lt;br/&gt;project, in part, depends on fundamental computer science research in the&lt;br/&gt;areas of distributed systems, software agents and intelligent tutoring, and&lt;br/&gt;virtual environments. The intent of this research is to produce a large,&lt;br/&gt;controlled study demonstrating the statistical significance of the impact&lt;br/&gt;of the above methods on student learning.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86142</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86142</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1490" target="n1492">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Systems for Learning Science and Assessing Student Learning</data>
      <data key="e_abstract">EIA- 0086142&lt;br/&gt;Slator, Brian&lt;br/&gt;North Dakota State University, Fargo&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Systems for Learning Science and Assessing Student Learning&lt;br/&gt;&lt;br/&gt;This five-year project to study science learning in authentic, immersive,&lt;br/&gt;virtual environments involving 1) simulated environments for teaching&lt;br/&gt;science topics, each framed according to a theoretical approach, role-based&lt;br/&gt;learning, 2) an innovative, integrated, distributed software platform for&lt;br/&gt;developing and hosting virtual environments, 3) empirical studies using an&lt;br/&gt;innovative protocol, scenario-based assessment, for measuring student&lt;br/&gt;learning in virtual worlds, and 4) a graduate-level summer school course&lt;br/&gt;for in-service teachers who will be trained, beginning in year three, to&lt;br/&gt;use virtual environments in their classrooms. This interdisciplinary&lt;br/&gt;project, in part, depends on fundamental computer science research in the&lt;br/&gt;areas of distributed systems, software agents and intelligent tutoring, and&lt;br/&gt;virtual environments. The intent of this research is to produce a large,&lt;br/&gt;controlled study demonstrating the statistical significance of the impact&lt;br/&gt;of the above methods on student learning.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86142</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86142</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1490" target="n1493">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Systems for Learning Science and Assessing Student Learning</data>
      <data key="e_abstract">EIA- 0086142&lt;br/&gt;Slator, Brian&lt;br/&gt;North Dakota State University, Fargo&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Systems for Learning Science and Assessing Student Learning&lt;br/&gt;&lt;br/&gt;This five-year project to study science learning in authentic, immersive,&lt;br/&gt;virtual environments involving 1) simulated environments for teaching&lt;br/&gt;science topics, each framed according to a theoretical approach, role-based&lt;br/&gt;learning, 2) an innovative, integrated, distributed software platform for&lt;br/&gt;developing and hosting virtual environments, 3) empirical studies using an&lt;br/&gt;innovative protocol, scenario-based assessment, for measuring student&lt;br/&gt;learning in virtual worlds, and 4) a graduate-level summer school course&lt;br/&gt;for in-service teachers who will be trained, beginning in year three, to&lt;br/&gt;use virtual environments in their classrooms. This interdisciplinary&lt;br/&gt;project, in part, depends on fundamental computer science research in the&lt;br/&gt;areas of distributed systems, software agents and intelligent tutoring, and&lt;br/&gt;virtual environments. The intent of this research is to produce a large,&lt;br/&gt;controlled study demonstrating the statistical significance of the impact&lt;br/&gt;of the above methods on student learning.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86142</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86142</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1491" target="n1492">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Systems for Learning Science and Assessing Student Learning</data>
      <data key="e_abstract">EIA- 0086142&lt;br/&gt;Slator, Brian&lt;br/&gt;North Dakota State University, Fargo&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Systems for Learning Science and Assessing Student Learning&lt;br/&gt;&lt;br/&gt;This five-year project to study science learning in authentic, immersive,&lt;br/&gt;virtual environments involving 1) simulated environments for teaching&lt;br/&gt;science topics, each framed according to a theoretical approach, role-based&lt;br/&gt;learning, 2) an innovative, integrated, distributed software platform for&lt;br/&gt;developing and hosting virtual environments, 3) empirical studies using an&lt;br/&gt;innovative protocol, scenario-based assessment, for measuring student&lt;br/&gt;learning in virtual worlds, and 4) a graduate-level summer school course&lt;br/&gt;for in-service teachers who will be trained, beginning in year three, to&lt;br/&gt;use virtual environments in their classrooms. This interdisciplinary&lt;br/&gt;project, in part, depends on fundamental computer science research in the&lt;br/&gt;areas of distributed systems, software agents and intelligent tutoring, and&lt;br/&gt;virtual environments. The intent of this research is to produce a large,&lt;br/&gt;controlled study demonstrating the statistical significance of the impact&lt;br/&gt;of the above methods on student learning.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86142</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86142</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1491" target="n1493">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Systems for Learning Science and Assessing Student Learning</data>
      <data key="e_abstract">EIA- 0086142&lt;br/&gt;Slator, Brian&lt;br/&gt;North Dakota State University, Fargo&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Systems for Learning Science and Assessing Student Learning&lt;br/&gt;&lt;br/&gt;This five-year project to study science learning in authentic, immersive,&lt;br/&gt;virtual environments involving 1) simulated environments for teaching&lt;br/&gt;science topics, each framed according to a theoretical approach, role-based&lt;br/&gt;learning, 2) an innovative, integrated, distributed software platform for&lt;br/&gt;developing and hosting virtual environments, 3) empirical studies using an&lt;br/&gt;innovative protocol, scenario-based assessment, for measuring student&lt;br/&gt;learning in virtual worlds, and 4) a graduate-level summer school course&lt;br/&gt;for in-service teachers who will be trained, beginning in year three, to&lt;br/&gt;use virtual environments in their classrooms. This interdisciplinary&lt;br/&gt;project, in part, depends on fundamental computer science research in the&lt;br/&gt;areas of distributed systems, software agents and intelligent tutoring, and&lt;br/&gt;virtual environments. The intent of this research is to produce a large,&lt;br/&gt;controlled study demonstrating the statistical significance of the impact&lt;br/&gt;of the above methods on student learning.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86142</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86142</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1492" target="n1493">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Systems for Learning Science and Assessing Student Learning</data>
      <data key="e_abstract">EIA- 0086142&lt;br/&gt;Slator, Brian&lt;br/&gt;North Dakota State University, Fargo&lt;br/&gt;&lt;br/&gt;Title: Information Technology Research: Systems for Learning Science and Assessing Student Learning&lt;br/&gt;&lt;br/&gt;This five-year project to study science learning in authentic, immersive,&lt;br/&gt;virtual environments involving 1) simulated environments for teaching&lt;br/&gt;science topics, each framed according to a theoretical approach, role-based&lt;br/&gt;learning, 2) an innovative, integrated, distributed software platform for&lt;br/&gt;developing and hosting virtual environments, 3) empirical studies using an&lt;br/&gt;innovative protocol, scenario-based assessment, for measuring student&lt;br/&gt;learning in virtual worlds, and 4) a graduate-level summer school course&lt;br/&gt;for in-service teachers who will be trained, beginning in year three, to&lt;br/&gt;use virtual environments in their classrooms. This interdisciplinary&lt;br/&gt;project, in part, depends on fundamental computer science research in the&lt;br/&gt;areas of distributed systems, software agents and intelligent tutoring, and&lt;br/&gt;virtual environments. The intent of this research is to produce a large,&lt;br/&gt;controlled study demonstrating the statistical significance of the impact&lt;br/&gt;of the above methods on student learning.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">86142</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">86142</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1495" target="n1496">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Constructing Probability Models for Large Corpora of Well-Informed but Probabilistically Incoherent Judgments</data>
      <data key="e_abstract">This interdisciplinary team (Psychology, Computational and Applied Mathematics, Statistics, Economics, Computer Science) has a goal of overcoming the weaknesses in inference engines in expert systems, decision support systems or in knowledge discovery systems that often work in environments with uncertain characteristics. The current techniques rely on Baysian theory and do not perform well in situations in which conditional independence cannot be guaranteed, or the probabilities provided by experts may not be sound. Since inferences based on probability calculations offer the best guarantee of sensible assessments of chance, efficient schemes have been developed for computing probabilities over complex event spaces. Underlying all such algorithms is a &quot;probability model,&quot; i.e., a representation of the chances of various combinations of events. In turn, probability models are constructed from an initial set of facts about uncertainty in the environment. These facts can sometimes be extracted from databases, using relative frequency as probability. Often, however, the needed probabilities must be obtained from an expert, who responds intuitively. Reliance upon experts raises the specter of incoherence, i.e., judgments that cannot be reconciled with any probability model at all. Indeed, maintaining coherence across a large set of judgments is both computationally and psychologically taxing, and seldom achieved. Incoherent judgment on the part of a single judge is compounded when it is desired to integrate the opinions of several judges. To exploit potentially incoherent and inconsistent judgments, special optimization algorithms are used to construct a compact probability model that best approximates all the judgments in play. The algorithms are tested by applying them to a body of expert opinion in some complex domain. Development of the algorithms will facilitate the automatic construction of artificial expert systems. Whenever a body of expert judgment can be assembled, the algorithms can be applied in view of creating a compact representation of the collective wisdom of the judges. The results of the theoretical research on the probability models will be applied to the analysis of air quality policy in Houston. A large probabilistic database will be established by culling measurements from air quality control stations around the city, expert judgements in environmental science and medicine, and the output of econometric and environmental models of the region. The project has the potential to have a significant intellectual impact in probability, applied learning, and datamining research communities and also provide a useful tool to environmental researchers and Houston decision-makers.&lt;br/&gt;&lt;br/&gt;http://www.ruf.rice.edu&gt;/~osherson</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97814e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.97814e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1495" target="n1497">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Constructing Probability Models for Large Corpora of Well-Informed but Probabilistically Incoherent Judgments</data>
      <data key="e_abstract">This interdisciplinary team (Psychology, Computational and Applied Mathematics, Statistics, Economics, Computer Science) has a goal of overcoming the weaknesses in inference engines in expert systems, decision support systems or in knowledge discovery systems that often work in environments with uncertain characteristics. The current techniques rely on Baysian theory and do not perform well in situations in which conditional independence cannot be guaranteed, or the probabilities provided by experts may not be sound. Since inferences based on probability calculations offer the best guarantee of sensible assessments of chance, efficient schemes have been developed for computing probabilities over complex event spaces. Underlying all such algorithms is a &quot;probability model,&quot; i.e., a representation of the chances of various combinations of events. In turn, probability models are constructed from an initial set of facts about uncertainty in the environment. These facts can sometimes be extracted from databases, using relative frequency as probability. Often, however, the needed probabilities must be obtained from an expert, who responds intuitively. Reliance upon experts raises the specter of incoherence, i.e., judgments that cannot be reconciled with any probability model at all. Indeed, maintaining coherence across a large set of judgments is both computationally and psychologically taxing, and seldom achieved. Incoherent judgment on the part of a single judge is compounded when it is desired to integrate the opinions of several judges. To exploit potentially incoherent and inconsistent judgments, special optimization algorithms are used to construct a compact probability model that best approximates all the judgments in play. The algorithms are tested by applying them to a body of expert opinion in some complex domain. Development of the algorithms will facilitate the automatic construction of artificial expert systems. Whenever a body of expert judgment can be assembled, the algorithms can be applied in view of creating a compact representation of the collective wisdom of the judges. The results of the theoretical research on the probability models will be applied to the analysis of air quality policy in Houston. A large probabilistic database will be established by culling measurements from air quality control stations around the city, expert judgements in environmental science and medicine, and the output of econometric and environmental models of the region. The project has the potential to have a significant intellectual impact in probability, applied learning, and datamining research communities and also provide a useful tool to environmental researchers and Houston decision-makers.&lt;br/&gt;&lt;br/&gt;http://www.ruf.rice.edu&gt;/~osherson</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97814e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.97814e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n564" target="n1495">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Constructing Probability Models for Large Corpora of Well-Informed but Probabilistically Incoherent Judgments</data>
      <data key="e_abstract">This interdisciplinary team (Psychology, Computational and Applied Mathematics, Statistics, Economics, Computer Science) has a goal of overcoming the weaknesses in inference engines in expert systems, decision support systems or in knowledge discovery systems that often work in environments with uncertain characteristics. The current techniques rely on Baysian theory and do not perform well in situations in which conditional independence cannot be guaranteed, or the probabilities provided by experts may not be sound. Since inferences based on probability calculations offer the best guarantee of sensible assessments of chance, efficient schemes have been developed for computing probabilities over complex event spaces. Underlying all such algorithms is a &quot;probability model,&quot; i.e., a representation of the chances of various combinations of events. In turn, probability models are constructed from an initial set of facts about uncertainty in the environment. These facts can sometimes be extracted from databases, using relative frequency as probability. Often, however, the needed probabilities must be obtained from an expert, who responds intuitively. Reliance upon experts raises the specter of incoherence, i.e., judgments that cannot be reconciled with any probability model at all. Indeed, maintaining coherence across a large set of judgments is both computationally and psychologically taxing, and seldom achieved. Incoherent judgment on the part of a single judge is compounded when it is desired to integrate the opinions of several judges. To exploit potentially incoherent and inconsistent judgments, special optimization algorithms are used to construct a compact probability model that best approximates all the judgments in play. The algorithms are tested by applying them to a body of expert opinion in some complex domain. Development of the algorithms will facilitate the automatic construction of artificial expert systems. Whenever a body of expert judgment can be assembled, the algorithms can be applied in view of creating a compact representation of the collective wisdom of the judges. The results of the theoretical research on the probability models will be applied to the analysis of air quality policy in Houston. A large probabilistic database will be established by culling measurements from air quality control stations around the city, expert judgements in environmental science and medicine, and the output of econometric and environmental models of the region. The project has the potential to have a significant intellectual impact in probability, applied learning, and datamining research communities and also provide a useful tool to environmental researchers and Houston decision-makers.&lt;br/&gt;&lt;br/&gt;http://www.ruf.rice.edu&gt;/~osherson</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97814e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.97814e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1495" target="n1499">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Constructing Probability Models for Large Corpora of Well-Informed but Probabilistically Incoherent Judgments</data>
      <data key="e_abstract">This interdisciplinary team (Psychology, Computational and Applied Mathematics, Statistics, Economics, Computer Science) has a goal of overcoming the weaknesses in inference engines in expert systems, decision support systems or in knowledge discovery systems that often work in environments with uncertain characteristics. The current techniques rely on Baysian theory and do not perform well in situations in which conditional independence cannot be guaranteed, or the probabilities provided by experts may not be sound. Since inferences based on probability calculations offer the best guarantee of sensible assessments of chance, efficient schemes have been developed for computing probabilities over complex event spaces. Underlying all such algorithms is a &quot;probability model,&quot; i.e., a representation of the chances of various combinations of events. In turn, probability models are constructed from an initial set of facts about uncertainty in the environment. These facts can sometimes be extracted from databases, using relative frequency as probability. Often, however, the needed probabilities must be obtained from an expert, who responds intuitively. Reliance upon experts raises the specter of incoherence, i.e., judgments that cannot be reconciled with any probability model at all. Indeed, maintaining coherence across a large set of judgments is both computationally and psychologically taxing, and seldom achieved. Incoherent judgment on the part of a single judge is compounded when it is desired to integrate the opinions of several judges. To exploit potentially incoherent and inconsistent judgments, special optimization algorithms are used to construct a compact probability model that best approximates all the judgments in play. The algorithms are tested by applying them to a body of expert opinion in some complex domain. Development of the algorithms will facilitate the automatic construction of artificial expert systems. Whenever a body of expert judgment can be assembled, the algorithms can be applied in view of creating a compact representation of the collective wisdom of the judges. The results of the theoretical research on the probability models will be applied to the analysis of air quality policy in Houston. A large probabilistic database will be established by culling measurements from air quality control stations around the city, expert judgements in environmental science and medicine, and the output of econometric and environmental models of the region. The project has the potential to have a significant intellectual impact in probability, applied learning, and datamining research communities and also provide a useful tool to environmental researchers and Houston decision-makers.&lt;br/&gt;&lt;br/&gt;http://www.ruf.rice.edu&gt;/~osherson</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97814e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.97814e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1496" target="n1497">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Constructing Probability Models for Large Corpora of Well-Informed but Probabilistically Incoherent Judgments</data>
      <data key="e_abstract">This interdisciplinary team (Psychology, Computational and Applied Mathematics, Statistics, Economics, Computer Science) has a goal of overcoming the weaknesses in inference engines in expert systems, decision support systems or in knowledge discovery systems that often work in environments with uncertain characteristics. The current techniques rely on Baysian theory and do not perform well in situations in which conditional independence cannot be guaranteed, or the probabilities provided by experts may not be sound. Since inferences based on probability calculations offer the best guarantee of sensible assessments of chance, efficient schemes have been developed for computing probabilities over complex event spaces. Underlying all such algorithms is a &quot;probability model,&quot; i.e., a representation of the chances of various combinations of events. In turn, probability models are constructed from an initial set of facts about uncertainty in the environment. These facts can sometimes be extracted from databases, using relative frequency as probability. Often, however, the needed probabilities must be obtained from an expert, who responds intuitively. Reliance upon experts raises the specter of incoherence, i.e., judgments that cannot be reconciled with any probability model at all. Indeed, maintaining coherence across a large set of judgments is both computationally and psychologically taxing, and seldom achieved. Incoherent judgment on the part of a single judge is compounded when it is desired to integrate the opinions of several judges. To exploit potentially incoherent and inconsistent judgments, special optimization algorithms are used to construct a compact probability model that best approximates all the judgments in play. The algorithms are tested by applying them to a body of expert opinion in some complex domain. Development of the algorithms will facilitate the automatic construction of artificial expert systems. Whenever a body of expert judgment can be assembled, the algorithms can be applied in view of creating a compact representation of the collective wisdom of the judges. The results of the theoretical research on the probability models will be applied to the analysis of air quality policy in Houston. A large probabilistic database will be established by culling measurements from air quality control stations around the city, expert judgements in environmental science and medicine, and the output of econometric and environmental models of the region. The project has the potential to have a significant intellectual impact in probability, applied learning, and datamining research communities and also provide a useful tool to environmental researchers and Houston decision-makers.&lt;br/&gt;&lt;br/&gt;http://www.ruf.rice.edu&gt;/~osherson</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97814e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.97814e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n564" target="n1496">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Constructing Probability Models for Large Corpora of Well-Informed but Probabilistically Incoherent Judgments</data>
      <data key="e_abstract">This interdisciplinary team (Psychology, Computational and Applied Mathematics, Statistics, Economics, Computer Science) has a goal of overcoming the weaknesses in inference engines in expert systems, decision support systems or in knowledge discovery systems that often work in environments with uncertain characteristics. The current techniques rely on Baysian theory and do not perform well in situations in which conditional independence cannot be guaranteed, or the probabilities provided by experts may not be sound. Since inferences based on probability calculations offer the best guarantee of sensible assessments of chance, efficient schemes have been developed for computing probabilities over complex event spaces. Underlying all such algorithms is a &quot;probability model,&quot; i.e., a representation of the chances of various combinations of events. In turn, probability models are constructed from an initial set of facts about uncertainty in the environment. These facts can sometimes be extracted from databases, using relative frequency as probability. Often, however, the needed probabilities must be obtained from an expert, who responds intuitively. Reliance upon experts raises the specter of incoherence, i.e., judgments that cannot be reconciled with any probability model at all. Indeed, maintaining coherence across a large set of judgments is both computationally and psychologically taxing, and seldom achieved. Incoherent judgment on the part of a single judge is compounded when it is desired to integrate the opinions of several judges. To exploit potentially incoherent and inconsistent judgments, special optimization algorithms are used to construct a compact probability model that best approximates all the judgments in play. The algorithms are tested by applying them to a body of expert opinion in some complex domain. Development of the algorithms will facilitate the automatic construction of artificial expert systems. Whenever a body of expert judgment can be assembled, the algorithms can be applied in view of creating a compact representation of the collective wisdom of the judges. The results of the theoretical research on the probability models will be applied to the analysis of air quality policy in Houston. A large probabilistic database will be established by culling measurements from air quality control stations around the city, expert judgements in environmental science and medicine, and the output of econometric and environmental models of the region. The project has the potential to have a significant intellectual impact in probability, applied learning, and datamining research communities and also provide a useful tool to environmental researchers and Houston decision-makers.&lt;br/&gt;&lt;br/&gt;http://www.ruf.rice.edu&gt;/~osherson</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97814e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.97814e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1496" target="n1499">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Constructing Probability Models for Large Corpora of Well-Informed but Probabilistically Incoherent Judgments</data>
      <data key="e_abstract">This interdisciplinary team (Psychology, Computational and Applied Mathematics, Statistics, Economics, Computer Science) has a goal of overcoming the weaknesses in inference engines in expert systems, decision support systems or in knowledge discovery systems that often work in environments with uncertain characteristics. The current techniques rely on Baysian theory and do not perform well in situations in which conditional independence cannot be guaranteed, or the probabilities provided by experts may not be sound. Since inferences based on probability calculations offer the best guarantee of sensible assessments of chance, efficient schemes have been developed for computing probabilities over complex event spaces. Underlying all such algorithms is a &quot;probability model,&quot; i.e., a representation of the chances of various combinations of events. In turn, probability models are constructed from an initial set of facts about uncertainty in the environment. These facts can sometimes be extracted from databases, using relative frequency as probability. Often, however, the needed probabilities must be obtained from an expert, who responds intuitively. Reliance upon experts raises the specter of incoherence, i.e., judgments that cannot be reconciled with any probability model at all. Indeed, maintaining coherence across a large set of judgments is both computationally and psychologically taxing, and seldom achieved. Incoherent judgment on the part of a single judge is compounded when it is desired to integrate the opinions of several judges. To exploit potentially incoherent and inconsistent judgments, special optimization algorithms are used to construct a compact probability model that best approximates all the judgments in play. The algorithms are tested by applying them to a body of expert opinion in some complex domain. Development of the algorithms will facilitate the automatic construction of artificial expert systems. Whenever a body of expert judgment can be assembled, the algorithms can be applied in view of creating a compact representation of the collective wisdom of the judges. The results of the theoretical research on the probability models will be applied to the analysis of air quality policy in Houston. A large probabilistic database will be established by culling measurements from air quality control stations around the city, expert judgements in environmental science and medicine, and the output of econometric and environmental models of the region. The project has the potential to have a significant intellectual impact in probability, applied learning, and datamining research communities and also provide a useful tool to environmental researchers and Houston decision-makers.&lt;br/&gt;&lt;br/&gt;http://www.ruf.rice.edu&gt;/~osherson</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97814e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.97814e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n564" target="n1497">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Constructing Probability Models for Large Corpora of Well-Informed but Probabilistically Incoherent Judgments</data>
      <data key="e_abstract">This interdisciplinary team (Psychology, Computational and Applied Mathematics, Statistics, Economics, Computer Science) has a goal of overcoming the weaknesses in inference engines in expert systems, decision support systems or in knowledge discovery systems that often work in environments with uncertain characteristics. The current techniques rely on Baysian theory and do not perform well in situations in which conditional independence cannot be guaranteed, or the probabilities provided by experts may not be sound. Since inferences based on probability calculations offer the best guarantee of sensible assessments of chance, efficient schemes have been developed for computing probabilities over complex event spaces. Underlying all such algorithms is a &quot;probability model,&quot; i.e., a representation of the chances of various combinations of events. In turn, probability models are constructed from an initial set of facts about uncertainty in the environment. These facts can sometimes be extracted from databases, using relative frequency as probability. Often, however, the needed probabilities must be obtained from an expert, who responds intuitively. Reliance upon experts raises the specter of incoherence, i.e., judgments that cannot be reconciled with any probability model at all. Indeed, maintaining coherence across a large set of judgments is both computationally and psychologically taxing, and seldom achieved. Incoherent judgment on the part of a single judge is compounded when it is desired to integrate the opinions of several judges. To exploit potentially incoherent and inconsistent judgments, special optimization algorithms are used to construct a compact probability model that best approximates all the judgments in play. The algorithms are tested by applying them to a body of expert opinion in some complex domain. Development of the algorithms will facilitate the automatic construction of artificial expert systems. Whenever a body of expert judgment can be assembled, the algorithms can be applied in view of creating a compact representation of the collective wisdom of the judges. The results of the theoretical research on the probability models will be applied to the analysis of air quality policy in Houston. A large probabilistic database will be established by culling measurements from air quality control stations around the city, expert judgements in environmental science and medicine, and the output of econometric and environmental models of the region. The project has the potential to have a significant intellectual impact in probability, applied learning, and datamining research communities and also provide a useful tool to environmental researchers and Houston decision-makers.&lt;br/&gt;&lt;br/&gt;http://www.ruf.rice.edu&gt;/~osherson</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97814e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.97814e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1497" target="n1499">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Constructing Probability Models for Large Corpora of Well-Informed but Probabilistically Incoherent Judgments</data>
      <data key="e_abstract">This interdisciplinary team (Psychology, Computational and Applied Mathematics, Statistics, Economics, Computer Science) has a goal of overcoming the weaknesses in inference engines in expert systems, decision support systems or in knowledge discovery systems that often work in environments with uncertain characteristics. The current techniques rely on Baysian theory and do not perform well in situations in which conditional independence cannot be guaranteed, or the probabilities provided by experts may not be sound. Since inferences based on probability calculations offer the best guarantee of sensible assessments of chance, efficient schemes have been developed for computing probabilities over complex event spaces. Underlying all such algorithms is a &quot;probability model,&quot; i.e., a representation of the chances of various combinations of events. In turn, probability models are constructed from an initial set of facts about uncertainty in the environment. These facts can sometimes be extracted from databases, using relative frequency as probability. Often, however, the needed probabilities must be obtained from an expert, who responds intuitively. Reliance upon experts raises the specter of incoherence, i.e., judgments that cannot be reconciled with any probability model at all. Indeed, maintaining coherence across a large set of judgments is both computationally and psychologically taxing, and seldom achieved. Incoherent judgment on the part of a single judge is compounded when it is desired to integrate the opinions of several judges. To exploit potentially incoherent and inconsistent judgments, special optimization algorithms are used to construct a compact probability model that best approximates all the judgments in play. The algorithms are tested by applying them to a body of expert opinion in some complex domain. Development of the algorithms will facilitate the automatic construction of artificial expert systems. Whenever a body of expert judgment can be assembled, the algorithms can be applied in view of creating a compact representation of the collective wisdom of the judges. The results of the theoretical research on the probability models will be applied to the analysis of air quality policy in Houston. A large probabilistic database will be established by culling measurements from air quality control stations around the city, expert judgements in environmental science and medicine, and the output of econometric and environmental models of the region. The project has the potential to have a significant intellectual impact in probability, applied learning, and datamining research communities and also provide a useful tool to environmental researchers and Houston decision-makers.&lt;br/&gt;&lt;br/&gt;http://www.ruf.rice.edu&gt;/~osherson</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97814e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.97814e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n564" target="n1499">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Constructing Probability Models for Large Corpora of Well-Informed but Probabilistically Incoherent Judgments</data>
      <data key="e_abstract">This interdisciplinary team (Psychology, Computational and Applied Mathematics, Statistics, Economics, Computer Science) has a goal of overcoming the weaknesses in inference engines in expert systems, decision support systems or in knowledge discovery systems that often work in environments with uncertain characteristics. The current techniques rely on Baysian theory and do not perform well in situations in which conditional independence cannot be guaranteed, or the probabilities provided by experts may not be sound. Since inferences based on probability calculations offer the best guarantee of sensible assessments of chance, efficient schemes have been developed for computing probabilities over complex event spaces. Underlying all such algorithms is a &quot;probability model,&quot; i.e., a representation of the chances of various combinations of events. In turn, probability models are constructed from an initial set of facts about uncertainty in the environment. These facts can sometimes be extracted from databases, using relative frequency as probability. Often, however, the needed probabilities must be obtained from an expert, who responds intuitively. Reliance upon experts raises the specter of incoherence, i.e., judgments that cannot be reconciled with any probability model at all. Indeed, maintaining coherence across a large set of judgments is both computationally and psychologically taxing, and seldom achieved. Incoherent judgment on the part of a single judge is compounded when it is desired to integrate the opinions of several judges. To exploit potentially incoherent and inconsistent judgments, special optimization algorithms are used to construct a compact probability model that best approximates all the judgments in play. The algorithms are tested by applying them to a body of expert opinion in some complex domain. Development of the algorithms will facilitate the automatic construction of artificial expert systems. Whenever a body of expert judgment can be assembled, the algorithms can be applied in view of creating a compact representation of the collective wisdom of the judges. The results of the theoretical research on the probability models will be applied to the analysis of air quality policy in Houston. A large probabilistic database will be established by culling measurements from air quality control stations around the city, expert judgements in environmental science and medicine, and the output of econometric and environmental models of the region. The project has the potential to have a significant intellectual impact in probability, applied learning, and datamining research communities and also provide a useful tool to environmental researchers and Houston decision-makers.&lt;br/&gt;&lt;br/&gt;http://www.ruf.rice.edu&gt;/~osherson</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">9.97814e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.97814e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1185" target="n1502">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Space-Time Spreading and Coding</data>
      <data key="e_abstract">PROJECT SUMMARY&lt;br/&gt;The channel fading and co-channel interference present in a wireless radio propagation channel presents a harsh challenge to the radio design engineer. Diversity techniques commonly used to combat the channel include the use of interleaved coded modulation and multiple antennas at the receiver end. Lately there has been a great deal of interest in the use of multiple antennas at the transmitter end, i.e., in using transmit diversity. Researchers have shown that the capacity of multi-antenna systems significantly exceeds that of conventional single-antenna systems. Space-time codes attempt to realize the potential increase in capacity by distributing code symbols intelligently in space (across the different transmit antennas) and time. This topic has received a great deal of attention since the BLAST space-time system was proposed and demonstrated at Bell Labs. This is because, owing to the additional dimensionality obtain by exploiting space, a spectral efficiency of 40 bits per sec per Hz at realistic signal to noise ratios was demonstrated (i.e., 20 times the spectral efficiency achieved in current cellular systems). The importance of space-time methods in communication and information theory was recently recognized by the selection of the work by Tarokh, Seshadri and Calderbank for the 1999 IEEE Information Theory Paper Prize. The great majority of space-time coding research has been based on the following assumptions:&lt;br/&gt;&lt;br/&gt;Single-user link or time-division multiplexed&lt;br/&gt;&lt;br/&gt;Complex Gaussian channel gains (frequency at fading) from each transmit (Tx) antenna to each receive (Rx) antenna.&lt;br/&gt;&lt;br/&gt;Independent channels for each Tx-Rx antenna pair.&lt;br/&gt;&lt;br/&gt;Equal average energy in each Tx-Rx antenna channel.&lt;br/&gt;&lt;br/&gt;Time non-selective channels (quasi-static assumption)&lt;br/&gt;&lt;br/&gt;Based on these assumptions, effective design rules have been established by considering the probability of pairwise codeword error. These design rules aim to maximize the diversity and coding gains. The assumptions described above best represent a system with a dense, rich scattering environment sur-rounding the Tx and Rx antenna arrays and stationary or slowly moving receivers. This model is applicable, for example, to an indoor office environment. It may also be accurate for a pedestrian cellular user in a dense urban environment, depending on the base-station antenna location. However, space-time channel measurements and models suggest that in many cases of practical interest, the above assumption of independence of channel gains is not accurate. The lack of like-signal interference may also not be applicable to code-division multiple access (CDMA)&lt;br/&gt;systems since a well-utilized CDMA system is limited by such interference. Virtually all third-generation mobile radio standards are based on CDMA systems as are the most effective wireless LAN systems. In a CDMA system, the effects of signature sequence design must also be considered together with coding and&lt;br/&gt;modulation.&lt;br/&gt;In this project, we have developed an expression for the pairwise error probability for a system model that is broadly applicable. Our model includes arbitrary correlation between Tx-Rx antenna channels, multiple CDMA users, and the potential for time variations in the channel gains. Based on this model we propose to develop design rules for space-time spreading and coding for CDMA systems. Our goal is to develop rules that are applicable to a variety of applications (i.e., mobile users and dense scattering, or fixed-point systems&lt;br/&gt;out of the scatter) and/or which are robust to uncertainties in the channel model.&lt;br/&gt;We propose to generalize the methods of space-time coding so that the approach may be used in CDMA systems and/or systems without rich scattering environments. Research directions include rules for spacetime spreading and code design, investigation of receiver structures and channel estimation/tracking, code design for simplified decoding and modified design rules for non-Euclidean distance receivers. This study will be conducted under different channel model assumptions and presumed knowledge at the receiver and transmitter of channel knowledge.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82987</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82987</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n946" target="n1503">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Algorithms for Active Storage</data>
      <data key="e_abstract">This project explores techniques for efficient computation on large, disk-based data sets. The research focuses on theoretical and practical aspects of ``active&apos;&apos; storage systems, in which each storage unit (a disk or group of disks) has some limited capability for local computation. A key goal of the project is to develop a theoretical model for active storage systems. The model is a basis for designing and evaluating algorithms for active storage systems, identifying useful computation kernels for active storage units, and deriving lower bounds for fundamental problems. The applied aspects of the project include design and implementation of a programming environment (PEARL) for active storage algorithms, and experimental evaluation of new algorithms for active storage. This project is a collaboration with a separately funded project team at Carleton University in Ottawa, Canada.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82986</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82986</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n756" target="n1506">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Energy and Quality of Service Aware Ad-Hoc Networking</data>
      <data key="e_abstract">The researchers propose an integrated approach to energy efficient collaborative communication in ad-hoc wireless networks. The interdisciplinary approach will address simultaneously all key levels of system design: data link layer power management, network layer energy, location, and quality of service-based routing, and application layer interfaces.&lt;br/&gt; The objective is to reduce communication power consumption while simultaneously increasing the communication quality between the collaborators. The model problem assumes that people are interacting via ad-hoc networking integrated into pocket computers for some common task. For the communication to be successfull, all participants must be reachable via the network and have sufficient energy to perform any necessary computation. At the heart of the problem is the distributed scheduling of communication between entities to reduce communication and computation energy. Participants must have an estimate of the remaining energy available by other participants as well as models of the communication topology. Decisions must be made whether to route traffic through adjacent nodes (reducing direct power expenditure while possibly increasing latency or global energy consumption) or increasing transmitter power (potentially reaching a broader set of participants). This objective will be reached by developing design methodologies in the following research areas:&lt;br/&gt; Link-layer controllers that adaptively learn low-power strategies for error correction &lt;br/&gt;codes, transmission power levels, and radio activity times for reduced overall power consumption, within the constraints of delay and error bounds.&lt;br/&gt; Network-layer controllers that use inter-node attenuation, available node energy, and current traffic flows to optimize routing so as to maximize the availability of all nodes.&lt;br/&gt; Network interfaces that allow the programmer to describe delivery constraints on messages such that communication schedules can reduce energy in a changing routing topology.&lt;br/&gt; System level power modeling that allows the application layer to trade additional processor power to reduce networking power.&lt;br/&gt; The project is targeted to the emerging spectrum of highly capable pocket or wearable computers and the ad-hoc networks formed by those computers. The researchers will both deploy an experimental infrastructure and use in-depth simulation models to evaluate our system. The prototype system will use a combination of laptop computers using 802.11 wireless networking and the advanced &quot;Itsy&quot; palm-top computer with lower bandwidth (and significantly lower power) interfaces. The simulation models will use realistic whole-system power models for advanced pocket computers coupled with a standard network simulator that we have enhanced with more realistic RF propagation models.&lt;br/&gt; This project is significant because it address a communication model that will be common with future generation computation and communication devices. It is realistic because it will use an experimental network. It is general because we will be able to use the simulation framework to model future compute and communication networks. Finally, it is achievable because it leverages the on-going work in optimizing channel assignment in wireless networks and energy efficient computing.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82998</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82998</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1507" target="n1508">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: The Virtual Trainer</data>
      <data key="e_abstract">This is the first year funding of a three-year continuing award. The goal of the &quot;Virtual Trainer&quot; is to create a computer-based animation that can interactively teach how to move. The hardware requirements for the Virtual Trainer in its final stage would be an inexpensive state-of-the-art personal computer equipped with a camera system. The Virtual Trainer will be able to demonstrate movements to its user, monitor the execution of these movements by the user, and suggest corrections in case of inadequate performance. The Virtual Trainer will be useful in a large number of applications, including rehabilitation of movement-impaired patients (e.g., stroke-patients), sport and exercise education, dance instruction, and interactive entertainment industry. Additionally, the technology developed for the Virtual Trainer has the potential to pioneer new algorithms for robot control using &quot;teaching from demonstration&quot;, to contribute to the development of automated monitoring systems for human environments, to the generation of humanoid computer simulations, and also to gaining new insights into biological motor control and the functioning of the nervous system. The research team of this project will primarily focus on issues of movement recognition and movement generation with the Virtual Trainer for rehabilitating stroke-impaired patients with upper and lower limb disabilities.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82995</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82995</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n474" target="n1509">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: An Interactive Experimental/Numerical Simulation System with Applications in MEMS Design</data>
      <data key="e_abstract">This project will build a new generation of numerical simulation systems by creating a feedback path between physical experiments and numerical solvers. There are a number of exciting implications of this data-adaptive simulation idea. Engineering fluid flows are inherently complex. This complexity limits measurement and precision, so engineers are forced to work with fluid flows based on very sparse information. Numerical solvers, on the other hand, can resolve tiny flow structures, but they generally run in an open-loop mode and are thus unverified. Coupling the two forms of technology offers powerful advantages to each. Comparisons against live experimental data will allow simulation algorithms to be verified quantitatively, in detail, and in-line. Once it is verified in this fashion, one can use the simulation with confidence on related problems. Once can also use the sensor information to correct the solver&apos;s data, or even to adjust the solver parameters on the fly. Moreover, once the solver is properly synchronized with the real system, one could use the former to explore the physics of the latter in more detail than sensors would allow - and still trust the results.&lt;br/&gt;&lt;br/&gt;A particularly compelling application area for data-adaptive simulation techniques is microelectromechanical systems (MEMS). This emerging technology is driving a revolution in engineering design that is placing new demands on numerical simulation. Accurate modeling of the interaction of tiny, flexible, moving structures with high-speed chaotic fluids is challenging. To resolve the fine details in this kind of simulation, computational fluid dynamics technology requires extremely fine meshes and the solution of very large systems of nonlinear equations. This makes it difficult to build production-quality computer-aided design (CAD) tools for MEMS, which in turn forces engineers to fabricate devices without testing them. Functional CAD tools would allow MEMS designers to achieve one-pass design, much as VLSI does now.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">83004</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">83004</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1509" target="n1511">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: An Interactive Experimental/Numerical Simulation System with Applications in MEMS Design</data>
      <data key="e_abstract">This project will build a new generation of numerical simulation systems by creating a feedback path between physical experiments and numerical solvers. There are a number of exciting implications of this data-adaptive simulation idea. Engineering fluid flows are inherently complex. This complexity limits measurement and precision, so engineers are forced to work with fluid flows based on very sparse information. Numerical solvers, on the other hand, can resolve tiny flow structures, but they generally run in an open-loop mode and are thus unverified. Coupling the two forms of technology offers powerful advantages to each. Comparisons against live experimental data will allow simulation algorithms to be verified quantitatively, in detail, and in-line. Once it is verified in this fashion, one can use the simulation with confidence on related problems. Once can also use the sensor information to correct the solver&apos;s data, or even to adjust the solver parameters on the fly. Moreover, once the solver is properly synchronized with the real system, one could use the former to explore the physics of the latter in more detail than sensors would allow - and still trust the results.&lt;br/&gt;&lt;br/&gt;A particularly compelling application area for data-adaptive simulation techniques is microelectromechanical systems (MEMS). This emerging technology is driving a revolution in engineering design that is placing new demands on numerical simulation. Accurate modeling of the interaction of tiny, flexible, moving structures with high-speed chaotic fluids is challenging. To resolve the fine details in this kind of simulation, computational fluid dynamics technology requires extremely fine meshes and the solution of very large systems of nonlinear equations. This makes it difficult to build production-quality computer-aided design (CAD) tools for MEMS, which in turn forces engineers to fabricate devices without testing them. Functional CAD tools would allow MEMS designers to achieve one-pass design, much as VLSI does now.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">83004</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">83004</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n474" target="n1511">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: An Interactive Experimental/Numerical Simulation System with Applications in MEMS Design</data>
      <data key="e_abstract">This project will build a new generation of numerical simulation systems by creating a feedback path between physical experiments and numerical solvers. There are a number of exciting implications of this data-adaptive simulation idea. Engineering fluid flows are inherently complex. This complexity limits measurement and precision, so engineers are forced to work with fluid flows based on very sparse information. Numerical solvers, on the other hand, can resolve tiny flow structures, but they generally run in an open-loop mode and are thus unverified. Coupling the two forms of technology offers powerful advantages to each. Comparisons against live experimental data will allow simulation algorithms to be verified quantitatively, in detail, and in-line. Once it is verified in this fashion, one can use the simulation with confidence on related problems. Once can also use the sensor information to correct the solver&apos;s data, or even to adjust the solver parameters on the fly. Moreover, once the solver is properly synchronized with the real system, one could use the former to explore the physics of the latter in more detail than sensors would allow - and still trust the results.&lt;br/&gt;&lt;br/&gt;A particularly compelling application area for data-adaptive simulation techniques is microelectromechanical systems (MEMS). This emerging technology is driving a revolution in engineering design that is placing new demands on numerical simulation. Accurate modeling of the interaction of tiny, flexible, moving structures with high-speed chaotic fluids is challenging. To resolve the fine details in this kind of simulation, computational fluid dynamics technology requires extremely fine meshes and the solution of very large systems of nonlinear equations. This makes it difficult to build production-quality computer-aided design (CAD) tools for MEMS, which in turn forces engineers to fabricate devices without testing them. Functional CAD tools would allow MEMS designers to achieve one-pass design, much as VLSI does now.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">83004</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">83004</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n279" target="n280">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: System Support for Energy Management in Mobile and Embedded Workloads</data>
      <data key="e_abstract">The goal of this project is to develop an integrated&lt;br/&gt;hardware/software infrastructure to support power management for&lt;br/&gt;battery-powered mobile and wireless applications. These future&lt;br/&gt;environments will support applications with demanding requirements&lt;br/&gt;such as disaster recovery. Energy conservation, especially for&lt;br/&gt;mobile and embedded devices, promises to have significant economic,&lt;br/&gt;environmental, and societal impacts.&lt;br/&gt;&lt;br/&gt;The activities focus on three key directions: i) the development of&lt;br/&gt;power measurement tools, workloads, and experimental methods to&lt;br/&gt;evaluate energy consumption, ii) the energy-aware APIs to allow&lt;br/&gt;application-directed power management, and iii) the development of&lt;br/&gt;system support for high-level solutions.&lt;br/&gt;&lt;br/&gt;These research projects all rely on experimental techniques for&lt;br/&gt;evaluating ideas. Making empirical measurements and observations on&lt;br/&gt;device and workload characteristics pinpoints the problem areas of&lt;br/&gt;greatest potential. Initially formulating simulation models narrows&lt;br/&gt;the solution space and allows consideration of new architectures.&lt;br/&gt;Finally, constructing working prototypes allows observation of all&lt;br/&gt;activity associated with real operating environments and offers&lt;br/&gt;deeper insights into their behavior. The popularity and&lt;br/&gt;accessibility of the palmtop and handheld platforms gives this&lt;br/&gt;research significant potential for immediate technology transfer.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82914</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82914</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n279" target="n281">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: System Support for Energy Management in Mobile and Embedded Workloads</data>
      <data key="e_abstract">The goal of this project is to develop an integrated&lt;br/&gt;hardware/software infrastructure to support power management for&lt;br/&gt;battery-powered mobile and wireless applications. These future&lt;br/&gt;environments will support applications with demanding requirements&lt;br/&gt;such as disaster recovery. Energy conservation, especially for&lt;br/&gt;mobile and embedded devices, promises to have significant economic,&lt;br/&gt;environmental, and societal impacts.&lt;br/&gt;&lt;br/&gt;The activities focus on three key directions: i) the development of&lt;br/&gt;power measurement tools, workloads, and experimental methods to&lt;br/&gt;evaluate energy consumption, ii) the energy-aware APIs to allow&lt;br/&gt;application-directed power management, and iii) the development of&lt;br/&gt;system support for high-level solutions.&lt;br/&gt;&lt;br/&gt;These research projects all rely on experimental techniques for&lt;br/&gt;evaluating ideas. Making empirical measurements and observations on&lt;br/&gt;device and workload characteristics pinpoints the problem areas of&lt;br/&gt;greatest potential. Initially formulating simulation models narrows&lt;br/&gt;the solution space and allows consideration of new architectures.&lt;br/&gt;Finally, constructing working prototypes allows observation of all&lt;br/&gt;activity associated with real operating environments and offers&lt;br/&gt;deeper insights into their behavior. The popularity and&lt;br/&gt;accessibility of the palmtop and handheld platforms gives this&lt;br/&gt;research significant potential for immediate technology transfer.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82914</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82914</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n280" target="n281">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: System Support for Energy Management in Mobile and Embedded Workloads</data>
      <data key="e_abstract">The goal of this project is to develop an integrated&lt;br/&gt;hardware/software infrastructure to support power management for&lt;br/&gt;battery-powered mobile and wireless applications. These future&lt;br/&gt;environments will support applications with demanding requirements&lt;br/&gt;such as disaster recovery. Energy conservation, especially for&lt;br/&gt;mobile and embedded devices, promises to have significant economic,&lt;br/&gt;environmental, and societal impacts.&lt;br/&gt;&lt;br/&gt;The activities focus on three key directions: i) the development of&lt;br/&gt;power measurement tools, workloads, and experimental methods to&lt;br/&gt;evaluate energy consumption, ii) the energy-aware APIs to allow&lt;br/&gt;application-directed power management, and iii) the development of&lt;br/&gt;system support for high-level solutions.&lt;br/&gt;&lt;br/&gt;These research projects all rely on experimental techniques for&lt;br/&gt;evaluating ideas. Making empirical measurements and observations on&lt;br/&gt;device and workload characteristics pinpoints the problem areas of&lt;br/&gt;greatest potential. Initially formulating simulation models narrows&lt;br/&gt;the solution space and allows consideration of new architectures.&lt;br/&gt;Finally, constructing working prototypes allows observation of all&lt;br/&gt;activity associated with real operating environments and offers&lt;br/&gt;deeper insights into their behavior. The popularity and&lt;br/&gt;accessibility of the palmtop and handheld platforms gives this&lt;br/&gt;research significant potential for immediate technology transfer.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82914</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82914</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1523" target="n1524">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Geometric Algorithms and Analytical Models: the Case of Ray Shooting</data>
      <data key="e_abstract">The purpose of Computational Geometry is to provide provably efficient algorithms and data structures for applications of a geometric nature. Unfortunately, the commonly adopted attitude of studying worst-case behavior has disserved the field by building a gap between the theoretical research and the growing community that implements and uses geometric algorithms. This project will develop frameworks that model more closely the behavior of algorithms of practical importance on actual data. To do this, it will concentrate on ray shooting, which is the bottleneck operation in the fundamental ray tracing technique for producing photo-realistic images in graphics. Ray shooting also has numerous other applications.&lt;br/&gt;&lt;br/&gt;Technically, the project will develop a new framework for predicting the &quot;average&quot; (rather than worst-case) performance of ray shooting on any decomposition- or hierarchy-based data structure on a given input. This would allow one to compare the expected performance of different approaches on a given data, with the eventual aim of being able to predict the cost of an operation, such as rendering a scene at a certain resolution, or optimizing the choice of a data structure to store a scene, prior to actual ray-shooting computation. It will also devise novel ray shooting/ray tracing algorithms that reduce the I/O-complexity for datasets too large to fit in main. In addition, It will attempt to extend the proposed performance-predicting framework to incorporate I/O-complexity as well, to cover the entire spectrum of the input sizes. Finally, the project will implement its algorithms, address the corresponding robustness issues, and investigate the accuracy of the predictive framework on practical data.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81964</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81964</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1523" target="n1525">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Geometric Algorithms and Analytical Models: the Case of Ray Shooting</data>
      <data key="e_abstract">The purpose of Computational Geometry is to provide provably efficient algorithms and data structures for applications of a geometric nature. Unfortunately, the commonly adopted attitude of studying worst-case behavior has disserved the field by building a gap between the theoretical research and the growing community that implements and uses geometric algorithms. This project will develop frameworks that model more closely the behavior of algorithms of practical importance on actual data. To do this, it will concentrate on ray shooting, which is the bottleneck operation in the fundamental ray tracing technique for producing photo-realistic images in graphics. Ray shooting also has numerous other applications.&lt;br/&gt;&lt;br/&gt;Technically, the project will develop a new framework for predicting the &quot;average&quot; (rather than worst-case) performance of ray shooting on any decomposition- or hierarchy-based data structure on a given input. This would allow one to compare the expected performance of different approaches on a given data, with the eventual aim of being able to predict the cost of an operation, such as rendering a scene at a certain resolution, or optimizing the choice of a data structure to store a scene, prior to actual ray-shooting computation. It will also devise novel ray shooting/ray tracing algorithms that reduce the I/O-complexity for datasets too large to fit in main. In addition, It will attempt to extend the proposed performance-predicting framework to incorporate I/O-complexity as well, to cover the entire spectrum of the input sizes. Finally, the project will implement its algorithms, address the corresponding robustness issues, and investigate the accuracy of the predictive framework on practical data.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81964</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81964</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1524" target="n1525">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Geometric Algorithms and Analytical Models: the Case of Ray Shooting</data>
      <data key="e_abstract">The purpose of Computational Geometry is to provide provably efficient algorithms and data structures for applications of a geometric nature. Unfortunately, the commonly adopted attitude of studying worst-case behavior has disserved the field by building a gap between the theoretical research and the growing community that implements and uses geometric algorithms. This project will develop frameworks that model more closely the behavior of algorithms of practical importance on actual data. To do this, it will concentrate on ray shooting, which is the bottleneck operation in the fundamental ray tracing technique for producing photo-realistic images in graphics. Ray shooting also has numerous other applications.&lt;br/&gt;&lt;br/&gt;Technically, the project will develop a new framework for predicting the &quot;average&quot; (rather than worst-case) performance of ray shooting on any decomposition- or hierarchy-based data structure on a given input. This would allow one to compare the expected performance of different approaches on a given data, with the eventual aim of being able to predict the cost of an operation, such as rendering a scene at a certain resolution, or optimizing the choice of a data structure to store a scene, prior to actual ray-shooting computation. It will also devise novel ray shooting/ray tracing algorithms that reduce the I/O-complexity for datasets too large to fit in main. In addition, It will attempt to extend the proposed performance-predicting framework to incorporate I/O-complexity as well, to cover the entire spectrum of the input sizes. Finally, the project will implement its algorithms, address the corresponding robustness issues, and investigate the accuracy of the predictive framework on practical data.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81964</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81964</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1532" target="n1533">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: An Active, Personalized, Adaptive, Multi-format Biological Information Delivery System</data>
      <data key="e_abstract">The explosive growth of biological information sources, available over the Internet, has given rise to both opportunities and challenges for biological and medical researchers. The opportunities they provide are both scientific (e.g., understanding the information encoded in elementary biological structures) as well as technological (e.g., new drug discovery). The challenges, on the other hand, lie in how to efficiently discover, among the vast volume of information, the items that are relevant or interesting to a given researcher. The objective of the proposed research is to investigate related basic research problems and develop a biological information delivery system in a collaborative project between computer scientists, information scientists, and biological researchers. The specific plans include developing methods to make the proposed system pro-active (surveying evolving on-line sources for relevant information), personalized (cognizant of a particular researcher&apos;s interests), adaptive (able to react to changes in the information sources as well as user interests or objectives), and capable of integrating multi-format data. The impact of this research is a significant enhancement in the ability of students and researchers in biological sciences to efficiently utilize on-line resources, while generating methods for computerized analysis of biological data and providing computerized support for new scientific discovery.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81944</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81944</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1532" target="n1534">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: An Active, Personalized, Adaptive, Multi-format Biological Information Delivery System</data>
      <data key="e_abstract">The explosive growth of biological information sources, available over the Internet, has given rise to both opportunities and challenges for biological and medical researchers. The opportunities they provide are both scientific (e.g., understanding the information encoded in elementary biological structures) as well as technological (e.g., new drug discovery). The challenges, on the other hand, lie in how to efficiently discover, among the vast volume of information, the items that are relevant or interesting to a given researcher. The objective of the proposed research is to investigate related basic research problems and develop a biological information delivery system in a collaborative project between computer scientists, information scientists, and biological researchers. The specific plans include developing methods to make the proposed system pro-active (surveying evolving on-line sources for relevant information), personalized (cognizant of a particular researcher&apos;s interests), adaptive (able to react to changes in the information sources as well as user interests or objectives), and capable of integrating multi-format data. The impact of this research is a significant enhancement in the ability of students and researchers in biological sciences to efficiently utilize on-line resources, while generating methods for computerized analysis of biological data and providing computerized support for new scientific discovery.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81944</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81944</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1532" target="n1535">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: An Active, Personalized, Adaptive, Multi-format Biological Information Delivery System</data>
      <data key="e_abstract">The explosive growth of biological information sources, available over the Internet, has given rise to both opportunities and challenges for biological and medical researchers. The opportunities they provide are both scientific (e.g., understanding the information encoded in elementary biological structures) as well as technological (e.g., new drug discovery). The challenges, on the other hand, lie in how to efficiently discover, among the vast volume of information, the items that are relevant or interesting to a given researcher. The objective of the proposed research is to investigate related basic research problems and develop a biological information delivery system in a collaborative project between computer scientists, information scientists, and biological researchers. The specific plans include developing methods to make the proposed system pro-active (surveying evolving on-line sources for relevant information), personalized (cognizant of a particular researcher&apos;s interests), adaptive (able to react to changes in the information sources as well as user interests or objectives), and capable of integrating multi-format data. The impact of this research is a significant enhancement in the ability of students and researchers in biological sciences to efficiently utilize on-line resources, while generating methods for computerized analysis of biological data and providing computerized support for new scientific discovery.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81944</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81944</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1532" target="n1536">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: An Active, Personalized, Adaptive, Multi-format Biological Information Delivery System</data>
      <data key="e_abstract">The explosive growth of biological information sources, available over the Internet, has given rise to both opportunities and challenges for biological and medical researchers. The opportunities they provide are both scientific (e.g., understanding the information encoded in elementary biological structures) as well as technological (e.g., new drug discovery). The challenges, on the other hand, lie in how to efficiently discover, among the vast volume of information, the items that are relevant or interesting to a given researcher. The objective of the proposed research is to investigate related basic research problems and develop a biological information delivery system in a collaborative project between computer scientists, information scientists, and biological researchers. The specific plans include developing methods to make the proposed system pro-active (surveying evolving on-line sources for relevant information), personalized (cognizant of a particular researcher&apos;s interests), adaptive (able to react to changes in the information sources as well as user interests or objectives), and capable of integrating multi-format data. The impact of this research is a significant enhancement in the ability of students and researchers in biological sciences to efficiently utilize on-line resources, while generating methods for computerized analysis of biological data and providing computerized support for new scientific discovery.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81944</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81944</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1533" target="n1534">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: An Active, Personalized, Adaptive, Multi-format Biological Information Delivery System</data>
      <data key="e_abstract">The explosive growth of biological information sources, available over the Internet, has given rise to both opportunities and challenges for biological and medical researchers. The opportunities they provide are both scientific (e.g., understanding the information encoded in elementary biological structures) as well as technological (e.g., new drug discovery). The challenges, on the other hand, lie in how to efficiently discover, among the vast volume of information, the items that are relevant or interesting to a given researcher. The objective of the proposed research is to investigate related basic research problems and develop a biological information delivery system in a collaborative project between computer scientists, information scientists, and biological researchers. The specific plans include developing methods to make the proposed system pro-active (surveying evolving on-line sources for relevant information), personalized (cognizant of a particular researcher&apos;s interests), adaptive (able to react to changes in the information sources as well as user interests or objectives), and capable of integrating multi-format data. The impact of this research is a significant enhancement in the ability of students and researchers in biological sciences to efficiently utilize on-line resources, while generating methods for computerized analysis of biological data and providing computerized support for new scientific discovery.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81944</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81944</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1533" target="n1535">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: An Active, Personalized, Adaptive, Multi-format Biological Information Delivery System</data>
      <data key="e_abstract">The explosive growth of biological information sources, available over the Internet, has given rise to both opportunities and challenges for biological and medical researchers. The opportunities they provide are both scientific (e.g., understanding the information encoded in elementary biological structures) as well as technological (e.g., new drug discovery). The challenges, on the other hand, lie in how to efficiently discover, among the vast volume of information, the items that are relevant or interesting to a given researcher. The objective of the proposed research is to investigate related basic research problems and develop a biological information delivery system in a collaborative project between computer scientists, information scientists, and biological researchers. The specific plans include developing methods to make the proposed system pro-active (surveying evolving on-line sources for relevant information), personalized (cognizant of a particular researcher&apos;s interests), adaptive (able to react to changes in the information sources as well as user interests or objectives), and capable of integrating multi-format data. The impact of this research is a significant enhancement in the ability of students and researchers in biological sciences to efficiently utilize on-line resources, while generating methods for computerized analysis of biological data and providing computerized support for new scientific discovery.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81944</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81944</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1533" target="n1536">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: An Active, Personalized, Adaptive, Multi-format Biological Information Delivery System</data>
      <data key="e_abstract">The explosive growth of biological information sources, available over the Internet, has given rise to both opportunities and challenges for biological and medical researchers. The opportunities they provide are both scientific (e.g., understanding the information encoded in elementary biological structures) as well as technological (e.g., new drug discovery). The challenges, on the other hand, lie in how to efficiently discover, among the vast volume of information, the items that are relevant or interesting to a given researcher. The objective of the proposed research is to investigate related basic research problems and develop a biological information delivery system in a collaborative project between computer scientists, information scientists, and biological researchers. The specific plans include developing methods to make the proposed system pro-active (surveying evolving on-line sources for relevant information), personalized (cognizant of a particular researcher&apos;s interests), adaptive (able to react to changes in the information sources as well as user interests or objectives), and capable of integrating multi-format data. The impact of this research is a significant enhancement in the ability of students and researchers in biological sciences to efficiently utilize on-line resources, while generating methods for computerized analysis of biological data and providing computerized support for new scientific discovery.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81944</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81944</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1534" target="n1535">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: An Active, Personalized, Adaptive, Multi-format Biological Information Delivery System</data>
      <data key="e_abstract">The explosive growth of biological information sources, available over the Internet, has given rise to both opportunities and challenges for biological and medical researchers. The opportunities they provide are both scientific (e.g., understanding the information encoded in elementary biological structures) as well as technological (e.g., new drug discovery). The challenges, on the other hand, lie in how to efficiently discover, among the vast volume of information, the items that are relevant or interesting to a given researcher. The objective of the proposed research is to investigate related basic research problems and develop a biological information delivery system in a collaborative project between computer scientists, information scientists, and biological researchers. The specific plans include developing methods to make the proposed system pro-active (surveying evolving on-line sources for relevant information), personalized (cognizant of a particular researcher&apos;s interests), adaptive (able to react to changes in the information sources as well as user interests or objectives), and capable of integrating multi-format data. The impact of this research is a significant enhancement in the ability of students and researchers in biological sciences to efficiently utilize on-line resources, while generating methods for computerized analysis of biological data and providing computerized support for new scientific discovery.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81944</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81944</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1534" target="n1536">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: An Active, Personalized, Adaptive, Multi-format Biological Information Delivery System</data>
      <data key="e_abstract">The explosive growth of biological information sources, available over the Internet, has given rise to both opportunities and challenges for biological and medical researchers. The opportunities they provide are both scientific (e.g., understanding the information encoded in elementary biological structures) as well as technological (e.g., new drug discovery). The challenges, on the other hand, lie in how to efficiently discover, among the vast volume of information, the items that are relevant or interesting to a given researcher. The objective of the proposed research is to investigate related basic research problems and develop a biological information delivery system in a collaborative project between computer scientists, information scientists, and biological researchers. The specific plans include developing methods to make the proposed system pro-active (surveying evolving on-line sources for relevant information), personalized (cognizant of a particular researcher&apos;s interests), adaptive (able to react to changes in the information sources as well as user interests or objectives), and capable of integrating multi-format data. The impact of this research is a significant enhancement in the ability of students and researchers in biological sciences to efficiently utilize on-line resources, while generating methods for computerized analysis of biological data and providing computerized support for new scientific discovery.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81944</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81944</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1535" target="n1536">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: An Active, Personalized, Adaptive, Multi-format Biological Information Delivery System</data>
      <data key="e_abstract">The explosive growth of biological information sources, available over the Internet, has given rise to both opportunities and challenges for biological and medical researchers. The opportunities they provide are both scientific (e.g., understanding the information encoded in elementary biological structures) as well as technological (e.g., new drug discovery). The challenges, on the other hand, lie in how to efficiently discover, among the vast volume of information, the items that are relevant or interesting to a given researcher. The objective of the proposed research is to investigate related basic research problems and develop a biological information delivery system in a collaborative project between computer scientists, information scientists, and biological researchers. The specific plans include developing methods to make the proposed system pro-active (surveying evolving on-line sources for relevant information), personalized (cognizant of a particular researcher&apos;s interests), adaptive (able to react to changes in the information sources as well as user interests or objectives), and capable of integrating multi-format data. The impact of this research is a significant enhancement in the ability of students and researchers in biological sciences to efficiently utilize on-line resources, while generating methods for computerized analysis of biological data and providing computerized support for new scientific discovery.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81944</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81944</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1537" target="n1538">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Automated Structuring of Text Information</data>
      <data key="e_abstract">At present, access to the information in large-scale text collections is largely limited to keyword-based searches which retrieve entire documents or passages. While such tools are often satisfactory in retrieving information on general topics, they provide little support for accessing information involving specific relationships, events, or facts.&lt;br/&gt;&lt;br/&gt;Information extraction technology offers the possibility of creating structured, tabular representations of selected relations from large text collections --- representations which can support more detailed document querying. Until now, however, developing extraction systems for a broad range of relations has been too expensive and time-consuming to consider its use in this way. Recent developments in extraction system customization offer the promise of substantially easing this task, and so making this approach to document indexing feasible.&lt;br/&gt;&lt;br/&gt;This research project will: 1) use corpus-based techniques to automatically identify the most common relationships within a sublanguage (the set of texts concerning a particular subject matter), and the different ways in which these relations are expressed in the text; 2)construct systems to extract information about these relationships from new text, building tabular summaries; and 3) provide a user interface for querying these relationships and accessing the underlying documents. Taken together, these tools should offer significant new capabilities for accessing the information in large text collections.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81962</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81962</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1539" target="n1540">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Interfaces for Supporting Over-The-Shoulder Learning</data>
      <data key="e_abstract">EIA-0081112&lt;br/&gt;Twidale, Michael&lt;br/&gt;University of Illinois at Urbana-Champaign&lt;br/&gt;&lt;br/&gt;ITR: Interfaces for Supporting Over-The-Shoulder Learning&lt;br/&gt;&lt;br/&gt;For many people, a key form of learning how to use software, for example,&lt;br/&gt;is not by taking a training course, nor reading a manual or online help, or&lt;br/&gt;experimenting with the software. Instead they may lean over the shoulder&lt;br/&gt;of a colleague at work and ask for help. This over-the-shoulder-learning&lt;br/&gt;(OTSL) is important to study to understand more about its relative&lt;br/&gt;importance and the circumstances in which it is and is not successful.&lt;br/&gt;Building on prior work studying informal collaborative help in libraries&lt;br/&gt;and offices, the research will address how often OTSL occurs and its&lt;br/&gt;significance as a way of learning, possible genres of OTSL, the evolution&lt;br/&gt;of learning a software application over time, the resources people use to&lt;br/&gt;support OTSL, barriers to OTSL as currently practiced, and the skills of&lt;br/&gt;efficient help-giving and determining how these skills may best be taught.&lt;br/&gt;This work will help determine the functionalities that have the greatest&lt;br/&gt;potential for improving the effectiveness of OTSL and contribute to&lt;br/&gt;fundamental research in user interface design and computer-supported&lt;br/&gt;cooperative work and learning.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81112</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">81112</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1541" target="n1542">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Diagnosis and Control of Network Variability by Massively Accessed Servers</data>
      <data key="e_abstract">This proposal seeks funding to investigate a number of basic research problems within the general framework of network-aware server protocols and services. Tackling these problems is critical for the continued scalability of the Internet.&lt;br/&gt; Mass servers are popular Internet servers which produce a substantial fraction of the traffic flowing through the network. Currently, such Mass servers are tuned to optimize their own performance (e.g. capacity, latency, I/O utilization), while overlooking the opportunity to monitor network conditions and to use that information advantageously. This information could be used both to improve network behavior (e.g. alleviate the problems resulting from congestion) and in further optimizing the performance of servicing requests. Thus, Mass servers are uniquely positioned (1) to observe and diagnose network conditions by tracking the flows that they generate, and (2) to manage and control network resources by better regulating and scheduling the traffic they inject into the network. These goals must be pursued over a wide spectrum of time scales to maximize the beneficial impact that can be achieved.&lt;br/&gt; On the shortest time scales, a Mass server can minimize packet loss by smoothing the otherwise bursty process of injecting packets into the network. At medium time scales, a Mass server can perform aggregate congestion management by bundling like connections to avoid the burstiness that results from competition among flows. At even longer time scales, a Mass server can map persistent hotspots in the network and optimize scheduling of connections to mitigate the impact of overusing those resources. These objectives are representative of projects which the researchers intend to undertake on these various time scales. &lt;br/&gt; The research work outlined in this proposal aims at achieving these benefits through the derivation of new measurement and analysis techniques to enable diagnosis of network conditions, and the development of new services and protocol to enable efficient network resource management and control.&lt;br/&gt; Proposed work in network measurement and diagnosis at Mass servers focuses on the use of passive and active probing techniques for the identification of network bottlenecks (e.g. congestion conditions) along various time scales. The techniques to be used range from multi-resolution analysis using discrete wavelet transforms, to maximum likelihood estimators for packet loss estimation.&lt;br/&gt; Empowered with such diagnostic information, the proposed work in network resource management and control at Mass servers focuses on alleviating the problems associated with network bottlenecks (e.g. large delays and jitters, low and unfair resource utilization) along various time scales. The techniques to be used range from the development of traffic pacing protocols to alleviate burstiness at the sub-round-trip-time scale, to aggregate congestion control algorithms for improving utilization of network resources and reducing jitter.&lt;br/&gt; A key component of the proposed work is implementation and prototyping. To that end, the utility of the tools and protocols developed will be demonstrated by integrating them into three modular components for distribution to the research community, namely: (1) BEACON: A collection of network measurement and diagnosis tools, (2) TURNPIKE: A collection of network management and control protocols and services, and (3) BACKBAY: A platform that supports the integration of BEACON and TURNPIKE functionality into a high performance web server architecture.&lt;br/&gt; The pursuit of the research goals outlined in this proposal is timely. Achieving these goals will leapfrog current piecemeal attempts aiming at supporting Internet growth. The research team assembled to pursue these ambitious goals has made significant, nationally-recognized contributions to the understanding of Internet traffic characteristics and has an established record in software development and technology transfer. Boston University is committed to supporting this team through substantial financial and infrastructural commitments that complement and leverage the support sought from NSF.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">9.9864e+06</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.9864e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1541" target="n1543">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Diagnosis and Control of Network Variability by Massively Accessed Servers</data>
      <data key="e_abstract">This proposal seeks funding to investigate a number of basic research problems within the general framework of network-aware server protocols and services. Tackling these problems is critical for the continued scalability of the Internet.&lt;br/&gt; Mass servers are popular Internet servers which produce a substantial fraction of the traffic flowing through the network. Currently, such Mass servers are tuned to optimize their own performance (e.g. capacity, latency, I/O utilization), while overlooking the opportunity to monitor network conditions and to use that information advantageously. This information could be used both to improve network behavior (e.g. alleviate the problems resulting from congestion) and in further optimizing the performance of servicing requests. Thus, Mass servers are uniquely positioned (1) to observe and diagnose network conditions by tracking the flows that they generate, and (2) to manage and control network resources by better regulating and scheduling the traffic they inject into the network. These goals must be pursued over a wide spectrum of time scales to maximize the beneficial impact that can be achieved.&lt;br/&gt; On the shortest time scales, a Mass server can minimize packet loss by smoothing the otherwise bursty process of injecting packets into the network. At medium time scales, a Mass server can perform aggregate congestion management by bundling like connections to avoid the burstiness that results from competition among flows. At even longer time scales, a Mass server can map persistent hotspots in the network and optimize scheduling of connections to mitigate the impact of overusing those resources. These objectives are representative of projects which the researchers intend to undertake on these various time scales. &lt;br/&gt; The research work outlined in this proposal aims at achieving these benefits through the derivation of new measurement and analysis techniques to enable diagnosis of network conditions, and the development of new services and protocol to enable efficient network resource management and control.&lt;br/&gt; Proposed work in network measurement and diagnosis at Mass servers focuses on the use of passive and active probing techniques for the identification of network bottlenecks (e.g. congestion conditions) along various time scales. The techniques to be used range from multi-resolution analysis using discrete wavelet transforms, to maximum likelihood estimators for packet loss estimation.&lt;br/&gt; Empowered with such diagnostic information, the proposed work in network resource management and control at Mass servers focuses on alleviating the problems associated with network bottlenecks (e.g. large delays and jitters, low and unfair resource utilization) along various time scales. The techniques to be used range from the development of traffic pacing protocols to alleviate burstiness at the sub-round-trip-time scale, to aggregate congestion control algorithms for improving utilization of network resources and reducing jitter.&lt;br/&gt; A key component of the proposed work is implementation and prototyping. To that end, the utility of the tools and protocols developed will be demonstrated by integrating them into three modular components for distribution to the research community, namely: (1) BEACON: A collection of network measurement and diagnosis tools, (2) TURNPIKE: A collection of network management and control protocols and services, and (3) BACKBAY: A platform that supports the integration of BEACON and TURNPIKE functionality into a high performance web server architecture.&lt;br/&gt; The pursuit of the research goals outlined in this proposal is timely. Achieving these goals will leapfrog current piecemeal attempts aiming at supporting Internet growth. The research team assembled to pursue these ambitious goals has made significant, nationally-recognized contributions to the understanding of Internet traffic characteristics and has an established record in software development and technology transfer. Boston University is committed to supporting this team through substantial financial and infrastructural commitments that complement and leverage the support sought from NSF.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">9.9864e+06</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.9864e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1542" target="n1543">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Diagnosis and Control of Network Variability by Massively Accessed Servers</data>
      <data key="e_abstract">This proposal seeks funding to investigate a number of basic research problems within the general framework of network-aware server protocols and services. Tackling these problems is critical for the continued scalability of the Internet.&lt;br/&gt; Mass servers are popular Internet servers which produce a substantial fraction of the traffic flowing through the network. Currently, such Mass servers are tuned to optimize their own performance (e.g. capacity, latency, I/O utilization), while overlooking the opportunity to monitor network conditions and to use that information advantageously. This information could be used both to improve network behavior (e.g. alleviate the problems resulting from congestion) and in further optimizing the performance of servicing requests. Thus, Mass servers are uniquely positioned (1) to observe and diagnose network conditions by tracking the flows that they generate, and (2) to manage and control network resources by better regulating and scheduling the traffic they inject into the network. These goals must be pursued over a wide spectrum of time scales to maximize the beneficial impact that can be achieved.&lt;br/&gt; On the shortest time scales, a Mass server can minimize packet loss by smoothing the otherwise bursty process of injecting packets into the network. At medium time scales, a Mass server can perform aggregate congestion management by bundling like connections to avoid the burstiness that results from competition among flows. At even longer time scales, a Mass server can map persistent hotspots in the network and optimize scheduling of connections to mitigate the impact of overusing those resources. These objectives are representative of projects which the researchers intend to undertake on these various time scales. &lt;br/&gt; The research work outlined in this proposal aims at achieving these benefits through the derivation of new measurement and analysis techniques to enable diagnosis of network conditions, and the development of new services and protocol to enable efficient network resource management and control.&lt;br/&gt; Proposed work in network measurement and diagnosis at Mass servers focuses on the use of passive and active probing techniques for the identification of network bottlenecks (e.g. congestion conditions) along various time scales. The techniques to be used range from multi-resolution analysis using discrete wavelet transforms, to maximum likelihood estimators for packet loss estimation.&lt;br/&gt; Empowered with such diagnostic information, the proposed work in network resource management and control at Mass servers focuses on alleviating the problems associated with network bottlenecks (e.g. large delays and jitters, low and unfair resource utilization) along various time scales. The techniques to be used range from the development of traffic pacing protocols to alleviate burstiness at the sub-round-trip-time scale, to aggregate congestion control algorithms for improving utilization of network resources and reducing jitter.&lt;br/&gt; A key component of the proposed work is implementation and prototyping. To that end, the utility of the tools and protocols developed will be demonstrated by integrating them into three modular components for distribution to the research community, namely: (1) BEACON: A collection of network measurement and diagnosis tools, (2) TURNPIKE: A collection of network management and control protocols and services, and (3) BACKBAY: A platform that supports the integration of BEACON and TURNPIKE functionality into a high performance web server architecture.&lt;br/&gt; The pursuit of the research goals outlined in this proposal is timely. Achieving these goals will leapfrog current piecemeal attempts aiming at supporting Internet growth. The research team assembled to pursue these ambitious goals has made significant, nationally-recognized contributions to the understanding of Internet traffic characteristics and has an established record in software development and technology transfer. Boston University is committed to supporting this team through substantial financial and infrastructural commitments that complement and leverage the support sought from NSF.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">9.9864e+06</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.9864e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1545" target="n1546">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Global Optimization in Computational Biology</data>
      <data key="e_abstract">This project is a collaboration between members of the global optimization computer science and the computational biology communities to develop new methods for protein structure prediction and ligand docking. If successful, the methods developed here will help solve two important problems in computational biology - the need for fast conformational searching to predict the structures of proteins or complexes of proteins with other proteins and ligands, and the need to improve folding and docking models. Both problems are fundamental to understanding how the molecules of life work. This project will not solve these problems itself, but will develop new computational methods that can contribute to their solution.&lt;br/&gt;&lt;br/&gt;Technically, there are two specific aims: (1) To develop efficient methods for searching conformational spaces to find globally optimal (native) conformations on energy landscapes. (2) To develop efficient methods for searching parameter spaces to find optimal parameters for the large, complex models that are common in computational biology. This work is based on Underestimator methods that do not search over the tops of energy landscapes like Monte Carlo, Simulated Annealing, and Molecular Dynamics, the current standard methods.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82146</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82146</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n547" target="n548">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Analysis of the Capacity Improvement for Wireless Networks with Multiple Transmit and Receive Antennas</data>
      <data key="e_abstract">It is very well known that there is sometimes a striking divergence between the continuous and the discrete. The theory of time scales has been initiated by Stefan Hilger ten years ago in order to unify study for differential and difference equations. Not only is it able to treat these two cases simultaneously, but it also can handle numerous other cases \in between&quot; the continuous and the discrete, and those cases might be important for applications. For this reason it also could be worth to think about the consequences of a better developed time scales theory for graduate and in particular for undergraduate education. As the theory is very new and virtually important to every area in analysis, work on this subject done now will be fundamental. If NSF funds this proposal we would expand the theory of dynamic equations on time scales in various ways. We would like to conduct study of linear dynamic systems on time scales which would result in a better understanding of higher order dynamic equations. For those the task of characterizing disconjugacy is an interesting one, and this question we also want to address for higher order Sturm-Liouville dynamic equations or, more general, linear Hamiltonian dynamic systems on time scales. Such systems also would be&lt;br/&gt;interesting if an eigenvalue parameter was involved, and we would like to derive an existence theorem, an expansion theorem, Rayleigh&apos;s Principle, and more general oscillation results for these eigenvalue problems. Another interesting task, which is intimately connected to the above problems, would be to establish a theory of variational analysis on time scales. To give necessary and sufficient conditions for strong and weak local minima would be the main concern of such a project, and it would also be of use to establish a Weierstrab Theory for variational problems on time scales.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81476</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81476</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n547" target="n549">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Analysis of the Capacity Improvement for Wireless Networks with Multiple Transmit and Receive Antennas</data>
      <data key="e_abstract">It is very well known that there is sometimes a striking divergence between the continuous and the discrete. The theory of time scales has been initiated by Stefan Hilger ten years ago in order to unify study for differential and difference equations. Not only is it able to treat these two cases simultaneously, but it also can handle numerous other cases \in between&quot; the continuous and the discrete, and those cases might be important for applications. For this reason it also could be worth to think about the consequences of a better developed time scales theory for graduate and in particular for undergraduate education. As the theory is very new and virtually important to every area in analysis, work on this subject done now will be fundamental. If NSF funds this proposal we would expand the theory of dynamic equations on time scales in various ways. We would like to conduct study of linear dynamic systems on time scales which would result in a better understanding of higher order dynamic equations. For those the task of characterizing disconjugacy is an interesting one, and this question we also want to address for higher order Sturm-Liouville dynamic equations or, more general, linear Hamiltonian dynamic systems on time scales. Such systems also would be&lt;br/&gt;interesting if an eigenvalue parameter was involved, and we would like to derive an existence theorem, an expansion theorem, Rayleigh&apos;s Principle, and more general oscillation results for these eigenvalue problems. Another interesting task, which is intimately connected to the above problems, would be to establish a theory of variational analysis on time scales. To give necessary and sufficient conditions for strong and weak local minima would be the main concern of such a project, and it would also be of use to establish a Weierstrab Theory for variational problems on time scales.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81476</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81476</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n547" target="n550">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Analysis of the Capacity Improvement for Wireless Networks with Multiple Transmit and Receive Antennas</data>
      <data key="e_abstract">It is very well known that there is sometimes a striking divergence between the continuous and the discrete. The theory of time scales has been initiated by Stefan Hilger ten years ago in order to unify study for differential and difference equations. Not only is it able to treat these two cases simultaneously, but it also can handle numerous other cases \in between&quot; the continuous and the discrete, and those cases might be important for applications. For this reason it also could be worth to think about the consequences of a better developed time scales theory for graduate and in particular for undergraduate education. As the theory is very new and virtually important to every area in analysis, work on this subject done now will be fundamental. If NSF funds this proposal we would expand the theory of dynamic equations on time scales in various ways. We would like to conduct study of linear dynamic systems on time scales which would result in a better understanding of higher order dynamic equations. For those the task of characterizing disconjugacy is an interesting one, and this question we also want to address for higher order Sturm-Liouville dynamic equations or, more general, linear Hamiltonian dynamic systems on time scales. Such systems also would be&lt;br/&gt;interesting if an eigenvalue parameter was involved, and we would like to derive an existence theorem, an expansion theorem, Rayleigh&apos;s Principle, and more general oscillation results for these eigenvalue problems. Another interesting task, which is intimately connected to the above problems, would be to establish a theory of variational analysis on time scales. To give necessary and sufficient conditions for strong and weak local minima would be the main concern of such a project, and it would also be of use to establish a Weierstrab Theory for variational problems on time scales.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81476</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81476</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n548" target="n549">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Analysis of the Capacity Improvement for Wireless Networks with Multiple Transmit and Receive Antennas</data>
      <data key="e_abstract">It is very well known that there is sometimes a striking divergence between the continuous and the discrete. The theory of time scales has been initiated by Stefan Hilger ten years ago in order to unify study for differential and difference equations. Not only is it able to treat these two cases simultaneously, but it also can handle numerous other cases \in between&quot; the continuous and the discrete, and those cases might be important for applications. For this reason it also could be worth to think about the consequences of a better developed time scales theory for graduate and in particular for undergraduate education. As the theory is very new and virtually important to every area in analysis, work on this subject done now will be fundamental. If NSF funds this proposal we would expand the theory of dynamic equations on time scales in various ways. We would like to conduct study of linear dynamic systems on time scales which would result in a better understanding of higher order dynamic equations. For those the task of characterizing disconjugacy is an interesting one, and this question we also want to address for higher order Sturm-Liouville dynamic equations or, more general, linear Hamiltonian dynamic systems on time scales. Such systems also would be&lt;br/&gt;interesting if an eigenvalue parameter was involved, and we would like to derive an existence theorem, an expansion theorem, Rayleigh&apos;s Principle, and more general oscillation results for these eigenvalue problems. Another interesting task, which is intimately connected to the above problems, would be to establish a theory of variational analysis on time scales. To give necessary and sufficient conditions for strong and weak local minima would be the main concern of such a project, and it would also be of use to establish a Weierstrab Theory for variational problems on time scales.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81476</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81476</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n548" target="n550">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Analysis of the Capacity Improvement for Wireless Networks with Multiple Transmit and Receive Antennas</data>
      <data key="e_abstract">It is very well known that there is sometimes a striking divergence between the continuous and the discrete. The theory of time scales has been initiated by Stefan Hilger ten years ago in order to unify study for differential and difference equations. Not only is it able to treat these two cases simultaneously, but it also can handle numerous other cases \in between&quot; the continuous and the discrete, and those cases might be important for applications. For this reason it also could be worth to think about the consequences of a better developed time scales theory for graduate and in particular for undergraduate education. As the theory is very new and virtually important to every area in analysis, work on this subject done now will be fundamental. If NSF funds this proposal we would expand the theory of dynamic equations on time scales in various ways. We would like to conduct study of linear dynamic systems on time scales which would result in a better understanding of higher order dynamic equations. For those the task of characterizing disconjugacy is an interesting one, and this question we also want to address for higher order Sturm-Liouville dynamic equations or, more general, linear Hamiltonian dynamic systems on time scales. Such systems also would be&lt;br/&gt;interesting if an eigenvalue parameter was involved, and we would like to derive an existence theorem, an expansion theorem, Rayleigh&apos;s Principle, and more general oscillation results for these eigenvalue problems. Another interesting task, which is intimately connected to the above problems, would be to establish a theory of variational analysis on time scales. To give necessary and sufficient conditions for strong and weak local minima would be the main concern of such a project, and it would also be of use to establish a Weierstrab Theory for variational problems on time scales.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81476</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81476</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n549" target="n550">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Analysis of the Capacity Improvement for Wireless Networks with Multiple Transmit and Receive Antennas</data>
      <data key="e_abstract">It is very well known that there is sometimes a striking divergence between the continuous and the discrete. The theory of time scales has been initiated by Stefan Hilger ten years ago in order to unify study for differential and difference equations. Not only is it able to treat these two cases simultaneously, but it also can handle numerous other cases \in between&quot; the continuous and the discrete, and those cases might be important for applications. For this reason it also could be worth to think about the consequences of a better developed time scales theory for graduate and in particular for undergraduate education. As the theory is very new and virtually important to every area in analysis, work on this subject done now will be fundamental. If NSF funds this proposal we would expand the theory of dynamic equations on time scales in various ways. We would like to conduct study of linear dynamic systems on time scales which would result in a better understanding of higher order dynamic equations. For those the task of characterizing disconjugacy is an interesting one, and this question we also want to address for higher order Sturm-Liouville dynamic equations or, more general, linear Hamiltonian dynamic systems on time scales. Such systems also would be&lt;br/&gt;interesting if an eigenvalue parameter was involved, and we would like to derive an existence theorem, an expansion theorem, Rayleigh&apos;s Principle, and more general oscillation results for these eigenvalue problems. Another interesting task, which is intimately connected to the above problems, would be to establish a theory of variational analysis on time scales. To give necessary and sufficient conditions for strong and weak local minima would be the main concern of such a project, and it would also be of use to establish a Weierstrab Theory for variational problems on time scales.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">81476</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">81476</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n323" target="n392">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Real-Time Systems-on-a-Chip</data>
      <data key="e_abstract">The embedded processor of the future will have multiple heterogeneous processors on a single chip integrated with a large shared memory. Many of the applications for this processor will have strict real-time requirements. This project investigates novel architectural and system software features that can allow such a processor to support complex real-time applications. Architectural features that limit shared memory contention and reduce synchronization overhead will be studied in order to reduce latencies and make thread execution times more predictable. The project will also explore the use of specialized I/O processors having direct and equal access to shared memory in order to minimize interference of I/O operations on real-time thread execution. Novel techniques for scheduling real-time threads on a heterogeneous multiprocessor-on-a-chip will also be studied.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82164</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82164</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n523" target="n1556">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research:Scalable Quality-of-Service Control for the Next Generation Internet: Fundamental Challenges and Effective Solutions</data>
      <data key="e_abstract">Today&apos;s Internet owes its great success to the simple, &quot;hour-glass&quot; IP network protocol architecture laid&lt;br/&gt;out twenty-five years ago. With rapid advances in networking technologies and explosive growth of rich&lt;br/&gt;multimedia content in recent years, the networking community finds itself at an important crossroads: what&lt;br/&gt;should be the next generation Internet architecture for controlling network resources and provide the quality&lt;br/&gt;of service (QoS) needed by emerging multimedia applications? There is a multidimensional spectrum of&lt;br/&gt;possible approaches to providing QoS guarantees. The choice of a QoS solution for the next generation&lt;br/&gt;Internet will have a substantial impact on both the evolution of the Internet itself, and on what it enables.&lt;br/&gt;Making the &quot;right&quot; choices requires the development of a fundamental understanding of the scalability of&lt;br/&gt;QoS controls and the impact of these controls on the efficacy of QoS provisioning.&lt;br/&gt;&lt;br/&gt;The goal of the proposed research is to develop a comprehensive, quantitative understanding of the&lt;br/&gt;fundamental trade-offs involved in various approaches toward providing scalable QoS guarantees. To this&lt;br/&gt;end, the researchers will develop coherent theories to systematically address the issue of scalability in QoS controls. The research program divides broadly into four areas:&lt;br/&gt;&lt;br/&gt;Aggregate network calculus for guaranteed flows: To gain a thorough understanding of the fine time-scale&lt;br/&gt;(e.g., packet-level) behavior of a network system in providing QoS performance guarantees, the researchers&lt;br/&gt;will develop an aggregate network calculus to study the impact of aggregate QoS control mechanisms&lt;br/&gt;on the performance and complexity of data plane operations. This theory is developed for guaranteed&lt;br/&gt;flows - flows which require the network to commit, either at a per-flow or an aggregate level, a&lt;br/&gt;certain amount of resources (e.g., bandwidth and buffer) throughout their life time, regardless of the&lt;br/&gt;network congestion status. The aggregate network calculus will provide a mathematical framework&lt;br/&gt;to quantify the impact of aggregate QoS controls on the fundamental trade-offs in QoS provisioning.&lt;br/&gt;It will also yield insights into the design of scalable data plane QoS control mechanisms.&lt;br/&gt;&lt;br/&gt;End-to-end QoS controls for responsive flows: The researchers will develop fluid models to study the impact of aggregate QoS control mechanisms on the end-to-end performance of responsive flows. A responsive&lt;br/&gt;flow responds to signs of network congestion, such as loss, by adapting its transmission rate. These&lt;br/&gt;models will enable us to develop a better understanding of the behavior of responsive flows such as&lt;br/&gt;TCP coupled with different aggregate QoS mechanisms and to design end-to-end QoS services for&lt;br/&gt;responsive flows.&lt;br/&gt;&lt;br/&gt;QoS control laws and control plane aggregation rules. The researchers will develop QoS control laws for capturing the slow time-scale, system-wide behavior of a network and aggregation rules that address the performance and complexity of control plane operations under aggregate QoS controls. These QoS control&lt;br/&gt;laws and aggregation rules will lead us to the design of distributed and centralized algorithms for&lt;br/&gt;scalable control plane operations.&lt;br/&gt;&lt;br/&gt;Scalable QoS mechanisms and service architectures as an integral part in developing these theories, the researchers will also design effective and scalable QoS mechanisms, and tools and techniques for quantifying and evaluating the trade-offs of various QoS solutions. Based on the results from these efforts, the researchers will study how various QoS solutions can be combined to construct meaningful end-to-end services.&lt;br/&gt;&lt;br/&gt;The research will blend formal modeling/analysis, experimentation/implementation, and evaluation. The&lt;br/&gt;understanding and insights gained as a result of the research will lead to the establishment of the theory,&lt;br/&gt;design principles, and guidelines for building scalable QoS controls for the future Internet. This, in turn,&lt;br/&gt;will allow reasoned and informed choices to be made as the next generation Internet takes shape.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85848</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85848</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n296" target="n1556">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research:Scalable Quality-of-Service Control for the Next Generation Internet: Fundamental Challenges and Effective Solutions</data>
      <data key="e_abstract">Today&apos;s Internet owes its great success to the simple, &quot;hour-glass&quot; IP network protocol architecture laid&lt;br/&gt;out twenty-five years ago. With rapid advances in networking technologies and explosive growth of rich&lt;br/&gt;multimedia content in recent years, the networking community finds itself at an important crossroads: what&lt;br/&gt;should be the next generation Internet architecture for controlling network resources and provide the quality&lt;br/&gt;of service (QoS) needed by emerging multimedia applications? There is a multidimensional spectrum of&lt;br/&gt;possible approaches to providing QoS guarantees. The choice of a QoS solution for the next generation&lt;br/&gt;Internet will have a substantial impact on both the evolution of the Internet itself, and on what it enables.&lt;br/&gt;Making the &quot;right&quot; choices requires the development of a fundamental understanding of the scalability of&lt;br/&gt;QoS controls and the impact of these controls on the efficacy of QoS provisioning.&lt;br/&gt;&lt;br/&gt;The goal of the proposed research is to develop a comprehensive, quantitative understanding of the&lt;br/&gt;fundamental trade-offs involved in various approaches toward providing scalable QoS guarantees. To this&lt;br/&gt;end, the researchers will develop coherent theories to systematically address the issue of scalability in QoS controls. The research program divides broadly into four areas:&lt;br/&gt;&lt;br/&gt;Aggregate network calculus for guaranteed flows: To gain a thorough understanding of the fine time-scale&lt;br/&gt;(e.g., packet-level) behavior of a network system in providing QoS performance guarantees, the researchers&lt;br/&gt;will develop an aggregate network calculus to study the impact of aggregate QoS control mechanisms&lt;br/&gt;on the performance and complexity of data plane operations. This theory is developed for guaranteed&lt;br/&gt;flows - flows which require the network to commit, either at a per-flow or an aggregate level, a&lt;br/&gt;certain amount of resources (e.g., bandwidth and buffer) throughout their life time, regardless of the&lt;br/&gt;network congestion status. The aggregate network calculus will provide a mathematical framework&lt;br/&gt;to quantify the impact of aggregate QoS controls on the fundamental trade-offs in QoS provisioning.&lt;br/&gt;It will also yield insights into the design of scalable data plane QoS control mechanisms.&lt;br/&gt;&lt;br/&gt;End-to-end QoS controls for responsive flows: The researchers will develop fluid models to study the impact of aggregate QoS control mechanisms on the end-to-end performance of responsive flows. A responsive&lt;br/&gt;flow responds to signs of network congestion, such as loss, by adapting its transmission rate. These&lt;br/&gt;models will enable us to develop a better understanding of the behavior of responsive flows such as&lt;br/&gt;TCP coupled with different aggregate QoS mechanisms and to design end-to-end QoS services for&lt;br/&gt;responsive flows.&lt;br/&gt;&lt;br/&gt;QoS control laws and control plane aggregation rules. The researchers will develop QoS control laws for capturing the slow time-scale, system-wide behavior of a network and aggregation rules that address the performance and complexity of control plane operations under aggregate QoS controls. These QoS control&lt;br/&gt;laws and aggregation rules will lead us to the design of distributed and centralized algorithms for&lt;br/&gt;scalable control plane operations.&lt;br/&gt;&lt;br/&gt;Scalable QoS mechanisms and service architectures as an integral part in developing these theories, the researchers will also design effective and scalable QoS mechanisms, and tools and techniques for quantifying and evaluating the trade-offs of various QoS solutions. Based on the results from these efforts, the researchers will study how various QoS solutions can be combined to construct meaningful end-to-end services.&lt;br/&gt;&lt;br/&gt;The research will blend formal modeling/analysis, experimentation/implementation, and evaluation. The&lt;br/&gt;understanding and insights gained as a result of the research will lead to the establishment of the theory,&lt;br/&gt;design principles, and guidelines for building scalable QoS controls for the future Internet. This, in turn,&lt;br/&gt;will allow reasoned and informed choices to be made as the next generation Internet takes shape.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85848</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85848</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n296" target="n523">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Collaborative Research:Scalable Quality-of-Service Control for the Next Generation Internet: Fundamental Challenges and Effective Solutions</data>
      <data key="e_abstract">Today&apos;s Internet owes its great success to the simple, &quot;hour-glass&quot; IP network protocol architecture laid&lt;br/&gt;out twenty-five years ago. With rapid advances in networking technologies and explosive growth of rich&lt;br/&gt;multimedia content in recent years, the networking community finds itself at an important crossroads: what&lt;br/&gt;should be the next generation Internet architecture for controlling network resources and provide the quality&lt;br/&gt;of service (QoS) needed by emerging multimedia applications? There is a multidimensional spectrum of&lt;br/&gt;possible approaches to providing QoS guarantees. The choice of a QoS solution for the next generation&lt;br/&gt;Internet will have a substantial impact on both the evolution of the Internet itself, and on what it enables.&lt;br/&gt;Making the &quot;right&quot; choices requires the development of a fundamental understanding of the scalability of&lt;br/&gt;QoS controls and the impact of these controls on the efficacy of QoS provisioning.&lt;br/&gt;&lt;br/&gt;The goal of the proposed research is to develop a comprehensive, quantitative understanding of the&lt;br/&gt;fundamental trade-offs involved in various approaches toward providing scalable QoS guarantees. To this&lt;br/&gt;end, the researchers will develop coherent theories to systematically address the issue of scalability in QoS controls. The research program divides broadly into four areas:&lt;br/&gt;&lt;br/&gt;Aggregate network calculus for guaranteed flows: To gain a thorough understanding of the fine time-scale&lt;br/&gt;(e.g., packet-level) behavior of a network system in providing QoS performance guarantees, the researchers&lt;br/&gt;will develop an aggregate network calculus to study the impact of aggregate QoS control mechanisms&lt;br/&gt;on the performance and complexity of data plane operations. This theory is developed for guaranteed&lt;br/&gt;flows - flows which require the network to commit, either at a per-flow or an aggregate level, a&lt;br/&gt;certain amount of resources (e.g., bandwidth and buffer) throughout their life time, regardless of the&lt;br/&gt;network congestion status. The aggregate network calculus will provide a mathematical framework&lt;br/&gt;to quantify the impact of aggregate QoS controls on the fundamental trade-offs in QoS provisioning.&lt;br/&gt;It will also yield insights into the design of scalable data plane QoS control mechanisms.&lt;br/&gt;&lt;br/&gt;End-to-end QoS controls for responsive flows: The researchers will develop fluid models to study the impact of aggregate QoS control mechanisms on the end-to-end performance of responsive flows. A responsive&lt;br/&gt;flow responds to signs of network congestion, such as loss, by adapting its transmission rate. These&lt;br/&gt;models will enable us to develop a better understanding of the behavior of responsive flows such as&lt;br/&gt;TCP coupled with different aggregate QoS mechanisms and to design end-to-end QoS services for&lt;br/&gt;responsive flows.&lt;br/&gt;&lt;br/&gt;QoS control laws and control plane aggregation rules. The researchers will develop QoS control laws for capturing the slow time-scale, system-wide behavior of a network and aggregation rules that address the performance and complexity of control plane operations under aggregate QoS controls. These QoS control&lt;br/&gt;laws and aggregation rules will lead us to the design of distributed and centralized algorithms for&lt;br/&gt;scalable control plane operations.&lt;br/&gt;&lt;br/&gt;Scalable QoS mechanisms and service architectures as an integral part in developing these theories, the researchers will also design effective and scalable QoS mechanisms, and tools and techniques for quantifying and evaluating the trade-offs of various QoS solutions. Based on the results from these efforts, the researchers will study how various QoS solutions can be combined to construct meaningful end-to-end services.&lt;br/&gt;&lt;br/&gt;The research will blend formal modeling/analysis, experimentation/implementation, and evaluation. The&lt;br/&gt;understanding and insights gained as a result of the research will lead to the establishment of the theory,&lt;br/&gt;design principles, and guidelines for building scalable QoS controls for the future Internet. This, in turn,&lt;br/&gt;will allow reasoned and informed choices to be made as the next generation Internet takes shape.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85848</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">85848</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1560" target="n1561">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: &apos;Globalization, Electronic Commerce and Social Impacts: The Influence of National Environments on Diffusion and Impacts of the Internet&apos;.</data>
      <data key="e_abstract">This research examines the global diffusion of Internet-based electronic commerce, how national environments and policies influence e-commerce use within countries, and the economic and social impacts of e-commerce. The project employs both quantitative and qualitative methodologies, including a comparative study of diffusion across 42 countries and detailed case studies in eight countries that look at the growth of e-commerce nationally, and also in three critical industry sectors: high-technology, financial services and retail. These studies will identify key trends in diffusion, critical environmental and policy factors that influence the diffusion, and major impacts of e-commerce. The project will be carried out by researchers from computer science, social systems and management and will involve data collection collaboration with experts from a total of eight countries, including Brazil, Denmark, France, Japan, Mexico, Singapore, Taiwan, and the United States. The outcomes of the research include 1) new scientific understanding of the use and impacts of e-commerce in different countries, 2) baseline benchmarks for future studies of industry, national and global trends, 3) strategic insights for business executives involved in global e-commerce, 4) policy insights for governments to promote and maximize the benefits of global commerce, and 5) extension of the research community through the education of graduate students.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">85852</data>
      <data key="e_expirationDate">2008-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85852</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n181" target="n1562">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Infrastructure to Support Research on Mixed Wired/Wireless Information Systems</data>
      <data key="e_abstract">EIA-0080119&lt;br/&gt;Towsley, Donald F.&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Infrastructure to Support Research on Mixed Wired/Wireless Information&lt;br/&gt;&lt;br/&gt;The goal of this project is research and development of control strategies, and services required by application suites executing over mixed wired/wireless networks. The future of networking will introduce a setting where individuals and embedded processes will communicate among themselves and with multimedia information servers over a network made up of diverse network technologies, including ad-hoc and cell-based wireless, and wired segments. This network will seamlessly provide a diverse range of information-based services. This reflects a fundamental shift in the way users will compute and communicate in the future, moving from today&apos;s wire-based network where users are immobile and know where to obtain services to an environment in which users are mobile, may be connected through wireless, perhaps even in an ad-hoc manner, and request and receive services in a transparent manner. &lt;br/&gt;&lt;br/&gt;The project will produce the fundamental advances required in the areas of coding and modulation, access protocols, routing, quality of service, operating systems (OS), database systems, security, and performance evaluation, that will be needed to produce this next generation network. These advances will occur as the result of an integrated, collaborative, and multidisciplinary effort spanning a wide range of disciplines in computer science and electrical engineering at the University of Massachusetts.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80119</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80119</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n296" target="n1562">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Infrastructure to Support Research on Mixed Wired/Wireless Information Systems</data>
      <data key="e_abstract">EIA-0080119&lt;br/&gt;Towsley, Donald F.&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Infrastructure to Support Research on Mixed Wired/Wireless Information&lt;br/&gt;&lt;br/&gt;The goal of this project is research and development of control strategies, and services required by application suites executing over mixed wired/wireless networks. The future of networking will introduce a setting where individuals and embedded processes will communicate among themselves and with multimedia information servers over a network made up of diverse network technologies, including ad-hoc and cell-based wireless, and wired segments. This network will seamlessly provide a diverse range of information-based services. This reflects a fundamental shift in the way users will compute and communicate in the future, moving from today&apos;s wire-based network where users are immobile and know where to obtain services to an environment in which users are mobile, may be connected through wireless, perhaps even in an ad-hoc manner, and request and receive services in a transparent manner. &lt;br/&gt;&lt;br/&gt;The project will produce the fundamental advances required in the areas of coding and modulation, access protocols, routing, quality of service, operating systems (OS), database systems, security, and performance evaluation, that will be needed to produce this next generation network. These advances will occur as the result of an integrated, collaborative, and multidisciplinary effort spanning a wide range of disciplines in computer science and electrical engineering at the University of Massachusetts.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80119</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80119</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1562" target="n1565">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Infrastructure to Support Research on Mixed Wired/Wireless Information Systems</data>
      <data key="e_abstract">EIA-0080119&lt;br/&gt;Towsley, Donald F.&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Infrastructure to Support Research on Mixed Wired/Wireless Information&lt;br/&gt;&lt;br/&gt;The goal of this project is research and development of control strategies, and services required by application suites executing over mixed wired/wireless networks. The future of networking will introduce a setting where individuals and embedded processes will communicate among themselves and with multimedia information servers over a network made up of diverse network technologies, including ad-hoc and cell-based wireless, and wired segments. This network will seamlessly provide a diverse range of information-based services. This reflects a fundamental shift in the way users will compute and communicate in the future, moving from today&apos;s wire-based network where users are immobile and know where to obtain services to an environment in which users are mobile, may be connected through wireless, perhaps even in an ad-hoc manner, and request and receive services in a transparent manner. &lt;br/&gt;&lt;br/&gt;The project will produce the fundamental advances required in the areas of coding and modulation, access protocols, routing, quality of service, operating systems (OS), database systems, security, and performance evaluation, that will be needed to produce this next generation network. These advances will occur as the result of an integrated, collaborative, and multidisciplinary effort spanning a wide range of disciplines in computer science and electrical engineering at the University of Massachusetts.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80119</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80119</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1562" target="n1566">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Infrastructure to Support Research on Mixed Wired/Wireless Information Systems</data>
      <data key="e_abstract">EIA-0080119&lt;br/&gt;Towsley, Donald F.&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Infrastructure to Support Research on Mixed Wired/Wireless Information&lt;br/&gt;&lt;br/&gt;The goal of this project is research and development of control strategies, and services required by application suites executing over mixed wired/wireless networks. The future of networking will introduce a setting where individuals and embedded processes will communicate among themselves and with multimedia information servers over a network made up of diverse network technologies, including ad-hoc and cell-based wireless, and wired segments. This network will seamlessly provide a diverse range of information-based services. This reflects a fundamental shift in the way users will compute and communicate in the future, moving from today&apos;s wire-based network where users are immobile and know where to obtain services to an environment in which users are mobile, may be connected through wireless, perhaps even in an ad-hoc manner, and request and receive services in a transparent manner. &lt;br/&gt;&lt;br/&gt;The project will produce the fundamental advances required in the areas of coding and modulation, access protocols, routing, quality of service, operating systems (OS), database systems, security, and performance evaluation, that will be needed to produce this next generation network. These advances will occur as the result of an integrated, collaborative, and multidisciplinary effort spanning a wide range of disciplines in computer science and electrical engineering at the University of Massachusetts.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80119</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80119</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n181" target="n296">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Infrastructure to Support Research on Mixed Wired/Wireless Information Systems</data>
      <data key="e_abstract">EIA-0080119&lt;br/&gt;Towsley, Donald F.&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Infrastructure to Support Research on Mixed Wired/Wireless Information&lt;br/&gt;&lt;br/&gt;The goal of this project is research and development of control strategies, and services required by application suites executing over mixed wired/wireless networks. The future of networking will introduce a setting where individuals and embedded processes will communicate among themselves and with multimedia information servers over a network made up of diverse network technologies, including ad-hoc and cell-based wireless, and wired segments. This network will seamlessly provide a diverse range of information-based services. This reflects a fundamental shift in the way users will compute and communicate in the future, moving from today&apos;s wire-based network where users are immobile and know where to obtain services to an environment in which users are mobile, may be connected through wireless, perhaps even in an ad-hoc manner, and request and receive services in a transparent manner. &lt;br/&gt;&lt;br/&gt;The project will produce the fundamental advances required in the areas of coding and modulation, access protocols, routing, quality of service, operating systems (OS), database systems, security, and performance evaluation, that will be needed to produce this next generation network. These advances will occur as the result of an integrated, collaborative, and multidisciplinary effort spanning a wide range of disciplines in computer science and electrical engineering at the University of Massachusetts.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80119</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80119</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n181" target="n1565">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Infrastructure to Support Research on Mixed Wired/Wireless Information Systems</data>
      <data key="e_abstract">EIA-0080119&lt;br/&gt;Towsley, Donald F.&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Infrastructure to Support Research on Mixed Wired/Wireless Information&lt;br/&gt;&lt;br/&gt;The goal of this project is research and development of control strategies, and services required by application suites executing over mixed wired/wireless networks. The future of networking will introduce a setting where individuals and embedded processes will communicate among themselves and with multimedia information servers over a network made up of diverse network technologies, including ad-hoc and cell-based wireless, and wired segments. This network will seamlessly provide a diverse range of information-based services. This reflects a fundamental shift in the way users will compute and communicate in the future, moving from today&apos;s wire-based network where users are immobile and know where to obtain services to an environment in which users are mobile, may be connected through wireless, perhaps even in an ad-hoc manner, and request and receive services in a transparent manner. &lt;br/&gt;&lt;br/&gt;The project will produce the fundamental advances required in the areas of coding and modulation, access protocols, routing, quality of service, operating systems (OS), database systems, security, and performance evaluation, that will be needed to produce this next generation network. These advances will occur as the result of an integrated, collaborative, and multidisciplinary effort spanning a wide range of disciplines in computer science and electrical engineering at the University of Massachusetts.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80119</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80119</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n181" target="n1566">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Infrastructure to Support Research on Mixed Wired/Wireless Information Systems</data>
      <data key="e_abstract">EIA-0080119&lt;br/&gt;Towsley, Donald F.&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Infrastructure to Support Research on Mixed Wired/Wireless Information&lt;br/&gt;&lt;br/&gt;The goal of this project is research and development of control strategies, and services required by application suites executing over mixed wired/wireless networks. The future of networking will introduce a setting where individuals and embedded processes will communicate among themselves and with multimedia information servers over a network made up of diverse network technologies, including ad-hoc and cell-based wireless, and wired segments. This network will seamlessly provide a diverse range of information-based services. This reflects a fundamental shift in the way users will compute and communicate in the future, moving from today&apos;s wire-based network where users are immobile and know where to obtain services to an environment in which users are mobile, may be connected through wireless, perhaps even in an ad-hoc manner, and request and receive services in a transparent manner. &lt;br/&gt;&lt;br/&gt;The project will produce the fundamental advances required in the areas of coding and modulation, access protocols, routing, quality of service, operating systems (OS), database systems, security, and performance evaluation, that will be needed to produce this next generation network. These advances will occur as the result of an integrated, collaborative, and multidisciplinary effort spanning a wide range of disciplines in computer science and electrical engineering at the University of Massachusetts.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80119</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80119</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n296" target="n1565">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Infrastructure to Support Research on Mixed Wired/Wireless Information Systems</data>
      <data key="e_abstract">EIA-0080119&lt;br/&gt;Towsley, Donald F.&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Infrastructure to Support Research on Mixed Wired/Wireless Information&lt;br/&gt;&lt;br/&gt;The goal of this project is research and development of control strategies, and services required by application suites executing over mixed wired/wireless networks. The future of networking will introduce a setting where individuals and embedded processes will communicate among themselves and with multimedia information servers over a network made up of diverse network technologies, including ad-hoc and cell-based wireless, and wired segments. This network will seamlessly provide a diverse range of information-based services. This reflects a fundamental shift in the way users will compute and communicate in the future, moving from today&apos;s wire-based network where users are immobile and know where to obtain services to an environment in which users are mobile, may be connected through wireless, perhaps even in an ad-hoc manner, and request and receive services in a transparent manner. &lt;br/&gt;&lt;br/&gt;The project will produce the fundamental advances required in the areas of coding and modulation, access protocols, routing, quality of service, operating systems (OS), database systems, security, and performance evaluation, that will be needed to produce this next generation network. These advances will occur as the result of an integrated, collaborative, and multidisciplinary effort spanning a wide range of disciplines in computer science and electrical engineering at the University of Massachusetts.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80119</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80119</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n296" target="n1566">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Infrastructure to Support Research on Mixed Wired/Wireless Information Systems</data>
      <data key="e_abstract">EIA-0080119&lt;br/&gt;Towsley, Donald F.&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Infrastructure to Support Research on Mixed Wired/Wireless Information&lt;br/&gt;&lt;br/&gt;The goal of this project is research and development of control strategies, and services required by application suites executing over mixed wired/wireless networks. The future of networking will introduce a setting where individuals and embedded processes will communicate among themselves and with multimedia information servers over a network made up of diverse network technologies, including ad-hoc and cell-based wireless, and wired segments. This network will seamlessly provide a diverse range of information-based services. This reflects a fundamental shift in the way users will compute and communicate in the future, moving from today&apos;s wire-based network where users are immobile and know where to obtain services to an environment in which users are mobile, may be connected through wireless, perhaps even in an ad-hoc manner, and request and receive services in a transparent manner. &lt;br/&gt;&lt;br/&gt;The project will produce the fundamental advances required in the areas of coding and modulation, access protocols, routing, quality of service, operating systems (OS), database systems, security, and performance evaluation, that will be needed to produce this next generation network. These advances will occur as the result of an integrated, collaborative, and multidisciplinary effort spanning a wide range of disciplines in computer science and electrical engineering at the University of Massachusetts.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80119</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80119</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1565" target="n1566">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Research Infrastructure: Infrastructure to Support Research on Mixed Wired/Wireless Information Systems</data>
      <data key="e_abstract">EIA-0080119&lt;br/&gt;Towsley, Donald F.&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;CISE Research Infrastructure: Infrastructure to Support Research on Mixed Wired/Wireless Information&lt;br/&gt;&lt;br/&gt;The goal of this project is research and development of control strategies, and services required by application suites executing over mixed wired/wireless networks. The future of networking will introduce a setting where individuals and embedded processes will communicate among themselves and with multimedia information servers over a network made up of diverse network technologies, including ad-hoc and cell-based wireless, and wired segments. This network will seamlessly provide a diverse range of information-based services. This reflects a fundamental shift in the way users will compute and communicate in the future, moving from today&apos;s wire-based network where users are immobile and know where to obtain services to an environment in which users are mobile, may be connected through wireless, perhaps even in an ad-hoc manner, and request and receive services in a transparent manner. &lt;br/&gt;&lt;br/&gt;The project will produce the fundamental advances required in the areas of coding and modulation, access protocols, routing, quality of service, operating systems (OS), database systems, security, and performance evaluation, that will be needed to produce this next generation network. These advances will occur as the result of an integrated, collaborative, and multidisciplinary effort spanning a wide range of disciplines in computer science and electrical engineering at the University of Massachusetts.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">80119</data>
      <data key="e_expirationDate">2006-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">80119</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1567" target="n1568">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: A Memory Intensive Compilation Environment Targeting VLIW and DSP Architectures</data>
      <data key="e_abstract">EIA-0079617&lt;br/&gt;Meleis, Waleed M.&lt;br/&gt;Northeastern University&lt;br/&gt;&lt;br/&gt;MRI: A Memory Intensive Compilation Environment Targeting VLIW and DSP Architectures&lt;br/&gt;&lt;br/&gt;This proposal is to acquire Linux/Unix TRU64 and Linux/X86 compute servers to enable the support of memory intensive compiler research at the Electrical and Computer Engineering Department of Northeastern University. The research projects that will use the requested compute servers include Optimization-centered code restructuring and scheduling, memory coloring and compaction targeting DSPs, and dynamic profiling, compilation and evaluation environments.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79617</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">79617</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1567" target="n1569">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: A Memory Intensive Compilation Environment Targeting VLIW and DSP Architectures</data>
      <data key="e_abstract">EIA-0079617&lt;br/&gt;Meleis, Waleed M.&lt;br/&gt;Northeastern University&lt;br/&gt;&lt;br/&gt;MRI: A Memory Intensive Compilation Environment Targeting VLIW and DSP Architectures&lt;br/&gt;&lt;br/&gt;This proposal is to acquire Linux/Unix TRU64 and Linux/X86 compute servers to enable the support of memory intensive compiler research at the Electrical and Computer Engineering Department of Northeastern University. The research projects that will use the requested compute servers include Optimization-centered code restructuring and scheduling, memory coloring and compaction targeting DSPs, and dynamic profiling, compilation and evaluation environments.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79617</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">79617</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1568" target="n1569">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: A Memory Intensive Compilation Environment Targeting VLIW and DSP Architectures</data>
      <data key="e_abstract">EIA-0079617&lt;br/&gt;Meleis, Waleed M.&lt;br/&gt;Northeastern University&lt;br/&gt;&lt;br/&gt;MRI: A Memory Intensive Compilation Environment Targeting VLIW and DSP Architectures&lt;br/&gt;&lt;br/&gt;This proposal is to acquire Linux/Unix TRU64 and Linux/X86 compute servers to enable the support of memory intensive compiler research at the Electrical and Computer Engineering Department of Northeastern University. The research projects that will use the requested compute servers include Optimization-centered code restructuring and scheduling, memory coloring and compaction targeting DSPs, and dynamic profiling, compilation and evaluation environments.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79617</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">79617</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1570" target="n1571">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: E-business and Business Telecommunications Center</data>
      <data key="e_abstract">EIA-0082001&lt;br/&gt;Shim, Jung&lt;br/&gt;Mississippi State University&lt;br/&gt;&lt;br/&gt;ITR: E-business and Business Telecommunications Center&lt;br/&gt;&lt;br/&gt;This proposed research is for planning the development and expansion of&lt;br/&gt;research, training, and education in electronic business (e-business) and&lt;br/&gt;telecommunications of an information systems program. Collaborative&lt;br/&gt;research, enhanced course offerings, equipment, and industry partnerships&lt;br/&gt;would result in new methods for educating citizens in information&lt;br/&gt;technology (IT), expand the supply of entrants into IT professional jobs,&lt;br/&gt;and increase the breadth and depth of existing computer science,&lt;br/&gt;information systems, and marketing programs. Research in&lt;br/&gt;telecommunications would be expanded to involve experimentation with remote&lt;br/&gt;teleconferencing systems to conduct IT work. Other foci include anywhere,&lt;br/&gt;anytime training over the Internet and experimentation with the&lt;br/&gt;effectiveness of video distance training methodologies. The overall goal&lt;br/&gt;would be to expand education and training efforts to form a regional center&lt;br/&gt;for the production of bachelor degree students with concentrations in&lt;br/&gt;e-commerce and telecommunications along with technical Internet-oriented&lt;br/&gt;coursework. In addition, faculty and graduate students would have&lt;br/&gt;opportunities to extend their training through telecommunications and&lt;br/&gt;e-business oriented training workshops.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82001</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82001</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1570" target="n1572">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: E-business and Business Telecommunications Center</data>
      <data key="e_abstract">EIA-0082001&lt;br/&gt;Shim, Jung&lt;br/&gt;Mississippi State University&lt;br/&gt;&lt;br/&gt;ITR: E-business and Business Telecommunications Center&lt;br/&gt;&lt;br/&gt;This proposed research is for planning the development and expansion of&lt;br/&gt;research, training, and education in electronic business (e-business) and&lt;br/&gt;telecommunications of an information systems program. Collaborative&lt;br/&gt;research, enhanced course offerings, equipment, and industry partnerships&lt;br/&gt;would result in new methods for educating citizens in information&lt;br/&gt;technology (IT), expand the supply of entrants into IT professional jobs,&lt;br/&gt;and increase the breadth and depth of existing computer science,&lt;br/&gt;information systems, and marketing programs. Research in&lt;br/&gt;telecommunications would be expanded to involve experimentation with remote&lt;br/&gt;teleconferencing systems to conduct IT work. Other foci include anywhere,&lt;br/&gt;anytime training over the Internet and experimentation with the&lt;br/&gt;effectiveness of video distance training methodologies. The overall goal&lt;br/&gt;would be to expand education and training efforts to form a regional center&lt;br/&gt;for the production of bachelor degree students with concentrations in&lt;br/&gt;e-commerce and telecommunications along with technical Internet-oriented&lt;br/&gt;coursework. In addition, faculty and graduate students would have&lt;br/&gt;opportunities to extend their training through telecommunications and&lt;br/&gt;e-business oriented training workshops.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82001</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82001</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1571" target="n1572">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: E-business and Business Telecommunications Center</data>
      <data key="e_abstract">EIA-0082001&lt;br/&gt;Shim, Jung&lt;br/&gt;Mississippi State University&lt;br/&gt;&lt;br/&gt;ITR: E-business and Business Telecommunications Center&lt;br/&gt;&lt;br/&gt;This proposed research is for planning the development and expansion of&lt;br/&gt;research, training, and education in electronic business (e-business) and&lt;br/&gt;telecommunications of an information systems program. Collaborative&lt;br/&gt;research, enhanced course offerings, equipment, and industry partnerships&lt;br/&gt;would result in new methods for educating citizens in information&lt;br/&gt;technology (IT), expand the supply of entrants into IT professional jobs,&lt;br/&gt;and increase the breadth and depth of existing computer science,&lt;br/&gt;information systems, and marketing programs. Research in&lt;br/&gt;telecommunications would be expanded to involve experimentation with remote&lt;br/&gt;teleconferencing systems to conduct IT work. Other foci include anywhere,&lt;br/&gt;anytime training over the Internet and experimentation with the&lt;br/&gt;effectiveness of video distance training methodologies. The overall goal&lt;br/&gt;would be to expand education and training efforts to form a regional center&lt;br/&gt;for the production of bachelor degree students with concentrations in&lt;br/&gt;e-commerce and telecommunications along with technical Internet-oriented&lt;br/&gt;coursework. In addition, faculty and graduate students would have&lt;br/&gt;opportunities to extend their training through telecommunications and&lt;br/&gt;e-business oriented training workshops.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82001</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82001</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n980" target="n1573">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">A General and Powerful Method for Program Optimization</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">2876</data>
      <data key="e_label">196148</data>
      <data key="e_expirationDate">2001-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">196148</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1575" target="n1576">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Acquisition of an Eight-Processor Sun 3500 Parallel Computer</data>
      <data key="e_abstract">EIA-0079466&lt;br/&gt;Richards, Bradley&lt;br/&gt;Vassar College&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of an Eight-Processor Sun 3500 Parallel Computer&lt;br/&gt;&lt;br/&gt;This proposal is to improve the scientific and pedagogical computational capabilities at Vassar College. Also, it is proposed to purchase an eight-processor Sun Enterprise 3500 shared-memory parallel computer with 4 gigabytes of RAM and 72 gigabytes of disk space. This machine will assist research groups focusing on the following problems: (i) parallel computing with the goal of improving the performance of parallel Java programs, (ii) hydrodynamics simulations of stellar interactions, and (iii) simulations to investigate reactions between BaCeO3 and water vapor. In addition to supporting faculty research, the new machine will also be invaluable for fostering and expanding student research and training.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79466</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">79466</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1575" target="n1577">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Acquisition of an Eight-Processor Sun 3500 Parallel Computer</data>
      <data key="e_abstract">EIA-0079466&lt;br/&gt;Richards, Bradley&lt;br/&gt;Vassar College&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of an Eight-Processor Sun 3500 Parallel Computer&lt;br/&gt;&lt;br/&gt;This proposal is to improve the scientific and pedagogical computational capabilities at Vassar College. Also, it is proposed to purchase an eight-processor Sun Enterprise 3500 shared-memory parallel computer with 4 gigabytes of RAM and 72 gigabytes of disk space. This machine will assist research groups focusing on the following problems: (i) parallel computing with the goal of improving the performance of parallel Java programs, (ii) hydrodynamics simulations of stellar interactions, and (iii) simulations to investigate reactions between BaCeO3 and water vapor. In addition to supporting faculty research, the new machine will also be invaluable for fostering and expanding student research and training.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79466</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">79466</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1576" target="n1577">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">MRI: Acquisition of an Eight-Processor Sun 3500 Parallel Computer</data>
      <data key="e_abstract">EIA-0079466&lt;br/&gt;Richards, Bradley&lt;br/&gt;Vassar College&lt;br/&gt;&lt;br/&gt;MRI: Acquisition of an Eight-Processor Sun 3500 Parallel Computer&lt;br/&gt;&lt;br/&gt;This proposal is to improve the scientific and pedagogical computational capabilities at Vassar College. Also, it is proposed to purchase an eight-processor Sun Enterprise 3500 shared-memory parallel computer with 4 gigabytes of RAM and 72 gigabytes of disk space. This machine will assist research groups focusing on the following problems: (i) parallel computing with the goal of improving the performance of parallel Java programs, (ii) hydrodynamics simulations of stellar interactions, and (iii) simulations to investigate reactions between BaCeO3 and water vapor. In addition to supporting faculty research, the new machine will also be invaluable for fostering and expanding student research and training.</data>
      <data key="e_pgm">1189</data>
      <data key="e_label">79466</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">79466</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1578" target="n1579">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Predictors of Women&apos;s Interest and Retention in Undergraduate IT Majors</data>
      <data key="e_abstract">Institution: University of Wisconsin - Parkside&lt;br/&gt;Proposal Number: EIA 0089957&lt;br/&gt;PI: Sylvia Beyer&lt;br/&gt;Title: Predictors of Women&apos;s Interest and Retention in Undergraduate IT Majors&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to study the barriers that discourage women from taking courses in IT and the causes of their low retention in IT. The project will consist of three studies. Study 1 uses a test-retest experiment to assess the impact of female IT role models on stereotypes and level of knowledge about IT. Study 2 is longitudinal; undeclared students&apos; course taking patterns in IT will be followed from their first semester to the end of their third year. Study 3 is also longitudinal and examines predictors of undergraduate women&apos;s retention in IT. This research will look at interest in IT along a continuum ranging from complete lack of interest, to lukewarm interest (taking a general level course which does not carry credit toward the major), to more serious interest (taking an introductory course designed for potential majors), to committed interest (declaring the major). Likewise, the study of attrition will look not only at female IT majors who drop out of IT but also at women who leave the IT pipeline after taking only one or two courses. This project has the potential to provide significant insights about the recruitment and retention of women in IT majors.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">89957</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">89957</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1581" target="n1582">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Departmental Factors in Gendered Attrition from Undergraduate IT Majors</data>
      <data key="e_abstract">Institution: University of Virginia&lt;br/&gt;Proposal Number: EIA 0089959&lt;br/&gt;PI: Joanne Cohoon&lt;br/&gt;Title: Departmental Factors in Gendered Attrition from Undergraduate IT Majors&lt;br/&gt; &lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to address the disproportionate loss of women from undergraduate programs in computer science. The objectives are to profile the top-producing U.S. computer science departments, to identify the departmental characteristics and practices that affect equal retention of male and female computer science majors, and to disseminate this information and make recommendations for action based on the study results. One component of the project will be a survey of chairpersons and faculty from 227 computer science departments. Fifteen of these departments will be randomly selected for on-site interviews of chairpersons, faculty, and students. This project has the potential to provide significant insights about what kind of educational environment is likely to retain female students in computer science at rates comparable to male students.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">89959</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">89959</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1581" target="n1583">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Departmental Factors in Gendered Attrition from Undergraduate IT Majors</data>
      <data key="e_abstract">Institution: University of Virginia&lt;br/&gt;Proposal Number: EIA 0089959&lt;br/&gt;PI: Joanne Cohoon&lt;br/&gt;Title: Departmental Factors in Gendered Attrition from Undergraduate IT Majors&lt;br/&gt; &lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to address the disproportionate loss of women from undergraduate programs in computer science. The objectives are to profile the top-producing U.S. computer science departments, to identify the departmental characteristics and practices that affect equal retention of male and female computer science majors, and to disseminate this information and make recommendations for action based on the study results. One component of the project will be a survey of chairpersons and faculty from 227 computer science departments. Fifteen of these departments will be randomly selected for on-site interviews of chairpersons, faculty, and students. This project has the potential to provide significant insights about what kind of educational environment is likely to retain female students in computer science at rates comparable to male students.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">89959</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">89959</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1582" target="n1583">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">ITW: Departmental Factors in Gendered Attrition from Undergraduate IT Majors</data>
      <data key="e_abstract">Institution: University of Virginia&lt;br/&gt;Proposal Number: EIA 0089959&lt;br/&gt;PI: Joanne Cohoon&lt;br/&gt;Title: Departmental Factors in Gendered Attrition from Undergraduate IT Majors&lt;br/&gt; &lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to address the disproportionate loss of women from undergraduate programs in computer science. The objectives are to profile the top-producing U.S. computer science departments, to identify the departmental characteristics and practices that affect equal retention of male and female computer science majors, and to disseminate this information and make recommendations for action based on the study results. One component of the project will be a survey of chairpersons and faculty from 227 computer science departments. Fifteen of these departments will be randomly selected for on-site interviews of chairpersons, faculty, and students. This project has the potential to provide significant insights about what kind of educational environment is likely to retain female students in computer science at rates comparable to male students.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">89959</data>
      <data key="e_expirationDate">2005-02-28</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">89959</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n803" target="n1586">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Experimental Partnership - FlexRAM: An Advanced Intelligent Memory System</data>
      <data key="e_abstract">EIA-0072102&lt;br/&gt;Josep Torrellas&lt;br/&gt;Univ. of Illinois-Champaign&lt;br/&gt;&lt;br/&gt;TITLE: Experimental Partnership: FlexRAM: An Advanced Intelligent Memory System&lt;br/&gt;&lt;br/&gt;Dramatic increases in the number of transistors that can be integrated on a single chip have enabled both microprocessor performance and memory chip capacity to rise spectacularly. However, they have also led to an increasingly constraining data transfer bottleneck between processor and memory system. Recognizing the need for new architectural approaches that alleviate this bottleneck, researchers have proposed the integration of processor and DRAM in a single chip. This architecture is popularly known as intelligent memory (IRAM) or processor-in-memory (PIM).&lt;br/&gt;&lt;br/&gt;Unfortunately, simple integration of current microprocessors and DRAM on a single chip often delivers only modest performance improvements-it moves an off-chip bottleneck on chip. Furthermore, the resulting system is often very hard to program. Finally, Little effort has been invested trying to identify a wide range of applications that can exploit this architecture effectively.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">72102</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">72102</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n561" target="n1586">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Experimental Partnership - FlexRAM: An Advanced Intelligent Memory System</data>
      <data key="e_abstract">EIA-0072102&lt;br/&gt;Josep Torrellas&lt;br/&gt;Univ. of Illinois-Champaign&lt;br/&gt;&lt;br/&gt;TITLE: Experimental Partnership: FlexRAM: An Advanced Intelligent Memory System&lt;br/&gt;&lt;br/&gt;Dramatic increases in the number of transistors that can be integrated on a single chip have enabled both microprocessor performance and memory chip capacity to rise spectacularly. However, they have also led to an increasingly constraining data transfer bottleneck between processor and memory system. Recognizing the need for new architectural approaches that alleviate this bottleneck, researchers have proposed the integration of processor and DRAM in a single chip. This architecture is popularly known as intelligent memory (IRAM) or processor-in-memory (PIM).&lt;br/&gt;&lt;br/&gt;Unfortunately, simple integration of current microprocessors and DRAM on a single chip often delivers only modest performance improvements-it moves an off-chip bottleneck on chip. Furthermore, the resulting system is often very hard to program. Finally, Little effort has been invested trying to identify a wide range of applications that can exploit this architecture effectively.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">72102</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">72102</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n561" target="n803">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Experimental Partnership - FlexRAM: An Advanced Intelligent Memory System</data>
      <data key="e_abstract">EIA-0072102&lt;br/&gt;Josep Torrellas&lt;br/&gt;Univ. of Illinois-Champaign&lt;br/&gt;&lt;br/&gt;TITLE: Experimental Partnership: FlexRAM: An Advanced Intelligent Memory System&lt;br/&gt;&lt;br/&gt;Dramatic increases in the number of transistors that can be integrated on a single chip have enabled both microprocessor performance and memory chip capacity to rise spectacularly. However, they have also led to an increasingly constraining data transfer bottleneck between processor and memory system. Recognizing the need for new architectural approaches that alleviate this bottleneck, researchers have proposed the integration of processor and DRAM in a single chip. This architecture is popularly known as intelligent memory (IRAM) or processor-in-memory (PIM).&lt;br/&gt;&lt;br/&gt;Unfortunately, simple integration of current microprocessors and DRAM on a single chip often delivers only modest performance improvements-it moves an off-chip bottleneck on chip. Furthermore, the resulting system is often very hard to program. Finally, Little effort has been invested trying to identify a wide range of applications that can exploit this architecture effectively.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">72102</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">72102</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n921" target="n1593">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Experimental Partnerships: MIT Raw Machine</data>
      <data key="e_abstract">EIA-0071841&lt;br/&gt;Amarasighe, Saman P.&lt;br/&gt;MIT&lt;br/&gt;&lt;br/&gt;CISE Experimental Partnerships: Partnerships: MIT Raw Machine&lt;br/&gt;&lt;br/&gt;Rapidly evolving technology places a billion transistors on a chip within reach of the computer architect. Several approaches to utilizing the large amount of silicon resources have been put forth, with the single important goal of obtaining the most performance out of approximately one square inch of silicon. These approaches exploit more parallelism in one or more instruction streams. Examples include more aggressive superscalars, multiscalars, processor-coupled designs, simultaneous multi-threading processors, multiprocessors on a chip and VLIWs. The goal of this project is to discover, implement and evaluate simple architectural mechanisms that scale with increasing VLSI clock speed, and software techniques that orchestrate high-level computations on the low-level architectural resources for maximum efficiency.</data>
      <data key="e_pgm">4725</data>
      <data key="e_label">71841</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">71841</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n916" target="n1593">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Experimental Partnerships: MIT Raw Machine</data>
      <data key="e_abstract">EIA-0071841&lt;br/&gt;Amarasighe, Saman P.&lt;br/&gt;MIT&lt;br/&gt;&lt;br/&gt;CISE Experimental Partnerships: Partnerships: MIT Raw Machine&lt;br/&gt;&lt;br/&gt;Rapidly evolving technology places a billion transistors on a chip within reach of the computer architect. Several approaches to utilizing the large amount of silicon resources have been put forth, with the single important goal of obtaining the most performance out of approximately one square inch of silicon. These approaches exploit more parallelism in one or more instruction streams. Examples include more aggressive superscalars, multiscalars, processor-coupled designs, simultaneous multi-threading processors, multiprocessors on a chip and VLIWs. The goal of this project is to discover, implement and evaluate simple architectural mechanisms that scale with increasing VLSI clock speed, and software techniques that orchestrate high-level computations on the low-level architectural resources for maximum efficiency.</data>
      <data key="e_pgm">4725</data>
      <data key="e_label">71841</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">71841</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n916" target="n921">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">CISE Experimental Partnerships: MIT Raw Machine</data>
      <data key="e_abstract">EIA-0071841&lt;br/&gt;Amarasighe, Saman P.&lt;br/&gt;MIT&lt;br/&gt;&lt;br/&gt;CISE Experimental Partnerships: Partnerships: MIT Raw Machine&lt;br/&gt;&lt;br/&gt;Rapidly evolving technology places a billion transistors on a chip within reach of the computer architect. Several approaches to utilizing the large amount of silicon resources have been put forth, with the single important goal of obtaining the most performance out of approximately one square inch of silicon. These approaches exploit more parallelism in one or more instruction streams. Examples include more aggressive superscalars, multiscalars, processor-coupled designs, simultaneous multi-threading processors, multiprocessors on a chip and VLIWs. The goal of this project is to discover, implement and evaluate simple architectural mechanisms that scale with increasing VLSI clock speed, and software techniques that orchestrate high-level computations on the low-level architectural resources for maximum efficiency.</data>
      <data key="e_pgm">4725</data>
      <data key="e_label">71841</data>
      <data key="e_expirationDate">2005-08-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">71841</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1598" target="n1599">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Experimental Partnership - Internet Interactive Team Video</data>
      <data key="e_abstract">EIA-0071954&lt;br/&gt;John R. Kender&lt;br/&gt;Columbia University&lt;br/&gt;&lt;br/&gt;TITLE: Experimental Partnership: Adaptive Interactive Team Video&lt;br/&gt;&lt;br/&gt;This project will create a collaborative virtual environment for group work and/or group study, in which semantically structured videos concerned with instruction, design, or prior discussion of a team effort are delivered over heterogeneous Internet links to heterogeneous platforms in an efficient and adaptive manner. The proposed system will enable: the measurement of the effectiveness of semantic summarization and indexing on the access and use of video resources, the measurement of the effectiveness of semantic summarization and indexing on the access and use of video resources, the measurement of the quality of service effects which heuristic forecasting and monitoring of team work actions will have on video delivery, and the measurement of the degree of success of system resource management algorithms for the perfecting and progressive refining of video segments into the caches of clients with varying capabilities from a server cluster subject to resource contention under varying loads. Additionally, this project will provide an efficient distributed environment useful for collaborative and educational purpose in its own right-one that will be tested in the PI&apos;s own courses.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">71954</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">71954</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1598" target="n1600">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Experimental Partnership - Internet Interactive Team Video</data>
      <data key="e_abstract">EIA-0071954&lt;br/&gt;John R. Kender&lt;br/&gt;Columbia University&lt;br/&gt;&lt;br/&gt;TITLE: Experimental Partnership: Adaptive Interactive Team Video&lt;br/&gt;&lt;br/&gt;This project will create a collaborative virtual environment for group work and/or group study, in which semantically structured videos concerned with instruction, design, or prior discussion of a team effort are delivered over heterogeneous Internet links to heterogeneous platforms in an efficient and adaptive manner. The proposed system will enable: the measurement of the effectiveness of semantic summarization and indexing on the access and use of video resources, the measurement of the effectiveness of semantic summarization and indexing on the access and use of video resources, the measurement of the quality of service effects which heuristic forecasting and monitoring of team work actions will have on video delivery, and the measurement of the degree of success of system resource management algorithms for the perfecting and progressive refining of video segments into the caches of clients with varying capabilities from a server cluster subject to resource contention under varying loads. Additionally, this project will provide an efficient distributed environment useful for collaborative and educational purpose in its own right-one that will be tested in the PI&apos;s own courses.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">71954</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">71954</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1599" target="n1600">
      <data key="e_effectiveDate">2000-09-15</data>
      <data key="e_title">Experimental Partnership - Internet Interactive Team Video</data>
      <data key="e_abstract">EIA-0071954&lt;br/&gt;John R. Kender&lt;br/&gt;Columbia University&lt;br/&gt;&lt;br/&gt;TITLE: Experimental Partnership: Adaptive Interactive Team Video&lt;br/&gt;&lt;br/&gt;This project will create a collaborative virtual environment for group work and/or group study, in which semantically structured videos concerned with instruction, design, or prior discussion of a team effort are delivered over heterogeneous Internet links to heterogeneous platforms in an efficient and adaptive manner. The proposed system will enable: the measurement of the effectiveness of semantic summarization and indexing on the access and use of video resources, the measurement of the effectiveness of semantic summarization and indexing on the access and use of video resources, the measurement of the quality of service effects which heuristic forecasting and monitoring of team work actions will have on video delivery, and the measurement of the degree of success of system resource management algorithms for the perfecting and progressive refining of video segments into the caches of clients with varying capabilities from a server cluster subject to resource contention under varying loads. Additionally, this project will provide an efficient distributed environment useful for collaborative and educational purpose in its own right-one that will be tested in the PI&apos;s own courses.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">71954</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">71954</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1601" target="n1602">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Digital Government: Information Technology Accomodation Research: Open a Door to Universal Access</data>
      <data key="e_abstract">EIA-0071126&lt;br/&gt;Nass, Clifford&lt;br/&gt;Stanford University&lt;br/&gt;&lt;br/&gt;Digital Government: Information Technology Accommodation Research: Open a Door to Universal Access&lt;br/&gt;&lt;br/&gt;This proposal describes a collaborative research project involving: Stanford University, Social Security Administration, Census Bureau, and General Services Administration (GSA). The goal of this research is to enable the partner agencies to better accommodate blind and visually impaired computer users, and to permit development of newer, smaller accessor systems for use with the Total Access System, a technology under development at Stanford. The long-term goal of this project is to create enabling technology that supports disabled members of the workforce at the Census Bureau and other Federal agencies and to transfer these technologies to the private sector.&lt;br/&gt;&lt;br/&gt;The research will be conducted over a three-year period. Year one will focus on the research to select and evaluate appropriate technologies and develop a prototype access system for the blind and to document its usability. During year two, the prototype will be improved based on usability test results and the Total Access System technology will be upgraded. Key technologies necessary for development (multi-modal input and miniaturization) will be developed and prototyped. The third year will bring these technologies together in a kiosk prototype.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">71126</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">71126</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1605" target="n1606">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: A New Computational Paradigm: Robustness as a Resource</data>
      <data key="e_abstract">The problem of numerical robustness and geometric consistency is well known&lt;br/&gt;in many areas of computational science. The issue is that inexact computer arith-&lt;br/&gt;metic leads to incorrect and inconsistent geometric conclusions (for example, is a&lt;br/&gt;point inside or outside a triangle). While computers are getting faster, software&lt;br/&gt;is not getting more robust. Indeed, the trend is towards more nonrobustness. We&lt;br/&gt;propose a new computational paradigm to reverse this trend.&lt;br/&gt;&lt;br/&gt;Robustness is often seen as an all-or-nothing proposition. Our new paradigm&lt;br/&gt;consists in viewing robustness as a computational resource, to be traded off against&lt;br/&gt;other resources such as speed. Each program defines a certain robustness-speed&lt;br/&gt;trade-off curve; we want to be able to run the program at any point along this&lt;br/&gt;curve. This proposal will develop the technology to make this capability effcient&lt;br/&gt;and easily accessible to all programmers. As a result, any programmer can produce&lt;br/&gt;nearly ordinary C/C++ code which can be run robustly. The implications of this&lt;br/&gt;paradigm are wide ranging, and will bring the fruits of robustness research into&lt;br/&gt;mainstream computing.&lt;br/&gt;We propose to (1) conduct basic research to support this new computing&lt;br/&gt;paradigm, (2) to create the technology and software tools to achieve this paradigm,&lt;br/&gt;and (3) to explore the applications of fast and usually robust algorithms in algo-&lt;br/&gt;rithm design. For (1), we will focus on effciency issues such as novel root bounds,&lt;br/&gt;incremental computation, guaranteed absolute precision for elementary functions.&lt;br/&gt;For (2), we expect to significantly extend the power, efficiency and usability of our&lt;br/&gt;Core Library and include capabilities such as symbolic perturbation. Finally an&lt;br/&gt;example of (3) concerns the general problem of checking of geometric structures and&lt;br/&gt;their applications in new efficient geometric algorithms.&lt;br/&gt;We propose to apply our robustness techniques and software to two significant&lt;br/&gt;applications in which nonrobustness problems are well-known:&lt;br/&gt;&lt;br/&gt;* Mesh Generation: we will construct the first fully robust mesh generator which&lt;br/&gt;will be deployed in a major ow solver system, Cart3d.&lt;br/&gt;&lt;br/&gt;* Geometric Modeling: we will build a robust geometric modeler which will be&lt;br/&gt;the first such system that is precision-sensitive.&lt;br/&gt;&lt;br/&gt;This proposal involves international collaboration with Professor Mehlhorn&apos;s&lt;br/&gt;Algorithms and Complexity Group at the Max-Planck Institute of Computer Sci-&lt;br/&gt;ence in Germany. Our domestic collaborator are Michael Aftosmis from NASA&lt;br/&gt;Ames Research Center (on mesh generation) and Shankar Krishnan from AT&amp;T&lt;br/&gt;Research Laboratories (on geometric modeling).</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82056</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82056</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1608" target="n1609">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Group Representations and Automatic Generation of Fast Algorithms for Discrete Signal Transforms</data>
      <data key="e_abstract">Proposal Summary&lt;br/&gt;In this research, we propose to use group representation theory to generate automatically fast&lt;br/&gt;algorithms for digital signal processing (DSP) transforms. Group representation theory provides a&lt;br/&gt;deeper understanding of the structure of signal transforms and a context to address fundamental&lt;br/&gt;questions in modeling and processing of signals. We propose to use representation theory to design&lt;br/&gt;new transforms with desirable characteristics.&lt;br/&gt;Our work is at the meta-level of DSP algorithm libraries (DSP-AL), like SPIRAL, [23]. SPI-&lt;br/&gt;RAL is a library of DSP algorithms that concatenates a formula generator block with a code&lt;br/&gt;generator block to produce optimized software implementations for a given computer. SPIRAL&lt;br/&gt;applies iteratively fast algorithms, the algorithmic rules, to generate a rich collection of alternative&lt;br/&gt;equivalent formulas (the formula space) for the same DSP algorithm. For each formula, SPIRAL&lt;br/&gt;then produces automatically optimized code that runs efficiently on the given computer. By&lt;br/&gt;searching over the formula space, SPIRAL generates automatically the formula and correspond-&lt;br/&gt;ing code implementation that matches in an optimized sense the algorithm to the hardware.&lt;br/&gt;What SPIRAL, or any other existing DSP-AL for that matter, does NOT do is the automatic&lt;br/&gt;generation of the fast algorithm, or algorithmic rules. This meta-level is the focus of our proposed&lt;br/&gt;research. We exploit group representation theory to develop the theoretical framework and the&lt;br/&gt;tools that produce automatically these fast algorithms for a number of DSP transforms. We will&lt;br/&gt;implement and interface these tools to a DSP-AL (SPIRAL) which will enable us to translate&lt;br/&gt;directly a fast DSP algorithm as generated by our tools to an efficient low-level language program.&lt;br/&gt;Generating a fast discrete signal transform, given as a matrix, consists of two steps: determin-&lt;br/&gt;ing the &quot;symmetry&quot; of the transform, which is a pair of representations under which the transform&lt;br/&gt;is invariant; decomposing stepwise the representations, giving rise to factorized decomposition ma-&lt;br/&gt;trices, which determine the factorization of the transform. The symmetry catches redundancy in&lt;br/&gt;the transform, and the decomposition of the representations turns the redundancy into a factoriza-&lt;br/&gt;tion of the transform - the fast algorithm. To realize this program, new results on decomposition&lt;br/&gt;matrices will be derived in the context of a constructive extension of standard representation&lt;br/&gt;theory, where representations are manipulated up to equality, not only up to equivalence.&lt;br/&gt;We will implement the algorithm for generating fast discrete signal transforms within a package&lt;br/&gt;for symbolic computation with group representations and structured matrices and interface it with&lt;br/&gt;a DSP-AL, namely SPIRAL.&lt;br/&gt;We consider different types of &quot;symmetry,&quot; going beyond regular representations to include&lt;br/&gt;arbitrary permutation and monomial representations, in order to capture in the representation&lt;br/&gt;framework a wide class of signal transforms. Besides the DFT, and trigonometric transforms, we&lt;br/&gt;will consider other transforms including wavelet transforms.&lt;br/&gt;We use the group representation framework to explore the connection between the &quot;symmetry&quot;&lt;br/&gt;of a signal transform and its properties with respect to signal processing. The use of a transform&lt;br/&gt;can be justified on the basis of the model underlying the data. We have shown this relation to be&lt;br/&gt;connected to the boundary conditions (b.c.) assumed in describing a certain class of models widely&lt;br/&gt;used in applications. These b.c.&apos;s also reflect the type of &quot;data extension&quot; that is hypothesized,&lt;br/&gt;for example, cyclic b.c.&apos;s versus signal periodic extension versus the discrete Fourier transform.&lt;br/&gt;This proposal will exploit the relations between the &quot;symmetry&quot; of the representation, the signal&lt;br/&gt;transform, and the signal models, enabling us to address some fundamental questions, namely&lt;br/&gt;how to design a signal transform which is adapted to a given signal model (i.e., reflects a desired&lt;br/&gt;symmetry) and is computationally the most efficient.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">9.9883e+06</data>
      <data key="e_expirationDate">2003-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.9883e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n377" target="n1615">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Static Checking in an Extended Type System</data>
      <data key="e_abstract">CCR-0082560&lt;br/&gt;Static Checking in an Extended Type System&lt;br/&gt;&lt;br/&gt;PIs: Natarajan Shankar and Sam Owre&lt;br/&gt;&lt;br/&gt;Abstract: &lt;br/&gt;A safe programming language is one whose type system can, at compile time,&lt;br/&gt;detect potential runtime errors such as null dereferences, out-of-bounds&lt;br/&gt;array indices, division by zero, and inapplicable method invocations. Few&lt;br/&gt;widely used programming languages are safe in this sense. Specification&lt;br/&gt;languages like PVS, however, contain safety features such as predicate&lt;br/&gt;subtypes and dependent types that can be used to ensure the absence of&lt;br/&gt;runtime errors. The design of safe programming languages requires an &lt;br/&gt;integration of specification and programming languages through the use &lt;br/&gt;of enriched type systems. These types increase the expressiveness&lt;br/&gt;and naturalness of both executable descriptions (programs) and&lt;br/&gt;non-executable descriptions (mathematical specifications).&lt;br/&gt;&lt;br/&gt;We extend the type systems for widely used languages, such as Java, with&lt;br/&gt;PVS-like specification constructs. We develop an effective static&lt;br/&gt;typechecker for this type system that detects many common programming&lt;br/&gt;errors. The research builds on advances in programming languages, type&lt;br/&gt;theories, program optimization techniques, decision procedures, and&lt;br/&gt;program analysis methods, and tools such as LCLint, ESC, and BANE. An&lt;br/&gt;extended type system for programming languages can be a foundation for the&lt;br/&gt;design and development of well-specified, efficient, and safe programs.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82560</data>
      <data key="e_expirationDate">2002-08-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">82560</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1617" target="n1618">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Contributions of Eye Movements and Shared Attention to Collaborative Tasks</data>
      <data key="e_abstract">During remote collaboration, partners have access to much less information than during face-to-face collaboration. This project uses psychological experiments to examine how knowing where a partner is looking affects performance and strategies on collaborative tasks. Phase 1 asks: When partners are physically co-present, how aware are they of where the other is attending, and how do they achieve shared attention? Phase II applies these basic results to remote collaborations; partners wear eye-trackers that transmit gaze information to each other&apos;s computer displays. A space of tasks and representations is explored: Tasks are varied in systematic ways (e.g., some lend themselves to parallel activity, while others require consensus for each step), and different representations of the same gaze information are compared. The goal is to understand which representations work best for which tasks. With technological advances making eyetracking easier, less cumbersome, and more affordable, a gaze-based computer interface may someday join the ranks of ubiquitous input devices like the mouse. If this technology is to be integrated into the &quot;every citizen interface,&quot; it is necessary to understand how people use the information in gaze to achieve a joint focus of attention. This could provide the foundations for new technology for computer-mediated collaboration.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82602</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82602</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1617" target="n1619">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Contributions of Eye Movements and Shared Attention to Collaborative Tasks</data>
      <data key="e_abstract">During remote collaboration, partners have access to much less information than during face-to-face collaboration. This project uses psychological experiments to examine how knowing where a partner is looking affects performance and strategies on collaborative tasks. Phase 1 asks: When partners are physically co-present, how aware are they of where the other is attending, and how do they achieve shared attention? Phase II applies these basic results to remote collaborations; partners wear eye-trackers that transmit gaze information to each other&apos;s computer displays. A space of tasks and representations is explored: Tasks are varied in systematic ways (e.g., some lend themselves to parallel activity, while others require consensus for each step), and different representations of the same gaze information are compared. The goal is to understand which representations work best for which tasks. With technological advances making eyetracking easier, less cumbersome, and more affordable, a gaze-based computer interface may someday join the ranks of ubiquitous input devices like the mouse. If this technology is to be integrated into the &quot;every citizen interface,&quot; it is necessary to understand how people use the information in gaze to achieve a joint focus of attention. This could provide the foundations for new technology for computer-mediated collaboration.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82602</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82602</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1618" target="n1619">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: Contributions of Eye Movements and Shared Attention to Collaborative Tasks</data>
      <data key="e_abstract">During remote collaboration, partners have access to much less information than during face-to-face collaboration. This project uses psychological experiments to examine how knowing where a partner is looking affects performance and strategies on collaborative tasks. Phase 1 asks: When partners are physically co-present, how aware are they of where the other is attending, and how do they achieve shared attention? Phase II applies these basic results to remote collaborations; partners wear eye-trackers that transmit gaze information to each other&apos;s computer displays. A space of tasks and representations is explored: Tasks are varied in systematic ways (e.g., some lend themselves to parallel activity, while others require consensus for each step), and different representations of the same gaze information are compared. The goal is to understand which representations work best for which tasks. With technological advances making eyetracking easier, less cumbersome, and more affordable, a gaze-based computer interface may someday join the ranks of ubiquitous input devices like the mouse. If this technology is to be integrated into the &quot;every citizen interface,&quot; it is necessary to understand how people use the information in gaze to achieve a joint focus of attention. This could provide the foundations for new technology for computer-mediated collaboration.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82602</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82602</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1620" target="n1621">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">ITR: A Data Model for Multiresolution Scientific Data</data>
      <data key="e_abstract">Interactive exploration effectively enables scientists to identify and interpret data, embedded in much larger datasets, that represents significant underlying phenomena. There is a critical need for data management support for the process of interactive exploration of very large data sets. However, scientific database research has not yet had a major impact on the way that scientists actually use scientific data. The lack of a comprehensive conceptual model for the scientific data has limited the success of current systems in becoming more general and less ad hoc. &lt;br/&gt;&lt;br/&gt;The principal goal of this research is to develop, validate, and prototype a formal data model for distributed multi-resolution scientific data that encapsulates its inherent structure to guide efficient database implementation. The model includes comprehensive geometry and topology-based features for describing a wide variety of sampling grids, features for representing sub-domains of a dataset that contain discovered knowledge, and error measures that reflect the accuracy or authenticity of each representation level. From a repository containing a dataset represented as a multi-resolution hierarchy, a scientist accesses successive levels of error-annotated detail to zoom into the meaningful areas, downloading data from these regions to a LAN and his/her workstation for further</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">82577</data>
      <data key="e_expirationDate">2005-01-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82577</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1624" target="n1625">
      <data key="e_effectiveDate">2000-09-01</data>
      <data key="e_title">Computational Markets for Decentralization of Complex Time-Dependent Activities</data>
      <data key="e_abstract">This research project investigates the design and analysis of market-based mechanisms for decentralized scheduling. Market price systems are well known to effectively decentralize decisions in idealized situations, but real scheduling problems present constraints and interdependencies that can defeat simple market schemes. In this project, market failures arising from the inability to couple interdependent resources are addressed through more sophisticated market structures that allow traders to express complex contingencies. The resulting system will be analyzed using game-theoretic and evolutionary techniques. Results from this work will lead to novel practical scheduling mechanisms that can be deployed within e-commerce and other distributed contexts. New design and analysis methodologies will demonstrate how to extend the resulting mechanisms to other classes of decentralized allocation problems.</data>
      <data key="e_pgm">6850</data>
      <data key="e_label">9.98872e+06</data>
      <data key="e_expirationDate">2004-08-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98872e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1629" target="n1630">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">ITW: Changing Perceptions: Agriculture, Education, and Information Technology in Mississippi</data>
      <data key="e_abstract">This CISE Information Technology Workforce (ITW) proposal requests funds to determine if an early intervention program promotes students&apos; interest in pursuing a career in IT. More specifically, this project will focus on the factors affecting students&apos; decisions to enter the IT workforce in Mississippi&apos;s agricultural industry. The target population will be students, especially underrepresented groups and females, enrolled in a new curriculum in Agriculture and Environmental Science and Technology (AEST). The schools using the AEST curriculum are primarily small, rural, comprehensive high schools and have an average of 45% minority student population. AEST program schools will be matched with non-AEST schools with similar demographics. This project has the potential to provide valuable insight about the impact of a culturally relevant intervention program on the interest of women and underrepresented groups in pursuing IT careers.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">89970</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">89970</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1629" target="n1631">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">ITW: Changing Perceptions: Agriculture, Education, and Information Technology in Mississippi</data>
      <data key="e_abstract">This CISE Information Technology Workforce (ITW) proposal requests funds to determine if an early intervention program promotes students&apos; interest in pursuing a career in IT. More specifically, this project will focus on the factors affecting students&apos; decisions to enter the IT workforce in Mississippi&apos;s agricultural industry. The target population will be students, especially underrepresented groups and females, enrolled in a new curriculum in Agriculture and Environmental Science and Technology (AEST). The schools using the AEST curriculum are primarily small, rural, comprehensive high schools and have an average of 45% minority student population. AEST program schools will be matched with non-AEST schools with similar demographics. This project has the potential to provide valuable insight about the impact of a culturally relevant intervention program on the interest of women and underrepresented groups in pursuing IT careers.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">89970</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">89970</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1630" target="n1631">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">ITW: Changing Perceptions: Agriculture, Education, and Information Technology in Mississippi</data>
      <data key="e_abstract">This CISE Information Technology Workforce (ITW) proposal requests funds to determine if an early intervention program promotes students&apos; interest in pursuing a career in IT. More specifically, this project will focus on the factors affecting students&apos; decisions to enter the IT workforce in Mississippi&apos;s agricultural industry. The target population will be students, especially underrepresented groups and females, enrolled in a new curriculum in Agriculture and Environmental Science and Technology (AEST). The schools using the AEST curriculum are primarily small, rural, comprehensive high schools and have an average of 45% minority student population. AEST program schools will be matched with non-AEST schools with similar demographics. This project has the potential to provide valuable insight about the impact of a culturally relevant intervention program on the interest of women and underrepresented groups in pursuing IT careers.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">89970</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">89970</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1635" target="n1636">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">ITW: Retaining Women in Computer Science Programs: The Impact of Pair-Programming</data>
      <data key="e_abstract">This CISE Information Technology Workforce (ITW) proposal requests funds to examine the effects of pair-programming on female students&apos; perceived self-confidence and interest in computer science as well as their retention and achievement in these courses. Approximately, 400 students enrolled in introductory computer science courses will complete a series of programming assignments in pairs (same and mixed gender pairs) or independently. It is hypothesized that women who program in pairs will produce better programs in terms of functionality and readability, report greater confidence in their solutions, enjoy programming more, and have higher retention rates in computer science and related areas than women who program independently. This project has the potential to provide valuable insights into the retention of women in computer science.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">89989</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">89989</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1635" target="n1637">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">ITW: Retaining Women in Computer Science Programs: The Impact of Pair-Programming</data>
      <data key="e_abstract">This CISE Information Technology Workforce (ITW) proposal requests funds to examine the effects of pair-programming on female students&apos; perceived self-confidence and interest in computer science as well as their retention and achievement in these courses. Approximately, 400 students enrolled in introductory computer science courses will complete a series of programming assignments in pairs (same and mixed gender pairs) or independently. It is hypothesized that women who program in pairs will produce better programs in terms of functionality and readability, report greater confidence in their solutions, enjoy programming more, and have higher retention rates in computer science and related areas than women who program independently. This project has the potential to provide valuable insights into the retention of women in computer science.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">89989</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">89989</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1636" target="n1637">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">ITW: Retaining Women in Computer Science Programs: The Impact of Pair-Programming</data>
      <data key="e_abstract">This CISE Information Technology Workforce (ITW) proposal requests funds to examine the effects of pair-programming on female students&apos; perceived self-confidence and interest in computer science as well as their retention and achievement in these courses. Approximately, 400 students enrolled in introductory computer science courses will complete a series of programming assignments in pairs (same and mixed gender pairs) or independently. It is hypothesized that women who program in pairs will produce better programs in terms of functionality and readability, report greater confidence in their solutions, enjoy programming more, and have higher retention rates in computer science and related areas than women who program independently. This project has the potential to provide valuable insights into the retention of women in computer science.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">89989</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">89989</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1638" target="n1639">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Algorithms and Software for Phylogenetic Supertrees</data>
      <data key="e_abstract">0075319&lt;br/&gt;Sanderson and Gusfield&lt;br/&gt; Supertrees are phylogenies (rooted evolutionary trees) assembled from smaller phylogenies that share some but not all taxa (twigs or end branches) in common. Thus, supertrees can make novel statements about relationships of taxa that do not co-occur on any single source tree while still retaining all the hierarchical information from all source trees. As a method of combining existing phylogenetic information, supertrees potentially solve many of the problems incurred by other methods, such as lack of homologous characters, incompatible data types, or combining trees with different taxa. However, despite the potential for constructing large trees and synthesizing phylogenetic relationships from trees based on disparate sources of evidence, supertree construction has mainly been an informal &quot;by hand&quot; operation, done without the use of specially developed software. Furthermore, there is little ground for assessing the reliability of such supertrees.&lt;br/&gt; In a collaboration among biologists and computer scientists, Drs. Sanderson, Gusfield, and colleagues are examining a new class of methods that is somewhat analogous to phylogenetic distance methods but uses a novel distance measure based on &quot;flips&quot; of cells in the matrix representation of the source trees. In addition to generating supertrees, flip-supertree methods will help systematists focus on poorly known or contentious taxa, thereby directing future systematic effort where it is most needed. Accuracy and speed of supertree methods will be tested through simulation studies, and through use of the web-based database of phylogenetic trees in TreeBASE, to determine methods most useful to systematists. The graphically oriented software to be developed will implement various supertree algorithms and offer improved methods to visualize and compare trees that share some, but not all, taxa in common.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">75319</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">75319</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1638" target="n1640">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Algorithms and Software for Phylogenetic Supertrees</data>
      <data key="e_abstract">0075319&lt;br/&gt;Sanderson and Gusfield&lt;br/&gt; Supertrees are phylogenies (rooted evolutionary trees) assembled from smaller phylogenies that share some but not all taxa (twigs or end branches) in common. Thus, supertrees can make novel statements about relationships of taxa that do not co-occur on any single source tree while still retaining all the hierarchical information from all source trees. As a method of combining existing phylogenetic information, supertrees potentially solve many of the problems incurred by other methods, such as lack of homologous characters, incompatible data types, or combining trees with different taxa. However, despite the potential for constructing large trees and synthesizing phylogenetic relationships from trees based on disparate sources of evidence, supertree construction has mainly been an informal &quot;by hand&quot; operation, done without the use of specially developed software. Furthermore, there is little ground for assessing the reliability of such supertrees.&lt;br/&gt; In a collaboration among biologists and computer scientists, Drs. Sanderson, Gusfield, and colleagues are examining a new class of methods that is somewhat analogous to phylogenetic distance methods but uses a novel distance measure based on &quot;flips&quot; of cells in the matrix representation of the source trees. In addition to generating supertrees, flip-supertree methods will help systematists focus on poorly known or contentious taxa, thereby directing future systematic effort where it is most needed. Accuracy and speed of supertree methods will be tested through simulation studies, and through use of the web-based database of phylogenetic trees in TreeBASE, to determine methods most useful to systematists. The graphically oriented software to be developed will implement various supertree algorithms and offer improved methods to visualize and compare trees that share some, but not all, taxa in common.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">75319</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">75319</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1639" target="n1640">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Algorithms and Software for Phylogenetic Supertrees</data>
      <data key="e_abstract">0075319&lt;br/&gt;Sanderson and Gusfield&lt;br/&gt; Supertrees are phylogenies (rooted evolutionary trees) assembled from smaller phylogenies that share some but not all taxa (twigs or end branches) in common. Thus, supertrees can make novel statements about relationships of taxa that do not co-occur on any single source tree while still retaining all the hierarchical information from all source trees. As a method of combining existing phylogenetic information, supertrees potentially solve many of the problems incurred by other methods, such as lack of homologous characters, incompatible data types, or combining trees with different taxa. However, despite the potential for constructing large trees and synthesizing phylogenetic relationships from trees based on disparate sources of evidence, supertree construction has mainly been an informal &quot;by hand&quot; operation, done without the use of specially developed software. Furthermore, there is little ground for assessing the reliability of such supertrees.&lt;br/&gt; In a collaboration among biologists and computer scientists, Drs. Sanderson, Gusfield, and colleagues are examining a new class of methods that is somewhat analogous to phylogenetic distance methods but uses a novel distance measure based on &quot;flips&quot; of cells in the matrix representation of the source trees. In addition to generating supertrees, flip-supertree methods will help systematists focus on poorly known or contentious taxa, thereby directing future systematic effort where it is most needed. Accuracy and speed of supertree methods will be tested through simulation studies, and through use of the web-based database of phylogenetic trees in TreeBASE, to determine methods most useful to systematists. The graphically oriented software to be developed will implement various supertree algorithms and offer improved methods to visualize and compare trees that share some, but not all, taxa in common.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">75319</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">75319</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n31" target="n492">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Collaborative Research: Supporting Effective Access Through User-and Topic-Based Language Models</data>
      <data key="e_abstract">This collaborative project combines the expertise and experience of Professors Bruce Croft and James Allan at the University of Massachusetts in the development and testing of information retrieval models and systems, with that of Professor Nicholas Belkin at Rutgers University in user modeling and user studies in interactive systems. Tools to support information retrieval and filtering have become common, but in many important respects their performance is mediocre. They make mistakes that are obvious and aggravating to users, and relevant documents are usually mixed with many others that are totally unrelated. These problems significantly lower the productivity and effectiveness of people using the tools, whether in education, science, business, or government. This project investigates a new approach to user and topic modeling that has the potential to significantly improve the effectiveness of information access and filtering. This approach is based on recent research on language models for information retrieval. Language models appear to capture the important aspects of user and domain modeling that have been observed in earlier experiments, and retrieval techniques based on document language models have been shown to be very effective. This project combines standard research methodology using TREC data, experimental and observational user studies in laboratory settings, and studies of the impact in operational environments with many users. It will significantly advance understanding of the impact of user- and topic-models on the important problems of information indexing, retrieval, and access.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">9.90702e+06</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90702e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1644" target="n1645">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">CRCD: Security Laboratory</data>
      <data key="e_abstract">0088028&lt;br/&gt;Gunter, Carl&lt;br/&gt;University of Pennsylvania&lt;br/&gt;&lt;br/&gt;CRCD: Security Laboratory&lt;br/&gt;&lt;br/&gt;This project involves the design of this university&apos;s Security Laboratory and security courses that serve as models for security education. The project integrates research in computer and network security into upper level undergraduate and introductory graduate engineering and computer and information science curricula. The project involves the creation of an instructional laboratory where students have access to and experiment with authentication methods, anti-virus software, encryption algorithms and protocols, firewalls and intrusion detection systems, and the development of network security topics in several courses including Introduction to Networks and Security and Computer Network Security. This project speaks to the widespread concern about security issues on intranets and the Internet, including: privacy policy, security design, security risk assessment, authentication techniques, encryption, incident management, and network architectures for security enforcement. In particular, this project focuses on the development of high-confidence software systems and it is also relevant to several areas of industrial and national importance including wireless information technology, IT research, and scalable information infrastructures. The transfer of this material into undergraduate and graduate courses is important for educating practitioners who develop applications and manage facilities that provide seamless availability of networks.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88028</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88028</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1646" target="n1647">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Applications of Morse Theory and Catastrophe Theory to Computer Graphics</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">2865</data>
      <data key="e_label">196226</data>
      <data key="e_expirationDate">2002-07-31</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">196226</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1556" target="n1649">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">CRCD: Curriculum Development and Infrastructure for an Advanced Systems Laboratory</data>
      <data key="e_abstract">087945&lt;br/&gt;Kurose, Jame F.&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;CRCD: Curriculum Development and Infrastructure for an Advanced Systems Laboratory&lt;br/&gt;&lt;br/&gt;This project involves collaboration between two institutions, the University of Massachusetts and Smith College. The project integrates current research results and cutting-edge practices in systems, specifically in the fields of operating systems, distributed systems, real-time systems, multimedia systems, and networking. The project incorporates educational modules that are research-driven, laboratory-based, hands-on, experiment-oriented, for both undergraduate and graduate students, in computing curricula. The new course and curriculum project modules are part of a pool of laboratory projects that are integrated into existing courses and serve as the foundations for two new courses. In addition to funding for module development, this project supports a multi-site Advanced Systems Laboratory (including 75 networked PCs and a half dozen routers - one server and PC cluster at Smith College and the remaining servers and PCs at University of Massachusetts) to serve approximately 400 students per year. The role of the laboratory is to permit students to use hands-on modules to implement, instrument, and experiment with hardware and software that integrate recent research results in the core areas of networking, distributed systems, operating systems, multimedia and real-time systems. A standard template guides module development that includes guidelines for module format and module resource (e.g., tools) requirements. The PIs are supported in this effort by the University of Massachusetts Video Instructional Program staff who have expertise in video-based education.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">87945</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">87945</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n523" target="n1556">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">CRCD: Curriculum Development and Infrastructure for an Advanced Systems Laboratory</data>
      <data key="e_abstract">087945&lt;br/&gt;Kurose, Jame F.&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;CRCD: Curriculum Development and Infrastructure for an Advanced Systems Laboratory&lt;br/&gt;&lt;br/&gt;This project involves collaboration between two institutions, the University of Massachusetts and Smith College. The project integrates current research results and cutting-edge practices in systems, specifically in the fields of operating systems, distributed systems, real-time systems, multimedia systems, and networking. The project incorporates educational modules that are research-driven, laboratory-based, hands-on, experiment-oriented, for both undergraduate and graduate students, in computing curricula. The new course and curriculum project modules are part of a pool of laboratory projects that are integrated into existing courses and serve as the foundations for two new courses. In addition to funding for module development, this project supports a multi-site Advanced Systems Laboratory (including 75 networked PCs and a half dozen routers - one server and PC cluster at Smith College and the remaining servers and PCs at University of Massachusetts) to serve approximately 400 students per year. The role of the laboratory is to permit students to use hands-on modules to implement, instrument, and experiment with hardware and software that integrate recent research results in the core areas of networking, distributed systems, operating systems, multimedia and real-time systems. A standard template guides module development that includes guidelines for module format and module resource (e.g., tools) requirements. The PIs are supported in this effort by the University of Massachusetts Video Instructional Program staff who have expertise in video-based education.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">87945</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">87945</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1556" target="n1565">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">CRCD: Curriculum Development and Infrastructure for an Advanced Systems Laboratory</data>
      <data key="e_abstract">087945&lt;br/&gt;Kurose, Jame F.&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;CRCD: Curriculum Development and Infrastructure for an Advanced Systems Laboratory&lt;br/&gt;&lt;br/&gt;This project involves collaboration between two institutions, the University of Massachusetts and Smith College. The project integrates current research results and cutting-edge practices in systems, specifically in the fields of operating systems, distributed systems, real-time systems, multimedia systems, and networking. The project incorporates educational modules that are research-driven, laboratory-based, hands-on, experiment-oriented, for both undergraduate and graduate students, in computing curricula. The new course and curriculum project modules are part of a pool of laboratory projects that are integrated into existing courses and serve as the foundations for two new courses. In addition to funding for module development, this project supports a multi-site Advanced Systems Laboratory (including 75 networked PCs and a half dozen routers - one server and PC cluster at Smith College and the remaining servers and PCs at University of Massachusetts) to serve approximately 400 students per year. The role of the laboratory is to permit students to use hands-on modules to implement, instrument, and experiment with hardware and software that integrate recent research results in the core areas of networking, distributed systems, operating systems, multimedia and real-time systems. A standard template guides module development that includes guidelines for module format and module resource (e.g., tools) requirements. The PIs are supported in this effort by the University of Massachusetts Video Instructional Program staff who have expertise in video-based education.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">87945</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">87945</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1556" target="n1652">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">CRCD: Curriculum Development and Infrastructure for an Advanced Systems Laboratory</data>
      <data key="e_abstract">087945&lt;br/&gt;Kurose, Jame F.&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;CRCD: Curriculum Development and Infrastructure for an Advanced Systems Laboratory&lt;br/&gt;&lt;br/&gt;This project involves collaboration between two institutions, the University of Massachusetts and Smith College. The project integrates current research results and cutting-edge practices in systems, specifically in the fields of operating systems, distributed systems, real-time systems, multimedia systems, and networking. The project incorporates educational modules that are research-driven, laboratory-based, hands-on, experiment-oriented, for both undergraduate and graduate students, in computing curricula. The new course and curriculum project modules are part of a pool of laboratory projects that are integrated into existing courses and serve as the foundations for two new courses. In addition to funding for module development, this project supports a multi-site Advanced Systems Laboratory (including 75 networked PCs and a half dozen routers - one server and PC cluster at Smith College and the remaining servers and PCs at University of Massachusetts) to serve approximately 400 students per year. The role of the laboratory is to permit students to use hands-on modules to implement, instrument, and experiment with hardware and software that integrate recent research results in the core areas of networking, distributed systems, operating systems, multimedia and real-time systems. A standard template guides module development that includes guidelines for module format and module resource (e.g., tools) requirements. The PIs are supported in this effort by the University of Massachusetts Video Instructional Program staff who have expertise in video-based education.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">87945</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">87945</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n523" target="n1649">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">CRCD: Curriculum Development and Infrastructure for an Advanced Systems Laboratory</data>
      <data key="e_abstract">087945&lt;br/&gt;Kurose, Jame F.&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;CRCD: Curriculum Development and Infrastructure for an Advanced Systems Laboratory&lt;br/&gt;&lt;br/&gt;This project involves collaboration between two institutions, the University of Massachusetts and Smith College. The project integrates current research results and cutting-edge practices in systems, specifically in the fields of operating systems, distributed systems, real-time systems, multimedia systems, and networking. The project incorporates educational modules that are research-driven, laboratory-based, hands-on, experiment-oriented, for both undergraduate and graduate students, in computing curricula. The new course and curriculum project modules are part of a pool of laboratory projects that are integrated into existing courses and serve as the foundations for two new courses. In addition to funding for module development, this project supports a multi-site Advanced Systems Laboratory (including 75 networked PCs and a half dozen routers - one server and PC cluster at Smith College and the remaining servers and PCs at University of Massachusetts) to serve approximately 400 students per year. The role of the laboratory is to permit students to use hands-on modules to implement, instrument, and experiment with hardware and software that integrate recent research results in the core areas of networking, distributed systems, operating systems, multimedia and real-time systems. A standard template guides module development that includes guidelines for module format and module resource (e.g., tools) requirements. The PIs are supported in this effort by the University of Massachusetts Video Instructional Program staff who have expertise in video-based education.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">87945</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">87945</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1565" target="n1649">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">CRCD: Curriculum Development and Infrastructure for an Advanced Systems Laboratory</data>
      <data key="e_abstract">087945&lt;br/&gt;Kurose, Jame F.&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;CRCD: Curriculum Development and Infrastructure for an Advanced Systems Laboratory&lt;br/&gt;&lt;br/&gt;This project involves collaboration between two institutions, the University of Massachusetts and Smith College. The project integrates current research results and cutting-edge practices in systems, specifically in the fields of operating systems, distributed systems, real-time systems, multimedia systems, and networking. The project incorporates educational modules that are research-driven, laboratory-based, hands-on, experiment-oriented, for both undergraduate and graduate students, in computing curricula. The new course and curriculum project modules are part of a pool of laboratory projects that are integrated into existing courses and serve as the foundations for two new courses. In addition to funding for module development, this project supports a multi-site Advanced Systems Laboratory (including 75 networked PCs and a half dozen routers - one server and PC cluster at Smith College and the remaining servers and PCs at University of Massachusetts) to serve approximately 400 students per year. The role of the laboratory is to permit students to use hands-on modules to implement, instrument, and experiment with hardware and software that integrate recent research results in the core areas of networking, distributed systems, operating systems, multimedia and real-time systems. A standard template guides module development that includes guidelines for module format and module resource (e.g., tools) requirements. The PIs are supported in this effort by the University of Massachusetts Video Instructional Program staff who have expertise in video-based education.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">87945</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">87945</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1649" target="n1652">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">CRCD: Curriculum Development and Infrastructure for an Advanced Systems Laboratory</data>
      <data key="e_abstract">087945&lt;br/&gt;Kurose, Jame F.&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;CRCD: Curriculum Development and Infrastructure for an Advanced Systems Laboratory&lt;br/&gt;&lt;br/&gt;This project involves collaboration between two institutions, the University of Massachusetts and Smith College. The project integrates current research results and cutting-edge practices in systems, specifically in the fields of operating systems, distributed systems, real-time systems, multimedia systems, and networking. The project incorporates educational modules that are research-driven, laboratory-based, hands-on, experiment-oriented, for both undergraduate and graduate students, in computing curricula. The new course and curriculum project modules are part of a pool of laboratory projects that are integrated into existing courses and serve as the foundations for two new courses. In addition to funding for module development, this project supports a multi-site Advanced Systems Laboratory (including 75 networked PCs and a half dozen routers - one server and PC cluster at Smith College and the remaining servers and PCs at University of Massachusetts) to serve approximately 400 students per year. The role of the laboratory is to permit students to use hands-on modules to implement, instrument, and experiment with hardware and software that integrate recent research results in the core areas of networking, distributed systems, operating systems, multimedia and real-time systems. A standard template guides module development that includes guidelines for module format and module resource (e.g., tools) requirements. The PIs are supported in this effort by the University of Massachusetts Video Instructional Program staff who have expertise in video-based education.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">87945</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">87945</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n523" target="n1565">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">CRCD: Curriculum Development and Infrastructure for an Advanced Systems Laboratory</data>
      <data key="e_abstract">087945&lt;br/&gt;Kurose, Jame F.&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;CRCD: Curriculum Development and Infrastructure for an Advanced Systems Laboratory&lt;br/&gt;&lt;br/&gt;This project involves collaboration between two institutions, the University of Massachusetts and Smith College. The project integrates current research results and cutting-edge practices in systems, specifically in the fields of operating systems, distributed systems, real-time systems, multimedia systems, and networking. The project incorporates educational modules that are research-driven, laboratory-based, hands-on, experiment-oriented, for both undergraduate and graduate students, in computing curricula. The new course and curriculum project modules are part of a pool of laboratory projects that are integrated into existing courses and serve as the foundations for two new courses. In addition to funding for module development, this project supports a multi-site Advanced Systems Laboratory (including 75 networked PCs and a half dozen routers - one server and PC cluster at Smith College and the remaining servers and PCs at University of Massachusetts) to serve approximately 400 students per year. The role of the laboratory is to permit students to use hands-on modules to implement, instrument, and experiment with hardware and software that integrate recent research results in the core areas of networking, distributed systems, operating systems, multimedia and real-time systems. A standard template guides module development that includes guidelines for module format and module resource (e.g., tools) requirements. The PIs are supported in this effort by the University of Massachusetts Video Instructional Program staff who have expertise in video-based education.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">87945</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">87945</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n523" target="n1652">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">CRCD: Curriculum Development and Infrastructure for an Advanced Systems Laboratory</data>
      <data key="e_abstract">087945&lt;br/&gt;Kurose, Jame F.&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;CRCD: Curriculum Development and Infrastructure for an Advanced Systems Laboratory&lt;br/&gt;&lt;br/&gt;This project involves collaboration between two institutions, the University of Massachusetts and Smith College. The project integrates current research results and cutting-edge practices in systems, specifically in the fields of operating systems, distributed systems, real-time systems, multimedia systems, and networking. The project incorporates educational modules that are research-driven, laboratory-based, hands-on, experiment-oriented, for both undergraduate and graduate students, in computing curricula. The new course and curriculum project modules are part of a pool of laboratory projects that are integrated into existing courses and serve as the foundations for two new courses. In addition to funding for module development, this project supports a multi-site Advanced Systems Laboratory (including 75 networked PCs and a half dozen routers - one server and PC cluster at Smith College and the remaining servers and PCs at University of Massachusetts) to serve approximately 400 students per year. The role of the laboratory is to permit students to use hands-on modules to implement, instrument, and experiment with hardware and software that integrate recent research results in the core areas of networking, distributed systems, operating systems, multimedia and real-time systems. A standard template guides module development that includes guidelines for module format and module resource (e.g., tools) requirements. The PIs are supported in this effort by the University of Massachusetts Video Instructional Program staff who have expertise in video-based education.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">87945</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">87945</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1565" target="n1652">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">CRCD: Curriculum Development and Infrastructure for an Advanced Systems Laboratory</data>
      <data key="e_abstract">087945&lt;br/&gt;Kurose, Jame F.&lt;br/&gt;University of Massachusetts&lt;br/&gt;&lt;br/&gt;CRCD: Curriculum Development and Infrastructure for an Advanced Systems Laboratory&lt;br/&gt;&lt;br/&gt;This project involves collaboration between two institutions, the University of Massachusetts and Smith College. The project integrates current research results and cutting-edge practices in systems, specifically in the fields of operating systems, distributed systems, real-time systems, multimedia systems, and networking. The project incorporates educational modules that are research-driven, laboratory-based, hands-on, experiment-oriented, for both undergraduate and graduate students, in computing curricula. The new course and curriculum project modules are part of a pool of laboratory projects that are integrated into existing courses and serve as the foundations for two new courses. In addition to funding for module development, this project supports a multi-site Advanced Systems Laboratory (including 75 networked PCs and a half dozen routers - one server and PC cluster at Smith College and the remaining servers and PCs at University of Massachusetts) to serve approximately 400 students per year. The role of the laboratory is to permit students to use hands-on modules to implement, instrument, and experiment with hardware and software that integrate recent research results in the core areas of networking, distributed systems, operating systems, multimedia and real-time systems. A standard template guides module development that includes guidelines for module format and module resource (e.g., tools) requirements. The PIs are supported in this effort by the University of Massachusetts Video Instructional Program staff who have expertise in video-based education.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">87945</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">87945</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1655" target="n1656">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">The Development of a Distributed Digital Library of Mathematical Monographs</data>
      <data key="e_abstract">The project will create a distributed repository of significant &lt;br/&gt;historical monographs in mathematics. The participation of these &lt;br/&gt;three institutions is significant not only because of their &lt;br/&gt;pioneering work in building digital libraries, but also because of &lt;br/&gt;their extraordinary collections in this topical area. Michigan will &lt;br/&gt;contribute 1,000 monographic volumes focusing on non-Euclidean &lt;br/&gt;geometry from its collection; Cornell has digitized 576 volumes of &lt;br/&gt;mathematical monographs, and will generate OCR to enhance access to &lt;br/&gt;their materials. The State and University Library Gottingen will &lt;br/&gt;contribute digitized monographs, dissertations and multivolume works &lt;br/&gt;of the electronic Mathematical Archive and the database &quot;Jahrbuch &lt;br/&gt;uber die Forschritte der Mathematik&quot;, funded by the Deutsche &lt;br/&gt;Forschungsgemeinschaft. Its collections are regarded as unparalleled &lt;br/&gt;in this area. These funding requested will be used primarily to &lt;br/&gt;develop an interoperability layer with the three strong digital &lt;br/&gt;library systems at these institutions. In doing so, the participants &lt;br/&gt;will focus on many of the issues central to the advancement of &lt;br/&gt;digital libraries, including distributed repositories and integration &lt;br/&gt;of digital resources, advanced access and retrieval, high levels of &lt;br/&gt;interoperability, and models for dissemination and use.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">85853</data>
      <data key="e_expirationDate">2003-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">85853</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1017" target="n1658">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">NSF-PI Conference 2000</data>
      <data key="e_abstract">The Principle Investigators of this proposal propose to organize a PI meeting for the Networking Research Programs (Networking Research Program and Special Projects in Networking) in the ANIR Division of NSF. The PI meeting will be held over a two-day period in addition to an opening night reception. The reception will be Wednesday evening, November 1, 2000 with two full days of technical sessions, Thursday, November 2, and Friday, November 3. The PI meeting will facilitate the exchange of research ideas among the PIs with active NSF awards from the Networking Research Programs and NSF program directors.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">96740</data>
      <data key="e_expirationDate">2001-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">96740</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1017" target="n1659">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">NSF-PI Conference 2000</data>
      <data key="e_abstract">The Principle Investigators of this proposal propose to organize a PI meeting for the Networking Research Programs (Networking Research Program and Special Projects in Networking) in the ANIR Division of NSF. The PI meeting will be held over a two-day period in addition to an opening night reception. The reception will be Wednesday evening, November 1, 2000 with two full days of technical sessions, Thursday, November 2, and Friday, November 3. The PI meeting will facilitate the exchange of research ideas among the PIs with active NSF awards from the Networking Research Programs and NSF program directors.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">96740</data>
      <data key="e_expirationDate">2001-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">96740</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1017" target="n1660">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">NSF-PI Conference 2000</data>
      <data key="e_abstract">The Principle Investigators of this proposal propose to organize a PI meeting for the Networking Research Programs (Networking Research Program and Special Projects in Networking) in the ANIR Division of NSF. The PI meeting will be held over a two-day period in addition to an opening night reception. The reception will be Wednesday evening, November 1, 2000 with two full days of technical sessions, Thursday, November 2, and Friday, November 3. The PI meeting will facilitate the exchange of research ideas among the PIs with active NSF awards from the Networking Research Programs and NSF program directors.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">96740</data>
      <data key="e_expirationDate">2001-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">96740</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n366" target="n1017">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">NSF-PI Conference 2000</data>
      <data key="e_abstract">The Principle Investigators of this proposal propose to organize a PI meeting for the Networking Research Programs (Networking Research Program and Special Projects in Networking) in the ANIR Division of NSF. The PI meeting will be held over a two-day period in addition to an opening night reception. The reception will be Wednesday evening, November 1, 2000 with two full days of technical sessions, Thursday, November 2, and Friday, November 3. The PI meeting will facilitate the exchange of research ideas among the PIs with active NSF awards from the Networking Research Programs and NSF program directors.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">96740</data>
      <data key="e_expirationDate">2001-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">96740</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1658" target="n1659">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">NSF-PI Conference 2000</data>
      <data key="e_abstract">The Principle Investigators of this proposal propose to organize a PI meeting for the Networking Research Programs (Networking Research Program and Special Projects in Networking) in the ANIR Division of NSF. The PI meeting will be held over a two-day period in addition to an opening night reception. The reception will be Wednesday evening, November 1, 2000 with two full days of technical sessions, Thursday, November 2, and Friday, November 3. The PI meeting will facilitate the exchange of research ideas among the PIs with active NSF awards from the Networking Research Programs and NSF program directors.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">96740</data>
      <data key="e_expirationDate">2001-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">96740</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1658" target="n1660">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">NSF-PI Conference 2000</data>
      <data key="e_abstract">The Principle Investigators of this proposal propose to organize a PI meeting for the Networking Research Programs (Networking Research Program and Special Projects in Networking) in the ANIR Division of NSF. The PI meeting will be held over a two-day period in addition to an opening night reception. The reception will be Wednesday evening, November 1, 2000 with two full days of technical sessions, Thursday, November 2, and Friday, November 3. The PI meeting will facilitate the exchange of research ideas among the PIs with active NSF awards from the Networking Research Programs and NSF program directors.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">96740</data>
      <data key="e_expirationDate">2001-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">96740</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n366" target="n1658">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">NSF-PI Conference 2000</data>
      <data key="e_abstract">The Principle Investigators of this proposal propose to organize a PI meeting for the Networking Research Programs (Networking Research Program and Special Projects in Networking) in the ANIR Division of NSF. The PI meeting will be held over a two-day period in addition to an opening night reception. The reception will be Wednesday evening, November 1, 2000 with two full days of technical sessions, Thursday, November 2, and Friday, November 3. The PI meeting will facilitate the exchange of research ideas among the PIs with active NSF awards from the Networking Research Programs and NSF program directors.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">96740</data>
      <data key="e_expirationDate">2001-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">96740</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1659" target="n1660">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">NSF-PI Conference 2000</data>
      <data key="e_abstract">The Principle Investigators of this proposal propose to organize a PI meeting for the Networking Research Programs (Networking Research Program and Special Projects in Networking) in the ANIR Division of NSF. The PI meeting will be held over a two-day period in addition to an opening night reception. The reception will be Wednesday evening, November 1, 2000 with two full days of technical sessions, Thursday, November 2, and Friday, November 3. The PI meeting will facilitate the exchange of research ideas among the PIs with active NSF awards from the Networking Research Programs and NSF program directors.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">96740</data>
      <data key="e_expirationDate">2001-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">96740</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n366" target="n1659">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">NSF-PI Conference 2000</data>
      <data key="e_abstract">The Principle Investigators of this proposal propose to organize a PI meeting for the Networking Research Programs (Networking Research Program and Special Projects in Networking) in the ANIR Division of NSF. The PI meeting will be held over a two-day period in addition to an opening night reception. The reception will be Wednesday evening, November 1, 2000 with two full days of technical sessions, Thursday, November 2, and Friday, November 3. The PI meeting will facilitate the exchange of research ideas among the PIs with active NSF awards from the Networking Research Programs and NSF program directors.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">96740</data>
      <data key="e_expirationDate">2001-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">96740</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n366" target="n1660">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">NSF-PI Conference 2000</data>
      <data key="e_abstract">The Principle Investigators of this proposal propose to organize a PI meeting for the Networking Research Programs (Networking Research Program and Special Projects in Networking) in the ANIR Division of NSF. The PI meeting will be held over a two-day period in addition to an opening night reception. The reception will be Wednesday evening, November 1, 2000 with two full days of technical sessions, Thursday, November 2, and Friday, November 3. The PI meeting will facilitate the exchange of research ideas among the PIs with active NSF awards from the Networking Research Programs and NSF program directors.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">96740</data>
      <data key="e_expirationDate">2001-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">96740</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1664" target="n1665">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Digital Government: Collecting and Using Geospatial Data in the Field: An Extensible Framework and Testbed</data>
      <data key="e_abstract">EIA-9983289&lt;br/&gt;Nusser, Sarah M&lt;br/&gt;Iowa State University&lt;br/&gt;&lt;br/&gt;Digital Government: Collecting and Using Geospatial Data in the Field: An Extensible Framework and Testbed&lt;br/&gt;&lt;br/&gt;This work will conceive, develop, and test an extensible framework to support the collection and use of geospatial data in the field. Partner Federal agencies include the Bureau of the Census, the US Geological Survey, and several agencies of the US Department of Agriculture.&lt;br/&gt;&lt;br/&gt;The proposed activities are designed to meet five key objectives:&lt;br/&gt;1. Develop a model documenting and formalizing the infrastructure, tools, and key capabilities required to support a flexible and extensible field data collection system.&lt;br/&gt;2. Conduct research on computer science tools and associated information technologies required to fully integrate digital geospatial data into the collection process.&lt;br/&gt;3. Conduct research on infrastructure components that are needed to implement the system in a manner that limits the complexity of the system from the vantagepoint of the user in the field.&lt;br/&gt;4. Investigate emerging field data collection technologies to determine how the usage of geospatial data is transformed by these new interfaces.&lt;br/&gt;5. Explore the framework model and research developments in an application environment by developing prototype components and testbeds that correspond to agency data collection settings.&lt;br/&gt;&lt;br/&gt;Six developments will be needed to address the research objectives:&lt;br/&gt;1. a user-driven framework model,&lt;br/&gt;2. a conceptual framework for conflation of heterogeneous geospatial data for field use,&lt;br/&gt;3. a multi-agent system to support tools required to use and collect geospatial data in the field,&lt;br/&gt;4. interoperable searching and discovery mechanism for prepared, existing, and potentially unknown sources of data,&lt;br/&gt;5. object-oriented warehouse designs for the field data collection environment, and&lt;br/&gt;6. evaluations of emerging field technologies and their impact on user activities.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98329e+06</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98329e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1664" target="n1666">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Digital Government: Collecting and Using Geospatial Data in the Field: An Extensible Framework and Testbed</data>
      <data key="e_abstract">EIA-9983289&lt;br/&gt;Nusser, Sarah M&lt;br/&gt;Iowa State University&lt;br/&gt;&lt;br/&gt;Digital Government: Collecting and Using Geospatial Data in the Field: An Extensible Framework and Testbed&lt;br/&gt;&lt;br/&gt;This work will conceive, develop, and test an extensible framework to support the collection and use of geospatial data in the field. Partner Federal agencies include the Bureau of the Census, the US Geological Survey, and several agencies of the US Department of Agriculture.&lt;br/&gt;&lt;br/&gt;The proposed activities are designed to meet five key objectives:&lt;br/&gt;1. Develop a model documenting and formalizing the infrastructure, tools, and key capabilities required to support a flexible and extensible field data collection system.&lt;br/&gt;2. Conduct research on computer science tools and associated information technologies required to fully integrate digital geospatial data into the collection process.&lt;br/&gt;3. Conduct research on infrastructure components that are needed to implement the system in a manner that limits the complexity of the system from the vantagepoint of the user in the field.&lt;br/&gt;4. Investigate emerging field data collection technologies to determine how the usage of geospatial data is transformed by these new interfaces.&lt;br/&gt;5. Explore the framework model and research developments in an application environment by developing prototype components and testbeds that correspond to agency data collection settings.&lt;br/&gt;&lt;br/&gt;Six developments will be needed to address the research objectives:&lt;br/&gt;1. a user-driven framework model,&lt;br/&gt;2. a conceptual framework for conflation of heterogeneous geospatial data for field use,&lt;br/&gt;3. a multi-agent system to support tools required to use and collect geospatial data in the field,&lt;br/&gt;4. interoperable searching and discovery mechanism for prepared, existing, and potentially unknown sources of data,&lt;br/&gt;5. object-oriented warehouse designs for the field data collection environment, and&lt;br/&gt;6. evaluations of emerging field technologies and their impact on user activities.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98329e+06</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98329e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1665" target="n1666">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Digital Government: Collecting and Using Geospatial Data in the Field: An Extensible Framework and Testbed</data>
      <data key="e_abstract">EIA-9983289&lt;br/&gt;Nusser, Sarah M&lt;br/&gt;Iowa State University&lt;br/&gt;&lt;br/&gt;Digital Government: Collecting and Using Geospatial Data in the Field: An Extensible Framework and Testbed&lt;br/&gt;&lt;br/&gt;This work will conceive, develop, and test an extensible framework to support the collection and use of geospatial data in the field. Partner Federal agencies include the Bureau of the Census, the US Geological Survey, and several agencies of the US Department of Agriculture.&lt;br/&gt;&lt;br/&gt;The proposed activities are designed to meet five key objectives:&lt;br/&gt;1. Develop a model documenting and formalizing the infrastructure, tools, and key capabilities required to support a flexible and extensible field data collection system.&lt;br/&gt;2. Conduct research on computer science tools and associated information technologies required to fully integrate digital geospatial data into the collection process.&lt;br/&gt;3. Conduct research on infrastructure components that are needed to implement the system in a manner that limits the complexity of the system from the vantagepoint of the user in the field.&lt;br/&gt;4. Investigate emerging field data collection technologies to determine how the usage of geospatial data is transformed by these new interfaces.&lt;br/&gt;5. Explore the framework model and research developments in an application environment by developing prototype components and testbeds that correspond to agency data collection settings.&lt;br/&gt;&lt;br/&gt;Six developments will be needed to address the research objectives:&lt;br/&gt;1. a user-driven framework model,&lt;br/&gt;2. a conceptual framework for conflation of heterogeneous geospatial data for field use,&lt;br/&gt;3. a multi-agent system to support tools required to use and collect geospatial data in the field,&lt;br/&gt;4. interoperable searching and discovery mechanism for prepared, existing, and potentially unknown sources of data,&lt;br/&gt;5. object-oriented warehouse designs for the field data collection environment, and&lt;br/&gt;6. evaluations of emerging field technologies and their impact on user activities.</data>
      <data key="e_pgm">1706</data>
      <data key="e_label">9.98329e+06</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.98329e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1668" target="n1669">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">CRCD: Constructive, Inquiry-Based, Multimedia Learning in Computer Science Education</data>
      <data key="e_abstract">0087977&lt;br/&gt;Blank, Glenn D.&lt;br/&gt;Lehigh University&lt;br/&gt;&lt;br/&gt;CRCD: Development of a Web-based Integrated Learning Environment for Manufacturing Engineering Education&lt;br/&gt;&lt;br/&gt;This project combines research in inquiry-based learning with emerging concepts in textual content identification and classification to the development of a multimedia learning system. The project applies a multimedia framework to the teaching of introductory and upper-level computer science courses for students with diverse learning styles, gender, and cultural backgrounds. In particular, the project concentrates on one introductory computer science course and two upper-level undergraduate/introductory graduate level courses, Object-Oriented Software Engineering and Artificial Intelligence with Applications in Textual Data Mining. Applying current research in data mining algorithms and knowledge base creation, a reference-librarian avatar guides learners through course content using a multimedia interface that seamlessly connects learners, networks, knowledge repositories and human instructors.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">87977</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">87977</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1668" target="n1670">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">CRCD: Constructive, Inquiry-Based, Multimedia Learning in Computer Science Education</data>
      <data key="e_abstract">0087977&lt;br/&gt;Blank, Glenn D.&lt;br/&gt;Lehigh University&lt;br/&gt;&lt;br/&gt;CRCD: Development of a Web-based Integrated Learning Environment for Manufacturing Engineering Education&lt;br/&gt;&lt;br/&gt;This project combines research in inquiry-based learning with emerging concepts in textual content identification and classification to the development of a multimedia learning system. The project applies a multimedia framework to the teaching of introductory and upper-level computer science courses for students with diverse learning styles, gender, and cultural backgrounds. In particular, the project concentrates on one introductory computer science course and two upper-level undergraduate/introductory graduate level courses, Object-Oriented Software Engineering and Artificial Intelligence with Applications in Textual Data Mining. Applying current research in data mining algorithms and knowledge base creation, a reference-librarian avatar guides learners through course content using a multimedia interface that seamlessly connects learners, networks, knowledge repositories and human instructors.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">87977</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">87977</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1669" target="n1670">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">CRCD: Constructive, Inquiry-Based, Multimedia Learning in Computer Science Education</data>
      <data key="e_abstract">0087977&lt;br/&gt;Blank, Glenn D.&lt;br/&gt;Lehigh University&lt;br/&gt;&lt;br/&gt;CRCD: Development of a Web-based Integrated Learning Environment for Manufacturing Engineering Education&lt;br/&gt;&lt;br/&gt;This project combines research in inquiry-based learning with emerging concepts in textual content identification and classification to the development of a multimedia learning system. The project applies a multimedia framework to the teaching of introductory and upper-level computer science courses for students with diverse learning styles, gender, and cultural backgrounds. In particular, the project concentrates on one introductory computer science course and two upper-level undergraduate/introductory graduate level courses, Object-Oriented Software Engineering and Artificial Intelligence with Applications in Textual Data Mining. Applying current research in data mining algorithms and knowledge base creation, a reference-librarian avatar guides learners through course content using a multimedia interface that seamlessly connects learners, networks, knowledge repositories and human instructors.</data>
      <data key="e_pgm">1359</data>
      <data key="e_label">87977</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">87977</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n759" target="n1672">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">SGER: Creating Digital Archives of 3D Artworks</data>
      <data key="e_abstract">Proposal : IIS-0087158&lt;br/&gt;Institution: Stanford University&lt;br/&gt;PI: Marc Levoy&lt;br/&gt;Title: Creating Digital Archives of 3D Artwork&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Recent improvements in laser rangefinder technology, together with &lt;br/&gt;algorithms for combining multiple range and color images, allows &lt;br/&gt;accurate digitization of the external shape and surface &lt;br/&gt;characteristics of many physical objects. This capability makes it &lt;br/&gt;possible for the first time to digitize and archive substantial &lt;br/&gt;bodies of three-dimensional artistic and cultural artifacts, such as &lt;br/&gt;statues, buildings, and archeological remains. Although the &lt;br/&gt;methodologies needed to create and manage digital archives of &lt;br/&gt;two-dimensional artifacts have matured substantially in the last ten &lt;br/&gt;years, the jump from two to three dimensions poses new problems. &lt;br/&gt;These are problems of both scale and substance, and they touch on &lt;br/&gt;every aspect of digital archiving: storage, indexing, searching, &lt;br/&gt;distribution, viewing, and piracy protection. This pilot project, &lt;br/&gt;will focus on the following subproblems of this new domain:&lt;br/&gt;&lt;br/&gt;(1) cataloguing 3D artworks using techniques from image-based rendering,&lt;br/&gt;(2) searching catalogues (and the Internet) for 3D models or rendered views&lt;br/&gt;(3) protecting 3D archives against piracy using robust 3D digital watermarking,&lt;br/&gt;(4) efficient streaming of 3D models over networks of limited bandwidth, and&lt;br/&gt; (5) real-time display of large 3D models on low-cost PCs.&lt;br/&gt;&lt;br/&gt;As test data, a 250 gigabyte archive of the sculptures of &lt;br/&gt;Michelangelo and the fragments of the Forma Urbis Romae, a giant &lt;br/&gt;marble map of ancient Rome will be used. The data for this archive &lt;br/&gt;was generated during a year-long digitization effort called the &lt;br/&gt;Digital Michelangelo Project.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">87158</data>
      <data key="e_expirationDate">2002-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">87158</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1676" target="n1677">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">BIOCOMPLEXITY: Scaling of Biodiversity: Physical and Biological Foundations of Ecological Principles</data>
      <data key="e_abstract">0083422&lt;br/&gt;Brown&lt;br/&gt; Underlying the diversity of life and the complexity of ecology is order that reflects the operation of fundamental physical and biological processes. Scaling relationships are emergent quantitative features of biodiversity. Some of them appear to be universal, occurring in virtually all taxa of organisms and kinds of environments. They are patterns of structure or dynamics that are self-similar or fractal-like over many orders of magnitude. They can be described mathematically by power functions. They allow extrapolation and prediction over a wide range of scales., They offer clues to underlying mechanisms that powerfully constrain biodiversity. This research will use the interplay of mathematical models and empirical measurements to elucidate the physical and biological principles that determine how the life history, abundance, distribution, and species richness of organisms scale with body size, space, and time. The program of research and education activities involves: (1) collaborations among physicists, mathematicians, geologists/hydrologists, biologists and ecologists, (2) interactions among scientists from seven institutions, (3) cooperation between the University of New Mexico, the Santa Fe Institute, and Los Alamos National Laboratory, and (4) interdisciplinary training for graduate students and postdoctoral fellows.</data>
      <data key="e_pgm">9134</data>
      <data key="e_label">83422</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">83422</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1676" target="n1678">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">BIOCOMPLEXITY: Scaling of Biodiversity: Physical and Biological Foundations of Ecological Principles</data>
      <data key="e_abstract">0083422&lt;br/&gt;Brown&lt;br/&gt; Underlying the diversity of life and the complexity of ecology is order that reflects the operation of fundamental physical and biological processes. Scaling relationships are emergent quantitative features of biodiversity. Some of them appear to be universal, occurring in virtually all taxa of organisms and kinds of environments. They are patterns of structure or dynamics that are self-similar or fractal-like over many orders of magnitude. They can be described mathematically by power functions. They allow extrapolation and prediction over a wide range of scales., They offer clues to underlying mechanisms that powerfully constrain biodiversity. This research will use the interplay of mathematical models and empirical measurements to elucidate the physical and biological principles that determine how the life history, abundance, distribution, and species richness of organisms scale with body size, space, and time. The program of research and education activities involves: (1) collaborations among physicists, mathematicians, geologists/hydrologists, biologists and ecologists, (2) interactions among scientists from seven institutions, (3) cooperation between the University of New Mexico, the Santa Fe Institute, and Los Alamos National Laboratory, and (4) interdisciplinary training for graduate students and postdoctoral fellows.</data>
      <data key="e_pgm">9134</data>
      <data key="e_label">83422</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">83422</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1677" target="n1678">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">BIOCOMPLEXITY: Scaling of Biodiversity: Physical and Biological Foundations of Ecological Principles</data>
      <data key="e_abstract">0083422&lt;br/&gt;Brown&lt;br/&gt; Underlying the diversity of life and the complexity of ecology is order that reflects the operation of fundamental physical and biological processes. Scaling relationships are emergent quantitative features of biodiversity. Some of them appear to be universal, occurring in virtually all taxa of organisms and kinds of environments. They are patterns of structure or dynamics that are self-similar or fractal-like over many orders of magnitude. They can be described mathematically by power functions. They allow extrapolation and prediction over a wide range of scales., They offer clues to underlying mechanisms that powerfully constrain biodiversity. This research will use the interplay of mathematical models and empirical measurements to elucidate the physical and biological principles that determine how the life history, abundance, distribution, and species richness of organisms scale with body size, space, and time. The program of research and education activities involves: (1) collaborations among physicists, mathematicians, geologists/hydrologists, biologists and ecologists, (2) interactions among scientists from seven institutions, (3) cooperation between the University of New Mexico, the Santa Fe Institute, and Los Alamos National Laboratory, and (4) interdisciplinary training for graduate students and postdoctoral fellows.</data>
      <data key="e_pgm">9134</data>
      <data key="e_label">83422</data>
      <data key="e_expirationDate">2006-09-30</data>
      <data key="e_div">0301</data>
      <data key="e_awardID">83422</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1681" target="n1682">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Protocols for Open Access Wireless Deployment</data>
      <data key="e_abstract">The purpose of this project is to demonstrate a new paradigm for wireless network services. The&lt;br/&gt;motivations for this project are the f ollowing:&lt;br/&gt; (1) Today, wireless service is either local to an institution (as in wireless LANs) or is provided as part of&lt;br/&gt;a regional or national service (as in cellular). But there could be other models. Individual providers of&lt;br/&gt;wireless service could put up a base station and o er service locally. Users could move among these&lt;br/&gt;providers and select a service based on requirements and prices. Just as we have over 5000 wireline&lt;br/&gt;Internet service providers today, many only serving a small area, we could have wireless coverage&lt;br/&gt;built bottom up.&lt;br/&gt; (2)Today, we see a restricted range of wireless devices - a laptop PC with a wireless card, a cell phone&lt;br/&gt;augmented with data capabilities, a highly integrated device such as a pager. There is not an open&lt;br/&gt;market for new wide-area wireless devices, because such devices today are tightly bound, both&lt;br/&gt;technically and through service contract, to a particular wireless service provider. But there could be a&lt;br/&gt;wide range of new consumer devices if the proper interfaces and modules were available.&lt;br/&gt; For this to work, two things are necessary. First, each of the broad mix of competing wireless services&lt;br/&gt;must be accessible to a wide variety of devices. What is needed is an overall system architecture that&lt;br/&gt;allows cheap, small, low-power consumer-level objects to access a wide variety of technically&lt;br/&gt;incompatible wireless services with ease, using open interfaces. Second, selection of a service among&lt;br/&gt;competing alternatives must be easy. For example, a manual process of selection and entry of a credit&lt;br/&gt;card would be too burdensome to succeed. What is needed is a model for automated dynamic negotiation,&lt;br/&gt;based on rules provided by the users and providers, together with a workable economic model and a&lt;br/&gt;simple micro-payment scheme.&lt;br/&gt; The goal of this research is to demonstrate two related innovations. The irst is a framework for&lt;br/&gt;automated negotiation for access to wireless services. The second is a small hardware device that the user&lt;br/&gt;can carry, a personal router, which contains the necessary wireless transceivers, implements the access&lt;br/&gt;negotiation protocol, and provides a network connection for the other devices and appliances that a person&lt;br/&gt;might choose to carry. By creation of an open interface between this personal router and other devices a&lt;br/&gt;user might carry, an environment can be created for the development of new devices and applications.&lt;br/&gt; The intellectual merit of this project is embodied in: a) the automated service negotiation framework, b)&lt;br/&gt;experience with systems that use policy constraints to guide automatic configuration; c) the related&lt;br/&gt;protocols for security and billing; d) the demonstration of the new application user interface paradigms&lt;br/&gt;implied by the personal router; and e) identification and definition of the interfaces between this device&lt;br/&gt;and the other consumer devices that can be connected to the Internet using it. The broader impacts are the&lt;br/&gt;possibility of creating a new market model or wireless service, based on small business investment and&lt;br/&gt;local competition, rather than service provision (only) by large, national-level corporations, and the&lt;br/&gt;development of an increased understanding of user pricing preference for communication services.&lt;br/&gt;Research that explores technology supporting alternative economic models is not likely to be done in&lt;br/&gt;private industry, and is thus especially appropriate for public-sector support.</data>
      <data key="e_pgm">4090</data>
      <data key="e_label">82503</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82503</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1018" target="n1019">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Flowspace: The Space Spanned by Pathlines, Timelines, Streaklines.</data>
      <data key="e_abstract">Scientific visualization - that is, presenting computed or measured scientific data to human senses - is a vital area of research. Pictures are often the clearest and fastest way to give a scientist an intuitive feeling for an experiment or simulation. Unfortunately, visualization of time-dependent vector fields (such as velocity of a fluid at all points) is currently limited to a few basic techniques that originally came from experimental methods. Every technique is a variation on streaklines and timelines, both built from particle paths. Crude extensions to algorithms developed for steady flows have recently supplemented these. Most notable among these are the computation and display of streamlines, but these are only physically relevant for steady-state fluid flows.&lt;br/&gt;&lt;br/&gt;This project will place these commonly used techniques within a general geometric and mathematical framework, and advance the notion that no single method is best suited for all flows. Therefore, it will study a more general class of algorithms, and will seek to match a vector field representation to the display of a desired time-dependent feature of the flow (e.g. vortices, eddies, or shocks). To explore this parameter space, the project will develop a suite of interactive vector field visualization tools using both software- and hardware-based techniques. The methods will be tested on prototypical unsteady vector fields, namely linear and quadratic fields in the spatial and temporal variables.</data>
      <data key="e_pgm">4080</data>
      <data key="e_label">83792</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">83792</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n541" target="n1687">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Dynamic Adaptive Wireless Networks with Autonomous Robot Nodes</data>
      <data key="e_abstract">Multihop wireless capabilities are enabling communication and coordination among autonomous nodes in unplanned environments and configurations. At the same time wireless channels present challenges of dynamic operating conditions, power constraints for autonomously-powered nodes, and complicated interactions between high level behavior and lower level channel characteristics (e.g. increased synchronized communication significantly degrades channel characteristics).&lt;br/&gt; The major goal of the research proposed here is the development, testing, and characterization of algorithms for scalable, application-driven, wireless network services using a heterogeneous collection of communicating mobile nodes. Some of these nodes will be autonomous (robots) in that their movements will not be human-controlled. The others will be portable, thus making them dependent on humans for transportation. While the focus of the work is on the mobile nodes, the project includs immobile computers on the network as well. The project emphasizes that most (though not all) of the mobile nodes will have modest sensing, computational, and communication resources.&lt;br/&gt; The chief scientific motivation behind the work is the design of robust, efficient, and scalable algorithms. The project hypothesizes that distributed algorithms that rely on local interactions have many compelling characteristics, resulting in these properties. There is significant overlap between the problems of coordinating the autonomous mobile nodes that carry some of the sensors and the algorithms that direct the flow of information from sources(s)to sink(s) in the network. Both sets of algorithms need to be carefully designed to improve robustness, efficiency, and scalability.&lt;br/&gt; As motivation the project proposes that the experimental part of the research be conducted on a testbed which simulates some characteristics of an urban post-earthquake scenario in a building. The sensors in the experiments will be distributed geographically (within the building) and linked by a wireless network. Many of the mobile nodes will be largely autonomous, serving as easily-accessible knowledge collectors and repositories, and exercising a wide range of independent options in the dispatch and control of information flow and resources. Other mobile nodes will be carried about the environment by people. The project will study issues of scale (how many sensor nodes does the application software accommodate), fault tolerance (how robust is the system to loss of sensors and/or communication) and efficiency (e.g. time vs. quality of service).&lt;br/&gt; As part of a one-year pilot study funded by NSF, the project has been conducting initial research in the issues underlying a system such as the one above. The project also recently received a substantial equipment grant from the Office of Naval Research to support the experimental portion of this work. The project has identified two key unsolved sub-problems that are relevant to the overall goals: localization and communication coverage. In this proposal the project discusses the broad research challenges in the area of communication and coordination of autonomous mobile nodes. The project then focuses on the two key problems as concrete questions that will be addressed in the research and describes a method involving simulation and experimentation to study them systematically.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">82498</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82498</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n541" target="n1688">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Dynamic Adaptive Wireless Networks with Autonomous Robot Nodes</data>
      <data key="e_abstract">Multihop wireless capabilities are enabling communication and coordination among autonomous nodes in unplanned environments and configurations. At the same time wireless channels present challenges of dynamic operating conditions, power constraints for autonomously-powered nodes, and complicated interactions between high level behavior and lower level channel characteristics (e.g. increased synchronized communication significantly degrades channel characteristics).&lt;br/&gt; The major goal of the research proposed here is the development, testing, and characterization of algorithms for scalable, application-driven, wireless network services using a heterogeneous collection of communicating mobile nodes. Some of these nodes will be autonomous (robots) in that their movements will not be human-controlled. The others will be portable, thus making them dependent on humans for transportation. While the focus of the work is on the mobile nodes, the project includs immobile computers on the network as well. The project emphasizes that most (though not all) of the mobile nodes will have modest sensing, computational, and communication resources.&lt;br/&gt; The chief scientific motivation behind the work is the design of robust, efficient, and scalable algorithms. The project hypothesizes that distributed algorithms that rely on local interactions have many compelling characteristics, resulting in these properties. There is significant overlap between the problems of coordinating the autonomous mobile nodes that carry some of the sensors and the algorithms that direct the flow of information from sources(s)to sink(s) in the network. Both sets of algorithms need to be carefully designed to improve robustness, efficiency, and scalability.&lt;br/&gt; As motivation the project proposes that the experimental part of the research be conducted on a testbed which simulates some characteristics of an urban post-earthquake scenario in a building. The sensors in the experiments will be distributed geographically (within the building) and linked by a wireless network. Many of the mobile nodes will be largely autonomous, serving as easily-accessible knowledge collectors and repositories, and exercising a wide range of independent options in the dispatch and control of information flow and resources. Other mobile nodes will be carried about the environment by people. The project will study issues of scale (how many sensor nodes does the application software accommodate), fault tolerance (how robust is the system to loss of sensors and/or communication) and efficiency (e.g. time vs. quality of service).&lt;br/&gt; As part of a one-year pilot study funded by NSF, the project has been conducting initial research in the issues underlying a system such as the one above. The project also recently received a substantial equipment grant from the Office of Naval Research to support the experimental portion of this work. The project has identified two key unsolved sub-problems that are relevant to the overall goals: localization and communication coverage. In this proposal the project discusses the broad research challenges in the area of communication and coordination of autonomous mobile nodes. The project then focuses on the two key problems as concrete questions that will be addressed in the research and describes a method involving simulation and experimentation to study them systematically.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">82498</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82498</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n541" target="n1689">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Dynamic Adaptive Wireless Networks with Autonomous Robot Nodes</data>
      <data key="e_abstract">Multihop wireless capabilities are enabling communication and coordination among autonomous nodes in unplanned environments and configurations. At the same time wireless channels present challenges of dynamic operating conditions, power constraints for autonomously-powered nodes, and complicated interactions between high level behavior and lower level channel characteristics (e.g. increased synchronized communication significantly degrades channel characteristics).&lt;br/&gt; The major goal of the research proposed here is the development, testing, and characterization of algorithms for scalable, application-driven, wireless network services using a heterogeneous collection of communicating mobile nodes. Some of these nodes will be autonomous (robots) in that their movements will not be human-controlled. The others will be portable, thus making them dependent on humans for transportation. While the focus of the work is on the mobile nodes, the project includs immobile computers on the network as well. The project emphasizes that most (though not all) of the mobile nodes will have modest sensing, computational, and communication resources.&lt;br/&gt; The chief scientific motivation behind the work is the design of robust, efficient, and scalable algorithms. The project hypothesizes that distributed algorithms that rely on local interactions have many compelling characteristics, resulting in these properties. There is significant overlap between the problems of coordinating the autonomous mobile nodes that carry some of the sensors and the algorithms that direct the flow of information from sources(s)to sink(s) in the network. Both sets of algorithms need to be carefully designed to improve robustness, efficiency, and scalability.&lt;br/&gt; As motivation the project proposes that the experimental part of the research be conducted on a testbed which simulates some characteristics of an urban post-earthquake scenario in a building. The sensors in the experiments will be distributed geographically (within the building) and linked by a wireless network. Many of the mobile nodes will be largely autonomous, serving as easily-accessible knowledge collectors and repositories, and exercising a wide range of independent options in the dispatch and control of information flow and resources. Other mobile nodes will be carried about the environment by people. The project will study issues of scale (how many sensor nodes does the application software accommodate), fault tolerance (how robust is the system to loss of sensors and/or communication) and efficiency (e.g. time vs. quality of service).&lt;br/&gt; As part of a one-year pilot study funded by NSF, the project has been conducting initial research in the issues underlying a system such as the one above. The project also recently received a substantial equipment grant from the Office of Naval Research to support the experimental portion of this work. The project has identified two key unsolved sub-problems that are relevant to the overall goals: localization and communication coverage. In this proposal the project discusses the broad research challenges in the area of communication and coordination of autonomous mobile nodes. The project then focuses on the two key problems as concrete questions that will be addressed in the research and describes a method involving simulation and experimentation to study them systematically.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">82498</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82498</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n541" target="n542">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Dynamic Adaptive Wireless Networks with Autonomous Robot Nodes</data>
      <data key="e_abstract">Multihop wireless capabilities are enabling communication and coordination among autonomous nodes in unplanned environments and configurations. At the same time wireless channels present challenges of dynamic operating conditions, power constraints for autonomously-powered nodes, and complicated interactions between high level behavior and lower level channel characteristics (e.g. increased synchronized communication significantly degrades channel characteristics).&lt;br/&gt; The major goal of the research proposed here is the development, testing, and characterization of algorithms for scalable, application-driven, wireless network services using a heterogeneous collection of communicating mobile nodes. Some of these nodes will be autonomous (robots) in that their movements will not be human-controlled. The others will be portable, thus making them dependent on humans for transportation. While the focus of the work is on the mobile nodes, the project includs immobile computers on the network as well. The project emphasizes that most (though not all) of the mobile nodes will have modest sensing, computational, and communication resources.&lt;br/&gt; The chief scientific motivation behind the work is the design of robust, efficient, and scalable algorithms. The project hypothesizes that distributed algorithms that rely on local interactions have many compelling characteristics, resulting in these properties. There is significant overlap between the problems of coordinating the autonomous mobile nodes that carry some of the sensors and the algorithms that direct the flow of information from sources(s)to sink(s) in the network. Both sets of algorithms need to be carefully designed to improve robustness, efficiency, and scalability.&lt;br/&gt; As motivation the project proposes that the experimental part of the research be conducted on a testbed which simulates some characteristics of an urban post-earthquake scenario in a building. The sensors in the experiments will be distributed geographically (within the building) and linked by a wireless network. Many of the mobile nodes will be largely autonomous, serving as easily-accessible knowledge collectors and repositories, and exercising a wide range of independent options in the dispatch and control of information flow and resources. Other mobile nodes will be carried about the environment by people. The project will study issues of scale (how many sensor nodes does the application software accommodate), fault tolerance (how robust is the system to loss of sensors and/or communication) and efficiency (e.g. time vs. quality of service).&lt;br/&gt; As part of a one-year pilot study funded by NSF, the project has been conducting initial research in the issues underlying a system such as the one above. The project also recently received a substantial equipment grant from the Office of Naval Research to support the experimental portion of this work. The project has identified two key unsolved sub-problems that are relevant to the overall goals: localization and communication coverage. In this proposal the project discusses the broad research challenges in the area of communication and coordination of autonomous mobile nodes. The project then focuses on the two key problems as concrete questions that will be addressed in the research and describes a method involving simulation and experimentation to study them systematically.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">82498</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82498</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1687" target="n1688">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Dynamic Adaptive Wireless Networks with Autonomous Robot Nodes</data>
      <data key="e_abstract">Multihop wireless capabilities are enabling communication and coordination among autonomous nodes in unplanned environments and configurations. At the same time wireless channels present challenges of dynamic operating conditions, power constraints for autonomously-powered nodes, and complicated interactions between high level behavior and lower level channel characteristics (e.g. increased synchronized communication significantly degrades channel characteristics).&lt;br/&gt; The major goal of the research proposed here is the development, testing, and characterization of algorithms for scalable, application-driven, wireless network services using a heterogeneous collection of communicating mobile nodes. Some of these nodes will be autonomous (robots) in that their movements will not be human-controlled. The others will be portable, thus making them dependent on humans for transportation. While the focus of the work is on the mobile nodes, the project includs immobile computers on the network as well. The project emphasizes that most (though not all) of the mobile nodes will have modest sensing, computational, and communication resources.&lt;br/&gt; The chief scientific motivation behind the work is the design of robust, efficient, and scalable algorithms. The project hypothesizes that distributed algorithms that rely on local interactions have many compelling characteristics, resulting in these properties. There is significant overlap between the problems of coordinating the autonomous mobile nodes that carry some of the sensors and the algorithms that direct the flow of information from sources(s)to sink(s) in the network. Both sets of algorithms need to be carefully designed to improve robustness, efficiency, and scalability.&lt;br/&gt; As motivation the project proposes that the experimental part of the research be conducted on a testbed which simulates some characteristics of an urban post-earthquake scenario in a building. The sensors in the experiments will be distributed geographically (within the building) and linked by a wireless network. Many of the mobile nodes will be largely autonomous, serving as easily-accessible knowledge collectors and repositories, and exercising a wide range of independent options in the dispatch and control of information flow and resources. Other mobile nodes will be carried about the environment by people. The project will study issues of scale (how many sensor nodes does the application software accommodate), fault tolerance (how robust is the system to loss of sensors and/or communication) and efficiency (e.g. time vs. quality of service).&lt;br/&gt; As part of a one-year pilot study funded by NSF, the project has been conducting initial research in the issues underlying a system such as the one above. The project also recently received a substantial equipment grant from the Office of Naval Research to support the experimental portion of this work. The project has identified two key unsolved sub-problems that are relevant to the overall goals: localization and communication coverage. In this proposal the project discusses the broad research challenges in the area of communication and coordination of autonomous mobile nodes. The project then focuses on the two key problems as concrete questions that will be addressed in the research and describes a method involving simulation and experimentation to study them systematically.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">82498</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82498</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1687" target="n1689">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Dynamic Adaptive Wireless Networks with Autonomous Robot Nodes</data>
      <data key="e_abstract">Multihop wireless capabilities are enabling communication and coordination among autonomous nodes in unplanned environments and configurations. At the same time wireless channels present challenges of dynamic operating conditions, power constraints for autonomously-powered nodes, and complicated interactions between high level behavior and lower level channel characteristics (e.g. increased synchronized communication significantly degrades channel characteristics).&lt;br/&gt; The major goal of the research proposed here is the development, testing, and characterization of algorithms for scalable, application-driven, wireless network services using a heterogeneous collection of communicating mobile nodes. Some of these nodes will be autonomous (robots) in that their movements will not be human-controlled. The others will be portable, thus making them dependent on humans for transportation. While the focus of the work is on the mobile nodes, the project includs immobile computers on the network as well. The project emphasizes that most (though not all) of the mobile nodes will have modest sensing, computational, and communication resources.&lt;br/&gt; The chief scientific motivation behind the work is the design of robust, efficient, and scalable algorithms. The project hypothesizes that distributed algorithms that rely on local interactions have many compelling characteristics, resulting in these properties. There is significant overlap between the problems of coordinating the autonomous mobile nodes that carry some of the sensors and the algorithms that direct the flow of information from sources(s)to sink(s) in the network. Both sets of algorithms need to be carefully designed to improve robustness, efficiency, and scalability.&lt;br/&gt; As motivation the project proposes that the experimental part of the research be conducted on a testbed which simulates some characteristics of an urban post-earthquake scenario in a building. The sensors in the experiments will be distributed geographically (within the building) and linked by a wireless network. Many of the mobile nodes will be largely autonomous, serving as easily-accessible knowledge collectors and repositories, and exercising a wide range of independent options in the dispatch and control of information flow and resources. Other mobile nodes will be carried about the environment by people. The project will study issues of scale (how many sensor nodes does the application software accommodate), fault tolerance (how robust is the system to loss of sensors and/or communication) and efficiency (e.g. time vs. quality of service).&lt;br/&gt; As part of a one-year pilot study funded by NSF, the project has been conducting initial research in the issues underlying a system such as the one above. The project also recently received a substantial equipment grant from the Office of Naval Research to support the experimental portion of this work. The project has identified two key unsolved sub-problems that are relevant to the overall goals: localization and communication coverage. In this proposal the project discusses the broad research challenges in the area of communication and coordination of autonomous mobile nodes. The project then focuses on the two key problems as concrete questions that will be addressed in the research and describes a method involving simulation and experimentation to study them systematically.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">82498</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82498</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n542" target="n1687">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Dynamic Adaptive Wireless Networks with Autonomous Robot Nodes</data>
      <data key="e_abstract">Multihop wireless capabilities are enabling communication and coordination among autonomous nodes in unplanned environments and configurations. At the same time wireless channels present challenges of dynamic operating conditions, power constraints for autonomously-powered nodes, and complicated interactions between high level behavior and lower level channel characteristics (e.g. increased synchronized communication significantly degrades channel characteristics).&lt;br/&gt; The major goal of the research proposed here is the development, testing, and characterization of algorithms for scalable, application-driven, wireless network services using a heterogeneous collection of communicating mobile nodes. Some of these nodes will be autonomous (robots) in that their movements will not be human-controlled. The others will be portable, thus making them dependent on humans for transportation. While the focus of the work is on the mobile nodes, the project includs immobile computers on the network as well. The project emphasizes that most (though not all) of the mobile nodes will have modest sensing, computational, and communication resources.&lt;br/&gt; The chief scientific motivation behind the work is the design of robust, efficient, and scalable algorithms. The project hypothesizes that distributed algorithms that rely on local interactions have many compelling characteristics, resulting in these properties. There is significant overlap between the problems of coordinating the autonomous mobile nodes that carry some of the sensors and the algorithms that direct the flow of information from sources(s)to sink(s) in the network. Both sets of algorithms need to be carefully designed to improve robustness, efficiency, and scalability.&lt;br/&gt; As motivation the project proposes that the experimental part of the research be conducted on a testbed which simulates some characteristics of an urban post-earthquake scenario in a building. The sensors in the experiments will be distributed geographically (within the building) and linked by a wireless network. Many of the mobile nodes will be largely autonomous, serving as easily-accessible knowledge collectors and repositories, and exercising a wide range of independent options in the dispatch and control of information flow and resources. Other mobile nodes will be carried about the environment by people. The project will study issues of scale (how many sensor nodes does the application software accommodate), fault tolerance (how robust is the system to loss of sensors and/or communication) and efficiency (e.g. time vs. quality of service).&lt;br/&gt; As part of a one-year pilot study funded by NSF, the project has been conducting initial research in the issues underlying a system such as the one above. The project also recently received a substantial equipment grant from the Office of Naval Research to support the experimental portion of this work. The project has identified two key unsolved sub-problems that are relevant to the overall goals: localization and communication coverage. In this proposal the project discusses the broad research challenges in the area of communication and coordination of autonomous mobile nodes. The project then focuses on the two key problems as concrete questions that will be addressed in the research and describes a method involving simulation and experimentation to study them systematically.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">82498</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82498</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1688" target="n1689">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Dynamic Adaptive Wireless Networks with Autonomous Robot Nodes</data>
      <data key="e_abstract">Multihop wireless capabilities are enabling communication and coordination among autonomous nodes in unplanned environments and configurations. At the same time wireless channels present challenges of dynamic operating conditions, power constraints for autonomously-powered nodes, and complicated interactions between high level behavior and lower level channel characteristics (e.g. increased synchronized communication significantly degrades channel characteristics).&lt;br/&gt; The major goal of the research proposed here is the development, testing, and characterization of algorithms for scalable, application-driven, wireless network services using a heterogeneous collection of communicating mobile nodes. Some of these nodes will be autonomous (robots) in that their movements will not be human-controlled. The others will be portable, thus making them dependent on humans for transportation. While the focus of the work is on the mobile nodes, the project includs immobile computers on the network as well. The project emphasizes that most (though not all) of the mobile nodes will have modest sensing, computational, and communication resources.&lt;br/&gt; The chief scientific motivation behind the work is the design of robust, efficient, and scalable algorithms. The project hypothesizes that distributed algorithms that rely on local interactions have many compelling characteristics, resulting in these properties. There is significant overlap between the problems of coordinating the autonomous mobile nodes that carry some of the sensors and the algorithms that direct the flow of information from sources(s)to sink(s) in the network. Both sets of algorithms need to be carefully designed to improve robustness, efficiency, and scalability.&lt;br/&gt; As motivation the project proposes that the experimental part of the research be conducted on a testbed which simulates some characteristics of an urban post-earthquake scenario in a building. The sensors in the experiments will be distributed geographically (within the building) and linked by a wireless network. Many of the mobile nodes will be largely autonomous, serving as easily-accessible knowledge collectors and repositories, and exercising a wide range of independent options in the dispatch and control of information flow and resources. Other mobile nodes will be carried about the environment by people. The project will study issues of scale (how many sensor nodes does the application software accommodate), fault tolerance (how robust is the system to loss of sensors and/or communication) and efficiency (e.g. time vs. quality of service).&lt;br/&gt; As part of a one-year pilot study funded by NSF, the project has been conducting initial research in the issues underlying a system such as the one above. The project also recently received a substantial equipment grant from the Office of Naval Research to support the experimental portion of this work. The project has identified two key unsolved sub-problems that are relevant to the overall goals: localization and communication coverage. In this proposal the project discusses the broad research challenges in the area of communication and coordination of autonomous mobile nodes. The project then focuses on the two key problems as concrete questions that will be addressed in the research and describes a method involving simulation and experimentation to study them systematically.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">82498</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82498</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n542" target="n1688">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Dynamic Adaptive Wireless Networks with Autonomous Robot Nodes</data>
      <data key="e_abstract">Multihop wireless capabilities are enabling communication and coordination among autonomous nodes in unplanned environments and configurations. At the same time wireless channels present challenges of dynamic operating conditions, power constraints for autonomously-powered nodes, and complicated interactions between high level behavior and lower level channel characteristics (e.g. increased synchronized communication significantly degrades channel characteristics).&lt;br/&gt; The major goal of the research proposed here is the development, testing, and characterization of algorithms for scalable, application-driven, wireless network services using a heterogeneous collection of communicating mobile nodes. Some of these nodes will be autonomous (robots) in that their movements will not be human-controlled. The others will be portable, thus making them dependent on humans for transportation. While the focus of the work is on the mobile nodes, the project includs immobile computers on the network as well. The project emphasizes that most (though not all) of the mobile nodes will have modest sensing, computational, and communication resources.&lt;br/&gt; The chief scientific motivation behind the work is the design of robust, efficient, and scalable algorithms. The project hypothesizes that distributed algorithms that rely on local interactions have many compelling characteristics, resulting in these properties. There is significant overlap between the problems of coordinating the autonomous mobile nodes that carry some of the sensors and the algorithms that direct the flow of information from sources(s)to sink(s) in the network. Both sets of algorithms need to be carefully designed to improve robustness, efficiency, and scalability.&lt;br/&gt; As motivation the project proposes that the experimental part of the research be conducted on a testbed which simulates some characteristics of an urban post-earthquake scenario in a building. The sensors in the experiments will be distributed geographically (within the building) and linked by a wireless network. Many of the mobile nodes will be largely autonomous, serving as easily-accessible knowledge collectors and repositories, and exercising a wide range of independent options in the dispatch and control of information flow and resources. Other mobile nodes will be carried about the environment by people. The project will study issues of scale (how many sensor nodes does the application software accommodate), fault tolerance (how robust is the system to loss of sensors and/or communication) and efficiency (e.g. time vs. quality of service).&lt;br/&gt; As part of a one-year pilot study funded by NSF, the project has been conducting initial research in the issues underlying a system such as the one above. The project also recently received a substantial equipment grant from the Office of Naval Research to support the experimental portion of this work. The project has identified two key unsolved sub-problems that are relevant to the overall goals: localization and communication coverage. In this proposal the project discusses the broad research challenges in the area of communication and coordination of autonomous mobile nodes. The project then focuses on the two key problems as concrete questions that will be addressed in the research and describes a method involving simulation and experimentation to study them systematically.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">82498</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82498</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n542" target="n1689">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Dynamic Adaptive Wireless Networks with Autonomous Robot Nodes</data>
      <data key="e_abstract">Multihop wireless capabilities are enabling communication and coordination among autonomous nodes in unplanned environments and configurations. At the same time wireless channels present challenges of dynamic operating conditions, power constraints for autonomously-powered nodes, and complicated interactions between high level behavior and lower level channel characteristics (e.g. increased synchronized communication significantly degrades channel characteristics).&lt;br/&gt; The major goal of the research proposed here is the development, testing, and characterization of algorithms for scalable, application-driven, wireless network services using a heterogeneous collection of communicating mobile nodes. Some of these nodes will be autonomous (robots) in that their movements will not be human-controlled. The others will be portable, thus making them dependent on humans for transportation. While the focus of the work is on the mobile nodes, the project includs immobile computers on the network as well. The project emphasizes that most (though not all) of the mobile nodes will have modest sensing, computational, and communication resources.&lt;br/&gt; The chief scientific motivation behind the work is the design of robust, efficient, and scalable algorithms. The project hypothesizes that distributed algorithms that rely on local interactions have many compelling characteristics, resulting in these properties. There is significant overlap between the problems of coordinating the autonomous mobile nodes that carry some of the sensors and the algorithms that direct the flow of information from sources(s)to sink(s) in the network. Both sets of algorithms need to be carefully designed to improve robustness, efficiency, and scalability.&lt;br/&gt; As motivation the project proposes that the experimental part of the research be conducted on a testbed which simulates some characteristics of an urban post-earthquake scenario in a building. The sensors in the experiments will be distributed geographically (within the building) and linked by a wireless network. Many of the mobile nodes will be largely autonomous, serving as easily-accessible knowledge collectors and repositories, and exercising a wide range of independent options in the dispatch and control of information flow and resources. Other mobile nodes will be carried about the environment by people. The project will study issues of scale (how many sensor nodes does the application software accommodate), fault tolerance (how robust is the system to loss of sensors and/or communication) and efficiency (e.g. time vs. quality of service).&lt;br/&gt; As part of a one-year pilot study funded by NSF, the project has been conducting initial research in the issues underlying a system such as the one above. The project also recently received a substantial equipment grant from the Office of Naval Research to support the experimental portion of this work. The project has identified two key unsolved sub-problems that are relevant to the overall goals: localization and communication coverage. In this proposal the project discusses the broad research challenges in the area of communication and coordination of autonomous mobile nodes. The project then focuses on the two key problems as concrete questions that will be addressed in the research and describes a method involving simulation and experimentation to study them systematically.</data>
      <data key="e_pgm">4095</data>
      <data key="e_label">82498</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">82498</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1693" target="n1694">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1693" target="n1695">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1693" target="n1696">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1693" target="n1697">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1693" target="n1698">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1693" target="n1699">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1693" target="n1700">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1693" target="n1701">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1693" target="n1702">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1693" target="n1703">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1694" target="n1695">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1694" target="n1696">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1694" target="n1697">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1694" target="n1698">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1694" target="n1699">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1694" target="n1700">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1694" target="n1701">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1694" target="n1702">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1694" target="n1703">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1695" target="n1696">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1695" target="n1697">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1695" target="n1698">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1695" target="n1699">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1695" target="n1700">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1695" target="n1701">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1695" target="n1702">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1695" target="n1703">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1696" target="n1697">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1696" target="n1698">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1696" target="n1699">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1696" target="n1700">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1696" target="n1701">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1696" target="n1702">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1696" target="n1703">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1697" target="n1698">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1697" target="n1699">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1697" target="n1700">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1697" target="n1701">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1697" target="n1702">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1697" target="n1703">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1698" target="n1699">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1698" target="n1700">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1698" target="n1701">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1698" target="n1702">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1698" target="n1703">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1699" target="n1700">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1699" target="n1701">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1699" target="n1702">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1699" target="n1703">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1700" target="n1701">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1700" target="n1702">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1700" target="n1703">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1701" target="n1702">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1701" target="n1703">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1702" target="n1703">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2: Creating the Digital Music Library</data>
      <data key="e_abstract">This project is to establish a Digital Music Library (DML) testbed. The testbed will focus on system architectures, content representation and metadata and network services. Although the &lt;br/&gt;project will address a wide range of multimedia digital libraries issues, it is unique in it&apos;s comprehensive approach to musical content and the internet - pressing contemporary issues capturing &lt;br/&gt;intense public and commercial interest. The project will involve a large team of interdisciplinary researchers at multiple sites. There is as of yet no comparable digital music library to that presented in &lt;br/&gt;the proposal. As a digital library system, the DML will provide integrated multimedia access to a large corpus of musical material. As a research and educational resource for a large, diverse group of &lt;br/&gt;communities, the project promises to draw out new uses and user needs and stimulate creative activities in many areas.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.90907e+06</data>
      <data key="e_expirationDate">2006-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.90907e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n649" target="n1054">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Integrated Sensitive Skin with Advanced Data Architecture</data>
      <data key="e_abstract">The primary objective of the proposed work is to investigate critical development issues to enable a two-dimensional sensor network called an artificial sensitive skin, with embedded integrated sensors and distributed signal-processing network. Specific technical objectives are summarized as follows. In the area of integrated sensors, the PI plans to develop efficient microfabrication technology to enable modular sensors with potentially high integration density. The benefit of a &quot;skin&quot; of multiple, distributed sensors can only be realized through the development and Implementation of signal processing architecture and algorithms. A scale-based distributed signal processing architecture will be developed and implemented for two sensor applications - tactile sensing and flow sensing. Integrated circuits need to be implemented under potentially limited and localized power supply in the sensor fabric. The PI proposes to develop low-power techniques for improving the energy efficiency of sensitive skin applications and demonstrate distributed error/noise tolerance techniques.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">80639</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">80639</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n249" target="n1054">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Integrated Sensitive Skin with Advanced Data Architecture</data>
      <data key="e_abstract">The primary objective of the proposed work is to investigate critical development issues to enable a two-dimensional sensor network called an artificial sensitive skin, with embedded integrated sensors and distributed signal-processing network. Specific technical objectives are summarized as follows. In the area of integrated sensors, the PI plans to develop efficient microfabrication technology to enable modular sensors with potentially high integration density. The benefit of a &quot;skin&quot; of multiple, distributed sensors can only be realized through the development and Implementation of signal processing architecture and algorithms. A scale-based distributed signal processing architecture will be developed and implemented for two sensor applications - tactile sensing and flow sensing. Integrated circuits need to be implemented under potentially limited and localized power supply in the sensor fabric. The PI proposes to develop low-power techniques for improving the energy efficiency of sensitive skin applications and demonstrate distributed error/noise tolerance techniques.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">80639</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">80639</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n249" target="n649">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Integrated Sensitive Skin with Advanced Data Architecture</data>
      <data key="e_abstract">The primary objective of the proposed work is to investigate critical development issues to enable a two-dimensional sensor network called an artificial sensitive skin, with embedded integrated sensors and distributed signal-processing network. Specific technical objectives are summarized as follows. In the area of integrated sensors, the PI plans to develop efficient microfabrication technology to enable modular sensors with potentially high integration density. The benefit of a &quot;skin&quot; of multiple, distributed sensors can only be realized through the development and Implementation of signal processing architecture and algorithms. A scale-based distributed signal processing architecture will be developed and implemented for two sensor applications - tactile sensing and flow sensing. Integrated circuits need to be implemented under potentially limited and localized power supply in the sensor fabric. The PI proposes to develop low-power techniques for improving the energy efficiency of sensitive skin applications and demonstrate distributed error/noise tolerance techniques.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">80639</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">80639</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1713" target="n1714">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">KDI: Molecular Information and Computer Modeling in Cardiac Electrophysiology</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">2860</data>
      <data key="e_label">196184</data>
      <data key="e_expirationDate">2003-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">196184</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1713" target="n1715">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">KDI: Molecular Information and Computer Modeling in Cardiac Electrophysiology</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">2860</data>
      <data key="e_label">196184</data>
      <data key="e_expirationDate">2003-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">196184</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1714" target="n1715">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">KDI: Molecular Information and Computer Modeling in Cardiac Electrophysiology</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">2860</data>
      <data key="e_label">196184</data>
      <data key="e_expirationDate">2003-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">196184</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1716" target="n1717">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Experimental Partnership - Real-Time Computer Vision Based Spatial Mapping and Referencing for Minimally Invasive Surgery</data>
      <data key="e_abstract">EIA-0000417&lt;br/&gt;Badrinath Roysam&lt;br/&gt;Rensselaer Polytechnic Institute&lt;br/&gt;&lt;br/&gt;Experimental Partnership-Real Time Computer Vision Based Spatial Mapping and Referencing for Minimally Invasive Surgery&lt;br/&gt;&lt;br/&gt;The goal is to develop core computer vision technology for a new generation of highly capable instruments for minimally invasive surgery. The specific applications are in opthamology. Developing the technology requires facing and overcoming several fundamental barriers that have plague computer vision systems. These include poor quality image data during surgery, difficult to model biological tissues, large scale and unpredictable motions of the eye, a need for extreme accuracy over long duration, a need for predictable response in a real-time implementation, and a need for transparency of the system as a whole.&lt;br/&gt;&lt;br/&gt;The computer vision technology will distinguish these instruments by making them &quot;spatially-aware&quot;. This means they include capabilities for spatially mapping and spatial referencing. Spatial mapping is the problem of building and maintaining a seamless, wide-area map (mosaic) of the three-dimensional region of surgical interest and its surrounding. This map will be constructed from images acquired during diagnostic exploration and forms the basis for absolute and hence verifiable spatial referencing.&lt;br/&gt;&lt;br/&gt;Novel aspects of the work in mapping include: (1) a fast, recursive technique for tracing elongated structures and then detecting their branching and cross-over points, (2) a feature-based image-to-image matching algorithm that used a high-order transformation model and a hierarchy of robust estimation techniques, (3) a joint optimization of all image-to-mosaic transformations simultaneously, and (4) a 3-D reconstruction of the retinal surface despite an inherent inability to calibrate the imaging system. These techniques are each useful in other domains besides ophthalmology.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">417</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">417</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1716" target="n1718">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Experimental Partnership - Real-Time Computer Vision Based Spatial Mapping and Referencing for Minimally Invasive Surgery</data>
      <data key="e_abstract">EIA-0000417&lt;br/&gt;Badrinath Roysam&lt;br/&gt;Rensselaer Polytechnic Institute&lt;br/&gt;&lt;br/&gt;Experimental Partnership-Real Time Computer Vision Based Spatial Mapping and Referencing for Minimally Invasive Surgery&lt;br/&gt;&lt;br/&gt;The goal is to develop core computer vision technology for a new generation of highly capable instruments for minimally invasive surgery. The specific applications are in opthamology. Developing the technology requires facing and overcoming several fundamental barriers that have plague computer vision systems. These include poor quality image data during surgery, difficult to model biological tissues, large scale and unpredictable motions of the eye, a need for extreme accuracy over long duration, a need for predictable response in a real-time implementation, and a need for transparency of the system as a whole.&lt;br/&gt;&lt;br/&gt;The computer vision technology will distinguish these instruments by making them &quot;spatially-aware&quot;. This means they include capabilities for spatially mapping and spatial referencing. Spatial mapping is the problem of building and maintaining a seamless, wide-area map (mosaic) of the three-dimensional region of surgical interest and its surrounding. This map will be constructed from images acquired during diagnostic exploration and forms the basis for absolute and hence verifiable spatial referencing.&lt;br/&gt;&lt;br/&gt;Novel aspects of the work in mapping include: (1) a fast, recursive technique for tracing elongated structures and then detecting their branching and cross-over points, (2) a feature-based image-to-image matching algorithm that used a high-order transformation model and a hierarchy of robust estimation techniques, (3) a joint optimization of all image-to-mosaic transformations simultaneously, and (4) a 3-D reconstruction of the retinal surface despite an inherent inability to calibrate the imaging system. These techniques are each useful in other domains besides ophthalmology.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">417</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">417</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1717" target="n1718">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Experimental Partnership - Real-Time Computer Vision Based Spatial Mapping and Referencing for Minimally Invasive Surgery</data>
      <data key="e_abstract">EIA-0000417&lt;br/&gt;Badrinath Roysam&lt;br/&gt;Rensselaer Polytechnic Institute&lt;br/&gt;&lt;br/&gt;Experimental Partnership-Real Time Computer Vision Based Spatial Mapping and Referencing for Minimally Invasive Surgery&lt;br/&gt;&lt;br/&gt;The goal is to develop core computer vision technology for a new generation of highly capable instruments for minimally invasive surgery. The specific applications are in opthamology. Developing the technology requires facing and overcoming several fundamental barriers that have plague computer vision systems. These include poor quality image data during surgery, difficult to model biological tissues, large scale and unpredictable motions of the eye, a need for extreme accuracy over long duration, a need for predictable response in a real-time implementation, and a need for transparency of the system as a whole.&lt;br/&gt;&lt;br/&gt;The computer vision technology will distinguish these instruments by making them &quot;spatially-aware&quot;. This means they include capabilities for spatially mapping and spatial referencing. Spatial mapping is the problem of building and maintaining a seamless, wide-area map (mosaic) of the three-dimensional region of surgical interest and its surrounding. This map will be constructed from images acquired during diagnostic exploration and forms the basis for absolute and hence verifiable spatial referencing.&lt;br/&gt;&lt;br/&gt;Novel aspects of the work in mapping include: (1) a fast, recursive technique for tracing elongated structures and then detecting their branching and cross-over points, (2) a feature-based image-to-image matching algorithm that used a high-order transformation model and a hierarchy of robust estimation techniques, (3) a joint optimization of all image-to-mosaic transformations simultaneously, and (4) a 3-D reconstruction of the retinal surface despite an inherent inability to calibrate the imaging system. These techniques are each useful in other domains besides ophthalmology.</data>
      <data key="e_pgm">1640</data>
      <data key="e_label">417</data>
      <data key="e_expirationDate">2006-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">417</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1719" target="n1720">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">SGER-Sensors on Flexible Substrates for Smart Skin</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;0090667&lt;br/&gt;Donald P. Butler and Zeynep Celik-Butler&lt;br/&gt;Southern Methodist University&lt;br/&gt;Small Grants for Exploratory Research (SGER): &lt;br/&gt; $70,000 - 12 mos.&lt;br/&gt;&lt;br/&gt;Sensors on Flexible Substrates for Smart Skin&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;This is a one-year Small Grant for Exploratory Research (SGER). This project concerns the development of suspended, micromachined sensors on flexible substrates. The project&apos;s results will form a basis for the production of other micromachined sensors such as pressure/strain sensors, &quot;hair-like&quot; touch and flow sensors, and accelerometers on flexible substrates. Flexible substrates can serve as the basis of a sensitive skin for humans and robots where sensors are distributed over the skin to provide the sense of touch or monitor the physiology of the wearer. A major obstacle in the development of vanadium oxide bolometers or lead titanate and similar oxide pyroelectric detectors on flexible substrates is the incompatibility between the thermal budget required by the detector material and the low maximum temperature of the flexible substrate. This work will solve the incompatibility between the two thermal budgets by the use of the PI&apos;s patented microbolometer material: semiconducting Yttrium Barium Copper Oxide (YBaCuO). The deposition temperature for this infrared sensitive material does not exceed 150 C. High quality nanocrystalline, semiconducting YBaCuO films can be deposited using rf sputtering at ambient temperature. Since there is no requirement for crystallization, high-temperature annealing steps are not necessary. &lt;br/&gt;&lt;br/&gt;The funding for the project is provided by two NSF divisions, as follows:&lt;br/&gt; IIS/CISE - $50,000&lt;br/&gt; ECS/ENG - $20,000</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">90667</data>
      <data key="e_expirationDate">2002-03-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">90667</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n296" target="n1556">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Collaborative Research (NSF-CNPq): Application Level Adaptation and Control for Retrieval and Delivery of Continuous Media over the Internet</data>
      <data key="e_abstract">Future networked multimedia information systems will carry a wide variety of applications including digital libraries, video, audio and image services, distance learning and collaboration, networked virtual environments, and entertainment. The main characteristics of multimedia applications that lead to difficulties in end-to-end systems design are that they have very large bandwidth and storage requirements, with vastly different performance and reliability requirements, often coupled with real-time constraints. Along with these characteristics, the highly interreliability requirements, often coupled with real-time constraints. Along with these characteristics, the highly interactive nature of a variety of multimedia applications, resulting in fairly unpredictable workloads, makes the design and evaluation of networked multimedia information systems an exceptionally challenging problem. In this proposal, we outline research on three aspects of this problem: (1) the design and evaluation of server resource allocation algorithms for CM servers in order to retrieve information efficiently and according to the QoS demanded by the application; (2)the development of performance evaluation techniques for evaluating new server designs. (3) the participants of this project bring expertise from a wide variety of areas: databases, distance learning, multi-media, networking, and performance evaluation to bear on problems in these areas. Another important feature of this proposal is that the participants have available four prototypes of state of the art multimedia systems: the Virtual World Data Server (VWDS) developed at UCLA: the prototype of a Video-on-Demand server developed in Brazil as part cooperative research project among Brazilian institutions; and the Multimedia Asynchronous Networked Individualized Courseware (MANIC) and the Internet Multimedia Proxy (IMP) both developed at UMass. These applications will be used to motivate the development of new algorithms for retrieving information and for maintaining the desired quality of service after sending the data over a wide area network. They will also form the basis of the many experimental and analytical studies that will be performed to evaluate these new algorithms. To aid in this evaluation, our group also developed three performance evaluation and modeling tools: a state-of-the-art tool for constructing performance and reliability models (Tangram-II) developed in Brazil jointly with UCLA; a symbolic model checking tool (VERUS): and a tool which facilitates design, development, and subsequent performance evaluation of designs of multimedia storage hierarchies (ViPEr-HiSS) under development at UMD. Thus, the environment of our labs as well the long distance among them will provide a unique testbed for this type of an evaluation due to the drastically different connectivities available to our applications, from gigabit low utilized links to intercontinental congested links. &lt;br/&gt;The proposed research represents a fundamentally important step in the design and performance evaluation of next generation information servers and the networked applications that will operate on top of them. As a result of our research we expect to have a better understanding of how storage server resource management policies, channel allocation policies, and network adaptation policies interact to satisfy the required QoS of CM applications, despite the fairly unpredictable network delays.</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">70067</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">70067</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1724" target="n1725">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">First-Order System Least Squares (FOSLS) for Partial Differential Equations</data>
      <data key="e_abstract">Abstract&lt;br/&gt;&lt;br/&gt;This is a renewal proposal to continue development of first--order system least squares (FOSLS) for numerical solution of partial differential equations (PDEs). It combines theoretical analysis, algorithm design, and software development, driven by several real applications, including aerodynamics, meteorology, elasticity, electromagnetics, particle transport, and porous flow. The goal is to develop accurate discretizations and fast solvers for the governing PDEs. The focus will be on the continued development of the FOSLS methodology, with special attention on developing methods that allow non-smooth problem character and solutions, and on further implementation of the methodology in the software package FOSPACK. Applications will include coupled systems, especially those arising in biological simulation, and porous media flow. Successful progress of this project would enable numerical simulations beyond current capabilities in many important applications of national interest.&lt;br/&gt;&lt;br/&gt;The central aim of this project is research in the field of computational mathematics. The purpose is to improve our understanding of the mathematics behind numerical computer simulation of complex physical phenomena. Such simulations are key to the study and control of many important processes, including groundwater flow, global change, energy production, biological modeling, and material science. One of the challenges in such simulations is the development of improved computational methods for solving the mathematical equations that arise in these models. The basic aim of this research is dramatic improvement in our ability to model increasingly more complicated and sophisticated processes with much greater accuracy and efficiency. This should pave the way for simulations that can provide scientists, engineers, and policy-makers with much more powerful tools to understand and improve our industry, science, and environment.</data>
      <data key="e_pgm">1271</data>
      <data key="e_label">84438</data>
      <data key="e_expirationDate">2003-09-30</data>
      <data key="e_div">0304</data>
      <data key="e_awardID">84438</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1726" target="n1727">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Virtual Environmentents as Laboratories for Studying Human Behavior: Modeling, Testing, and Validation</data>
      <data key="e_abstract">The objective of this research is to investigate the use of virtual environments as a medium for the study of human behavior in dynamic environments. The project has two thrusts: a computational component directed at advancing scenario modeling techniques to meet the needs of experiments for replicable experiences that adapt to subject behavior, and an experimental component that investigates children&apos;s bicycle riding behavior in a virtual bicycling simulator. Virtual environments present an exciting new medium for the study of human behavior that combines the rigor of controlled laboratory experiments with the ecological validity of natural experiments. They immerse subjects in worlds that appear physically real, but where conditions can be controlled. In addition, they can realistically simulate dangerous circumstances without risking injury to subjects. To exploit the potential of virtual environments as laboratories for studying human behavior, significant advances must be made in techniques for controlling the dynamics of virtual environments populated with vehicles, pedestrians, bicyclists, and traffic lights. In experiments, the right things must happen at the right time and place. Furthermore, the activity should maintain an appearance of spontaneity; subjects should feel that they have freedom of action and that other objects are behaving normally. The research will investigate on-line direction as a means to adaptively coordinate the behaviors of simulated objects during experiments. Psychological studies will serve as a proving ground to test and harden scenario control methodologies. Experiments conducted in virtual environments will be compared to experiments conducted in real environments to validate the use of simulators as laboratories for the study of human behavior. This work integrates research on high-fidelity simulation, control of complex behaviors, human factors, and developmental psychology. The research will advance virtual environment technology, experimental methods, and simulator validation, and increase our understanding of a leading cause of childhood injuries.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">2535</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">2535</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1726" target="n1728">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Virtual Environmentents as Laboratories for Studying Human Behavior: Modeling, Testing, and Validation</data>
      <data key="e_abstract">The objective of this research is to investigate the use of virtual environments as a medium for the study of human behavior in dynamic environments. The project has two thrusts: a computational component directed at advancing scenario modeling techniques to meet the needs of experiments for replicable experiences that adapt to subject behavior, and an experimental component that investigates children&apos;s bicycle riding behavior in a virtual bicycling simulator. Virtual environments present an exciting new medium for the study of human behavior that combines the rigor of controlled laboratory experiments with the ecological validity of natural experiments. They immerse subjects in worlds that appear physically real, but where conditions can be controlled. In addition, they can realistically simulate dangerous circumstances without risking injury to subjects. To exploit the potential of virtual environments as laboratories for studying human behavior, significant advances must be made in techniques for controlling the dynamics of virtual environments populated with vehicles, pedestrians, bicyclists, and traffic lights. In experiments, the right things must happen at the right time and place. Furthermore, the activity should maintain an appearance of spontaneity; subjects should feel that they have freedom of action and that other objects are behaving normally. The research will investigate on-line direction as a means to adaptively coordinate the behaviors of simulated objects during experiments. Psychological studies will serve as a proving ground to test and harden scenario control methodologies. Experiments conducted in virtual environments will be compared to experiments conducted in real environments to validate the use of simulators as laboratories for the study of human behavior. This work integrates research on high-fidelity simulation, control of complex behaviors, human factors, and developmental psychology. The research will advance virtual environment technology, experimental methods, and simulator validation, and increase our understanding of a leading cause of childhood injuries.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">2535</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">2535</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1727" target="n1728">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Virtual Environmentents as Laboratories for Studying Human Behavior: Modeling, Testing, and Validation</data>
      <data key="e_abstract">The objective of this research is to investigate the use of virtual environments as a medium for the study of human behavior in dynamic environments. The project has two thrusts: a computational component directed at advancing scenario modeling techniques to meet the needs of experiments for replicable experiences that adapt to subject behavior, and an experimental component that investigates children&apos;s bicycle riding behavior in a virtual bicycling simulator. Virtual environments present an exciting new medium for the study of human behavior that combines the rigor of controlled laboratory experiments with the ecological validity of natural experiments. They immerse subjects in worlds that appear physically real, but where conditions can be controlled. In addition, they can realistically simulate dangerous circumstances without risking injury to subjects. To exploit the potential of virtual environments as laboratories for studying human behavior, significant advances must be made in techniques for controlling the dynamics of virtual environments populated with vehicles, pedestrians, bicyclists, and traffic lights. In experiments, the right things must happen at the right time and place. Furthermore, the activity should maintain an appearance of spontaneity; subjects should feel that they have freedom of action and that other objects are behaving normally. The research will investigate on-line direction as a means to adaptively coordinate the behaviors of simulated objects during experiments. Psychological studies will serve as a proving ground to test and harden scenario control methodologies. Experiments conducted in virtual environments will be compared to experiments conducted in real environments to validate the use of simulators as laboratories for the study of human behavior. This work integrates research on high-fidelity simulation, control of complex behaviors, human factors, and developmental psychology. The research will advance virtual environment technology, experimental methods, and simulator validation, and increase our understanding of a leading cause of childhood injuries.</data>
      <data key="e_pgm">6845</data>
      <data key="e_label">2535</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">2535</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1730" target="n1731">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Logic-based Modeling Analysis, and Implementation of Workflow Management Systems</data>
      <data key="e_abstract">A workflow is a collection of cooperating, coordinated activities designed to carry out a well-defined complex process, such as trip planning, graduate student registration procedure, or a business process in a large enterprise. A human, a device, or a program might perform an activity in a workflow. Workflow management systems (WfMS) provide a framework for capturing the interaction among the activities in a workflow and are recognized as a new paradigm for integrating disparate systems, including legacy systems. Automated workflow management is becoming increasingly important, as it is one of the enabling technologies for business-to-business e-commerce. Ideally, a WfMS should be able to help the user in analyzing and reasoning about complex business processes. Unfortunately, present-day systems do not provide sufficient support for this activity and the virtual lack of analysis and reasoning facilities in current workflow management systems is considered a serious problem. To tackle this problem, a formal specification model with a well-defined semantics is needed. The objective of this project is to conduct research in workflow management and to develop a robust and expressive model for dynamic workflows (i.e., workflows where tasks and their interrelationships change over time) as well as a technological infrastructure (prototypes) for implementing verifiable workflow management systems. The techniques will be validated in the area of supply chain management in collaboration with industry partners. The research is expected to result in the development and distribution of an open source tool for building and managing dynamic workflows. &lt;br/&gt;Relevant links:&lt;br/&gt;http://xsb.sourceforge.net/&lt;br/&gt;http://www.cs.sunysb.edu/~lmc/&lt;br/&gt;http://www.cs.sunysb.edu/~workflow/</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">72927</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">72927</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1730" target="n1732">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Logic-based Modeling Analysis, and Implementation of Workflow Management Systems</data>
      <data key="e_abstract">A workflow is a collection of cooperating, coordinated activities designed to carry out a well-defined complex process, such as trip planning, graduate student registration procedure, or a business process in a large enterprise. A human, a device, or a program might perform an activity in a workflow. Workflow management systems (WfMS) provide a framework for capturing the interaction among the activities in a workflow and are recognized as a new paradigm for integrating disparate systems, including legacy systems. Automated workflow management is becoming increasingly important, as it is one of the enabling technologies for business-to-business e-commerce. Ideally, a WfMS should be able to help the user in analyzing and reasoning about complex business processes. Unfortunately, present-day systems do not provide sufficient support for this activity and the virtual lack of analysis and reasoning facilities in current workflow management systems is considered a serious problem. To tackle this problem, a formal specification model with a well-defined semantics is needed. The objective of this project is to conduct research in workflow management and to develop a robust and expressive model for dynamic workflows (i.e., workflows where tasks and their interrelationships change over time) as well as a technological infrastructure (prototypes) for implementing verifiable workflow management systems. The techniques will be validated in the area of supply chain management in collaboration with industry partners. The research is expected to result in the development and distribution of an open source tool for building and managing dynamic workflows. &lt;br/&gt;Relevant links:&lt;br/&gt;http://xsb.sourceforge.net/&lt;br/&gt;http://www.cs.sunysb.edu/~lmc/&lt;br/&gt;http://www.cs.sunysb.edu/~workflow/</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">72927</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">72927</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1731" target="n1732">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Logic-based Modeling Analysis, and Implementation of Workflow Management Systems</data>
      <data key="e_abstract">A workflow is a collection of cooperating, coordinated activities designed to carry out a well-defined complex process, such as trip planning, graduate student registration procedure, or a business process in a large enterprise. A human, a device, or a program might perform an activity in a workflow. Workflow management systems (WfMS) provide a framework for capturing the interaction among the activities in a workflow and are recognized as a new paradigm for integrating disparate systems, including legacy systems. Automated workflow management is becoming increasingly important, as it is one of the enabling technologies for business-to-business e-commerce. Ideally, a WfMS should be able to help the user in analyzing and reasoning about complex business processes. Unfortunately, present-day systems do not provide sufficient support for this activity and the virtual lack of analysis and reasoning facilities in current workflow management systems is considered a serious problem. To tackle this problem, a formal specification model with a well-defined semantics is needed. The objective of this project is to conduct research in workflow management and to develop a robust and expressive model for dynamic workflows (i.e., workflows where tasks and their interrelationships change over time) as well as a technological infrastructure (prototypes) for implementing verifiable workflow management systems. The techniques will be validated in the area of supply chain management in collaboration with industry partners. The research is expected to result in the development and distribution of an open source tool for building and managing dynamic workflows. &lt;br/&gt;Relevant links:&lt;br/&gt;http://xsb.sourceforge.net/&lt;br/&gt;http://www.cs.sunysb.edu/~lmc/&lt;br/&gt;http://www.cs.sunysb.edu/~workflow/</data>
      <data key="e_pgm">6855</data>
      <data key="e_label">72927</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">72927</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n279" target="n1743">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">CRCD: Modules and Courses for Ubiquitous and Mobile Computing</data>
      <data key="e_abstract">0088078&lt;br/&gt;Astrachan, Owen L.&lt;br/&gt;Duke University&lt;br/&gt;&lt;br/&gt;CRCD: Modules and Courses for Ubiquitous and Mobile Computing&lt;br/&gt;&lt;br/&gt;This project educates students in the techniques and technologies required to deploy next generation wireless information systems. Courses developed for this project focus on ubiquitous and wireless computing. And, these same technologies are used in classrooms to deliver the curriculum as a part of the project that supports &quot;active learning.&quot; The areas of research chosen for migration into the curriculum include mobile code (placement and migration of code to adapt to rapidly changing clients, networks, and service characteristics), transcoding (transforming multimedia web content to save bandwidth and thus energy consumption at the destination device), active name architectures (where resource names are decoupled from specific hosts when resolving services), and energy aware operating systems (where the goal is to make basic interactions of hardware and software as energy efficient as possible for local computation). The project migrates research topics into advanced undergraduate courses and graduate courses, developing modules, assignments, software and curricular support that engage and educate students in the technologies of mobile and wireless information systems. The materials developed are integrated into five computer science courses. The courses apply active lectures, a form of active learning, to deliver content.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88078</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88078</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n279" target="n281">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">CRCD: Modules and Courses for Ubiquitous and Mobile Computing</data>
      <data key="e_abstract">0088078&lt;br/&gt;Astrachan, Owen L.&lt;br/&gt;Duke University&lt;br/&gt;&lt;br/&gt;CRCD: Modules and Courses for Ubiquitous and Mobile Computing&lt;br/&gt;&lt;br/&gt;This project educates students in the techniques and technologies required to deploy next generation wireless information systems. Courses developed for this project focus on ubiquitous and wireless computing. And, these same technologies are used in classrooms to deliver the curriculum as a part of the project that supports &quot;active learning.&quot; The areas of research chosen for migration into the curriculum include mobile code (placement and migration of code to adapt to rapidly changing clients, networks, and service characteristics), transcoding (transforming multimedia web content to save bandwidth and thus energy consumption at the destination device), active name architectures (where resource names are decoupled from specific hosts when resolving services), and energy aware operating systems (where the goal is to make basic interactions of hardware and software as energy efficient as possible for local computation). The project migrates research topics into advanced undergraduate courses and graduate courses, developing modules, assignments, software and curricular support that engage and educate students in the technologies of mobile and wireless information systems. The materials developed are integrated into five computer science courses. The courses apply active lectures, a form of active learning, to deliver content.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88078</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88078</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n281" target="n1743">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">CRCD: Modules and Courses for Ubiquitous and Mobile Computing</data>
      <data key="e_abstract">0088078&lt;br/&gt;Astrachan, Owen L.&lt;br/&gt;Duke University&lt;br/&gt;&lt;br/&gt;CRCD: Modules and Courses for Ubiquitous and Mobile Computing&lt;br/&gt;&lt;br/&gt;This project educates students in the techniques and technologies required to deploy next generation wireless information systems. Courses developed for this project focus on ubiquitous and wireless computing. And, these same technologies are used in classrooms to deliver the curriculum as a part of the project that supports &quot;active learning.&quot; The areas of research chosen for migration into the curriculum include mobile code (placement and migration of code to adapt to rapidly changing clients, networks, and service characteristics), transcoding (transforming multimedia web content to save bandwidth and thus energy consumption at the destination device), active name architectures (where resource names are decoupled from specific hosts when resolving services), and energy aware operating systems (where the goal is to make basic interactions of hardware and software as energy efficient as possible for local computation). The project migrates research topics into advanced undergraduate courses and graduate courses, developing modules, assignments, software and curricular support that engage and educate students in the technologies of mobile and wireless information systems. The materials developed are integrated into five computer science courses. The courses apply active lectures, a form of active learning, to deliver content.</data>
      <data key="e_pgm">1709</data>
      <data key="e_label">88078</data>
      <data key="e_expirationDate">2005-09-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">88078</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1747" target="n1748">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Traffic Management in IP Networks</data>
      <data key="e_abstract">There is an immense demand for quality of service (QoS) in the Internet. One key element of quality of service is traffic management. Since the network traffic is bursty, it is difficult to make any QoS guarantees without proper control of traffic. Currently, Internet Protocol (IP) has only minimal traffic management capabilities. The packets are dropped when the queue exceeds the buffer capacity. The transmission control protocol (TCP) uses the packet drop as a signal of congestion and reduces its load. While in the past, this strategy has worked satisfactorily, there is need for better strategies for two reasons. First, a large part of the traffic, particularly, voice and video traffic does not use TCP. Continuous media traffic uses User Datagram Protocol (UDP). The proportion of UDP traffic is increasing at a faster pace than TCP traffic. The UDP traffic is congestion insensitive in the sense that UDP sources do not reduce their load in response to congestion. Second, the bandwidth of the networks as well as the distances are increasing. For very high distance-bandwidth product networks, packet drop is not the optimal congestion indication. Several megabytes of data may be lost in the time required to detect and respond to packet losses. Therefore, a better strategy for traffic management in IP networks is required.&lt;br/&gt;&lt;br/&gt;Recognizing the need for direct feedback of congestion information, the Internet Engineering Task Force (IETF) has come up with an Explicit Congestion Notification (ECN) method for IP routers. A bit in the IP header is set when the routers are congested. ECN is much more powerful than the simple packet drop indication used by existing routers and is suitable for high distance-bandwidth networks. Unfortunately, to realize the full potential of ECN, several questions need to be answered.&lt;br/&gt;&lt;br/&gt;In this research proposal, the PIs propose a comprehensive program of research on traffic management in IP networks. They propose to develop a new set of traffic management algorithms for IP networks based on Explicit Congestion Notification mechanism. A total of 18 different issues will be analyzed. The PIs have identified potential solutions and approaches for each of these issues. Specifically, they propose to work on a new congestion detection and buffer management scheme for routers, a mechanism for TCP to react to ECN messages from the network. One of the important goals of this research is to make TCP traffic management algorithm free of any bias based on round trip time and number of congested gateways traveled.&lt;br/&gt;&lt;br/&gt;The proposed research will be based on theoretical analysis and simulations. The PI&apos;s approach will be a formal analysis of simple scenarios, heuristic analysis of more complex scenarios and validation using simulations. The emphasis will be to develop simple solutions. However, the performance lost in exchange of simplicity will be theoretically analyzed.&lt;br/&gt;&lt;br/&gt;Traffic management is the key in providing QoS. Currently, a significant amount of NSF, DARPA, and other research funding as well as energy in networking is being spent on QoS issues. When QoS based solutions (integrated services, differentiated services, or multiprotocol label switching) are deployed, the need for traffic management will become apparent and the PIs expect to see an immediate need for proper methods for traffic management. This proposal is, therefore, timely and important.</data>
      <data key="e_pgm">4097</data>
      <data key="e_label">9.98064e+06</data>
      <data key="e_expirationDate">2004-04-30</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">9.98064e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1751" target="n1752">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">BIOCOMPLEXITY-- INCUBATION ACTIVITY: Deeply Understanding Complex Systems through Relationship Analysis</data>
      <data key="e_abstract">EIA-0083758&lt;br/&gt;Bieber, Michael&lt;br/&gt;New Jersey Institute of Technology &lt;br/&gt;&lt;br/&gt;Title: Biocomplexity: Deeply understanding complex systems through relationship analysis&lt;br/&gt;&lt;br/&gt;This project is an incubation activity aimed at seeding a multidisciplinary group in biocomplexity research. Through preliminary collaborative work, this group has developed an approach for analyzing complex relationships in natural (biological and others) environments, in a way similar to those for analyzing complex computing and information systems. The goals of this project are to better understanding of this new approach and to test its extensibility from a single domain to multiple domains. The project will support the interaction and collaboration among several faculty members, graduate students across several institutions. A center piece of the project is the holding of two workshops that bring together researchers from various disciplines within the biological, physical, and social sciences. The outcomes of this work will contribute significantly to the enhancement of the project team for future research and education in biocomplexity.</data>
      <data key="e_pgm">1366</data>
      <data key="e_label">83758</data>
      <data key="e_expirationDate">2002-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">83758</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1751" target="n1753">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">BIOCOMPLEXITY-- INCUBATION ACTIVITY: Deeply Understanding Complex Systems through Relationship Analysis</data>
      <data key="e_abstract">EIA-0083758&lt;br/&gt;Bieber, Michael&lt;br/&gt;New Jersey Institute of Technology &lt;br/&gt;&lt;br/&gt;Title: Biocomplexity: Deeply understanding complex systems through relationship analysis&lt;br/&gt;&lt;br/&gt;This project is an incubation activity aimed at seeding a multidisciplinary group in biocomplexity research. Through preliminary collaborative work, this group has developed an approach for analyzing complex relationships in natural (biological and others) environments, in a way similar to those for analyzing complex computing and information systems. The goals of this project are to better understanding of this new approach and to test its extensibility from a single domain to multiple domains. The project will support the interaction and collaboration among several faculty members, graduate students across several institutions. A center piece of the project is the holding of two workshops that bring together researchers from various disciplines within the biological, physical, and social sciences. The outcomes of this work will contribute significantly to the enhancement of the project team for future research and education in biocomplexity.</data>
      <data key="e_pgm">1366</data>
      <data key="e_label">83758</data>
      <data key="e_expirationDate">2002-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">83758</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1751" target="n1754">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">BIOCOMPLEXITY-- INCUBATION ACTIVITY: Deeply Understanding Complex Systems through Relationship Analysis</data>
      <data key="e_abstract">EIA-0083758&lt;br/&gt;Bieber, Michael&lt;br/&gt;New Jersey Institute of Technology &lt;br/&gt;&lt;br/&gt;Title: Biocomplexity: Deeply understanding complex systems through relationship analysis&lt;br/&gt;&lt;br/&gt;This project is an incubation activity aimed at seeding a multidisciplinary group in biocomplexity research. Through preliminary collaborative work, this group has developed an approach for analyzing complex relationships in natural (biological and others) environments, in a way similar to those for analyzing complex computing and information systems. The goals of this project are to better understanding of this new approach and to test its extensibility from a single domain to multiple domains. The project will support the interaction and collaboration among several faculty members, graduate students across several institutions. A center piece of the project is the holding of two workshops that bring together researchers from various disciplines within the biological, physical, and social sciences. The outcomes of this work will contribute significantly to the enhancement of the project team for future research and education in biocomplexity.</data>
      <data key="e_pgm">1366</data>
      <data key="e_label">83758</data>
      <data key="e_expirationDate">2002-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">83758</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1752" target="n1753">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">BIOCOMPLEXITY-- INCUBATION ACTIVITY: Deeply Understanding Complex Systems through Relationship Analysis</data>
      <data key="e_abstract">EIA-0083758&lt;br/&gt;Bieber, Michael&lt;br/&gt;New Jersey Institute of Technology &lt;br/&gt;&lt;br/&gt;Title: Biocomplexity: Deeply understanding complex systems through relationship analysis&lt;br/&gt;&lt;br/&gt;This project is an incubation activity aimed at seeding a multidisciplinary group in biocomplexity research. Through preliminary collaborative work, this group has developed an approach for analyzing complex relationships in natural (biological and others) environments, in a way similar to those for analyzing complex computing and information systems. The goals of this project are to better understanding of this new approach and to test its extensibility from a single domain to multiple domains. The project will support the interaction and collaboration among several faculty members, graduate students across several institutions. A center piece of the project is the holding of two workshops that bring together researchers from various disciplines within the biological, physical, and social sciences. The outcomes of this work will contribute significantly to the enhancement of the project team for future research and education in biocomplexity.</data>
      <data key="e_pgm">1366</data>
      <data key="e_label">83758</data>
      <data key="e_expirationDate">2002-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">83758</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1752" target="n1754">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">BIOCOMPLEXITY-- INCUBATION ACTIVITY: Deeply Understanding Complex Systems through Relationship Analysis</data>
      <data key="e_abstract">EIA-0083758&lt;br/&gt;Bieber, Michael&lt;br/&gt;New Jersey Institute of Technology &lt;br/&gt;&lt;br/&gt;Title: Biocomplexity: Deeply understanding complex systems through relationship analysis&lt;br/&gt;&lt;br/&gt;This project is an incubation activity aimed at seeding a multidisciplinary group in biocomplexity research. Through preliminary collaborative work, this group has developed an approach for analyzing complex relationships in natural (biological and others) environments, in a way similar to those for analyzing complex computing and information systems. The goals of this project are to better understanding of this new approach and to test its extensibility from a single domain to multiple domains. The project will support the interaction and collaboration among several faculty members, graduate students across several institutions. A center piece of the project is the holding of two workshops that bring together researchers from various disciplines within the biological, physical, and social sciences. The outcomes of this work will contribute significantly to the enhancement of the project team for future research and education in biocomplexity.</data>
      <data key="e_pgm">1366</data>
      <data key="e_label">83758</data>
      <data key="e_expirationDate">2002-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">83758</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1753" target="n1754">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">BIOCOMPLEXITY-- INCUBATION ACTIVITY: Deeply Understanding Complex Systems through Relationship Analysis</data>
      <data key="e_abstract">EIA-0083758&lt;br/&gt;Bieber, Michael&lt;br/&gt;New Jersey Institute of Technology &lt;br/&gt;&lt;br/&gt;Title: Biocomplexity: Deeply understanding complex systems through relationship analysis&lt;br/&gt;&lt;br/&gt;This project is an incubation activity aimed at seeding a multidisciplinary group in biocomplexity research. Through preliminary collaborative work, this group has developed an approach for analyzing complex relationships in natural (biological and others) environments, in a way similar to those for analyzing complex computing and information systems. The goals of this project are to better understanding of this new approach and to test its extensibility from a single domain to multiple domains. The project will support the interaction and collaboration among several faculty members, graduate students across several institutions. A center piece of the project is the holding of two workshops that bring together researchers from various disciplines within the biological, physical, and social sciences. The outcomes of this work will contribute significantly to the enhancement of the project team for future research and education in biocomplexity.</data>
      <data key="e_pgm">1366</data>
      <data key="e_label">83758</data>
      <data key="e_expirationDate">2002-09-30</data>
      <data key="e_div">0506</data>
      <data key="e_awardID">83758</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1755" target="n1756">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">DLI-2 Shuhai Wenyuan Classical Digital Database and Interactive Internet Worktable</data>
      <data key="e_abstract">This project entitled &quot;Shuhai Wenyuan Classical Digital Database and Interactive Internet Worktable&quot; will create a digital corpus and internet-based resources to allow world wide use of seminal texts from China&apos;s classical period. The project will involve bringing together specialist in Classical Chinese language, thought, and culture, and information technologists to produce tools and access methods to materials that have thus far been limited to a select group of students and scholars. By doing so the project intends to open up new areas of study and research for learners of all ages.&lt;br/&gt;&lt;br/&gt;The data content will be freely available via the web and offer Chinese texts, English examples, cultural and philosophical notes, grammar notes, and a search engine designed for a variety of tasks.</data>
      <data key="e_pgm">6857</data>
      <data key="e_label">9.91081e+06</data>
      <data key="e_expirationDate">2004-05-31</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">9.91081e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1757" target="n1758">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">ITW: Assessing an Untapped Supply of Information Technology Workers: Adult Women &amp; Underrepresented Minorities</data>
      <data key="e_abstract">This CISE Information Technology Workforce (ITW) proposal requests funds to analyze, through six unique surveys (i.e. alumni, recent graduates, enrolled, changed to a non-IT program, stopped-or dropped-out), the environmental, educational, and workforce issues faced by over 8,500 adult females and 8,900 minority students. The goal of this research is to obtain an understanding of the backgrounds, motivations, preferences and support systems that adult women and minority IT students express upon entrance and exit from a non-tradional university. This project has the potential to provide valuable insight about how to educate and provide support services to women and underrepresented groups who are interested in pursuing IT careers.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">91574</data>
      <data key="e_expirationDate">2002-03-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">91574</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1757" target="n1759">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">ITW: Assessing an Untapped Supply of Information Technology Workers: Adult Women &amp; Underrepresented Minorities</data>
      <data key="e_abstract">This CISE Information Technology Workforce (ITW) proposal requests funds to analyze, through six unique surveys (i.e. alumni, recent graduates, enrolled, changed to a non-IT program, stopped-or dropped-out), the environmental, educational, and workforce issues faced by over 8,500 adult females and 8,900 minority students. The goal of this research is to obtain an understanding of the backgrounds, motivations, preferences and support systems that adult women and minority IT students express upon entrance and exit from a non-tradional university. This project has the potential to provide valuable insight about how to educate and provide support services to women and underrepresented groups who are interested in pursuing IT careers.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">91574</data>
      <data key="e_expirationDate">2002-03-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">91574</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1758" target="n1759">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">ITW: Assessing an Untapped Supply of Information Technology Workers: Adult Women &amp; Underrepresented Minorities</data>
      <data key="e_abstract">This CISE Information Technology Workforce (ITW) proposal requests funds to analyze, through six unique surveys (i.e. alumni, recent graduates, enrolled, changed to a non-IT program, stopped-or dropped-out), the environmental, educational, and workforce issues faced by over 8,500 adult females and 8,900 minority students. The goal of this research is to obtain an understanding of the backgrounds, motivations, preferences and support systems that adult women and minority IT students express upon entrance and exit from a non-tradional university. This project has the potential to provide valuable insight about how to educate and provide support services to women and underrepresented groups who are interested in pursuing IT careers.</data>
      <data key="e_pgm">2885</data>
      <data key="e_label">91574</data>
      <data key="e_expirationDate">2002-03-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">91574</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1760" target="n1761">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">ITW: Bringing Women and Minorities into the IT Workforce: The Role of Non-Traditional Educational Pathways</data>
      <data key="e_abstract">Institution: American Association for the Advancement of Science&lt;br/&gt;Proposal Number: EIA 0090004&lt;br/&gt;PI: Shirley M. Malcom&lt;br/&gt;Title: Bringing Women and Minorities into the IT Workforce: The Role of Non-Traditional Educational Pathways&lt;br/&gt;&lt;br/&gt;This CISE Information Technology Workforce (ITW) proposal requests funds to study the role that non-traditional educational pathways play in the education of IT professionals, in particular, their role in preparing underrepresented minorities and women for IT careers. The study will consist of four primary tasks. The first task will be the collection of demographic data and statistical information to characterize non-traditional students and educational pathways. The second task will be the collection of detailed information on the education and careers of non-traditional students in the Virginia-Maryland-D.C. region through surveys of former students and interviews with educators, employers, and employees. The third task will be data analysis. The final task will be a science policy conference to develop a policy agenda for federal and state governments based on the results of the first three tasks. This project has the potential to provide valuable insights into the education of underrepresentation of women and minorities for IT careers.</data>
      <data key="e_pgm">1713</data>
      <data key="e_label">90004</data>
      <data key="e_expirationDate">2004-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">90004</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1764" target="n1765">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Climate System Dynamics on Long Time Scales</data>
      <data key="e_abstract">Abstract&lt;br/&gt;ATM-0082131&lt;br/&gt;Ghil, Michael&lt;br/&gt;University of California, Los Angeles&lt;br/&gt;&lt;br/&gt;The research bears on the problems of internal and external causes of climatic change. It emphasizes the role of the atmosphere in climate variability from months to years, of the oceans in variability from years to decades, and of the coupled ocean-atmosphere-ice-sheet system from decades to millennia. The methodology uses tools from fluid dynamics, dynamical systems theory, scientific computing and nonparametric statistics. The dynamical tools will be applied to a hierarchy of atmospheric, oceanic and coupled models. These models range from the simplest models with a small number of variables, such as box models of the ocean, to fully coupled general circulation models (GCMs).&lt;br/&gt;&lt;br/&gt;The PIs will study coupling of climate subsystems across timescales using various mathematical and statistical techniques. On the intraseasonal time scale of 10-100 days, the PIs will consider two complementary descriptions of low-frequency atmospheric variability, episodic and periodic, i.e. multiple weather regimes and intraseasonal oscillations. On the subannual-to-interannual time scale of 100 days to 10 or more years, the research emphasizes internal variability of the mid-latitude oceans&apos; wind-driven circulation. This circulation is studied for prescribed time-constant and seasonally varying wind stress acting on ocean-only models, as well as for fully coupled ocean-atmosphere models. On time scales of decades-to-centuries and longer, the PIs will use comprehensive coupled models such as CSM and IPSL. The simulations will be analyzed and compared with instrumental and paleo-records to determine and describe the relative importance of natural variability and anthropogenic climate change on these time scales. The work is important because it will increase understanding of climate variability across various time scales.</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">82131</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">82131</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n1764" target="n1766">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Climate System Dynamics on Long Time Scales</data>
      <data key="e_abstract">Abstract&lt;br/&gt;ATM-0082131&lt;br/&gt;Ghil, Michael&lt;br/&gt;University of California, Los Angeles&lt;br/&gt;&lt;br/&gt;The research bears on the problems of internal and external causes of climatic change. It emphasizes the role of the atmosphere in climate variability from months to years, of the oceans in variability from years to decades, and of the coupled ocean-atmosphere-ice-sheet system from decades to millennia. The methodology uses tools from fluid dynamics, dynamical systems theory, scientific computing and nonparametric statistics. The dynamical tools will be applied to a hierarchy of atmospheric, oceanic and coupled models. These models range from the simplest models with a small number of variables, such as box models of the ocean, to fully coupled general circulation models (GCMs).&lt;br/&gt;&lt;br/&gt;The PIs will study coupling of climate subsystems across timescales using various mathematical and statistical techniques. On the intraseasonal time scale of 10-100 days, the PIs will consider two complementary descriptions of low-frequency atmospheric variability, episodic and periodic, i.e. multiple weather regimes and intraseasonal oscillations. On the subannual-to-interannual time scale of 100 days to 10 or more years, the research emphasizes internal variability of the mid-latitude oceans&apos; wind-driven circulation. This circulation is studied for prescribed time-constant and seasonally varying wind stress acting on ocean-only models, as well as for fully coupled ocean-atmosphere models. On time scales of decades-to-centuries and longer, the PIs will use comprehensive coupled models such as CSM and IPSL. The simulations will be analyzed and compared with instrumental and paleo-records to determine and describe the relative importance of natural variability and anthropogenic climate change on these time scales. The work is important because it will increase understanding of climate variability across various time scales.</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">82131</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">82131</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n1765" target="n1766">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Climate System Dynamics on Long Time Scales</data>
      <data key="e_abstract">Abstract&lt;br/&gt;ATM-0082131&lt;br/&gt;Ghil, Michael&lt;br/&gt;University of California, Los Angeles&lt;br/&gt;&lt;br/&gt;The research bears on the problems of internal and external causes of climatic change. It emphasizes the role of the atmosphere in climate variability from months to years, of the oceans in variability from years to decades, and of the coupled ocean-atmosphere-ice-sheet system from decades to millennia. The methodology uses tools from fluid dynamics, dynamical systems theory, scientific computing and nonparametric statistics. The dynamical tools will be applied to a hierarchy of atmospheric, oceanic and coupled models. These models range from the simplest models with a small number of variables, such as box models of the ocean, to fully coupled general circulation models (GCMs).&lt;br/&gt;&lt;br/&gt;The PIs will study coupling of climate subsystems across timescales using various mathematical and statistical techniques. On the intraseasonal time scale of 10-100 days, the PIs will consider two complementary descriptions of low-frequency atmospheric variability, episodic and periodic, i.e. multiple weather regimes and intraseasonal oscillations. On the subannual-to-interannual time scale of 100 days to 10 or more years, the research emphasizes internal variability of the mid-latitude oceans&apos; wind-driven circulation. This circulation is studied for prescribed time-constant and seasonally varying wind stress acting on ocean-only models, as well as for fully coupled ocean-atmosphere models. On time scales of decades-to-centuries and longer, the PIs will use comprehensive coupled models such as CSM and IPSL. The simulations will be analyzed and compared with instrumental and paleo-records to determine and describe the relative importance of natural variability and anthropogenic climate change on these time scales. The work is important because it will increase understanding of climate variability across various time scales.</data>
      <data key="e_pgm">5979</data>
      <data key="e_label">82131</data>
      <data key="e_expirationDate">2007-09-30</data>
      <data key="e_div">0406</data>
      <data key="e_awardID">82131</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n1768" target="n1769">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">SGER: A Mobile Platform for Shared Locomotion Control for the Elderly (Exploratory Research)</data>
      <data key="e_abstract">This project seeks to investigate control systems for a human-machine shared locomotion platform to assist the human in pedestrian mobility. &lt;br/&gt;The project is specifically motivated by the need of the elderly. The elderly are often restricted in their mobility and must rely on canes, walkers, and ultimately, wheelchairs for indoor mobility. Restrictions in mobility lead to a loss of independence for the elderly, often resulting in nursing home admission. This research will develop a control system for a semi-autonomous, wheeled &quot;walker&quot; (c.f. a lightweight frame used in the aid of walking) in which the user provides the locomotive force and the user and the walker collaborate on the steering. Specific investigation includes developing shared control techniques that assists the ambulatory elderly in guidance in known environments, providing basic safety measures in known and unknown environments (i.e. curb detection and automatic braking), implementing a hardware prototype and empirically testing the performance of the system using a variety of discrete situations developed in concert with geriatrics professionals.The essence of this research is to explore new control techniques appropriate for shared control with elderly operators.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">4247</data>
      <data key="e_expirationDate">2001-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">4247</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1770" target="n1771">
      <data key="e_effectiveDate">2000-10-01</data>
      <data key="e_title">Practical Robot Path Planning with Contact and Velocity Constraints</data>
      <data key="e_abstract">The goal of this work is to explore the fundamental properties of optimal paths &lt;br/&gt;for robots subject to contact and velocity constraints, and to implement &lt;br/&gt;practical path planners based on that exploration. This is a collaboration &lt;br/&gt;to combine work on time-optimal paths with work on path planning &lt;br/&gt;algorithms. The primary application is to plan time-optimal paths for a &lt;br/&gt;differential drive mobile robot that navigates a known environment. &lt;br/&gt;The planner will construct an explicit model of the configuration &lt;br/&gt;space and will search for time-optimal collision-free paths. This &lt;br/&gt;defines the goals for the theoretical work: to identify constraints on &lt;br/&gt;the motions that reduce the complexity of the search. Optimality &lt;br/&gt;and velocity constraints are both a source of such constraints. A better &lt;br/&gt;understanding of time-optimal nonholonomic paths will lead to efficient &lt;br/&gt;planning of efficient motions.</data>
      <data key="e_pgm">6840</data>
      <data key="e_label">82339</data>
      <data key="e_expirationDate">2004-09-30</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">82339</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1777" target="n1778">
      <data key="e_effectiveDate">2000-11-01</data>
      <data key="e_title">FRG: Multiscale Simulation of Atomistic Processes in Nanostructured Materials</data>
      <data key="e_abstract">0085344&lt;br/&gt;Kalia&lt;br/&gt;This award supports a Focused Research Group at Louisiana State University for research and education on computational materials. The grant is jointly supported by the Division of Materials Research and the Division for Advanced Computational Infrastructure and Research and is a blend of condensed matter physics, materials science and computer science. The objective of the research is to understand how the bonding between dissimilar materials at the atomic level determines structure and macroscopic properties such as adhesion, friction, stiffness, and fracture toughness. The research will focus on: (1) ceramic composites (SiC fibers coated with silica in a Si3N4 matrix and aluminum oxide matrix containing aluminum oxide fibers coated with LaPO4); (2) metal/ceramic interfaces (Al/SiC and Ti/TiO2) and nanostructured composites of passivated metallic nanoparticles; and (3) oxidation, fracture and nanoindentation in these materials.&lt;br/&gt;&lt;br/&gt;These applications require a methodology that can describe physical and mechanical processes over several decades of length scales. Quantum mechanical (QM) simulations based on the density functional theory will be preformed in regions where atomic bonds are formed or broken; molecular dynamics (MD) simulations will be carried out in nonlinear regions surrounding the QM region; and the finite-element (FE) approach with constitutive input from QM or MD calculations will be used in regions far away from the process zones. The QM, MD, and FE schemes will be integrated with an approach based on control theory. Algorithms will be designed to carry out these hybrid QM/MD/FE simulations in a metacomputing environment with multiple parallel machines, mass storage devices, and immersive and interactive virtual environments on a Grid with high-speed networks.&lt;br/&gt;&lt;br/&gt;The Concurrent Computing Laboratory for Materials Simulation, where the research will be performed, has a record of innovative educational activities including a joint MS/PhD program in computer science and physics. Efforts are underway for a joint masters degree in computer science and applied physics. In addition, a web-based computational physics course is being taught simultaneously at LSU and the Delft University of Technology in The Netherlands. As part of this grant, a workshop will be established to mentor and recruit minority students.&lt;br/&gt;%%%&lt;br/&gt;This award supports a Focused Research Group at Louisiana State University for research and education on computational materials. The grant is jointly supported by the Division of Materials Research and the Division for Advanced Computational Infrastructure and Research and is a blend of condensed matter physics, materials science and computer science. The objective of the research is to understand how the bonding between dissimilar materials at the atomic level determines structure and macroscopic properties such as adhesion, friction, stiffness, and fracture toughness. &lt;br/&gt;&lt;br/&gt;The Concurrent Computing Laboratory for Materials Simulation, where the research will be performed, has a record of innovative educational activities including a joint MS/PhD program in computer science and physics. Efforts are underway for a joint masters degree in computer science and applied physics. In addition, a web-based computational physics course is being taught simultaneously at LSU and the Delft University of Technology in The Netherlands. As part of this grant, a workshop will be established to mentor and recruit minority students.&lt;br/&gt;***</data>
      <data key="e_pgm">1765</data>
      <data key="e_label">85344</data>
      <data key="e_expirationDate">2003-05-31</data>
      <data key="e_div">0307</data>
      <data key="e_awardID">85344</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1777" target="n1779">
      <data key="e_effectiveDate">2000-11-01</data>
      <data key="e_title">FRG: Multiscale Simulation of Atomistic Processes in Nanostructured Materials</data>
      <data key="e_abstract">0085344&lt;br/&gt;Kalia&lt;br/&gt;This award supports a Focused Research Group at Louisiana State University for research and education on computational materials. The grant is jointly supported by the Division of Materials Research and the Division for Advanced Computational Infrastructure and Research and is a blend of condensed matter physics, materials science and computer science. The objective of the research is to understand how the bonding between dissimilar materials at the atomic level determines structure and macroscopic properties such as adhesion, friction, stiffness, and fracture toughness. The research will focus on: (1) ceramic composites (SiC fibers coated with silica in a Si3N4 matrix and aluminum oxide matrix containing aluminum oxide fibers coated with LaPO4); (2) metal/ceramic interfaces (Al/SiC and Ti/TiO2) and nanostructured composites of passivated metallic nanoparticles; and (3) oxidation, fracture and nanoindentation in these materials.&lt;br/&gt;&lt;br/&gt;These applications require a methodology that can describe physical and mechanical processes over several decades of length scales. Quantum mechanical (QM) simulations based on the density functional theory will be preformed in regions where atomic bonds are formed or broken; molecular dynamics (MD) simulations will be carried out in nonlinear regions surrounding the QM region; and the finite-element (FE) approach with constitutive input from QM or MD calculations will be used in regions far away from the process zones. The QM, MD, and FE schemes will be integrated with an approach based on control theory. Algorithms will be designed to carry out these hybrid QM/MD/FE simulations in a metacomputing environment with multiple parallel machines, mass storage devices, and immersive and interactive virtual environments on a Grid with high-speed networks.&lt;br/&gt;&lt;br/&gt;The Concurrent Computing Laboratory for Materials Simulation, where the research will be performed, has a record of innovative educational activities including a joint MS/PhD program in computer science and physics. Efforts are underway for a joint masters degree in computer science and applied physics. In addition, a web-based computational physics course is being taught simultaneously at LSU and the Delft University of Technology in The Netherlands. As part of this grant, a workshop will be established to mentor and recruit minority students.&lt;br/&gt;%%%&lt;br/&gt;This award supports a Focused Research Group at Louisiana State University for research and education on computational materials. The grant is jointly supported by the Division of Materials Research and the Division for Advanced Computational Infrastructure and Research and is a blend of condensed matter physics, materials science and computer science. The objective of the research is to understand how the bonding between dissimilar materials at the atomic level determines structure and macroscopic properties such as adhesion, friction, stiffness, and fracture toughness. &lt;br/&gt;&lt;br/&gt;The Concurrent Computing Laboratory for Materials Simulation, where the research will be performed, has a record of innovative educational activities including a joint MS/PhD program in computer science and physics. Efforts are underway for a joint masters degree in computer science and applied physics. In addition, a web-based computational physics course is being taught simultaneously at LSU and the Delft University of Technology in The Netherlands. As part of this grant, a workshop will be established to mentor and recruit minority students.&lt;br/&gt;***</data>
      <data key="e_pgm">1765</data>
      <data key="e_label">85344</data>
      <data key="e_expirationDate">2003-05-31</data>
      <data key="e_div">0307</data>
      <data key="e_awardID">85344</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1778" target="n1779">
      <data key="e_effectiveDate">2000-11-01</data>
      <data key="e_title">FRG: Multiscale Simulation of Atomistic Processes in Nanostructured Materials</data>
      <data key="e_abstract">0085344&lt;br/&gt;Kalia&lt;br/&gt;This award supports a Focused Research Group at Louisiana State University for research and education on computational materials. The grant is jointly supported by the Division of Materials Research and the Division for Advanced Computational Infrastructure and Research and is a blend of condensed matter physics, materials science and computer science. The objective of the research is to understand how the bonding between dissimilar materials at the atomic level determines structure and macroscopic properties such as adhesion, friction, stiffness, and fracture toughness. The research will focus on: (1) ceramic composites (SiC fibers coated with silica in a Si3N4 matrix and aluminum oxide matrix containing aluminum oxide fibers coated with LaPO4); (2) metal/ceramic interfaces (Al/SiC and Ti/TiO2) and nanostructured composites of passivated metallic nanoparticles; and (3) oxidation, fracture and nanoindentation in these materials.&lt;br/&gt;&lt;br/&gt;These applications require a methodology that can describe physical and mechanical processes over several decades of length scales. Quantum mechanical (QM) simulations based on the density functional theory will be preformed in regions where atomic bonds are formed or broken; molecular dynamics (MD) simulations will be carried out in nonlinear regions surrounding the QM region; and the finite-element (FE) approach with constitutive input from QM or MD calculations will be used in regions far away from the process zones. The QM, MD, and FE schemes will be integrated with an approach based on control theory. Algorithms will be designed to carry out these hybrid QM/MD/FE simulations in a metacomputing environment with multiple parallel machines, mass storage devices, and immersive and interactive virtual environments on a Grid with high-speed networks.&lt;br/&gt;&lt;br/&gt;The Concurrent Computing Laboratory for Materials Simulation, where the research will be performed, has a record of innovative educational activities including a joint MS/PhD program in computer science and physics. Efforts are underway for a joint masters degree in computer science and applied physics. In addition, a web-based computational physics course is being taught simultaneously at LSU and the Delft University of Technology in The Netherlands. As part of this grant, a workshop will be established to mentor and recruit minority students.&lt;br/&gt;%%%&lt;br/&gt;This award supports a Focused Research Group at Louisiana State University for research and education on computational materials. The grant is jointly supported by the Division of Materials Research and the Division for Advanced Computational Infrastructure and Research and is a blend of condensed matter physics, materials science and computer science. The objective of the research is to understand how the bonding between dissimilar materials at the atomic level determines structure and macroscopic properties such as adhesion, friction, stiffness, and fracture toughness. &lt;br/&gt;&lt;br/&gt;The Concurrent Computing Laboratory for Materials Simulation, where the research will be performed, has a record of innovative educational activities including a joint MS/PhD program in computer science and physics. Efforts are underway for a joint masters degree in computer science and applied physics. In addition, a web-based computational physics course is being taught simultaneously at LSU and the Delft University of Technology in The Netherlands. As part of this grant, a workshop will be established to mentor and recruit minority students.&lt;br/&gt;***</data>
      <data key="e_pgm">1765</data>
      <data key="e_label">85344</data>
      <data key="e_expirationDate">2003-05-31</data>
      <data key="e_div">0307</data>
      <data key="e_awardID">85344</data>
      <data key="e_dir">03</data>
    </edge>
    <edge source="n1781" target="n1782">
      <data key="e_effectiveDate">2000-11-15</data>
      <data key="e_title">Workshop: Student Scholarship Program for the International Conference on Machine Learning (ICML 2001)</data>
      <data key="e_abstract">This is funding to subsidize expenses of student participants in a special poster session the PIs are organizing as part of the 18th annual International Conference on Machine Learning (ICML&apos;2001), which will be held June 28-July 1 in Williamstown, MA (the PIs are the Program Co-Chairs of the conference). This is the premier conference for machine learning research. The PIs&apos; goals are to integrate students into their research sub-communities and to give them a forum to talk to the leading researchers in the field about their PhD research. To this end, a poster session will be organized on one of the first nights of the conference where each sponsored student will have a poster. To ensure sufficient traffic, the PIs will require each program committee member to sign up ahead of time on the Web to see at least 5 posters that evening, on the basis of the students&apos; abstracts. The students will also get a one page abstract submission to the conference proceedings. The PIs feel this plan has benefits over the more traditional PhD workshops organized by other conferences, as detailed in the proposal. The PIs will choose among the applicants for the scholarships, on the basis of a submitted abstract and vita plus a letter of recommendation from the student&apos;s advisor. They expect to make 30-40 awards of approximately $500 apiece. The mentoring work will be distributed among the members of the program committee. The poster session will provide students with invaluable exposure to outside perspectives on their work at a critical time in their research. This is an inexpensive yet highly effective means of encouraging young and upcoming researchers in machine learning.</data>
      <data key="e_pgm">6856</data>
      <data key="e_label">4495</data>
      <data key="e_expirationDate">2002-02-28</data>
      <data key="e_div">0502</data>
      <data key="e_awardID">4495</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1791" target="n1792">
      <data key="e_effectiveDate">2000-12-01</data>
      <data key="e_title">ITW: Women, Minorities, and Technology</data>
      <data key="e_abstract">This CISE Information Technology Workforce (ITW) proposal requests funds to study the underrepresentation of females and minorities in IT careers. The study will use a theoretical framework based on expectancies, values, and achievement behaviors. It was developed to study why females are underrepresented in mathematics, physics, and engineering. For the past 17 years, this framework has been used to study educational and occupational choices among children, adolescents, and young adults, using longitudinal survey designs that include multiple indicators from children, their parents, teachers, and school records. As a result, there are three longitudinal data sets that will be used in the study. In addition, the necessary supplementary IT-related data will be collected and added to the Michigan Study of Adolescent Life Transitions (MSALT) data set. Part of this data will be collected via surveys and the rest via interviews. This project has the potential to provide valuable insights into the underrepresentation of women and minorities IT careers.</data>
      <data key="e_pgm">1397</data>
      <data key="e_label">89972</data>
      <data key="e_expirationDate">2005-11-30</data>
      <data key="e_div">0405</data>
      <data key="e_awardID">89972</data>
      <data key="e_dir">04</data>
    </edge>
    <edge source="n1793" target="n1794">
      <data key="e_effectiveDate">2000-12-01</data>
      <data key="e_title">KDI: Automated Learning in Network Traffic Control</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">1271</data>
      <data key="e_label">196397</data>
      <data key="e_expirationDate">2003-09-30</data>
      <data key="e_div">0503</data>
      <data key="e_awardID">196397</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1796" target="n1797">
      <data key="e_effectiveDate">2000-12-01</data>
      <data key="e_title">Test Solutions for Next-Generation Embedded Memories</data>
      <data key="e_abstract">This project involves the development of new fault models and on-chip test methods for large embedded memories in systems-on-chip environments. The fault models are being developed using multidimensional device modeling and simulation tools that provide dimensional control over individual process layers. These fault models are validated with physical experimentation. The on-chip test methods include combining the dynamic power supply current, iDDT, with traditional deterministic BIST to reduce defect escapes. High-speed iDDT testing circuits are being developed to catch frequency-dependent faults. This project is being developed in collaboration with Intel Corporation.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">9.91241e+06</data>
      <data key="e_expirationDate">2005-11-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.91241e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1796" target="n1798">
      <data key="e_effectiveDate">2000-12-01</data>
      <data key="e_title">Test Solutions for Next-Generation Embedded Memories</data>
      <data key="e_abstract">This project involves the development of new fault models and on-chip test methods for large embedded memories in systems-on-chip environments. The fault models are being developed using multidimensional device modeling and simulation tools that provide dimensional control over individual process layers. These fault models are validated with physical experimentation. The on-chip test methods include combining the dynamic power supply current, iDDT, with traditional deterministic BIST to reduce defect escapes. High-speed iDDT testing circuits are being developed to catch frequency-dependent faults. This project is being developed in collaboration with Intel Corporation.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">9.91241e+06</data>
      <data key="e_expirationDate">2005-11-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.91241e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1797" target="n1798">
      <data key="e_effectiveDate">2000-12-01</data>
      <data key="e_title">Test Solutions for Next-Generation Embedded Memories</data>
      <data key="e_abstract">This project involves the development of new fault models and on-chip test methods for large embedded memories in systems-on-chip environments. The fault models are being developed using multidimensional device modeling and simulation tools that provide dimensional control over individual process layers. These fault models are validated with physical experimentation. The on-chip test methods include combining the dynamic power supply current, iDDT, with traditional deterministic BIST to reduce defect escapes. High-speed iDDT testing circuits are being developed to catch frequency-dependent faults. This project is being developed in collaboration with Intel Corporation.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">9.91241e+06</data>
      <data key="e_expirationDate">2005-11-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.91241e+06</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1660" target="n1799">
      <data key="e_effectiveDate">2000-12-31</data>
      <data key="e_title">Integrated Multicast for Ad Hoc Networks (IMAHN-NSF)</data>
      <data key="e_abstract"></data>
      <data key="e_pgm">1083</data>
      <data key="e_label">196418</data>
      <data key="e_expirationDate">2003-12-31</data>
      <data key="e_div">0505</data>
      <data key="e_awardID">196418</data>
      <data key="e_dir">05</data>
    </edge>
    <edge source="n1801" target="n1802">
      <data key="e_effectiveDate">2000-12-15</data>
      <data key="e_title">Development of Efficient and Accurate Numerical Techniques for the Design of Embedded Three Dimensional RF Components</data>
      <data key="e_abstract">This project is on fast and accurate computational techniques for the design of 3D embedded RF components. The numerical method to be investigated is based on a full-wave Mixed Potential Integral Equation formulation that has the capability of providing accurate modeling of 3-D interactions and handling arbitrary multi-layered media with different dielectric constants. Critical mathematical and algorithmic breakthroughs will be explored in the areas of fast calculation of multi-layered dyadic Green&apos;s functions over substrate, fast matrix solution of impedance matrices, high order basis functions, and fast frequency sweep techniques. Promising results from our initial investigations indicate that special numerical techniques superior to the techniques used in present software tools can be developed for 3D RF components in multi-layered media, and may lead to significant impact to the design of wireless systems. This project will also provide an opportunity for close interactions among the university team members and National Semiconductor Corporation - the industrial partner. Such an effort will strengthen the graduate education in both engineering and applied mathematics at participating universities, while industry will be benefited by potential fundamental breakthroughs of innovative and exploring research at universities.</data>
      <data key="e_pgm">4710</data>
      <data key="e_label">9.98838e+06</data>
      <data key="e_expirationDate">2003-11-30</data>
      <data key="e_div">0501</data>
      <data key="e_awardID">9.98838e+06</data>
      <data key="e_dir">05</data>
    </edge>
  </graph>
</graphml>
