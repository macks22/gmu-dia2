{"74508":{"abstract":"EIA-0219061<br\/>Marcotte, Edward M<br\/>University of Texas<br\/><br\/>ITR: Development of novel computational methods for genome-wide discovery of gene function and networks<br\/><br\/>Of the roughly 40,000 genes encoded by the human genome, as with every other genome sequenced to date, about half are completely uncharacterized and of unknown function. There is a broad need for methods to discover the functions of these thousands of uncharacterized genes, as well as how the gene products participate in networks, pathways and systems in the cell. This project's goal is to develop novel computational methods for discovering the functions of genes on a genome-wide scale, and to discover how the genes are organized into systems and pathways. Several novel methods are proposed capable of providing such information. The essence of these methods, called \"non-homology\" methods, is that they analyze contextual properties of genes-such as which organisms the genes appear in, which genes can be found fused together, and other properties-in addition to the sequences of the genes. Such contextual properties turn out to provide a tremendous increase in the ability of computational methods to discover gene function and gene networks. At the core of this proposal is a novel graph-based method for associating genes together that operate in the same cellular pathway, essentially resulting in a \"functional map\" of the genes. This method is combined with a new computational method for finding specific physical interaction partners for proteins in large sequence families, and with methods to visualize the resultant complex gene networks. Application of these methods should improve our understanding of the functions of the thousands of uncharacterized genes in each sequenced genome.","title":"ITR: Development of Novel Computational Methods for Genome-Wide Discovery of Gene Function and Networks","awardID":"0219061","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["526110"],"PO":["565136"]},"75619":{"abstract":"EIA-0224417<br\/>Aravind K. Joshi<br\/>Mark Liberman<br\/>University of Pennsylvania<br\/><br\/>CISE RR: Discourse Penn Trebank and Multimodal FORM: Development of Two Richly Annotated Corpora<br\/><br\/>This project, providing critical resources for research discourse modeling and conversational interaction, aims at developing new technologies and systems for information retrieval and human computer interaction. Centering on the construction of annotated corpora, two large-scale resources, one in the discourse domain and one in the dialog domain will be built:<br\/><br\/>1. Discourse Penn Treebank (DPTB) and<br\/>2. MultiFORM: Augmenting the FORM corpus with body movements, speech, and intonation.<br\/><br\/>The former project develops a large scale and reliably annotated corpus that will encode coherence relations associated with discourse connectives, including their argument structure and anaphoric links, thus exposing a clearly defined level of discourse structure and supporting the extraction of a range of inferences associated with discourse connectives. This annotation will be \"on top of\" the Penn Treebank (PTB) annotations as well as the predicate-argument annotations of PTB (called the Proposition Bank or Prop Bank). The latter involves a corpus of gesture-annotated videos, FORM that was designed to be extensible in order to eventually represent the entire multimodal experience of conversational interaction. This multimodal FORM , MultiFORM, will be created by adding body movement, speech and syntactic structure, and intonation. Large-scale annotated corpora have played a critical role in speech and natural language research by enabling large-scale integration of statistical knowledge (derived from the corpora) with linguistic knowledge (as represented in annotations) leading to scientific and technological advances. Representative examples constitute robust parsing and automatic extraction of relations and coreferences and their applications to information extraction, question answering, summarization, and machine translation. PTB, a resource developed a decade ago, represents an example of such a resource that impacts natural language processing worldwide. PTB deals with corpora at the sentence level warranting a new large scale and reliable discourse and dialog structure annotated corpora. Although intellectual and practical connections exist between studies of the structures of discourse and dialog, the initial requirements for resources to study these areas diverge while overlapping in conception. On the discourse side, we need for corpora that deals with the kinds of structures found in composed text such as journalistic articles. The dialog side needs to focus on interactions among people and on extemporized rather than pre-composed material.","title":"CISE Research Resources: Discourse Penn Treebank and Multimodal FORM: Development of Two Richly Annotated Corpora","awardID":"0224417","effectiveDate":"2002-10-15","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2890","name":"CISE RESEARCH RESOURCES"}}],"PIcoPI":["472144","507484"],"PO":["557609"]},"70704":{"abstract":"EIA-0203908 Craig D. Chambers University of Washington Efficient, Adaptable Software via Staged Compilation<br\/><br\/>A common strategy for supporting adaptable software is to use a just-in-time run-time compilation model, as in Java. However, this approach sacrifices performance, since any run-time optimizations must be quick and therefore simple to avoid slowing the program, with excessive compilation overhead. To achieve high performance, particularly for highly modular software, a whole-program static compiler can be used. But this strategy sacrifices run-time extension and adaptability, and forgoes the usual benefits of separate compilation as well.<br\/><br\/>This research is investigating a more flexible approach, staged compilation, that strives to combine the high performance advantages of static compilation with the flexibility advantages of dynamic compilation. In the staged compilation model, each part of a program passes through multiple compiler stages on its journey from source code to optimized machine code, including stages at separate compilation time, library link time, complete-program link time, and run-time","title":"NGS: Efficient, Adaptable Software via Staged Compilation","awardID":"0203908","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2884","name":"NEXT GENERATION SOFTWARE PROGR"}}],"PIcoPI":["450774","332082"],"PO":["301532"]},"70727":{"abstract":"EIA-0203974 Chrisochoides, Nikos P. College of William and Mary Mesh Generation and Optimistic Computation on the Grid . The goal of this proposal is to investigate exactly these issues using an identified application as a test case. Mesh generation and dynamic manipulation is an integral part of many important scientific computing applications and must take place on the same platform with the rest of the application. Mesh generation is a memory-intensive application with major impact on the overall performance of the end-to-end field simulations-an important class of applications that could benefit greatly from a grid platform. Traditional approaches for generating, partitioning, and placing very large meshes have tow weakness when used on grids: (i) I\/O and data movement due to mesh re-partitioning, for adaptive applications, is prohibitively expensive, and (ii) there is a trade-off between the performance of field solvers and the quality of the resulting elements and partitions.","title":"NGS: Mesh Generation and Optimistic Computation on the Grid","awardID":"0203974","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2884","name":"NEXT GENERATION SOFTWARE PROGR"}}],"PIcoPI":["561580","335186","283354"],"PO":["301532"]},"70718":{"abstract":"Title: Collaborative Research: Continuous Compilation: A New Approach to Aggressive and Adaptive Code Transformation<br\/> <br\/> Today's challenge for optimization research is to develop new techniques and approaches that yeild performance improvements that were typical of early optimization research-20 to 40 percent and more. In this research, we address this challenge by investigation and developing an innobative system or applying optimizations. Our system, the Continuous Compilation System (COCO), applies optimizations both statically at compile-time and dynamically at run-time using optimization plans developed at compile time and adapted at run time.<br\/><br\/> To demonstrate the practicality and utility of our approach, initially we will apply COCO to several large, long-running applications. Examples of such applications include ecological simulations, weather simulation, architecture simulators, and VLSI routing and placement applications. As we gain insight and experience with the application of continuous compilation, we will investigate its effectiveness on other types of codes such as multimedia applications, signal processing applications, and web servers.","title":"NGS: Collaborative: Continuous Compilation: A New Approach to Aggressive and Adaptive Code Transformation","awardID":"0203945","effectiveDate":"2002-10-01","expirationDate":"2004-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2884","name":"NEXT GENERATION SOFTWARE PROGR"}}],"PIcoPI":["438734","560516"],"PO":["301532"]},"60807":{"abstract":"Title: NGS: Efficient Script-Based Application Development for Networked High Performance Computing Environments.<br\/><br\/>This action will continue the research on the GrADS technologies for building Grid applications and to expand their applicability to new classes of applications. In particular, the work will concentrate on workflow- style applications from mesoscale weather forecasting and computational biology. In addition, it will explore enhanced technologies for automatic construction of grid applications from high-level specifications, along with new mechanisms for performance monitoring and rescheduling. The results will be demonstrated on new versions of the two GrADS testbeds.","title":"NGS: GrADS: Efficient Script-Based Application Development for Networked High Performance Computing Environments","awardID":"0103759","effectiveDate":"2002-10-01","expirationDate":"2004-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["24590","229226","517356","309293"],"PO":["301532"]},"75090":{"abstract":"Interdomain routing performs the critical function of gluing together individual pieces of the Internet topology to create a connected data delivery infrastructure. Today this critical function is performed by the Border Gateway Protocol (BGP) [rfc1771] which establishes reachability information among Autonomous Systems (ASes). However despite its importance, current measurements and analysis have not led to a basic understanding of BGP's dynamics, performance under stress, fundamental weaknesses, and potential breaking points (if any). Although a few data collection points have been set up in the last few years <br\/>[ipe,routeviews], the routing data collected by these measurement points are mixed with measurement artifacts [ftntalk], thus the data do not necessarily reflect the protocol's behavior in actual operation. <br\/> <br\/>In order for the Internet to continue its unprecedented growth, the interdomain routing protocol must continue to evolve to meet ever increasing and sometimes contradictory requirements. There is a general belief that the current BGP routing protocol may be unable to meet its new requirements (for instance, accomodating the sharp increase in use of site multi-homing, which keeps routing tables from optimally small sizes[huston:scale:2001]). BGP is generally thought to be reaching the end of its useful lifetime, although this has not been validated by analysis or measurements [nimrod,irtfrr,huitema:ipng,huston:scale:2001]. Due to the lack of a shared understanding of the problem and lack of sufficient data and analysis, there is no consensus on where\/when BGP collapses and what (if anything) should be done. <br\/> <br\/>To address the above critical questions facing interdomain routing, the researchres have assembled a team with research and operational experience, and expertise in network protocols, algorithms, modeling and analysis. The resarchers have identified the following fundamental technical requirements that the global routing must meet: it must scale in order to handle the growth (both in the number of users and in the richness of connectivity); security and resilience are critical issues, so it must continue to function in face of ever increasing faults and attacks; it must be able to fully utilize the rich Internet connectivity; and it must both allow network operators to apply various policy constraints and implementors to easily extend the protocol's functionality when needed. <br\/> <br\/>Based on the above criteria the researchers propose to tackle the challenge with the following 3 steps. (1) Develop measurement methodologies and collect data necessary to understand the current BGP operation, its overhead, dynamics under stress, potential vulnerabilities, inadequacies in functionality. The research will base this measurement effort on precise requirements that isidentiied as lacking in existing data, such as for the data not to be collected over vulnerable multihop links [ftntalk]. A new effort at University of Oregon, separate from this proposal, is the measurement companion, if funded. (2) Guided by our measurement and analysis, evaluate several proposed design approaches, including meeting the requirements by tinkering with BGP, by a NIMROD-like [nimrod] maps-approach, by two different approaches to handling multihoming scalability, and by a Clean Slate approach of a complete BGP replacement. Each of these approaches emphasizes different aspects of the interdomain routing problem. The researchers believe there are fundamental trade-offs between many of the desired technical requirements and that these trade-offs are currently not well understood. The combination of measurement and rigorous analysis with a team including operations expertise will bring these trade-offs into clear view. (3) Based on the data analysis and design evaluations the researchers will produce a final approach as the recommendation for moving forward. <br\/> <br\/>Through iterations of the above steps, the proposed research undertaking is expected to produce new understanding of current interdomain routing operations, their dynamics and resilience (or lack of it), and vulnerabilities; a new analysis will also be produced that draws on direct and intensive measurement and operations knowledge to capture the fundamental trade-offs among interdomain routing requirements; and a conclusion will be reached on how to meet the future Internet's interdomain routing needs.","title":"Collaborative Research: Beyond BGP: Flexible and Scalable Interdomain Routing (BGGP)","awardID":"0221435","effectiveDate":"2002-10-01","expirationDate":"2007-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4095","name":"SPECIAL PROJECTS IN NET RESEAR"}}],"PIcoPI":[195191],"PO":["557315"]},"79041":{"abstract":"EIA- 0240689<br\/>Edwin Rood<br\/>West Virginia University<br\/><br\/>Title: Workshop for a Biometric Research Agenda<br\/><br\/>The objective of this workshop is to develop a consensus for a biometric research agenda by bringing together the national experts in various disciplines converging to the field of biometrics. The disciplines include natural science and engineering, social science, political science, economics, and other disciplines.<br\/><br\/>The workshop will address the following topics:<br\/><br\/>What biometric technologies are currently deployed, currently available but not yet deployed, and in development that could be deployed in the foreseeable future?<br\/><br\/>What measures of effectiveness are required to ensure that these technologies help to provide security?<br\/><br\/>What are the economic verses effectiveness trade-offs of implementing these technologies?<br\/><br\/>What are the implications of biometric technologies for personal security and preservation of individual liberties, and for lifestyle within our society?","title":"Workshop for a Biometric Research Agenda; Morgantown, WV; Spring 2003","awardID":"0240689","effectiveDate":"2002-10-01","expirationDate":"2004-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1994","name":"BIOINFORMATICS PROGRAM"}}],"PIcoPI":["334363",205829],"PO":["565136"]},"68174":{"abstract":"The study of computer networks, while a relatively young science, is no different from any scientific inquiry and as such needs to be built on top of a theory-validated-by-experiment foundation. In this award we focus on the use of computer-based simulation which provides the greatest flexibility in creating a general experimentation environment. Simulation offers several important challenges, however. First is the fact that the Internet is an ever changing, highly heterogeneous environment that is typically very difficult to model. Second is the question of how to faithfully reflect the large scale of the Internet in a computer-based simulation. We are primarily interested in addressing this latter challenge in this research. Our interest in large-scale network simulation is motivated by the fact that we view network simulation as an important tool to understand the true effect of a new protocol, mechanism, network service, or application when widely deployed on a large network such as the Internet. It is clearly problematic to simply extend conclusions derived from small simulations and make inferences regarding effect and behavior when extended to a large scale.<br\/><br\/>While simulation of networks at the scale of the Internet remains infeasible today, there has been considerable progress made in the last few years in increasing the scale capability of network simulations. Our work uses this improved scaling capability as a starting point. This award represents an effort to go beyond the question of how to perform a large scale simulation run and address the question of how to perform a large-scale simulation experiment. A simulation experiment typically consists of multiple related runs and is also defined by a specific validation and\/or evaluation objective (e.g., what will happen if an ISP enables a certain queue management algorithm in all its routers). We note that our work focuses primarily on packet-level discrete-event simulation, as opposed to other approaches that allow scaling through aggregation and approximation. Our goal is to understand the limits of faithful packet-level discrete event simulation as a vehicle for experimentation and validation.<br\/><br\/>With the goal of investigating the question of how to perform large-scale simulation experiments we focus on two principal areas of research:<br\/><br\/> <br\/>1. We will develop and analyze techniques that allow for the efficient execution of multiple network simulation runs. A key observation is that the multiple runs that make up an experiment are often related. Exploiting this fact, we will develop techniques to allow computations from one run to be reused in other runs, thereby reducing the amount of time to complete a set of runs. Our preliminary work demonstrates the feasibility of this approach. We note that while many of our basic ideas can be applied to simulations in general, the specifics of their efficient instantiation is very much domain dependent. Much of the research in this project will focus on exploiting this idea in the context of network simulations for specific, important classes of network protocols and architectures. We consider two specific approaches, one based on the use of updateable simulations and the other based on simulation cloning. <br\/><br\/>2. Our second area of research uses as a starting point a presumed ability to perform large-scale network simulation experiments. The main question we ask is how large should a simulation experiment be in order for one to reach correct ``Internet-scale\" conclusions? This is admittedly a very hard question to answer. We feel, however, that recent advances in large-scale network simulations have made it possible to finally begin to address this question. We are now capable of considering the effect of non-trivial scaling of a network simulation on the results derived from it. The goal of this work is to develop and analyze a systematic, well-motivated approach for answering the \"how large is large-enough?\" question. We present a strawman approach which will represent the starting point of our investigation. We also provide experimental evidence demonstrating how scale can affect simulation results.","title":"Large-Scale Computer Network Experimentation Through Simulation","awardID":"0136939","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1766","name":"STRATEGIC TECH FOR INTERNET"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4095","name":"SPECIAL PROJECTS IN NET RESEAR"}}],"PIcoPI":["536648","550428","434457"],"PO":["543507"]},"75093":{"abstract":"Interdomain routing performs the critical function of gluing together individual pieces of the Internet topology to create a connected data delivery infrastructure. Today this critical function is performed by the Border Gateway Protocol (BGP) [rfc1771] which establishes reachability information among Autonomous Systems (ASes). However despite its importance, current measurements and analysis have not led to a basic understanding of BGP's dynamics, performance under stress, fundamental weaknesses, and potential breaking points (if any). Although a few data collection points have been set up in the last few years <br\/>[ipe,routeviews], the routing data collected by these measurement points are mixed with measurement artifacts [ftntalk], thus the data do not necessarily reflect the protocol's behavior in actual operation. <br\/><br\/>In order for the Internet to continue its unprecedented growth, the interdomain routing protocol must continue to evolve to meet ever increasing and sometimes contradictory requirements. There is a general belief that the current BGP routing protocol may be unable to meet its new requirements (for instance, accomodating the sharp increase in use of site multi-homing, which keeps routing tables from optimally small sizes[huston:scale:2001]). BGP is generally thought to be reaching the end of its useful lifetime, although this has not been validated by analysis or measurements [nimrod,irtfrr,huitema:ipng,huston:scale:2001]. Due to the lack of a shared understanding of the problem and lack of sufficient data and analysis, there is no consensus on where\/when BGP collapses and what (if anything) should be done. <br\/><br\/>To address the above critical questions facing interdomain routing, the researchres have assembled a team with research and operational experience, and expertise in network protocols, algorithms, modeling and analysis. The resarchers have identified the following fundamental technical requirements that the global routing must meet: it must scale in order to handle the growth (both in the number of users and in the richness of connectivity); security and resilience are critical issues, so it must continue to function in face of ever increasing faults and attacks; it must be able to fully utilize the rich Internet connectivity; and it must both allow network operators to apply various policy constraints and implementors to easily extend the protocol's functionality when needed. <br\/><br\/>Based on the above criteria the researchers propose to tackle the challenge with the following 3 steps. (1) Develop measurement methodologies and collect data necessary to understand the current BGP operation, its overhead, dynamics under stress, potential vulnerabilities, inadequacies in functionality. The research will base this measurement effort on precise requirements that is identiied as lacking in existing data, such as for the data not to be collected over vulnerable multihop links [ftntalk]. A new effort at University of Oregon, separate from this proposal, is the measurement companion, if funded. (2) Guided by our measurement and analysis, evaluate several proposed design approaches, including meeting the requirements by tinkering with BGP, by a NIMROD-like [nimrod] maps-approach, by two different approaches to handling multihoming scalability, and by a Clean Slate approach of a complete BGP replacement. Each of these approaches emphasizes different aspects of the interdomain routing problem. The researchers believe there are fundamental trade-offs between many of the desired technical requirements and that these trade-offs are currently not well understood. The combination of measurement and rigorous analysis with a team including operations expertise will bring these trade-offs into clear view. (3) Based on the data analysis and design evaluations the researchers will produce a final approach as the recommendation for moving forward. <br\/><br\/>Through iterations of the above steps, the proposed research undertaking is expected to produce new understanding of current interdomain routing operations, their dynamics and resilience (or lack of it), and vulnerabilities; a new analysis will also be produced that draws on direct and intensive measurement and operations knowledge to capture the fundamental trade-offs among interdomain routing requirements; and a conclusion will be reached on how to meet the future Internet's interdomain routing needs.","title":"Collaborative Research: Beyond BGP: Flexible and Scalable Interdomain Routing (BGGP)","awardID":"0221453","effectiveDate":"2002-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4095","name":"SPECIAL PROJECTS IN NET RESEAR"}}],"PIcoPI":["543560"],"PO":["7594"]},"72190":{"abstract":"This project leverages advances in statistical learning theory, machine vision, and massively parallel very-large-scale-integration technology to develop a custom-trainable, versatile, self-contained, and mobile system for visually impaired users. The system will aid the user in interacting freely with other people and the environment, by rapidly detecting and localizing key visual environmental cues and rapidly recognizing and identifying familiar people and objects. At the core of the system is the \"Kerneltron\", a massively parallel Support Vector \"Machine\" (SVM) in silicon. The SVM hardware will be trained on-line by the end user to accommodate a variety of visual detection and recognition tasks in everyday situations through presentation of examples. The recognition core will be embedded in a portable prototype visual aid, interfacing with a CCD camera front-end, and an audio synthesizer back-end. Menu-driven keypad control will allow direct input and feedback from the user in training and directing the system. The user interface will be based on \"OpenEyes\", a wearable computer vision system for the blind. Proof of concept demonstration of the hardware system and evaluation of the training and test performance will be conducted with feedback from volunteer impaired users.","title":"Trainable Visual Aids for Object Detection and Identification","awardID":"0209289","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6845","name":"HUMAN COMPUTER INTER PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6846","name":"UNIVERSAL ACCESS"}}],"PIcoPI":["523294","549477",186939,186940],"PO":["565227"]},"74280":{"abstract":"EIA-0217879<br\/>Northwestern University<br\/>Ferster, David<br\/><br\/> Collaborative Research: CRCNS: Detection and Recognition of Objects in Visual Cortex<br\/><br\/>A three way collaboration between the laboratories of Profs. T. Poggio at MIT, D. Ferster at Northwestern University and C. Koch at Caltech is exploring and evaluating the hypotheses that the cortical organization and the neural mechanisms of visual recognition can be explained by a coherent theoretical framework built on two existing computational models for recognition and attention and, secondly, that a combination of physiological work on monkeys and cats, together with visual psychophysics can be used to test and refine the theory. The research is organized into three main projects. The work at MIT is guided by a quantitative hierarchical model of recognition, probing the relations between identification and categorization and the properties of selectivity and invariance of the neural mechanisms in IT cortex. The work at Northwestern University is testing a key prediction of the model about the nature of the pooling operation (a max operation vs. a linear sum) performed by complex cells in V1. The experiments are done in the anesthetized cat, intracellularly, to allow for a characterization of the underlying circuit and biophysical mechanisms. Finally, work at Caltech is extending the basic model of recognition by integrating it with a saliency-based attentional model. The computational component of this work, centered around the development of a quantitative model of visual recognition, constitutes the primary tool to enforce interactions between the investigators: the model suggests experiments and guides planning and interpreting new experiments.","title":"Collaborative Research: CRCNS: Detection and Recognition of Objects in Visual Cortex","awardID":"0217879","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["193317"],"PO":["564318"]},"73191":{"abstract":"This project is related to the digitization of the Television News Archive at Vanderbilt University. The issues to be addressed during the project include:<br\/><br\/>1. Evaluation of alternate delivery technologies (streaming video methods and different kinds of video servers).<br\/><br\/>2. Technology for identifying program segments, and labeling the results of the segmentation.<br\/><br\/>3. Acquisition methods for future content acquisition.<br\/><br\/>4. Time stamp technology and uses.<br\/><br\/>5. Algorithms and processes for signal conditioning for increasing quality of the data archive.<br\/><br\/>The Vanderbilt archive is crucial for historians: it contains the only records of TV news for some years starting in 1968. However, it is currently stored on obsolete analog technology. This proposal takes advantage of the need to digitize the material to explore techniques that could be applied to many other kinds of video digitization projects.","title":"ITR: Television News Archieve: A Pilit to Explore Technologies and Methodologies for Digitization","awardID":"0213875","effectiveDate":"2002-10-01","expirationDate":"2004-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":[189619,189620],"PO":["562519"]},"76051":{"abstract":"The goal of this project is to bring together young and experienced college faculty to encourage scholarship in the teaching of computational linguistics. This would take place in a workshop on \"Effective Tools And Methodologies For Teaching Natural Language Processing And Computational Linguistics\" to be held during the annual conference of the Association for Computational Linguistics on July 7, 2002 in Philadelphia, PA. The workshop will enable new and prospective computational linguistics faculty to learn from their peers and to share resources related to teaching NLP and CL.","title":"Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics; Philadelphia, PA","awardID":"0226408","effectiveDate":"2002-10-01","expirationDate":"2003-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6845","name":"HUMAN COMPUTER INTER PROGRAM"}}],"PIcoPI":["466946"],"PO":["565227"]},"78880":{"abstract":"This project will organize and host a Workshop for all Principal Investigators on NSF-supported network research and infrastructure projects. The ultimate goal of the Workshop is to foster a sense of community and collaboration among NSF-supported network research and infrastructure development Principal Investigators (PI-participants).<br\/><br\/>The Workshop will be held in Washington D.C. during the fourth quarter of 2002 and will feature three main components: plenary session presentations on current issues and topics in network research, discussion groups on these same topics and poster session presentations. The PI will develop and maintain a Workshop website to provide attendees with up-to-date information about the Workshop and to also serve as the final presentation archive for all talks and papers presented at the Workshop. This PI will also be responsible for handling all aspects of setting up the workshop, communicating with the PI-participants, and reimbursing the PI-participants.","title":"ANIR Network PI Workshop; Washington, DC; Fourth Quarter 2002","awardID":"0239770","effectiveDate":"2002-10-01","expirationDate":"2003-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4090","name":"ADVANCED NET INFRA & RSCH"}}],"PIcoPI":["437955"],"PO":["565090"]},"75371":{"abstract":"This project will develop intelligent robotic tools and a telepresence environment for performing off-pump coronary artery bypass graft (CABG) surgery. In this context, off-pump CABG surgery means that the surgery is done while heart is still beating instead of using a cardiopulmonary bypass machine and stopping the heart to perform heart surgery. Although on-pump open heart surgery has been refined over the last 30 years and is now a common and successful procedure it remains highly invasive and fraught with potential complications. Due to the complications resulting from using cardiopulmonary bypass machine, performing CABG off-pump is highly desirable. However, off-pump CABG surgery can be potentially performed for only a small proportion (~10%) of all CABG surgeries due to technological limitations. Use of intelligent robotics technology promises an alternative and superior way of performing off-pump CABG surgery. The intelligent telerobotic tools that will be developed in this project will actively track and cancel the relative motion between the surgical instruments and the heart by Active Relative Motion Cancelling (ARMC), allowing CABG surgeries to be performed on a beating heart with technical perfection equal to traditional on pump procedures. <br\/><br\/>A major focus of the research will be on developing complementary sensing systems, and algorithms for intelligent and model based fusing of information supplied form different systems for superior performance, and model based confidence evaluation to detect and handle variations in heart rhythm and arrythmias. The sensory systems that will be studied include sonomicrometry, vision, whisker, force\/torque, and MEMS based inertia sensors.","title":"Intelligent Robotic Tools and Telepresence Environment for Off-Pump (Beating Heart) Coronary Artery Bypass Graft Surgery","awardID":"0222743","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}}],"PIcoPI":["526900"],"PO":["335186"]},"74282":{"abstract":"EIA-0217884<br\/>Robbins, Kay A<br\/>Univ of Texas <br\/><br\/>CRCNS: Collaborative Research : How is Information Coded in Turtle Visual Cortex ?<br\/><br\/>Visual stimuli evoke a propagating wave of activity in the visual cortex of freshwater turtles. Preliminary work suggested that information about the position of stimuli in visual space is coded in the spatiotemporal dynamics of these waves. Effectively, there may be a map of visual space to the dynamics of the visual cortex. This hypothesis is being examined in a collaborative effort involving three laboratories. David Senseman in San Antonio is using voltage sensitive dye methods to record the waves produced by presenting spots of light at 35 spots on the retina. These studies will characterize the features of the map based on repeated presentations of stimuli at 35 loci. Philip Ulinski in Chicago is developing a large-scale model of the visual pathway of turtles. Models of individual retinal ganglion cells that combine both classic filter-based approaches to modeling ganglion cells, with compartmental modeling of ganglion cells are being constructed. They are being used to construct 35 patches of a model retina that match the 35 loci. Physiological studies of the biophysics of neurons in the lateral geniculate complex of turtles are being carried out. They are used to develop a model of the lateral geniculate complex, which is the last step in modeling the retino-geniculate-cortical pathway. Bijoy Ghosh in St. Louis is developing refined estimation techniques that allow the position of a visual stimulus to be estimated from the dynamics of the cortical waves. This work is providing the mathematical framework needed to characterize a potential map of visual space to the dynamics of the wave. This work is significant because it is characterizing a novel method of coding information in visual cortex that may apply to higher order cortical areas in mammals, as well as turtles.","title":"CRCNS: Collaborative Research: How is Information Coded in Turtle Visual Cortex?","awardID":"0217884","effectiveDate":"2002-10-01","expirationDate":"2007-12-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["385349","385350"],"PO":["564318"]},"75041":{"abstract":"Interdomain routing performs the critical function of gluing together individual pieces of the Internet topology to create a connected data delivery infrastructure. Today this critical function is performed by the Border Gateway Protocol (BGP) [rfc1771] which establishes reachability information among Autonomous Systems (ASes). However despite its importance, current measurements and analysis have not led to a basic understanding of BGP's dynamics, performance under stress, fundamental weaknesses, and potential breaking points (if any). Although a few data collection points have been set up in the last few years <br\/>[ipe,routeviews], the routing data collected by these measurement points are mixed with measurement artifacts [ftntalk], thus the data do not necessarily reflect the protocol's behavior in actual operation. <br\/> <br\/>In order for the Internet to continue its unprecedented growth, the interdomain routing protocol must continue to evolve to meet ever increasing and sometimes contradictory requirements. There is a general belief that the current BGP routing protocol may be unable to meet its new requirements (for instance, accomodating the sharp increase in use of site multi-homing, which keeps routing tables from optimally small sizes[huston:scale:2001]). BGP is generally thought to be reaching the end of its useful lifetime, although this has not been validated by analysis or measurements [nimrod,irtfrr,huitema:ipng,huston:scale:2001]. Due to the lack of a shared understanding of the problem and lack of sufficient data and analysis, there is no consensus on where\/when BGP collapses and what (if anything) should be done. <br\/> <br\/>To address the above critical questions facing interdomain routing, the researchres have assembled a team with research and operational experience, and expertise in network protocols, algorithms, modeling and analysis. The resarchers have identified the following fundamental technical requirements that the global routing must meet: it must scale in order to handle the growth (both in the number of users and in the richness of connectivity); security and resilience are critical issues, so it must continue to function in face of ever increasing faults and attacks; it must be able to fully utilize the rich Internet connectivity; and it must both allow network operators to apply various policy constraints and implementors to easily extend the protocol's functionality when needed. <br\/> <br\/>Based on the above criteria the researchers propose to tackle the challenge with the following 3 steps. (1) Develop measurement methodologies and collect data necessary to understand the current BGP operation, its overhead, dynamics under stress, potential vulnerabilities, inadequacies in functionality. The research will base this measurement effort on precise requirements that isidentiied as lacking in existing data, such as for the data not to be collected over vulnerable multihop links [ftntalk]. A new effort at University of Oregon, separate from this proposal, is the measurement companion, if funded. (2) Guided by our measurement and analysis, evaluate several proposed design approaches, including meeting the requirements by tinkering with BGP, by a NIMROD-like [nimrod] maps-approach, by two different approaches to handling multihoming scalability, and by a Clean Slate approach of a complete BGP replacement. Each of these approaches emphasizes different aspects of the interdomain routing problem. The researchers believe there are fundamental trade-offs between many of the desired technical requirements and that these trade-offs are currently not well understood. The combination of measurement and rigorous analysis with a team including operations expertise will bring these trade-offs into clear view. (3) Based on the data analysis and design evaluations the researchers will produce a final approach as the recommendation for moving forward. <br\/> <br\/>Through iterations of the above steps, the proposed research undertaking is expected to produce new understanding of current interdomain routing operations, their dynamics and resilience (or lack of it), and vulnerabilities; a new analysis will also be produced that draws on direct and intensive measurement and operations knowledge to capture the fundamental trade-offs among interdomain routing requirements; and a conclusion will be reached on how to meet the future Internet's interdomain routing needs.","title":"Collaborative Research: Beyond BGP: Flexible and Scalable Interdomain Routing (BGGP)","awardID":"0221243","effectiveDate":"2002-10-01","expirationDate":"2004-10-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4095","name":"SPECIAL PROJECTS IN NET RESEAR"}}],"PIcoPI":["427568","335768","559197"],"PO":["250082"]},"77099":{"abstract":"ABSTRACT<br\/>0231511<br\/>Nicholas M. Pattrikalakis<br\/>Mass Inst. of Tech<br\/><br\/>We propose to study the topology of the union of a finite collection of boxes that covers an<br\/>intersection curve in the plane and in 3D space. The representation of curves that are the<br\/>result of surface intersections is nearly never exact, and thus certain approximations schemes are<br\/>employed. The result is often a piecewise linear approximation cl, which lies within a certain<br\/>neighborhood N, of the actual curve c. Much of the research in this area has been focused on<br\/>the approximation cl of c and how c and cl are\/are not similar. We propose to develop suficient<br\/>conditions on the collection of the boxes and the intersection curve, so that the resulting union<br\/>of the collection is a topological manifold homotopically equivalent to the given curve. In the<br\/>case where the curve has no self-intersections, this collection can serve as another means of<br\/>representing the curve, and thus this new representation can be used in a variety of engineering<br\/>applications. We believe this to be an innovative exploratory study appropriate for SGER<br\/>funding.","title":"SGER: Topological Issues of Intersection Curves","awardID":"0231511","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":[201136,"317581",201138],"PO":["321058"]},"71093":{"abstract":"The recent metamorphosis of the Internet-from a mere best-effort transport medium to an open communication and computation infrastructure necessitates the development of robust abstractions that facilitate its use to support a constantly increasing number of applications, in compliance with widely-accepted correctness standards that ensure a verifiably safe, fair, secure, and efficient access of Internet resources.<br\/><br\/>Programming new Internet applications and services suffers from the same lack of organizing principles that afflicted programming systems some thirty years ago. The researchers believe that recognizing network flows as the central abstraction around which to organize a programming system for the Internet is perhaps the key organizing principle. Speciffically, to rapidly experiment with and deploy a wide range of new services within the existing constraints of the Internet infrastructure, it is necessary to adopt a more powerful model for the naming, creation, composition, sharing, and processing of Internet flows.<br\/><br\/>Towards this goal the researchers promote Internet flows to first-class values, i.e., entities that are directly accessed, acted on and programmed according to needs. As a consequence, the task of developing an Internet application or service (e.g., a CDN broker or an end-system multicast) becomes the task of writing an appropriate program to manipulate Internet flows (along with other first-class values relevant to the application). To enable the speciffication and compilation of programs manipulating Internet flows, in particular application-specific Internet protocols, it is necessary to conduct basic research that encompasses two computer science disciplines: Networking Systems and programming Languages. Basic research in networking systems is needed to develop the core services that support the various abstractions to be presented to protocol programmers. Basic research in programming languages is needed to develop the primitives and formalisms that enable the expression of protocol properties and the verification of certain safety properties of the protocol.<br\/><br\/>The proposed work will focus on the following specific areas: (1) Transport Services to support various degrees of reliability, congestion management, and timeliness, (2) Routing Services to support various degrees of mobility and multicasting, (3) End-to-End Services to support various degrees of QoS and security, (4) Naming mechanisms to support flexible composition of Internet services, and (5) Type inference to support efficient and safe flow-oriented programming.<br\/>Towards the goals outlined above, the proposed research will be a collaborative effort carried primarily by members of established research groups in Networking Systems and in Programming Languages.<br\/><br\/>A key component of the proposed work is implementation and prototyping. To that end, the utility of a paradigm in which Internet flows are first-class values will be demonstrated by implementing NetBench a programming environment in which a core set of network flow types and operations will be supported, along with a type-checking and type-inference system to handle types of network flows.<br\/><br\/>The pursuit of the research goals outlined in this proposal is timely. Achieving these goals will leapfrog current piecemeal attempts aiming at supporting Internet growth. The researchers believe that the proposed effort will improve the flexibility, reliability, safety, and security of network software development environments, leading to economic efficiencies similar to the higher productivity of software developers arising from traditional programming language research. The research team assembled to pursue these ambitious goals has made significant, nationally-recognized contributions to research in Programming Languages and Internet Technologies and has an established record in software development and technology transfer. Boston University is committed to supporting this team through substantial financial and infrastructural commitments that complement and leverage the support sought from NSF.","title":"ITR: Internet Flows as First-Class Values: Support for Dynamic, Flexible Internet Services","awardID":"0205294","effectiveDate":"2002-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7363","name":"RES IN NETWORKING TECH & SYS"}}],"PIcoPI":["526817","562061","438360","313342","68374"],"PO":["565090"]},"79332":{"abstract":"ABSTRACT: <br\/>0241919<br\/>Paulraj, Arogyaswami<br\/>Stanford University<br\/><br\/>Recent work in multiple antenna communications (MIMO systems) has cleverly exploited rich scattering in the environment to build wireless links with multiple antennas that offer huge capacity gains. Systems studied so far are narrowband systems. Use of very wideband signals offers new ways to exploit rich scattering. Also, sensor systems that exploit rich scattering have attracted little attention. <br\/><br\/>The proposed research will explore multiple antenna signal processing applications for wireless communications and sensor systems that use wideband signaling in a rich scattering channel. Multiple antennas, wideband transmission and rich scattering, each provide a number of leverages, which must be carefully exploited. <br\/><br\/>While multiple antenna systems and rich scattering provide increased capacity, diversity and sensitivity, wideband transmission offers increased diversity, and, in the case of sensors, better resolution. The proposed research will study: <br\/> <br\/>1. Signal processing in wireless communications and sensor systems. <br\/>2. Fundamental performance limits (ergodic\/outage capacity, etc), space-time coding and receiver design for multiple antenna communication systems with wideband signaling in rich scattering environments. <br\/>3. Fundamental limits to resolution in sensor systems exploiting rich scattering and wideband signaling","title":"SGER: Communications and Sensing in Wideband Rich Scattering Environments","awardID":"0241919","effectiveDate":"2002-10-15","expirationDate":"2004-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"V474","name":"NSA-NEWLY EMERGING COMM SYS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"V011","name":"NATIONAL SECURITY AGENCY-NEWLY"}}],"PIcoPI":["535565"],"PO":["564898"]},"71171":{"abstract":"The volume of hypertext on the World Wide Web is dwarfed by the amount<br\/>of data made available in networked databases, which has been estimated to be 400 to 550 times larger than the WWW hypertext. This proposal refers to this data as the Federated Facts and Figures on the Internet, or simply the FFF. The goal of this work is to explore the mechanisms for -- and consequences of -- aggressively leveraging this resource.<br\/><br\/>The proposal has three aspects. First, it describes algorithms and systems for exploiting facts and figures on the Internet. In particular, it proposes adaptive query processing for the Telegraph system, to adjust to the volatility characteristic of the Internet. It also proposes extending Telegraph to ``trawl'' large amounts of data from the FFF, by running recursive queries over multiple data sources. The second goal of the proposal is to explore the ramifications of providing FFF tools to the broad Internet user base. This includes an investigation of policy -- both social and technical -- and the economic incentives and ramifications surrounding such policies. The third goal is to explore the design space of countermeasures that can prevent FFF technologies from being misused.<br\/><br\/>.","title":"ITR: Data on the Deep Web: Queries, Trawls, Policies and Countermeasures","awardID":"0205647","effectiveDate":"2002-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}}],"PIcoPI":["229257","229259","451018"],"PO":["469867"]},"75021":{"abstract":"This project proposes an investigation of the performance and continued viability of BGP system, including development of new techniques for correlating massive volumes of routing and topology data and tools to support modeling and prediction of routing behavior. The research will include evaluation of incongruity between topology and articulated routing policy, and modeling of peering policy dynamics.","title":"Routing and Peering Analysis for Enhancing Internet Performance and Security","awardID":"0221172","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4095","name":"SPECIAL PROJECTS IN NET RESEAR"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"V024","name":"DEFENSE-AMENDMENT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"V041","name":"DEF INFORMATION SYS AGENCY-"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"W586","name":"DEFENSE-INF OF INTERNET SOURCE"}}],"PIcoPI":["521741"],"PO":["7594"]},"88122":{"abstract":"The goal of this research project is to create an integrated, interactive, 3-D virtual environment consisting of anatomical parts, and physiological and pathophysiological processes. The approach consists of modeling: 1) Deformable soft tissue for anatomical organs, 2) Anatomical parts that are responsive to physiological changes, 3) Physiological behavior models, 4) Formal methods for integrating anatomical parts and physiological process variables and parameters, 5) Recovery of patient-specific data for anatomy (through computer vision techniques) and physiology, and 6) Real-time simulation and visualization methods. The results of this research will contribute: 1) New methodologies to enhance medical education through immersive visualization and interaction, and 2) Clinically-relevant and patient-specific models of acute injuries in trauma. The goal of the educational plan is the use of the software that will be developed from the proposed research in courses at the University of Pennsylvania. Through the exposure to a sophisticated and formal approach to science, new students are expected to appreciate and develop a deeper understanding of mathematics, physics, computer vision, computer graphics, and virtual environments, and their synthesis.","title":"Interactive Virtual Environment for Modeling Anatomy and Physiology","awardID":"0335126","effectiveDate":"2002-10-01","expirationDate":"2004-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6845","name":"HUMAN COMPUTER INTER PROGRAM"}}],"PIcoPI":["522465"],"PO":["565227"]},"77001":{"abstract":"The next explosive growth in networks will come from connecting together billions of low cost, low power sensors, effectors, and smart devices. These will be communicating primarily through wireless means for reasons of mobility, ease of deployment, aesthetics, and cost. Even if such networks start out as special purpose local networks, people anticipate that they will come together in many ways. They will certainly be interconnected with each other through gateways to the broader wired Internet. The goal in this proposed project is to find the right architecture to enable Internet-like gains in the new context of wireless connectivity. <br\/><br\/>In order for wireless networks to support a wide range of applications and be suitable for mass deployment, they will need to posses the following characteristics: (1) negligible interference that allows peaceful co-existence with other independent wireless systems operating over the shared spectrum; (2) managing interference between nodes to efficiently and fairly share bandwidth; (3) dynamic and energy efficient routing and packet relaying algorithms that support mobility of network nodes; (4) scalability to support a large number of heterogeneous devices and links; (5) precise positioning capabilities to provide location information for the devices for which this is important; (6) robust and energy efficient network protocols that tolerate failure of some network nodes; (7) extremely low power wireless transceivers to ensure longevity for the energy-limited nodes; and (8) small and low cost wireless transceivers to enable widespread deployment. <br\/><br\/>This proposal is to study the above in the context of ultra-wideband (UWB) wireless signaling and multi-hop routing. Research is intended to achieve multiple objectives, Develop <br\/>(1) Efficient algorithms to determine the fundamental tradeoffs involved in tracking the positions of devices within a network of heterogeneous nodes; <br\/>(2) Robust and efficient protocols for routing digital communications within such networks and explore the fundamental capacity limits of such systems; <br\/>(3) Distributed signal processing algorithms that are network-energy and position aware to take advantage of correlations at the application layer to reduce resource consumption throughout the network hierarchy; <br\/>(4) Extremely low power, highly integrated single-chip CMOS architecture to UWB transceivers, and (5) An integrated test environment by combining an in-house FPGA-based testbed (as the digital back-end) with UWB analog front-end from AetherWire Inc. This will ultimately have approximately 30 UWB nodes from Aether Wire Inc. to test and therefore further discover issues involved with such networks.","title":"Ultra-Wideband Based Next-Generation Wireless Networking","awardID":"0230963","effectiveDate":"2002-10-01","expirationDate":"2004-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1766","name":"STRATEGIC TECH FOR INTERNET"}}],"PIcoPI":["243579","521731","523728","560235"],"PO":["250082"]},"79333":{"abstract":"ABSTRACT<br\/>0241921<br\/>Paulraj, Arogyaswami<br\/>Stanford University<br\/><br\/>Space-time (multiple antenna) techniques offer a variety of leverages to improve wireless systems performance. These include diversity, spatial multiplexing and interference reduction. Wireless standards bodies are incorporating space-time techniques into new standards. However, many existing wireless systems such as GSM and IS-95 rely on a single transmit and receive antenna (SISO) and therefore cannot benefit from space-time technology. This proposal aims to develop a novel approach that brings many of the benefits of space-time technology to SISO wireless by introducing an intermediate node with multiple receive and multiple transmit antennas. We characterize a SISO link with an intermediate node as a SI(MM)SO link. In such links, there is also the possibility of a SISO link bypass to the SI(MM)SO link. <br\/><br\/>The research objectives are <br\/> <br\/>1. Characterizing Shannon channel capacity of SI(MM)SO channels with bypass. <br\/>2. Space-time processing schemes at the intermediate node. <br\/>3. Extensions of the SI(MM)SO idea to MIMO channels. <br\/> <br\/>While theoretical work is the primary goal of this proposal, experimental support is available and can help measure real SI(MM)SO channels.","title":"SGER: Wireless Links with Space-time Intermediate Nodes","awardID":"0241921","effectiveDate":"2002-10-15","expirationDate":"2004-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["535565"],"PO":["564898"]},"74692":{"abstract":"ABSTRACT<br\/>0220011<br\/>Djuric, Petar<br\/>SUNY @ Stony Brook<br\/><br\/>Optimization of Reconfigurable Architectures for Efficient Implementation of <br\/>Particle Filters<br\/><br\/>In recent years particle filters have attracted great attention in several research communities. These filters are used in problems where time-varying signals must be processed in real time and the objective is to estimate various unknowns of the signals and to detect events described by the signals. The standard solutions of such problems in many applications are based on the <br\/>Kalman or extended Kalman filters. In situations when the problems are highly nonlinear or the noise that distorts the signals is non-Gaussian, the Kalman filters provide solutions that may be far from optimal. <br\/>A major drawback of the particle filters is that their implementation is computationally very intensive. They are, however, inherently parallelizable, and special hardware can be built for their implementation that can meet the stringent requirements of real-time processing. In this research, <br\/>reconfigurable and physically feasible VLSI architectures for particle filters are developed. In the development of these architectures, many important problems are researched. The most critical of them is the balancing of hardware and software, which itself is tightly related to other important issues. They include reductions of computational complexities by transformations and <br\/>approximations, investigation of the degree of parallelism implemented in the filter, investigation of various interconnection mechanisms, random communication schemes, hardware optimization, and design of low power VLSI processors. This effort also includes building of reconfigurable hardware so that it is suitable for different types of particle filters.","title":"ITR: Optimization of Reconfigurable Architectures for Efficient Implementation of Particle Filters","awardID":"0220011","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4720","name":"SIGNAL PROCESSING SYS PROGRAM"}}],"PIcoPI":["563567","226063"],"PO":["564898"]},"65870":{"abstract":"The future high-speed networks are expected to support a variety of services such as data,<br\/>voice, image, and video with diverse traffic flow characteristics and Quality-of-Service (QoS)<br\/>requirements. Developing a framework for the design, control and management of wide<br\/>area packet networks which can effciently support such QoS requirements is a fundamental<br\/>challenge.<br\/><br\/>Conventional frameworks for the teletraffic analysis of networks and QoS guarantees are<br\/>either deterministic or probabilistic. In the deterministic approach, the worst-case traffic<br\/>envelopes are used. This most likely will lead to conservative resource allocation policies<br\/>and inefficient usage of the network resources. In the standard probabilistic frameworks,<br\/>the traffic is usually modeled as a variant of Poisson process, e.g., MMPP, MAP, etc. Such<br\/>models are amenable to traditional teletraffic analysis but only capture simple and limited<br\/>correlation structures in the data.<br\/><br\/>The researchers propose to significantly expand the analytical methodology in solving complex teletraffic problems arising in modern computer and communications networks. Their approach<br\/>which is rooted in classical ballot theorem handles a large class of arrival processes including<br\/>the standard Markovian processes, general periodic processes, processes described by<br\/>time-series (auto-regressive and moving average), or even general processes described by the<br\/>joint distribution of number of arrivals in equally-spaced non-overlapping discrete intervals.<br\/>Special attention will be paid in deriving algorithmic solutions which are numerically robust<br\/>and efficient even when the system utilization is high.<br\/><br\/>The researchers proposed approach is unifying, it avoids root-findings, and unlike the matrix analytical approach or recently introduced state-space spectral decomposition method, it does not<br\/>involve potentially expensive iterations. Further, it is transform-free and may provide a<br\/>simple form of solution amenable for further analytical studies such as tail behavior or<br\/>asymptotic analysis.<br\/><br\/>The researchers general arrivals processes could be used for the end-to-end performance analysis once the departure process from a network node is appropriately approximated or probabilistically<br\/>bounded. Since they are not limited to traditional Markovian models, such an approach<br\/>appears to be viable.<br\/><br\/>The researchers propose to implement our general algorithms and make them available as a scientific package to the research community.","title":"Advances in Modern Performance Analysis: A New Approach","awardID":"0126263","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4097","name":"NETWORKING RESEARCH"}}],"PIcoPI":["285452"],"PO":["565090"]},"75330":{"abstract":"By their nature, volumetric data sets, such as CT, MRI and the Visible Human cryosection <br\/>images, are not subject to the same form of visual inspection and cognitive refinement that <br\/>human experts apply when viewing and interpreting 2D images. <br\/>The internal cycle of choices made by humans as they visually survey an image is disrupted and<br\/>forced into an external process where users must adapt to the technical requirements of the<br\/>visualization system. This project proposes to continue research into an immersive visualization<br\/>system that is a first step toward creating a user interaction paradigm that more naturally<br\/>allows users to apply their interpretive expertise to the visualization and segmentation of<br\/>volumetric data sets. In this environment, segmentation, the process of isolating discrete structures within a data volume, is performed in parallel with visualization. Users have continuous control of the visualization and segmentation algorithms and can freely exercise their expert<br\/>interpretive knowledge. <br\/> <br\/>The current implementation, called the Immersive Segmentation Environment, emphasizes the <br\/>controlled application of high computational cost algorithms to local regions of the data volume. <br\/>Using stereo graphics, the user is presented with the illusion that the data volume occupies <br\/>physical space. The user interacts with the system using a 3D space tracked probe. Visualization <br\/>and segmentation algorithms continuously operate on the data volume within a local <br\/>neighborhood of the user.s position. This neighborhood corresponds to the user.s visual focus <br\/>point. The algorithms initialize themselves based upon an analysis of the local neighborhood and <br\/>heuristics. The system is implemented as a client\/server application and consequently allows a <br\/>user at a low-cost workstation to effectively use remote computational resources to visualize <br\/>large volumetric data sets. The system implements two main classes of segmentation algorithms. <br\/>Both determine initial algorithm parameters based upon a cluster analysis of the local neighborhood and are characterized by an iterative refinement approach. <br\/> <br\/>The proposed research will continue to investigate the capabilities of this unique user interaction <br\/>model for visualizing, exploring and interpreting volumetric data sets. <br\/><br\/>The research plan contained in this proposal is a natural outgrowth of prior work performed by <br\/>the PI. This ongoing work has had a positive impact on the curriculum, student research projects <br\/>and faculty research within the Computer Science department at the UW-La Crosse.","title":"VISUALIZATION: RUI: User-Directed Segmentation and Visualization of Volumetric Data Sets in a Collaborative, Remotely Hosted Environment","awardID":"0222519","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["347771"],"PO":["565272"]},"74483":{"abstract":"EIA-0218909<br\/>Paul J. Fortier<br\/>University of Massachusetts at Dartmouth<br\/><br\/>Title: \"Collaborative research: Computerized Decision Aid for Acute Care Nursing\"<br\/><br\/> This project addressed problems associated with building Decision Support Systems to serve nursing. Most healthcare environments call upon nurse clinicians to make complex decisions on a contiual basis. Automated decision support systems backed by evidence-based knowledge and standardized guides, such as clinical algorithms, are capable of supporting clinical nursing decisions. However, effective real-time access is limited. This project addresses this problem by delivering current clinical knowledge via an off-the-shelf handheld computer using wireless access to a central server. Innovative data mining and knowledge discovery algorithms, using a combination of case based and rule based learning with added confidence measures, permit bi-directional inferencing based on individual client data. This technology provides real-time decision support for the multiple cases and sequential decisions characterzing present nursing practice. Nurses are able to consider a full range of alternative explanations, determine addtional data needs, find, isolate and examine patient case outliers for additional diagnostic data or verify the appropriateness of a selected strategy. A fully developed system may have the capacity to maintain a history of series of decisions and outcomes thereby, overtime, improving the case base and rule bases used for decision support. Outcomes of this real time decision support may include timely health care, less biased decisions, and improved patient outcomes.","title":"ITR: Collaborative Research: Computerized Decision Aid for Acute Care Nursing","awardID":"0218909","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["426280",193445,193446,193447],"PO":["564181"]},"84120":{"abstract":"PI: Shiyu Zhou<br\/>Proposal Number: 9985310<br\/>Institution: U of Pennsylvania<br\/><br\/>Abstract<br\/><br\/>Randomized computation has become one of the major research fields<br\/>in Computer Science since its emergence in the late 1970's.<br\/>In spite of its seemingly enhanced power, as far as we know,<br\/>randomness may not provide any computational advantage <br\/>(by more than a polynomial factor) over determinism.<br\/>The first objective of this project is to investigate the issue of<br\/>reducing randomness in computation via explicit combinatorial constructions.<br\/><br\/>Derandomization in space-bounded computation is the major subject to<br\/>study here, with the focus on pseudorandom generator constructions.<br\/>In particular, the constructions of pseudorandom generators for constant-width<br\/>read-once branching programs, and of discrepancy sets for combinatorial<br\/>rectangles are to be examined.<br\/><br\/>Derandomizing the well-known randomized log-space algorithm for <br\/>undirected graph connectivity problem will be investigated further.<br\/>In the meantime, a variant of the connectivity problem<br\/>in the model of directed graphs with tree structures will also be examined,<br\/>hoping this can provide a better understanding of the relationship between <br\/>non-deterministic and deterministic computations.<br\/><br\/>An advanced topic course (graduate or undergraduate) on explicit<br\/>combinatorial constructions and their applications will be designed.<br\/>The goal of the course is to enable the students to attain a solid<br\/>understanding of the mathematical foundation of explicit<br\/>constructions as well as their applications to algorithm design<br\/>and computational complexity.<br\/><br\/>Mobile computing over wireless channels has emerged as a rapidly growing<br\/>technology and gained a large amount of attention in recent years.<br\/>The highly asymmetric nature of the communication environment<br\/>in this context gives rise to many new challenges concerning the<br\/>issues of information dissemination and retrieval. Indexed data<br\/>scheduling is to design data broadcast schemes that<br\/>minimize both the average waiting time and energy consumption<br\/>of the clients in retrieving information on air.<br\/>Another objective of this project is to investigate <br\/>the possibility of applying the ideas from indexed data scheduling<br\/>to the design of communication protocols for multimedia applications<br\/>in wireless mobile communication networks.","title":"CAREER: Explicit Combinatorial Constructions, Indexed Data Scheduling, and Their Applications","awardID":"0315147","effectiveDate":"2002-10-01","expirationDate":"2004-08-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":["560580"],"PO":["565251"]},"71173":{"abstract":"EIA-0205652<br\/>Salchenberger, Linda <br\/>Loyola University of Chicago<br\/><br\/>Title: ITR: The Community Information Technology Entrepreneurship Project<br\/><br\/>Loyola University has been awarded an ITR grant to educate students and contribute to the economic development of impoverished communities by means of collaborative projects with small community businesses and not-for-profit organizations in Chicago and surrounding areas that are heavily populated with groups that are under-represented in science and technology. The project will build a technology outreach program that brings together in a mentoring relationship, faculty, students and community members to create an IT entrepreneurship program which focuses on business software development<br\/><br\/>Two academic goals of the project are to enhance the computer science education and experience of university students, and instill a sense of civic engagement in students and faculty. The research design will use data gathering and analysis techniques to measure outcomes related to these and other goals. The benefits to Loyola students will include hands-on experience in helping to develop real world software applications, as well as an understanding of the poor communities. The benefits to the Austin community include the learning of IT skills, a sense of entrepreneurship in IT and improvement in their environment coupled with financial gains.","title":"ITR: The Community Information Technology Entrepreneurship Project","awardID":"0205652","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}}],"PIcoPI":["347811","464068","464068",184332],"PO":["564181"]},"74693":{"abstract":"Intrusion detection systems, as their name suggests, are designed simply to detect intrusions. They may, at most, recommend courses of response, and for good reason. Attacks are not always detected when and where they occur. It makes good design sense to separate the detection facility<br\/>from the response facility.<br\/><br\/>Response systems to date have focused primarily on backup and recovery. Comparatively little effort has been spent on immediate response, or \"first-aid\" services; and what effort there has been is mainly in the area of network filtering and blocking. FACS is designed to fill the need for further services, such as suspending or disabling services and user accounts, and sequestering files for forensic analysis.<br\/><br\/>FACS will integrate these responses with local system policy, so that the system administrator's knowledge of the resources and users available on the system is taken properly into account. In this way, more important data and accounts can be given higher priority; careful attention is<br\/>paid to services that have more dangerous failure modes; trust is appropriately accorded (or not) to various outside domains providing information about attacks and responses; and so forth. Note that these are not attributes accessible to the operating system, but personal knowledge that the administrator would have as a matter of course.<br\/><br\/>The FACS system will be constructed at multiple levels, enabling local host responses, responses across a local network, as well as responses between networks. FACS will design and incorporate a response prescription language enabling uniform and machine-independent specification of<br\/>appropriate responses to attacks as they occur in real time. This language will also facilitate verification against the local system policy.<br\/><br\/>FACS will enable systems to present a more complete defense against attacks, by bridging the gap between attack detection and complete restoration. And from a scientific perspective, it will give us insight into the complex small-scale interactions between attacks and recovery efforts.","title":"ITR: First Aid For Computer Systems","awardID":"0220016","effectiveDate":"2002-10-01","expirationDate":"2006-01-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["427568","226310"],"PO":["7594"]},"71096":{"abstract":"The information age is built on software. Web browsers and servers, office productivity tools, anti-lock brake systems, cellular telephones, and online trading systems are all implemented in software. Most of that code is prepared for execution using a compiler.<br\/><br\/>For four decades, compilers have applied a fixed sequence of transformations to the code. The fact that different transformations, applied in different orders, can produce different results has been known, but the techniques for picking effective transformation sequences have not. The computational power available with modern processors makes it possible to experimentally discover good transformation orders.<br\/><br\/>This project will systematically explore the problems of choosing transformation sequences. This five-year program will develop practical techniques for building adaptive compilers - compilers that change their behavior in response to the input program and the end-user's stated goals for optimization. This will require experimentation, algorithm development, application of techniques from machine learning, and basic software engineering. <br\/><br\/>This project will produce the tools, techniques, and insights needed to make adaptive compilation both practical and productive. The resulting compilers will give users greater control over the run-time characteristics of their programs and will reduce the performance variability that plagues compiled code today.","title":"ITR: Building Practical Compilers Based on Adaptive Search","awardID":"0205303","effectiveDate":"2002-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}}],"PIcoPI":["229226","193837","309293"],"PO":["565272"]},"77762":{"abstract":"Talcott<br\/>CCR-0234462<br\/>\"FORMAL CHECKLISTS FOR REMOTE AGENT DEPENDABILITY\"<br\/><br\/>Deep Space Missions involve a tight integration of physical and software systems that must function autonomously over a prolonged time. These autonomous agents need to be robust and able to react in real time to state changes without aid of earth control. The Mission Data System (MDS) framework, consisting of an architecture, tools, and libraries of reusable components, has been developed by NASA to address this problem. This project builds on two key ideas of the MDS approach: a state-based approach to system design and a goal-oriented approach to operation. It develops a formal framework with methods and supporting tools for increasing the dependability of goal based operation of space systems. In particular, a formal approach to the analysis of goal net specifications is being developed that enables assertions to be made about their dependability level. A set of formal checklists (formal analysis suites) will be produced along with supporting tools that can be used to achieve more predictable dependability of goal nets and goal-based operation. The checklists will provide a qualitative means of measuring dependability of goal achievers. A spectrum of analysis techniques of different strengths will be developed to allow for achieving different levels of dependability.<br\/>To experimentally validate the ideas, the framework will be applied to goal achievers for a representative set of domains in the context of the MDS test bed. The experimental work will guide the development of formal checklists. The resulting case studies will also serve as templates for further application of the formal framework. Certified packages of goals, goal nets and corresponding software modules developed for one mission can be re-used in future missions. The formal technology developed in this project will be applicable to a wide range of domains and physical situations.","title":"Formal Checklists for Remote Agent Dependability","awardID":"0234462","effectiveDate":"2002-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4097","name":"NETWORKING RESEARCH"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7214","name":"HIGHLY DEPENDABLE COMPUTING"}}],"PIcoPI":["550115","474612"],"PO":["564388"]},"76453":{"abstract":"Survivors of the Shoah Visual History Foundation was established by Steven Spielberg in1994 to collect the testimonies of a large number of Holocaust survivors, witnesses, liberators, and rescuers. Since then, the Shoah Foundation has videotaped over 51,700 testimonies in 57 countries and 32 languages. The testimonies comprise approximately 120,000 hours of video. <br\/><br\/>Currently, the Shoah Foundation is supported by a National Science Foundation grant under the ITR program to enable Johns Hopkins University, the University of Maryland, and IBM to perform research experiments in voice and speech recognition in Holocaust testimonies. The Shoah Foundation requests funding to establish a high performance network connection (155 Mb\/s) between the Foundation digital library and researchers on Internet2. Initially, this network connection will facilitate the voice and speech recognition efforts by increasing the amount of data and the speed of access to data these researchers currently have. Once this connection is established, other academic research efforts can be enabled in the future. Currently the Shoah Foundation is a sponsored affiliate of Internet2 by the University of Illinois at Chicago. This sponsorship, and hundreds of requests from university researchers, show the desire of the academic community to gain access to the archive. It will also help bring testimony material directly into classrooms, meeting the Shoah Foundation new mission to overcome prejudice, intolerance, and bigotryand the suffering they causethrough the educational use of the Foundation visual history testimonies.","title":"HPNC: Connecting the Visual History Foundation 180 Tera-Byte Archive to Internet 2","awardID":"0228938","effectiveDate":"2002-10-01","expirationDate":"2004-09-30","fundingAgent":[{"dir":{"id":"01","name":"Office of OFFICE OF THE DIRECTOR                  ","abbr":"O\/D"},"div":{"id":"0111","name":"Office of CYBERINFRASTRUCTURE","abbr":"OCI"},"pgm":{"id":"4088","name":"HIGH PERF NETWK CONNECT-SCIENG"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"V275","name":"NSA-HPNC"}}],"PIcoPI":["314965"],"PO":["288674"]},"77795":{"abstract":"Communication protocols form an integral part of many complex <br\/>software systems. In many such systems, protocols' impact on<br\/>system dependability far exceeds the fraction of the total <br\/>number of lines of code devoted to their implementation. <br\/>Unfortunately, there exists virtually no systematic approach for <br\/>design and evaluation of protocols that considers dependability.<br\/><br\/>Previously, we have developed a systematic, semi-automatic <br\/>approach for validation of communication protocols. Our approach <br\/>uses semi-formal representations of protocols and efficient search <br\/>(forward search as well as fault-oriented test generation) to <br\/>analyze protocols over the entire range of scenarios, including <br\/>external events, faults (such as loss of packets, communication <br\/>link failures, loss of state in communication nodes, and so on), <br\/>as well as network topologies. The effectiveness of our approach <br\/>has been demonstrated via its application to a varied range of <br\/>protocols.<br\/><br\/>In this project, we will undertake three main tasks. First, we <br\/>will extend our previously developed framework to compute values <br\/>of appropriate dependability metrics for existing protocols of <br\/>interest to the NASA and the HDCCSR community. Second, we will <br\/>develop the first systematic approach to design new protocols that <br\/>satisfy desired dependability criteria. Finally, we will use <br\/>the testbed provided by NASA in conjunction with the expertise <br\/>and software made available by NASA and other HDCCSR researchers <br\/>to fine-tune our approaches and to demonstrate that: (i) accurate <br\/>values of dependability metrics can indeed be computed for <br\/>existing communication protocols using a systematic, semi-<br\/>automatic approach, (ii) new protocols can be designed in a <br\/>systematic, semi-automated manner to meet desired dependability <br\/>criteria, and (iii) enhancements in the dependability of <br\/>communication protocols can indeed significantly enhance the <br\/>dependabilities of many software-hardware systems of interest to <br\/>NASA and the HDCCSR community. <br\/><br\/>In addition to above technical contributions, the proposed <br\/>research will have broader impact on society. First, results of <br\/>the proposed research will enable improvements in dependability <br\/>of critical infrastructure, such as the air-traffic control <br\/>system. Second, the research will develop new approaches and <br\/>practical case studies that will provide us with a unique <br\/>opportunity to educate a large and diverse student body in <br\/>techniques for development of dependable protocols and systems.","title":"Obtaining Highly Dependable Communication Protocols","awardID":"0234600","effectiveDate":"2002-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7214","name":"HIGHLY DEPENDABLE COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"W379","name":"NASA-HIGHLY DEPENDABLE COMPUTI"}}],"PIcoPI":["535238","550970"],"PO":["565272"]},"75397":{"abstract":"In order to understand large vector fields, scientists typically attempt to simplify and visualize the data. Broadly speaking, there are two methods for visualizing vector field feature extraction techniques and general visualization techniques, that offer complementary advantages. <br\/>Feature based techniques extract important features such as topology (critical point and critical curves), attachment or separation lines, vortices, bifurcations or shock waves, and attempt to present a simplified view of the vector field.<br\/>There are two main dificulties with this approach. First, too many features may clutter the visualization. Therefore, there is a need to prune spurious and insignificant features.<br\/>Second, feature visualization alone may fail to provide a comprehensive view of the data. This can be remedied by using popular general visualization techniques such as streamlines, line integral convolution (LIC), and stream surfaces.<br\/>However, most attempts using this approach destroy the underlying important features early in the<br\/>simplification or compression approach, as we have demonstrated in our preliminary work. Therefore,<br\/>there is a need to tie the feature simplification process with the simplification of the underlying vector field in order to produce consistent visualization.<br\/>In this work, we propose to develop controlled feature simplification and preservation algorithms<br\/>for 2D vector fields that remove insignificant features and preserve important ones by removing noise and by utilizing feature-driven similarity metrics to measure and control the simplification<br\/>process.<br\/>The proposed algorithms will simplify the features and the underlying vector field simulatneously<br\/>in order to produce consistent visualization. We will extend these algorithms to 3D and ime-dependent vector fields. Additional challenges include: (i) designing efficient algorithms and structure-similarity metrics based on the properties of more complex 3D features such as the 3D separation and attachement lines, vortices and shock wave regions, and (ii) tracking simplied features over time as bifurcations may take place. We will also quantify and visualize the uncertainty or loss due to simplification in order to give a better sense to the scientist as to what is lost or compromised during the simplification process.<br\/>We will apply our algorithms to computational fluid dynamics and environmental data. We expect that the algorithms and techniques developed in this project will be applicable to a wide variety of disciplines including aeronautics, meteorology, oceanography, environmental sciences, astronomy, geographic information sciences, computer graphics, and computer-aided geometric design.","title":"VISUALIZATION: Feature Driven Simplification and Visualization of Vector Fields","awardID":"0222900","effectiveDate":"2002-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7329","name":"COMPILERS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7352","name":"COMPUTING PROCESSES & ARTIFACT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":[196097],"PO":["565272"]},"76255":{"abstract":"This proposal for a joint EU-US workshop will bring together the active researchers from the US and European Union who are investigating information and communication technologies in the home. This workshop is important as scientists in a number of countries are studying technology in the home, but because of publication lag times are not aware of each other's results. This workshop will serve as a forum to address the role of technology in 1) household behavior, including socialization, time use, and gender differences, 2) social interaction, including the impact of the Internet on social capital and quality of relationships, 3) employment and new forms of work, and 4) country differences in the use and impact of technology in the home. One important output will be an academic book documenting the current state of the theoretical and empirical research.","title":"EU-US Workshop on Domestic Impact of Information and Communication Technology","awardID":"0227969","effectiveDate":"2002-10-01","expirationDate":"2004-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6850","name":"DIGITAL SOCIETY&TECHNOLOGIES"}}],"PIcoPI":["560995"],"PO":["495796"]},"76387":{"abstract":"Abstract<br\/><br\/>Assembling the Tree of Life: Phylogeny of Spiders<br\/><br\/>Ward C. Wheeler, Jonathon A. Coddington, Gustavo Hormiga, Lorenzo Prendini, and Petra Sierewald<br\/><br\/>American Museum of natural History<br\/><br\/>A grant has been awarded to Dr. Ward Wheeler of the American Museum of Natural History and his colleagues Dr. Lorenzo Prendini (AMNH), Dr. Jonathan Coddington (National Museum of Natural History, Smithsonian Institution), Dr. Gustavo Hormiga (George Washington University) and Dr. Petra Sierwald (Field Museum of Natural History) to examine the evolutionary history and biodiversity of spiders. Spiders are among the oldest and most diverse groups of terrestrial organisms on our planet, with fossils dating back to the Devonian (c. 380 million years ago) and a current diversity of over 37,500 described species placed in 3,471 genera and 109 families. Spiders stand out because of their ecological importance as the dominant predators of insects. It is no exaggeration to say that without spiders, insect pest populations would soar and humans would be greatly affected. Furthermore, spiders are already model organisms in biochemical (silk proteins and venom), behavioral (especially sexual and web-building behaviors) and ecological (foraging, predator-prey systems, integrated pest management) research. Accordingly, understanding their evolutionary history is a critical component in the NSF's Assembling the Tree of Life program. <br\/>The aim of this Tree of Life proposal to produce a robust phylogeny of all the deepest branches within the spiders, by combining a massive amount of newly generated comparative genomic data with a substantial set of new and re-assessed data on morphology and behavior. The PIs will use high-throughput DNA sequencing to examine at least 50 \"loci\" for representatives of at least 500 genera of spiders and their closest relatives (the whipscorpions and allies). The computational challenges posed by the resulting large data matrices will be analyzed using new computer software, designed in large part by members of the group and using massively parallel processing to achieve supercomputing capability. These organisms included in the study will purposefully include all the previously most-favored study organisms of ethologists, ecologists, physiologists, and developmental and molecular biologists, thus integrating and contextualizing their research.","title":"ATOL: Assembling the Tree of Life: Phylogeny of Spiders","awardID":"0228699","effectiveDate":"2002-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0406","name":"Office of INTL SCIENCE & ENGINEERING","abbr":"OISE"},"pgm":{"id":"5980","name":"WESTERN EUROPE PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1994","name":"BIOINFORMATICS PROGRAM"}},{"dir":{"id":"06","name":"Directorate for DIRECTORATE FOR GEOSCIENCES             ","abbr":"GEO"},"div":{"id":"0603","name":"Division of EARTH SCIENCES","abbr":"EAR"},"pgm":{"id":"1571","name":"SCEC"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"1171","name":"PHYLOGENETIC SYSTEMATICS"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"1198","name":"Biodiversity: Discov &Analysis"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0804","name":"Division of EMERGING FRONTIERS","abbr":"EF"},"pgm":{"id":"7689","name":"ASSEMBLING THE TREE OF LIFE"}}],"PIcoPI":["423537","433667","535370","499838","546276"],"PO":["54897"]},"76068":{"abstract":"ABSTRACT<br\/>0226504<br\/>Thomas Peters<br\/>U of Connecticut<br\/><br\/>Research on surface reconstruction has prospered by productive interactions between computational<br\/>geometers and classical topologists. The initial reconstruction techniques on unorganized point<br\/>data relied upon visual inspection to verify topological correctness. Recent advances have led to<br\/>rigorous sufficient conditions that guarantee correct topological form first, by equivalence under<br\/>homeomorphisms and subsequently by the stronger ambient isotopy. These theoretical advances<br\/>have led to improved surface reconstructions for reverse engineering and medical modeling for a<br\/>significant subset of the surfaces considered in those applications. This prompts a speculative<br\/>unifying intellectual question: Can contemporary modifications of classical difierential geometry,<br\/>topology and knot theory lead to a broader scope for computational topology that will result in<br\/>further algorithmic improvements? More specifically, can these classical mathematical tools be<br\/>applied to extend the domain of surface reconstruction algorithms? This research will explore<br\/>how this already successful synergy between classical mathematics and computer science can be<br\/>accelerated. Graduate student support is requested and this successful project outcome will then<br\/>serve as an exemplar for further student involvement in interdisciplinary work between computer<br\/>scientists and mathematicians.","title":"SGER: Computational Topology for Surface Reconstruction","awardID":"0226504","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["258220","482465","486281"],"PO":["321058"]},"75640":{"abstract":"EIA 0224369<br\/>Pandey, Raju<br\/>Aksoy, Demet; Devanbu, Premkimar T.; Olsson, Ronald A.<br\/>University of California - Davis <br\/><br\/>CISE RR: Infrastructure for Research in Parallel and Distributed Computing <br\/><br\/>This proposal, supporting parallel computing, distributed systems, ubiquitous (autonomous) systems, and satellite communication, aims at establishing a parallel and distributed computing laboratory within the center for software systems. The lab will include four clusters, each connected through high speed network. The parallel computing cluster will host several multiprocessor systems; the distributed system cluster will include workstations and PCs, the ubiquitous systems cluster will connect embedded devices (including handheld device sensors); and the satellite cluster will host equipment for initiating the downlink communication for disseminating the data. The sensor network will primarily be connected through an ad hoc wireless network that will be routed to the main high-speed backbone. The infrastructure will meet the needs of four research projects in<br\/>1. Distributed Systems,<br\/>2. Parallel Computing,<br\/>3. Distributed Systems Software Engineering, and<br\/>4. Wide Area Data Dissemination.<br\/>The distributed systems cluster will enable the distributed systems group to develop highly efficient, scalable, robust, and secure distributed systems that adapt to changes underlying systems conditions. Further, the ubiquitous systems cluster will permit the group to develop new application-specific routing algorithms, novel programming models customizable middleware that seamlessly integrate sensor-based systems and scalable resource allocation, and conservation algorithms for large sensor-based systems. The satellite communication cluster, along with the distributed systems cluster, will enable the distributed data dissemination group to develop highly scalable, robust, and secure data-dissemination techniques with adaptable quality of service. The concurrent programming group will use the parallel computing cluster to evaluate their work on both concurrent programming languages and efficient runtime systems for parallel systems. On the educational side, the infrastructure will be used as experimental test-beds in several courses and will form the basis for providing research students with development and analysis skills in parallel and distributed systems.","title":"CISE Research Resources: Infrastructure for Research in Parallel and Distributed Computing","awardID":"0224469","effectiveDate":"2002-10-01","expirationDate":"2006-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2890","name":"CISE RESEARCH RESOURCES"}}],"PIcoPI":[196817,"284993","562714",196820],"PO":["557609"]},"72153":{"abstract":"Finding sequences similar to a given query sequence in a large data set<br\/>is a fundamental problem in many database applications including<br\/>computational genomics and proteomics, computational finance, audio, text<br\/>and multimedia image processing. This proposal considers proximity search<br\/>problems for strings and sequences in such applications where existing<br\/>methods typically employ distance functions based on character edits only<br\/>(commonly unweighted edit distance) and modify off-the shelf index<br\/>structures to improve search efficiency. However, in many applications<br\/>block edit operations such as translocations, deletions and copies, as well<br\/>as linear transformations on strings are as common and important<br\/>as character edits.<br\/><br\/>Unfortunately, computation of distance measures that allow block edits<br\/>are generally known to be NP-hard. Thus the investigators propose to work<br\/>on practical methods for approximating the block edit distances and performing<br\/>approximate searches under such measures. These measures are many times<br\/>not symmetric (transforming a long sequence to a short one may be<br\/>performed through a single deletion whereas transforming the short<br\/>sequence to the long one may require many copy operations) or do not<br\/>satisfy the triangular inequality (string space under edit operations is<br\/>fundamentally different from Euclidean space) and thus do not provide a metric.<br\/><br\/>However many of them can be proven to be almost-metrics, i.e.,<br\/>the symmetry and\/or the triangular inequalities may be satisfied within<br\/>multiplicative constants. This property can be fundamental in sequence<br\/>proximity search as it seems possible that known distance based indexing<br\/>methods for metric spaces could be extended for use in almost-metric<br\/>spaces. Thus a main component of the proposal is the investigation of<br\/>the limitations and extendibility of distance based indexes for<br\/>almost-metrics and other string\/sequence measures which may be<br\/>non-symmetric or fail to satisfy the triangular inequality because they<br\/>allow block edits.<br\/><br\/>An alternative\/complementary approach to distance based indexing is based<br\/>on approximate random mappings of the space of items to be searched to<br\/>Euclidean spaces. This general strategy has received considerable<br\/>attention in the context of vector spaces. The mappings used guarantee<br\/>(with high probability) to reduce the number of comparisons during search<br\/>from linear to poly-logarithmic, provided some error of approximation can<br\/>be tolerated in the answers obtained. This approach was later generalized<br\/>to sequence spaces through a mapping of sequences under block edit distance<br\/>to binary strings under Hamming distance. The error tolerance is not<br\/>very practical. Thus a second component of this proposal is to obtain<br\/>novel mappings of sequence spaces to Euclidean spaces within smaller<br\/>approximation errors. The goal is to generalize such mappings to both<br\/>character and block based edit distances, especially those which assign<br\/>weights to edit operations (that reflect the likeliness of that<br\/>particular edit).","title":"Improving Sequence Proximity Search","awardID":"0209145","effectiveDate":"2002-10-01","expirationDate":"2006-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}}],"PIcoPI":["393279",186846,"560190"],"PO":["469867"]},"72043":{"abstract":"This is a renewal t o award IIS-9874781.","title":"A Digital Library of Vertebrate Morphology Using High-Resolution X-Ray CT","awardID":"0208675","effectiveDate":"2002-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6857","name":"DIGITAL LIBRARIES AND ARCHIVES"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1104","name":"Division of UNDERGRADUATE EDUCATION","abbr":"DUE"},"pgm":{"id":"7444","name":"NATIONAL SMETE DIGITAL LIBRARY"}}],"PIcoPI":["536900"],"PO":["433760"]},"74584":{"abstract":"This award addresses an important fundamental problem in computational mathematics: how to optimize complex systems described by partial differential equations (PDEs). The focus is on PDE simulations that can scale into millions of variables, and hundreds or thousands of processors. The size of the problems and the complexity of the techniques for solving these PDEs pose major challenges to modern optimization methods. The project uses a general framework for solving optimization problems in interaction with PDE solvers.","title":"ITR: Collaborative Research: Optimization of Systems Governed by Partial Differential Equations","awardID":"0219438","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["517236"],"PO":["381214"]},"75398":{"abstract":"Despite many advances in the visualization community over the past decade,<br\/>effective and efficient representations for three-dimensional flow fields are still elusive.<br\/>Many techniques show promise, but do not represent a complete picture of<br\/>the flow.<br\/>This project consists of a systematic investigation of the current<br\/>techniques, their advantages and disadvantages, research and analysis of combined techniques which complement each other, and research into new techniques.<br\/>Cognitive analysis and user studies will be undertaken to investigate existing research. Many of these techniques, including those of the PIs, proposed a method of representing or rendering a flow<br\/>field, with little selfexamination of the limitations towards garnering a true understanding of the flow<br\/>field.<br\/>Several of these techniques have obvious limitations, but present an improvement over<br\/>the dearth of any good techniques for three-dimensional flow visualization. By<br\/>combining several techniques appropriately, more improvements can be<br\/>made. <br\/>Are these improvements sufficient? What combinations work and which ones do not?<br\/>These questions will be investigated and quantified in this research.<br\/>Finally, we will ask: How can new techniques be developed that will overcome some of these<br\/>limitations?Our subjective understanding of the field allows us to propose several<br\/>avenues of investigation. These include an-isotropic volume renderings, embedded<br\/>with flow differentiators; schematic texture algorithms for opaque and semi-transparent stream<br\/>surfaces; level-of-detail models for representing the flow; multi-resolution representation<br\/>of underlying features; and new representations that provide a more meaningful context<br\/>for the flow.<br\/>This research will greatly aid computational scientists across many disciplines: computational fluid dynamics scientists, environment scientists, atmospheric scientists, computational geological scientists and biomedical researchers.<br\/>A primary component of their research is the investigation or simulation of natural phenomena.<br\/>This phenomena is generally dynamic, leading to complex and interesting flow fields. For<br\/>three-dimensional computations, effective tools to understand, represent and convey these<br\/>flows are lacking. This research will provide a firm foundation for research in this area, as well as lead to new algorithms offering a much clearer representation for flows.","title":"VISUALIZATION: Effective Visualizations for Complex 3- and 4-Dimensional Flow Fields","awardID":"0222903","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["475211",196100,"533228"],"PO":["565272"]},"72176":{"abstract":"Certification of medical device software is a major concern for the Food and Drug Administration: it is costly and time-consuming for the companies involved, and there is a growing suspicion that the current certification requirements are becoming more and more inadequate with increasing system complexity. At present certification of medical device software is process-oriented. There is a strong desire to move to a more product-oriented approach, but it is as yet unclear what methods\/evidence should be required. The goal of this research is to show that existing formal methods can be adapted and augmented such that they can be used effectively in the certification process. The research to achieve this consists of the following three components:<br\/><br\/>Automatic Generation of User Models:<br\/>For many medical devices operability is an important concern, but it is not explicitly designed for. Automatic generation of user models (that is, what does the user need to know about the system to be able to operate it) allows evaluation of operability in early stages of the design and guarantees a correct correspondence between user model and machine model. The project is formalizing user model requirements and developing methods to construct these models automatically from the machine model.<br\/><br\/>Run-time Verification: <br\/>Run-time verification can complement design-time verification when the system is too complex to be realistically verified in full. Run-time analysis techniques can also be used for performance modeling and improvement of the system based on run-time characteristics. The project investigates the usefulness of run-time analysis in the certification process, both in the specification phase<br\/>(to stress test the model generated from the specification), and in actual operation (to establish audit trails and catch unexpected behaviors). The research is directed at the development of adequate specification languages and logics for run-time verification, and efficient data structures for program instrumentation.<br\/><br\/>Case study: <br\/>At the request of the FDA, the Walter Reed Army Institute of Research (WRAIR) has made available a requirements document for a medium-sized medical device. It is the intent to use this system as a basis to study the feasibility of product-oriented certification. The modeling<br\/>of this device has already started and some preliminary analyses have been performed. The goal is to use formal methods to produce a fulll set of proof documents and evaluate, in cooperation with the FDA, whether these documents would be considered adequate evidence for certification.","title":"Towards Certification by Verification","awardID":"0209237","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2801","name":"EMBEDDED & HYBRID SYSTEMS(EHS)"}}],"PIcoPI":["309451"],"PO":["561889"]},"71098":{"abstract":"This project models and builds systems that augment human physical capabilities for performing skilled tasks. Areas of importance for human-machine physical collaboration include micro-manipulation such as microsurgery or microassembly, and remote operation (eg in outer space or under the ocean). It is also important to study such techniques in areas where great dexterity is required, such as medical palpation.<br\/>These systems are also important for training the disabled and in various educational applcations.<br\/><br\/>Human-machine collaborations of this sort, where a person and a robot are both \"holding\" the same knife or stick, differ from traditional interfaces in their richness of sensory inputs and the coupled computation and external physical reality. The inclusion of the human in the feedback loop makes these systems more complex than older robotics applications; human expectations and reactions must be modeled and provided for in the system. Depending on the application, vision, sound, and force-feedback may be involved; reaction times may vary; and the robot manipulator may be used to add stability, micro-scale capability, and\/or safety to the system.","title":"ITR: Modeling Synthesis and Analysis of Human-Machine Collaborative Systems","awardID":"0205318","effectiveDate":"2002-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6857","name":"DIGITAL LIBRARIES AND ARCHIVES"}}],"PIcoPI":["434705","531055","553223","520895"],"PO":["433760"]},"75355":{"abstract":"This is a proposal for funding in support of the annual Clemson mini-Conference on Combinatorial Optimization, now in its 16th consecutive year of existence. This day-and-a-half long conference is held every year at Clemson University, Clemson, South Carolina, on a Thursday and Friday, usually sometime during the first two weeks of October. The mini-Conference invites about 12 speakers to give 40-minute talks on their current research. Speakers are chosen equally between the fields of discrete mathematics (including combinatorics, graph theory, operations research and coding theory), and algorithmic computer science (including computation theory, computational<br\/>complexity and the design and analysis of algorithms).","title":"Clemson mini-Conference on Combinatorial Optimization","awardID":"0222648","effectiveDate":"2002-10-01","expirationDate":"2004-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":[195980,195981,195982,"436244"],"PO":["543507"]},"65697":{"abstract":"This project focuses on new approaches to maximizing power consumption in handheld and other wireless devices by combining a network-centric view of the power-consumption issue (aximizing longevity for the network rather than device by devic) with the more traditional device-centric view (power management capabilities for the individual device). The project will model and evaluate new architectures and algorithms that allow the operating system to accept input from the applications and interact closely with the network for minimizing overall energy in the wireless system. The focus of the research is: <br\/><br\/>1) power-management of individual nodes <br\/>2) power-aware routing, in the sense that the power is minimized for the overall network and not only for individual messages or streams. <br\/>3) a combination of the variable voltage scheduling with the power-aware routing. <br\/><br\/>It is anticipated that through algorithm development and analysis, simulation and testbed implementations, the results of this project will lead to a better understanding of how to provide more efficient power management to nodes with rechargeable batteries, and how to integrate CPU and network power management under one paradigm.","title":"Power-Autonomous Wireless Networks: Controlling CPU and Transmission Power with Rechargeable Capabilities","awardID":"0125704","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4097","name":"NETWORKING RESEARCH"}}],"PIcoPI":["533851","554329"],"PO":["565090"]},"75388":{"abstract":"This project will develop a systematic process of mapping Augmented Reality information organization to the human brain's prodigious capacity for spatial cognition. Fundamental to human thinking and utilizing much of the brain's resources is spatial cognition: a matrix for mental imagery manipulation, mathematical reasoning, spatial mental models of time, certain aspects of language organization, and memory organization. A neuropsychological human factors route to AR interface design may produce a solid basis for systematic theory, research, and guidelines for the design of mobile, AR interfaces. To accomplish this the research will: (1) extend neurological and behavioral research on human spatial cognition to systematically map egocentric (i.e., body centered) spatial perception and object manipulation in AR systems; (2) add new capabilities to the METLAB's ImageTclAR toolkit in support of this project and spatial AR interface design, and (3) develop multi-user mobile AR prototypes with both body and environmentally stabilized components using new hybrid tracking techniques based on visual tracking and ShapeTape sensors. It will also systematically conduct Human Computer Interaction experiments to evaluate and confirm the effect of new Mobile Infospaces interface techniques on: information object search and perception, tool and data object semantics, ease of object manipulation and procedural performance, memory for the location and properties of data tools and objects, group navigation, situational awareness, and cognitive maps of the environment. Finally, the project will organize and systematize findings and interface design experience into Mobile Infospaces AR Design Guidelines.<br\/><br\/>The new models and methods developed for AR system design will impact mobile AR research and development in two main ways. For research, it will provide a empirically validated theoretical foundation for augmented reality design using a neuropsychological and behaviorally valid human factors model of spatial cognition in information displays. For interface design, it will provide information display guidelines and ImageTclAR interface development tools that can establish parameters for optimizing the placement of tools, organization of menus and data, and clustering of procedures for mobile, augmented-reality and immersive virtual reality users.","title":"Mobile Infospaces: Mapping Information to Neuropsychological Patterns of Human-Spatial Cognition to Support Wearable, Augmented-Reality Interface Design","awardID":"0222831","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6845","name":"HUMAN COMPUTER INTER PROGRAM"}}],"PIcoPI":[196076,"345982"],"PO":["565227"]},"71121":{"abstract":"This proposal describes SLAM, a scalable network architecture integrating millions of real-world sensors with actuators and distributed software applications. SLAM will enable a broad variety of novel monitoring and control applications including rapid disaster response, scalable crime detection and prevention, facilities maintenance, asset monitoring, and navigation. SLAM solves three problems:<br\/><br\/>1. Full exploitation of a sensor's data stream requires knowledge of contextual information, particularly location and time.<br\/>2. Fine-grained monitoring of millions of assets and facilities requires the physical deployment of sensors in the environment an intensive and cumbersome manual task.<br\/>3. Use of deployed sensors\/actuators by distributed software applications requires network infrastructure.<br\/><br\/>The SLAM architecture has three main components that address these issues:<br\/><br\/>1. Cricket, a ubiquitous and precise location infrastructure. No current location-sensing technology works everywhere in all places and at all times. Cricket is a novel multi-sensor location architecture to solve this problem, using a combination of RF and ultrasound indoors and at building perimeters, and GPS outdoors. Cricket incorporates self-configuration algorithms and energy-efficient protocols for scalability and longevity. <br\/> 2. An activated environment and efficient activation method. SLAM requires that the subject environment be activated with sensors and actuators. Without special attention, the activation process could become unmanageable due to the complexity of the environment. Therefore SLAM provides virtual location-based tagging, typically for immobile objects. The human installer affixes virtual tags to physical regions or objects by pointing at them with a Cricket-equipped handheld device, triggering an association of a unique identifier and the tagged entity's location and other attributes in a persistent store. This eases environment activation.<br\/>3. A scalable network infrastructure connects sensor information and events to software handlers. The network consists of fixed and mobile sensor proxies, physically co-located with the objects and events they monitor, to integrate location, identity, and temporal information to form an event stream. Sensors and their proxies communicate using sensor-specific low-energy communication protocols. Applications are written as event handlers distributed across the network. SLAM provides support for dynamically distributing handlers across proxies and compute servers, routing events to handlers, and performing query processing operations. <br\/><br\/>The proposed SLAM architecture introduces three innovative ideas: ubiquitous, energy-efficient location infrastructure (drawing on ideas from beacon-based location systems, computational geometry, and wireless networking); virtual region and object tagging for environment activation and asset management (drawing on ideas from geometric modeling and database management systems); and distributed proxy-based event and response processing (drawing on ideas from networking and database systems).<br\/><br\/>Starting with an existing environment (a building, campus, or town), the operational model to put SLAM in place is as follows. First, the location infrastructure is activated. Location beacons are placed in the environment, and a digital representation of the environment is constructed, enabling location inference anywhere within the environment. Second, the environment is activated. Sensors and virtual and physical tags are affixed to objects of interest within the environment (and environment representation). Third, the SLAM network is activated, connecting raw sensor<br\/>data streams to sensor proxies. The proxies annotate sensor data streams with location and temporal information, and forward them to appropriate handlers via the event-processing network. Handlers produce further events, as well as actions and notifications to be forwarded to actuators or humans.<br\/>As a challenging test case, we plan to deploy SLAM on a large university campus with millions of interesting entities. These include many sensors in offices, machine rooms, physical plant, and laboratories to monitor power, temperature, humidity, and pressure; smoke and fire detectors; burglar alarms and physical intrusion detection systems; motion detectors; monitors of leaks, floods, chemicals, and hazardous materials; large-scale theft- and crime-prevention apparatus, and navigation aids. The goal is to monitor the university's physical assets and improve the personal safety of over ten thousand individuals moving in and around thousands of offices, labs, and common spaces in hundreds of buildings.<br\/><br\/>The target SLAM system will focus initially on three capabilities at MIT with a variety of interested partners: efficient facilities monitoring and maintenance (with MIT Physical Plant); scalable asset monitoring for inventory, crime preventi","title":"ITR: Scalable Location-Aware Monitoring (SLAM) Systems","awardID":"0205445","effectiveDate":"2002-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}}],"PIcoPI":["279427","483585","508283","527736"],"PO":["565090"]},"74773":{"abstract":"Simulation and visualization technologies that have become fundamental to contemporary science, engineering, design, and medicine have made possible new ways of seeing and knowing and thereby can change the way practitioners think. This project will explore these processes as they are experienced by scientific and engineering professionals and the ways in which these new technologies reconfigure their professional identities. Five professional fields in which practitioners use simulation and visualization technologies will be examined: archaeologists working telerobotically in deep water; weapons scientists running simulations; neuroscientists imaging the brain; physical chemists analyzing nanoparticles with atomic force microscopy; and architects building in the virtual world. The methods include field interviews, oral histories, and workshops. Fundamental questions, approaches and techniques will be developed for further research and teaching. It is also expected that professional practice will be illuminated in a way that is helpful to researchers and designers in the development of ever more appropriate technologies.","title":"ITR: Information Technologies and Professional Identity: A Comparative Study of the Effects of Virtuality","awardID":"0220347","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["536772","456029","495183","313140","565005"],"PO":["564456"]},"75400":{"abstract":"Rapid and substantial improvements in computing power and sensor and imaging technologies produce data sets of ever increasing size. No longer is it possible to rely exclusively on traditional approaches to interactive data exploration and visualization. It is becoming increasingly important to provide technology that enables scientists to analyze their data interactively at higher levels of abstraction. Two approaches are<br\/>crucial to making possible higher-level data exploration: (1) hierarchical data exploration relying on a data format representing a data set at multiple approximation levels and (2) segmentation- (or feature-based) data exploration. Approach (2) as becoming increasingly important as it allows scientists to study qualitative behavior of their data, which, in turn, can lead to significant compression of data, i.e., the geometrical representations for extracted segments or features are typically much more storage-efficient than original data. We will do research based on approach (2), and our goal is to devise an entire new framework supporting more efficient and effective data exploration via segmentation. Complex scientific data sets represent physical phenomena with billions of elements. These data sets are multi-valued, muti-dimensional and time-varying, and we are no longer able to fully analyze them with merely traditional visualization technology. One can consider various paradigms when developing tools for the exploration of such data sets, and one<br\/>must consider that scientists rarely needs to examine entire data sets. Typically, the interest is in particular regions where certain properties hold. Tools should therefore be developed that allow a scientist to specify the properties of interest, and to segment data sets accordingly.<br\/>The proposed approach supports the definition of higher-level data properties, the efficient extraction of the implied data, and the effective visual representation of the extracted data. Our framework is aimed at multi-valued time-varying data sets, where, for example, grid vertices might have multiple associated scalar, vector and tensor quantities.<br\/>Our goal is to devise new algorithms that support the numerically robust extraction of regions (or boundaries of these regions) that represent similar qualitative behavior. We propose this ''segmentation'' approach to massive data set exploration as we believe that it is a necessary to provide scientists with exploration technology that supports higher-level data representation coupled with real-time system<br\/>behavior.<br\/>The challenge is to generate ''segmented data'' from given multi-valued data sets, store the segmented data efficiently, generate the boundaries of segment boundaries, and display these boundaries. We propose an integrated scheme that supports common data presentation for segmentation and that can be applied to a number of data types (scalar, vector or tensor).<br\/>In addition, we will combine the segmentation framework with new multiresolution techniques---multiresolution techniques that shall support the extraction and visualization of segmented data and extracted segment boundaries at multiple resolution levels. The need for new tools to explore multi-valued time-varying data sets is paramount. The innovative framework we propose to develop will augment current data exploration paradigms. The new framework will allow scientists to define, segment and track meaningful derived data regions.<br\/>Scientists will be able to focus attention to those areas in a data set that carry largest information content, and the power of our framework lies in the fact that it will be possible to more effectively define and extract these area.","title":"VISUALIZATION: Effective and Efficient Segmentation Frameworks for Scientific Data Exploration","awardID":"0222909","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"4080","name":"ADVANCED COMP RESEARCH PROGRAM"}}],"PIcoPI":["451576","495182"],"PO":["565272"]},"71176":{"abstract":"EIA-0205663 University of Colorado Charbel Farhat ITR: A Data Driven Environment for<br\/>Multiphysics Applications<br\/><br\/>The objective of this project is to enable an efficient prediction capability for the response of multiscale interdisciplinary continuous interacting systems. This will be done by an effective union of information technologies, coupled multiphysics sciences, and automated massive experimentation for data gathering to steer adaptive modeling of the observed systemic behavior.","title":"ITR: A Data Driven Environment for Multiphysics Applications","awardID":"0205663","effectiveDate":"2002-10-01","expirationDate":"2004-12-31","fundingAgent":[{"dir":{"id":"04","name":"Directorate for DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE","abbr":"SBE"},"div":{"id":"0406","name":"Office of INTL SCIENCE & ENGINEERING","abbr":"OISE"},"pgm":{"id":"5979","name":"CENTRAL & EASTERN EUROPE PROGR"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}}],"PIcoPI":["226214","292489",184344,"292491"],"PO":["301532"]},"71187":{"abstract":"Novel health monitoring strategies for Highway Bridges and Constructed Facilities are of<br\/>primary significance to the vitality of our economy. Using latest enabling technologies, the objectives of health monitoring are to detect and assess the level of damage to the civil infrastructure due to severe loading events (caused by natural loads or man-made events) and\/or progressive environmental deterioration. Damage identification is performed based on changes in salient response features of the structure, as measured by deployed sensor arrays. Due to the challenging nature of the technical problems associated with this topic, substantial research efforts during the past thirty years were undertaken by many researchers in many areas related to this broad interdisciplinary topic. The proposed research will build on these developments, and address a number of fundamental and basic research challenges towards a next-generation, versatile, efficient, and practical health monitoring strategy. In such a strategy, data from thousands of sensors will be analyzed with long-term and real-time assessment decisionmaking implications. A flexible and scalable software architecture\/framework will be developed to integrate real-time heterogeneous sensor data, database and archiving systems, computer vision, data analysis and interpretation, numerical simulation of complex structural systems, visualization, probabilistic risk analysis, and rational statistical decision making procedures. This development will be undertaken in a concerted and focused comprehensive approach by an inter-disciplinary team of Computer Scientists (CS) and Structural Engineers (SE). It is believed that this inter-disciplinary<br\/>approach will synergize the resolution of basic technical challenges and allow development of the<br\/>framework for future applications in this field. The new framework will also speed up the discovery of new knowledge related to the progressive or sudden deterioration of civil infrastructure systems and the corresponding damage mechanisms. The planned research activities will not only culminate in the deployment of a robust, field-implementable monitoring system, but it will also advance the research frontiers in several active, cutting-edge research areas involving grid storage (curated databases, filesystems, database systems), knowledge-based data integration and advanced query processing, information extraction (data mining, modeling, analysis and visualization), knowledge extraction (reliability\/risk analysis, structural health assessment, physics-based model development), and decision support systems (e.g., emergency response, preventive maintenance, rehabilitation).<br\/><br\/>The entire project will be developed around actual Bridge Testbeds in cooperation with the<br\/>California Department of Transportation (Caltrans), and Industry Partners. These Testbeds will be<br\/>densely instrumented and continuously monitored, and the recorded response databases will be made available for maximum possible use by interested researchers and engineers worldwide. The actual recorded data streams from both laboratory models and bridge testbeds will be a major component for all phases of this research effort. An Internet Portal will integrate all elements and act as a Gateway for the Project.<br\/><br\/>The proposed 5 year project duration will allow the opportunity for resolving key basic research issues of relevance to Structural Health Monitoring, and collaboration between CS and SE<br\/>is simply a necessity. State-of-the-art data acquisition, transmission, and management, involvement of computer vision, refinement of nonlinear system identification and modeling, and practical<br\/>implementation constitute the basic research framework. Applications include long-term condition<br\/>assessment and emergency response after natural or man-made disasters and acts of terrorism for all<br\/>types of large constructed facilities. From a broader perspective, the proposed effort will be a major boost in defining and shaping additional long-term interaction and collaboration opportunities between CS and SE, with wide national and international implications, as well as strongly benefiting from leveraging resources and ongoing monitoring activities.","title":"ITR: Collaborative Research: An Integrated Framework for Health Monitoring of Highway Bridges and Civil Infrastructure","awardID":"0205720","effectiveDate":"2002-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1688","name":"ITR LARGE GRANTS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0703","name":"Division of CIVIL, MECHANICAL, & MANUFACT","abbr":"CMMI"},"pgm":{"id":"1631","name":"CIVIL INFRASTRUCTURE SYSTEMS"}}],"PIcoPI":["434470","509001","423905","518814"],"PO":["434241"]},"75785":{"abstract":"Two years of support are being requested if at all possible. The reason for this is <br\/>because it is necessary to complete several key steps before submission as a regular <br\/>proposal. The first summer is to prepare papers for presentation and\/or appearance in <br\/>conferences and\/or journals. During the ensuing academic year, the intellectual content <br\/>of this potentially controversial material will be circulated for comment, both publicly in <br\/>talks and privately among colleagues. Based on the feedback received, the ideas put forth <br\/>will be modified and corrected. Only then does it make sense to use the following <br\/>summer to involve an undergraduate student in implementing the salient features of the <br\/>proposed work for both a greater depth of understanding and demonstration purposes. <br\/> By this point in time, that would mean there should be preliminary results <br\/>appearing in print, with supporting documentation on the web explaining this material. <br\/>In addition, more capable assistive computing technology will be available by then (to <br\/>help me overcome my hand disability), so that I might be able to more actively <br\/>participate in the implementation details of that work. When all of this is readily in place, <br\/>then it will be possible to submit a competitive regular proposal.","title":"A Mathematical Logic for Physically Feasible Computation","awardID":"0225063","effectiveDate":"2002-10-01","expirationDate":"2005-11-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}}],"PIcoPI":[197235],"PO":["499399"]},"66875":{"abstract":"EIA-0130843 Chris D. Cox University of Tennessee Hybrid Testbed for Evaluation of Cell-Cell Communication Models in Prokaryotes<br\/> Information processing strategies utilized by biological cells are fundamentally different than those used for silicon-based computing and thereby hold great promise for the development of improved algorithms, software, and hardware. However, our understanding of how genetic networks control information flow in biological systems needs to be further developed. To this end, various research programs are underway to develop models of genetic networks; however, the success of this effort could falter unless tools to directly observe cell processes and validate the models are developed and integrated into the model development process. The proposed research program seeks to develop a hybrid testbed to calibrate and validate models of genetic regulatory networks controlling quorum sensing behavior in prokaryotic organisms. The benefits or [sic] this project to biological information technology include: i) development of a system for calibration and validation of genetic regulatory models leading to a better understanding of intracellular information flow; ii) improved technologies for silicon-to-cell and cell-to-cell silicon information transfer; and iii) a better understanding of information transfer between cells through chemical signaling. Project objectives, methods, and impacts are described below.","title":"Biological Information Technology Systems - BITS: Hybrid Testbed for Evaluation of Cell-Cell Communication Models in Prokaryotes","awardID":"0130843","effectiveDate":"2002-10-01","expirationDate":"2006-02-28","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"V612","name":"DARPA-COMP MODELS & SIMULATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"V013","name":"DARPA-COMP MODELS  SIM OF QUOR"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"W269","name":"DARPA-COMPUTATIONAL MODELS & S"}}],"PIcoPI":["506934","138507","396064","457728"],"PO":["474792"]},"76555":{"abstract":"ABSTRACT<br\/>0229339<br\/>Erik Rosenthal <br\/>U of New Haven<br\/><br\/>The problem SAT determining whether a formula in propositional logic is satisfiable has<br\/>a long history in computer science. It is important in complexity theory and has myriad applications<br\/>in fields such as robotics, optical character recognition, database systems, computer<br\/>architecture, circuit design, and verification, to name but a few.<br\/>The traditional view of much of the mechanical theorem proving community has been that<br\/>interesting applications of automated deduction require first-order rather than propositional<br\/>logic. That view has begun to change in light of recent successes with propositional logic.<br\/>Propositional logic is of course intractiable (unless NP = P). One approach to this problem<br\/>with knowledge representaion and database systems is knowledge compilation preprocessing<br\/>the underlying propositional theory. The idea is to do as much computation as possible in an<br\/>offline phase. Then queries in the online phase can be handled quickly. Horn clauses, ordered<br\/>binary decision diagrams, sets of prime implicates\/implicants, and decomposable negation<br\/>normal form (DNNF) have all been proposed as targets of such compilation.<br\/>Path dissolution is an inference mechanism that works naturally with formulas in NNF. It<br\/>is strongly complete in the sense that any sequence of link activations will eventually terminate,<br\/>producing a linkless formula called the full dissolvent. The paths that remain are models of the<br\/>original formula. Full dissolvents have been used effectively for computing the prime implicants<br\/>and implicates of a formula.<br\/>Decomposable negation normal form was first introduced in 1998 and is currently being<br\/>studied for application to knowledge representation and database systems. A formula in DNNF<br\/>is a full dissolvent, and it appears that many of the advantages of DNNF exist with full dissolvents.<br\/>It also appears that a full dissolvent of a formula can be obtained more effciently than<br\/>a DNNF representation of the formula. The main thrust of this project will be to examine the<br\/>relationship of DNNF and full dissolvents.","title":"SGER: Path Dissolution in Propositional Logic","awardID":"0229339","effectiveDate":"2002-10-01","expirationDate":"2004-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["337750"],"PO":["321058"]},"77534":{"abstract":"The workshop will focus on providing directions for data mining. Specifically the focus will be on the following areas:<br\/>1. Data Mining for Bioinformatics<br\/>2. Data Mining for Security<br\/>3. Semantic Web and Data Mining<br\/>4. Data Mining for Pervasive Computing<br\/><br\/>Prominent researchers in data mining and machine learning as well as government program managers will be invited to participate in the workshop. The output will be a report describing the areas of future research in data mining.","title":"Next Generation Data Mining (NGDM'02) Workshop","awardID":"0233587","effectiveDate":"2002-10-01","expirationDate":"2003-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6856","name":"ARTIFICIAL INTELL & COGNIT SCI"}}],"PIcoPI":["558176","533183","421169"],"PO":["563751"]},"71100":{"abstract":"Valuable information about the world around us is being lost. Every day we lose information of both current and historical significance, simply because it is too difficult to capture 3D models of our environment. Today, such models are primarily created by hand, making measurements with tape measures and entering the results manually. Even after models are constructed, we do not know the best ways to visualize them effectively.<br\/><br\/>This research proposes to design and develop an end-to-end solution to the problem of documentation, dissemination, and display of real-world environments. To focus the research, two driving problems are targeted: forensic science and education. This research brings together an interdisciplinary team including the law enforcement community, the Federal Bureau of Investigation and the Armed Forces Institute of Pathology. Additionally, one of our team members, from the New Orleans Museum of Art, is directing a major exhibition in which we propose to include a virtual environment of Monticello, President Jefferson's home.<br\/><br\/>We will tackle the problems of acquiring, processing, representing, rendering and display of real- world data, focusing on the two target applications. Our goal is to design and build a complete end-to-end prototype system. The work includes:<br\/><br\/>Acquisition. We will mount an accurate rangefinder on a motorized cart and tackle the problem of capturing as complete a model as possible. This requires accurate tracking and a solution to the next best view problem in a practical sense, including resolution, reflectance, and importance of the surfaces.<br\/>Processing. A very difficult problem is the registration of color with depth data, and of scans from multiple locations. We will develop algorithms to robustly register the data, and filter to reduce outliers and noise.<br\/>Representation. We will develop algorithms to process the massive amounts of data (100 MB per scan, tens of scans per room) into both efficient geometric and image-based representations. We will compare and contrast the representations in terms of quality and efficiency.<br\/>Rendering. We will develop hierarchical and out-of-core algorithms to render the image- based and geometric models at interactive rates, and research perceptually driven rendering techniques to maximize rendering efficiency.<br\/>Display. This is perhaps the most difficult part of the problem because it involves the way humans perceive the environment. We will develop a hybrid projector\/head-mounted-display system to provide stereo views to multiple viewers in a life-sized walking environment. The goal is to give the participants a sense of presence, of being in Mr. Jefferson's house.","title":"ITR: Collaborative Research: Image-Based Rendering in Forensic Reconstruction and Historical Preservation","awardID":"0205324","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}}],"PIcoPI":["451003"],"PO":["565272"]},"71133":{"abstract":"Andresen Abstract<br\/><br\/>This project is building sensor net infrastructure to support intelligent, mobile medical monitoring devices that continuously assess state of health in concentrated and distributed cattle herds. <br\/><br\/>Research is conducted to develop and integrate the following areas of information technology:<br\/><br\/>- Receiver systems that manage wireless traffic and prioritize overlapping signal streams. <br\/>- Scheduling algorithms that adaptively determine where data analysis should occur and which areas require more in-depth analysis.<br\/>- Algorithms that search for data patterns that may indicate problems.<br\/>- Security mechanisms that maintain confidentiality of economic data and herd health information associated with individual farms, while providing epidemiological data of statistical validity.<br\/>- Low cost system components (affordable by farmers and producers <br\/>who may have tenuous financial stability).<br\/>- Robust packaging and linking technology for biomedical sensors, GPS receivers, and Bluetooth-enabled devices for survival in difficult environments.<br\/><br\/>The experimental component of the project uses durable, small sensors (e.g., to report animal identity, position, temperature, blood pressure, and other physiological data), with Bluetooth-compliant monitoring stations near cattle congregation points, such as feed bunks and watering troughs. These stations upload data from nearby environmental sensors, Bluetooth-enabled devices with global positioning capability that are worn by the animals, and wearable\/remote biomedical sensors. These results are integrated with previous sensor data, weather reports, weather forecasts, infrared camera images, and prior health assessments. The project is developing initial algorithms to perform rapid analysis on local data prior to uploading summary data for the ranch to regional databases so that these data can be correlated with data provided by other producers. Significant findings can then be immediately broadcast to appropriate medical personnel and producers.<br\/><br\/>Monitoring systems will improve the ability of the animal sciences industry to react to and predict disease onset and its epidemiological spread (e.g., mad cow disease, hoof and mouth disease), whether from natural or terrorist events. Trend analysis, information storage, and health prediction lessons learned from this effort will have immediate application to distributed medical systems targeted at assessing and predicting human state of health and the spread of disease in human populations. These networked embedded sensor systems have implications for homeland security, veterinary medicine, the agriculture industry, and in the long term, human medicine and quality of life.","title":"ITR: Veterinary Telemedicine - Proactive Herd Health Management for Disease Prevention from Farm to Market","awardID":"0205487","effectiveDate":"2002-10-01","expirationDate":"2004-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2801","name":"EMBEDDED & HYBRID SYSTEMS(EHS)"}}],"PIcoPI":["559268","563764","226278","226279",184182],"PO":["561889"]},"74686":{"abstract":"Complex spatial structures permeate the environment of the future. Nanoscale molecular machines allow sophisticated methods of computing and integrating data with environments. Human-scale industrial designs provide transportation solutions with greater control and efficiency. Architectural forms, growing in both complexity and inter-connectivity, define our experience in the ever-expanding city. While many advances have been made in visualizing these shapes in three dimensions, designers have yet to leverage the power of Information Technology in the early stages of creation. Initial ideas are still mostly constructed with two-dimensional paper sketches or simple physical tools, such as the molecular modeling kits commonly used in organic chemistry. Digital representations in contrast are mutable, dynamic, and re-configurable at a lowest level of the computer. These are difficult for humans to manipulate effectively with contemporary 2D interfaces. Thus designers, engineers, artists and architects must choose either a traditional, pre-Information Technology environment, where they can freely play with simple building blocks, or a digital space where complex elements can be composed only after exhaustive data transformations, strict planning and foresight, and the occasional trick. The work proposed here bridges this divide by creating tools that facilitate (1) the construction of 3D models with the speed of pencil sketches; (2) physical intuition of tangible models; (3) fluid representation of digital data structures; and (4) active<br\/>response of algorithmic simulation.<br\/><br\/>The results of the work will have immediate application in the fields of architecture, industrial design, and molecular chemistry. Because the research team treats these separate problems within a common framework the results will be broadly applicable to many diverse problems, fundamentally altering the human ability to create structures in space.","title":"ITR: Constructive Visualization: Understanding Spatial Relationships Through Interaction","awardID":"0219979","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["320958"],"PO":["565272"]},"71188":{"abstract":"Methods and technologies for effectively supporting application development have not kept pace with the changing character of applications and development groups. Applications increasingly span multiple organizations, involving thousands of interacting and heterogeneous computers. The creation and evolution of these inter-organizational systems typically involves many separate development organizations, who contribute to multiple software systems that must interact to support inter-organizational activities. This research project is focused on developing the knowledge for effectively building and evolving software for <br\/>distributed inter-organizational systems and by distributed multi-organization teams. As the problem is intrinsically interdisciplinary, so is the approach. This project will create novel development methods and <br\/>technologies based upon an integrated social and technological perspective. Insights come from many quarters. From a technical standpoint, the emergence of adaptive software architectures and event-based technologies suggest novel approaches to inter-organizational applications. On the social side, previous successes in coordination and awareness technologies must be brought to inter-organizational settings. An overarching concern is that inter-organizational settings provide no central authority - rather, social arrangements and interactions play dominant roles in shaping technology development and deployment. The project will be empirically grounded through research partnerships with four external organizations.","title":"ITR: An Integrated Social and Technical Approach to the Development of Distributed, Inter-organizational Applications","awardID":"0205724","effectiveDate":"2002-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}}],"PIcoPI":["533779","483634","432332","518420","464219"],"PO":["564456"]},"65897":{"abstract":"A Peer-to-peer networked system is a collaborating group of Internet nodes which overlay their own special-purpose network on top of the Internet. Such a system performs its own application-level routing on top of IP routing. These systems share some of the same characteristics as the Internet in that they can grow to be quite large, need to utilize distributed control and configuration, employ a naming scheme that allows them to address a node without knowing its exact whereabouts, and possess a routing mechanism that allows each node to meaningfully communicate with the rest of the system. Typically a peer-to-peer network performs a very specific purpose such as distributed data storage, cache replication, multicasting etc. and uses normal Internet functionality for all other purposes. These systems are diverse enough that it would not be desirable to make modifications to the IP routing\/naming protocols to support each instance of such a system.<br\/><br\/>Peer-to-peer systems such as Napster and Gnutella have, of late, received a fair amount of<br\/>attention in the non-technical literature. More importantly, several research groups have recently<br\/>designed a variety of peer-to-peer systems as infrastructure for flexibly evolving the Internet, for<br\/>large-scale network storage, for anonymous publishing, and for application-level multicasting. At<br\/>the core of many of these systems lie innovative and novel distributed algorithms for disseminating<br\/>content. The principal investigators classify these systems into structured systems where there is<br\/>global consensus on where a document is stored, and unstructured systems where no such consensus exists.<br\/><br\/>Peer-to-peer systems have a rich theoretical structure, and sophisticated algorithmic techniques<br\/>have been employed in the systems currently under development. In preliminary work, the princi-<br\/>pal investigators use the small-world model to improve the performance of a specific unstructured<br\/>system called Freenet. Motivated by their preliminary work, this proposal plans to take a principled<br\/>look at the properties of various algorithms proposed in the peer-to-peer literature. Some of the<br\/>specific questions that the principal investigators intend to address are understanding the funda-<br\/>mental bounds on the performance of peer-to-peer systems, designing algorithms that allow these<br\/>systems to perform at peak availability and minimum latency, studying the impact of topology and<br\/>caching on latency, and understanding the role that the small-world model might play in improving<br\/>the performance of these systems.<br\/><br\/>The overall attention that the peer-to-peer technology has garnered in the recent past, and the<br\/>various proposed businesses based on this model, point to the potential broad impact that this work can have. The principal investigators hope that this work will provide the systems community with the theoretical tools necessary to rigorously understand the behavior of some of their designs as well as rules of thumb to choose between alternative architectures, protocols, and algorithms.<br\/><br\/>The principal investigators intend to integrate some of this material into a graduate course<br\/>on \\Algorithmic Issues in Communication Networks\". Their classroom treatment of these topics<br\/>will be formal, but with clearly indicated motivations from and applications to real systems. The<br\/>lecture notes and course projects developed by the principal investigators will be made publicly<br\/>available.","title":"Algorithm for Networked Peer-to-Peer Systems","awardID":"0126347","effectiveDate":"2002-10-01","expirationDate":"2006-03-31","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4097","name":"NETWORKING RESEARCH"}}],"PIcoPI":["553707","429621"],"PO":["565090"]},"72079":{"abstract":"Feron, Eric<br\/>CCR-0208831<br\/><br\/>MIT is developing ARETES, an 'Architecture for Efficient and Trusty Embedded Systems', which is a unified framework to design the critical control and communication infrastructure support for safe and efficient embedded systems. ARETES has four components to it: development of a mathematical framework for assessing safety and efficiency of the systems of interest, design of computationally efficient algorithms, development of the necessary software modules, and simulation case studies.<br\/><br\/>To meet the safety and efficiency requirements, ARETES is adopting a control theoretic approach. The idea is to use and innovate upon the existing theory via three-faceted approach to account for the performance objective and resource constraints while safeguarding against uncertainty in the environment, component level imperfections, and faults in the inter-component communications. As the first facet, the existing multiplier theory will be extended to guarantee the worst case performance for resource constrained systems. To give some specific examples, the best fit integral quadratic constraint (IQC) characterization will be derived for the inherent uncertainties, and for the bandwidth saturation and delay constraints imposed by the inter-component coordination. As the second facet, a database of elementary blocks to approximate the hybrid models of such systems will be compiled. Links between the multiplier theory and hybrid systems analysis methods will be established via novel concepts such as a 'robust hybrid automaton'. As the third key facet, a set of parameterized test problems will be developed to characterize the impact of task scheduling and the above mentioned imperfections on the online software complexity and the offline software verification process. ARETES aims at deriving analytical performance bounds for these canonical models and will develop a library of scalable software modules to facilitate the simulation studies.<br\/><br\/>The theoretical advances being pursued in ARETES include robust control theory since the best fit IQC characterization facilitates the least conservative input-output stability analysis. ARETES will also develop links between the input-output hybrid system analysis approach and the multiplier theory approach. The canonical models and the associated library of algorithms will be directly useful in deriving analytical performance bounds for various scenarios of interest. Besides its relevance to research organizations and industry, this will have a significant technological value in academia. <br\/><br\/>Technology being developed in ARETES has immense utility in providing the critical infrastructure support necessary in various applications involving coordinated agents that are either fully or partially autonomous. Examples of such applications include coordinated intrusion alert systems, monitoring of hazardous terrains, air traffic control systems, slowdown warning systems for intelligent vehicle highways, animation industry applications employing teams of coordinated intelligent inanimate objects, coordinated target tracking in scientific explorations, and educative projects such as the complex trajectory planning of a team of mini-helicopters in a laboratory setting,","title":"Architecture for Efficient and Trusty Embedded Systems","awardID":"0208831","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2801","name":"EMBEDDED & HYBRID SYSTEMS(EHS)"}}],"PIcoPI":["495325"],"PO":["561889"]},"71365":{"abstract":"Novel health monitoring strategies for Highway Bridges and Constructed Facilities are of<br\/>primary significance to the vitality of our economy. Using latest enabling technologies, the objectives of health monitoring are to detect and assess the level of damage to the civil infrastructure due to severe loading events (caused by natural loads or man-made events) and\/or progressive environmental deterioration. Damage identification is performed based on changes in salient response features of the structure, as measured by deployed sensor arrays. Due to the challenging nature of the technical problems associated with this topic, substantial research efforts during the past thirty years were undertaken by many researchers in many areas related to this broad interdisciplinary topic. The proposed research will build on these developments, and address a number of fundamental and basic research challenges towards a next-generation, versatile, efficient, and practical health monitoring strategy. In such a strategy, data from thousands of sensors will be analyzed with long-term and real-time assessment decisionmaking implications. A flexible and scalable software architecture\/framework will be developed to integrate real-time heterogeneous sensor data, database and archiving systems, computer vision, data analysis and interpretation, numerical simulation of complex structural systems, visualization, probabilistic risk analysis, and rational statistical decision making procedures. This development will be undertaken in a concerted and focused comprehensive approach by an inter-disciplinary team of Computer Scientists (CS) and Structural Engineers (SE). It is believed that this inter-disciplinary<br\/>approach will synergize the resolution of basic technical challenges and allow development of the<br\/>framework for future applications in this field. The new framework will also speed up the discovery of new knowledge related to the progressive or sudden deterioration of civil infrastructure systems and the corresponding damage mechanisms. The planned research activities will not only culminate in the deployment of a robust, field-implementable monitoring system, but it will also advance the research frontiers in several active, cutting-edge research areas involving grid storage (curated databases, filesystems, database systems), knowledge-based data integration and advanced query processing, information extraction (data mining, modeling, analysis and visualization), knowledge extraction (reliability\/risk analysis, structural health assessment, physics-based model development), and decision support systems (e.g., emergency response, preventive maintenance, rehabilitation).<br\/><br\/>The entire project will be developed around actual Bridge Testbeds in cooperation with the<br\/>California Department of Transportation (Caltrans), and Industry Partners. These Testbeds will be<br\/>densely instrumented and continuously monitored, and the recorded response databases will be made available for maximum possible use by interested researchers and engineers worldwide. The actual recorded data streams from both laboratory models and bridge testbeds will be a major component for all phases of this research effort. An Internet Portal will integrate all elements and act as a Gateway for the Project.<br\/><br\/>The proposed 5 year project duration will allow the opportunity for resolving key basic research issues of relevance to Structural Health Monitoring, and collaboration between CS and SE<br\/>is simply a necessity. State-of-the-art data acquisition, transmission, and management, involvement of computer vision, refinement of nonlinear system identification and modeling, and practical<br\/>implementation constitute the basic research framework. Applications include long-term condition<br\/>assessment and emergency response after natural or man-made disasters and acts of terrorism for all<br\/>types of large constructed facilities. From a broader perspective, the proposed effort will be a major boost in defining and shaping additional long-term interaction and collaboration opportunities between CS and SE, with wide national and international implications, as well as strongly benefiting from leveraging resources and ongoing monitoring activities.","title":"ITR: Collaborative Research: An Integrated Framework for Health Monitoring of Highway Bridges and Civil Infrastructure","awardID":"0206275","effectiveDate":"2002-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}}],"PIcoPI":["226241"],"PO":["7594"]},"70166":{"abstract":"In this proposed RUI research, we will apply our current research results supported by NSF<br\/>to design efficient parallel algorithms and to develop software to reliable numerical solutions<br\/>for both unconstrained and constrained multivariate global nonlinear optimization problems.<br\/>To achieve high reliability, even in the presence of uncertainty in the data, roundo error, and<br\/>nonlinearities by finite digit computations, we apply interval arithmetic in this project. The basic<br\/>algorithms to be used are interval branch-and-bound method and interval Newton\/generalized<br\/>bisection method.<br\/>In designing parallel algorithm and developing portable software, we will take full advantages<br\/>of parallel computing to significantly reduce not only elapsed computation time but also total<br\/>amount of computation due to the spatial nature of the problem we address. To achieve high efficiency,<br\/>we will balance workload dynamically among available processors through inter-processor<br\/>communication. The software will be architecture independent. General sparsity and scalability<br\/>will be considered as well. The research results, parallel software package, installation and user<br\/>guides, and testing examples will be freely disseminated through the Internet.","title":"Parallel Reliable Global Optimization with Interval Arithmetic","awardID":"0202042","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}}],"PIcoPI":["347186"],"PO":["381214"]},"74522":{"abstract":"This research studies and designs coordinated trafficconditioning, network monitoring, flow control, and provisions the networkproperly to meet the demands for data and multimedia traffic<br\/>in various applications. An edge router component to monitor a large network for service level agreement (SLA) violations and bandwidth theft attacks is developed. This network monitoring scheme involves only edge routers. The conditioner and flow control components alleviate the congestion and unfairness in resource allocation. The edge routers share the congestion information with upstream routers to save resource wastage in the downstream domains. <br\/><br\/>Continuous monitoring of network activity is required to maintain confidence in the security of networks with quality of service (QoS) support. These solutions have to be scalable in order to<br\/>deploy them in the heterogeneous Internet. A scalable edge router cannot use excessive per-flow information and cannot involve core routers. The researchers follow this principle in designing edge routers to achievescalability. <br\/><br\/>The following is a list of contributions that are obtainedthrough experiments, simulation, and collaboration with industry.<br\/><br\/>1. For the edge router, we determine the optimal traffic assignment rate for each traffic class and weight assignments at the queue to maximize the profit for the service provider of a network<br\/>domain. Proper provisioning provides service level agreement bounds, such as throughput, delay, and loss for each user.<br\/><br\/>2. To ensure all flows are getting their share of SLA, the flows of a network domain is monitored for possible SLA violation and bandwidth theft attack. We define and employ throughput,<br\/>delay, packet loss, and security as QoS parameters for the design of an edge-to-edge SLA monitoring scheme. This contributes to detecting service violations and attacks, especially denial of service (DoS) attacks.<br\/><br\/>3. The traffic conditioners at the edge routers intelligently mark and shape packets differentially based on the class parameters and according to network state. Through experimental studies,<br\/>we show that the flow characteristics provide better resource utilization and improve the application level quality of service.<br\/><br\/>4. The edge routers detect and regulate unresponsive flows that cause poor performance for adaptive flows, such as TCP, which retreats during congestion. The ingress (entry) edge routers<br\/>propagate the congestion information to the egress (exit) routers of a previous upstream network domain. This reduces the resource wastage at the downstream network due to undelivered<br\/>packets.","title":"ITR: Scalable Edge Router for Differentiated Services Networks","awardID":"0219110","effectiveDate":"2002-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"4097","name":"NETWORKING RESEARCH"}}],"PIcoPI":["540775"],"PO":["434241"]},"74412":{"abstract":"EIA-0218521<br\/>Lin, David<br\/>Cornell University-State<br\/><br\/>CRCNS: Modeling Pathfinding and Target Recognition in the Olfactory System<br\/><br\/>Statement for public consumption<br\/><br\/><br\/>The proper development of the nervous system is essential to the f unction of any organism. The nervous system integrates sensory input, coordinates motor output, controls autonomic functions, and shapes memory and cognition. To accomplish these tasks, the nervous system employs billions of neurons that will form trillions of connections with their appropriate targets. For example, in the olfactory system, odorant in the environment are detected by olfactory sensory neurons (OSNs) located in the nose. <br\/>In order for odorant information to be properly transmitted to the brain, however, these neurons must be connected, or \"wired together,\" with their appropriate partners in the brain. How this critical feat of pathfinding and target recognition is accomplished is still largely unknown. In one model, this process is mediated b cues located at various points within the brain that would interact with and guide neurons to their target. These cues would form, in essence, a \"roadmap\", that would help guide neurons to their target.<br\/>In this proposal, we will combine bioinformatics, statistical methods, and genomics to begin to determine the shape of this map. As a model system, we will study the ability of olfactory sensory neurons (OSNs) to identify their targets in the olfactory bulb.","title":"CRCNS: Modeling Pathfinding and Target Recognition in the Olfactory System","awardID":"0218521","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":[193254,193255,"253546"],"PO":["564318"]},"71156":{"abstract":"New directions in clustering and learning<br\/><br\/>Faced with ever-larger amounts of data, researchers, government <br\/>institutions, corporations and even the general public seek tools that help them deal <br\/>with large bodies of information, identify patterns in it, learn what these<br\/>patterns mean, and act upon that information in a timely fashion. <br\/>Developing such tools involves a novel and interesting blend of algorithms, <br\/>statistics, AI, and machine learning. The project assembles a team of experts (four<br\/>from academia and two from industry) in these areas to attack an interesting<br\/>and meaningful subset of such problems which have the general flavor of<br\/>clustering or learning.<br\/><br\/>The defining philosophy of this proposal is that no clear boundary <br\/>Separates the twin notions of clustering and learning. Clustering is usually driven<br\/>by the end goal of learning, but can also be viewed as a learning task<br\/>in itself since it results in a more compact description of the data.<br\/>By the same token all learning involves clustering of some sort, and<br\/>in fact this viewpoint is implicit in recent papers in the learning <br\/>literature. The project takes an integrated view of the entire problem of learning<br\/>patterns in data, starting from streaming computations that might produce<br\/>representative sketches of the data as it streams by, to problems of <br\/>clustering data into meaninful patterns (with attendant problems of outlier removal,<br\/>multiobjective optimization etc.), to learning algorithms that fit<br\/>sophisticated models (SVMs, bayesian nets, gaussian mixtures etc.) for <br\/>inference and reasoning tasks.<br\/><br\/>The investigators believe that all these disparate algorithmic efforts have<br\/>unifying ideas. Furthermore, their synergistic approach throws up several<br\/>interesting ideas of its own that could lead to significant advances. <br\/>Examples: include using coding theoretic ideas in disparate applications such as <br\/>Multiclass learning (a broad class of learning problems including text and speech <br\/>categorization, part-of-speech tagging, gesture recognition etc.) and shape recognition<br\/>in vision; the use of clustering ideas to do dimension reduction (offering<br\/>an alternative to popular SVD based approaches), and using ideas from<br\/>approximation algorithms and clustering to do near-optimal model fitting<br\/>for models such as bayesian nets.<br\/><br\/>The project also includes a management and educational plan that involves<br\/>dissemination of the ideas of this research through development of new <br\/>courses and also pieces of learning software that will be placed in the public domain.<br\/>Algorithms developed in as part of this project will be tested on large <br\/>datasets, including those obtained from Google Inc. Some algorithmic ideas will <br\/>also be implemented in industry (including Google).","title":"ITR: New directions in clustering and learning","awardID":"0205594","effectiveDate":"2002-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}}],"PIcoPI":["521734","549429","542007"],"PO":["499399"]},"71046":{"abstract":"EIA- 0205111<br\/>Eduard Hovy<br\/> Information Sciences Institute, USC<br\/><br\/>This grant will explore the automated construction of domain ontologies across governmental datasets. There are many \"data silos\" in government; that is, collections of data which cannot be cross-compared or queried. This is a major problem in presenting government information to consumers which is tailored according to their interests, rather than according to the organizational structure of government agencies. While such automated tools cannot completely construct accurate ontologies, they can take much of the labor intensiveness out of such efforts, particularly where the number of terms or variables is so large (in the thousands) as to prevent manual efforts. Working with the US Environmental Protection Agency, which has many datasets of interest to the public, the research team will develop a semi-automated toolkit to aid in the construction of ontologies.","title":"ITR: Information Discovery in Digital Government: Self-extending Topic Maps and Ontologies (GrowOnto)","awardID":"0205111","effectiveDate":"2002-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1706","name":"DIGITAL GOVERNMENT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"V050","name":"NSA-WORLD RENOWNED EXPERTS FRO"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"V177","name":"NSA-MKIDS PROJECT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"V242","name":"CIA-MACHINE TRANSLATION(MT)"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"V325","name":"NSA-MKIDS PROJECT"}}],"PIcoPI":["543277"],"PO":["371077"]},"71167":{"abstract":"Mobile systems primarily processing multimedia data are expected to become<br\/>a dominant computing platform for many application domains. The design of<br\/>such systems imposes several new challenges, as it must consider demanding,<br\/>dynamic, and multidimensional resource requirements and constraints, with<br\/>energy becoming a first-class resource. At the same time, the ability of<br\/>multimedia applications to trade off output quality for system resources<br\/>and the difference between their peak and average demands offers a huge<br\/>opportunity for optimization.<br\/><br\/>A promising approach to meet the challenges of mobile multimedia systems,<br\/>therefore, is to design all system layers with an ability to adapt in<br\/>response to system or application changes. Further, to reap the full<br\/>benefits of these adaptations, all system layers must cooperate to reach a<br\/>system-wide globally-optimal configuration. This research seeks to develop<br\/>and demonstrate an integrated cross-layer adaptive system where hardware<br\/>and all software layers cooperatively adapt to changing system resources<br\/>and application demands, seeking to maximize user satisfaction while<br\/>meeting resource constraints of energy, time, and bandwidth. This work is<br\/>expected to have a large impact because it will expose sources of<br\/>substantial performance improvement not available before, for a platform of<br\/>increasing importance to many application domains.","title":"ITR: Collaborative Hardware-Software Adaptation for Multimedia Applications","awardID":"0205638","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}}],"PIcoPI":["470450","551063","561784","561785"],"PO":["325495"]},"74698":{"abstract":"Emulsions, or suspensions of droplets of one liquid phase in another, occur in a wide variety of chemical, biological and materials processes. Some examples include liquid-liquid extraction, the flow of blood cells, thermal induced phase separation for the formation of microporous membranes and the formation of the immiscible polymer blends with superior mechanical, thermal and electrical properties. In these and many other applications the emulsion is processed so that drops can undergo large <br\/>deformations, break-up, and coalesce. The proposed project is an interdisciplinary collaboration concerned with large-scale micromechanical simulations of low-Reynolds-number or Stokesian emulsions. To deal with numerous challenges associated with this class of problems, the project brings together three researchers with expertise in geometrical modeling and visualization, fluid mechanical modeling and computations, and fast boundary element methods. The principal objectives of the project are twofold. First, explore, develop and implement various tightly integrated computational methods that will significantly advance existing computer simulation capabilities for Stokesian emulsions. Second, conduct simulations critical to the better qualitative understanding of the coalescence-induced-coalescence phenomenon and sheared emulsions containing highly deformable second-phase droplets capable of coalescence and break-up.","title":"ITR: Large-Scale Simulations of Emulsions","awardID":"0220037","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["517319",194049,"507424"],"PO":["565272"]},"74346":{"abstract":"EIA-0218186<br\/>Ghosh, Bijoy K<br\/>Washington University<br\/><br\/>CRCNS: Collaborative Research : How is Information Coded in Turtle Visual Cortex ?<br\/><br\/>Visual stimuli evoke a propagating wave of activity in the visual cortex of freshwater turtles. Preliminary work suggested that information about the position of stimuli in visual space is coded in the spatiotemporal dynamics of these waves. Effectively, there may be a map of visual space to the dynamics of the visual cortex. This hypothesis is being examined in a collaborative effort involving three laboratories. David Senseman in San Antonio is using voltage sensitive dye methods to record the waves produced by presenting spots of light at 35 spots on the retina. These studies will characterize the features of the map based on repeated presentations of stimuli at 35 loci. Philip Ulinski in Chicago is developing a large-scale model of the visual pathway of turtles. Models of individual retinal ganglion cells that combine both classic filter-based approaches to modeling ganglion cells, with compartmental modeling of ganglion cells are being constructed. They are being used to construct 35 patches of a model retina that match the 35 loci. Physiological studies of the biophysics of neurons in the lateral geniculate complex of turtles are being carried out. They are used to develop a model of the lateral geniculate complex, which is the last step in modeling the retino-geniculate-cortical pathway. Bijoy Ghosh in St. Louis is developing refined estimation techniques that allow the position of a visual stimulus to be estimated from the dynamics of the cortical waves. This work is providing the mathematical framework needed to characterize a potential map of visual space to the dynamics of the wave. This work is significant because it is characterizing a novel method of coding information in visual cortex that may apply to higher order cortical areas in mammals, as well as turtles.","title":"CRCNS: Collaborative Research: How is Information Coded in Turtle Visual Cortex?","awardID":"0218186","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["457288"],"PO":["564318"]},"74765":{"abstract":"EIA-0220301 <br\/>John Hetling <br\/>University of Illinois-Chicago <br\/><br\/>BITS: Sensory Coding and Pattern Recognition with Hybrid Olfactory<br\/><br\/>The natural olfactory system combines high sensitivity with broadband detection, excellent discrimination, and fast response times. These characteristics are a result of the sensor array design and the down-stream neural processing. In addition to being relevant to the development of an artificial nose, the pattern recognition problem faced by olfactory systems is analogous in many respects to problems in data mining, especially related to DNA micro arrays. To further our understanding of the biological solution to this general problem, it will be advantageous to record from multiple olfactory neurons simultaneously. This proposal seeks to develop a novel hybrid-device olfactory biosensor, and then to use this system to investigate signal coding and pattern recognition. This will provide information about the coding strategy of the natural system, and provide a test case for new approaches to pattern recognition in complex signals.","title":"Sensory Coding and Pattern Recognition with Hybird Olfactory Biosensor","awardID":"0220301","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1713","name":"WORKFORCE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1994","name":"BIOINFORMATICS PROGRAM"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"W565","name":"NIH-PA ON COMPUT NEUROSCIENCE"}}],"PIcoPI":["288514",194232,194233],"PO":["564318"]},"74402":{"abstract":"Distributed denial of service (DDOS) attacks have emerged as a prevalent way to take down web sites and have imposed financial losses to companies. The CSI\/FBI survey (CSI 2001) shows that 36% of respondents in the last 12-months period have detected denial of service, which imposed more than $4.2 million financial losses. The effectiveness of DDOS defenses depends on many factors such that the nature of the network's topology, the specific attack scenario, and various <br\/>characteristics of the network routers. However, little research has focused on the tradeoffs inherent in this complex system. The researchers are developing a computational testbed to study security policies and the associated technologies that provide defenses against DDOS attacks. The researchers are using this framework to evaluate various policies and technologies. Out model and the ensuing analyses are informed by research in the areas of computer science, information science, organizational theory and social networks.<br\/><br\/>There have been a number of proposals on how to control the on-going DDOS attack traffic. None have been widely deployed. The effectiveness of DDOS defenses depends on many factors, such as the type of network topology, the type of attacks and whether all ISPs are compliant in <br\/>establishing defenses. However, little is known about the interactions among these factors. Knowing what tradeoffs will occur as these factors vary will enable stakeholders to make more informed security policy decisions in which they adjust for the chance that others may not make <br\/>the same decisions. Our research illuminates these tradeoffs. Moreover, the computational model the researchers are building enables the user to examine the tradeoffs associated with various DDOS defenses and attack scenarios at the router level.<br\/><br\/>The researchers focus on two basic research questions. First, how do ISPs provide DDOS defenses at the lowest cost while their subscribers remain satisfied with the availability of network connections during attacks? A cost-performance analysis of the effectiveness of DDOS defenses is being conducted using results from the computational model. This cost-performance analysis will aid ISPs and local network administrators in their evaluation of DDOS defenses. Second, the researchers ask where are the critical points in a network to deploy defenses? The researchers examine the impact of network topology on the deployment location of defenses. Graph level <br\/>indices and models from social network studies will be used to categorize network topologies and to select deployment locations for defenses. This analysis will provide guidance to decision makers.<br\/><br\/>Benefits of this work research include: The policy framework the researchers are developing will help ISPs and subscribers to consider the benefits of providing DDOS defenses and to realize the tradeoffs in DDOS defenses. Results from this study will enable decision makers to make more <br\/>informed security policy decisions for computer networks. It is costly and unethical to conduct real world experiments of DDOS attacks on large networks. This research will provide a cost effective and ethical means for evaluating various attack scenarios and defenses. Further, topological measures developed in this research should be useful for studies of other large-scale topologies. As such, this work extends social network measures typically used on small person-to-person <br\/>networks to large-scale computer networks. Finally, this research provides a theoretical basis for evaluating DDOS defenses building on interdisciplinary studies from the fields of computer science, <br\/>information science, organizational theory and social network analysis.","title":"ITR: Modeling Distributed Denial of Service Attacks and Defenses","awardID":"0218466","effectiveDate":"2002-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"V050","name":"NSA-WORLD RENOWNED EXPERTS FRO"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["268366"],"PO":["565090"]},"74688":{"abstract":"A fundamental challenge in network design is how to choose the right protocol, with the right parameter settings, for some given circumstances. A key problem that lacks a systematic formal framework is the scalability issue in the design space: networks rapidly grow not only in size and heterogeneity, but also in the number of available design choices, such as the right protocol to select, or just setting the right parameters for a given protocol.<br\/><br\/>To help remedy this situation we propose MERIT, a formal framework for systematically assessing protocols in dynamic scenarios. The key novelty of the researcher's solution is its inherent scalability: rather than comparing the different protocols directly, the researchers compute an inherent measure of the protocol. Using this measure the researchers define a spectrum,<br\/>called a MERIT spectrum, for the protocol that characterizes it in a way that ensures comparability with other protocols without porting them to the same platform. In addition, we develop a systematic way of analyzing these spectra.<br\/><br\/>While the traditional qualitative and quantitative measures areinformative, they do not provide a satisfactory framework for a systematic and unified comparison of protocols. For example, as shown in the literature, the resulting ranking often strongly depends on the chosen simulation model. The researchers take a new, orthogonal direction in the assessment of routing protocols in MERIT. Specifically, the MERITframework provides a systematic and general way to rank any given routing protocol by comparing it to a theoretical, yet efficiently computable, optimum in its own system, rather than to a competing protocol.<br\/><br\/>For the problem of routing in a mobile ad hoc network, the researchers show that the theoretical optimum can be meaningfully defined and computed. The basis of the researchers approach is a characterization based on the ratio of the MEan ``Real'' to the ``Ideal'' cosT, or MERIT, of the protocol, and the researchers call the measure the MERIT ratio. The MERIT ratio as a function of<br\/>some parameters defines the MERIT spectrum of the protocol, yielding a multi-faceted representation of protocol effectiveness.<br\/><br\/>The successful preliminary theoretical and experimental results of the MERIT framework have encouraged the researchers to develop three primary research directions: (1) Further development of the mathematical model in the MERIT framework, with special emphasis on creating formal methods to analyze MERIT spectra. (2) Creating a software tool for the systematic study of routing protocols using the MERIT framework. (3) Extensive experimentation with real routing protocols to capture and systematically compare their behaviour through analyzing their MERITspectra.<br\/><br\/>Regarding the broader impact of the project, the contributions combine to impact the research in the field of dynamic networks. For mobile ad hoc networks, there is an impact in both theoretical model development and also simulation based performance analysis. The three primary research directions of the project are appropriate to help train graduate students to become highly skilled to conduct both theoretical work with strong mathematical foundations, as well as experimental<br\/>work using realistic network models in simulation. The methodologies will also be introduced into the graduate curriculum as well as research seminars that may also involve students from underrepresented groups. While the original motivation for MERIT was to assess routing<br\/>protocols in mobile ad hoc networks, the methods are also applicable to other dynamic scenarios. Thus, the benefit to society, in addition to the production of well educated and skilled rofessionals, is that networks will run more efficiently since MERIT provides a methodology<br\/>to select the right protocol for the job. The results will be broadly disseminated in conference and journal papers, as well as book chapters.","title":"ITR: A Formal Framework for Systematic Protocol Assessment","awardID":"0220001","effectiveDate":"2002-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["451890","521384"],"PO":["7594"]},"78923":{"abstract":"EIA- 0239993<br\/> H.V. Jagadish<br\/>University of Michigan<br\/><br\/>Title: Workshop on Data Management Technology for Molecular and Cell Biology<br\/><br\/>This workshop, to be held in February 2003, will define a research agenda for data management technology in support of bioinformatics applications. Specifically, it will be supporting basic and applied research in molecular and cell biology, genomics, functional genomics, structural biology, biochemistry, genetics, molecular phylogeny, pharmacology, pharmcogenomics, chemoinformatics, systems biology of the cell, and industrial microbiology. This workshop aims to look into novel approaches of database management to fit the needs of bioinformatics research applications, rather than continue to support conventional database technology. <br\/><br\/>The workshop will include substantial participation from bioinformatics researchers and developers, and biological and pharmaceutical researchers. Workshop topics will include but are not limited to the following: data integration: terminology management, seamless access, decision support; sequence data, shape data, graph and tree data, array and matrix data; similarity queries, pattern matching queries, recursive and graph queries; and workflow and experiment management.","title":"Workshop on Data Management Technology for Molecular and Cell Biology","awardID":"0239993","effectiveDate":"2002-10-01","expirationDate":"2004-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1713","name":"WORKFORCE"}}],"PIcoPI":["533266","563727"],"PO":["565136"]},"67989":{"abstract":"Information personalization refers to the automatic adjustment of information content, structure, and presentation tailored to an individual user. The goal of this research project is to develop a modeling methodology for information personalization. The methodology developed in this project termed PIPE (\"Personalization is Partial Evaluation\") makes no commitments to a particular personalization algorithm or format for information resources. Instead it emphasizes the representation of information systems in a way that allows their subsequent personalization. With this methodology, web sites and other information resources can be modeled and personalized for users' information-seeking goals. The specific activities conducted in this project include (1) characterizing the types of information systems for which this methodology is applicable and (2) constructing \"personable\" information system designs. The first activity is approached by defining how information systems are constructed and the representations they afford. The second activity involves the definition of a \"personability\" metric for evaluating information system designs. Human-computer interaction methodologies and systematic procedures for evaluation need to be enhanced to provide the needed input for the model formulation and validation. The results of this project will help define \"personable information spaces\" rigorously. With a formal model for personalization, we can design better information systems that can help users achieve their information-seeking goals.","title":"SGER: Personalization by Partial Evaluation","awardID":"0136182","effectiveDate":"2002-10-01","expirationDate":"2004-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6855","name":"INFORMATION & KNOWLEDGE MANAGE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6856","name":"ARTIFICIAL INTELL & COGNIT SCI"}}],"PIcoPI":["549540","498314"],"PO":["563751"]},"70750":{"abstract":"EIA-0204028 Alexandru Nicolau University of California-Irvine An Application Development Environment for Complex Heterogeneous Distributed Real-Time Embedded Computing Platforms<br\/><br\/>This proposal, describes a number of coordinated strategies that will develop to ensure robust and automatic composition of ( DRE) distributed real-time and embedded systems, based on advances in architectural and resource descriptions that enable concurrent exploration. The DRE applications to be explored in this work are representative of a range of real world distributed computing scenarios form Grids to microelectronic systems-on-chip (SOCs). SOC-based computing resources incorporating diverse sensing and substantial processing capabilities are useful in many DRE application domains, such as avionics, biomedical computing resources and tele-medicine, remote sensing, space exploration, and command and control systems. Examples of domains that could benefit from these advances include: automated transportation systems distance learning, tele-medicine, analysis for combat situations, video conferencing, virtual reality simulation, and weather forecasting and analyses.<br\/><br\/>The core of our proposal focuses on coordinated compile-time and runtime strategies that enable simultaneous and integrated optimization of (1) DRE application and middleware software and (2) the underlying hardware platform consisting of distributed high-performance processing elements and customized memory systems.","title":"NGS: An Application Development Environment for Complex Heterogeneous Distributed Real-Time Embedded Computing Platforms","awardID":"0204028","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2884","name":"NEXT GENERATION SOFTWARE PROGR"}}],"PIcoPI":["532656","541836","457653","534923","384407"],"PO":["301532"]},"73842":{"abstract":"EIA-0216178<br\/>David R. Alexander<br\/>Wichita State University<br\/><br\/>MRI: Acquisition of High Performance Computer For Wichita State University<br\/><br\/>This proposal from an institution in an EPSCoR state, expanding the high performance computer in their High Performance Computing Center, enables the faculty in physics, chemistry, mechanical and aeronautical engineering, and their students to solve large-scale computational problems. Over 120 faculty, staff, and students will use this centralized facility. The National Institute would also use the machine cycles for Aviation Research (NIAR). The current facilities have been outgrown. Below follows some examples of the projects underway:<br\/><br\/>1. Integrated Computational Tool for Hypersonic Flow Simulations<br\/>2. Evaluation and Retro-fit of Fail-Safety in KC-135 Fuselage Structure<br\/>3. Design of Plasma Sprayed Coatings through Computer Simulation<br\/>4. Computational Analysis in Adhesive-Bonded Composite Joints<br\/>5. Quantum Neural Networks<br\/>6. Model Atmospheres for Cool Stars<br\/>7. Mixed-Metal Complexes: Multielectron Transfer Agents<br\/>8. Molecular Modeling of Dimeric and Polymeric Ionic Liquids","title":"MRI: Acquisition of High Performance Computer For Wichita State University","awardID":"0216178","effectiveDate":"2002-10-01","expirationDate":"2004-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1189","name":"MAJOR RESEARCH INSTRUMENTATION"}},{"dir":{"id":"11","name":"Directorate for DIRECT FOR EDUCATION AND HUMAN RESOURCES","abbr":"EHR"},"div":{"id":"1108","name":"Division of EXPER PROG TO STIM COMP RSCH","abbr":"EPS"},"pgm":{"id":"9150","name":"EXP PROG TO STIM COMP RES"}}],"PIcoPI":["473578"],"PO":["557609"]},"73149":{"abstract":"EIA-0213662<br\/>Margolis, Jane <br\/>UCLA<br\/><br\/>Title: Constructing the Computer Science Pipeline: How high school structures and norms narrow access to Computer Science for underrepresented minority students<br\/><br\/>This ITR- and ITWF-sponsored research project seeks to understand the institutional, contextual constraints and opportunities that affect underrepresented minority students' decisions to study (or not study) computer science at the high school level. The researchers will investigate how the computer science pipeline gets constructed. \"Technological portraits\" for three Los Angeles public high schools will be developed and in-depth interviews will be conducted with approximately forty district-level technology administrators, principals, counselors, computer science and math teachers, and technology and magnet coordinators at these three schools. Each of these three schools has a high numbers of African-American and Latino\/a students.<br\/><br\/>The researchers will study the perceptions and thinking of those who help to construct the pipeline-namely, state, district, and local educators. In particular, they will focus on 1) educators' perceptions of what a computer science curriculum should be for their students; 2) the courses and curriculum sequences that these educators shape; 3) the criteria they use to judge which students should study computer science and which are not capable of success ; and 4) how these norms and structures that influence the construction of the pipeline translate into educators' role in shaping students' course-taking choices. Dissemination of findings is an important part of this proposal and the investigators will hold on-going discussions with school educators about these issues. Printed materials will be produced for distribution in the three schools, and meetings\/discussions will be held with the teachers about the research findings. The findings will be disseminated nationally.","title":"ITR: Constructing the Computer Science Pipeline: How High School Structures and Norms Narrow Access to Computer Science for Underrepresented Minority Students","awardID":"0213662","effectiveDate":"2002-10-01","expirationDate":"2006-06-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1713","name":"WORKFORCE"}}],"PIcoPI":["324990","528136"],"PO":["289456"]},"71115":{"abstract":"Valuable information about the world around us is being lost. Every day we lose information of both current and historical significance, simply because it is too difficult to capture 3D models of our environment. Today, such models are primarily created by hand, making measurements with tape measures and entering the results manually. Even after models are constructed, we do not know the best ways to visualize them effectively.<br\/><br\/>This research proposes to design and develop an end-to-end solution to the problem of documentation, dissemination, and display of real-world environments. To focus the research, two driving problems are targeted: forensic science and education. This research brings together an interdisciplinary team including the law enforcement community, the Federal Bureau of Investigation and the Armed Forces Institute of Pathology. Additionally, one of our team members, from the New Orleans Museum of Art, is directing a major exhibition in which we propose to include a virtual environment of Monticello, President Jefferson's home.<br\/><br\/>We will tackle the problems of acquiring, processing, representing, rendering and display of real- world data, focusing on the two target applications. Our goal is to design and build a complete end-to-end prototype system. The work includes:<br\/><br\/>Acquisition. We will mount an accurate rangefinder on a motorized cart and tackle the problem of capturing as complete a model as possible. This requires accurate tracking and a solution to the next best view problem in a practical sense, including resolution, reflectance, and importance of the surfaces.<br\/>Processing. A very difficult problem is the registration of color with depth data, and of scans from multiple locations. We will develop algorithms to robustly register the data, and filter to reduce outliers and noise.<br\/>Representation. We will develop algorithms to process the massive amounts of data (100 MB per scan, tens of scans per room) into both efficient geometric and image-based representations. We will compare and contrast the representations in terms of quality and efficiency.<br\/>Rendering. We will develop hierarchical and out-of-core algorithms to render the image- based and geometric models at interactive rates, and research perceptually driven rendering techniques to maximize rendering efficiency.<br\/>Display. This is perhaps the most difficult part of the problem because it involves the way humans perceive the environment. We will develop a hybrid projector\/head-mounted-display system to provide stereo views to multiple viewers in a life-sized walking environment. The goal is to give the participants a sense of presence, of being in Mr. Jefferson's house.","title":"ITR: Collaborative Research: Image-Based Rendering in Forensic Science, Education, and Historical Preservation","awardID":"0205425","effectiveDate":"2002-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}}],"PIcoPI":["561957","550455","409896",184104,"332385"],"PO":["565272"]},"74646":{"abstract":"The world is rapidly being populated by a wide array of information appliances such as computers, personal digital assistants, video cameras, digital light projectors, and cable set-top boxes. These<br\/>appliances are no longer simply islands of technology, but have the ability to communicate with other devices in their environment. Future Internet applications will increasingly make use of these<br\/>devices by organizing them in a distributed fashion. Examples of these applications include distributed sensor arrays, tele-immersion, computer-supported collaborative workspaces (CSCW), ubiquitous computing environments, and complex multistream, multimedia presentations. In these applications, no one device or computational resource produces or manages all of the data streams<br\/>transmitted. Instead, these applications will be distributed over a collection of devices in an environment. These types of applications are called \"cluster-to-cluster\" (C-to-C) applications.<br\/><br\/>In a C-to-C application, a set of processes distributed on a cluster of devices or computers communicates with another set of processes on a remote cluster of devices or computers. Processes are distributed on the device clusters at either end to manage the data streams and control the application. Unfortunately, traditional network technologies and protocols are ill-equipped to support these kinds of applications.<br\/><br\/>The fundamental problem is that current transport-level protocols operate in isolation from other traffic flows and within a strict understanding of \"end-to-end.\" The transport-level protocols used by multiple, related, data streams that share a common path have no coordination mechanism. As a result, each process in a C-to-C application competes with other processes within the same application for network resources. Also, each transport-level protocol must make independent assessments of current network conditions (e.g., delay, congestion, etc.) which can lead to inconsistent and adversarial protocol dynamics. Furthermore, the numerous data streams produced by a C-to-C application will have semantic relationships known only to the application. These relationships affect how the data should be transmitted and the operation of transport-level protocols. A C-to-C application needs a global view of performance across all of its flows<br\/>in order to make adaptations to dynamic network conditions. Independent adaptation of the separate flows uninformed of aggregate application performance is insufficient.<br\/><br\/>A key characteristic of C-to-C applications is that a large portion of the communication path is shared among all data flows. Although no two data flows may share an entire end-to-end path, all data flows share the path between the two clusters. Furthermore, this shared common path is likely to be where network conditions vary the most and the source of congestion due to outside traffic. Exploiting this characteristic, our general approach to the problem is to introduce mechanisms at the points of aggregation for data streams in a cluster application that determine network conditions along the shared common path and provide a consistent and coordinated view of available network resources.<br\/><br\/>Specifically, a major contribution of our work will be to introduce an additional protocol between the network level (IP) and the transport level (TCP, UDP, etc.). We call this protocol the \"Coordination Protocol\" (CP). This additional protocol will be used to communicate information between forwarding mechanisms within the cluster infrastructure on either end of the application. The information exchanged by these mechanisms is used to estimate the dynamic network conditions along the shared path (i.e., congestion, latency, jitter, etc.). Because a single mechanism is charged with estimating network conditions across all flows of a multistream application, all individual transport-level protocols involved in a C-to-C application receive a consistent and coordinated view of current network conditions. The application can then react to congestion, loss, and other network events in a manner that incorporates application-level knowledge and achieves global objectives.<br\/><br\/>The major features of the researchers approach are:<br\/> - Provides a consistent measurement of network conditions across all application flows.<br\/> - Preserves end-to-end semantics of transport-level protocols.<br\/> - Locally deployable.<br\/> - Independence from application architecture.<br\/> - Serves as a framework for other types of coordination.<br\/><br\/>As a whole, the aggregate network behavior of all the flows of a C-to-C application should respond appropriately to congestion and other network conditions. How each individual stream responds,<br\/>however, can only be determined at the application level because only the application has an understanding of how the streams are related. The coordination protocol we will develop provides cluster ap","title":"ITR: Protocol Coordination for Multistream Applications","awardID":"0219780","effectiveDate":"2002-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["292988"],"PO":["7594"]},"74679":{"abstract":"In the past few decades, high-performance computing has driven the development of practical medical applications that are now widely available, such as magnetic resonance imaging and computerized tomography. In recent years, information processing is undergoing rapid advances driven by the use of distributed computing systems connected by world-wide networks. Analogous to power grids, \"computational grids\" have the potential to provide seamless access to high-performance computers from ubiquitous, network-enabled devices. The unprecedent levels of computation enabled by this model may foster the development of new medical applications that can substantially improve healthcare. An example is found in a class of emerging medical applications that use Light-Scattering Spectroscopy (LSS) imaging to allow in-vivo detection of pre-cancerous changes in human epithelium. Effective deployments of LSS imaging will depend on the availability of high levels of performance and require access to remote resources. This project aims to improve the state-of-the-art in data management techniques for grids to allow seamless and high-performance integration of data generated by medical instrumentation devices with distributed computers. The long-term objective is to enable deployments of network-computing based medical applications for early cancer detection that require processing capabilities beyond those available in healthcare facilities. In particular, this project will develop computing techniques to enable accurate and fast analyses of LSS images.<br\/><br\/>To this end, this project will focus on three specific aims. The first aim is the development of<br\/>high-performance, parallel implementations of LSS analysis algorithms. The second aim is the development of data management techniques that allow on-demand access to data generated by a network-enabled instrumentation device from off-site computing resources; these techniques are based on the notion of per-user virtual file system proxies that are controlled by grid middleware and allow latencyhiding performance enhancements to be decoupled from operating system and application implementations. The third aim consists of the integration of the proposed solutions with computational grid infrastructures to enable dissemination to the research community. The results of this project will be used and lead to the development and implementation of such LSS imaging system for clinical use in collaboration with the Northwestern University Medical School and Northwestern Memorial Hospital.","title":"ITR: Fine-Grain Data Management in Computational Grids and Applications in Network-Enabled Medical Imaging for Early Cancer Detection","awardID":"0219925","effectiveDate":"2002-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["558495","559053"],"PO":["565272"]},"75889":{"abstract":"This project is a novel decentralized infrastructure, based on distributed hash tables (DHTs), that will enable a new generation of large-scale distributed applications. The key technology on which we build, DHTs, are robust in the face of failures, attacks and unexpectedly high loads. They are scalable, achieving large system sizes without incurring undue overhead. They are self-configuring, automatically incorporating new nodes without manual intervention or oversight. They simplify distributed programming by providing a clean and flexible interface. And, finally, they provide a shared infrastructure simultaneously usable by many applications.<br\/><br\/>The approach advocated here is a radical departure from both the centralized client-server and the application-specific overlay models of distributed applications. This new approach will not only change the way large-scale distributed systems are built, but could potentially have far reaching societal implications as well. The main challenge in building any distributed system lies in dealing with security, robustness, management, and scaling issues; today each new system must solve these problems for itself, requiring significant hardware investment and sophisticated software design. The shared distributed infrastructure will relieve individual applications of these burdens, thereby greatly reducing the barriers to entry for large-scale distributed services.<br\/><br\/>Our belief that DHTs are the right enabling infrastructure is based on two conjectures: (1) a DHT with application-independent, unconstrained keys and values provides a general purpose interface upon which a wide variety of distributed applications can be built, and (2) distributed applications that make use of the DHT-based infrastructure inherit basic levels of security, robustness, ease of operation, and scaling. Much of the thrust of the proposed research is an exploration of these two conjectures.<br\/><br\/>We will investigate the first conjecture, that the DHT abstraction can support a wide range of applications, by building a variety of DHT-based systems. Our recent work has used DHTs to support such varied applications as distributed file systems, multicast overlay networks, event notification systems, and distributed query processing. DHTs simplify the structure of these systems by providing general-purpose key\/value naming rather than imposing structured keys (e.g., hierarchical names in DNS). These systems are early prototypes, but they suggest that DHTs may be as useful to distributed applications as ordinary hash tables are to programs.<br\/><br\/>The second conjecture relies on techniques for creating robust, secure, and self-organizing infrastructures out of many mutually distrustful nodes. Our initial work on robust DHT designs gives us confidence that such techniques are within reach. The bulk of our proposed research will be devoted to the in-depth study of these techniques, with the express aim of producing a sound and coherent design for the infrastructure. To investigate the real-world behavior of our design, we will create a large-scale open testbed for which we will distribute our infrastructure software, some enabling libraries, and a few key compelling applications.<br\/><br\/>In addition to its impact on the creation of distributed applications, our research program will have benefits in education and outreach. Given their current importance, security, robustness, and the design of distributed systems should become central topics in undergraduate computer science education. To this end, we are planning a new interdisciplinary course that will address these issues, and bring them into sharper focus early in the undergraduate course sequence.<br\/><br\/>Our testbed and research agenda is also a good vehicle for encouraging the participation of organizations not traditionally involved in networking and systems research. Participation in the testbed requires little cost (a PC and an Internet connection) and minimal levels of systems expertise and over-sight. Moreover, because the material is closely related to the P2P systems with which many students are familiar, the project might appeal to students who would not normally be attracted to research in this area. Based on this premise, we plan an active outreach program to underrepresented populations at non-research undergraduate institutions.","title":"ITR: Robust Large-Scale Distributed Systems","awardID":"0225660","effectiveDate":"2002-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1688","name":"ITR LARGE GRANTS"}}],"PIcoPI":["562011","560562"],"PO":["565090"]},"75912":{"abstract":"This award provides funds to subsidize the travel and housing expenses of students selected to participate in the sixth SIGART\/AAAI Doctoral Consortium, which will be held on July 28 and 29, 2002, in Edmonton, Canada. The consortium will be collocated with the Eighteenth National Conference on Artificial Intelligence (NCAI), which will be held from July 28 to August 1 in Edmonton. ACM\/SIGART and AAAI together organized the first six Doctoral Consortia, which have also been collated with NCAI (except for last year, when the consortium was collocated with the International Joint Conference on Artificial Intelligence (IJCAI)). <br\/><br\/>At the consortium, Ph.D. students who are doing their dissertations on AI-related topics present their proposed research, and receive feedback from a panel of established researchers as well as from the other students. This provides the students with invaluable exposure to outside perspectives on their work, at a critical time in their research, and also enables them to explore their career objectives. There will also be a panel on career options as part of the consortium. This workshop contributes to the professional development of young scientists who will lead this growing field in the coming decades.","title":"AAAI-2002 SIGART\/AAAI Doctoral Consortium","awardID":"0225754","effectiveDate":"2002-10-01","expirationDate":"2003-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6856","name":"ARTIFICIAL INTELL & COGNIT SCI"}}],"PIcoPI":["558297"],"PO":["564456"]},"75615":{"abstract":"EIA 0224387<br\/>Czarkowski, Dariusz<br\/> Erkip, Elza <br\/>Goodman, Davis J.<br\/>Karri, Ramesh.<br\/>Wang, Yao<br\/>Polytechnic University of New York <br\/><br\/>Title: CISE RR: Instrumentation for Research on Energy Aware Multimedia Information Terminals <br\/><br\/>This project, performing studies related to minimizing the energy consumed by mobile cellular and wireless networking hardware, builds a framework to understand, model and measure power consumption, as well as dynamic allocation of resources to affect power consumption in portable multimedia communication devices. Three platforms constitute the focus for implementing signal processing algorithms (general microprocessors, e.g., Pentium and ARM), digital signal processors (DSP), and reconfigurable hardwarefine grain field programmable gate arrays (FPGA) and coarse grain Application Specific Programmable Processors (ASPP). The work considers two communications systems that will carry the majority of traffic in wireless Internets: cellular networks and wireless local area networks (WLAN). Because battery life depends not only on total energy drain but also on the voltages employed in the terminal and the temporal profile of energy consumption, the research includes experiments with lithium batteries to determine the effects on battery life of continuous discharge and pulsed discharge with various peak-to-average ratios. The results of these experiments are then merged with the results of signal processing and communications studies in an energy-management testbed that verifies predictions and demonstrates the effects of the new adaptation techniques. Hence, the following four components form the research plan: <br\/>Theoretical and simulation studies exploring power allocation among source coding, channel coding, encryption, and transmission in a multimedia, multiuser setting <br\/>Measurement and design study examining the power and energy requirements of signal processing algorithms and producing a software library of software modules, FPGA configurations, and ASPPs that can be dynamically selected by a portable device <br\/>Measurement and design study creating an intelligent power supply system that takes into account effects of battery discharge characteristics and demands of algorithms implemented on microprocessors, DSPs, FPGAs, and ASPPs.<br\/>Creation of experimental testbed running in the WLAN network from the results of the first three studies <br\/>The experimental testbed addresses three areas of research: <br\/>Power allocation among different components of the processing pipeline (source coding, channel coding, encryption, transmission); <br\/>Power and energy requirements of signal processing algorithms, producing a library of various granularity modules (software, FPGA, ASPP) to be selected dynamically; and<br\/>Power supply system that takes into account both battery characteristics and algorithms needs.<br\/>On the educational side, specific plans involve undergraduate students hands-on work, organization of workshop, and summer school. Industrial partners and international collaboration also form part of the research.","title":"CISE-RR: Instrumentation for Research on Energy Aware Multimedia Information Terminals","awardID":"0224387","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2890","name":"CISE RESEARCH RESOURCES"}}],"PIcoPI":["258464","491898","550604","289115","559524"],"PO":["557609"]},"74405":{"abstract":"EIA-0218479<br\/>Ulinski, Philip S<br\/>Univ. of Chicago<br\/><br\/>CRCNS: Collaborative Research : How is Information Coded in Turtle Visual Cortex ?<br\/><br\/>Visual stimuli evoke a propagating wave of activity in the visual cortex of freshwater turtles. Preliminary work suggested that information about the position of stimuli in visual space is coded in the spatiotemporal dynamics of these waves. Effectively, there may be a map of visual space to the dynamics of the visual cortex. This hypothesis is being examined in a collaborative effort involving three laboratories. David Senseman in San Antonio is using voltage sensitive dye methods to record the waves produced by presenting spots of light at 35 spots on the retina. These studies will characterize the features of the map based on repeated presentations of stimuli at 35 loci. Philip Ulinski in Chicago is developing a large-scale model of the visual pathway of turtles. Models of individual retinal ganglion cells that combine both classic filter-based approaches to modeling ganglion cells, with compartmental modeling of ganglion cells are being constructed. They are being used to construct 35 patches of a model retina that match the 35 loci. Physiological studies of the biophysics of neurons in the lateral geniculate complex of turtles are being carried out. They are used to develop a model of the lateral geniculate complex, which is the last step in modeling the retino-geniculate-cortical pathway. Bijoy Ghosh in St. Louis is developing refined estimation techniques that allow the position of a visual stimulus to be estimated from the dynamics of the cortical waves. This work is providing the mathematical framework needed to characterize a potential map of visual space to the dynamics of the wave. This work is significant because it is characterizing a novel method of coding information in visual cortex that may apply to higher order cortical areas in mammals, as well as turtles.","title":"CRCNS: Collaborative Research: How is Information Coded in Turtle Visual Cortex ?","awardID":"0218479","effectiveDate":"2002-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":[193230],"PO":["564318"]},"77826":{"abstract":"ABSTRACT<br\/>Proposal Number 0234689<br\/>TITLE Verifying Properties of Systems Software<br\/>PI Alex Aiken<br\/><br\/>This research focuses on enforcing critical properties in software such as device drivers, high-performance servers, network protocols, and embedded systems. The plan is to design, build and experiment with an analysis tool for C and C++ programs that will achieve two goals:<br\/><br\/>(1) It will check the full range of critical properties of systems software, from low-level checks, such as preventing buffer overruns and null pointer dereferences, to high-level, program-specific<br\/>checks, such as verifying the steps of a protocol are executed in the correct order.<br\/><br\/>(2) The tool will be used for investigating the trade-offs between static (at compile-time) and dynamic (at run-time) enforcement of properties.<br\/><br\/>The impacts of this work should include: first, a better understanding of when it is sufficient to use the weaker but less expensive (in programmer effort) checking at runtime instead of static checking at compile-time; a coherent model of how to integrate very low-level and high-level program checking in a single system; and a prototype tool that others without specialized knowledge of program analysis can use to improve the quality of complex systems written in C and C++.","title":"Verifying Properties of Systems Software","awardID":"0234689","effectiveDate":"2002-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7214","name":"HIGHLY DEPENDABLE COMPUTING"}}],"PIcoPI":["507674","451874"],"PO":["564388"]},"75406":{"abstract":"As the objective of vision (human and machine) is to compute a hierarchy of increasingly abstract interpretations of the observed images or image sequences, it is of fundamental importance to know what are the concepts used at each level of interpretation. In more plain language, what are the visual \"strokes\", visual \"characters\", and visual \"words\"? Or what are the visual \"electrons\", \"atoms\" and \"molecules\"? The goal of the proposed research is to discover dictionaries of various levels of visual concepts that correspond to fundamental topologic, photometric, geometric, and dynamic structures of the images and scenes. In a mathematical language, these structures are the low dimensional manifolds embedded in very high dimensional image space.<br\/><br\/> More specifically, we propose to construct top-down generative models for natural images, 3D surfaces, human faces, video sequences, and 2D shape contours. The fundamental atomic structures are defined by parameters in the generative models, and these parameters are estimated by fitting the models to the training data. These structures are intrinsic to the ensemble of natural images and video. We propose stochastic (Markov chain Monte Carlo) learning algorithms which is capable of computing globally optimal solutions.","title":"Learning Fundamentals Atomic Image Structures From Natural Images, Video and Shapes","awardID":"0222967","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6840","name":"ROBOTICS"}}],"PIcoPI":["546023","456867"],"PO":["317663"]},"74438":{"abstract":"EIA-0218693<br\/>Poggio, Tomaso<br\/>MIT<br\/><br\/> Collaborative Research: CRCNS: Detection and Recognition of Objects in Visual Cortex<br\/><br\/>A three way collaboration between the laboratories of Profs. T. Poggio at MIT, D. Ferster at Northwestern University and C. Koch at Caltech is exploring and evaluating the hypotheses that the cortical organization and the neural mechanisms of visual recognition can be explained by a coherent theoretical framework built on two existing computational models for recognition and attention and, secondly, that a combination of physiological work on monkeys and cats, together with visual psychophysics can be used to test and refine the theory. The research is organized into three main projects. The work at MIT is guided by a quantitative hierarchical model of recognition, probing the relations between identification and categorization and the properties of selectivity and invariance of the neural mechanisms in IT cortex. The work at Northwestern University is testing a key prediction of the model about the nature of the pooling operation (a max operation vs. a linear sum) performed by complex cells in V1. The experiments are done in the anesthetized cat, intracellularly, to allow for a characterization of the underlying circuit and biophysical mechanisms. Finally, work at Caltech is extending the basic model of recognition by integrating it with a saliency-based attentional model. The computational component of this work, centered around the development of a quantitative model of visual recognition, constitutes the primary tool to enforce interactions between the investigators: the model suggests experiments and guides planning and interpreting new experiments.","title":"Collaborative Research: CRCNS: Detection and Recognition of Objects in Visual Cortex","awardID":"0218693","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["523294",193317,"435190","463935","438544"],"PO":["564318"]},"74229":{"abstract":"This research will study the interpersonal activity between trainer\/trainee in a collaborative visual inspection Virtual Environment and will involve a new Collaborative Virtual Environment (CVE) capable of real-time capture and visualization of eye movements of one of the participants in the CVE. Of particular importance is the recording and real-time depiction of visual deictic references made with the eyes by either instructor or student in the environment. Deictic references are associated with pointing and verbal expressions (such as \"look at this\" or \"see that?\"). Collection of explicit visual deictic references, as made by the eyes, will facilitate task-based analysis and modeling of visual attention during human-human interaction in Virtual and Physical Reality.<br\/><br\/>The main objective of this project is to develop a new interactive 3D visual inspection system in Virtual Reality, based on an existing single-person virtual aircraft inspection environment. To allow the measurement of visual deictic reference, it will extend the current single-person Virtual Environment (VE) to a Collaborative Virtual Environment (CVE) which will allow the simultaneous participation of two individuals. The main technological innovations are extensions to available eye tracking techniques to allow real-time capture and visualization of eye movements of one of the participants during simultaneous participation in the CVE. The fundamental exploration of visual deictic reference will be addressed during evaluation of visual inspection of the environment by the instructor (expert) and the trainee (novice).<br\/><br\/>Existing approaches have failed to yield significant improvements in inspection performance. The successful completion of this research will provide a better understanding of the issues related to human-human interaction and training. Results from this research are expected to advance our understanding of complex multi-party environments where gaze-based communication is employed. The CVE will allow visualization and analysis of human attentional factors in shared-space environments. The study of visual deictic reference will lead to novel insights into human learning and behavior, notably learned improvement of visual search strategies. The research effort involves a multidisciplinary team of researchers with expertise in virtual reality systems, vision based research, human\/machine systems design, inspection systems design, and training and computer supported collaborative work.<br\/><br\/>Educational activities involving the curriculum and laboratory training will be closely integrated with the research, involving both undergraduate and graduate students. Although primarily focused on aircraft inspection, the anticipated results have the potential to impact a wide range of tasks in the maintenance and service industries, such as inspection of products in a manufacturing environment, flight crew systems training and training of air traffic control personnel, real-time military target acquisition, process control monitoring, and baggage inspection (aviation security).","title":"ITR: Visual Deictic Reference in a Collaborative Virtual Environment for Visual Search Training","awardID":"0217600","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["525585","408667","479564"],"PO":["564456"]},"70720":{"abstract":"Jack W. Davidson Illinois Institute of Technology Collaborative Research: Continuous Compilation: A New Approach to Aggressive and Adaptive Code Transformation<br\/><br\/> Today's challenge for optimization research is to develop new techniques and approaches that yield performance improvements that were typical of early optimization research-20 to 40 percent and more. In this research, we address this challenge by investigation and developing an innovative system or applying optimizations. Our system, the Continuous Compilation System (CoCo), applies optimizations both statically at compile-time and dynamically at run-time using optimization plans developed at compile time and adapted at run time.<br\/><br\/> To demonstrate the practicality and utility of our approach, initially we will apply CoCo to several large, long-running applications. Examples of such applications include ecological simulations, weather simulation, architecture simulators, and VLSI routing and placement applications. As we gain insight and experience with the application of continuous compilation, we will investigate its effectiveness on other types of codes such as multimedia applications, signal processing applications, and web servers.","title":"NGS: Collaborative Research: Continuous Compilation: A New Approach to Aggressive and Adaptive Code Transformation","awardID":"0203956","effectiveDate":"2002-10-01","expirationDate":"2004-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2884","name":"NEXT GENERATION SOFTWARE PROGR"}}],"PIcoPI":["385658"],"PO":["301532"]},"74835":{"abstract":"EIA 02-20590<br\/>Hartley, Roger T.<br\/>Leung, Hing Pontelli, <br\/>Enrico Ranjan, Desh<br\/> Tran, Son<br\/>New Mexico State University <br\/><br\/>Title: Frameworks for the Development of Efficient and Scalable Knowledge-Based Systems<br\/> <br\/>This proposal, expanding activities geared to attract and retain Native American students, aims at developing a solid collaborative research backbone, enhancing its educational program, and improving participation of a diverse student population in computer-related disciplines. The backbone, enabling a cooperative research efforts in the Computer Science Department, fosters research and educational collaborations with other departments (specifically Biology and Psychology) and with other research institutions. The proposed research backbone is articulated in four inter-dependent threads: <br\/><br\/>Data structures and methodologies for efficient parallel execution of logic and constraint programming languages <br\/><br\/>Languages and methodologies for the design of knowledge based systems <br\/><br\/>Application of knowledge-based technology in Semantic Web, Universal Accessibility, and Computational Biology <br\/><br\/>Automated debugging and component-based programming for knowledge-based applications<br\/>Enhancement of the educational program is expected to lead to improved recruitment and retention, increased transition towards graduate programs, and a stronger integration between research and education at the graduate<br\/> and undergraduate level. These goals will be accomplished through the introduction of a pathways system throughout the undergraduate curriculum, where different pathways will accommodate the diverse student backgrounds. Focusing on an educational model for the training of Native American students in Computer Science, the institution expects to improve participation of a diverse student population in computer-related disciplines, with particular focus on the creation of an educational model for the training of Native American students in Computer Science. The proposed infrastructure provides research support, in the form of computing equipment (e.g., a 64-processor Beowulf platform, HP shared memory platform, robotic equipment) and human resources support (for faculty, students, and visiting scientists). The educational infrastructure includes the creation of a new computing classroom and provides human resources for the development of the new educational programs. A research team, consisting of 14 investigators, will benefit from this infrastructure by strengthening their interdisciplinary research through cross-fertilization of new ideas.","title":"MII: Frameworks for the Development of Efficient and Scalable Knowledge-based Systems","awardID":"0220590","effectiveDate":"2002-10-01","expirationDate":"2009-09-30","fundingAgent":[{"dir":{"id":"03","name":"Directorate for DIRECT FOR MATHEMATICAL & PHYSICAL SCIEN","abbr":"MPS"},"div":{"id":"0309","name":"Division of CHEMISTRY","abbr":"CHE"},"pgm":{"id":"1253","name":"OFFICE OF MULTIDISCIPLINARY AC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"2860","name":"THEORY OF COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"6856","name":"ARTIFICIAL INTELL & COGNIT SCI"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1713","name":"WORKFORCE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2802","name":"TRUSTED COMPUTING"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2876","name":"DISTRIBUTED SYSTEMS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2885","name":"CISE RESEARCH INFRASTRUCTURE"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7399","name":"CISE MINOR INST INFRA (MII) PR"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7584","name":"ITR-BROADENING PARTICIPATION"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1706","name":"DIGITAL GOVERNMENT"}},{"dir":{"id":"06","name":"Directorate for DIRECTORATE FOR GEOSCIENCES             ","abbr":"GEO"},"div":{"id":"0601","name":"Division of edDATA NOT AVAILABLE","abbr":"EDNE"},"pgm":{"id":"4710","name":"DES AUTO FOR MICRO & NANO SYS"}},{"dir":{"id":"07","name":"Directorate for DIRECTORATE FOR ENGINEERING             ","abbr":"ENG"},"div":{"id":"0705","name":"Division of ENGINEERING EDUCATION AND CENT","abbr":"EEC"},"pgm":{"id":"1340","name":"ENGINEERING EDUCATION"}},{"dir":{"id":"08","name":"Directorate for DIRECT FOR BIOLOGICAL SCIENCES          ","abbr":"BIO"},"div":{"id":"0801","name":"Division of ENVIRONMENTAL BIOLOGY","abbr":"DEB"},"pgm":{"id":"4715","name":"COMPUTER SYSTEMS ARCHITECTURE"}}],"PIcoPI":["268559","340589","482838","561141","435957"],"PO":["565272"]},"74626":{"abstract":"This proposal concerns new improvements that have the potential to achieve significant speed-up for the fast multipole method (FMM) for use in solving the Helmholtz and other problems used to model phenomena encountered in electromagnetics, acoustics, biology etc. Solving larger problems holds promise for better design on the one hand, and elucidation of new physics\/biology on the other. Discretizations of the partial differential equations arising from these equations yield large systems of equations for which both direct and iterative solution techniques are expensive.<br\/><br\/>The introduction by Rokhlin & Greengard of the FMM generated tremendous interest in the scientific computing community, as it demonstrated a way to generate structure and achieve fast solution of equations without relying on the discretization. Despite its promise, the algorithm has not achieved widespread implementation for many practically important problems that could use the promised speedups. Some researchers have reported that the approximate integrals both make implementation difficult, and in practice they have been shown to introduce stability problems. We have recently derived exact expressions for the translation and rotation of multipole solutions of the Helmholtz equation, which enable fast computation via simple recursions. Further we have obtained very promising results on the properties of the translation operators that enable creation of very tight error bounds. Our translations have the same asymptotic complexity as the standard integral expressions, but with much smaller coefficients. We have also found that the translation operator can be decomposed into the product of sparse recurrence matrices and this can be the basis for a T(p2) algorithm, which we propose to pursue. Based on these expressions, we will develop software for solution of different problems using the FMM. To be useful in pushing ahead the information technology revolution our software will be well documented and published in accessible peer reviewed forums. Such availability will act to improve adoption by large numbers of practitioners.","title":"ITR\/SF&IT: Fast Multipole Translation Algorithms for Solution of the 3D Helmholtz Equation","awardID":"0219681","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"V518","name":"NGA-KNOW THE EARTH-SHOW THE"}}],"PIcoPI":["442474","450658","485207","532931","532929"],"PO":["565272"]},"71128":{"abstract":"Conventional computer architecture has been based on and dominated by the<br\/>capacity poor systems we were forced to build 20 and 50 years ago. For<br\/>over three decades, the Instruction Set Architecture (ISA) has formed the<br\/>basic architectural abstraction for scalability. As wonderful as the ISA<br\/>abstraction has been, its range of utility is coming to an end. Our<br\/>systems have grown orders of magnitude in size and speed, and many of the<br\/>simplifying assumptions which underly the ISA are limiting forward<br\/>scalability rather than enabling them. This effort explores a<br\/>reformulation of our basic assumptions and machine abstractions to<br\/>accommodate the large capacity computing systems we can now build. Whereas<br\/>the sequence of primitive instructions was the core fixed point in ISA<br\/>model, this effort proposes stream-connected graphs as the model fixed<br\/>point for these Decentralized Streaming Architectures (DSA). This way<br\/>communication is abstracted directly in the architecture facilitating<br\/>parallelism and optimization for physical locality. DSA will give us a<br\/>single, unifying system model for future computing systems which scales<br\/>from modest, single-chip, single-processor systems, to multiprocessor ICs,<br\/>to a wide range of heterogeneous System-on-a-Chip designs, to large-scale,<br\/>heterogeneous, multi-component systems that evolve over decade long<br\/>lifecycles.","title":"ITR: Decentralized Streaming Architecture (DSA) for High Capacity Systems","awardID":"0205471","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}}],"PIcoPI":["342426","497005","313642"],"PO":["325495"]},"74516":{"abstract":"EIA-0219088<br\/>Koch, Christof<br\/>California Institute of Technology<br\/><br\/> Collaborative Research: CRCNS: Detection and Recognition of Objects in Visual Cortex<br\/><br\/>A three way collaboration between the laboratories of Profs. T. Poggio at MIT, D. Ferster at Northwestern University and C. Koch at Caltech is exploring and evaluating the hypotheses that the cortical organization and the neural mechanisms of visual recognition can be explained by a coherent theoretical framework built on two existing computational models for recognition and attention and, secondly, that a combination of physiological work on monkeys and cats, together with visual psychophysics can be used to test and refine the theory. The research is organized into three main projects. The work at MIT is guided by a quantitative hierarchical model of recognition, probing the relations between identification and categorization and the properties of selectivity and invariance of the neural mechanisms in IT cortex. The work at Northwestern University is testing a key prediction of the model about the nature of the pooling operation (a max operation vs. a linear sum) performed by complex cells in V1. The experiments are done in the anesthetized cat, intracellularly, to allow for a characterization of the underlying circuit and biophysical mechanisms. Finally, work at Caltech is extending the basic model of recognition by integrating it with a saliency-based attentional model. The computational component of this work, centered around the development of a quantitative model of visual recognition, constitutes the primary tool to enforce interactions between the investigators: the model suggests experiments and guides planning and interpreting new experiments.","title":"Collaborative Research: CRCNS: Detection and Recognition of Objects in Visual Cortex","awardID":"0219088","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0506","name":"Division of EXPERIMENTAL & INTEG ACTIVIT","abbr":"EIA"},"pgm":{"id":"1705","name":"BIOLOGY & INFORMATION TECHNOLO"}}],"PIcoPI":["435190"],"PO":["564318"]},"71018":{"abstract":"ITR: Multi-scale Analysis, Modeling, and Simulation<br\/>This proposal will operate in the context of the Caltech Center for Integrative Multi-scale Modeling and Simulation (CIMMS) whose purpose is to develop, and foster multi-scale innovations in a wide range of areas spanning the physical, mathematical, information, and computational sciences. It provides the infrastructure that allows for the mix of domain experts from fluids, materials and geophysics with specialists in mathematical and computational analysis. <br\/>This proposal aims to:<br\/>1. Develop fundamental insights into multi-scale interactions, especially the coarse graining of dynamics and how they affect the accuracy of naive coarse-scale simulations; <br\/>2. Assemble existing ideas about multi-scale interactions scattered across a wide range of disciplines into a coherent whole, ready for broad dissemination;<br\/>3. Codify and develop systematic techniques to accurately model and simulate across space and time scales;<br\/>4. To test them systematically in a wide range of important scientific settings;<br\/>5. Develop and distribute software and associated infrastructure supporting the rapid and thorough diffusion of these techniques;<br\/>6. Raise consciousness among scientists, engineers, educators, and society in general about the importance of accurate physical modeling, the fundamental role of the multi-scale hierarchy as an obstacle to naive modeling and the existence of powerful ideas and convenient tools available to scale the hierarchy.<br\/>Overarching Research Themes. Two key themes cut across and unifying the scientific research: modeling across scales, and multi-scale algorithms. They reflect two groups of researchers: those developing modeling techniques to get the physics and computations right across scales and those who develop the computational and information technology infrastructure, including algorithms and other mathematical tools, such as ridgelets and curvelets. The first years will concentrate on macromolecules and crystalline materials, and imaging and data analysis. In future years, other areas may be included, such as geophysics, space sciences and additional topics in biology, all of which present challenging multi-scale problems.","title":"ITR: Multiscale Analysis, Modeling, and Simulation","awardID":"0204932","effectiveDate":"2002-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"1688","name":"ITR LARGE GRANTS"}}],"PIcoPI":["448655","564848"],"PO":["565272"]},"71139":{"abstract":"We propose to develop a comprehensive framework for the joint analysis of audio-visual signals obtained<br\/>from spatially distributed microphones and cameras. We desire solutions to the audio-visual sensing problem that will scale to an arbitrary number of cameras and microphones and can address challenging environments in which there are multiple speech and nonspeech sound sources and multiple moving people and objects. Recently it has become relatively inexpensive to deploy tens or even hundreds of cameras and microphones in an environment. Many applications could benefit from ability to sense in both modalities.<br\/><br\/>There are two levels at which joint audio-visual analysis can take place. At the signal level, the challenge<br\/>is to develop representations that capture the rich dependency structure in the joint signal and deal success-fully issues such as variable sampling rates and varying temporal delays between cues. At the spatial level the challenge is to compensate for the distortions introduced by the sensor location and pool information across sensors to recover 3-D information about the spatial environment.<br\/><br\/>For many applications, it is highly desirable if the solution method is self-calibrating, and does not<br\/>require an extensive manual calibration process every time a new sensor is added or an old sensor is moved<br\/>or replaced. Removing the burden of manual calibration also makes it possible to exploit ad hoc sensor<br\/>networks which could arise, for example, from wearable microphones and cameras.<br\/><br\/>We propose to address the following four research topics:<br\/><br\/>1. Representations and learning methods for signal level fusion.<br\/>2. Volumetric techniques for fusing spatially distributed audio-visual data.<br\/>3. Self-calibration of distributed microphone-camera systems<br\/>4. Applications of audio-visual sensing.<br\/><br\/>For example, this proposal includes considerable work on lip and facial analysis to improve voice<br\/>communications.","title":"ITR: Analysis of Complex Audio-Visual Events Using Spatially Distributed Sensors","awardID":"0205507","effectiveDate":"2002-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}}],"PIcoPI":["508256",184201,"550806"],"PO":["433760"]},"74704":{"abstract":"Rapid advances in networking and Internet technologies has fueled the emergence of the \"software as a service\" model for enterprise computing that enables organizations to outsource many Information Technology (IT) services. This model allows organizations to concentrate on their core business instead of sustaining large investments in IT. IT outsourcing results in savings from the economies of scale due to leveraging of hardware, software, personnel, as well as maintenance and upgrade costs. Outsourcing is a common practice in Enterprise Resource Planning (ERP) and Customer Relationship Management (CRM) domains and it is gaining popularity in basic services such as email, storage and disaster protection.<br\/><br\/>This research will explore the data privacy challenges that arise in outsourcing data management services. Data management systems are among the most common, expensive, and complex software systems used by almost all types of organizations. In the envisioned \"database as a service\" (DAS) model, the client's data resides on the premises of the service provider and is accessed using SQL queries. Since clients' data as a very valuable asset, the service provider must implement sufficient security measures to guarantee data privacy. The research will explore the resulting challenges: (1) Privacy protection from malicious outsiders: protecting service providers from theft of customer data (e.g., hackers breaking into a provider's site and scanning all disks). (2) Privacy protection from database service providers: assuring that clients' encrypted data cannot be decrypted at the service provider. Thus, techniques to evaluate queries over encrypted data at the service provider need to be developed. (3) Ensuring Integrity of the Results: developing scalable techniques to ensure that the service provider returns exactly the right answer set to the client's query.","title":"ITR: Privacy in Database-As-A-Service (DAS) Model","awardID":"0220069","effectiveDate":"2002-10-01","expirationDate":"2007-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"7371","name":"CYBER TRUST"}}],"PIcoPI":["532654","515756","515757"],"PO":["521752"]},"74726":{"abstract":"EIA - 0220153<br\/>Hood, Leroy<br\/>Institute for Systems Biology <br\/><br\/>TITLE: Development of an integrated computational and experimental approach to predict biological networks in Halobacterium sp.<br\/><br\/>The extremely halophilic archaeon Halobacterium sp. NRC-1, whose sequenced genome contains 2,630 genes, is being used as a model for predicting biological networks in microbial systems. The genes are compared across species to identify putative protein interactions by integrating identified protein\/protein interactions from orthologous proteins, domain gene fusions, gene localization, and mRNA and protein expression data using both existing and new algorithms. The functional biomodules are being determined by integrating this information with additional information from biochemical pathways, shared promoter motifs, and a new protein distance algorithm. These approaches are then integrated into a visualization and integration platform, Cytoscape, being developed at the Institute for Systems Biology (ISB).","title":"ITR: Development of an Integrated Computational and Experimental Approach to Predict Biological Networks in Halobacterium sp","awardID":"0220153","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["415743",194121,"554018",194123,"196175"],"PO":["565136"]},"71129":{"abstract":"The problems of 3D shape representation, analysis, manipulation,<br\/>and estimation from unstructured sensor data are central not only<br\/>to computer vision, but are also of key significance to geometric<br\/>modeling, computer graphics, computational geometry, medical imaging,<br\/>surveillance, among others. The goal of this proposal is to investigate<br\/>approaches to these problems in the context of tackling certain<br\/>key problems in archaeology. This area of application offers<br\/>the following three distinct advantages: (i) it is a rich source<br\/>of free-form shapes, e.g., pottery sherds, marble plates and reliefs,<br\/>sculpture, column capitals, building facades, etc., which are<br\/>easily accessible, in contrast to 3D free-form shapes obtained<br\/>from medical images which require expert segmentation; this data<br\/>is also richer and more variable than data available in a typical<br\/>manufacturing environ-ment; (ii) the problems are of a generic<br\/>nature and the proposed approaches can be applied to a variety<br\/>of other domains; (iii) this application area forms a key bridge<br\/>between the Physical Sciences and the Humanities.","title":"ITR: 3D Free Form Models for the Representation, Manipulation, and Recovery of Shape, with Applications to Archaeology and Virtual Sculpting","awardID":"0205477","effectiveDate":"2002-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1687","name":"ITR MEDIUM (GROUP) GRANTS"}}],"PIcoPI":["520864","333515","550631",184164,184165],"PO":["564456"]},"75617":{"abstract":"EIA 0224401<br\/>Murphy, Robin R.<br\/>University of South Florida <br\/><br\/>Title: CISE RR: R4: Rescue Robots for Research and Response <br\/><br\/>This project, developing and maintaining robotics kits for nationwide research, aims at facilitating research into robot-assisted urban search and rescue (USAR) by providing resources to overcome barriers presenting research involvement (lack of access to domain experts and meaningful sites combined with the need for expensive specialized equipment), while at the same time increasing the availability of mobile robots for an emergency response. This work supports the<br\/>Expansion of existing caches of robots suitable for USAR research in two different regions,<br\/>Training on these robots, data collection, and access to these robots and data sets via the Internet,<br\/>Field data collection exercises with fire rescue professionals at USAR sites, and<br\/> Loan of robots to individual researchers. <br\/>The work facilitates groups, without rescue robots or access to domain experts, to have access to data and\/or work directly with USAR robots and rescue experts in the field. <br\/><br\/>Rescue robotics is a complex domain in need of fundamental breakthroughs in distributed computing, networking and communications, software engineering and interoperability, real-time multi-threaded control, user interfaces and human-robot interaction, robotics, sensors, and artificial intelligence (AI). Hence, the Computing Research Association declared this research area a \"grand challenge.\" Four types of research equipment, micro-robots, mini-robots, image processing and video capturing equipment, and sensors, enable researchers to collect the data and evaluate their work in the cited areas. In addition to integrating existing technology and eliciting effective cooperation between professionals and researchers in the case of an emergency, the project provides education in robotics for homeland defense, and the potential to safeguard lives of rescue workers and locate victims.","title":"CISE Research Resources: R4: Rescue Robots for Research and Response","awardID":"0224401","effectiveDate":"2002-10-01","expirationDate":"2008-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2890","name":"CISE RESEARCH RESOURCES"}}],"PIcoPI":["565226"],"PO":["557609"]},"74528":{"abstract":"Although the internet's origins lie in computer and communication technology, control and stability theory have also had a major influence on its development. After the development of the additive increase, multiplicative decrease algorithm by Jacobson in 1988, the first significant Active <br\/>Queue Management (AQM) algorithm introduced was Random Early Detection (RED). In RED, the basic idea is to let the senders know about a high queue occupancy situation by increasing the <br\/>drop probability at the gateways as a linear function of the time-averaged queue occupancy. Field observations of anomalous behavior for TCP networks with and without deployment of RED have been made that show irregular load batching, load oscillations, and high parametric sensitivity. <br\/>Given the heterogeneity of the internet, and the associated wide variations of parameters, extending the range of stable operation in parameter space is clearly very important. In this research program, tools from nonlinear dynamical systems and from bifurcation and chaos control will be harnessed for the analysis and control of congestion in networks with one or several bottlenecks with TCP-type and\/or UDP-type (User Datagram Protocol) traffic. The project emphasizes work of a <br\/>fundamental nature to develop an understanding of network congestion dynamics and address some of the difficult systems issues that are at the heart of the congestion avoidance and <br\/>control problem. The project builds on recent work of the PI's group on the analysis of nonlinear dynamics in TCP-RED and mixed traffic networks. The project also leverages past contributions of the PI on control of nonlinear instabilities (bifurcation control) and on closed-loop monitoring of the <br\/>stability of nonlinear uncertain systems.","title":"ITR: Nonlinear Dynamics-Based Robust Congestion Control","awardID":"0219162","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["559435"],"PO":["7594"]},"75639":{"abstract":"EIA 0224465<br\/>Shankar, Natarajan<br\/>Stickel, Mark E.<br\/>SRI International <br\/><br\/>Title: CISE RR: QPQ: An Open Source Deductive Software Repository <br\/><br\/>This project, establishing a deductive software repository, aims at making software more available, reliable, durable, higher quality, uniform. Deductive software consists of symbolic routines for logical manipulations including decision procedures, solvers, rewriters, model checkers, and theorem provers. It serves as the semantic foundation for many scientific and engineering applications in areas such as hardware and software verification, program synthesis and analysis, data- and knowledge-based systems, artificial intelligence, linguistics, and e-commerce. This work outlines an initiative for deductive software components called QPQ (\"quid pro quo\") as a marketplace for publishing, exchanging, and refining deductive software components. The refereed publication and distribution of such scientific software components in open source often yields higher quality, greater visibility, and accelerated productivity. Efficient implementation of deductive algorithms, an extremely complex and delicate task, requires serious investment in basic infrastructure such as concrete and abstract syntax, syntactic tools (parsers, typecheckers, pretty-printer), primitive operations (substitution, matching, unification, normalization), and advanced operations (constraint satisfaction, forward and backward chaining, rewriting, simplification, decision procedures, induction methods). Potential impacts include availability, reliability, quality, durability, productivity, visibility, uniformity, synergy, serendipity, and history. The following principles will be employed in running the repository: authenticity, uniqueness, access, relevance, format, quality and version control, and academic credit. QPQ is already recognized as a valuable resource for research and education in deductive software. With its role in facilitating the semantic web and through other applications of logico-symbolic computing, the QPQ repository might become crucial to society.","title":"CISE Research Resources: QPQ: An Open Source Deductive Software Repository","awardID":"0224465","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2890","name":"CISE RESEARCH RESOURCES"}}],"PIcoPI":[196814,"555363"],"PO":["557609"]},"70722":{"abstract":"EIA-0203960 Marty A. Humphrey University of Virginia Legion-G: Delivering a Scalable and Secure Programming Model for Grid Computing<br\/><br\/>To date, Grid computing lacks a secure, persistent, fault-tolerant, Grid-enabled object model that is standards-based and interoperable with the isolated Grid computing solutions that are emerging from the work in the Global Grid Forum. Grid-enabled objects must be dynamically instantiated and located, and users and administrators need to be able to better visualize performance and debugging, and graphically move objects across Grid boundaries. This project addresses these problems with three contributions. First, the lack of flexibility will be solved via the creation of Legion-G, a revolutionary \"applications-level\" interface of Legion to Globus. Second, new scheduling support for components across Grids will be developed. This \"Continuous Scheduling\" modeling will facilitate faster customization and more powerful, dynamic control of Grid-based component applications, resulting in higher performance and productivity. Third, the Grid Debugging Visualizer (GDV) will give Grid systems administrators and Grid users more precise, real-time information regarding the timing errors and functional errors of the component-based Grid computations.","title":"NGS: Delivering a Scalable and Secure Programming Model for Grid Computing","awardID":"0203960","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2884","name":"NEXT GENERATION SOFTWARE PROGR"}}],"PIcoPI":["466826"],"PO":["301532"]},"70733":{"abstract":"EIA-0203984 Jack J. Dongarra University of Tennessee Component-based Frameworks for High-Performance Distributed Scientific Applications <br\/><br\/>Scientific computer applications are continuing to evolve to solve more complex physical problems. The result is an increase in composite applications that are built from smaller element applications, each of which model a simpler physical problem. As composite applications are generally large, they tend to be built by teams. In addition, these composite applications tend to be distributed over a number of computers using a combination of data-parallel and task-parallel designs. They typically share data via files and databases. More sophisticated designs will include message passing, remote event signals, and other distributed computing techniques.<br\/><br\/>To realize this vision, we must overcome significant scientific and technical obstacles. The proposed project includes research on a programming model for large, composite, scientific applications that are targeted to run in computer Grid environments. This research will focus on developing efficient techniques to program and execute these applications including support for managing behavioral aspects of integrating element applications into a composite application. This research will be use to develop a component-based framework where a clear, concise, and accurate description of such applications can be created and used to execute the application in a high-performance, distributed environment.","title":"NGS: Component-based Frameworks for High-Performance Distributed Scientific Applications","awardID":"0203984","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"1640","name":"INFORMATION TECHNOLOGY RESEARC"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0505","name":"Division of COMPUTER AND NETWORK SYSTEMS","abbr":"CNS"},"pgm":{"id":"2884","name":"NEXT GENERATION SOFTWARE PROGR"}}],"PIcoPI":["558550","501527",183104],"PO":["301532"]},"70865":{"abstract":"ABSTRACT<br\/>0204377<br\/>Shewchuk, Jonathan<br\/>U of Calif Berkeley<br\/><br\/>We intend to develop fast, versatile, and visually accurate computational models for viscoplas-tic<br\/>materials ranging from stiff, non-compliant solids to low viscosity fluids. We are designing<br\/>these models for applications where visual realism, computation speed, and robustness are the<br\/>predominant requirements (with numerical accuracy being subordinate). Examples of such<br\/>applications include real-time interactive training simulations (e.g. surgical simulation or haz-ardous<br\/>duty simulations) and off-line generation of visualizations (e.g. cinematic effects or<br\/>accident reenactment).<br\/>To achieve this goal, we must develop fast, guaranteed-quality methods for generating and in-crementally<br\/>updating unstructured (irregular) triangular and tetrahedral meshes. Dynamically changing meshes are a necessity to model the complete range of viscoplastic materials, espe-cially where large deformations and mixing may occur. Thus, the actions of the numerical simulation and the remeshing algorithms must be tightly integrated, especially if we wish to minimize errors due to interpolation and reinterpolation. To ensure that our dynamic meshing algorithms and implementations are useful for other applications as well, we will develop a general methodology for communicating information between the numerical simulation and<br\/>the mesh generator.","title":"Animating Viscoplastic Materials with Dynamically Changing Meshes","awardID":"0204377","effectiveDate":"2002-10-01","expirationDate":"2005-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"2865","name":"NUMERIC, SYMBOLIC & GEO COMPUT"}},{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0501","name":"Division of COMPUTER & COMMUNICATION FOUND","abbr":"CCF"},"pgm":{"id":"7453","name":"GRAPHICS & VISUALIZATION"}}],"PIcoPI":["408891","563370"],"PO":["532791"]},"74727":{"abstract":"EIA-0220154<br\/>Gusfield, Daniel<br\/>University of California-Davis<br\/><br\/>ITR: Algorithmic Problems in Population-Scale Genomics<br\/><br\/>Now that genomic level technologies are widely available (for sequencing, resequencing, <br\/>micro-array screening of sequences etc.), the dream of comparing sequence variations at the population level is starting to become a reality. These comparisons will be used to help to identify the genetic basis of disease susceptibility, and will have additional uses. This shift to opulation-scale genomics introduces a new set of computational problems, and provides a huge opportunity for high-impact algorithm development and research.<br\/><br\/>This project focuses on novel, critical computational problems <br\/>that arise in population-scale genomic data acquisition and analysis. The specific computational problems to be addressed arise out of on-going population-scale investigations into <br\/>population-level genomic variability. It will focus on novel computational problems that have not been previously formulated and addressed, and problems <br\/>where additional formulations are needed to better capture the <br\/>relevant biology. Although the algorithmic techniques will be grounded in theoretical computer science and discrete mathematics (and the results will be of interest in those fields), the standards for success will be the ultimate applicability of the results in genomics. The research will be conducted at several levels: modeling and defining important problems; finding and developing efficient algorithms; implementing and distributing software for the most important results; and applying the software on population-scale genomic data. Our first focus is on problems related to computationally extracting haplotype information from genotype information, and the construction and use of haplotype maps. The larger significance of the project will be the <br\/>development of powerful computational tools for use by geneticists and gene mappers, which will help to more effectively identify the genetic bases for disease susceptibility and other important genetic traits.","title":"ITR: Algorithmic Problems in Population-Scale Genomics","awardID":"0220154","effectiveDate":"2002-10-01","expirationDate":"2006-09-30","fundingAgent":[{"dir":{"id":"05","name":"Directorate for DIRECT FOR COMPUTER & INFO SCIE & ENGINR","abbr":"CSE"},"div":{"id":"0502","name":"Division of INFORMATION & INTELLIGENT SYST","abbr":"IIS"},"pgm":{"id":"1686","name":"ITR SMALL GRANTS"}}],"PIcoPI":["518691","447975"],"PO":["565136"]}}